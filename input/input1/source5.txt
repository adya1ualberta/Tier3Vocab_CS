we have developed three computer programs for comparisons of protein and dna sequences they can be used to search sequence data bases evaluate similarity scores and identify periodic structures based on local sequence similarity the fasta program is a more sensitive derivative of the fastp program which can be used to search protein or dna sequence data bases and can compare a protein sequence to a dna sequence data base by translating the dna data base as it is searched fasta includes an additional step in the calculation of the initial pairwise similarity score that allows multiple regions of similarity to be joined to increase the score of related sequences the program can be used to evaluate the significance of similarity scores using a shuffling method that preserves local sequence composition the lfasta program can display all the regions of local similarity between two sequences with scores greater than a threshold using the same scoring parameters and a similar alignment algorithm these local similarities can be displayed as a graphic matrix plot or as individual alignments in addition these programs have been generalized to allow comparison of dna or protein sequences based on a variety of alternative scoring matrices we have been developing tools for the analysis of protein and dna sequence similarity that achieve a balance of sensitivity and selectivity on the one hand and speed and memory requirements on the other three years ago we described the fastp program for searching amino acid sequence data bases which uses a rapid technique for finding identities shared between two sequences and exploits the biological constraints on molecular evolution fastp has decreased the time required to search the national biomedical research foundation nbrf protein sequence data base by more than two orders of magnitude and has been used by many investigators to find biologically significant similarities to newly sequenced proteins there is a trade off between sensitivity and selectivity in biological sequence comparison methods that can detect more distantly related sequences increased sensitivity frequently increase the similarity scores of unrelated sequences decreased selectivity in this paper we describe a new version of fastp fasta which uses an improved algorithm that increases sensitivity with a small loss of selectivity and a negligible decrease in speed we have also developed a related program lfasta for local similarity analyses of dna or amino acid sequences these programs run on commonly available microcomputers as well as on larger machines methods the search algorithm we have developed proceeds through four steps in determining a score for pair wise similarity fastp and fasta achieve much of their speed and selectivity in the first step by using a lookup table to locate all identities or groups of identities between two dna or amino acid sequences during the first step of the comparison the ktup parameter determines how many consecutive identities are required in a match for example if ktup for a dna sequence comparison only those identities that occur in a run of four consecutive matches are examined in the first step the best diagonal regions are found using a simple formula based on the number of ktup matches and the distance between the matches without considering shorter runs of identities conservative replacements insertions or deletions in the second step of the comparison we rescore these regions using a scoring matrix that allows conservative replacements and runs of identities shorter than ktup to contribute to the similarity score for protein sequences this score is usually calculated using the matrix although scoring matrices based on the minimum number of base changes required for a replacement or on an alternative measure of similarity can also be used with fasta for each of these best diagonal regions a subregion with maximal score is identified we will refer to this region as the initial region the best initial regions from fig la are shown in fig the fastp program uses the single best scoring initial region to characterize pair wise similarity the initial scores are used to rank the library sequences fasta goes one step further during a library search it checks to see whether several initial regions may be joined together given the locations of the initial regions their respective scores and a joining penalty analogous to a gap penalty fasta calculates an optimal alignment of initial regions as a combination of compatible regions with maximal score fasta uses the resulting score to rank the library sequences we limit the degradation of selectivity by including in the optimization step only those initial regions whose scores are above a threshold this process can be seen by comparing fig with fig fig shows the highest scoring initial regions after rescoring with the matrix the best initial region reported by fastp is marked with an asterisk fig shows an optimal subset of initial regions that can be joined to form a single alignment in the fourth step of the comparison the highest scoring library sequences are aligned using a modification of the optimization method described by needleman and wunsch and smith and waterman this final comparison considers all possible alignments of the query and library sequence that fall within a band centered around the highest scoring initial region fig with the fastp program optimization frequently improved the similarity scores of related sequences by factors of or because fasta calculates an initial similarity score based on an optimization of initial regions during the library search the initial score is abbreviation nbrf national biomedical research foundation the publication costs of this article were defrayed in part by page charge payment this article must therefore be hereby marked advertisement in accordance with u s c solely to indicate this fact proc natl acad sci usa a n x b fig identification of sequence similarities by fasta the four steps used by the fasta program to calculate the initial and optimal similarity scores between two sequences are shown a identify regions of identity b scan the regions using a scoring matrix and save the best initial regions initial regions with scores less than the joining threshold are dashed the asterisk denotes the highest scoring region reported by fastp c optimally join initial regions with scores greater than a threshold the solid lines denote regions that are joined to make up the optimized initial score d recalculate an optimized alignment centered around the highest scoring initial region the dotted lines denote the bounds of the optimized alignment the result of this alignment is reported as the optimized score much closer to the optimized score for many sequences in fact unlike fastp the fasta method may yield initial scores that are higher than the corresponding optimized scores local similarity analyses molecular biologists are often interested in the detection of similar subsequences within longer sequences in contrast to fastp and fasta which report only the one highest scoring alignment between two sequences local sequence comparison tools can identify multiple alignments between smaller portions of two sequences local similarity searches can clearly show the results of gene duplications see fig or repeated structural features see fig and are frequently displayed using a graphic matrix plot which allows one to detect regions of local similarity by eye optimal algorithms for sensitive local sequence comparison can have tremendous computational requirements in time and memory which make them impractical on microcomputers and when comparing longer sequences on larger machines as well the program for detecting local similarities lfasta uses the same first two steps for finding initial regions that fasta uses however instead of saving initial regions lfasta saves all diagonal regions with similarity scores greater than a threshold lfasta and fasta also differ in the construction of optimized alignments instead of focusing on a single region lfasta computes a local alignment for each initial region thus lfasta considers all of the initial regions shown in fig instead of just the diagonal shown in fig furthermore lfasta considers not only the band around each initial region but also potential sequence alignments for some distance before and after the initial region starting at the end of the initial region an optimization proceeds in the reverse direction until all possible alignment scores have gone to zero the location of the maximal local similarity score in the reverse direction is then used to start a second optimization that proceeds in the forward direction an optimal path starting from the forward maximum is then displayed the local homologies can be displayed as sequence alignments see fig or on a two dimensional graphic matrix style plot see figs and statistical significance the rapid sequence comparison algorithms we have developed also provide additional tools for evaluating the statistical significance of an alignment there are approximately protein sequences with million amino acid residues in the nbrf protein sequence library and any computer program that searches the library by calculating a similarity score for each sequence in the library will find a highest scoring sequence regardless of whether the alignment between the query and library sequence is biologically meaningful or not accompanying the previous version of fastp was a program for the evaluation of statistical significance rdf which compares one sequence with randomly permuted versions of the potentially related sequence we have written a new version of rdf that has several improvements i calculates three scores for each shuffled sequence one from the best single initial region as found by fastp a second from the joined initial regions used by fasta and a third from the optimized diagonal it can be used to evaluate amino acid or dna sequences and allows the user to specify the scoring matrix to be employed thus sequences found using the scoring matrix can be evaluated using the identity or genetic code matrix iii the user may specify either a global or local shuffle routine locally biased amino acid or nucleotide composition is perhaps the most common reason for high similarity scores of dubious biological significance high scoring alignments between query and library sequences may be due to patches of hydrophobic or charged amino acid residues or to a t or g c rich regions in dna a simple monte carlo shuffle analysis that constructs random sequences by taking each residue in one sequence and placing it randomly along the length of the new sequence will break up these patches of biased composition as a result the scores of the shuffled sequences may be much lower than those of the unshuffled sequence and the sequences will appear to be related alternatively shuffled sequences can be constructed by permuting small blocks of or residues so that while the order of the sequence is destroyed the local composition is not by shuffling the residues within short blocks along the sequence patches of g c or a t rich regions in dna for example are undisturbed evaluating significance with a local shuffle is more stringent than the global approach and there may be some circumstances in which both should be used in conjunction whereas two proteins that share a common evolutionary ancestor may have clearly significant similarity scores using either shuffling strategy proteins related because of secondary structure or hydropathic profile may have similarity scores whose significance decreases dramatically when the results of global and local shuffling are compared implementation the fasta lfasta package of sequence analysis tools is written in the c programming language and has been implemented under the unix vax vms and ibm pc dos operating systems versions of the program that run on the ibm pc are limited to query se l i i biochemistry pearson and lipman biochemistry pearson and lipman table fasta and fastp initial scores of the t cell receptor rwmsav versus the nbrf data base initial score nbrf code sequence fasta fastp rwhuav t cell receptor a chain klhure ig k chain v i region ig k chain v region ig k chain precursor v regions ig k chain v region ig a chain v iii region ig k chain precursor v region rwmsbv t cell receptorp chain precursor rwhuvy t cell receptor chain precursor rwhugv t cell receptor y chain precursor t cell surface glycoprotein rwmsvb t cell receptor chain precursor ig heavy chain v region glhudw ig heavy chain v ii region the average fastp score mean sd the average fasta score mean sd the mean and sd were computed excluding scores v variable quences of residues library sequences can be any length copies of the program are available from the authors although fasta and lfasta were designed for protein and dna sequence comparison they use a general method that can be applied to any alphabet with arbitrary match mismatch scoring values all the scoring parameters including match mismatch values values for the first residue in a gap and subsequent residues in the gap and other parameters that control the number of sequences to be saved and the histogram intervals can be specified without changing the program examples comparison of fasta with fastp to demonstrate the superiority of the fasta method for computing the initial score we compared the protein sequence of a t cell receptor a chain nbrf code rwmsav with all sequences in the nbrf protein data baser and computed initial scores with both the present and previous methods the t cell receptor is a member of the immunoglobulin superfamily in release of the data base this superfamily has members fastp placed immunoglobulin superfamily sequences in the top scoring sequences related sequences received initial scores less than four standard deviations above the mean score fasta placed superfamily members in the top scoring sequences only related sequences scored below four standard deviations above the mean table contains specific examples from this data base search although there is often little difference in the two methods this example shows that in a number of cases the new method obtains significantly higher scores between related sequences nucleic acid data base search fasta can also be used to search dna sequence data bases either by comparing a dna query sequence to the dna library or by comparing an amino acid query sequence to the dna library by translating each library dna sequence in all six possible reading frames we compared the nucleotide rat transforming growth factor type a mrna genbank locus rattgfa with all the mammalian sequences in release of gen bank we set ktup see methods and the search was completed in under min on an ibm pcat microcom table dna data base search of rat transforming growth factor rattgfa versus mammalian sequences genbank score locus sequence initial optimized humtfgam human tgf mrna human tgf gene exon human tgf gene end mouse rrna gene mouse rrna gene musmhdd mhc class i h hummetif1 metallothionein mt if gene musrglp rrna end mrna a type i procollagen the sequences having the highest initial scores are given tgf transforming growth factor mhc major histocompatibility complex puter the top scoring library sequences are shown in table although it can be seen that the top scoring sequences are clearly related to rattgfa there are other high scoring sequences that are probably not related and the mouse epidermal growth factor found in the translated data base search table is not found among the top scoring sequences to further examine the similarity detected between rattgfa and a mouse rrna gene cluster we used the program for monte carlo analysis of statistical significance the window for local shuffling was set to bases of the shuffled comparisons data not shown obtained an initial score greater than the observed initial score and shuffled sequences obtained optimized scores greater than the observed optimized score therefore the similarity between rattgfa and is unlikely to be significant translated nucleic acid data base search when searching for sequences that encode proteins amino acid sequence comparisons are substantially more sensitive than dna sequence comparisons because one can use scoring matrices like the matrix that discriminate between conservative and nonconservative substitutions a variant of fasta tfasta can be used to compare a protein sequence to a dna sequence library it translates the dna sequences into each of six possible reading frames on the fly tfasta translates the dna sequences from beginning to end it includes both intron and exon sequences in the translated protein sequence termination codons are translated into unknown x amino acids table shows the results of a translating search of the mammalian sequences in the gen bank dna data base using the rattgfa protein sequence as the query and ktup in the translated search the mouse epidermal growth factor now obtains an initial score higher than any unrelated sequences however which was found in the dna data base search but only contains translated codons is no longer among the top scoring sequences local similarities fig displays the output of a local similarity analysis ktup of a chimpanzee al globin mrna and rabhbapt a rabbit a globin gene including the complete coding sequence and a flanking pseudo globin gene lfasta can either display a graphic matrix style plot of the local homologies fig or the alignments themselves fig the right most three alignments fig match the corresponding regions of the mrna to exon subsequences from the pseudogene we note that the fasta initial score for the comparison of chphtprotein identification resource protein sequence database natl biomed res found washington dc release embl genbank genetic sequence database intelligenetics mountain view ca tape release proc natl acad sci usa distribution of extracorpuscular hemoglobin in plasma concentration and light absorption was linear figure indicating that the color intensity of the benzidine oxidatively transformed was directly proportional to the content of hemoglobin on paper in order to compare the relationship between hemoglobin concentration and light absorption for free and protein bound hemoglobin the following study was made hemoglobin was added to plasma in vitro in increments of from to mg per cent over a concentration range of to mg per cent following incubation of plasma at c in order to promote protein binding electrophoresis staining and photometric analysis were carried out in the usual manner protein binding was maximal in this experiment at a hemoglobin concentration of mg per cent at concentrations greater than mg per cent free hemoglobin appeared in increasing concentrations assuming that the concentration of pbh remained at mg per cent at all concentrations in excess of this maximal binding capacity the concentration of free hemoglobin at these higher to mg per cent concentrations was calculated as the difference between the known total hemoglobin concentration and the concentration of pbh mg per cent knowing therefore the actual concentrations of free and protein bound hemoglobin the light absorption of each fraction was measured and compared at each concentration studied figure illustrates that at all concentrations studied the relationship between hemoglobin concentration and light absorption of the oxidized benzidine bands was linear and was the same for free and q q free hb area cmn bound hb area fig relationship between concentration and light absorption benzidine stain of free hemoglobin relative to protein bound hemoglobin the concentration of free hemoglobin relative to the concentration of protein bound hemoglobin is plotted against the respective ratios of the light absorption of these constituents when stained with benzidine at each ratio of concentrations studied the straight line relationship shown indicates that the relationship between concentration and light absorption is the same for free and protein bound hemoglobin protein bound hemoglobin calculations of the concentrations of free and protein bound hemoglobin as determined by the photometric technique were therefore in close agreement with the actual concentrations of these components table i a similar study of methemalbumin revealed that the relationship between concentration and table i in vitro study of the recovery of free and protein bound hemoglobin hb as determined by photometric analysis total free hb free calculated bound hb bound calculated hb actual calculated free actual actual calculated bound actual 130 44 hemoglobin values are expressed as concentration in mg per cent biochemistry pearson and lipman fig repeated structure in the myosin heavy chain lfasta was used to compare the caenorhabditis elegans myosin heavy chain protein sequence nbrf code mwkw with itself using the scoring matrix the solid dashed and dotted lines denote decreasing similarity scores the solid lines had initial region scores greater than and optimized local scores greater than the longer dashed lines had initial region and optimized local scores greater than and respectively and the shorter dashed lines had initial region and optimized local scores greater than and respectively homologous regions with lower scores are plotted with dots scores must be carefully evaluated the monte carlo analysis of statistical significance provided by a program such as can often be critical in evaluating a borderline similarity previously we suggested ranges of z values observed score mean of shuffled scores standard deviation of shuffled scores corresponding to approximate significance levels however the z values determined in a monte carlo analysis become less useful as the distribution of shuffled scores diverges from a normal distribution as is found with fasta therefore we now focus on the highest scores of the shuffled sequences for example if in shuffled comparisons several random scores are as high or higher than the observed score then the observed similarity is not a particularly unlikely event one can have more confidence if in shuffled comparisons no random score approaches the observed score in general our experience has led us to be conservative in evaluating an observed similarity in an unlikely biological context these programs provide a group of sequence analysis tools that use a consistent measure for scoring similarity and constructing alignments fasta and lfasta all use the same scoring matrices and similar alignment algorithms so that potentially related library sequences discovered after the search of a sequence data base can be evaluated further from a variety of perspectives in addition lfasta can also show alternative alignments between sequences with periodic structures or duplications a computer program that progressively evaluates the hydrophilicity and hydrophobicity of a protein along its amino acid sequence has been devised for this purpose a hydropathy scale has been composed wherein the hydrophilic and hydrophobic properties of each of the amino acid side chains is taken into consideration the scale is based on an amalgam of experimental observations derived from the literature the program uses a moving segment approach that continuously determines the average hydropathy within a segment of predetermined length as it advances through the sequence the consecutive scores are plotted from the amino to the carboxy terminus at the same time a midpoint line is printed that corresponds to the grand average of the hydropathy of the amino acid compositions found in most of the sequenced proteins in the case of soluble globular proteins there is a remarkable correspondence between the interior portions of their sequence and the regions appearing on the hydrophobic side of the midpoint line as well as the exterior portions and the regions on the hydrophilic side the correlation was demonstrated by comparisons between the plotted values and known structures determined by crystallography in the case of membrane bound proteins the portions of their sequences that are located within the lipid bilayer are also clearly delineated by large uninterrupted areas on the hydrophobic side of the midpoint line as such the membrane spanning segment of these proteins can be identified by this procedure although the method is not unique and embodies principles that have long been appreciated its simplicity and its graphic nature make it a very useful tool for the evaluation of protein structures introduction one of the most persistent and absorbing problems in protein chemistry has been the unraveling of the various forces involved in folding polypeptide chains into their unique conformations insight into this question has been gained both from a consideration of non covalent forces as they apply in model systems and from a detailed examination of the actual structures of protein molecules it is generally accepted that to a rough approximation two opposing but not independent tendencies are reflected in the final structure of a protein when it folds the resulting compromise allows hydrophilic side chains access to the aqueous solvent while at the same time minimizing contact between hydrophobic side chains and o academic press inc london i td j kyte and k f water recently however it has been noticed that there are important subtle deviations from these expectations lifson sander janin chothia suggesting that the extent to which residues are buried depends not only upon strict hydrophobicity but also upon steric effects that determine packing between the secondary structures in the crowded interior of the macromolecule n evertheless if one could evaluate the contrary forces of hgdrophobicity and hydrophilicity inherent within the residues themselves then it would be possible perhaps at least to distinguish the exterior portions of a protein from the interior ones on the basis of the amino acid sequence alone moreover in the case of a protein that interacts directly with the alkane portion of a phospholipid bilayer in a membrane there is general agreement that the amino acid side chains involved are chiefly hydrophobic once again an appropriate evaluation of a given amino acid sequence should be able to predict whether or not a given peptide segment is sufficiently hydrophobic to interact with or reside within the interior of the membrane considerable effort has already been expended in devising schemes for predicting three dimensional aspects from amino acid sequences alone the most notable of these have dealt with the prediction of local secondary structure chou fasman wu kabat garnier et al these are empirical methods in that they utilize a library of known protein structures from which t he distribution of the amino acids among various conformational settings is rigorously tallied if the frequency with which the individual amino acids or short peptides occur in ihelices p sheets or reverse turns is known any seyuence can be syst ematicallj scanned and the probability of those secondary structures can be evaluated interestingly even earlier attempts had been made to predict the general shape of a protein on the basis of the types of amino acids it contained thus in the light of the general observation that the interiors of water soluble proteins are predominantly composed of hydrophobic amino acids while the hydrophilic sidechains are on the exterior where they can interact with water fisher and bigelow tried to correlate the sizes and shapes of proteins with their overall amino acid compositions recently a method for displaying the distribution of hydrophobicity over the sequence of a protein was presented by rose and rose roy this procedure combines the progressive evaluation approach of the secondar st ructure predictions with the earlier empirical observation that the hydrophobic side chains tend to be buried within the native structure chothia rose roy also have demonstrated convincingly that this approach can distinguish regions of interior sequence from regions of exterior sequence in this paper we describe a simple computer program similar to that employed by rose and rose roy that systematically evaluates the hydrophilic and hydrophobic tendencies of a polypeptide chain the present program uses a hydropathy scale in which each amino acid has been assigned a value reflecting its relative hydrophilicity and hydrophobicity the program continuously determines the average hydropathy of a moving segment as it advances through the sequence from the amino to the carboxy terminus as such the procedure gives a graphic visualization of the hpdropathic character of the evaluation of protein hydropathy chain from one end to the other tracking the hydrophilic and hydrophobic regions relative to a universal midline we have examined in detail the profiles of several proteins whose three dimensional structures are known and have found excellent agreement between the observed interiors and the calculated hydrophobic regions on the one hand and the observed exterior portions and the calculated hydrophilic regions on the other we have also examined a number of membrane proteins and have been able to identify membrane spanning segments as well as those hydrophobic regions that anchor certain proteins in membranes experimental procedures a the computer program the computer program soap assigns the appropriate hydropathy value to each residue in a given amino acid sequence and then successively sums those values starting at the ammo terminal within overlapping segments displaced from each other by one residue although a segment of any size can be chosen ordinarily spans of or were employed odd numbers being used so that a given sum could be plotted above the middle residue of the segment thus in the case of soap the first value corresponds to the sum of the hydropathies of residues to and is plotted at location the second value corresponds to the sum for residues to and is plotted at location and so on the program was written originally in the language c kernighan ritchie for use in the software system unix which is leased from the western electric co because unix is now widely used and because c compilers are now available for many computers the original program is supplied as a short appendix to this paper so that interested readers may employ it directly plots may be obtained from any terminal that prints a standard character output the program has also been modified for use with a more sophisticated system linked to a zeta plotter we are grateful to s dempsey department of chemistry university of california san diego for these modifications in this latter format the values are presented as averages rather than sums and all the figures accompanying this paper were obtained with this system b sequence library the characterization of a large number of different proteins was facilitated by the fact that we had two extensive libraries of amino acid sequences already stored in the computer one of these was a set of sequences that can be purchased from the national biomedical research foundation and that includes all the sequences from the atlas of protein sequence and structure dayhoff the other newat was a collection of approx sequences gleaned from the original literature and covering the period to doolittle c choice of hydropathy values ideally the most satisfying way to determine the hydrophobic or hydrophilic inclinations of a given amino acid side chain i e its hydropathyt would be to measure its partition coefficient between water and a non interacting isotropic phase and to calculate from that partition coefficient a transfer free energy for example ethanol is a solvent that has been proposed as a phase resembling the interior of a protein nozaki tanford in this case however the choice of solvent may have been a matter of convenience rather than design since the solubilities of many amino acids in ethanol and water were already known cohn edsall and the theoretical basis for deriving the transfer free energies of the t since hydrophilicity and hydrophobicity are no more than the two extremes of a spectrum a term that defines that spectrum would be as useful as either just as the term light is as useful as violet light or red light hydropathy strong feeling about water has been chosen for this purpose j kyte and r f doolittlk individual amino acid side chains between ethanol and water from these values had already been formulated cohn edsell the transfer free energies from water to ethanol for various amino acid side chains are presented in table the assumption that ethanol is a neutral non interacting solvent may not be warranted however nor is it clear that any solvent could ever meet that condition this difficulty has been noted in the past and it has been suggested that water vapor transfer free energies would be a less complicated index of hydropathy hine mookerjee the watervapor partition coefficients for model compounds identical to each of amino acid sidechains were assembled by wolfenden et al from the tables published by hine mookerjee they reported as well experimental determinations for four additional previously unavailable values wolfenden et al all of these values expressed as transfer free energies between an aqueous solution and the condensed vaport are also presented in table it is possible with these free energies to examine the claim that ethanol is a useful solvent with which to model the interior of a protein nozaki tanford the last column in table presents the transfer free energies for the model compounds between ethanol and the condensed vapor if ethanol were an isotropic non interacting phase these values would be fairly constant representing only the dispersion forces lost during vaporization it is clear however that this is not t he case and that ethanol retains many of the unpredictable peculiarities of water itself therefore contrary to the choice made by rose no use will be made here of the hydrophobicity scale based on solubilities in ethanol another source of information bearing upon the tendency of a given side chain to prefer the interior of a protein to the exterior is the tabulation of residue accessibilities calculated by chothia from the atomic co ordinates of globular proteins indeed the ensemble average of the actual locations inside or outside of a side chain should be a direct evaluation of its hydropathy when it is in a protein chothia presented two sets of values the fraction of the total number of a given residue that is more than buried in the native structures on the one hand and the fraction that is buried on the other it has already been noted wolfenden et al that there is a strong correlation between the fraction buried and the water vapor transfer free energies interestingly there is an even stronger correlation between the fraction o buried and the same water vapor transfer free energies table in table the transfer free energies the fractions o buried and the fraction i buried have all been normalized arbitrarily in order that the values may be compared directly we have used both the water vapor transfer free energies and the interior exterior distribution of amino acid side chains determined by chothia in assigning the final hydropathy values table results presented later in this paper indicate clearly that the number in the second place of the hydropathy values is of little consequence to the t in the present tabulation table a small correction kcal mol was applied to the former values wolfenden et al to eliminate any entropy of mixing from the values the transfer must occur between standard states chosen in such a way that no changes in volume are involved if the aqueous standard state is chosen as the infinitely dilute solution at mole fraction the volume of the solute in the aqueous phase by definition will be its apparent molal volume the gas at mole fraction can be compressed mathematically to this same molal volume this is accomplished readily by employing the relationship ac rtln i for the adiabatic change in the volume of an ideal gas from v to i specifically the solute vapor at a standard state of mole fraction at standard temperature c and pressure i i mol is contracted to a volume equal to its particular apparent molal volume in mol i therefore the formula used for this correction was agiran esfte r n i z rtln where n equilibrium mole fraction in aqueous phase n equilibrium mole fraction in the vapor at standard temperature and pressure and y is the partition coefficient in the units m m as tabulated b him mookerjee the advantage of this choice of standard states is that free energies of salvation are directly presented and only the molecular interactions between water and the solutes are reflected in the values evaluation of protein hydropathy table free energies of transfer for the side chains of the amino acids between various phases agrans skcra l mol i side chain water into water into ethanol into mol i condensed vapor ethanol condensed vapor leucine isoleucine valine alanine phenylalanine methionine cysteine threonine serine tryptophan tyrosine lysine glutamine asparagine glutamic acid histidine aspartic acid the apparent molal volumes at c of model compounds for the side chains wolfenden et al of the structure rh where r is the side chain of a given amino acid hanch r coo are either the observed values a tabulated by cohn et al or values calculated b by the methods of cohn et al which themselves were adapted from traube the water vapor partition coefficients for the various model compounds are available in the tables published by c hine mookerjee or d wolfenden et al the standard states chosen for the free energies are mole fraction for the solution and the condensed vapor at a volume equal to its apparent molal volume the water ethanol transfer free energies were copied directly from the tabulations of c cohn edsall or f nozaki tanford the standard states in each solvent are mole fraction the transfer free energies for the ionized side chains g were corrected to ph wolfenden et al using the following pk values tanford lysine histidine glutamic acid and aapartic acid hydropathy profiles and as a result we did not hesitate to adjust the values subjectively when only this level of accuracy was in question nevertheless we tried to derive the best numbers we could from the data listed in the last columns of table the hydropathy values for valine phenylalanine threonine serine and histidine were simple averages of the other numbers in the table when of the numbers for a given amino acid was significantly different from the other the mean of the other was used this was done for cysteine cystine methionine and isoleucine after a good deal of futile discussion concerning the differences among glutamic acid aspartic acid asparagine and glutamine we came to the conclusion that they all had indistinguishable hydropathies and set their hydropathy value by averaging all of the normalized water vapor transfer free energies and the normalized fractions of side chains buried because the structural information was so uncertain tryptophan was simply assigned its normalized transfer free energy glycine was arbitrarily assigned the hydropathy value which was the weighted mean of the hydropathy values for all of the sequences in our data base because it was clear from a careful analysis of the actual distribution of glycine that it is not hydropathic that is to say it does not have strong feelings about water on the basis of both the transfer free energy scale and the fraction buried alanine ought to be more hydrophobic on our scale its value exceeding that 110 j kyte and r f doolittle all values in the last columns result from arbitrary normalization to spread them between and the normalization functions were a dg table b fraction looo buried chothia fraction buried chothia of leucine we find it difficult to accept that a single methyl group can elicit more hydrophobic force than a cluster of methyl groups and for that reason we have arbitrarily lowered the hydropathy value of the alanine side chain to a point half way between the hydropathy value of glycine and the value determined for alanine when the transfer energy and its distribution were used no suitable model exists for proline and in terms of its tendency to become buried it is fairly hydrophilic its hydropathy value was made somewhat more hydrophobic than this consideration because of its methylene groups the hydropathy value for arginine was arbitrarily assigned to the lowest point of the scale because it was difficult to accept the fact that tyrosine is a hydrophilic amino acid even though the available data in table indicate that it is its hydropathy vafue was subjectively raised to one closer to the water vapor jransfer free energy than the structural data would have yielded similarly the hydropathy value for leueine was also raised above the average of the structural data and the transfer free energy and the hydropathy value for lysine was lowered none of these last adjustments the result of personal bias and heated discussion between the authors affects the hydropathy profiles in any significant way results a choice of parameters the effectiveness of the program and the progressive evaluation approach in general depend upon two decisions first we had to determine how large a span of consecutive residues yields a hydropathy profile that most consistently reflects the evaluation of protein hydropathy exterior and interior portions of proteins second we had to determine how critical the hydropathy assignments for the individual amino acids are to the outcome of the calculations for example is the profile of a given sequence radically changed if the hydropathy values for one or more residues are changed by an arbitrary factor z we met these problems directly by examining the same protein sequences under a variety of conditions chym i chym chym o i i i i a i i i sequence number fig soap profiles of bovine chymotrypsinogen chym at different span settings and the solid bars above the midpoint line on the soap profile denote interior regions as determined by crystallography freer et al similarly the solid bars below the midpoint line indicate regions that are on the outside of the molecule j kyte and r f doolittle with respect to the most effective choice of span we compared the hydropathy profiles of a number of different proteins over a range of spans from to residues selected profiles from two of these surveys for chymotrypsin and lactate dehydrogenase respectively are shown in figures and naturally the hydropathy profiles using the shortest spans are noisier than intermediate spans and runs employing spans less than seven residues were generally unsatisfactory long spans on the other hand tended to miss small consistent features frequent and subjective analysis of the degree of correlation of the profiles with the exteriors and interiors of globular proteins see below as well as the resolution of the profile itself revealed that information content was maximized when the spans were set at to residues the impact of the choice of hydropathy values was examined in two different i l i l a j sequence number fig soap profiles of dogfish lactate dehydrogenaae ldh at different span settings and the solid bars above the midpoint line on the soap profile denote interior regions as determined by the crystallographic study of the protein eventhoff et al similarly the solid bars below the midpoint line indicate regions that are known to be on the outside of the molecule evaluation of protein hydropathy ways as an initial test the side chains were assigned to three groups according to their rank on the hydropathy scale table thus arginine lysine asparagine aspartic acid glutamine glutamic acid and histidine were assigned to cluster i proline tyrosine serine tryptophan threonine and glycine to cluster ii and alanine methionine cysteine cystine phenylalanine leucine valine and isoleucine to cluster iii the individual values contributing to each cluster were averaged cluster i cluster ii and cluster iii and the mean values incorporated into a modified soap program called lard comparisons of lard against soap in the cases of chymotrypsinogen and lactic dehydrogenase are shown in figures and although the patterns exhibit some general similarit ies as might be expected since the moving average itself tends to have a leveling aspect an experimental approach loses nothing by using the best values available rather than settling for less precise estimates as a second test the values of four of the most controversial assignments were shifted radically in order to assess the impact on the hydropathy profile thus the values for tyrosine histidine proline and tryptophan all of which have arguably nozaki tanford low hydropathy scores table were arbitrarily increased by o units when the same two proteins were examined with this modified scale there was a noticeable if modest change in the patterns figs and that the change was modest is partly due to the fact that histidine and tryptophan are among the least common amino acids b exterior and interior segments of globular proteins detailed comparisons between the hydropathy profiles of two globular proteins and their published three dimensional structures are presented in figures and in the case of bovine chymotrysinogen a judgement was made about each sidechain on the basis of its position in the standard model that had been constructed in the laboratory of professor j kraut department of chemistry university of california san diego as a part of the crystallographic study of that protein freer et al a variety of hydropathy profiles of the chymotrypsinogen sequence were obtained and compared with the actual locations of the residues in the model structure fig the best agreement between strongly hydrophobic segments and interior regions and strongly hydrophilic segments and the exterior was obtained with a setting of nine residues examination of the results reveals that for the most part agreement between the actual structure and the location expected from the hydropathy of a certain region is quite satisfactory in particular two of the regions that lie on the exterior of this protein whose electron density is poorly defined and that show the greatest rearrangements during zymogen activation freer et al residues to and to exhibit very high hydrophilicity consistent with their loose external attachment to the structure the five major regions of the profile that lie below the midpoint line are all external sequences in the native protein and nine of the major regions that lie above the midpoint line are internal a consideration of those few places where the correlation fails is also illuminating in the case of residues to for example this segment in the model i i a 220 240 sequence number pro soap profiles of bovine chymotrypsinogen chym using different hydropathy values for the amino acids in the top panel the program lard used a set of clustered values in which case the amino acids were divided into sets hvdrophobic net xl and hydrophilic in thr lower panel s the program used radically different weighting factors for some of the more controvrrsial amino acid assignments including those of histidine tryptophan tyrosine and proline in the middle panel the program used the standard set of assignments presented in table all plots utilize a span setting of runs along the exterior of the protein even though the hydropathy profile shows it to have a very hydrophobic character this nine residue sequence lys leu lys ile ala lys val phe lys contains four positive charges intermingled with live very hydrophobic residues the contradiction arises from the fact that the high concentration of positive charge does not weigh hea vily enough in the moving evaluation of protein hydropathy sequence number frc soap profiles of dogfish lactate dehydrogenase in which different hydropathies were used for the amino acids all plots utilize a span setting of see legend to fig for meanings of and average to offset the alkane side chains present in this rather unusual sequence a close examination of the model reveals that the five hydrophobic side chains are all directed toward the interior while the lysine side chains point out into the aqueous environment in the case of dogfish lactate dehydrogenase fig the designations of external and internal residues had already been made and published eventhoff et al and the hydropathy profile correlates well with these crystallographic findings the five major regions of the profile that lie below the midpoint line are all external sequences in the native protein and six of the eight major regions above the midline are internal sequences again as with chymotrypsinogen the profile was least successful in evaluating those regions in which the main chain is only partly buried such as the regions between residues and and and where the backbone repeatedly passes in and out of the aqueous phase j kyte and r f doolittle c membrane bound proteins the issue of the hydropathy of a particular sequence of amino acids assumes added significance when membrane bound proteins are considered the nm of alkane that forms the bilayer is an invariant structural aspect with which such proteins must contend it is generally accepted that the adaptation t o this extremely hydrophobic environment is accomplished by the evolution of long hydrophobic sequences in those proteins whose destiny it is to become a component of a biological membrane a hydropathy profile of the protein glycophorin tomita et al unmistakably identifies the archetypal membrane spanning sequence long ago noted by others tomita marchesi that stretches from residues to fig in this example the polypeptide chain only crosses the bilayer once polar segments extending into the aqueous environment on either side a similar case is known to exist for vesicular stomatitis virus glycoprotein rose et al a partial profile of which is also shown in figure cytochrome on the other hand is a protein for which the exact disposition of the hydrophobic sequence that anchors the protein in the membrane is less clear fig although as noted previously strittmatter et al the carboxy terminus is the general location of the embedded hydrophobic segment the precise extent of the buried portion has not been determined unambiguously the hydropathy profile indicates that the membrane associated portion begins at residue fleming et al on the other hand have reported experiments that they believe suggest that the cluster of tryptophans at residues and 112 is deep within the membrane a conclusion clearly at variance with the hydropathy profile fig the fact that the tryptophans are surrounded by aspartic acid asparagine and serine residues however certainly strengthens the conclusion that this region is not within the alkane of the bilayer in the native structure in the case of bacteriorhodopsin khorana et ccl a protein that is located in the membranes of certain halophilic bacteria live of the seven transmembrane shafts observed in the low resolution electron density function henderson unwin are clearly identified by the hydropathy profile fig the two segments nearest to the carboxy terminus between residues and are not resolved from each other although the profile clearly indicates that both are buried in the membrane in this latter case a point halfway along was arbitrarily chosen as the point at which the chain doubles back the seven transmembrane sequences are aligned next to each other in table d membrane spanning sequences it was of considerable interest to explore whether or not the hydropathy profile could identify within the linear sequence of a membrane bound protein of unknown structure those portions that span the bilayer and distinguish them from sequences that merely pass through the center of the protein itself certainly casual observation of sequences known to be inserted into the alkane phase of the membrane figs and suggests that this should be possible to this end the hydropathy profiles of approximately soluble proteins chosen at random were evaluation of protein hydropathy g et i c i i sequence number sequence number i i i i i i vsvg 500 fm soap profiles for different proteins that have membrane affiliation in the upper panel the plot is that of erythrocyte glycophorin glyc which has an easily recognized membrane spanning segment in the region of residues to in the middle panel rabbit cytochrome b is depicted in this case there is a membrane anchoring unit involving the zo residue carboxy terminal segment in the lower panel the carboxy terminal region of vesicular stomatitis virus glycoprotein vsvg is shown a membrane spanning segment is clearly evident from residues to all profiles are at evaluation of protein hydropathy examined and the most hydrophobic region from each was picked from this preliminary collection a group of twelve residue sequences which were judged to be the most hydrophobic of the lot was chosen for closer inspection it was assumed that since these were in each case the most hydrophobic region in the entire sequence of a given protein they would serve as the most extreme models for a peptide that traverses the interior of a protein from these proteins the most hydrophobic segment of each span length from to residues was identified and its average hydropathy tabulated the collected values for each span length were compared directly with those of the most hydrophobic segments of the same span length taken from bacteriophage coat protein nakashima konigsberg glycophorin and the seven transmembrane sequences of bacterial rhodopsin table these nine hydrophobic sequences each of which is known to span the membrane were chosen as models for a sequence which in the native protein is within the bilayer the discrimination between the segments from the soluble proteins as a group and those from the membrane spanning sequences was most unequivocal when the span was lengthened to residues fig this may be due to the fact that protein spanning sequences passing through the interior are usually shorter than membrane spanning sequences nevertheless from an examination of table it can be concluded that when the hydropathy of a given residue segment averages greater than there is a high probability that it will be one of the sequences in a i i i i i i i i ii span pig comparison between the hydropathy of sequences that span membranes and the hydropathy of those that span proteins the most hydrophobic sequences from globular proteins table except for lactic dehydrogenase were compared with membrane spanning sequences table for each span length the average hydropathies of the most hydrophobic segments were collected and the means and standard deviations of the values were calculated these are presented as a function of the span length for the membrane spanning group o o and the protein spanning group o o 120 j kyte and r f doolittle table nineteen residue hydropathy averages for the most hydrophobic sequences from various proteins protein length sequence position mean hydropathy soluble dogfish lactate dehydrogenase klebsiella aerogenes ribitol dehydrogenase human transferrin rabbit phosphorylase bovine chymotrypsinogen a lobster glyceraldehyde p dehydrogensse bovine prothrombin bacillus stearothermophilus phosphofructokinase human carbonic anhydrase b escherichia wli dihydrofolate reductase bovine carboxypeptidase a bovine proalbumin membrane spanning coat protein human glycophorin halobacterium halobium bacteriorhodopsin membrane bound protein that spans the membrane furthermore membranespanning sequences are more hydrophobic than sequences that pass through the center of a protein and they can be distinguished from the latter by their hydropathy the sequences of subunits i to v and vii of cytochrome oxidase were then examined as examples of membrane spanning polypeptides fuller et al about which much less is known all of the sequences that averaged greater than over at least a residue span were identified as well as some even longer more hydrophobic segments all of these candidates for membrane spanning sequences are presented in table the sequences of six of the seven or more subunits of cytochrome oxidase have been published including all of the largest they account for residues more than of the total protein from this consideration and the information gathered in table it can be concluded that about of the mass of cytochrome oxidase is located within the bilayer e grand averages of hydropathy in the past many claims have been made to the effect that information about the size and shape of a protein bigelow fisher or its affiliation with a membrane capaldi vanderkooi could be obtained from its amino acid evaluation of protein hydropathy table candidates for membrane spanning sequences in cytochrome oxidase composition alone to explore this possibility in the present context we compared the overall hydropathy of a large number of sequences by simply programming the computer to sum the hydropathy values of all the amino acids and dividing by the number of residues in the sequence to obtain a gravy score the gravy scores were plotted as a function of total sequence length inasmuch as it has long been thought that larger globular proteins need more hydrophobic amino acids in order to fill up their interiors bigelow fisher the distribution obtained when the overall hydropathies of fully sequenced soluble enzymes are plotted as a function of their total length indicates that the average hydropathy of soluble protein is independent of the sequence length and earlier conclusions about possible correlations between the size and shape of a protein and its amino acid composition bigelow fisher may have been overstated included in figure are the gravy scores for several membrane embedded proteins whose sequences have been established these values lie well above those for the soluble proteins gravy scores were also calculated for other membranespanning proteins on the basis of their amino acid compositions as determined from amino acid analysis of timed hydrolyses table although the gravy scores for these membrane bound proteins are also quite high in every case exceeding the mean of the compositions of sequenced soluble proteins when their spread is j kyte and r f doolittle gravy plot i odn i a a o e oaao no a d x xx x l i i i i i i i i i i i i i 500 900 ic lo length residues fra plot of mean hydropathies gravy scores of various proteins against their lengths x fully sequenced soluble enzymes whose amino acid sequences have been taken from the recent literature membrane embedded proteins whose sequences have been determined bacteriorhodopsin yeast mitochondrial cytochromr oxidase subunits j to iii cytochromr h and the oii oli atpase subunit and carbodiimide sensitive mitochondrial pro a putative proteins inferred from the unidentified reading frames found in the dna of human mitochondria anderson et al table average hydropathy gravy for the entire amino acid composition of a collection of membrane spnning proteins protein gravy references yeast cytochrome b h bacterium h urn bacteriorhodopsin cytochrome oxidase yeast bovine a human glucose carrierb bovine rhodopsinb human anion carrierb canine na ii atpase a subunitb rabbit atpaseb torpedo cnlifornica acetylcholine receptorb 05 nobraga tzagoloff khorana et al sogin hinkle heller pober stryer ho guidotti drickemer steck et al kyte allen et al vandlen et al a from sequence b from composition from all subunits listed in table evaluation of protein hydropathy compared to the spread of the values for an array of soluble proteins fig the claim that membrane bound proteins can be distinguished from soluble proteins by their amino acid compositions alone capaldi vanderkooi appears tenuous there remains the possibility however that the unexpected hydrophilicity of the membrane spanning proteins whose compositions are known only from amino acid analysis may actually be due to a failure to hydrolyze membrane spanning sequences completely even after hours at c discussion the equilibrium that determines the unique molecular structure of a protein is the one that exists between it and a random coil anfinsen it is generally assumed that this process can be described as a simple two state equilibrium between the native structure and the random coil and experimental results consistent with this assumption have been presented tanford if this is indeed the case the individual contributions to the overall free energy change for this isomerization would be the most critical factors in determining the outcome rather than any kinetic features of the reaction these thermodynamic forces by the very nature of the process must be non covalent interactions several provocative discussions of these matters have been presented cohn edsall kauzmann jencks chothia moreover it has been demonstrated definitively by experimental observation that neither hydrogen bonds klotz franzen nor ionic interactions cohn edsall nor dispersion forces deno berkheimer can provide any net favorable free energy for the formation of the native structure in aqueous solution therefore by exclusion and perhaps for the lack of a better candidate hydrophobic forces kauzmann have attracted the most attention in discussions of this process felicitously this has drawn attention to the significant role of the aqueous solvent per se the hydrophobic force is simply that force arising from the strong cohesion of the solvent which drives molecules lacking any favorable interactions with the water molecules themselves from the aqueous phase jencks in the case of the formation of the native structure from the random coil this force participates in the reaction because hydrophobic side chains which are exposed to water in the extended coil are removed to the interior of the protein during the folding of the native structure chothia this transfer appears to provide the only favorable free energy available to drive the reaction to completion therefore the more aversion water has for a given amino acid side chain the more free energy is gained when that residue or a portion of it ends up inside the native structure conversely and of equal importance it is also the case that the more attraction water has for a functional group on an amino acid side chain the more free energy is lost when that functional group is removed from water during the folding process this point becomes clear upon examination of the data in table when it is realized that most of the free energies of transfer from water to the condensed vapor are actually unfavorable many by a considerable amount this is due of course to the fact that water participates in strong interactions with hydrogenbond donors and acceptors klotz farnham as well as to the need to j kyte and r f doolittle neutralize charged side chains as a result one of the major free energy deficits in the folding of a protein results from the requirement to unsolvate those hydrophilic functional groups destined for the interior some of this investment is returned when hydrogen bonds are formed in the interior nevertheless because of geometric constraints the hydrophilic side chains in the center of a protein participate in far fewer hydrogen bonds than they would in the unfolded and exposed random coil where both the donors and acceptors interact fully with water as such there is a high probability that significant free energy will be lost whenever a hydrophilic residue is removed from water during the folding process it is undeniably the case therefore that both the hydrophobicity and the hydrophilicity of a given sequence of amino acids affect the outcome of the equilibrium between the random coil and the native structure although one or the other of these two properties is often emphasized to make a particular point neither is more important than the other for example it is often stated that the interior of a protein is formed from its hydrophobic sequences but it is seldom pointed out that the interior of the protein is also formed because the hydrophilic sequences cannot be buried thus it can be concluded that any description of the folding process that fails to consider either hydrophobicity or hydrophilicity is discarding half of the information contained within the sequence of the protein it has also been pointed out lifson sander janin chothia chothia janin that the packing properties of residues such as leucine isoleucine and valine might have an effect independent of hydropathy on the folding process as the interior of the protein is fitted together jnfortunately very little is known about the features of this steric interplay and our understanding of the folding process does not extend beyond the conclusion that hydropathy is of central importance to it the conclusion that can be drawn from all of these considerations is that to a first approximation the native structure of a protein molecule will be that structure that permits the removal of the greatest amount of hydrophobic surface area and the smallest number of hydrophilic positions from exposure to water bigelow fisher chothia the obvious prediction that follows from this conclusion is that the most hydrophobic sequences in a protein will be found in the interior of the native structure and the most hydrophilic sequences will be found on the exterior in order to exploit this prediction with the greatest success the most accurate evaluations of the hydrophobicity and hydrophilicit y of each amino acid side chain should be formulated to this end a number of hydropathy scales have been proposed in other publications but in our view they all suffer from serious drawbacks those based on water ethanol transfer free energies xozaki b tanford segrest cy feldman rose are imperfect due to the peculiarities of ethanol as a solvent which seem almost as unusual as those of water itself table a scale based on the partition coefficient between the bulk aqueous phase and the air water interface bull breese also seems a poor choice because the hydrogen bonds that must be broken and the charges that must be neutralized to remove a residue from the aqueous phase during the formation of the native structure probably remain intact at the air water interface and are thus not a evaluation of protein hydropathy factor in the overall reaction in a more complicated attempt zimmerman et ai completely neglected the very large solvation energies associated with the hydrophilic side chains jencks in formulating their polarity ranking which is based on electrostatic forces in a vacuum furthermore the crystal lattice energies inherent in the amino acid solubilities that were used for hydrophobicity parameters are also disregarded by these authors finally a scale proposed by von heijne blomberg although sophisticated in its intent relies entirely on theoretical calculations with scant reference to any empirical observation the water vapor partition free energies table l which were first applied to the problem of protein folding by wolfenden et al also have shortcomings the use of the vapor as the reference state leads to the incorporation of the dispersion forces into the transfer free energies since there is in all likelihood only a negligible and unpredictable contribution of dispersion forces to the process of protein folding deno berkheimer these in principle should be subtracted from each value unfortunately it is not even clear at the moment what the order of magnitude of these free energies is let alone their individual values jencks if they are roughly the same for each side chain then their only effect would be to shift the whole scale uniformly without affecting the relative position of each if these forces are roughly proportional to the volume of the side chains they should also be fairly constant table l but difficulties could arise with very large and very small side chains such as tryptophan or glycine and alanine respectively furthermore it is not clear whether the vacuum is an adequate model for the interior of a protein with its collection of heterogeneous polarizabilities and oriented dipoles nevertheless as pointed out by wolfenden et al the values for these transfer free energies correlate remarkably well with the actual distribution of the side chains between the interior and exterior of protein molecules chothia if it is assumed based on the observed correlation of the transfer free energies and the actual distribution of the side chains wolfenden et al that both of these parameters are to the first approximation measurements of the hydropathy of a given amino side chain then the best available hydropathy index should be based on a consideration of both of these quantities this follows from the fact that each of them suffers from its own unique uncertainties the transfer free energies incorporate dispersion forces of unknown magnitude the distributions based on examination of several protein structures are calculated from a limited data base and are biased by steric features that are not yet understood since none of the drawbacks is shared by these two independent measures of hydropathy the most satisfactory index should be formulated from a consideration of all of the available information as has been done here in addition the hydropathy scale presented here unlike many earlier ones spans the entire hydropathy spectrum from the hydrophobic end to the hydrophilic for the reasons discussed above this is an essential aspect of any scale it is a point that was also emphasized by wolfenden et al the hydropathy values presented in table being singular numbers do not have associated with them an indication of their uncertainty such as for example a standard deviation in retrospect some of these parameters are more reliable than j kyte and r f doolittle others the most unequivocal values are those associated with leucine isoleucine valine phenylalanine methionine threonine serine lysine glutamine and asparagine these ten residues together comprise slightly more than half of the present census of the amino acids found in proteins most of these side chains have partial specific volumes between and cm3 mol which suggests that dispersion forces may not influence their rank their relative positions change little from the fraction buried to the fraction buried to the free energy of transfer table as such these residues anchor the scale and are probably those most responsible for its success there is a group of amino acids that are less reliable cysteine is complicated by the problem of disulfide bonds proline by the lack of an adequate model compound in the transfer free energies as well as its tendency to participate in turns on the exterior and aspartic acid glutamic acid and tyrosine by the large differences between their tendency to be buried and their free energies of transfer certain amino acids tryptophan tyrosine glutamic acid and histidine are very reluctant to bury the last of their surface area while some alanine glycine and cysteine are far more likely than the others to become fully buried finally arginine was arbitrarily assigned a parameter of even though no arginine was found to be even as much as buried chothia and no model compound for arginine was employed in the water vapor transfer studies of wolfenden et al it is possible that the parameter for this side chain should be even more negative wolfenden et al glycine and alanine are especially difficult to categorize both lack satisfactory model compounds for phase transfer studies because methane is such a small molecule its relative hydrophobicity is probably seriously overestimated by water vapor transfer energy because of the ambiguity introduced by dispersion forces indeed the use of hydrogen gas as a model compound for glycine wolfenden et al is such an extreme case of the problem that arises from the contributions of the dispersion forces when molecules of such radically different electron densities are compared that its water vapor transfer free energy is probably a meaningless number in this context and it has not been included in table on the other hand both alanine and glycine are quite insensitive to becoming fully buried table which suggests that the side chains contribute little energy one way or the other to protein folding because they are not hydropathic the conclusion from these arguments is that the more alanine and glycine a segment contains the more equivocal its hydropathy becomes a rather interesting and unforeseen feature of the interaction between the various side chains and water is that the aromatic amino acids tryptophan tyrosine and histidine are far more polar than previously thought xozaki tanford it has been noted that aromatic compounds are more soluble in water by an order of magnitude than their surface areas would indicate hermann the phenylalanine side chain however is much less hydrophilic than the other three and this suggests that it is the heteroatoms in the latter that are the major contributors to their hydrophilicity in this context tryptophan is one of the most difficult residues to which to assign a hydropathy index it has a fairly positive water vapor transfer free energy evaluation of protein hydropathy table but much of this may result from large favorable dispersion forces due to the residue large volume the opposite problem to the one experienced with glycine and alanine examination of the actual location of tryptophan in a number of proteins chothia however clearly indicates that this sidechain is infrequently totally buried table in the specific case of the interaction of gramicidin with a phospholipid bilayer it should be mentioned that its tryptophan residues are clustered at the two ends of the pore rather than being distributed evenly throughout again suggesting an unexpected hydrophilicity for these residues and a reluctance to bury the last of surface table although the hydropathy of tryptophan is relatively unimportant in considerations of soluble proteins since its frequency is only about l there are indications that it may be very significant in membrane affiliated sequences in particular of the tryptophan residues in ca atpase seem to be within sequences directly associated with the bilayer allen et al another instance is the tryptophan cluster in cytochrome b noted above which the hydropathy profile clearly positions at the aqueous interface fig finally earlier claims that tryptophan was the most hydrophobic of the amino acids were based entirely on transfer free energies between water and ethanol or dioxane nozaki tanford it was not recognized at that time that when the tryptophan side chain which possesses a hydrogen bond donor only is transferred between water a solvent with equal numbers of donors and acceptors and ethanol or dioxane solvents with excesses of hydrogen bond acceptors there is a net increase of one mole of hydrogen bond mole indole formed during the transfer causing the side chain to appear much more hydrophobic than it actually is using the same logic it is clear that in a protein solution which necessarily contains more acceptors than donors the removal of the donor on tryptophan from access to the solvent is a significantly unfavorable reaction it seems when all points are considered objectively that tryptophan is a fairly hydrophilic side chain the particular values chosen for the amino acid hydropathies embody one of the major differences between the method presented here and a similar one proposed earlier by rose he chose to employ water ethanol transfer free energies in his scale the disadvantages of which are noted above he also chose to ignore at least in principle the hydrophilic force the attraction that the aqueous solvent exhibits for many side chains by simply assigning a value of zero to all side chains for which partition free energies were not listed in the tables of nozaki tanford in addition rose curve smoothing procedure although mathematically sound tends to remove a great deal of the simplicity and clarity of the unsmoothed moving average in the program described in this paper the meaning of each value is clearly understood and a more distinct and graphic rendering of the sequence obtained in addition we have extended the use of this approach to the area of membranespanning segments of protein sequences in this regard the most novel feature of the approach is that membrane spanning segments can be identified and distinguished from sequences that merely pass through the interior of a protein table since it is these membrane spanning portions of sequences that have proven to be most difficult to study allen et al a method for their j kyte and r f doolittle identification ought to be quite useful for instance the sequence of bacteriorhodopsin fig was correlated previously with an electron density map on the basis of several criteria engelman et al all of these earlier arguments were based however on an initial assignment of the transmembrane regions within the sequence no explanation of how these decisions had been made was presented and in lieu of this it can be assumed that the assignments made in table are based on more objective considerations if the present assignments are correct the criterion of ion pairing used in the earlier study is no longer meaningful because the partners in the purported ion pairs would be displaced from each other the belief that the difficulty of buried charges can be overcome by forming an ionpair is known to be naive inasmuch as virtually the same amount of free energy is required to bury an ion pair as to bury a single fixed charge yarsegian the problem of burying charge in a protein is solved not by forming ion pairs but by titering the charge and forming a strong internal hydrogen bond with the neutralized acid or base in this light the interactions proposed earlier between lysines and arginines on the one hand and carboxylates on the other are poor choices for two reasons carboxylates are weak bases and would be unable to withdraw the proton effectively from the very weak cationic acids as a result charge would be ineffectively neutralized furthermore the difference in pk values between arginine or lysine and carboxylate is large which would cause the hydrogen bond to be a weak one jencks a much more favorable choice on both counts would be a hydrogen bond between lysine or arginine and tyrosinate anion in fact when the aligned sequences table are examined two potential hydrogen bonds of this type become immediately apparent those between lysine and tyrosine on the one hand and tyrosine and arginine on the other in fact neutral strong hydrogen bonds of this type in which the proton is retained preferentially by the phenolic oxygen have been observed directly in lpsinetyrosine co polymers kristof zundel the biological function of bacteriorhodopsin must also be considered it has been suggested that this enzyme is a light driven proton pump and it can be assumed that the protons are passed across the membrane along a relay system of lone pairs the most reasonable possibility is that this relay system is a string of hydrogenbonded carboxylic acids since these groups can transfer protons efficiently through space by a simple rotation furthermore only a string of carboxylic acids could shuttle protons fast enough to keep up with the turnover of the enzyme a proton on lysine or arginine cannot be transferred to the lone pair of a water molecule in aqueous solution at a rate any greater than about second while the proton on a carboxylic acid can be transferred in the same reaction at second jencks although the former rate may be enhanced in a well organized hydrogenbonded network wang it is unlikely that a proton traversing bacteriorhodopsin could pass through a hydrogen bond containing a basic amino acid side chain again examination of the aligned sequences table suggests that the carboxylic acid side chains at residues 212 and or some subset of these are distributed appropriately across the membrane to form such a relay system finally the present assignment of the membrane spanning sequences further evaluation of protein hydropathy weakens the arguments used earlier engelman et al to correlate the sequence of bacteriorhodopsin with the electron density profile in the first place the total scattering power of the a sequence in table is not less than those of the others and this would make its correlation with a low intensity shaft unnecessary more to the point of the present discussion the model preferred in the earlier study engelman et al places sequence b one of the most hydrophobic table at a location where it is completely surrounded by protein and sequence f clearly the most hydrophilic table at a location well exposed to the alkane of the bilayer these considerations demonstrate that an examination of the hydropathy of a given sequence may provide additional information to the crystallographer in situations where structural decisions are ambiguous an assignment that is different from the previous one and that satisfies the demands of hydropathy more successfully would be to place sequence a into shaft b into shaft c into shaft d into shaft e into shaft f into shaft and g into shaft in the enumeration of engelman et al this assignment positions the most hydrophilic sequence f in the location most shielded from the alkane and the most hydrophobic g in the location most exposed to the alkane juxtaposes sequences f g c and b forming the proton relay system as well as permitting the strong hydrogen bonds mentioned earlier and places the retinal that is attached to lysine bayley et al in the very center of the carboxylate relay system as well as at the location that it occupies within the projected neutron density profile king et al furthermore no crossovers of the sequence are required and the only long connection coincides fortuitously with the longest stretch of polar sequence residues to this elaboration as well as others presented earlier is an example of the information that might be gained from an informed consideration of a hydropathy profile vmd is a molecular graphics program designed for the display and analysis of molecular assemblies in particular biopolymers such as proteins and nucleic acids vmd can simultaneously display any number of structures using a wide variety of rendering styles and coloring methods molecules are displayed as one or more representations in which each representation embodies a particular rendering method and coloring scheme jor a selected subset of atoms the atoms displayed in each representation are chosen using an extensive atom selection syntax which includes boolean operators and regular expressions vmd provides a complete graphical user interface for program control as well as a text interface using the tcl embeddable parser to allow for complex scripts with variable substitution control loops and function calls full session logging is supported which produces a vmd command script for later playback high resolution raster images of displayed molecules may be produced by generating input scripts for use by a number of photorealistic image rendering applications vmd has also been expressly designed with the ability to animate molecular dynamics md simulation trajectories imported either from files or from a direct connection to a running md simulation vmd is the visualization component of mdscope a set of tools for interactive problem solving in structural biology which also includes the parallel md program namd and the mdcomm software used to connect the visualization and simulation programs vmd is written in c using an object oriented design the program including source code and extensive documentation is freely available via anonymous ftp and through the world wide web keywords molecular modeling molecular dynamics visualization interactive visualization introduction the burgeoning number of experimentally resolved biomolecular structures has resulted in an increasing need for color plates for this article are on page and address reprint requests to dr schulten at the theoretical biophysics group university of illinois and beckman institute north matthews urbana illinois e mail kschulte ks uiuc edu received october revised december accepted january computational tools for molecular visualization and analysis several excellent packages exist for the graphical display of static molecular structures such as ribbons xmol midas setor grasp and others molecular dynamics md studies of the function of the structurally known biopolymers are being widely applied the results of such studies however are typically large molecular trajectory files which represent substantial amounts of dynamical data that require suitable visualization tools e g to initiate molecular dynamics simulations and to display sequences of structures the increased speed and parallelization of computers make it possible today to carry out molecular dynamics studies interactively in order to probe key properties of biopolymers such as potential energy barriers along chosen reaction pathways this approach requires a new molecular graphics user interface the program vmd has been developed for interactive graphical display of molecular systems in particular biopolymers such as proteins or nucleic acids the motivation for the development of vmd has been to provide a welldocumented and freely available program that is easy to use and modify and which addresses several challenges in molecular graphics including the following support for the display of dynamic data such as molecular trajectories generated by molecular dynamics calculations direct interaction with a separate molecular dynamics application in order to provide a graphical user interface and visualization console for the simulation program the capability to work with three dimensional immersive display devices such as a large screen stereo projection facility text based as well as mouse based user interface controls including user customizable menus and program extensions using an interpreted scripting language display of molecules in a wide range of rendering styles easily selected by the user production of high quality hardcopy images of currently displayed molecular systems we describe in this article the features and structure of the program vmd first mentioning the current implementation of the program and then discussing the major program capabilities and functionality we also describe the use of vmd coupled with a large screen stereo projection system as a collaborative tool for several researchers to employ for analysis and discussion of molecular assemblies journal of molecular graphics by elsevier science inc avenue of the americas new york ny pii 96 finally we discuss the use of vmd for interactive molecular dynamics modeling and list the current availability of documentation and source code implementation vmd is written in c using an object oriented design that assists maintenance of the program and the addition of new features the distribution of vmd includes documentation describing how to compile install use and modify the program for different hardware and software configurations vmd requires either the silicon graphics gl library or the opengl library for three dimensional graphics rendering a sample vmd session is shown in color plate which illustrates the three components of the user interface the graphics display window the graphical user interface windows and the vmd console on the right in color plate i is the graphics display window in which molecules are rendered and interactively rotated translated and scaled via mouse controls a user customizable pop up menu is available in this window the image shown in the graphics display window is referred to am the current scene vmd contains options for rendering the scene to a high quality raster image file am described below vmd uses xforms v for the graphical user interface which uses a collection of fiwms to represent the graphical controls available to the user examples of some of the forms available in vmd are shown on the left in color plate i the graphical user interface gui in vmd includes a toolbar that provides access to the specific torms for tasks such am changing the current molecular display characteristics or animating selected molecules using molecular dynamics trajectories below the graphics display window in color plate i is the vmd text console which displays informative messages and provides a command prompt fbr keyboard control of the program all actions in vmd are available via text commands full session logging and playback are possible users can wrile scripts that may be run at any time and that can be executed for example by entering a short text command or through a user customized pop up menu selection vmd employs tel an embeddable interpreted script lanouage parser to process text commands tcl provides a full met of scripting capabilities for the program and makes possible complex features such as variable substitution conditional expressions control loops and subroutines a user customizable configuration file is read when vmd starts up which may be used to define a personalized vmd working environment and to define new commands and program extensions as well as to create customized pop up mentl controls capabilities new molecules are read into vmd from a set of molecular structure coordinate files or may be read from a mingle coordinate file the molecular structure file contains static information about the system such as bond connectivity and atomic mass and charge values the molecular coordinate file contains the positions of all the atoms that make up the molecule when a structure file is not provided the bond connectivity for each atom is determined by vmd through a nearest neighbor distance search vmd understands the charmm compatible psf protein struclnre file fl rmat and the brookhaven pdb t coordinate file format in addition to molecular file lkwmats vmd can read and display images in raster input file format owing to the large number of molecular data file formats currently in use vmd implements an imerface to the program babel to read data from formats other than pdb or psf if babel is installed it is used to convert files from alternate lormats into pdb format before being read by vmd this also includes multiple coordinate set data files such as those in xyz fornml a key feature in vmd is the abilil to work with molecular dynamics simulation programs and to display the simulated molecule am its motion is computed as discussed below thus new structures may be loaded into vmd through connection to a running simulation instead of being read from a file once loaded however a structure obtained from a network connection is treated in the same manner as if read from disk animating molecular structures for each molecule displayed by vmd there is an associated animation li t which is a collection of atomic coordinate sets for the molecule controls are awfilable to play back the trajectory with options to control the animation speed fl ame increment and animation direction new trajectory coordinate sets may be read fl om pdb files or from dcd files binary compatible with the molecular dynamics refinement programs charmm and x plor io these coordinate sets may be loaded when the rnolecule is initially read into vmd or may be loaded later a molecuh r trajectoqr editor is also available with options to delete specific fl ames or met of frames from lhe animation list and to write coordinate sets to files displaying molecular structures any number of molecules may be displayed and manipulated in the graphics display window as shown in color plate i each structure is drawn as several relnwsenlations or views of the molecule multiple views of a molecule may be shown simultaneously to create a complex image of the system a view is just one particular method of drawing the molecule and consists of three characteristics an atom subset selection which determines which atoms are to be included in the view a remlering style which determines the primitives used to draw the atoms bonds and other molecule components table i i lists many of the rendering styles available in vmd a colorinv lnethod which determines what color to use for the components of the view numerous coloring methods are available some of which are listed in table views are created or modified xia text commands or more easily by using the graphical interface controls color plate shows a detailed representation of a protein dimer j mol graphics vol february table selected molecule rendering styles available in or off and may be interactively moved to new positions vmd with the mouse style description lines bonds vdw cpk dotted licorice tube ribbon surf caaoon simple lines for bonds points for atoms lighted cylinders for bonds no atoms solid lighted van der waals spheres for atoms no bonds scaled van der waals spheres for atoms cylinders for bonds dotted van der waals spheres for atoms no bonds solid spheres and cylinders with equal radii cylindrical tube through c atoms flat ribbon through c atoms solvent accessible surface of selected atoms b simplified secondary structure representations for nucleic acids the p atoms are used busing the surf algorithm developed at the university of north carolina complexed with a segment of dna molecules may be displayed using either an orthographic or a perspective projection the atom selection capabilities of vmd are quite extensive and include a flexible syntax for complex selection expressions each atom in a molecule has several characteristics keywords are used to select the atoms that have values matching a specified criterion for example each atom has a set of character string names an x y z position and other numeric values such as charge and mass each atom also has boolean characteristics identifying it for example as a member of an amino acid or nucleic acid or as part of the protein backbone boolean operations may be used to select atoms based on multiple characteristics and parentheses may be used to select the order of evaluation for example the selection resname asp or resname gly and mass ii selects all atoms that are in aspartic acid or glycine residues and that have a mass greater than similarly the selection backbone and within of resid selects all backbone atoms that are within units e g within of residue regular expressions may be used in strings to specify quickly many different names in a selection when a selection changes during a trajectory such as the atoms within some radius of another set of atoms the user can request an update of the selection each time a new animation time step is displayed when displaying solid objects vmd can make use of available hardware accelerated lighting capabilities a capability exploited in other molecular visualization packages such as setor up to four independent infinitely distant light sources may be positioned in the scene and used to illuminate the displayed objects each light can be turned on raster image generation an interface to a number of image rendering packages is provided in vmd and can be used to create input scripts for use by these programs the generated input scripts may then be read by the selected package to create a raster output image of the graphics scene displayed by vmd suitable for publication or slides table lists the currently supported image rendering packages the capability to create input scripts for image rendering programs may be combined with the tcl scripting language available in vmd to easily create high quality movies of a molecule as an example the following script may be used to create a series of images showing a previously loaded molecule rotating about the y axis by creating input scripts for the program that are processed immediately after they are created tcl script to create a movie of a rotating molecule for set i i set i expr i l create and process a script set scriptfile rotate i set imagefile rotate i rgb render sscriptfile catch exec render sscriptfile sgi imagefile rotate the current image by degrees about the y axis rot y by after loading a molecule this script would be invoked simply by typing assuming it was in a file called movie tcl table selected molecule coloring styles available in vmd style description name color determined by atom name resname color determined by residue name segname each segment shown in a different color molecule each molecule shown in a different color chain color determined by one character chain identifier beta color scale based on the beta values of the pdb file occupancy color scale based on the occupancy values of the pdb file mass color scale based on the atomic mass of each atom charge color scale based on the atomic charge of each atom pos color scale based on the distance of each atom from the center j mol graphics vol february table available rendering program output formats name description pov rayshade radiance art fast raster file generator persistence of vision ray tracing package rayshade ray tracing package radiance lighting simulation and rendering system vort ray tracer play movie tcl more complex transformations are performed in a similar manner stereo display the scene displayed in the graphics window may be rendered in stereo to enhance the appearance and information content of the displayed systems the stereo viewing parameters which include the eye separation distance and the focal length can be interactively controlled by the user two stereo display formats are available in vmd a side by side stereo display which splits the display window into two halves a left eye view and a right eye view this format may be used with all graphics display hardware and is suitable for preparing images for publication a crystal eyes stereo display which utilizes special hardware available on many graphics workstations to display stereo images to be viewed with liquid crystalshuttered glasses the stereo viewing modes may be used with standard display monitors or with external stereo display equipment as an example vmd is currently being used with a threedimensional projection system for the display and analysis of molecules viewed simultaneously by several users this system consists of a ceiling mounted projector echoing onto a large by foot screen the stereo images displayed on a graphics workstation monitor the large screen display makes it possible for many viewers to study collaboratively at a large magnification a biopolymer system the stereo display environment enhances the information content of the scene through the addition of visual depth cues vmd contains several options to configure the graphics display for use with large screen projection systems e g to set the projection screen vertical and horizontal dimensions one problem encountered when several people view the same computer generated stereo image is that each person sees the scene from a different perspective to help alleviate this problem vmd supports the use of spatial tracking devices such as the polhemus fastrak polhemus inc colchester vt that measure the position and orientation of sensors relative to a fixed position in order to provide a set of pointers vmd displays visual representations of the pointers on the screen such that all viewers perceive the pointers at the same positions relative to the molecules new features and user interface methods are currently being added to vmd for use with this stereo projection environment to provide a natural problem solving environment for structural biology the goal of these enhancements is to help untie the users from the keyboard to make possible manipulation and analysis of molecular structures by several collaborators at the same time in a natural working environment the user interface controls of vmd are being updated to make use of each available pointer as a mouse that would be used for rotation and control of the molecules just as the normal mouse is used also being explored are an audio user interface to allow users to issue spoken commands user interface controls such as these may eventually replace or augment keyboard and menu controls trajectory analysis tools vmd includes functions for analyzing molecular structures and trajectories these functions access the internal vmd data structures to return or modify characteristics such as charge mass and position for individual atoms residues and molecules more complex analysis capabilities such as computing the rms deviation or correlation functions of a dynamics trajectory can then be implemented as tcl scripts without the need to modify the vmd source code as an example the following commands illustrate the use of the vmd analysis language to query and change the center of mass of a specific residue the first step is to create an atom selection and bind it to an identifier vmd set atomselect top resid information about the atoms in this selection may then be retrieved using the get option vmd smy sel get name mass x y z info n 488 863 h 770 467 ca 981 909 cb 147 880 c 865 444 o 801 924 using these atom selection commands and a set of vector manipulation procedures implemented in tcl the function to compute the center of mass of a selected set of atoms is then realized as follows tcl procedure to compute the center of mass of a set of atoms proc com sel set atoms sel get mass x y z set totalmass set com veczero foreach atom satoms set mass lindex satom set pos irange satom set totalmass expr stotalmass smass set com vecadd com vecscale smass pos return vecscale com expr stotalmass l l j mol graphics vol february the application of this procedure is straightforward executing the new command corn with a previously defined atom selection identifier as an argument returns the center of mass of the atoms in that selection vmd com my sel info 14667 this information could then be used for example to move the center of mass of the atoms to some other position for example to the origin vmd moveby vecinvert corn sel in addition to the calculation of center of mass motion several other tcl procedures are available in vmd for dynamic trajectory analysis including computation of rmsd values autocorrelation functions and ramachandran plot qb qj angle data new analysis functions can be easily developed and added to vmd by the user interactive molecular dynamics vmd is designed to act as a visualization console and graphical front end for a molecular dynamics application running on a remote supercomputer or high performance workstation vmd uses a set of daemons and library routines known as the mdcomm software to broker the communication of data and commands between vmd and a remote simulation program the mdcomm software interface is independent of the particular remote application the daemons buffer data transfer between the application and vmd and act as managers for the running jobs and interprocess communication once a particular simulation program has been suitably modified to work as an mdcomm client vmd can be used with that program without change vmd is the visualization component of a larger set of computational tools for structural biology known as mdscope ra mdscope includes not only vmd and mdcomm but also the parallel molecular dynamics program namd namd is a parallel portable molecular dynamics program written in c which implements the charmm force field and contains numerous simulation options namd uses a spatial decomposition algorithm to distribute the computation tasks among parallel processors which partitions the volume of space occupied by the simulated molecule into uniform cubes known as patches assigned to different processing nodes while namd and vmd may be used independently of each other taken together vmd namd and mdcomm constitute mdscope information on mdscope may be obtained from the mdscope www home page http www ks uiuc edu research mdscope vmd provides an interface to initialize a new simulation and can serve also to monitor and control certain simulation parameters after a simulation is started and a connection between vmd and the application is made molecular structures are communicated to vmd as they are calculated the connection between vmd and a running simulation may be dropped and later reestablished e g to allow a user to check on the progress of a running simulation without having to stop and restart the calculation molecules displayed in this manner may also be manipulated and rendered in the same fashion as molecules loaded from data files and transferred molecular structures may be saved in an animation list for storage and playback example of use color plate illustrates an example of the use of vmd for interactive molecular dynamics the first step in establishing a connection to a running simulation is to select a host computer and to choose between starting a new simulation or reconnecting to a running job color plate the initial connection to the host computer returns the lists of available applications and running simulations from a daemon running on the host computer selecting an available application in order to start a new simulation in this case a simulation using namd results in vmd requesting from the host computer the list of parameters necessary to start the job color plate once all the parameters are entered the simulation program is launched and the static molecular structure data are returned to vmd as the molecular trajectory is computed the coordinate sets are sent to vmd for display color plate the available graphical user interface controls may be used to view or modify simulation parameters or display characteristics or to terminate the simulation color plate also shows several atoms together with the applied forces the latter specified by the user with the mouse the horizontal and vertical arrows these forces have transformed the small polypeptide from an alpha helix conformation to the form shown in color plate once a connection to a running simulation has been established vmd provides options to modify simulation parameters such as the temperature and remote connection parameters such as the frequency with which coordinate sets are communicated from the simulation program to vmd the user can also directly participate in the simulation through the addition of perturbative or guiding forces to selected atoms or residues a feature developed and studied earlier in the program sculpt forces are added using the mouse to indicate the magnitude and direction of the additional interaction on selected atoms these forces are communicated to the simulation program and incorporated into the dynamics the use of interactive guiding forces in simulations of biomolecular systems is currently being applied or considered tbr a number of projects two systems being examined with these interactive tools are the protein bacteriorhodopsin and the binding of biotin to the protein avidin for bacteriorhodopsin br a kda membrane protein used to convert light energy into a proton gradient for atp synthesis in halobacterium halobium molecular dynamics simulations are being performed using interactive forces to guide the positioning of water molecules near key residues such water molecules could not be resolved in the experimentally determined br structure but are considered to play key roles in the proton transfer mechanism of br j mol graphics vol february interactive forces are also being considered for the study of the interaction of the atom biotin vitamin with the protein avidin avidin is a kda protein found in tetrameric form in animal and reptile egg white which has a strong binding affinity for biotin tm lnteractively applied forces could be used to pull biotin from the binding site in avidin in order to shed light on the molecular mechanism of how avidin accommodates the biotin structure documentation extensive documentation on how to use the visualization features of vmd and how to modify and extend the program is available the following documents in postscript format are provided via anonymous tip t an installation guide describing how to compile and install vmd a user guide explaining the capabilities and features of vmd a programmer guide listing the structure and layout of vmd and indicating how to add new capabilities an on line feature is also available availability the complete set of source code and documentation files for vmd as well as a precompiled binary for silicon graphics workstations is available free of charge for noncommercial use via anonymous ftp from the ftp server ftp ks uiuc edu in the directory pub mdscope vmd up to date information on vmd may be obtained by accessing the vmd www home page http llwww ks uiuc edu researchlvmd vmd is available primarily for silicon graphics workstations running version or later of the irix operating system the gram has also been compiled and run on hewlett packard pa workstations running version of the hp ux operating system and on ibm rs workstations running aix map interpretation remains a critical step in solving the structure of a macromolecule errors introduced at this early stage may persist throughout crystallographic refinement and result in an incorrect structure the normally quoted crystallographic residual is often a poor description for the quality of the model strategies and tools are described that help to alleviate this problem these simplify the model building process quantify the goodness of fit of the model on a per residue basis and locate possible errors in peptide and side chain conformations introduction x ray crystallography is the most powerful tool available to provide detailed three dimensional structural information of macromolecules and has led to new insights into how structure determines protein function the last ten years have seen a technical revolution in a number of vital stages in solving a new protein structure these include for example the use of modern molecular biology techniques to overexpress proteins thatare normally present in minute amounts our ability to collect more accurate diffraction data has improved by the development of electronic two dimensional area detectors and powerful synchrotron based x ray sources even with rotatinganode generators area detectors frequently allow the collection of a complete high resolution data set from a single crystal these factors directly affect the quality of electron density maps phased by the method of multiple isomorphous replacement m i r initial models are now routinely improved by crystallographic refinement a number of leastsquares algorithms have been described for this purpose these include the use in the refinement of model restraints hendrickson konnert constraints and restraints sussman holbrook church kim explicit molecular mechanics force fields jack author to whom correspondence should be addressed 020110 00 levitt fast fourier transform methods to speed up the calculations agarwal and more recently force field based molecular dynamics algorithms briinger kuriyan karplus fujinaga gros van gunsteren fortunately the decrease in the price performance of computers has allowed us more or less to keep up with the increased computational demands of some of these algorithms the interpretation of mir maps to produce an initial molecular model is a critical step that remains problematic at this stage in the process errors can be introduced that either cannot be removed by refinement or require many alternating cycles of refinement and manual refitting incorrect models can be refined to crystallographic r factors that up to a few years ago would have been considered eminently respectable especially for large multi subunit structures three dimensional computer graphics workstations are now widely used for constructing models in mir maps one computer program in particular frodo has been widely used jones and is available on a range of workstations in an attempt to improve the ability to interpret maps and then to construct more accurate models jones thirup introduced the use of skeletons coupled with a protein database of the best refined protein structures to build the initial model this work suggested that all protein models could be built from fragments of existing structures in this paper we describe our extensions to these ideas and our initial attempts at reducing the subjectivity involved in building models with the overall procedure shown in fig we are able to go from an initial ca trace to a well refined model without manual intervention this should allow the crystallographer to spend more time considering alternative hypotheses without worrying about most of the detailed fitting of the model to the electron density map we are aware that unfortunately such a procedure can also be misused acting as a black box to produce a totally incorrect structure o international union of crystallography t a jones j y zou s w cowan and m kjeldgaard an introduction to our ideas are implemented in a new computer graphics program o information is maintained in a database that can be updated by the user by the program itself or by other utility programs each molecule in the database has a user defineable name the usual structural data associated with a molecule are converted into nine vectors of information table we refer to some of these vectors as properties that can be associated with residues e g the amino acid sequence or atoms e g the temperature factors properties can be used for colouring purposes or to select the atoms that are to be displayed any number of molecules can be stored in the database and any number of graphical objects can be created from a molecule map interpretation before a complete model can be built it is necessary to understand how the protein main chain folds through the experimentally determined threedimensional matrix of the electron density map in particular it is necessary to decide on the correspondence between the protein sequence and the map in parallel one attempts to develop an idea for the overall or a significant part of the fold during this initial stage one frequently recognizes features in the density that may correspond to a part of the sequence one then tries to extend the sequence assignment in either direction until the alignment breaks down this produces a series of hypotheses that may be contradictory and requires both an overview and a detailed description of the map the overview can be produced with a skeleton representation of the density greer that has been implemented in a computer graphics program williams the detailed description can be obtained by the usual contoured net representation these representations have been map calculate skeletonised map edit skeletonised map assign ca positions from skeleton autobuild main chain autobuild side chain each residue each residue restore stereochemistry crystallographic refinement fig strategy overview for model building in an electron density map table o datablocks used to represent protein models and skeletons protein name ai_atom_b ai_residue_type du nters skeleton name anoi_residue_type nters orthogonal atomic coordinates names of atoms e g ca temperature factors of atoms occupancies of atoms atomic numbers name of residue e g amino acid name e g ala pointers for the residue to atom data residue centre of gravity and radius orthogonal atomic coordinates name of residue just one amino acid name just one pointers for the residue to atom data skeleton status codes skeleton connectivity codes combined jones thirup and used in our laboratory to solve a number of new protein structures myelin jones bergfors unge sedzik two types of rubisco molecules schneider lindqvist br lorimer andersson et al papd holmgren br cellobiohydrolase ii cbhii rouvinen bergfors teeri knowles jones and ribonucleotide reductase nordlund sj sberg eklund our new way of working with skeletons differs mainly due to the advantages of using o each skeleton is treated as a molecule with a number of extra database vectors table one is an atomic property used to specify the status of each skeleton atom these codes are mapped to userdefined colours when the skeleton atoms are displayed we recommend the use of a simple classification probable main chain possible main chain and side chain however situations may arise where a more complex set of assignments may be needed for example when a team of people are trying to interpret a map it may be useful to highlight changes made by different members of the team a second vector contains a description of how the skeleton atoms are connected because of errors in the phases used in the map calculation the density is rarely continuous from the amino to the carboxy terminus editing of the skeleton connectivity is therefore required to produce a continuous trace this is accomplished by commands that break or make connections between skeleton atoms one usually works with at least two objects made from a single skeleton one showing the proposed main chain trace within a large volume e g a sphere of and the other showing all skeleton atoms within a smaller radius e g 112 improved methods for building protein models building the model jones thirup demonstrated that given the ca coordinates of a protein one could reconstruct the main chain by linking together peptide fragments of different lengths these fragments were taken from a library of refined protein structures they further demonstrated that an initial model could be built by using the electron density skeleton as a framework to locate peptide fragments this was done interactively by specifying that certain skeleton atoms be used as ca guide positions or by allowing the program to place them along the skeleton at suitable distances the latter method occasionally results in a residue being skipped in a turn since our ultimate aim is to automate model building fully we now introduce a stage where the position of each ca atom in the molecule is explicitly defined at present this is achieved interactively by the user placing a particular ca at the position of a skeleton atom building a model from such a set of guide coordinates is a well known problem to protein crystallographers diamond tones and others purisma scheraga likewise numerous algorithms could be developed to use a protein structure database to reconstruct the whole protein from the ca trace one could for example use fixed length fragments variable length fragments satisfying some cutoff criterion jones thirup claessens van cutsem lasters wodak or dynamic programming algorithms to locate the minimum number of fragments where each fragment satisfies some r m cutoff the resulting model should have as a minimum requirement the side chains pointing roughly in the correct direction to reduce the amount of manual intervention later in the refinement the main chain carbonyl o atoms should also be correctly oriented our experience o i l o i i i i i i residue number fig the longest fragments found in a database of well refined structures that fit retinol binding protein with r m cutoffs of lower curve and upper curve the long fragments found around residue in the sequence correspond to the start of a four turn o helix suggests that the first qualitative requirement is met when the r m deviation between ca guide points and the database fragment is correct carbonyl o atoms in general require a r m fit of a in fig we show the longest fragments that can be located in our normal database of structures that fit residues in retinol binding protein rbp with and a cutoffs this gives average fragment lengths of and residues for the two cutoffs we could therefore build any part of rbp with a fragment of residues and be sure of getting the side chain directions roughly correct but we would require fragments of residues to get the correct peptide orientations since our aim is not to use the minimum number of fragments but to build an accurate structure fragments of residues are used this allows a better chance of recognizing low frequency conformations the complete backbone structure is built with a simple extension of our original scheme outlined in fig that we refer to as autobuilding at residue i in the structure the best fitting fragment is found that matches the ca of i to i however only residues i to i have their coordinates updated from the fragment because the other main chain atoms of residues i and i are not fixed by the superposition the next fragment is chosen by stepping forward residues comparing i and i and repeating the process this algorithm does not build either amino or carboxy terminal residues and therefore requires an extra residue at each end of the chain some deviation from standard bond lengths and angles will occur at the linkage between tripeptides and because of deviations in the structures making up the databank these deviations are however small and can be initially ignored the fitting algorithms make use of pre calculated distance matrices to speed up the comparisons jones thirup to test the quality of models produced by this procedure we have taken the ca coordinates of cbhii and rebuilt the main chain under various conditions this structure is a suitable example because it is very well refined r factor to resolution it is a large a fl protein residues with loops containing extensive non regular secondary structure the r m deviation of the rebuilt model is a for ca atoms and a for all main chain c i x x x x x x x x x fig the main chain of autobuilt structures is produced by identifying the best fitting pentapeptides in the database these fragments are combined overlapping by two residues only those residues marked by crosses have their coordinates updated by the transformed fragment coordinates t a jones j y zou s w cowan and m kjeldgaard table autobuilding of cbhii from ca coordinates containing random errors the r m deviations refer to the fit of the autobuilt structure to the correct structure r m main r m carbonyl ca error r m ca chain o atom i i atoms n ca cfl c o not unexpectedly the carbonyl o atoms show the largest deviation 04 a similar value for main chain atoms has been obtained by reid thornton in reconstructing clostridium flavodoxin smith burnett darling ludwig from ca coordinates using the frodo database in reality the guide ca atoms taken from an experimental map will contain severe errors partly because of the skeletonizing algorithm and partly because of phase errors in the map this has been simulated by introducing random errors into the refined coordinates of cbhii and autobuilding from the randomized ca coordinates the deviations of the rebuilt ca model are compared with the correct structure in fig below an introduced r m error of the autobuilt model shows a deviation from the correct structure that is slightly worse than the introduced error above this value the autobuilt model ca is a better fit to the correct ca coordinates table shows the deviation of the main chain atoms and carbonyl o atoms for errors approaching values to be expected in map interpretation oa o t fit io correcl model fig the effect of random errors in guide coordinates on the autobuilt model below the solid line the autobuilt struture is worse than the guide coordinates above the line the autobuilt structure is better the model is cellobiohydrolase ii model the side chains of the best refined structures show a high preference for discrete conformations james sielecki these conformations termed rotamers have been recently tabulated ponder richards mcgregor islam sternberg not all amino acids have well defined rotamers in particular lysine and arginine side chains are poorly modelled however for the remaining residues we consider building anything but rotamers into the initial model to be a mistake if we choose the rotamer closest to the conformation observed in our refined cbhii model the r m fit after autobuilding from the ca guide coordinates is a for all atoms taking the most common rotamer gives an overall fit of a a residue r factor when describing the goodness of fit of an initial model we are often forced to use vague qualitative expressions this is directly related to the subjective nature of map interpretation while the crystallographic r factor unambiguously shows how well a particular model matches the observed structure factors it still shows relatively poor discrimination of major errors in a model jones a more useful measure would indicate the position of possible errors in a structure wierenga kalk hol have published how well their structure fits their map on a per residue basis unaware of their efforts we have independently developed a more quantitative function that can be used to remove much of the subjectivity of map interpretation we believe this function will also be useful in localizing serious errors in map interpretation consider an electron density map on a grid gi given a set of coordinates a calculated electron density can be built up on an identical grid by assuming a gaussian distribution function for each atom diamond jones liljas the atoms are forced to have an overall temperature factor and the grid densities are scaled together with a single scale factor for residue i the electron density of a selected group of atoms within this residue is built on a third identical grid for every non zero element in we then calculate the real space fit for that residue as pobs pcalc pobs pcalc where pobs is taken from the equivalent element in and pcalc is the equivalent element in the function may be used to demonstrate the continuity of the main chain by using just the n c a c c and o atoms in the calculation alternatively by defining side chain atoms the function can identify where the protein sequence is out of register with the density the result of the calculation can be added to the o database as a residue property that in turn can be improved methods for building protein models used for colouring and or atom selection purposes when displaying the model figs a and b show the main chain fit for two well refined proteins rbp and cbhii r factors of and at a resolution respectively cbhii has two molecules in the asymmetric unit that we e co n t i i resxduo number a t j h g o u o r residue number b i i i l i i i i i i i i i 450 resldue number c fig plots of main chain real space fit residuals for a refined retinol binding protein model b refined cellobiohydrolase ii core model the core protein begins at residue in the sequence c partly refined cellobiohydrolase ii core model table a simulation to demonstrate the use of residue real space fits to determine out of register errors the side chains of cbhii model were mutated optimally fitted to the density by hand and then further refined with x plop the real space fits rsf are made for all atoms in each residue the two regions were chosen at random region residue 232 correct asn leu gly thr pro lys cys sequence rsf 21 shifted asn gly thr pro lys cys cys sequence rsf 29 region residue 141 145 correct asp lys thr pro leu met sequence rsf shifted lys thr pro leu met met sequence fit refer to as a and b clearly all of the cbhii main chain has continuous density but in rbp there is one region with poor main chain density fig c shows the main chain fit of a cbhii model obtained during its crystallographic refinement this model mt with an r factor of for all data in the resolution range a was extensively rebuilt two regions were identified where the sequence was out of register with the density and a number of localized poorly fitting areas were found in either the a or b molecule both of the out of register regions the first nterminal residues and can be recognized by the poor real space residue fit the remaining spikes mostly correspond to residues that could be rebuilt by copying the equivalent atoms from the other chain the residue residual could also be used to search directly for out of register errors we have simulated this by deliberately introducing such errors in the a chain of our best refined cbhii model optimizing the fit of the side chain to the density by hand and then crystallographically refining the structure steps of powell minimization with x plor brfinger et al the residue real space fits for two experiments are shown in table where the correct alignments are clearly identified an automatic procedure to search for and residue mismatches would require procedures to optimize the mutated side chain to the density these tools have been developed and will be described in a separate publication in the above formulation the grid acts as an envelope within which to make the grid sum calculations other ways of forming the envelope may be better suited for certain applications for example when searching for out of register errors the current method of defining the envelope gives poor discrimit a jones j y zou s w cowan and m kjeldgaard table autobuild model statistics rsfmc and rsfah are the average residue real space fit factors calculated for main chain and all of the residue respectively r refers to the normal crystallographic r factor and is calculated for all measurements in the resolution range the model numbering is explained in fig the r m deviations are with respect to our best refined model the r m deviations in brackets are calculated where arginine and lysine residues are treated as alanines r m r m r m r m r m model rsfmc rsfan r ca o main side all 420 225 51 m 05 m 87 19 68 miio 506 05 70 m 38 00 0 0 0 246 0 96 38 53 93 99 53 0 0 0 0 44 0 0 nation when a small side chain fits in the density meant for a longer side chain other envelopes can be made to improve this particular situation also other grid sum calculations could be employed such as correlation coefficients as alternative indices of fit the careful monitoring of temperature factors especially for refinements carried out at high resolution has been widely used to monitor coordinate errors indeed the large peak in the real space fit function of rbp in fig a corresponds to a region of high temperature factors the main advantage of using the real space approach therefore is its applicability at any stage of the modelling procedure including the initial construction of a model it can also be used for studies at lower resolution where the temperature factors may be poorly defined we are not aware of any disadvantage of using the real space approach instead of studying temperature factors mg skeleton g n etac we rotamets j sa m rsr t d rogulaflze mlll fig experiments on building models in the mir map of myelin sa refers to crystallographic refinement using simulated annealing with the program x plor and the protocol shown in table table x plor protocol used for all model refinements values as recommended in the x plor manual used unless stated otherwise below resolution overall temperature factor 0 nbonds dielectric constant e 0 no charges on lys glu asp and arg side chains check stage to obtain the weighting terms w a and we steps powell minimization harmonic repulsive term 40 steps powell minimization normal van der waals terms cycles molecular dynamics at k preparation stage conjugate gradient minimization harmonic restraints on the ca of kj mole t 40 steps powell minimization harmonic repulsive term 160 steps powell minimization normal van der waals terms slow cool stage steps of molecular dynamics at each temperature starting from k and dropping k until reaching k time step 0 0005 ps final stage conjugate gradient minimization steps of powell minimization without phase restraints time scale on a stellar on average h of cpu automatic refinement we can now ask ourselves if it is possible to produce a completely refined structure from just a ca trace for a test case we have chosen a smaller crystallographic problem myelin protein jones et al and have carried out a number of experiments that have been summarized in fig and table models have been crystallographically refined using the same simulated annealing protocol table and the program x plor in the original study the structure was solved from a map phased with two derivatives using anomalous dispersion the derivatives have identical sites the crystal contains three molecules in the asymmetric unit each molecule consisting of amino acids native diffraction data have been collected to resolution originally a model of one chain was built improved methods for building protein models with frodo from a skeleton using fragments from the database this model was then used to build the other two chains and the three molecules refined to the density using the method described by jones liljas the results were checked at the display and manually refitted where necessary this model referred to as has an r factor of without fatty acid ligand and has no out of register errors our current best model was obtained after numerous alternating cycles of crystallographic refinement by simulated annealing and manual rebuilding model has an r factor of for all data in the resolution range a the average residue fits both main chain and all atom to the mir map are approximately the same for both models for comparison purposes we have repeated the refinement of using the protocol of table to give in our first experiment the same edited skeleton originally used to build was used to make an initial ca trace atoms were placed where we had left a side chain branch point in the skeleton a complete molecule was autobuilt using the most frequent rotamer for each side chain the other two chains were built from this model by applying the known non crystallographic operators to give model this model was crystallographically refined to give as judged by the r factor this refined model is good and the r factor shows a significant drop from to the r m fit of the models to however is not so impressive the main chain atoms show an improvement from to 09 but the side chain atoms still have a high r m deviation 95 most of these side chain errors are localized in to a few residues in particular the longest amino acids this is serious for myelin because this protein is particularly rich in arginine and lysine residues out of residues shows a worse average fit to the mir map than does model despite a lower r factor real space refinement of a model to a map should in theory improve the goodness of fit diamond however with a rough initial model great care must be taken since volume fitting algorithms have a large radius of convergence this can easily result in large side chains such as phenylalanine rings moving into main chain density for a moderately well fitting model the situation can be alleviated by refining into a residual map jones liljas this is calculated by subtracting the scaled density built up using the current model from the experimental map when an atom or a group of atoms is to be refined its model density is first added back to the residual map in the strategy outlined in fig 1 after the mainchain autobuild the main chain atoms should have a reasonable fit to the density therefore only these atoms should be subtracted from the experimental map for each residue keeping the ca fixed and allowing a rotational search of the whole residue we can then find the rotamer that best fits the density all residues in this model in our experiment should now approximately fit the electron density in the next step therefore all atoms can be subtracted from the density to form the residual map for each residue we then carry out a rigid body rotation and translation search to find the best fit to the density this model will have relatively poor stereochemistry at the peptide linkage and should be regularized models and as judged by the residue fits agrees with the mir map as well as and the model after refinement is better than judged by both r factor and r m fit to however there are a number of obvious errors that can be recognized by inspecting those residues having the poorest fits fig 7 the two worst fitting residues in the a chain leu and met are clearly wrong when viewed at the display a model with a final lower r factor can be produced by interactively deciding on the choice of rotamer in the initial model in this model there is a poor initial fit to the mir map since no real space refinement or manual fitting was carried out the refined model m l shows improved fit and lower r factor fig shows a histogram of how equivalent atoms in and are spatially separated of the atoms are within 1 of one another the average residue r m deviations are plotted in fig according to amino acid type not surprisingly the worst errors occur for arginine and lysine residues i e those residues having the worst rotamers the behaviour of serine residues appears surprising but this is due to one residue ser 1 at the amino terminus deviations of 7 and for each chain likewise the asparagine value is influenced by errors in asn and asn these errors are also likely to be overestimates 0 o 03 0 40 00 120 residue number fig 7 the all atom residue real space fit of the a chain of myelin model m to the mir map t a jones j y zou s w cowan and m kjeldgaard because only the coordinates of the a chain were fitted to the density when comparing m to model there are significantly more large deviations observed for atoms in the b and c chains the r m differences between the three chains in the final refined model is 1 a for all atoms and 0 for ca atoms to illustrate the inadequacy of the normal crystallographic r factor we have built a completely back wards structure of myelin in this model residue i is built at residue i the autobuilt structure refines to an r factor of model the real space fit values start badly and do not r improve upon refinement a plot of the main chain torsion angles does not clearly distinguish between models and similarly the r m deviations of bond lengths angles and fixed dihedral angles have normal values 1 realistically we cannot expect to autobuild a model better than which was the result of many hours careful modelling however we had hoped that 0 after simulated annealing the best autobuilt models would be as good as those obtained starting from as a control therefore has been refined with the same protocol this model is the best model we have produced as judged by r factor r m fit and density fit we are aware of and believe we can overcome the problems associated with modelling lys arg residues if we omit them from 1 i i i u o ioooii i oh il t ool il i 51051125121275135 distance fig histogram of separations of equivalent atoms in models and of myelin the atoms in the three chains are included m c m ot r gi v ala pro se cys th val lie leu h aspasn ghj gin phe tvr trp met lys ar 0 amino acid fig r m deviations of and of myelin according to amino acid type the protein has no histidine residues the comparison the r m differences of and to then differ by only 0 k locating errors in the model during refinement databases can also be used to monitor the quality of structures undergoing crystallographic refinement in 40 residue number a o t i i i i i 250 i residue number b n t i jt i i i 150 200 t i 250 300 residue number c fig carbonyl o atom error indications pepflip for a a well refined high resolution structure crambin b and c models and of the cytochrome of the reaction centre improved methods for building protein models this publication we are concerned with errors in peptide orientation and side chain conformation to monitor peptide errors each pentapeptide in the structure is compared with the database at position i in the sequence therefore we locate the bestfitting fragments to the zone i i the r m deviation of the carbonyl o atom of residue i to the equivalent o atoms of these database fragments is then used as an index of fit the distribution for a well refined structure crambin hendrickson teeter is shown in fig a the small peaks in this function occur at loops connecting secondary structure elements and are the result of some fanning in the orientation of the peptide planes fig b shows the result obtained from a partly refined structure model of the cytochrome of the reaction centre from rhodopseudomonas viridis deisenhofer michel this plot is more representative of the results obtained while monitoring a refinement the very sharp peaks having r m deviations correspond to peptides where the o atom points in the opposite direction from the database structures fig we have compared the suggestions made from this calculation with the independent actions taken by deisenhofer michel during their refinement table shows a good correlation between the suggested errors and the actions taken in producing model in their refinement our experience with this method suggests that every residue showing a deviation a is worth investigating the method gives false peaks for structures with cis peptides because there are too few such structures in the database it also highlights conformations that have a moderate but not absolute requirement for a glycine at the next residue in the sequence such conformations frequently have carbonyl o atoms orientated in one direction for the glycine and in the opposite direction for non glycine residues thus model of the cytochrome also shows spikes that persist to their final model the side chain conformations can be monitored to find the r m deviation to each possible rotamer for the residue the lowest value is taken as the index of fig the worst carbonyl of cytochrome model thick lines including side chain atoms with the database structure overlaid thin lines main chain atoms the carbonyl o atom of residue points in the opposite direction compared to the database structure table sorted list of proposed peptide flips for reaction centre cytochrome model the column gives the distance separating the carbonyl o atoms in models and the action column states the action carried out by deisenhofer during his refinement residue r m o action 35 peptide flip 95 32 peptide flip peptide flip 94 peptide flip 92 peptide flip 0 no action but polyproline region 0 28 no action 83 0 26 no action 82 0 no action but 47 was a region of many errors 77 0 20 no action but peptide flip 1 half peptide flip 128 67 0 18 no action 0 no action but peptide flip 64 0 40 no action 0 39 no action 59 peptide flip 19 0 no action but peptide flip 2 51 1 half peptide flip fit high values may correspond to errors in the structure as implemented both algorithms calculate residue properties that can be used with o for colouring and selection purposes the implementation is noteworthy in another respect the programs generate files of o commands that can be activated one by one to place the user at the trouble spot activating the necessary commands to illustrate the problem the user then simply agrees that there is an error and corrects it or moves on to the next problem we intend to develop this idea further to other problems so that the user is presented with a suggestion the reasons and then has to decide this work has been supported by uppsala university and the swedish natural science research council mk was supported by the bioregulation centre at aarhus university we thank dr johan deisenhofer for making his coordinates available to us and dr rik weiranga for pointing out to us his independent use of real space fitting abstract the design implementation and capabilities of an extensible visualization system ucsf chimera are discussed chimera is segmented into a core that provides basic services and visualization and extensions that provide most higher level functionality this architecture ensures that the extension mechanism satisfies the demands of outside developers who wish to incorporate new features two unusual extensions are presented multiscale which adds the ability to visualize large scale molecular assemblies such as viral coats and collaboratory which allows researchers to share a chimera session interactively despite being at separate locales other extensions include multalign viewer for showing multiple sequence alignments and associated structures viewdock for screening docked ligand orientations movie for replaying molecular dynamics trajectories and volume viewer for display and analysis of volumetric data a discussion of the usage of chimera in real world situations is given along with anticipated future directions chimera includes full user documentation is free to academic and nonprofit users and is available for microsoft windows linux apple mac os x sgi irix and hp unix from http www cgl ucsf edu chimera wiley periodicals inc j comput chem key words molecular graphics extensibility visualization multiscale modeling sequence alignment introduction since its inception the ucsf computer graphics laboratory has worked on molecular visualization systems to meet the needs of researchers in the field beginning with mms in to our current offering ucsf henceforth chimera chimera immediate predecessor the midas midasplus henceforth midas provided us with the insight that extensibil ity should be considered critically important in the design of a visualization system midas was a highly successful molecular graphics system however it was relatively difficult for users to add new function ality the first extension mechanism introduced into midas was the ability to send an annotated protein data bank pdb file repre senting the currently displayed scene to an external program 5 this was motivated by our desire to interface to rendering programs such as once this extension mechanism was avail able it became relatively easy to develop rendering back ends of our own and soon thereafter we developed both a fast space filling renderer with shadows conic 7 and a jane richardson rib bon depiction program with many capabilities ribbonjr further more outside developers exploited the mechanism in ways that we had not anticipated thomas hynes then with genentech wrote neon a program to allow midas to depict shadowed ball and stick scenes neon acted as a filter between midas and conic taking the pdb file output by midas and producing another pdb file with an order of magnitude more atoms neon output contained larger radius atoms for balls and series of many closely spaced smaller radius atoms to simulate sticks to our surprise conic was able to process the neon file quickly enough seconds to minutes on computers of the day to be usable this whole neon concept was something that would never have occurred to us and opened our eyes to the power of an effective extension mechanism a second extension mechanism was subsequently created to allow midas to communicate on an ongoing basis with external programs the midas user could send commands to an external program and the external program could issue midas commands to cause changes in the midas session although this mechanism was also quite successful it was ultimately constrained to the available midas command set the restrictions imposed by the original correspondence to t e ferrin e mail tef cgl ucsf edu contract grant sponsor nih contract grant number 2004 wiley periodicals inc pettersen et al vol no journal of computational chemistry figure 1 a single frame of a molecular dynamics trajectory of a buckytube in water shown with the movie extension a lens has been placed over the center of the tube to strip away the surface and reveal the hydrogen bonded chain of waters passing through the tube the trajectory was computed with polarizable molecular dynamics using the amber sander module j caldwell ucsf unpublished data midas design were proving problematic and we were motivated to create a new system with greater extensibility chimera was designed with extensibility as a primary goal we also wanted chimera to be portable to a wide variety of platforms and to include state of the art graphics capabilities such as trans parency and interactive ball and stick space filling ribbon and solid surface representations another design goal was to make chimera accessible to users at all skill levels by providing both a graphical menu window interface and a command line interface chimera architecture chimera primary programming language is python 9 impor tantly python is an interpreted object oriented programming lan guage that is also easy to learn and very readable because python is interpreted it is good for rapid development and debugging readability is important for a team development project like chimera and an easy to learn language enables others to develop extensions without undue effort chimera includes the python standard idle interactive development to help diagnose problems during extension development chimera is divided into a core and extensions the core pro vides basic services and molecular graphics capabilities all higher level functionality is provided through extensions this design with the bulk of chimera functions provided by extensions en sures that the extension mechanism is robust enough to handle the needs of outside researchers wanting to extend chimera in novel ways extensions can be integrated into the chimera menu system and can present a separate graphical user interface as needed using the tkinter tix and or toolkits the chimera core consists of a c layer that handles time critical operations e g graphics rendering and a python layer that handles all other functions all significant c data and functions are made accessible to the python layer core capabili ties include molecular file input output molecular surface gener ation using the msms algorithm and aspects of graphical dis play such as wire frame ball and stick ribbon and sphere representations transparency control near and far clipping planes and lenses screen areas with different display attributes see fig 1 another core service is maintenance and display of the current selection users may select parts of structures by picking with the mouse by making menu choices e g aromatic rings or via certain extension actions the selected structure areas are high lighted with a particular color or a colored outline extensions can query for the contents of the selection many menu actions such as coloring or setting the display style work on the current selection figure 2 bluetongue virus core particle pdb identifier with double stranded rna attached to the surface top trimers in the outer protein layer that are equivalent under the icosahedral symmetry are given the same color the free end of the rna attaches to other viral particles in the crystal a closeup of the inner layer bottom shows ball and stick and ribbon models and a surface at higher resolution the core also maintains a trigger mechanism wherein changes to core data structures or state are reported to extensions that have registered callbacks with the corresponding trigger for example there is a selection changed trigger that fires whenever the current selection changes extensions are written either entirely in python or in a combi nation of python and c c the latter using a shared library loaded at runtime extensions can be placed in the chimera installation directory which would make the extension available to all users or in the user own file area extensions are loaded on demand typically when the user accesses a menu entry that starts the extension the class structure of molecular data and other extension programming information can be found at http www cgl ucsf edu chimera docs programmersguide examples we demonstrate chimera extensibility by presenting several extensions the multiscale and collaboratory extensions are quite unique and demonstrate the wide spectrum of abilities that can be generated the others provide insight into the use of core facilities by extensions and the integration of extensions with the chimera environment multiscale the multiscale extension adds capabilities for interactively explor ing large molecular assemblies we have focused on viral struc tures condensed chromosomes and ribosomes additional exam ples include cytoskeletal fibers and motors flagellar structures and chaperonins the multiscale extension displays structures from the and generates their multimeric forms by using transforma tion matrices to position the subunits multiscale can also be used with large assemblies where there are no repeated subunits pdb chains can be displayed as low resolution surfaces or in any of the standard molecular representations available in chimera multi scale permits biologically meaningful levels of quaternary struc ture to be defined the abilities to build multimeric forms display low resolution representations and define levels of structure are important for interactively exploring large complexes multiscale uses chimera core molecular display abilities data structures file reading and selection management and the volume viewer extension for surface calculation and rendering this ready to use infrastructure allowed us to focus on the new capa bilities needed for displaying complexes most of the available atomic resolution viral structures are icosahedral particles with fold symmetry pdb files provide atomic coordinates for only one subunit to build a multimeric model subunits are positioned using rotation translation matrices read from the pdb file header pdb remark records give matrices that can be used to generate the biological oligomeriza tion state crystallographic smtry records and noncrystallo graphic symmetry mtrix records matrices or matrices inferred from the space group record of crystal structures can also be used multiscale visualization capabilities have revealed shortcomings in the matrix information for many large scale struc ture entries that otherwise would have been difficult to detect we are working with the pdb to find and correct these entries for efficiency the multiscale extension only loads atomic coordinates for subunits when they are needed when a model is first displayed only a low resolution surface representation is shown so no additional copies of the coordinates need to be loaded chimera atomic and residue level display styles are also available but are typically used for only a small number of subunits so that the amount of detail depicted does not overwhelm the user the multiscale extension does not use the high resolution molecular surfaces that are a core feature of chimera such sur faces would render too slowly for a large multimer and provide too high a level of detail to best illustrate the organization of subunits the multiscale extension was originally written entirely in python to speed up the surface calculation certain critical rou tines were rewritten in c converting these routines from interpreted python to compiled c made them run about times faster translation is generally straightforward because py thon objects such as molecules atoms and lists have equivalent c objects rendering the surface with opengl another time critical step uses the c module in the volume viewer extension subunits can be selected with the mouse to simplify selecting larger pieces of a structure new structure levels can be defined hierarchically for example the bluetongue virus core has two protein layers the outer layer being composed of trimers fig 2 for this structure it is useful to define inner and outer layers and trimers as structural levels after an individual outer layer monomer is selected the selection can then be pro moted to the containing trimer and subsequently promoted to the whole layer of trimers the whole outer layer can then be hidden if the object of interest is the inner protein layer besides being promoted a selection can be extended to all identical copies of the currently selected subunits it is also possible to select just the subunits for which atomic information has been loaded structure levels can be specified in a python script structural hierarchies are sometimes described in text in the headers of pdb files the mmcif file format available from the has a limited ability to describe such higher levels of structure in a computer readable form but few submitted data sets provide this informa tion for multimeric structures investigations are often facilitated by the presence of more than one copy of the asymmetric unit the bluetongue virus illustrates how working with the full viral shell can aid analysis the crystallographic data used to determine the capsid structure revealed viral double stranded rna stuck to the outside of the fig 2 to investigate the specific atomic contacts between the rna and virus it is helpful to locate the several subunits of the icosahedral particle adjacent to the rna by inspecting the full particle these can then be exam ined using an all atom display to determine the contacts account ing for the stickiness of the capsid the multiscale extension is intended for problems where both large scale and atomic scale details are relevant the tools needed to explore models with many levels of structure and large numbers of atoms are necessarily complex the chimera multiscale exten sion has addressed only the most immediate needs we anticipate increasing its capabilities significantly in future releases multalign viewer the multalign viewer extension allows chimera to display se quence alignments together with associated structures fig 3 1608 pettersen et al vol no journal of computational chemistry figure 3 three structures in chimera associated with sequences in an alignment shown by multalign viewer the sequences with color swatches behind their names are associated with the pectate lyase struc tures 45 46 and shown in yellow magenta and cyan respectively the structures were superimposed using the sequence align ment the fits were refined by iteratively removing bad residue pairings the sequences are colored by secondary structure strand and helix regions are pink and gold respectively and selected structure regions are green indicated on the structures with a green outline chimera zone selection method was used to select all residues within 3 å of the active site metal ion in one of the structures once associations have been set up many useful features become active positioning the mouse over a sequence character shows the number of the corresponding residue in the structure in a status area making selections on the structures highlights the corresponding regions of the sequence alignment dragging boxes on the sequence alignment selects and highlights the correspond ing structure regions structure regions can be selected based on conservation in the alignment greatly facilitating coloring by conservation level or showing only conserved side chains sec ondary structure elements can be depicted on the alignment with colored boxes clicking on residues in the sequence makes the chimera window zoom in on the corresponding structure residues structures can be superimposed using the sequence alignment optionally using only highly conserved residues and also option ally iteratively refining the fit by pruning poorly superimposed residues an alignment can be searched with a literal string or a pros pattern matches are highlighted on the alignment and can also be highlighted on associated structures other extensions can call multalign viewer to show align ments for example the ssd structure superposition database 22 extension uses multalign viewer to show sequence alignments corresponding to structural alignments of interest multalign viewer is under active development important short term goals are to provide more sophisticated editing facilities and to display and interact with phylogenetic information viewdock multalign viewer can read and write sequence alignments in a wide variety of popular formats currently aln aligned fasta gcg msf gcg rsf aligned nbrf pir and stockholm multalign viewer facilitates analysis of alignments in the con text of structural information and vice versa first structures in chimera must be associated with their corresponding sequences in an alignment when multalign viewer opens an alignment it examines the structures currently open in chimera and checks each chain for high sequence identity with an alignment sequence chains with high identity are associated with the best matching sequence however only one chain per structure is associated with a sequence multalign viewer registers with the chimera core model opened trigger so that as new structures are opened they will also be examined and associated if appropriate conversely if the alignment sequence names are recognizable as including 20 or identifiers using a few simple criteria the researcher can use a multalign viewer menu item or preference setting to load all of the corresponding structures into chimera this was easy to implement because the chimera core offers functions for opening pdb scop files based on their identifiers and will retrieve them via the world wide web or local disk as appropriate if multalign viewer fails to make an appropriate automatic association between a sequence and structure it can be manually directed to make the association associations are indicomputer graphic cated by showing the color of the associated model behind the name of the sequence fig 3 the viewdock extension facilitates interactive screening of ligand orientations from dock 23 dock calculates possible binding orientations given the structures of ligand and receptor molecules often a large database of compounds is searched against a target protein where each compound is treated as a ligand and the target is treated as the receptor simple scoring methods are used to identify the most favorable binding modes of a given molecule and then to rank the molecules the output consists of a large number of candidate ligands in the binding orientations considered most favorable by dock figure the viewdock interface lists docked molecules clicking on a line displays just the corresponding molecule and shows its information in the lower part of the panel ribose monophosphate is shown docked to h ras carbon atoms are light gray oxygen atoms are red nitrogen atoms are blue and phosphorus atoms are cyan hydrogens are not shown potential hydrogen bonds are indi cated with yellow lines viewdock reads the dock output and provides a convenient interface for filtering results in the context of the target structure when a line in the list of compounds is clicked just the corre sponding molecule is shown in the putative binding site and its information is shown in the lower part of the panel fig 4 the information may include compound name description and various scores and score components any of the descriptors can be shown in the list and or used to sort it it is also possible to view more than one docked molecule at a time compounds can be deleted from the list if visual inspection reveals them to be unsuitable compounds can also be screened by the number of hydrogen bonds formed with the target structure and by whether or not the hydrogen bonds involve specified groups in the site hydrogen bond detection uses a set of detailed distance and angle criteria from a published small molecule crystal sur and is an extension provided capability of chimera movie the movie extension allows chimera to show molecular dynamics trajectories the trajectories may be played forward or backward either a single frame at a time or continuously all generic chi mera capabilities such as coloring hydrogen bond detection lenses fig 1 and saving pdb files are available for use with the trajectory movie explicitly supports execution of a script python or chimera commands at each frame this makes it easy for example to save images for later assembly into a quicktime or mpeg video movie currently supports all versions of trajectory files and support for and formats is in progress volume viewer the volume viewer extension displays three dimensional grid data such as density maps from electron microscope recon structions or x ray crystallography calculated electrostatic poten tial and solvent occupancy from molecular dynamics simulations it reads several file formats or mrc brix or 30 or xplor 32 spider 33 delphi34 or potential maps priism netcdf 37 38 and scoring grids dis plays isosurfaces meshes and translucent solids and allows in teractive adjustment of thresholds transparency and brightness volume data is often displayed with related molecular models the display is automatically updated when settings are changed for example dragging a threshold indicator shown on a histogram of data values updates the displayed surface or mesh for large data sets subsampling can be used to improve the response time when the display is rotated or a threshold is changed subsampling with step size 2 renders the data after omitting every other data plane along each axis data sets of by by values can be displayed with a new threshold once a second or rotated at frames per second on generic desktop pc hardware equipped with a mid range graphics adapter a subregion of the data can be chosen by dragging a box with the mouse and then shown instead of the whole data set subregions can be named so that it is easy to return to them in later sessions the volume path tracer extension allows marker placement and path tracing in grid data markers are placed by mouse click the marker is positioned on the closest visible data maximum along the line of sight under the cursor markers can be moved after they are placed consecutively placed markers can be linked with segments to trace a path additional connections can be added with the mouse to build simple structural models volume path tracer was developed to trace protein backbones in intermediate resolu tion 5 8 å density maps from electron cryo microscopy fig 5 and fluorescently labeled chromosomes in multiwavelength light microscopy data fig markers and associated connecting segments are implemented using the same mechanism as atoms and bonds thus display styles and colors can be changed in the same way as for atoms and bonds distances between pairs of markers and between markers and molecular structures can be measured and traced structures can be aligned using chimera molecule manipulation capabili ties traced paths can be displayed as smoothly interpolated curves fig 6 markers and connecting segments can be saved in an file for analysis by other software collaboratory chimera collaboratory extension enables researchers at geo graphically distant locations to share a molecular modeling session in real time by default all users connected to the same session have equal control over the models structures being viewed a change made by any participant is immediately propagated to all other participants so that a synchronized view of the data is maintained throughout a session because of the complex nature of molecular models inter active examination of models in a real time collaborative environ ment is far more effective than traditional asynchronous forms of communication such as passing molecular data back and forth through e mail a crucial element of real time collaboration soft ware is the efficient transfer of information not only must the software ensure that information is transmitted rapidly it must also consider the availability of network resources such as bandwidth and transmit information in an efficient format so that the appli cation can run in parallel with other bandwidth heavy applications such as videoconferencing software given these considerations application independent collaboration tools are not as responsive for sharing molecular modeling sessions such desktop sharing applications e g microsoft netmeeting and virtual network function by transmitting the contents of the screen from the workstation running the application to all who are sharing it this can be bandwidth intensive especially for molecular graphics where most operations alter a large amount of screen content instead the collaboratory works on a lower level by transmitting small messages describing just the data that has been modified the collaboratory utilizes a star architecture with a central hub connected to multiple nodes each node is a user running chimera while the hub is a separately running application that acts as a rendezvous point between the nodes participants instances of chimera are notified when a model has been opened closed or modified a data file need only be present on the system of the participant who opens the file in chimera parameters monitored pettersen et al vol no journal of computational chemistry figure 5 density map for rice dwarf virus capsid protein shown as a mesh left the map was obtained by electron cryo microscopy and has 6 8 å resolution computationally identified alpha helices are shown as and the connecting turns have been hand traced using chimera volume path tracer on the right is a crystal structure of the distantly related bluetongue virus capsid protein for changes include display color molecular representation and spatial properties such as position and orientation change tracking is accomplished using the chimera core trigger mechanism corba common object request broker architecture mes sages are used to relay information they are platform and lan guage independent allowing the collaboratory to work on a va riety of systems the person who starts the hub acts as the session administrator the administrator can specify a password that others must supply to join the session the administrator can also obtain information about current participants such as username and total time con nected close the session to new participants and control whether each user can affect the state of the session participants can join the session at any time when they sign in their local instances of chimera will be updated to reflect the global state of the session participants can leave the collaborative session at any time and continue their modeling sessions in an isolated setting several features facilitate remote collaboration a participant can display the position of his pointer on collaborators screens to draw attention to a region of interest although users are expected to communicate primarily by telephone or an independent video conferencing application the collaboratory provides a chat mechanism for passing text messages this can be especially useful for transferring urls or sequence information users can also view other participants commands as they are entered and see who is actively manipulating the models the star architecture scales well because each node has knowl edge of only the hub not other nodes the collaboratory has been used productively with three participants in a session and in a test situation has been able to accommodate up to users in practice human factors tend to limit the size of a useful collaboration interpersonal communication can be difficult when subtle visual cues such as body language and eye contact are missing as is often the case with videoconferencing however when used in a sensi ble collaborative setting i e efficient voice video transmission a reasonable number of participants the chimera collaboratory and its rich set of features help to alleviate these issues results chimera is freely available to academic and nonprofit research ers from http www cgl ucsf edu chimera and can be licensed by commercial institutions for a fee extensions to chimera developed by outside researchers can be redistributed freely since chimera s first public release in march downloads of chimera have steadily increased amounting to approxi mately per month at this writing and totaling more than 000 multiple downloads to the same ip address for the same os platform in a single 24 h period are counted as a single download as stated earlier portability has been one of the primary design goals of chimera our implementation based on many widely available standards such as opengl has proven to be quite portable and there are chimera distributions for microsoft win figure 6 fluorescently labeled drosophila chromosome imaged by wide field deconvolution microscopy m lowenstein and j sedat ucsf unpublished data three fluorophores were used to label specific segments of chromosome two homologous copies of the chromosome are shown the cyan isosurface is the nuclear envelope the individual fluorescent spots have been marked with the volume path tracer extension and paths connecting the markers are shown as smooth tubes traced structures from many cells can be clustered to study structural patterns of chromosome organization in the nucleus dows xp nt linux apple mac os x sgi irix and hp unix the graphical menu window and command line interfaces pro vide rich and overlapping sets of functionality which will continue to grow as chimera is developed detailed user documentation is included with the program our goal of enabling others to extend chimera has enjoyed increasing success as the use of chimera has become more wide spread currently extensions written and or distributed by others include ssd which extends chimera for use with the structure superposition a web accessible resource viewfea ture which allows chimera to show results from feature a program for predicting metal binding or active sites in biomol ecules and emanimator s ludtke baylor college of medicine unpublished which makes it easy to create animation sequences in chimera and save them as mpeg files in addition chimera is supported as a display application for sequence and structure information by the web accessible databases and the structure function linkage database babbitt laboratory ucsf discussion chimera has been designed to facilitate the addition of new func tionality in particular nearly all concepts for example atoms bonds and molecules are represented as python objects which means they can take advantage of the object oriented nature of python the downside of making everything into python objects is a performance penalty both in speed and in memory usage we decided to favor programmability over performance whereas performance deficiencies can be addressed transparently to end users by algorithmic changes and hardware improvements such as increased processor speed and faster graphics adapters modifica tions to user and programming interfaces are typically more dis ruptive currently one of our biggest challenges is to improve perfor mance in the context of the large scale systems for which the multiscale extension was designed although many structures from crystallography and nmr determination are not very large tens of thousands of atoms the complexity increases rapidly as one considers objects such as viral particles the order of magni tude increases in scale require better algorithms for simplifying the representation and more efficient use of desktop computing re sources in the orthogonal direction of programmability we are inves tigating avenues for facilitating interactions between chimera ex tensions in our current implementation extensions must explicitly monitor predefined triggers to detect changes made by the chimera core and other extensions the change detection granularity is currently quite coarse meaning that extensions may need to sift through a large amount of data to detect a few interesting data modifications we plan to alter the low level object implementa tion to address these shortcomings abstract this paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene the features are invariant to image scale and rotation and are shown to provide robust matching across a substantial range of affine distortion change in viewpoint addition of noise and change in illumination the features are highly distinctive in the sense that a single feature can be correctly matched with high probability against a large database of features from many images this paper also describes an approach to using these features for object recognition the recognition proceeds by matching individual features to a database of features from known objects using a fast nearest neighbor algorithm followed by a hough transform to identify clusters belonging to a single object and finally performing verification through least squares solution for consistent pose parameters this approach to recognition can robustly identify objects among clutter and occlusion while achieving near real time performance keywords invariant features object recognition scale invariance image matching introduction image matching is a fundamental aspect of many prob lems in computer vision including object or scene recognition solving for structure from multiple im ages stereo correspondence and motion tracking this paper describes image features that have many prop erties that make them suitable for matching differing images of an object or scene the features are invariant to image scaling and rotation and partially invariant to change in illumination and camera viewpoint they are well localized in both the spatial and frequency do mains reducing the probability of disruption by occlu sion clutter or noise large numbers of features can be extracted from typical images with efficient algorithms in addition the features are highly distinctive which allows a single feature to be correctly matched with high probability against a large database of features providing a basis for object and scene recognition the cost of extracting these features is minimized by taking a cascade filtering approach in which the more expensive operations are applied only at locations that pass an initial test following are the major stages of computation used to generate the set of image features scale space extrema detection the first stage of computation searches over all scales and image lo cations it is implemented efficiently by using a difference of gaussian function to identify poten tial interest points that are invariant to scale and orientation keypoint localization at each candidate location a detailed model is fit to determine location and scale keypoints are selected based on measures of their stability orientation assignment one or more orientations are assigned to each keypoint location based on local image gradient directions all future operations are performed on image data that has been transformed relative to the assigned orientation scale and loca tion for each feature thereby providing invariance to these transformations lowe keypoint descriptor the local image gradients are measured at the selected scale in the region around each keypoint these are transformed into a repre sentation that allows for significant levels of local shape distortion and change in illumination this approach has been named the scale invariant fea ture transform sift as it transforms image data into scale invariant coordinates relative to local features an important aspect of this approach is that it gen erates large numbers of features that densely cover the image over the full range of scales and locations a typ ical image of size pixels will give rise to about stable features although this number depends on both image content and choices for various parame ters the quantity of features is particularly important for object recognition where the ability to detect small objects in cluttered backgrounds requires that at least features be correctly matched from each object for reliable identification for image matching and recognition sift features are first extracted from a set of reference images and stored in a database a new image is matched by indi vidually comparing each feature from the new image to this previous database and finding candidate matching features based on euclidean distance of their feature vectors this paper will discuss fast nearest neighbor algorithms that can perform this computation rapidly against large databases the keypoint descriptors are highly distinctive which allows a single feature to find its correct match with good probability in a large database of features however in a cluttered image many features from the background will not have any correct match in the database giving rise to many false matches in ad dition to the correct ones the correct matches can be filtered from the full set of matches by identify ing subsets of keypoints that agree on the object and its location scale and orientation in the new image the probability that several features will agree on these parameters by chance is much lower than the probability that any individual feature match will be in error the determination of these consistent clus ters can be performed rapidly by using an efficient hash table implementation of the generalized hough transform each cluster of or more features that agree on an object and its pose is then subject to further detailed verification first a least squared estimate is made for an affine approximation to the object pose any other image features consistent with this pose are identified and outliers are discarded finally a detailed compu tation is made of the probability that a particular set of features indicates the presence of an object given the accuracy of fit and number of probable false matches object matches that pass all these tests can be identified as correct with high confidence related research the development of image matching by using a set of local interest points can be traced back to the work of moravec on stereo matching using a corner de tector the moravec detector was improved by harris and stephens to make it more repeatable un der small image variations and near edges harris also showed its value for efficient motion tracking and structure from motion recovery harris and the harris corner detector has since been widely used for many other image matching tasks while these feature detectors are usually called corner detectors they are not selecting just corners but rather any image location that has large gradients in all directions at a predeter mined scale the initial applications were to stereo and short range motion tracking but the approach was later ex tended to more difficult problems zhang et al showed that it was possible to match harris corners over a large image range by using a correlation window around each corner to select likely matches outliers were then removed by solving for a fundamental ma trix describing the geometric constraints between the two views of rigid scene and removing matches that did not agree with the majority solution at the same time a similar approach was developed by torr for long range motion matching in which geometric con straints were used to remove outliers for rigid objects moving within an image the ground breaking work of schmid and mohr showed that invariant local feature matching could be extended to general image recognition prob lems in which a feature was matched against a large database of images they also used harris corners to select interest points but rather than matching with a correlation window they used a rotationally in variant descriptor of the local image region this al lowed features to be matched under arbitrary orien tation change between the two images furthermore they demonstrated that multiple feature matches could accomplish general recognition under occlusion and distinctive image features from scale invariant keypoints clutter by identifying consistent clusters of matched features the harris corner detector is very sensitive to changes in image scale so it does not provide a good ba sis for matching images of different sizes earlier work by the author lowe extended the local feature approach to achieve scale invariance this work also described a new local descriptor that provided more distinctive features while being less sensitive to local image distortions such as viewpoint change this current paper provides a more in depth development and analysis of this earlier work while also present ing a number of improvements in stability and feature invariance there is a considerable body of previous research on identifying representations that are stable under scale change some of the first work in this area was by crowley and parker who developed a repre sentation that identified peaks and ridges in scale space and linked these into a tree structure the tree structure could then be matched between images with arbitrary scale change more recent work on graph based match ing by shokoufandeh et al provides more dis tinctive feature descriptors using wavelet coefficients the problem of identifying an appropriate and con sistent scale for feature detection has been studied in depth by lindeberg he describes this as a problem of scale selection and we make use of his results below recently there has been an impressive body of work on extending local features to be invariant to full affine transformations baumberg tuyte laars and van gool mikolajczyk and schmid schaffalitzky and zisserman brown and lowe this allows for invariant matching to fea tures on a planar surface under changes in orthographic projection in most cases by resampling the image in a local affine frame however none of these approaches are yet fully affine invariant as they start with initial feature scales and locations selected in a non affine invariant manner due to the prohibitive cost of explor ing the full affine space the affine frames are are also more sensitive to noise than those of the scale invariant features so in practice the affine features have lower repeatability than the scale invariant features unless the affine distortion is greater than about a degree tilt of a planar surface mikolajczyk wider affine in variance may not be important for many applications as training views are best taken at least every de grees rotation in viewpoint meaning that recognition is within degrees of the closest training view in or der to capture non planar changes and occlusion effects for objects while the method to be presented in this paper is not fully affine invariant a different approach is used in which the local descriptor allows relative feature posi tions to shift significantly with only small changes in the descriptor this approach not only allows the de scriptors to be reliably matched across a considerable range of affine distortion but it also makes the features more robust against changes in viewpoint for non planar surfaces other advantages include much more efficient feature extraction and the ability to identify larger numbers of features on the other hand affine invariance is a valuable property for matching planar surfaces under very large view changes and further research should be performed on the best ways to com bine this with non planar viewpoint invariance in an efficient and stable manner many other feature types have been proposed for use in recognition some of which could be used in addition to the features described in this paper to provide fur ther matches under differing circumstances one class of features are those that make use of image contours or region boundaries which should make them less likely to be disrupted by cluttered backgrounds near object boundaries matas et al have shown that their maximally stable extremal regions can produce large numbers of matching features with good stabil ity mikolajczyk et al have developed a new descriptor that uses local edges while ignoring unre lated nearby edges providing the ability to find stable features even near the boundaries of narrow shapes su perimposed on background clutter nelson and selinger have shown good results with local features based on groupings of image contours similarly pope and lowe used features based on the hierarchi cal grouping of image contours which are particularly useful for objects lacking detailed texture the history of research on visual recognition con tains work on a diverse set of other image properties that can be used as feature measurements carneiro and jepson describe phase based local features that represent the phase rather than the magnitude of local spatial frequencies which is likely to provide improved invariance to illumination schiele and crowley have proposed the use of multidimensional histograms summarizing the distribution of measurements within image regions this type of feature may be particu larly useful for recognition of textured objects with lowe deformable shapes basri and jacobs have demonstrated the value of extracting local region boundaries for recognition other useful properties to incorporate include color motion figure ground dis crimination region shape descriptors and stereo depth cues the local feature approach can easily incorporate novel feature types because extra features contribute to robustness when they provide correct matches but otherwise do little harm other than their cost of compu tation therefore future systems are likely to combine many feature types detection of scale space extrema as described in the introduction we will detect key points using a cascade filtering approach that uses effi cient algorithms to identify candidate locations that are then examined in further detail the first stage of key point detection is to identify locations and scales that can be repeatably assigned under differing views of the same object detecting locations that are invariant to scale change of the image can be accomplished by searching for stable features across all possible scales using a continuous function of scale known as scale space witkin it has been shown by koenderink and lindeberg that under a variety of reasonable assumptions the only possible scale space kernel is the gaussian function therefore the scale space of an image is defined as a function l x y σ that is produced from the convolution of a variable scale gaussian g x y σ with an input image i x y there are a number of reasons for choosing this function first it is a particularly efficient function to compute as the smoothed images l need to be com puted in any case for scale space feature description and d can therefore be computed by simple image subtraction in addition the difference of gaussian function pro vides a close approximation to the scale normalized laplacian of gaussian σ g as studied by lindeberg lindeberg showed that the normal ization of the laplacian with the factor σ is required for true scale invariance in detailed experimental com parisons mikolajczyk found that the maxima and minima of σ g produce the most stable image features compared to a range of other possible image functions such as the gradient hessian or harris cor ner function the relationship between d and σ g can be un derstood from the heat diffusion equation parameter ized in terms of σ rather than the more usual t σ g σ g σ from this we see that g can be computed from the finite difference approximation to g σ using the difference of nearby scales at kσ and σ σ g g g x y kσ g x y σ σ σkσ and therefore g x y kσ g x y σ k σ g l x y σ g x y σ i x y this shows that when the difference of gaussian func where is the convolution operation in x and y tion has scales differing by a constant factor it already incorporates the σ scale normalization required for and the scale invariant laplacian the factor k in the equation is a constant over all scales and therefore does g x y σ e not influence extrema location the approximation er σ ror will go to zero as k goes to but in practice we have to efficiently detect stable keypoint locations in scale found that the approximation has almost no impact on the stability of extrema detection or localization for space we have proposed lowe using scale space extrema in the difference of gaussian function even significant differences in scale such as k convolved with the image d x y σ which can be an efficient approach to construction of d x y σ is shown in fig the initial image is incrementally computed from the difference of two nearby scales sep convolved with gaussians to produce images separated arated by a constant multiplicative factor k by a constant factor k in scale space shown stacked in d x y σ g x y kσ g x y σ i x y the left column we choose to divide each octave of scale space i e doubling of σ into an integer num l x y kσ l x y σ ber of intervals so k we must produce distinctive image features from scale invariant keypoints figure for each octave of scale space the initial image is repeatedly convolved with gaussians to produce the set of scale space images shown on the left adjacent gaussian images are subtracted to produce the difference of gaussian images on the right after each octave the gaussian image is down sampled by a factor of and the process repeated images in the stack of blurred images for each octave so that final extrema detection covers a complete oc tave adjacent image scales are subtracted to produce the difference of gaussian images shown on the right once a complete octave has been processed we resam ple the gaussian image that has twice the initial value of σ it will be images from the top of the stack by taking every second pixel in each row and column the accuracy of sampling relative to σ is no different than for the start of the previous octave while computation is greatly reduced figure maxima and minima of the difference of gaussian im ages are detected by comparing a pixel marked with x to its neighbors in regions at the current and adjacent scales marked with circles local extrema detection in order to detect the local maxima and minima of d x y σ each sample point is compared to its eight neighbors in the current image and nine neighbors in the scale above and below see fig it is selected only if it is larger than all of these neighbors or smaller than all of them the cost of this check is reasonably low due to the fact that most sample points will be eliminated following the first few checks an important issue is to determine the frequency of sampling in the image and scale domains that is needed to reliably detect the extrema unfortunately it turns out that there is no minimum spacing of sam ples that will detect all extrema as the extrema can be arbitrarily close together this can be seen by consid ering a white circle on a black background which will have a single scale space maximum where the circular positive central region of the difference of gaussian function matches the size and location of the circle for a very elongated ellipse there will be two max ima near each end of the ellipse as the locations of maxima are a continuous function of the image for some ellipse with intermediate elongation there will be a transition from a single maximum to two with the maxima arbitrarily close to each other near the transition lowe figure the top line of the first graph shows the percent of keypoints that are repeatably detected at the same location and scale in a transformed image as a function of the number of scales sampled per octave the lower line shows the percent of keypoints that have their descriptors correctly matched to a large database the second graph shows the total number of keypoints detected in a typical image as a function of the number of scale samples therefore we must settle for a solution that trades off efficiency with completeness in fact as might be expected and is confirmed by our experiments extrema that are close together are quite unstable to small per turbations of the image we can determine the best choices experimentally by studying a range of sampling frequencies and using those that provide the most reli able results under a realistic simulation of the matching task frequency of sampling in scale the experimental determination of sampling frequency that maximizes extrema stability is shown in figs and these figures and most other simulations in this pa per are based on a matching task using a collection of real images drawn from a diverse range including outdoor scenes human faces aerial photographs and industrial images the image domain was found to have almost no influence on any of the results each image was then subject to a range of transformations includ ing rotation scaling affine stretch change in brightness and contrast and addition of image noise because the changes were synthetic it was possible to precisely predict where each feature in an original image should appear in the transformed image allowing for measure ment of correct repeatability and positional accuracy for each feature figure shows these simulation results used to ex amine the effect of varying the number of scales per octave at which the image function is sampled prior to figure the top line in the graph shows the percent of keypoint locations that are repeatably detected in a transformed image as a function of the prior image smoothing for the first level of each octave the lower line shows the percent of descriptors correctly matched against a large database extrema detection in this case each image was resam pled following rotation by a random angle and scaling by a random amount between of times the orig inal size keypoints from the reduced resolution image were matched against those from the original image so that the scales for all keypoints would be be present in the matched image in addition image noise was added meaning that each pixel had a random number added from the uniform interval where pixel values are in the range equivalent to pro viding slightly less than bits of accuracy for image pixels distinctive image features from scale invariant keypoints the top line in the first graph of fig shows the percent of keypoints that are detected at a matching location and scale in the transformed image for all examples in this paper we define a matching scale as being within a factor of of the correct scale and a matching location as being within σ pixels where σ is the scale of the keypoint defined from eq as the standard deviation of the smallest gaussian used in the difference of gaussian function the lower line on this graph shows the number of keypoints that are correctly matched to a database of keypoints using the nearest neighbor matching procedure to be described in section this shows that once the key point is repeatably located it is likely to be useful for recognition and matching tasks as this graph shows the highest repeatability is obtained when sampling scales per octave and this is the number of scale samples used for all other experiments throughout this paper it might seem surprising that the repeatability does not continue to improve as more scales are sampled the reason is that this results in many more local ex trema being detected but these extrema are on average less stable and therefore are less likely to be detected in the transformed image this is shown by the second graph in fig which shows the average number of keypoints detected and correctly matched in each im age the number of keypoints rises with increased sam pling of scales and the total number of correct matches also rises since the success of object recognition often depends more on the quantity of correctly matched key points as opposed to their percentage correct match ing for many applications it will be optimal to use a larger number of scale samples however the cost of computation also rises with this number so for the ex periments in this paper we have chosen to use just scale samples per octave to summarize these experiments show that the scale space difference of gaussian function has a large number of extrema and that it would be very expensive to detect them all fortunately we can detect the most stable and useful subset even with a coarse sampling of scales frequency of sampling in the spatial domain just as we determined the frequency of sampling per oc tave of scale space so we must determine the frequency of sampling in the image domain relative to the scale of smoothing given that extrema can be arbitrarily close together there will be a similar trade off between sam pling frequency and rate of detection figure shows an experimental determination of the amount of prior smoothing σ that is applied to each image level before building the scale space representation for an octave again the top line is the repeatability of keypoint de tection and the results show that the repeatability con tinues to increase with σ however there is a cost to using a large σ in terms of efficiency so we have cho sen to use σ which provides close to optimal repeatability this value is used throughout this paper and was used for the results in fig of course if we pre smooth the image before ex trema detection we are effectively discarding the high est spatial frequencies therefore to make full use of the input the image can be expanded to create more sample points than were present in the original we double the size of the input image using linear inter polation prior to building the first level of the pyra mid while the equivalent operation could effectively have been performed by using sets of subpixel offset filters on the original image the image doubling leads to a more efficient implementation we assume that the original image has a blur of at least σ the min imum needed to prevent significant aliasing and that therefore the doubled image has σ relative to its new pixel spacing this means that little additional smoothing is needed prior to creation of the first oc tave of scale space the image doubling increases the number of stable keypoints by almost a factor of but no significant further improvements were found with a larger expansion factor accurate keypoint localization once a keypoint candidate has been found by com paring a pixel to its neighbors the next step is to per form a detailed fit to the nearby data for location scale and ratio of principal curvatures this information al lows points to be rejected that have low contrast and are therefore sensitive to noise or are poorly localized along an edge the initial implementation of this approach lowe simply located keypoints at the location and scale of the central sample point however recently brown has developed a method brown and lowe for fitting a quadratic function to the local sample points to determine the interpolated location of the maximum and his experiments showed that this provides a sub stantial improvement to matching and stability his lowe approach uses the taylor expansion up to the quadratic terms of the scale space function d x y σ shifted so that the origin is at the sample point d x d d t xt d x x x where d and its derivatives are evaluated at the sample point and x x y σ t is the offset from this point the location of the extremum xˆ is determined by tak ing the derivative of this function with respect to x and setting it to zero giving xˆ d d x as suggested by brown the hessian and derivative of d are approximated by using differences of neigh boring sample points the resulting linear system can be solved with minimal cost if the offset xˆ is larger than in any dimension then it means that the ex tremum lies closer to a different sample point in this case the sample point is changed and the interpolation performed instead about that point the final offset xˆ is added to the location of its sample point to get the interpolated estimate for the location of the extremum the function value at the extremum d xˆ is useful for rejecting unstable extrema with low contrast this can be obtained by substituting eqs into giving d xˆ d d t xˆ x for the experiments in this paper all extrema with a value of d xˆ less than were discarded as before we assume image pixel values in the range figure shows the effects of keypoint selection on a natural image in order to avoid too much clutter a low resolution by pixel image is used and key points are shown as vectors giving the location scale and orientation of each keypoint orientation assign ment is described below figure a shows the origi nal image which is shown at reduced contrast behind figure this figure shows the stages of keypoint selection a the pixel original image b the initial keypoints locations at maxima and minima of the difference of gaussian function keypoints are displayed as vectors indicating scale orientation and location c after applying a threshold on minimum contrast keypoints remain d the final keypoints that remain following an additional threshold on ratio of principal curvatures distinctive image features from scale invariant keypoints the subsequent figures figure b shows the one so that α rβ then keypoints at all detected maxima and minima of the tr h α β rβ β r difference of gaussian function while c shows the keypoints that remain following removal of those det h αβ rβ r with a value of d xˆ less than part d will be which depends only on the ratio of the eigenval explained in the following section ues rather than their individual values the quantity r r is at a minimum when the two eigenvalues eliminating edge responses are equal and it increases with r therefore to check that the ratio of principal curvatures is below some for stability it is not sufficient to reject keypoints with threshold r we only need to check low contrast the difference of gaussian function will tr h r have a strong response along edges even if the loca det h tion along the edge is poorly determined and therefore r unstable to small amounts of noise this is very efficient to compute with less than a poorly defined peak in the difference of gaussian floating point operations required to test each keypoint function will have a large principal curvature across the experiments in this paper use a value of r the edge but a small one in the perpendicular direction the principal curvatures can be computed from a which eliminates keypoints that have a ratio between the principal curvatures greater than the transi hessian matrix h computed at the location and scale tion from fig c to d shows the effects of this of the keypoint operation dx x dx y h dx y dyy orientation assignment the derivatives are estimated by taking differences of by assigning a consistent orientation to each keypoint based on local image properties the keypoint descrip neighboring sample points tor can be represented relative to this orientation and the eigenvalues of h are proportional to the princi therefore achieve invariance to image rotation this ap pal curvatures of d borrowing from the approach used proach contrasts with the orientation invariant descrip by harris and stephens we can avoid explicitly tors of schmid and mohr in which each image computing the eigenvalues as we are only concerned property is based on a rotationally invariant measure with their ratio let α be the eigenvalue with the largest the disadvantage of that approach is that it limits the magnitude and β be the smaller one then we can com descriptors that can be used and discards image infor pute the sum of the eigenvalues from the trace of h and mation by not requiring all measures to be based on a their product from the determinant consistent rotation following experimentation with a number of ap tr h dx x dyy α β proaches to assigning a local orientation the follow ing approach was found to give the most stable results det h dx x dyy dx y αβ the scale of the keypoint is used to select the gaussian smoothed image l with the closest scale so that all in the unlikely event that the determinant is negative computations are performed in a scale invariant man the curvatures have different signs so the point is dis ner for each image sample l x y at this scale the carded as not being an extremum let r be the ratio be gradient magnitude m x y and orientation θ x y tween the largest magnitude eigenvalue and the smaller is precomputed using pixel differences m x y y l x y l x y l x y θ x y x l x y l x y l x tan l x y y lowe an orientation histogram is formed from the gradient orientations of sample points within a region around the keypoint the orientation histogram has bins cov ering the degree range of orientations each sam ple added to the histogram is weighted by its gradient magnitude and by a gaussian weighted circular win dow with a σ that is times that of the scale of the keypoint peaks in the orientation histogram correspond to dominant directions of local gradients the highest peak in the histogram is detected and then any other lo cal peak that is within of the highest peak is used to also create a keypoint with that orientation therefore for locations with multiple peaks of similar magnitude there will be multiple keypoints created at the same lo cation and scale but different orientations only about of points are assigned multiple orientations but these contribute significantly to the stability of match ing finally a parabola is fit to the histogram values closest to each peak to interpolate the peak position for better accuracy figure shows the experimental stability of loca tion scale and orientation assignment under differ ing amounts of image noise as before the images are rotated and scaled by random amounts the top line shows the stability of keypoint location and scale as signment the second line shows the stability of match ing when the orientation assignment is also required to be within degrees as shown by the gap between the top two lines the orientation assignment remains accurate of the time even after addition of figure the top line in the graph shows the percent of keypoint locations and scales that are repeatably detected as a function of pixel noise the second line shows the repeatability after also requiring agreement in orientation the bottom line shows the final percent of descriptors correctly matched to a large database pixel noise equivalent to a camera providing less than bits of precision the measured variance of orienta tion for the correct matches is about degrees rising to degrees for noise the bottom line in fig shows the final accuracy of correctly matching a key point descriptor to a database of keypoints to be discussed below as this graph shows the sift fea tures are resistant to even large amounts of pixel noise and the major cause of error is the initial location and scale detection the local image descriptor the previous operations have assigned an image lo cation scale and orientation to each keypoint these parameters impose a repeatable local coordinate system in which to describe the local image region and therefore provide invariance to these parameters the next step is to compute a descriptor for the local image region that is highly distinctive yet is as invariant as possible to remaining variations such as change in illumination or viewpoint one obvious approach would be to sample the local image intensities around the keypoint at the appropriate scale and to match these using a normalized correla tion measure however simple correlation of image patches is highly sensitive to changes that cause mis registration of samples such as affine or viewpoint change or non rigid deformations a better approach has been demonstrated by edelman et al their proposed representation was based upon a model of biological vision in particular of complex neurons in primary visual cortex these complex neurons respond to a gradient at a particular orientation and spatial fre quency but the location of the gradient on the retina is allowed to shift over a small receptive field rather than being precisely localized edelman et al hypoth esized that the function of these complex neurons was to allow for matching and recognition of objects from a range of viewpoints they have performed de tailed experiments using computer models of object and animal shapes which show that matching gradi ents while allowing for shifts in their position results in much better classification under rotation for ex ample recognition accuracy for objects rotated in depth by degrees increased from for correlation of gradients to using the complex cell model our implementation described below was inspired by this idea but allows for positional shift using a different computational mechanism distinctive image features from scale invariant keypoints figure a keypoint descriptor is created by first computing the gradient magnitude and orientation at each image sample point in a region around the keypoint location as shown on the left these are weighted by a gaussian window indicated by the overlaid circle these samples are then accumulated into orientation histograms summarizing the contents over subregions as shown on the right with the length of each arrow corresponding to the sum of the gradient magnitudes near that direction within the region this figure shows a descriptor array computed from an set of samples whereas the experiments in this paper use descriptors computed from a sample array descriptor representation figure illustrates the computation of the keypoint de scriptor first the image gradient magnitudes and ori entations are sampled around the keypoint location using the scale of the keypoint to select the level of gaussian blur for the image in order to achieve ori entation invariance the coordinates of the descriptor and the gradient orientations are rotated relative to the keypoint orientation for efficiency the gradients are precomputed for all levels of the pyramid as de scribed in section these are illustrated with small arrows at each sample location on the left side of fig a gaussian weighting function with σ equal to one half the width of the descriptor window is used to as sign a weight to the magnitude of each sample point this is illustrated with a circular window on the left side of fig although of course the weight falls off smoothly the purpose of this gaussian window is to avoid sudden changes in the descriptor with small changes in the position of the window and to give less emphasis to gradients that are far from the center of the descriptor as these are most affected by misregistration errors the keypoint descriptor is shown on the right side of fig it allows for significant shift in gradient po sitions by creating orientation histograms over sample regions the figure shows eight directions for each orientation histogram with the length of each ar row corresponding to the magnitude of that histogram entry a gradient sample on the left can shift up to sample positions while still contributing to the same histogram on the right thereby achieving the objective of allowing for larger local positional shifts it is important to avoid all boundary affects in which the descriptor abruptly changes as a sample shifts smoothly from being within one histogram to another or from one orientation to another therefore trilin ear interpolation is used to distribute the value of each gradient sample into adjacent histogram bins in other words each entry into a bin is multiplied by a weight of d for each dimension where d is the distance of the sample from the central value of the bin as measured in units of the histogram bin spacing the descriptor is formed from a vector containing the values of all the orientation histogram entries cor responding to the lengths of the arrows on the right side of fig the figure shows a array of orienta tion histograms whereas our experiments below show that the best results are achieved with a array of histograms with orientation bins in each therefore the experiments in this paper use a element feature vector for each keypoint finally the feature vector is modified to reduce the effects of illumination change first the vector is nor malized to unit length a change in image contrast in which each pixel value is multiplied by a constant will multiply gradients by the same constant so this contrast change will be canceled by vector normalization a brightness change in which a constant is added to each image pixel will not affect the gradient values as they are computed from pixel differences therefore the de scriptor is invariant to affine changes in illumination however non linear illumination changes can also oc cur due to camera saturation or due to illumination lowe changes that affect surfaces with differing orienta tions by different amounts these effects can cause a large change in relative magnitudes for some gradients but are less likely to affect the gradient orientations therefore we reduce the influence of large gradient magnitudes by thresholding the values in the unit fea ture vector to each be no larger than and then renor malizing to unit length this means that matching the magnitudes for large gradients is no longer as impor tant and that the distribution of orientations has greater emphasis the value of was determined experimen tally using images containing differing illuminations for the same objects descriptor testing there are two parameters that can be used to vary the complexity of the descriptor the number of orienta tions r in the histograms and the width n of the n n array of orientation histograms the size of the resulting descriptor vector is as the complexity of the descriptor grows it will be able to discriminate bet ter in a large database but it will also be more sensitive to shape distortions and occlusion figure shows experimental results in which the number of orientations and size of the descriptor were varied the graph was generated for a viewpoint trans formation in which a planar surface is tilted by de grees away from the viewer and image noise is added this is near the limits of reliable matching as it is in these more difficult cases that descriptor perfor mance is most important the results show the percent of keypoints that find a correct match to the single clos est neighbor among a database of keypoints the graph shows that a single orientation histogram n is very poor at discriminating but the results continue to improve up to a array of histograms with orientations after that adding more orienta tions or a larger descriptor can actually hurt matching by making the descriptor more sensitive to distortion these results were broadly similar for other degrees of viewpoint change and noise although in some sim pler cases discrimination continued to improve from already high levels with and higher descriptor sizes throughout this paper we use a descriptor with orientations resulting in feature vectors with dimensions while the dimensionality of the descriptor may seem high we have found that it consistently per forms better than lower dimensional descriptors on a range of matching tasks and that the computational cost of matching remains low when using the approximate nearest neighbor methods described below sensitivity to affine change the sensitivity of the descriptor to affine change is ex amined in fig the graph shows the reliability of keypoint location and scale selection orientation as signment and nearest neighbor matching to a database as a function of rotation in depth of a plane away from figure this graph shows the percent of keypoints giving the correct match to a database of keypoints as a function of width of the n n keypoint descriptor and the number of orientations in each histogram the graph is computed for images with affine viewpoint change of degrees and addition of noise figure this graph shows the stability of detection for keypoint location orientation and final matching to a database as a function of affine distortion the degree of affine distortion is expressed in terms of the equivalent viewpoint rotation in depth for a planar surface distinctive image features from scale invariant keypoints a viewer it can be seen that each stage of computation has reduced repeatability with increasing affine distor tion but that the final matching accuracy remains above out to a degree change in viewpoint to achieve reliable matching over a wider viewpoint angle one of the affine invariant detectors could be used to select and resample image regions as discussed in section as mentioned there none of these ap proaches is truly affine invariant as they all start from initial feature locations determined in a non affine invariant manner in what appears to be the most affine invariant method mikolajczyk has proposed and run detailed experiments with the harris affine de tector he found that its keypoint repeatability is below that given here out to about a degree viewpoint an gle but that it then retains close to repeatability out to an angle of degrees which provides better performance for extreme affine changes the disadvan tages are a much higher computational cost a reduc tion in the number of keypoints and poorer stability for small affine changes due to errors in assigning a consis tent affine frame under noise in practice the allowable range of rotation for objects is considerably less than for planar surfaces so affine invariance is usually not the limiting factor in the ability to match across viewpoint change if a wide range of affine invariance is desired such as for a surface that is known to be pla nar then a simple solution is to adopt the approach of pritchard and heidrich in which additional sift features are generated from affine transformed ver sions of the training image corresponding to degree viewpoint changes this allows for the use of standard sift features with no additional cost when process ing the image to be recognized but results in an in crease in the size of the feature database by a factor of matching to large databases an important remaining issue for measuring the dis tinctiveness of features is how the reliability of match ing varies as a function of the number of features in the database being matched most of the examples in this paper are generated using a database of im ages with about keypoints figure shows how the matching reliability varies as a function of database size this figure was generated using a larger database of images with a viewpoint depth rota tion of degrees and image noise in addition to the usual random image rotation and scale change figure the dashed line shows the percent of keypoints cor rectly matched to a database as a function of database size using a logarithmic scale the solid line shows the percent of keypoints assigned the correct location scale and orientation images had ran dom scale and rotation changes an affine transform of degrees and image noise of added prior to matching the dashed line shows the portion of image features for which the nearest neighbor in the database was the correct match as a function of database size shown on a logarithmic scale the leftmost point is matching against features from only a single image while the rightmost point is selecting matches from a database of all features from the images it can be seen that matching reliability does decrease as a function of the number of distractors yet all indications are that many correct matches will continue to be found out to very large database sizes the solid line is the percentage of keypoints that were identified at the correct matching location and orientation in the transformed image so it is only these points that have any chance of having matching de scriptors in the database the reason this line is flat is that the test was run over the full database for each value while only varying the portion of the database used for distractors it is of interest that the gap be tween the two lines is small indicating that matching failures are due more to issues with initial feature lo calization and orientation assignment than to problems with feature distinctiveness even out to large database sizes application to object recognition the major topic of this paper is the derivation of dis tinctive invariant keypoints as described above to lowe demonstrate their application we will now give a brief description of their use for object recognition in the presence of clutter and occlusion more details on ap plications of these features to recognition are available in other papers lowe se et al object recognition is performed by first matching each keypoint independently to the database of key points extracted from training images many of these initial matches will be incorrect due to ambiguous fea tures or features that arise from background clutter therefore clusters of at least features are first identi fied that agree on an object and its pose as these clusters have a much higher probability of being correct than in dividual feature matches then each cluster is checked by performing a detailed geometric fit to the model and the result is used to accept or reject the interpretation keypoint matching the best candidate match for each keypoint is found by identifying its nearest neighbor in the database of keypoints from training images the nearest neighbor is defined as the keypoint with minimum euclidean distance for the invariant descriptor vector as was de scribed in section however many features from an image will not have any correct match in the training database because they arise from background clutter or were not detected in the training images therefore it would be useful to have a way to discard features that do not have any good match to the database a global threshold on dis tance to the closest feature does not perform well as some descriptors are much more discriminative than others a more effective measure is obtained by com paring the distance of the closest neighbor to that of the second closest neighbor if there are multiple training images of the same object then we define the second closest neighbor as being the closest neighbor that is known to come from a different object than the first such as by only using images known to contain different objects this measure performs well because correct matches need to have the closest neighbor significantly closer than the closest incorrect match to achieve reli able matching for false matches there will likely be a number of other false matches within similar distances due to the high dimensionality of the feature space we can think of the second closest match as provid ing an estimate of the density of false matches within this portion of the feature space and at the same time identifying specific instances of feature ambiguity figure the probability that a match is correct can be deter mined by taking the ratio of distance from the closest neighbor to the distance of the second closest using a database of keypoints the solid line shows the pdf of this ratio for correct matches while the dotted line is for matches that were incorrect figure shows the value of this measure for real image data the probability density functions for cor rect and incorrect matches are shown in terms of the ratio of closest to second closest neighbors of each key point matches for which the nearest neighbor was a correct match have a pdf that is centered at a much lower ratio than that for incorrect matches for our ob ject recognition implementation we reject all matches in which the distance ratio is greater than which eliminates of the false matches while discarding less than of the correct matches this figure was generated by matching images following random scale and orientation change a depth rotation of degrees and addition of image noise against a database of keypoints efficient nearest neighbor indexing no algorithms are known that can identify the exact nearest neighbors of points in high dimensional spaces that are any more efficient than exhaustive search our keypoint descriptor has a dimensional feature vector and the best algorithms such as the k d tree friedman et al provide no speedup over ex haustive search for more than about dimensional spaces therefore we have used an approximate algo rithm called the best bin first bbf algorithm beis and lowe this is approximate in the sense that it returns the closest neighbor with high probability distinctive image features from scale invariant keypoints the bbf algorithm uses a modified search ordering for the k d tree algorithm so that bins in feature space are searched in the order of their closest distance from the query location this priority search order was first examined by arya and mount and they pro vide further study of its computational properties in arya et al this search order requires the use of a heap based priority queue for efficient determina tion of the search order an approximate answer can be returned with low cost by cutting off further search after a specific number of the nearest bins have been explored in our implementation we cut off search af ter checking the first nearest neighbor candidates for a database of keypoints this provides a speedup over exact nearest neighbor search by about orders of magnitude yet results in less than a loss in the number of correct matches one reason the bbf algorithm works particularly well for this prob lem is that we only consider matches in which the near est neighbor is less than times the distance to the second nearest neighbor as described in the previous section and therefore there is no need to exactly solve the most difficult cases in which many neighbors are at very similar distances clustering with the hough transform to maximize the performance of object recognition for small or highly occluded objects we wish to iden tify objects with the fewest possible number of fea ture matches we have found that reliable recognition is possible with as few as features a typical im age contains or more features which may come from many different objects as well as background clut ter while the distance ratio test described in section will allow us to discard many of the false matches arising from background clutter this does not remove matches from other valid objects and we often still need to identify correct subsets of matches contain ing less than inliers among outliers many well known robust fitting methods such as ransac or least median of squares perform poorly when the percent of inliers falls much below fortunately much better performance can be obtained by cluster ing features in pose space using the hough transform hough ballard grimson the hough transform identifies clusters of features with a consistent interpretation by using each feature to vote for all object poses that are consistent with the fea ture when clusters of features are found to vote for the same pose of an object the probability of the interpre tation being correct is much higher than for any single feature each of our keypoints specifies parameters location scale and orientation and each matched keypoint in the database has a record of the keypoint parameters relative to the training image in which it was found therefore we can create a hough trans form entry predicting the model location orientation and scale from the match hypothesis this prediction has large error bounds as the similarity transform im plied by these parameters is only an approximation to the full degree of freedom pose space for a object and also does not account for any non rigid deforma tions therefore we use broad bin sizes of degrees for orientation a factor of for scale and times the maximum projected training image dimension using the predicted scale for location to avoid the problem of boundary effects in bin assignment each keypoint match votes for the closest bins in each dimension giving a total of entries for each hypothesis and fur ther broadening the pose range in most implementations of the hough transform a multi dimensional array is used to represent the bins however many of the potential bins will remain empty and it is difficult to compute the range of possible bin values due to their mutual dependence for example the dependency of location discretization on the se lected scale these problems can be avoided by using a pseudo random hash function of the bin values to in sert votes into a one dimensional hash table in which collisions are easily detected solution for affine parameters the hough transform is used to identify all clusters with at least entries in a bin each such cluster is then subject to a geometric verification procedure in which a least squares solution is performed for the best affine projection parameters relating the training image to the new image an affine transformation correctly accounts for rotation of a planar surface under orthographic projec tion but the approximation can be poor for rotation of non planar objects a more general solution would be to solve for the fundamental matrix luong and faugeras hartley and zisserman how ever a fundamental matrix solution requires at least point matches as compared to only for the affine so lution and in practice requires even more matches for good stability we would like to perform recognition lowe with as few as feature matches so the affine solution provides a better starting point and we can account for errors in the affine approximation by allowing for large residual errors if we imagine placing a sphere around an object then rotation of the sphere by de grees will move no point within the sphere by more than times the projected diameter of the sphere for the examples of typical objects used in this pa per an affine solution works well given that we allow residual errors up to times the maximum projected dimension of the object a more general approach is given in brown and lowe in which the initial solution is based on a similarity transform which then progresses to solution for the fundamental matrix in those cases in which a sufficient number of matches are found the affine transformation of a model point x y t to an image point u v t can be written as u x tx v y ty where the model translation is tx ty t and the affine rotation scale and stretch are represented by the mi parameters we wish to solve for the transformation parameters so the equation above can be rewritten to gather the unknowns into a column vector x y u x y v tx ty this equation shows a single match but any number of further matches can be added with each match con tributing two more rows to the first and last matrix at least matches are needed to provide a solution we can write this linear system as ax b the least squares solution for the parameters x can be determined by solving the corresponding normal equations x ata which minimizes the sum of the squares of the distances from the projected model locations to the correspond ing image locations this least squares approach could readily be extended to solving for pose and internal parameters of articulated and flexible objects lowe outliers can now be removed by checking for agree ment between each image feature and the model given the more accurate least squares solution we now re quire each match to agree within half the error range that was used for the parameters in the hough trans form bins if fewer than points remain after discarding outliers then the match is rejected as outliers are dis carded the least squares solution is re solved with the remaining points and the process iterated in addition a top down matching phase is used to add any further matches that agree with the projected model position these may have been missed from the hough trans form bin due to the similarity transform approximation or other errors the final decision to accept or reject a model hypoth esis is based on a detailed probabilistic model given in a previous paper lowe this method first computes the expected number of false matches to the model pose given the projected size of the model the number of features within the region and the accuracy of the fit a bayesian analysis then gives the probabil ity that the object is present based on the actual number of matching features found we accept a model if the final probability for a correct interpretation is greater than for objects that project to small regions of an image features may be sufficient for reliable recog nition for large objects covering most of a heavily textured image the expected number of false matches is higher and as many as feature matches may be necessary recognition examples figure shows an example of object recognition for a cluttered and occluded image containing objects the training images of a toy train and a frog are shown on the left the middle image of size pix els contains instances of these objects hidden behind others and with extensive background clutter so that detection of the objects may not be immediate even for human vision the image on the right shows the final correct identification superimposed on a reduced con trast version of the image the keypoints that were used for recognition are shown as squares with an extra line distinctive image features from scale invariant keypoints figure the training images for two objects are shown on the left these can be recognized in a cluttered image with extensive occlusion shown in the middle the results of recognition are shown on the right a parallelogram is drawn around each recognized object showing the boundaries of the original training image under the affine transformation solved for during recognition smaller squares indicate the keypoints that were used for recognition figure this example shows location recognition within a complex scene the training images for locations are shown at the upper left and the pixel test image taken from a different viewpoint is on the upper right the recognized regions are shown on the lower image with keypoints shown as squares and an outer parallelogram showing the boundaries of the training images under the affine transform used for recognition lowe to indicate orientation the sizes of the squares corre spond to the image regions used to construct the de scriptor an outer parallelogram is also drawn around each instance of recognition with its sides correspond ing to the boundaries of the training images projected under the final affine transformation determined during recognition another potential application of the approach is to place recognition in which a mobile device or vehicle could identify its location by recognizing familiar lo cations figure gives an example of this application in which training images are taken of a number of loca tions as shown on the upper left these can even be of such seemingly non distinctive items as a wooden wall or a tree with trash bins the test image of size by pixels on the upper right was taken from a view point rotated about degrees around the scene from the original positions yet the training image locations are easily recognized all steps of the recognition process can be imple mented efficiently so the total time to recognize all objects in figs or is less than seconds on a ghz pentium processor we have implemented these algorithms on a laptop computer with attached video camera and have tested them extensively over a wide range of conditions in general textured planar surfaces can be identified reliably over a rotation in depth of up to degrees in any direction and under almost any illumination conditions that provide suf ficient light and do not produce excessive glare for objects the range of rotation in depth for reliable recognition is only about degrees in any direction and illumination change is more disruptive for these reasons object recognition is best performed by integrating features from multiple views such as with local feature view clustering lowe these keypoints have also been applied to the prob lem of robot localization and mapping which has been presented in detail in other papers se et al in this application a trinocular stereo system is used to de termine estimates for keypoint locations keypoints are used only when they appear in all images with con sistent disparities resulting in very few outliers as the robot moves it localizes itself using feature matches to the existing map and then incrementally adds features to the map while updating their positions using a kalman filter this provides a robust and ac curate solution to the problem of robot localization in unknown environments this work has also addressed the problem of place recognition in which a robot can be switched on and recognize its location anywhere within a large map se et al which is equiva lent to a implementation of object recognition conclusions the sift keypoints described in this paper are par ticularly useful due to their distinctiveness which en ables the correct match for a keypoint to be selected from a large database of other keypoints this distinc tiveness is achieved by assembling a high dimensional vector representing the image gradients within a local region of the image the keypoints have been shown to be invariant to image rotation and scale and robust across a substantial range of affine distortion addition of noise and change in illumination large numbers of keypoints can be extracted from typical images which leads to robustness in extracting small objects among clutter the fact that keypoints are detected over a com plete range of scales means that small local features are available for matching small and highly occluded ob jects while large keypoints perform well for images subject to noise and blur their computation is effi cient so that several thousand keypoints can be ex tracted from a typical image with near real time per formance on standard pc hardware this paper has also presented methods for using the keypoints for object recognition the approach we have described uses approximate nearest neighbor lookup a hough transform for identifying clusters that agree on object pose least squares pose determination and final verification other potential applications include view matching for reconstruction motion tracking and segmentation robot localization image panorama as sembly epipolar calibration and any others that require identification of matching locations between images there are many directions for further research in deriving invariant and distinctive image features sys tematic testing is needed on data sets with full view point and illumination changes the features described in this paper use only a monochrome intensity image so further distinctiveness could be derived from includ ing illumination invariant color descriptors funt and finlayson brown and lowe similarly local texture measures appear to play an important role in human vision and could be incorporated into fea ture descriptors in a more general form than the single spatial frequency used by the current descriptors an attractive aspect of the invariant local feature approach distinctive image features from scale invariant keypoints to matching is that there is no need to select just one feature type and the best results are likely to be ob tained by using many different features all of which can contribute useful matches and improve overall robustness another direction for future research will be to in dividually learn features that are suited to recognizing particular objects categories this will be particularly important for generic object classes that must cover a broad range of possible appearances the research of weber et al and fergus et al has shown the potential of this approach by learning small sets of local features that are suited to recognizing generic classes of objects in the long term feature sets are likely to contain both prior and learned features that will be used according to the amount of training data that has been available for various object classes abstract clustal x is a new windows interface for the widely used progressive multiple sequence alignment program clustal w the new system is easy to use providing an integrated system for performing multiple sequence and profile alignments and analysing the results clustal x displays the sequence alignment in a window on the screen a versatile sequence colouring scheme allows the user to highlight conserved features in the alignment pull down menus provide all the options required for traditional multiple sequence and profile alignment new features include the ability to cut and paste sequences to change the order of the alignment selection of a subset of the sequences to be realigned and selection of a sub range of the alignment to be realigned and inserted back into the original alignment alignment quality analysis can be performed and low scoring segments or exceptional residues can be highlighted quality analysis and realignment of selected residue ranges provide the user with a powerful tool to improve and refine difficult alignments and to trap errors in input sequences clustal x has been compiled on sun solaris on silicon graphics digital unix on decstations microsoft windows bit for pcs linux elf for pcs and macintosh powermac introduction the most widely used method in molecular biology to align sets of nucleotide or amino acid sequences is to build up a multiple alignment progressively the most closely related groups of sequences are aligned first and then these groups are gradually aligned together keeping the early alignments fixed this approach works well when the sequences are sufficiently closely related however a globally optimal solution or a biologically significant one cannot be guaranteed in more difficult cases where many sequences have residue identity this automatic method becomes less reliable any misaligned regions introduced in previous stages of the progressive alignment are not corrected later as new information from other sequences is added in such cases the automatic alignments need to be refined either manually or automatically numerous sequence editors have been developed which allow the user to display and manually make or modify an alignment eg these programs are useful for making small refinements to an alignment but the totally manual alignment of large numbers of sequences is not feasible manual alignment is also highly subjective hence is at least as likely as the automatic alignment process to result in errors in the alignment the clustal x interface has been written to provide a single environment in which the user can perform multiple alignments view the results and if necessary refine and improve the alignment tools for alignment quality analysis have been developed which allow the user to highlight low scoring regions in the alignment options are available for automatically correcting these low scoring regions by realigning a misaligned sequence or a selected region of an alignment in earlier clustal programs nested text menus provided all the options to do multiple sequence profile alignments and simple phylogenetic tree generation the output alignments were written to file for display printing or further manipulation with these simple menus the clustal programs could be highly portable and run on essentially all computers portability has been a major factor in the widespread usage of the clustal series for sequence alignment on the other hand much more attractive and powerful user interfaces can be built using non portable windows systems the ncbi software development toolkit version national center for biotechnology information bethesda md provides one solution to the windows portability problem it interfaces between the application code and various host windowing systems including the x window system macintosh windows and microsoft windows we have made use of the toolkit to provide a portable windows interface termed clustal x to whom correspondence should be addressed tel fax email jeanmougin igbmc u strasbg fr clustal x is a new graphical interface to the clustal w program which displays the sequence alignment in a window on the screen allowing the user to move easily between different parts of the alignment pull down menus provide all the options familiar to users of the text menu driven clustal w plus several new features a versatile configurable colouring system is used to highlight conserved residue features in the alignment options to mark suspect regions and realign selected residue ranges give the user more information and control over the alignment process and allow difficult alignments to be gradually built and refined materials and methods we make use of the ncbi vibrant virtual interface for biological research and technology development library which acts as an interface between the application code of clustal x and the host windowing system the ncbi libraries are linked to the clustal x code providing mechanisms for displaying windows menus buttons etc in this way the clustal x code remains independent of the underlying operating system and computer the clustal x code is written in ansi c and should be portable to any machine capable of supporting the ncbi vibrant toolkit clustal x is available for a number of platforms including sun solaris irix on silicon graphics digital unix on decstations microsoft windows 32 bit for pcs linux elf for pcs and macintosh powermac the source code is provided for anyone wishing to port to any other platform supported by the vibrant project the source code for clustal x and several executable versions for different machines are freely available by anonymous ftp to ftp igbmc u strasbg fr hypertext documentation can be viewed at www igbmc u strasbg fr bioinfo clustalx the ncbi vibrant toolkit is available by anonymous ftp from ncbi nlm nih gov installation the clustal x program is easily installed by copying the executable file to a system directory which can be seen by all users several parameter files named par and an on line help text file clustalx hlp for ms windows otherwise are also required by clustal x these files should be copied to one of the following directories i the user current directory ii the user home directory iii any of the directories specified by the path environment variable results algorithms checking alignment quality three methods of alignment quality analysis are implemented i a quality score is calculated for each column in the alignment which depends on the amino acid variability in the column a high score indicates a highly conserved column a low score indicating a less well conserved position the scores are automatically plotted in the window display fig ii the residues which get exceptionally low scores in the above quality calculation can be highlighted in the alignment display fig iii low scoring segments in each sequence of the alignment can be highlighted these are found by summing negative scores in the profile built from the sequence alignment figs and nucleic acids research vol no highlighted residues may be expected to occur at a moderate frequency in all the sequences because of their steady divergence due to the natural processes of evolution although the most divergent sequences are likely to have the most outliers however the highlighted residues are especially useful in pointing to sequence misalignments these can arise due to various reasons i partial or total misalignments caused by a failure in the alignment algorithm ii partial or total misalignments because at least one of the sequences in the given set is partly or completely unrelated to the other sequences iii frameshift translation errors in a protein sequence causing local mismatched regions to be heavily highlighted see discussion for more details occasionally highlighted residues may point to regions of some biological significance this might happen for example if a protein alignment contains a sequence which has acquired new functions relative to the main sequence set quality scores suppose we have an alignment of m sequences of length n then the alignment can be written as n n am am am am n suppose we also define a residue comparison matrix c of size r r where r is the number of residues c a b is the score for aligning residue a with residue b all scores in this matrix are positive the problem is to calculate a score for the conservation of the jth position in the alignment vingron and sibbald used a geometric analysis based on a continuous sequence space in order to compare sequence weighting methods the method defines an n dimensional space where n is the length of the alignment each sequence can be placed in the space and the distance between two sequences is defined as the euclidean distance between the sequences in the space we have applied an analogous approach to each position in the alignment an r dimensional space is defined in which each column of the alignment can be considered for a specified position j in the alignment each sequence consists of a single residue which is assigned a point s in the space for sequence i position j the point s is defined as c aij c a s ij c r aij we then calculate a consensus value x for the jth position in the alignment x is defined as r fi j c i i r m fi j c i xi m r fi j c i r i m nucleic acids research vol no figure the clustal x window in multiple alignment mode an alignment of some eftu proteins is displayed low scoring segments are highlighted using a white character on a black background exceptional residues are shown as a white character on a grey background the quality analysis reveals two anomalously low scoring regions ruler positions in and in these were found to be caused by frameshift errors two more sequences and not shown here have residue sequencing errors in this region which clustal x will also highlight where fi j is the count of residues i at position j in the alignment now if s is the position of sequence i in the r dimensional space we can calculate the distance di between each sequence residue i and the consensus position x di r xr sr r where xr is the rth dimension of position x and sr is the rth dimension of position s we define the quality score for the jth position in the alignment as the mean of the sequence distances di m di quality score i finally the scores are normalised by multiplying by the percen tage of sequences which have residues and not gaps at this position these scores are used in measure i above as an estimate of the conservation of each alignment column fig exceptional residues it would be useful for each column in the alignment to identify those sequences in the above calculations which are found a long way from the consensus point i e which have a large distance di thus lowering the quality score for the column for the jth position in the alignment we take the set of sequences which have a residue at this position and not a gap the distances di for this set of sequences are arranged in an array in order of size from smallest to largest we can find the upper and lower quartiles the distances lying one quarter of the way from the top and bottom of the array respectively and the inter quartile range the difference between the two quartiles a residue aij is considered as an exception in measure ii above if the sequence distance di is greater than upper quartile inter quartile range scaling factor the scaling factor can be adjusted by the user to select the proportion of residue exceptions that will be highlighted in the alignment display exceptional residues in an eftu alignment are shown in figure this calculation runs automatically in a very short time each time the screen is updated low scoring segments given the above alignment of m sequences of length n and a residue exchange matrix we can build a profile which is weighted for sequence divergence methods for calculat ing sequence weights are discussed by henikoff and henikoff here we calculate sequence weights directly from a neighbour joining tree using the branch proportional method which corrects for unequal representation by downweighting similar sequences and upweighting divergent ones each sequence is assigned a weight wi in the residue comparison matrix c the scores for common residue substitutions are positive while rarer substitutions are scored negatively by default the gonnet pam matrix is used but the user may supply a different matrix e g a lower pam value is appropriate if the sequences are closely related the profile p has a column of scores for each position in the alignment the column is of height r and consists of a score nucleic acids research vol no figure detection and correction of misaligned segments with clustal x a a set of eftus tested for low scoring regions highlights a part of the sequence which we deliberately misaligned by incorrect gap insertion the range selected to be realigned is marked above the alignment b after removal of gaps and realignment of the selected residue range the sequence is now correctly aligned and the erroneous gaps have been removed the low scoring segment check the column conservation indicators above the alignment and the quality graph below it all reflect the improvement in the alignment for each residue in the matrix c the profile score for residue r at position j in the alignment is defined as c r aij wim p r j i m wi i for the jth position in the ith sequence the score sij is defined as sij p aij j the low scoring regions in the ith sequence are found by summing the scores sij along the alignment in both the forward and backward directions if the sum is found to be positive it is reset to zero the forward phase can be described by the following recurrence relations fj sij if fj sij t fj if fj sijh w if j having found the regions in the sequence which have negative fj scores these regions are then refined by removing those positions at the end of each segment which have a positive profile score sij the fj scores for these positions are reset to zero similarly the backward phase can be described as bj sij if bj sij t bj if bj sij w if j n the regions in the sequence which have negative bj scores are again refined by removing those positions at the beginning of each segment which have a positive profile score sij the bj scores for these positions are reset to zero nucleic acids research vol no the calculation is repeated for each sequence compared with a profile for all aligned sequences except itself the low scoring segments defined as those positions for which both fj and bj are negative are then highlighted in the display figs and the low scoring segment calculation is done when the user selects the calculate low scoring segments option it takes only a few seconds to perform unless the alignment is very large making it a practical tool for interactive use implementation clustal x displays a window on the screen including a set of pull down menus on line help is available the exact format of the screen will depend on the host computer and the operating system the user may select one of two modes i multiple alignment mode which displays a single display area for multiple sequence alignment or ii profile alignment mode which has two display areas allowing the user to use previously aligned sequences for alignment figure shows a clustal x window in multiple alignment mode alignments or individual sequences can be loaded into the display areas displayed on the screen using the menu options scroll bars allow easy movement to different parts of the alignment extra lines are added to the sequence data displaying a ruler an indicator of alignment conservation plus any secondary structure data which was found in the input alignment file the order of the sequences in the display can be changed by clicking on the sequence names and selecting the cut and paste options from the menus in profile alignment mode these options also allow the user to move sequences from one profile to the other the sequences can be saved at any time to an alignment file in one of a number of file formats the sequence display can also be saved in a colour postscript file for printing on a postscript printer colouring the alignment display the sequences are automatically coloured to highlight conserved regions of the alignment the colours used and the specification of the conservation of residues can be configured by the user the rules governing the colouring of residues are read from a colour parameter file which can be loaded at any time two types of colouring rules are defined i a residue can be assigned a specific colour regardless of its position in the alignment in this case all occurrences of the residue will be coloured in the alignment display ii a residue can be assigned different colours depending on the consensus of the alignment at each position in this way for example conserved hydrophobic or hydrophilic positions in the alignment can be highlighted fig realigning divergent regions in difficult cases with a family of highly divergent sequences it is possible that misalignments are introduced during the multiple alignment process clustal x provides two simple mechanisms for realigning the most divergent regions i misaligned sequences may be selected by clicking on the sequence names a single menu option then removes these sequences from the alignment set and realigns them to the remaining sequences ii the second option allows the user to specify a range of the alignment to be realigned in this case the selected sub range of the alignment is removed and multiply aligned using the standard progressive multiple sequence alignment method the sub range is then fitted back into the full alignment using these two options the original multiple sequence alignment may be iteratively improved and refined discussion ideally methods for multiple sequence alignment should guarantee to find the biologically correct alignment for a set of sequences in practice this is difficult to achieve firstly it is difficult to define an optimal alignment between divergent nucleotide or protein sequences even given tertiary structural information secondly methods that find an optimal multiple sequence alignment have been impractical to implement mainly due to their computational cost as computer performance improves methods which iterate toward an optimal alignment are likely to become useful meanwhile the heuristical approach of progressive alignment is most often used as the algorithm is reasonably fast and minimises error in alignments of moderate difficulty however because the full information in the sequence set is not used to align each sequence it can be possible to see one or more misaligned sequence segments in the output alignment in such cases the sequences would be expected to align correctly if the full information was used or if alignment parameters such as gap penalties were adjusted when we developed clustal w we gave the user the ability to iterate the alignment process by realigning an alignment or by profile aligning sequences to an alignment in this way the user could choose to iterate the alignment process thereby overcoming some of the defects of progressive alignment with clustal x we have taken this capability further by building in algorithms to target the problem regions of an alignment and letting the user realign solely the suspect residue ranges using these tools high quality alignments of divergent sequence sets are produced more quickly and with greater confidence than has previously been possible by progressive alignment many programs have been developed which allow to a greater or lesser degree manual intervention in the automatic alignment process for example somap was designed to run under the dec vms operating system the program allows the user to manually build up a multiple sequence alignment it can accept automatic alignments created by the original clustal program to provide a starting point for the manual editing process seaview is a unix x window based multiple sequence alignment editor which is interfaced to the clustal w program seqpup don gilbert biology department indiana university bloomington in is a sequence editor and analysis program which can launch external applications such as clustal w to perform sequence alignment seqlab wisconsin package version genetics computer group gcg madison wi is a graphical user interface based on the osf motif windowing system it displays sequence alignments on the screen and includes powerful sequence editing facilities the pileup program is interfaced to the seqlab editor to perform automatic multiple sequence alignments numerous mac and pc alignment editors have also been developed most of these editors will accept alignment output from clustal programs however using clustal x the amount of time spent editing alignments by hand should be minimised while a hand edited alignment can itself be returned for error checking clustal x is not confined to either vms or unix work stations but also runs on macintosh and pc computers the program provides a flexible approach to the problem of the multiple alignment of large numbers of sequences the methods used can be applied equally well to both nucleotide and amino acid sequences an initial automatic alignment using the traditional progressive pairwise approach provides a good starting point for further refinement the alignments are displayed on the screen and the user can move around easily between different parts of the alignment a versatile residue colouring scheme based on the conservation of each position in the alignment automatically highlights conserved or special features alignment analysis and error detection tools for alignment quality analysis have been developed and incorporated into the package a quality estimate for each position in the alignment is plotted on the screen fig highly conserved positions in the alignment will get a high quality score whilst either low conservation or exceptional residues at a partially conserved position will lower the score for the column the exceptional residues which may be due to misalignment of the sequences or simply divergence can be highlighted in the alignment fig sometimes these may be of biological interest although most divergence is due to neutral evolutionary processes several methods for calculating the conservation of an alignment column have been developed zvelebil et al used physico chemical properties of amino acids to quantify the conservation of a position in an alignment in order to predict protein secondary structure smith and smith define the information density of a sub region assuming that all amino acids are informationally equivalent sander and schneider calculate a variation entropy for each column brouillet et al calculate the mean and standard deviation of the pairwise distances between amino acids in each column of an alignment using a distance matrix none of these methods were found to be ideal for incorporation into clustal x apart from the latter none of the methods use a standard residue exchange matrix as is needed for consistency with the alignment process as well as providing a natural way to allow the user to customise the quality analysis by varying the matrix the advantage of the geometric interpretion developed for clustal x is that statistical methods can then be applied to define a mean value for the column and distances can be measured between each sequence and the mean upper limits for the expected distance between any residue and the mean value can be defined and thus exceptional residues can be identified low scoring segments in the sequences can also be highlighted in the alignment figs and low scoring segments most often result from one of three major causes high divergence between the sequences errors in input sequences most notably frameshifts and misalignments if the cause can be ascribed to high divergence the alignment may not be wrong but should be regarded as unreliable in the low scoring segment in particularly unreliable segments clustal x may mark out every sequence the alignment in such a region is likely to be meaningless frameshift errors are more frequent than usually realised in the alignment of eftus taken from swiss prot release four sequences have short frameshifts within the region shown in figure suspect sequences can be investigated with frameshifting alignment programs such as pairwise in wisetools or framesearch in the gcg package it is important to detect and remove sequences containing errors as they confound many types of inferences based on multiple alignments and may themselves also cause the propagation of further alignment errors we have found the low scoring segments test to be remarkably powerful picking up a number of frameshifts and leading to the correction of many misalignments not every highlighted region nucleic acids research vol no is false but by checking them over the major errors are almost always uncovered nevertheless there are situations where the test may give a false sense of alignment accuracy this could happen when aligning sequences with strong amino acid residue biases reduced sequence complexity tandem repeats are another case since superposition of the wrong repeats could still give a high scoring alignment alignments of highly divergent membrane proteins are tricky on both counts since there are many transmembrane helices with hydrophobic amino acid biases more specialised detailed alignment analysis programs are available the advantages of clustal x are that the quality analyses are very fast as well as being integrated into the alignment package and the results are displayed graphically on the screen with any low scoring regions highlighted by shading the alignment background this interactive system provides an efficient and flexible approach to alignment analysis and correction correcting misaligned regions in figure a model protein misalignment has been set up for clarity the closely related eftu sequences have been deliberately misaligned genuine misalignments would normally be highly divergent with only a few identities in particularly conserved columns in such cases if the correct alignment can be ascertained this may be by matches between residue similarities rather than identities in the example a misaligned segment of is first detected and marked by applying the low scoring segments algorithm fig next a region of the alignment spanning the error is selected using the cursor the menu option reset all gaps before alignment is toggled on in this example there are falsely inserted gaps that must be deleted this is not always the case and if the existing gaps seem correct the option can stay switched off now the realign selected residue range option is invoked the misaligned region is now rapidly and correctly aligned again and the false gaps are deleted fig this time the low scoring segments algorithm finds only short segments ascribable to natural sequence divergence realignments in which the gaps are left in may result in columns with nothing but padding characters in which case there is a menu option available to delete these the realignment process uses the alignment parameter default settings or as they are set up by the user misaligned regions are often more divergent than other regions of the alignment which means that the alignment score may not be much higher than misaligned alternatives therefore it may be necessary to lower gap penalties to allow the sequences to align this is tested by trial and error however the user should be aware of two factors that already affect the gap penalties in the local realignment there is no gap penalty at the ends of a selected region so it is free to put new gaps there judicious selection of the range boundaries can direct gaps to desired sites gap penalties are also lowered at existing gaps if these are retained these factors mean that the selected range may give a better alignment without having to lower the gap penalties further uses for the low scoring segments in clustal x the new algorithm for marking low scoring segments has been implemented for visual interaction however the algorithm has the potential for wider usage there are currently many projects to automatically produce databases of nucleic acids research vol no multiple sequence alignments the alignments tend not to be of high quality as it has been difficult to distinguish good and bad aligned regions rapidly and reliably removing sequences with low scoring segments below a cut off score should dramatically improve these alignments as all sequences that contain major errors or are too divergent to align can be trapped the algorithm also has the potential to automatically establish the domain boundaries in sets of partially related multi domain proteins in this case the smith waterman best local alignment algorithm finding the approximate regions encompassing the homologous domains would be harnessed to the forwards backwards approach summing both the positive and negative scoring segments in order to define sharp boundaries a simpler application would be end trimming in an alignment since the termini of proteins are often poorly conserved abstract summary the clustal w and clustal x multiple sequence alignment programs have been completely rewritten in cþþ this will facilitate the further development of the alignment algorithms in the future and has allowed proper porting of the programs to the latest versions of linux macintosh and windows operating systems availability the programs can be run on line from the ebi web server http www ebi ac uk tools the source code and executables for windows linux and macintosh computers are available from the ebi ftp site ftp ftp ebi ac uk pub software contact clustalw ucd ie introduction multiple sequence alignments are now one of the most widely used bioinformatics analyses they are needed routinely as parts of more complicated analyses or analysis pipelines and there are several very widely used packages e g clustal w thompson et al clustal x thompson et al t coffee notredame et al mafft katoh et al and muscle edgar clustal is also the oldest of the currently most widely used programs having been first distributed by post on floppy disks in the late it was initially written in microsoft fortran for ms dos and originally ran on ibm compatible personal computers as four separate executable programs higgins and sharp these were later rewritten in c and merged into a single program clustal v higgins et al that was distributed for vax vms unix apple macintosh and ibm compatible pcs these programs were distributed from the embl file server stoehr and omond an e mail and ftp server based at the embl in heidelberg germany the current clustal programs all derive from clustal w thompson et al which incorporated a novel position specific scoring scheme and a weighting scheme for down weighting over represented sequence groups the w to whom correspondence should be addressed stands for weights these programs have been amended and added to many times since in order to increase functionality and to increase sensitivity the user friendliness has also been greatly enhanced by the addition in of a full graphical user interface thompson et al this has made the code complicated to maintain and develop as the graphical interface must be constantly modified and recompiled for new operating systems and desktop environments windows macintosh vms unix and linux by the late clustal w and clustal x were the most widely used multiple alignment programs they were able to align medium sized data sets very quickly and were easy to use the alignments were of sufficient quality not to require manual editing or adjustment very often this situation changed greatly with the appearance of the first custom made benchmark test set for multiple alignment programs balibase thompson et al this was followed by the appearance of t coffee which was able to make very accurate alignments of very divergent proteins but only for small sets of sequences given its high computational cost with the increase in processing speed of desktop computers and subsequent optimisation of the t coffee code the latter is now practical for routine use on moderately sized alignment problems more recently mafft and muscle appeared which were initially at least as accurate as clustal in terms of alignment accuracy but which were also extremely fast and able to align many thousands of sequences over the past or years these programs have also gradually become more and more accurate with difficult alignments nonetheless clustal w and clustal x continue to be very widely used increasingly on websites the ebi clustal site gets literally millions of multiple alignment jobs per year it is in this context that we developed clustal w and clustal x these programs were rewritten in cþþ with a simple object model in order to make it easier to maintain the code and more importantly to make it easier to modify or even replace some of the alignment algorithms we have produced two new programs which are very similar in look and feel to the older version programs but which can now be managed more easily we have also made some minor adjustments to the the author published by oxford university press all rights reserved for permissions please email journals permissions oxfordjournals org m a larkin et al fig clustalx screenshot on mac os x alignment algorithms we have included new code for upgma guide trees as an alternative to the usual neighbor joining guide trees this helps speed up the alignment of extremely large data sets of tens of thousands of sequences we have also included an iterative alignment facility to increase alignment accuracy clustal x is the new version of the clustal x graphical alignment tool the original clustal x was developed using ncbi vibrant toolbox the vibrant toolbox is no longer supported which led to problems compiling clustal x on newer versions of operating systems the graphical interface sections of clustal x have been completely rewritten using the qt gui toolbox qt is an easy to use multi platform cþþ gui toolkit the code need only be compiled once on each of the platforms the qt toolbox provides a native look and feel on windows linux and mac platforms clustal x has the same functionality as clustal x new features two new options have been included in clustal w to allow faster alignment of very large data sets and to increase alignment accuracy the default options of clustal w and clustal x are the same as clustal w and will give the same alignment results the guide trees in clustal have been calculated using the neighbor joining nj method for the past years or so in the earliest versions of the program upgma was used upgma is faster than nj but prone to cluster long branches together when evolutionary rates are very unequal in different lineages both algorithms have complexity of o but upgma is faster for a given data set and the difference becomes pronounced with very large n on a standard desktop pc it is possible to cluster sequences in less than a minute using upgma while nj would take over an hour we have reimplemented a very efficient algorithm for upgma which can be called by using the command line option it is marginally less accurate on the balibase benchmark but on large alignments e g globin sequences this is offset by the savings in processing time h versus h iteration is a quick and effective method of refining alignments a remove first iteration scheme which optimizes the weighted sum of pairs wsp score has been included in this version of clustal during each iteration step each sequence is removed from the alignment in turn and realigned if the wsp score is reduced then the resulting alignment is retained the iteration scheme can be used to either refine the final alignment or at each step in the progressive alignment iterating during the progressive alignment tends to be more accurate but also much more time consuming as there are nodes in the guide tree the command line option refines the final alignment while the option incorporates the scheme into the progressive alignment the number of iteration cycles is set via the command line option numiters molecular graphics still plays an important role in the deter mination of protein structures using x ray crystallographic data despite on going efforts to automate model building functions such as side chain placement loop ligand and fragment tting structure comparison analysis and validation are routinely performed using molecular graphics lower ê resolution dmin worse than a data in particular need interactive tting the introduction of frodo jones and then o jones et al to the eld of protein crystallography was in each case revolutionary each in their time breaking new ground in demonstrating what was possible with the current hardware these tools allowed protein crystallographers to enjoy what is widely held to be the most thrilling part of their work giving birth as it were to a new protein structure the program suite collaborative computational project number is an integrated collection of software for macromolecular crystallography with a scope ranging from data processing to structure re nement and validation until recently molecular graphics had not been part of the suite with the recent computational and graphical performance of relatively cheap hardware the time had arrived for to provide graphical functionality for knowledge based semi automatic building using powerful modern languages in a exible extendible package potterton et al is an initiative by to provide libraries and a molecular graphics application that is a popular system for represent ation modelling structure determination analysis and validation the aim is to provide a system that is easy to use and a platform for developers who wish to integrate macro molecular computation with a molecular graphics interface there are several modules to such graphical functionality the protein model building map tting tools described here are only a part these tools are available as a stand alone software package coot a map tting program has to provide certain functionality which is not required by a molecular display program these functions include symmetry coordinates electron density map contouring and the ability to move the coordinates in various ways such as model idealization or according to side chain rotamer probabilities doi acta cryst 2132 research papers the map tting and model building functions described here have a functionality broadly similar to that of programs such as o xtalview from x t mcree or quanta accelrys san diego ca usa however in the spirit of the program suite it is possible for others to read and modify the program coot attempts generally speaking to provide more trans parency ease of use better extendability semi automated model building methods and convenient integration with programs of the suite program functions coot has been substantially built around two major libraries mmdb krissinel et al a library for the handling of macromolecular coordinates and clipper cowtan a library for crystallographic objects and computation thereof the various functions of coot are split into stand alone classes in the sense that an attempt has been made to minimize the dependence of the classes on anything other than the above libraries with portability in mind special effort was made not to introduce gui dependences into the interface to coot library of tools coot is event driven functions are only run as a result of user action typically moving or clicking the mouse symmetry coordinate symmetry is recomputed and redisplayed at every recentre event for each molecule for which the user wishes to display symmetry symmetry atoms are displayed within a particular distance criterion of the display centre by using a set of pre computed guide points that mark the extents of the molecule and applying the symmetry operators and cell shifts to these guide points a set of operator indexes and cell shifts are generated that may contain symmetry related atoms close to the screen centre where close is de ned by a user settable parameter for each of these sets all atoms in the molecule are transformed and a check is made for each to see if it is within the symmetry display radius of the position at the centre of the screen thus symmetry is kept current and relevant to the current display centre electron density because coot is based on the clipper libraries it is easy to generate maps by reading a le of structure factors that contain phase information typically an mtz le density is not limited to any particular part of the unit cell the relevant symmetry related density is generated and then contoured automatically using clipper functionality the electron density maps can be simply recontoured provoked by script or keyboard or mouse events at a different level using a predetermined increment every map displayed or un displayed is regenerated and contoured this process is not optimally fast but simpli es the user interface interface to refmac on reading an mtz le one can optionally assign para meters for running refmac murshudov et al refmac is a program of the suite for maximum likelihood based macromolecular re nement after a period of interactive model building the user can choose to use refmac to re ne the current coordinates in combination with mtz parameters coot blocks until refmac has terminated and then automatically reads the newly generated re ned coordinates and mtz le from which a map is generated and displayed rigid body refinement clipper library functions provide easy access to the map gradients for a selected coordinate set the map gradients at the atom centres are averaged a shift is applied to all the selected atoms that is some simple fraction of the average gradient the rotational component of the rigid body re ne ment is generated in the following manner the rotations to be calculated x y and z are small rotations around the coordinate axes the centre of rotation v being the centre of the rotating atoms let vpi be the projection on to the xy plane of the vector between the position of atom i and v the unit vector being vp the dot product of the gradient with vp provides dvp i i i angle is arctan d v these angles are the required c vpi j j c available for each atom and they are averaged to obtain three perpendicular rotations x y and z these angle transfor mations are applied to the coordinates the application of transformations continues until the average shift length is less ê than a this is a reasonable approach for much of a protein structure but could behave badly where there is a combina tion of relatively heavy and light atoms such as sulfates or methionines this problem could be countered by weighting the atom density score by the atomic weight rotamers the rotamer library used in coot is the backbone independent library of dunbrack cohen it is formed from a reasonably large sample set chains is reasonably up to date may and provides a more accurate estima tion of the population of rare rotamers the coot function auto t rotamer takes a set of most likely rotamers for a particular side chain and generates coordinates for each rotamer each test rotamer is then rigid body re ned and the nal position is scored according to the t to the density the residue backbone atoms are included in the set of re ned atoms the best t rotamer is chosen and replaces the previous coordinates regularization and refinement molecular graphics model building requires the ability to regularize idealize the coordinates of the model in order to do so the ideal values of the geometry of the macromolecule acta cryst 2132 emsley cowtan model building tools for molecular graphics research papers should be known these ideal values can come in various forms the interface in coot reads the mmcif dictionaries of refmac which de ne idea values and estimated standard deviations for bond lengths angles torsions planes and chiral centres coot uses the polak ribiere variant of the bfgs broyden fletcher goldfarb shanno conjugate gradient multi variable function minimizer to optimize the coordinates the analytical gradient derivations are described in appendix a fitting to the map as described above the map gradients are provided by a clipper function these map gradients at the positions of the atom centres are simply multiplied by a user changeable scaling factor and added to the geometric terms to de ne the target function this is called re nement in coot finding ligands a map can be masked by a set of coordinates typically those of the currently determined atoms of the protein model this approach leaves a map that has positive density at places where there are no atoms to represent that density similar in fact to an fo ÿ fc map this masked map is searched for clusters of density above a particular level the clustering of the grid points of the asymmetric unit into potential ligand sites is performed conveniently using a recursive neighbour search of the map the clusters are sorted according to size and electron density value eigenvalues and eigenvectors are calculated for each cluster of grid points similarly the eigenvalues and eigenvectors of the search ligands there can of course be just one search ligand are computed the parameters being the positions of the atom centres the eigenvalues of the ligands are compared with the eigenvalues of each of the electron density clusters and if they are suf ciently similar the ligand is placed into the cluster by matching the centre of the test ligand and the centre of the cluster the ligand is oriented in each of the four different orientations that provide coinciding eigenvectors and then rigid body re ned and scored the score is simply the sum of the electron density at the atom centres the score at each site for each different ligand is compared and the best t highest score with suf cient fraction of atoms in positive density after the rigid body re nement is chosen this last check ensures that oversized ligands are not tted into small clusters flexible ligands instead of having a series of different ligand compounds the search ligands can be gener ated from a single ligand that has rotatable bonds the ligand dictionary provides a description of the geometry of the ligand including torsions these torsions are randomly sampled for a number of trials by default to provide coordinates that can be checked against the potential ligand sites as described above an enhancement would be to allow the determination of the number of trials to depend on the number of torsions finding water molecules the electron density is clustered as described for ligands for clusters that have a ê volume below a certain upper limit a which stops water molecules being placed in multi atom ligand sites a starting position is determined from the mean position of the grid coordinates of the cluster this position is then optimized by re ning the position to the local maximum as determined by cubic interpolation of the map a map sphericity test is then applied the variance of the cubic interpolated electron density ê at points and a from the local maximum in positive and negative offsets along the x y and z axes are determined the variances are summed and must be lower than a user ê changeable cutoff default e a the successful posi tions are then compared with the coordinates of the protein o and n atoms if the distance is between user changeable ê criteria default a then the position is accepted as a solvent o atom and optionally added to the protein model add terminal residue given the selection of a terminal residue which also could merely be the start of a gap of unplaced residues two residue type independent randomly selected pairs are made from clipper ramachandran distribution of pairs these angles are used to generate positions of c c o and n main chain atoms for the neighbouring two residues using the peptide geometry this set of atoms then undergo rigid body re nement to optimize the t to the map the score of the t and the positions of the atoms are recorded this procedure is then repeated a number of times by default the main chain atoms of the neighbouring residue best t atoms are then offered as a position of the neighbouring residue the atoms of the next neighbouring residue are discarded skeletonization and ca building coot uses a clipper map to generate and store the skeleton this approach is convenient because like electron density maps the skeleton can be displayed on the y anywhere in the crystal i e it is not limited to a precalculated region the clipper skeletonization algorithm is similar to that employed in dm from cowtan a skeleton bond bone is drawn between neighbouring map grid points if both parts are marked as skeleton points the skeleton can be further trimmed by recursive tip removal a tip being a grid point with one or zero neighbours this process removes side chains and potentially parts of the termini but provides an easy means of identifying the fold and non crystallographic symmetry like some validation kleywegt and other attempts at automated model building morris et al old eld hubbard old eld a likelihood distribution for the pseudo torsion angle c n ðc n ðc n ð c n versus the angle c n ðc n ðc n has been generated from high resolution structures in the pdb berman et al fig once at least three c atoms have been placed this is used as prior knowledge in the placement of the next c position in the following manner ê skeleton points between and a from the current c position which has an associated nearby skeleton point are selected these skeleton points are tested for direct connec tivity to the current skeleton point skeleton points that are emsley cowtan model building tools for molecular graphics acta cryst 2132 c pseudo torsion angle versus opening angle for proteins in the pdb used in the likelihood assignment of potential c positions directly connected are assigned a score of those that are unconnected are assigned a score of for each selected ê skeleton point a test point is then generated a from the current c position in the direction of skeleton point a c pseudo torsion angle and angle pair are generated from the position of the test point the current c position and the two previously assigned c positions this pseudo torsion angle and angle pair are used to generate a score by looking up the value in the internal representation of fig using cubic interpolation this value is combined with the skeleton based score for this particular test point this procedure is then repeated in a look ahead manner assuming that the test point is a member of the set of four c positions generating the c pseudo torsion angle pair the most likely solution for the look ahead is combined with the score for the current test point the test points are then sorted by combined score and interactively offered as potential positions for the next c atom the positions with the best score being offered rst occasionally usually as a result of a positional error in the ê current c position a is the wrong distance to the next correct c position thus the user is allowed to change the ê length to something other than a the depth of the look ahead in the current implementation is at level but could trivially be extended in tests a level look ahead was better but took too long to be considered pleasantly interactive this algorithm has room for improvement for example by considering the value of the density at the test point and along the c pseudo bond one third and two thirds of the way along the bond corresponding to positions that are close to the peptide c and n atoms old eld c coordinates are converted to main chain coordinates in a manner similar to that previously described jones thirup esnouf angles the middle atom a middle atom is somewhat more tricky than an end atom because the derivatives of ab and a b are not so trivial let us change the indexing so that we are actually talking about the middle atom l angles an end atom atoms k or m this case is simpler because there are no cross terms in r xk and q xk ˆ where b is a unit vector in the direction of b b b b this de nition of the torsion angle is used rather than the more common de nition which uses three cross products because our version and its derivatives are faster to calculate let us split the expression up into tractable portions the evaluation of in the program will combine these expressions starting at the end the most simple from the primatives where eij is the distance of the ith plane restraint jth atom from the ith plane restraint least squares plane recall the equation of a plane ax by cz d firstly the centres of the sets of atoms xcen ycen zcen are determined the plane is moved so that it crosses the origin and therefore d it is moved back later the problem then involves three equations three unknowns and an eigenvalue problem with the smallest eigenvalue corresponding to the best t plane the least squares planes of the plane restraints are recal culated at every iteration the authors thank garib murshudov eleanor j dodson jack quine and the many coot testers kc is supported by the royal society grant no pe is funded by bbsrc grant no statistics textbooks in the social behavioral and biomed ical sciences typically stress the importance of power analy ses by definition the power of a statistical test is the prob ability that its null hypothesis will be rejected given that it is in fact false obviously significance tests that lack sta tistical power are of limited use because they cannot reliably discriminate between and the alternative hypothesis of interest however although power analyses are indispens able for rational statistical decisions it was not until the late that power charts see e g scheffé and power tables see e g cohen were supplemented by more efficient precise and easy to use power analysis programs for personal computers goldstein g power erd felder faul buchner can be seen as a second generation power analysis program designed as a stand alone application to handle several types of statistical tests commonly used in social and behavioral research in the past years this program has been found useful not only in the social and behavioral sciences but also in many other disci plines that routinely apply statistical tests including biology baeza stotz genetics akkad et al ecol ogy sheppard forest and wildlife research mellina hinch donaldson pearson the geosciences bus bey pharmacology quednow et al and med ical research gleissner clusmann sassen elger helm staedter g power was evaluated positively in the reviews of which we are aware kornbrot ortseifen bruckner burke kieser thomas krebs it has been used in several power tutorials e g buchner erdfelder faul erdfelder buchner faul brandt levin sheppard and in statis tics textbooks e g field keppel wickens myers well rasch friese hofmann naumann nevertheless the user feedback that we re ceived coincided with our own experience in showing some limitations and weaknesses of g power that required a major extension and revision in the present article we describe g power a program that was designed to address the problems of g power we begin with an outline of the major improvements in g power and then discuss the types of power analyses cov ered by this program next we describe program handling and the types of statistical tests to which it can be applied we then discuss the statistical algorithms of g power and their accuracy finally program availability and some inter net resources supporting users of g power are described improvements in g power in comparison with g power g power is an improvement over g power in five major respects first whereas g power requires the e erdfelder erdfelder psychologie uni mannheim de copyright psychonomic society inc faul erdfelder lang and buchner dos and mac os operating systems that were com mon in the but are now outdated g power runs on the personal computer platforms currently in widest use windows xp windows vista and mac os x the windows and mac versions of the program are es sentially equivalent they use the same computational routines and share very similar user interfaces for this reason we will not differentiate between these versions in what follows users simply have to make sure to download the version appropriate for their operating system second whereas g power is limited to three types of power analyses g power supports five different ways to assess statistical power in addition to the a pri ori post hoc and compromise power analyses that were already covered by g power the new program offers sensitivity analyses and criterion analyses third g power provides dedicated power analysis options for a variety of frequently used t f z and exact tests in addition to the standard tests covered by g power the tests captured by g power and their effect size parameters are described in the program han dling section importantly users are not limited to these tests because g power also offers power analyses for generic t f z and binomial tests for which the non centrality parameter of the distribution under may be entered directly in this way users are provided with a flexible tool for computing the power of basically any statistical test that uses t f z or binomial reference distributions fourth statistical tests can be specified in g power using two different approaches the distribution based ap proach and the design based approach in the distribution based approach users select the family of the test statistic t f z or exact test and the particular test within that family this is how power analyses were specified in g power in addition a separate menu in g power provides access to power analyses via the design based approach users select the parameter class to which the statistical test refers correlations means proportions regression coefficients variances and the design of the study e g number of groups independent vs depen dent samples on the basis of the feedback we received about g power we expect that some users might find the design based input mode more intuitive and easier to use fifth g power supports users with enhanced graph ics features the details of these features will be outlined in the program handling section types of statistical power analyses the power of a statistical test is the complement of which denotes the type ii or beta error probability of falsely retaining an incorrect statistical power de pends on three classes of parameters the significance level i e the type i error probability of the test the size of the sample used for the test and an effect size parameter defining and thus indexing the degree of deviation from in the underlying population de pending on the available resources the actual phase of the research process and the specific research question five different types of power analysis can be reasonable cf erdfelder et al erdfelder faul buchner we describe these methods and their uses in turn a priori power analyses in a priori power analyses cohen sample size n is computed as a function of the required power level the prespecified significance level and the population effect size to be detected with probability a priori analyses provide an efficient method of controlling statistical power before a study is actually con ducted see e g bredenkamp hager and can be recommended whenever resources such as the time and money required for data collection are not critical post hoc power analyses in contrast to a priori power analyses post hoc power analyses cohen often make sense after a study has already been conducted in post hoc analyses is computed as a function of the population effect size parameter and the sample size used in a study it thus becomes possible to assess whether or not a published statistical test in fact had a fair chance of rejecting an in correct importantly post hoc analyses like a priori analyses require an effect size specification for the underlying population post hoc power analyses should not be confused with so called retrospective power anal yses in which the effect size is estimated from sample data and used to calculate the observed power a sample estimate of the true power retrospective power analy ses are based on the highly questionable assumption that the sample effect size is essentially identical to the effect size in the population from which it was drawn zumbo hubley obviously this assumption is likely to be false and the more so the smaller the sample in addition sample effect sizes are typically biased estimates of their population counterparts richardson for these reasons we agree with other critics of retrospective power analyses e g gerard smith weerakkody hoe nig heisey kromrey hogarty lenth steidl hayes schauber rather than use retrospective power analyses researchers should specify population effect sizes on a priori grounds to specify the effect size simply means to define the minimum degree of violation of a researcher would like to detect with a probability not less than cohen definitions of small medium and large effects can be helpful in such effect size specifications see e g smith bayen however researchers should be aware of the fact that these conventions may have different meanings for differ ent tests cf erdfelder et al compromise power analyses in compromise power analyses erdfelder erdfelder et al müller manz hoyer both and are computed as functions of the ef fect size n and the error probability ratio q to illustrate setting q to would mean that the researcher prefers balanced type i and type ii error risks g power whereas a q of would imply that cf cohen compromise power analyses can be useful both before and after data collection for example an a priori power analysis might result in a sample size that exceeds the available resources in such a situation a researcher could specify the maximum affordable sample size and using a compromise power analysis compute and associated with say q alternatively if a study has already been conducted but has not yet been analyzed a researcher could ask for a reasonable decision criterion that guarantees perfectly balanced error risks i e given the size of the sample and the critical effect size in which he or she is interested of course compromise power analyses can easily result in unconventional sig nificance levels greater than in the case of small samples or effect sizes or less than in the case of large samples or effect sizes however we believe that the benefit of balanced type i and type ii error risks often offsets the costs of violating significance level conven tions cf gigerenzer krauss vitouch sensitivity analyses in sensitivity analyses the critical population effect size is computed as a function of and n sensitivity analyses may be particularly useful for evaluating pub lished research they provide answers to questions such as what effect size was a study able to detect with a power of given its sample size and as specified by the author in other words what is the minimum ef fect size to which the test was sufficiently sensitive in addition it may be useful to perform sensitivity analyses before conducting a study to see whether given a lim ited n the size of the effect that can be detected is at all realistic or for instance much too large to be expected realistically criterion analyses finally criterion analyses compute and the associ ated decision criterion as a function of the effect size and a given sample size criterion analyses are alter natives to post hoc power analyses they may be reason able whenever the control of is less important than the control of in case of goodness of fit tests for statistical models for example it is most important to minimize the risk of wrong decisions in favor of the model re searchers could thus use criterion analyses to compute the significance level which is compatible with for a small effect size whereas g power was limited to the first three types of power analysis g power covers all five types on the basis of the feedback we received from g power users we believe that any question related to statistical power that arises in research practice can be categorized under one of these analysis types program handling using g power typically involves the following four steps select the statistical test appropriate for the problem choose one of the five types of power analyses defined in the previous section provide the input parameters required for the analysis and click on calculate to obtain the results in the first step the statistical test is chosen using the distribution based or the design based approach g power users probably have adapted to the distribution based approach one first selects the family of the test statistic t f z or exact test using the test fam ily menu in the main window the statistical test menu adapts accordingly showing a list of all tests available for the test family for the two groups t test for example one would first select the t family of distributions and then means difference between two independent means two groups in the statistical test menu see figure al ternatively one might use the design based approach of test selection with the tests pull down menu in the top row it is possible to select the parameter class to which the statistical test refers i e correlation and regression means proportions variances or generic and the de sign of the study e g number of groups independent vs dependent samples for example a researcher would select means followed by two independent groups to specify the two groups t test see figure the design based approach has the advantage that test options refer ring to the same parameter class e g means are located in close proximity whereas in the distribution based ap proach they may be scattered across different distribution families in the second step the type of power analysis menu in the center of the main window should be used to choose the appropriate analysis type in the third step the power analysis input parameters are specified in the lower left of the main window to illustrate an a priori power analysis for a two groups t test would require a decision between a one tailed and a two tailed test a specification of cohen effect size measure d under the significance level the required power of the test and the preferred group size allocation ratio the final step consists of clicking on calculate to obtain the output in the lower right of the main window for instance input parameters specifying a one tailed t test a medium effect size of d and an allocation ratio of would result in a total sample size of n observation units in each group see figures and the noncentrality pa rameter y defining the t distribution under the decision criterion to be used i e the critical value of the t statis tic the degrees of of the t test and the actual power value are also displayed note that the actual power will often be slightly larger than the prespecified power in a priori power analyses the reason is that noninteger sample sizes are always rounded up by g power to obtain integer values consistent with a power level not lower than the prespecified one in addition to the numerical output g power dis plays the central and the noncentral test statistic distributions along with the decision criterion and the as sociated error probabilities in the upper part of the main window see figure this supports understanding of the effects of the input parameters and is likely to be a faul erdfelder lang and buchner figure the distribution based approach of test specification in g power useful visualization tool in the teaching of or the learning about inferential statistics the distributions plot can be printed saved or copied by clicking on the right mouse button inside the plot area the input and output of each power calculation in a g power session is automatically written to a protocol that can be displayed by selecting the protocol of power analyses tab in the main window it is possible to clear the protocol or to print save and copy the protocol in the same way as the distributions plot because cohen book on power analysis appears to be well known in the social and behavioral sciences we made use of his effect size measures whenever possible researchers unfamiliar with these measures and users who prefer to compute cohen measures from more basic parameters can click on the determine button to the left of the effect size input field see figures and a drawer will open next to the main window and provide access to an effect size calculator tailored to the selected test see figure for the two groups t test for example users can specify the means and the common sd à in the populations underlying the groups to calculate cohen d à clicking on the calculate and transfer to main window button copies the computed ef fect size to the appropriate field in the main window another useful option is the power plot window see figure which is opened by clicking on x y plot for a range of values on the lower right side of the main win dow see figures and by selecting the appropriate parameters for the y and x axes one parameter effect size or sample size can be plotted as a function of any other parameter of the remaining two parameters one can be chosen to draw a fam ily of graphs whereas the fourth parameter is kept constant for instance sample size can be drawn as a function of the power for several different population effects sizes while is kept at a particular value the plot may be printed saved or copied by clicking on the right mouse button inside the plot area selecting the table tab reveals the data un derlying the plot they may be copied to other applications the power plot window inherits all input parameters of the analysis that is active when the x y plot for a range of g power figure the design based approach of test specification in g power and the effect size drawer figure the power plot window of g power faul erdfelder lang and buchner values button is clicked only some of these parameters can be directly manipulated in the power plot window for instance switching from a plot of a two tailed test to a plot of a one tailed test requires choosing the tail one option in the main window and then clicking on the x y plot for a range of values button types of statistical tests g power provides power analyses for test statistics following t f or standard normal distributions under either exact or asymptotic and noncentral distributions of the same test families under in addition it includes power analyses for some exact tests in tables we briefly describe the tests currently covered by g power table lists the symbols used in tables and their meanings tests for correlation and regression table summarizes the procedures supported for test ing hypotheses on correlation and regression one sample tests are provided for the point biserial model that is the model for correlations between a binary variable and a continuous variable and for correlations between two normally distributed variables cohen chap the latter test uses the exact sample correlation coefficient distribution barabesi greco or optionally a large sample approximation based on fisher r to z trans formation the two sample test for differences between two correlations uses cohen chap effect size q and is based on fisher r to z transformation cohen de fines qs of and as small medium and large effects respectively the two procedures available for the multiple regres sion model handle the cases of a test of an overall effect that is the hypothesis that the population value of is different from zero and a test of the hypoth esis that adding more predictors increases the value of cohen chap according to cohen criteria effect sizes f of and are considered small medium and large respectively tests for means univariate case table summarizes the power analysis procedures for tests on means g power supports all cases of the t test for means described by cohen chap the test for independent means the test of the null hypothesis that the population mean equals some specified value one sample case and the test on the means of two dependent samples matched pairs cohen d and dz are used as effect size indices cohen defines ds of and as small medium and large effects respectively effect size dialogs are available to compute the appropriate effect size param eter from means and sds for example assume we want to compare visual search times for targets embedded in rare versus frequent local contexts in a within subjects design cf hoffmann sebald experiment it is ex pected that the mean search time for targets in rare contexts e g msec should decrease by at least msec i e to msec in frequent contexts as a consequence of local contextual cuing if prior evidence suggests population sds of say à msec in each of the conditions and a correlation of between search times in the two con ditions we can use the effect size drawer of g power for the matched pairs t test to calculate the effect size dz see the second row of table for the formula by selecting a post hoc power analysis for one tailed matched pairs t tests we easily see that for dz and n participants the power is only thus provided that the assumptions outlined above are appropri ate the nonsignificant statistic t obtained by hoffmann and sebald experiment p might in fact be due to a type ii error this interpretation would be consistent with the fact that hoffmann and sebald ob table symbols and their meanings as used in the tables symbols meaning i population mean in group i vector of population means in group i i xy population mean of the difference n total sample size ni sample size in group i à standard deviation in the population à standard deviation of the effect àxy standard deviation of the difference noncentrality parameter of the noncentral f and distribution y noncentrality parameter of the noncentral t distribution df degrees of freedom numerator and denominator degrees of freedom respectively population correlation in group i squared multiple correlation coefficients corresponding to the proportion of y b variance that can be accounted for by multiple regression on the set of predictor variables a and a b respectively population variance covariance matrix m matrix of regression parameters population means c contrast matrix contrasts between rows of m a contrast matrix contrasts between columns of m i probability of success in group i g power table tests for correlation and regression test null noncentrality parameter test family hypothesis effect size other parameters and degrees of freedom difference from t tests zero point biserial n model df n difference from exact c constant constant tests correlation c bivariate normal inequality of two z tests q q correlation z ln i coefficients i i multiple f tests a number of f n regression f y a predictors p a p deviation of y a n p from zero multiple f tests a b a r total number of f n regression f y a b y a predictors p q increase of y a b a b n p number of tested predictors q b served significant local contextual cuing effects in each of the other four experiments they reported the procedures provided by g power to test effects in between subjects designs with more than two groups i e one way anova designs and general main effects and interactions in factorial anova designs of any order are identical to those in g power erdfelder et al in all these cases the effect size f as defined by cohen is used in a one way anova the effect size drawer can be used to compute f from the means and group sizes of k groups and an sd common to all groups for tests of effects in factorial designs the effect size drawer offers the possibility of computing effect size f from the vari ance explained by the tested effect and the error variance cohen defines fs of and as small medium and large effects respectively new in g power are procedures for analyzing main effects and interactions for a b mixed designs where a is a between subjects factor or an enumeration of the groups generated by cross classification of several between subjects factors and b is a within subjects fac tor or an enumeration of the repeated measures generated by cross classification of several within subjects factors both the univariate and the multivariate approaches to re peated measures o brien kaiser are supported the multivariate approach will be discussed below the univariate approach is based on the sphericity assump tion this assumption is correct if in the population all variances of the repeated measurements are equal and all correlations between pairs of repeated measurements are equal if all the distributional assumptions are met then the univariate approach is the most powerful method muller barton o brien kaiser unfortunately the assumption of equal correlations is violated quite often which can lead to very misleading results in order to compensate for such adverse effects in tests of within effects or between within interactions the noncentrality parameter and the degrees of freedom of the f distribu tion can be multiplied by a correction factor e geisser greenhouse huynh feldt e if the sphericity assumption is met and approaches m with increasing degrees of violation of sphericity where m denotes the number of repeated measurements g power provides three separate yet very similar rou tines to calculate power in the univariate approach for between effects within effects and interactions if the to be detected effect size f is known these procedures are very easy to apply to illustrate berti münzer schröger and pechmann compared the pitch discrimination ability of musicians and control subjects between subjects factor a for different interference conditions within subjects factor b assuming that a b and a b effects of medium size f see cohen table of the present article should be detected given a correlation of between repeated measures and a significance level of the power values of the f tests for the a main effect the b main effect and the a b interaction are easily computed as and re spectively by inserting f the total sample size the number of groups the number of repeti tions and into the appropriate input fields of the procedures designed for these tests if the to be detected effect size f is unknown it must be computed from more basic parameters characterizing the expected population scenario under to demonstrate the general procedure we will show how to do post hoc power analyses in the scenario illustrated in figure as suming the variance and correlations structure defined in matrix we first consider the power of the within effect we select the f tests family the repeated mea faul erdfelder lang and buchner table tests for means univariate case test null noncentrality parameter test family hypothesis effect size other parameters and degrees of freedom c c difference from t tests d y d nå constant one df n sample case xy inequality of t tests dz x y y dz nå two dependent df n x y means matched pairs x y y2 x y inequality of t tests d d two independent n n means df n anova fixed f tests i effects one i k way inequality of multiple means f number of f groups k k nj i n k k n anova fixed f tests i f total number of f effects i k cells in the q multifactor design k n k designs and degrees of planned freedom of the comparisons tested effect q anova f tests i f levels of f repeated i k between factor k u m measures m between effects levels of k repeated measures n k factor m anova f tests i f f repeated i m m u measures population within effects correlation m e among repeated n k m e measures anova f tests ij i f f repeated å j m for within and u measures i k between within j m within between k m e interactions interactions n k m e nonsphericity correction e sures within factors anova approach test and post hoc as the type of power analysis both the number of groups and repetitions fields are set to total sample size is set to and error probability to referring to matrix we insert in the corr among rep mea sures input field and since sphericity obviously holds in this case set nonsphericity correction e to to deter mine effect size f we first calculate à the variance of the time time time i ni group group sr1 group 333 j 13 figure sample repeated measures designs three groups are repeatedly measured at three dif ferent times the shaded portion of the table is the postulated matrix m of population means ij the last column of the table contains the sample size of each group the symmetric matrices sri specify two dif ferent covariance structures between measurements taken at different times the main diagonal contains the sds of the measurements at each time and the off diagonal elements contain the correlations between pairs of measurements taken at different times g power within effect from the three column means j of matrix m and the grand mean we get s 10 13 667 clicking on the determine button next to the effect size label opens the effect size drawer we choose the from variances option and set variance explained by special effect to 357 and variance within groups to clicking on the calculate and transfer to main window button calculates an effect size f 2572 and transfers f to the effect size field in the main window clicking on calculate yields the results the power is the critical f value with and is and the noncentrality parameter is 52 the procedure for tests of between within interactions ef fects repeated measures within between interac tions anova approach is almost identical to that just described the only difference is in how the effect size f is computed here we first calculate the variance of the residual values ij i j of matrix m s 10 10 889 667 889 9 90123 using the effect size drawer in the same way as above we get an effect size f 1532 which results in a power of to test between effects we choose repeated measures between factors anova approach and set all parameters to the same values as before note that in this case we do not need to specify e no correction is necessary because tests of between factors do not require the sphericity assumption to calculate the effect size we use effect size from means in the effect size drawer we select three groups set sd à within each group to 9 and insert for each group the cor responding row mean i of m 15 12 and an equal group size of effect size f 1719571 is cal culated and the resulting power is note that g power can easily handle pure repeated measures designs without any between subjects factors see e g frings wentura schwarz müller by choosing the repeated measures within fac tors anova approach procedure and setting the num ber of groups to tests for mean vectors multivariate case g power contains several procedures for performing power analyses in multivariate designs see table all these tests belong to the f test family the hotelling t tests are extensions of univariate t tests to the multivariate case in which more than one dependent variable is measured instead of two single means two mean vectors are compared and instead of a single variance a variance covariance matrix is consid ered rencher in the one sample case posits that the vector of population means is identical to a speci fied constant mean vector the effect size drawer can be used to calculate the effect size from the difference c and the expected variance covariance matrix under for example assume that we have two variables a difference vector c under vari ances and a covariance of 98 rencher p to perform a post hoc power analysis choose f tests then multivariate hotelling t one group and set the analysis type to post hoc enter in the response variables field and then click on the determine button next to the effect size label in the effect size drawer at input method means and choose variance covariance matrix and click on specify edit input values under the means tab insert 88 in both input fields under the cov sigma tab in sert 79 and 28 in the main diagonal and 98 as the off diagonal element in the lower left cell clicking on the calculate and transfer to main window button initiates the calculation of the effect size 380 and transfers it to the main window for this effect size and a total sample size of n the power amounts to the procedure in the two group case is exactly the same with the following exceptions first in the effect size drawer two mean vectors have to be specified second the group sizes may differ the manova tests in g power refer to the multi variate general linear model o brien muller o brien shieh y xb e where y is n p of rank p x is n r of rank r and the r p matrix b contains fixed coefficients the rows of e are taken to be independent p variate normal random vectors with mean and p p positive definite covariance matrix the multivariate general linear hypothesis is cba where c is c r with full row rank and a is p a with full column rank in g power is assumed to be zero has a c degrees of freedom all tests of the hypothesis refer to the matrices h t cxt wx n cbu cbu ct nh and e ut un r where is a q q essence model matrix is a q q di x w agonal matrix containing weights wj nj n and xt x t n x wx see o brien shieh p let be the min a c eigenvalues of and the eigenvalues of n r that is i in n r g power offers power analyses for the multivariate model following either the approach outlined in muller and peterson muller lavange landesman ramey ramey or alternatively the approach of o brien and shieh shieh both approaches approximate the exact distributions of wilks u rao the hotelling lawley t pillai samson the hotelling lawley faul erdfelder lang and buchner table tests for mean vectors multivariate case test null noncentrality parameter test family hypothesis effect size other parameters and degrees of freedom hotelling t f tests number of c vå t v å difference from response k v c constant mean variables k n k vector hotelling t f tests number of n n vå t v å difference response v between two variables k k mean vectors n k manova f tests cm effect size number of noncentrality parameter global effects means matrix fmult groups g and degrees of freedom m depends on the test number of depend on the test statistic contrast statistics response and algorithm used see matrix c wilks u variables k effect size column and hotelling table lawley t hotelling manova f tests number of lawley t special effects groups g pillai v number of and algorithms predictors p muller number of peterson response variables k o brien shieh manova f testscma levels of repeated means matrix between factor measures m k between between levels of effects contrast repeated matrix c measures factor manova f tests within m repeated contrast measures matrix a within effects manova f tests repeated measures between within interactions t mckeon and pillai v pillai mijares by f distributions and are asymptotically equivalent table outlines details of both approximations the type of statistic u t t v and the approach muller peterson or o brien shieh can be selected in an options dialog that can be evoked by clicking on the options but ton at the bottom of the main window the approach of muller and peterson has found widespread use for instance it has been adopted in the spss software package we nevertheless recommend the approach of o brien and shieh because it has a number of advantages unlike the method of muller and peterson it provides the exact noncentral f distri bution whenever the hypothesis involves at most positive eigenvalues its approximations for eigenvalues are almost always more accurate than those of muller and peterson method which systematically underestimates power and it provides a simpler form of the noncentrality parameter that is n where is not a function of the total sample size g power provides procedures to calculate the power for global effects in a one way manova and for special effects and interactions in factorial manova designs these procedures are the direct multivariate analogues of the anova routines described above table sum marizes information that is needed in addition to the formulas given above to calculate effect size f from hy pothesized values for mean matrix m corresponding to matrix b in the model covariance matrix and contrast matrix c which describes the effect under scrutiny the effect size drawer can be used to calculate f from known values of the statistic u t t or v note however that the transformation of t to f depends on the sample size thus this test statistic seems not very well suited for a priori analyses in line with bredenkamp and erdfelder we recommend v as the multivariate test statistic another group of procedures in g power supports the multivariate approach to power analyses of repeated measures designs g power provides separate but very similar routines for the analysis of between effects within effects and interactions in simple a b designs where a is a between subjects factor and b a within subjects factor to illustrate the general procedure we describe in some detail a post hoc analysis of the within effect for note mp muller peterson algorithm os o brien and shieh algorithm and are eigenvalues of the effect size matrix for details and the meaning of the variables a c r and n see text on p the scenario illustrated in figure assuming the variance and correlations structure defined in matrix we first choose f tests then repeated measures within factors manova approach in the type of power analysis menu we choose post hoc we click on the options button to open a dialog in which we deselect the use mean correlation in effect size calculation option we choose pillai v statistic and the o brien and shieh algorithm back at the main window we set both number of groups and repetitions to total sample size to and error probability to to compute the effect size f v for the pillai statistic we open the effect size drawer by clicking on the determine button next to the effect size label in the effect size drawer select as procedure effect size from mean and variance covariance matrix and as input method sd and correlation matrix clicking on specify edit matrices opens another window in which we specify the hypothesized parameters under the means tab we insert our means matrix m under the cov sigma tab we choose sd and correlation and insert the values of because this matrix is always symmetric it suf fices to specify the lower diagonal values after closing the dialog and clicking on calculate and transfer to main window we get a value of 1791 for pillai v and the effect size f v 4672 clicking on calculate shows that the power is the analyses of between effects and interaction effects are performed analogously tests for proportions the support for tests on proportions has been greatly enhanced in g power table summarizes the tests that are currently implemented in particular all tests on pro portions considered by cohen are now available including the sign test chap the z tests for the differ ence between two proportions chap and the tests for goodness of fit and contingency tables chap the sign test is implemented as a special case c of the more general binomial test also available in g power that a single proportion has a specified value c in both procedures cohen effect size g is used and exact power values based on the binomial distribution are cal culated note however that due to the discrete nature of the binomial distribution the nominal value of usually cannot be realized since the tables in chapter of cohen book use the value closest to the nominal value even if it is higher than the nominal value the tabulated power values faul erdfelder lang and buchner note a d indicate alternative effect size measures are sometimes larger than those calculated by g power g power always requires the actual not to be larger than the nominal value numerous procedures have been proposed to test the null hypothesis that two independent proportions are iden tical cohen d agostino chase belanger suissa shuster upton and g power implements several of them the simplest procedure is a z test with optional arcsin transformation and optional conti nuity correction besides these two computational options one can also choose whether cohen s effect size measure h or alternatively two proportions are used to specify the alternate hypothesis with the options use continuity cor rection off and use arcsin transform on the procedure calculates power values close to those tabulated by cohen chap with both use continuity correction and use arcsin transform off the uncorrected approxima tion is computed fleiss with use continuity cor rection on and use arcsin transform off the corrected approximation is computed fleiss a second variant is fisher s exact conditional test hase man normally g power calculates the exact unconditional power however despite the highly opti mized algorithm used in g power long computation times may result for large sample sizes e g n 000 therefore a limiting n can be specified in the options dialog that determines at which sample size g power switches to a large sample approximation a third variant calculates the exact unconditional power for approximate test statistics t table summarizes the supported statistics the logic underlying this procedure is to enumerate all possible outcomes for the bi nomial table given fixed sample sizes in the two respective groups this is done by choosing as success frequencies and in the first and the second groups respectively any combination of the values and x2 given the success probabilities in the two respective groups the probability of observing a table x with success frequencies x2 is note xi success frequency in group i ni sample size in group i n n2 total sample size ˆi xi ni the z tests in the table are more commonly known as tests the equivalent z test is used to provide two sided tests to calculate power and the actual type i error the test statistic t is computed for each table and compared with the critical value t if a denotes the set of all ta bles x rejected by this criterion that is those with t t then the power and the level are given by x a p x and x a p x where denotes the success probability in both groups as assumed in the null hypothesis note that the actual level can be larger than the nominal level the preferred input method proportions difference risk ratio or odds ratio see table and the test statistic to use see table can be changed in the options dialog note that the test statistic actually used to analyze the data must be chosen for large sample sizes the exact computation may take too much time therefore a limiting n can be specified in the options dialog that determines at which sample size g power switches to large sample approximations g power also provides a group of procedures to test the hypothesis that the difference risk ratio or odds ratio of a proportion with respect to a specified reference pro portion is different under from a difference risk ratio or odds ratio of the same reference proportion assumed in these procedures are available in the exact test family as proportions inequality offset two indepen dent groups unconditional the enumeration proce dure described above for the tests on differences between proportions without offset is also used in this case in the tests without offset the different input parameters e g differences risk ratio are equivalent ways of specifying two proportions the specific choice has no influence on the results in the case of tests with offset however each input method has a different set of available test statistics the preferred input method see table and the test sta tistic to use see table can be changed in the options dialog as in the other exact procedures the computation may be time consuming and a limiting n can be specified in the options dialog that determines at which sample size g power switches to large sample approximations also new in g power is an exact procedure to calcu late the power for the mcnemar test the null hypothesis of this test states that the proportions of successes are identi cal in two dependent samples figure shows the structure of the underlying design a binary response is sampled from the same subject or a matched pair in a standard con dition and in a treatment condition the null hypothesis s t is formally equivalent to the hypothesis for the odd ratio or 12 21 to fully specify we need to specify not only the odds ratio but also the proportion of discordant pairs d that is the expected proportion of responses that differ in the standard and the treatment conditions the exact procedure used in g power calcu lates the unconditional power for the exact conditional test which calculates the power conditional on the number of discordant pairs nd let p nd i be the probability that the number of discordant pairs is i then the unconditional power is the sum over all i 0 n of the conditional power for nd i weighted with p nd i this procedure is very efficient but for very large sample sizes the exact computation may take too much time again a limiting n that determines at which sample size g power switches to a large sample approximation can be specified in the op tions dialog the large sample approximation calculates note xi success frequency in group i ni sample size in group i n n1 n2 total sample size ˆi xi ni y difference between proportions postulated in risk ratio postulated in odds ratio postulated in the power on the basis of an ordinary one sample binomial test with bin n d 0 as the distribution under and bin n d or or as the distribution tests for variances table 9 summarizes important properties of the two procedures for testing hypotheses on variances that are currently supported by g power in the one group case the null hypothesis that the population variance has a specified value c is tested the variance ratio c is used as the effect size the central and noncentral distributions corresponding to and respectively are central distributions with n dfs because and are based on the same mean to compare the variance distributions under both hypotheses the distribution is scaled with the value r postulated for the ratio c in the alternate hypothesis that is the noncentral distribution is ostle malone in the two groups case states that the variances in two populations are identical as in the one sample case two central f distri butions are compared the distribution being scaled by the value of the variance ratio postulated in generic tests besides the specific routines described in tables 2 9 that cover a considerable part of the tests commonly used g power provides generic power analysis routines that may be used for any test based on the t f z or binomial distribution in generic routines the parameters of the central and noncentral distributions are specified directly to demonstrate the uses and limitations of these generic routines we will show how to do a two tailed power analy sis for the one sample t test using the generic routine the results can be compared with those of the specific rou tine available in g power for that test first we select the t tests family and then generic t test the generic test option is always located at the end of the list of tests next we select post hoc as the type of power analysis we choose a two tailed test and as error probability we now need to specify the noncentrality parameter y and the degrees of freedom for our test we look up the definitions for the one sample test in table and find that y d nå and df n assuming a medium effect of d 0 and n we arrive at y 0 5 5 2 5 and df after inserting these values and clicking on calculate we obtain a power of 6697 the critical value t 2 0639 corresponds to the specified in this post hoc power analysis the generic routine is almost as simple as the specific routine the main disadvantage of the generic routines is however that the dependence of the noncen trality parameter on the sample size is implicit as a con sequence we cannot perform a priori analyses automati cally rather we need to iterate n by hand until we find an appropriate power value statistical methods and numerical algorithms the subroutines used to compute the distribution func tions and the inverse of the noncentral t f z and binomial distributions are based on the c version of the dcdflib available from www netlib org random which was slightly modified for our purposes g power does not provide the approximate power analyses that were available in the speed mode of g power 2 two ar guments guided us in supporting exact power calculations only first four digit precision of power calculations may be mandatory in many applications for example both compromise power analyses for very large samples and error probability adjustments in case of multiple tests of significance may result in very small values of or westermann hager second as a consequence of improved computer technology exact calculations have become so fast that the speed gain associated with ap proximate power calculations is not even noticeable thus from a computational standpoint there is little advantage to using approximate rather than exact methods cf brad ley russell reeve program availability and internet support to summarize g power is a major extension of and improvement over g power 2 in that it offers easy to apply power analyses for a much larger variety of common statistical tests program handling is more flexible easier to understand and more intuitive than in g power 2 reducing the risk of erroneous applications the added graphical features should be useful for both research and teaching purposes thus g power is likely to become a useful tool for empirical researchers and students of ap plied statistics like its predecessor g power is a noncommercial program that can be downloaded free of charge copies of the mac and windows versions are available only at www psycho uni duesseldorf de abteilungen aap users interested in distributing the program in another way must ask for permission from the authors commer cial distribution is strictly forbidden the g power web page offers an expanding web based tutorial describing how to use the program along with examples users who let us know their e mail ad dresses will be informed of updates although considerable effort has been put into program development and evalu ation there is no warranty whatsoever users are asked to kindly report possible bugs and difficulties in program handling to gpower feedback uni duesseldorf de faul erdfelder lang and buchner author note manuscript preparation was supported by grant sfb project from the deutsche forschungsgemeinschaft and a grant from the state of baden württemberg germany landesforschungsprogramm evidenzbasierte stressprävention correspondence concerning this article should be addressed to f faul institut für psychologie christian albrechts universität olshausenstr d kiel germany or to e erdfelder lehrstuhl für psychologie iii universität mannheim schloss ehrenhof ost d mannheim germany e mail ffaul psychologie uni kiel de or erdfelder psychologie uni mannheim de notes the observed power is reported in many frequently used computer programs e g the manova procedure of spss 2 we recommend checking the degrees of freedom reported by g power by comparing them for example with those reported by the program used to analyze the sample data if the degrees of freedom do not match the input provided to g power is incorrect and the power calculations do not apply plots of the central and noncentral distributions are shown only for tests based on the t f z or binomial distribution no plots are shown for tests that involve an enumeration procedure e g the mcne mar test we thank dave kenny for making us aware of the fact that the t test correlation power analyses of g power 2 are correct only in the point biserial case i e for correlations between a binary variable and a continuous variable the latter being normally distributed for each value of the binary variable for correlations between two continu ous variables following a bivariate normal distribution the t test cor relation procedure of g power 2 overestimates power for this reason g power offers separate power analyses for point biserial correlations in the t family of distributions and correlations between two normally distributed variables in the exact distribution family however power values usually differ only slightly between procedures to illustrate as sume we are interested in the power of a two tailed test of for continuously distributed measures derived from two implicit association tests iats differing in content assume further that due to method specific variance in both versions of the iat the true pearson correlation is actually 30 effect size given and n see back schmukle egloff p an exact post hoc power analysis for correlations differences from constant one sample case reveals the correct power value of 63 choosing the incorrect correla tion point biserial model procedure from the t test family would result in 1 65 manuscript received december 8 2006 accepted for publication january rich feature hierarchies for accurate object detection and semantic segmentation ross jeff trevor jitendra uc berkeley and icsi rbg jdonahue trevor malik eecs berkeley edu abstract r cnn regions with cnn features warped region aeroplane no object detection performance as measured on the person yes canonical pascal voc dataset has plateaued in the last cnn few years the best performing methods are complex en tvmonitor no semble systems that typically combine multiple low level input extract region compute classify image features with high level context in this paper we image proposals cnn features regions propose a simple and scalable detection algorithm that im figure object detection system overview our system proves mean average precision map by more than takes an input image extracts around bottom up region relative to the previous best result on voc achieving proposals computes features for each proposal using a large a map of our approach combines two key insights convolutional neural network cnn and then classifies each one can apply high capacity convolutional neural net region using class specific linear svms r cnn achieves a mean works cnns to bottom up region proposals in order to average precision map of on pascal voc for localize and segment objects and when labeled training comparison reports map using the same region pro posals but with a spatial pyramid and bag of visual words ap data is scarce supervised pre training for an auxiliary task proach the popular deformable part models perform at followed by domain specific fine tuning yields a signifi cant performance boost since we combine region propos inspired hierarchical and shift invariant model for pattern als with cnns we call our method r cnn regions with recognition was an early attempt at just such a process cnn features we also present experiments that provide the neocognitron however lacked a supervised training al insight into what the network learns revealing a rich hier gorithm lecun et al provided the missing algorithm archy of image features source code for the complete sys by showing that stochastic gradient descent via backprop tem is available at http www cs berkeley edu agation can train convolutional neural networks cnns a rbg rcnn class of models that extend the neocognitron cnns saw heavy use in the e g but then introduction fell out of fashion particularly in computer vision with the features matter the last decade of progress on various rise of support vector machines in krizhevsky et al visual recognition tasks has been based considerably on the rekindled interest in cnns by showing substantially use of sift and hog but if we look at perfor higher image classification accuracy on the imagenet large mance on the canonical visual recognition task pascal scale visual recognition challenge ilsvrc voc object detection it is generally acknowledged their success resulted from training a large cnn on that progress has been slow during with small million labeled images together with a few twists on le gains obtained by building ensemble systems and employ cun cnn e g max x rectifying non linearities and ing minor variants of successful methods dropout regularization sift and hog are blockwise orientation histograms the significance of the imagenet result was vigorously a representation we could associate roughly with complex debated during the ilsvrc workshop the central cells in the first cortical area in the primate visual path issue can be distilled to the following to what extent do way but we also know that recognition occurs several the cnn classification results on imagenet generalize to stages downstream which suggests that there might be hier object detection results on the pascal voc challenge archical multi stage processes for computing features that we answer this question decisively by bridging the are even more informative for visual recognition chasm between image classification and object detection fukushima neocognitron a biologically this paper is the first to show that a cnn can lead to dra matically higher object detection performance on pascal our system is also quite efficient the only class specific voc as compared to systems based on simpler hog like computations are a reasonably small matrix vector product features achieving this result required solving two prob and greedy non maximum suppression this computational lems localizing objects with a deep network and training a property follows from features that are shared across all cat high capacity model with only a small quantity of annotated egories and that are also two orders of magnitude lower detection data dimensional than previously used region features cf unlike image classification detection requires localiz one advantage of hog like features is their simplic ing likely many objects within an image one approach ity it easier to understand the information they carry al frames localization as a regression problem however work though shows that our intuition can fail us can we from szegedy et al concurrent with our own indi gain insight into the representation learned by the cnn cates that this strategy may not fare well in practice they perhaps the densely connected layers with more than report a map of on voc compared to the million parameters are the key they are not we achieved by our method an alternative is to build a lobotomized the cnn and found that a surprisingly large sliding window detector cnns have been used in this way proportion of its parameters can be removed with for at least two decades typically on constrained object cat only a moderate drop in detection accuracy instead by egories such as faces and pedestrians in order probing units in the network we see that the convolutional to maintain high spatial resolution these cnns typically layers learn a diverse set of rich features figure only have two convolutional and pooling layers we also understanding the failure modes of our approach is also considered adopting a sliding window approach however critical for improving it and so we report results from the units high up in our network which has five convolutional detection analysis tool of hoiem et al as an immedi layers have very large receptive fields pixels ate consequence of this analysis we demonstrate that a sim and strides pixels in the input image which makes ple bounding box regression method significantly reduces precise localization within the sliding window paradigm an mislocalizations which are the dominant error mode open technical challenge before developing technical details we note that be instead we solve the cnn localization problem by op cause r cnn operates on regions it is natural to extend it erating within the recognition using regions paradigm as to the task of semantic segmentation with minor modifi argued for by gu et al in at test time our method cations we also achieve state of the art results on the pas generates around category independent region pro cal voc segmentation task with an average segmentation posals for the input image extracts a fixed length feature accuracy of on the voc test set vector from each proposal using a cnn and then classi fies each region with category specific linear svms we object detection with r cnn use a simple technique affine image warping to compute our object detection system consists of three modules a fixed size cnn input from each region proposal regard the first generates category independent region proposals less of the region shape figure presents an overview of these proposals define the set of candidate detections avail our method and highlights some of our results since our able to our detector the second module is a large convo system combines region proposals with cnns we dub the lutional neural network that extracts a fixed length feature method r cnn regions with cnn features vector from each region the third module is a set of class a second challenge faced in detection is that labeled specific linear svms in this section we present our design data is scarce and the amount currently available is insuffi decisions for each module describe their test time usage cient for training a large cnn the conventional solution to detail how their parameters are learned and show results on this problem is to use unsupervised pre training followed pascal voc by supervised fine tuning e g the second major contribution of this paper is to show that supervised pre module design training on a large auxiliary dataset ilsvrc followed by region proposals a variety of recent papers offer meth domain specific fine tuning on a small dataset pascal ods for generating category independent region proposals is an effective paradigm for learning high capacity cnns examples include objectness selective search when data is scarce in our experiments fine tuning for de category independent object proposals constrained tection improves map performance by percentage points parametric min cuts cpmc multi scale combinatorial after fine tuning our system achieves a map of on grouping and cires an et al who detect mitotic cells voc compared to for the highly tuned hog by applying a cnn to regularly spaced square crops which based deformable part model dpm are a special case of region proposals while r cnn is ag a tech report describing r cnn first appeared at http arxiv nostic to the particular region proposal method we use se org abs in nov lective search to enable a controlled comparison with prior only class specific computations are dot products between features and svm weights and non maximum suppression in practice all dot products for an image are batched into a single matrix matrix product the feature matrix is typi aeroplane bicycle bird car cally and the svm weight matrix is n figure warped training samples from voc train where n is the number of classes this analysis shows that r cnn can scale to thousands detection work e g of object classes without resorting to approximate tech niques such as hashing even if there were classes feature extraction we extract a dimensional fea the resulting matrix multiplication takes only seconds on ture vector from each region proposal using the caffe a modern multi core cpu this efficiency is not merely the implementation of the cnn described by krizhevsky et result of using region proposals and shared features the al features are computed by forward propagating a uva system due to its high dimensional features would mean subtracted rgb image through five con be two orders of magnitude slower while requiring volutional layers and two fully connected layers we refer of memory just to store linear predictors compared to readers to for more network architecture details just for our lower dimensional features in order to compute features for a region proposal we it is also interesting to contrast r cnn with the recent must first convert the image data in that region into a form work from dean et al on scalable detection using dpms that is compatible with the cnn its architecture requires and hashing they report a map of around on voc inputs of a fixed pixel size of the many possi at a run time of minutes per image when introducing ble transformations of our arbitrary shaped regions we opt distractor classes with our approach detectors can for the simplest regardless of the size or aspect ratio of the run in about a minute on a cpu and because no approxi candidate region we warp all pixels in a tight bounding box mations are made map would remain at section around it to the required size prior to warping we dilate the tight bounding box so that at the warped size there are ex training actly p pixels of warped image context around the original box we use p figure shows a random sampling of supervised pre training we discriminatively pre trained warped training regions the supplementary material dis the cnn on a large auxiliary dataset ilsvrc with cusses alternatives to warping image level annotations i e no bounding box labels pre training was performed using the open source caffe cnn test time detection library in brief our cnn nearly matches the perfor at test time we run selective search on the test image mance of krizhevsky et al obtaining a top error rate to extract around region proposals we use selective percentage points higher on the ilsvrc valida search fast mode in all experiments we warp each tion set this discrepancy is due to simplifications in the proposal and forward propagate it through the cnn in or training process der to read off features from the desired layer then for each class we score each extracted feature vector using the domain specific fine tuning to adapt our cnn to the svm trained for that class given all scored regions in an new task detection and the new domain warped voc image we apply a greedy non maximum suppression for windows we continue stochastic gradient descent sgd each class independently that rejects a region if it has an training of the cnn parameters using only warped re intersection over union iou overlap with a higher scoring gion proposals from voc aside from replacing the cnn selected region larger than a learned threshold imagenet specific way classification layer with a ran domly initialized way classification layer for the run time analysis two properties make detection effi voc classes plus background the cnn architecture is un cient first all cnn parameters are shared across all cate changed we treat all region proposals with iou over gories second the feature vectors computed by the cnn lap with a ground truth box as positives for that box class are low dimensional when compared to other common ap and the rest as negatives we start sgd at a learning rate proaches such as spatial pyramids with bag of visual word of 10th of the initial pre training rate which al encodings the features used in the uva detection system lows fine tuning to make progress while not clobbering the for example are two orders of magnitude larger than initialization in each sgd iteration we uniformly sample ours vs dimensional positive windows over all classes and background the result of such sharing is that the time spent com windows to construct a mini batch of size we bias puting region proposals and features image on a gpu the sampling towards positive windows because they are ex or image on a cpu is amortized over all classes the tremely rare compared to background voc test aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv map dpm uva regionlets segdpm r cnn r cnn bb table detection average precision on voc test r cnn is most directly comparable to uva and regionlets since all methods use selective search region proposals bounding box regression bb is described in section at publication time segdpm was the top performer on the pascal voc leaderboard dpm and segdpm use context rescoring not used by the other methods object category classifiers consider training a binary gion proposal algorithm to classify regions their method classifier to detect cars it clear that an image region builds a four level spatial pyramid and populates it with tightly enclosing a car should be a positive example simi densely sampled sift extended opponentsift and rgb larly it clear that a background region which has nothing sift descriptors each vector quantized with word to do with cars should be a negative example less clear codebooks classification is performed with a histogram is how to label a region that partially overlaps a car we re intersection kernel svm compared to their multi feature solve this issue with an iou overlap threshold below which non linear kernel svm approach we achieve a large im regions are defined as negatives the overlap threshold provement in map from to map while also was selected by a grid search over on a being much faster section our method achieves sim validation set we found that selecting this threshold care ilar performance map on voc test fully is important setting it to as in decreased map by points similarly setting it to decreased map visualization ablation and modes of error by points positive examples are defined simply to be the ground truth bounding boxes for each class visualizing learned features once features are extracted and training labels are ap first layer filters can be visualized directly and are easy plied we optimize one linear svm per class since the to understand they capture oriented edges and oppo training data is too large to fit in memory we adopt the nent colors understanding the subsequent layers is more standard hard negative mining method hard neg challenging zeiler and fergus present a visually attrac ative mining converges quickly and in practice map stops tive deconvolutional approach in we propose a simple increasing after only a single pass over all images and complementary non parametric method that directly in supplementary material we discuss why the positive shows what the network learned and negative examples are defined differently in fine tuning the idea is to single out a particular unit feature in the versus svm training we also discuss why it necessary network and use it as if it were an object detector in its own to train detection classifiers rather than simply use outputs right that is we compute the unit activations on a large from the final layer of the fine tuned cnn set of held out region proposals about million sort the proposals from highest to lowest activation perform non results on pascal voc maximum suppression and then display the top scoring re following the pascal voc best practices we gions our method lets the selected unit speak for itself validated all design decisions and hyperparameters on the by showing exactly which inputs it fires on we avoid aver voc dataset section for final results on the aging in order to see different visual modes and gain insight voc datasets we fine tuned the cnn on voc into the invariances computed by the unit train and optimized our detection svms on voc we visualize units from layer which is the max trainval we submitted test results to the evaluation server pooled output of the network fifth and final convolutional only once for each of the two major algorithm variants with layer the feature map is and without bounding box regression dimensional ignoring boundary effects each unit has table shows complete results on voc we com a receptive field of pixels in the original pare our method against four strong baselines including pixel input a central unit has a nearly global view segdpm which combines dpm detectors with the while one near the edge has a smaller clipped support output of a semantic segmentation system and uses ad each row in figure displays the top activations for ditional inter detector context and image classifier rescor a unit from a cnn that we fine tuned on voc ing the most germane comparison is to the uva system trainval six of the functionally unique units are visu from uijlings et al since our systems use the same re alized the supplementary material includes more these figure top regions for six units receptive fields and activation values are drawn in white some units are aligned to concepts such as people row or text other units capture texture and material properties such as dot arrays and specular reflections voc test aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv map r cnn r cnn r cnn r cnn ft r cnn ft r cnn ft r cnn ft bb dpm dpm st dpm hsc 51 table detection average precision on voc test rows show r cnn performance without fine tuning rows show results for the cnn pre trained on ilsvrc and then fine tuned ft on voc trainval row includes a simple bounding box regression bb stage that reduces localization errors section rows present dpm methods as a strong baseline the first uses only hog while the next two use different feature learning approaches to augment or replace hog units were selected to show a representative sample of what tures it multiplies a weight matrix by the the network learns in the second row we see a unit that feature map reshaped as a dimensional vector and fires on dog faces and dot arrays the unit corresponding to then adds a vector of biases this intermediate vector is the third row is a red blob detector there are also detectors component wise half wave rectified x max x for human faces and more abstract patterns such as text and layer is the final layer of the network it is imple triangular structures with windows the network appears mented by multiplying the features computed by by a to learn a representation that combines a small number of weight matrix and similarly adding a vector class tuned features together with a distributed representa of biases and applying half wave rectification tion of shape texture color and material properties the we start by looking at results from the cnn without subsequent fully connected layer has the ability to model fine tuning on pascal i e all cnn parameters were pre a large set of compositions of these rich features trained on ilsvrc only analyzing performance layer by layer table rows reveals that features from ablation studies generalize worse than features from this means performance layer by layer without fine tuning to un that or about million of the cnn parameters derstand which layers are critical for detection performance can be removed without degrading map more surprising is we analyzed results on the voc dataset for each of the that removing both and produces quite good results cnn last three layers layer was briefly described even though features are computed using only of in section the final two layers are summarized below the cnn parameters much of the cnn representational layer is fully connected to to compute fea power comes from its convolutional layers rather than from the much larger densely connected layers this finding sug see how our error types compare with dpm a full sum gests potential utility in computing a dense feature map in mary of the analysis tool is beyond the scope of this pa the sense of hog of an arbitrary sized image by using only per and we encourage readers to consult to understand the convolutional layers of the cnn this representation some finer details such as normalized ap since the would enable experimentation with sliding window detec analysis is best absorbed in the context of the associated tors including dpm on top of features plots we present the discussion within the captions of fig ure and figure performance layer by layer with fine tuning we now r cnn animals r cnn ft animals r cnn ft bb animals look at results from our cnn after having fine tuned its pa rameters on voc trainval the improvement is strik 6400 the first dpm feature learning method dpm st total false positives total false positives total false positives augments hog features with histograms of sketch token figure distribution of top ranked false positive fp types probabilities intuitively a sketch token is a tight distri each plot shows the evolving distribution of fp types as more fps bution of contours passing through the center of an image are considered in order of decreasing score each fp is catego patch sketch token probabilities are computed at each pixel rized into of types loc poor localization a detection with by a random forest that was trained to classify pixel an iou overlap with the correct class between and or a du patches into one of sketch tokens or background plicate sim confusion with a similar category oth confusion with a dissimilar object category bg a fp that fired on back the second method dpm hsc replaces hog with ground compared with dpm see significantly more of histograms of sparse codes hsc to compute an hsc our errors result from poor localization rather than confusion with sparse code activations are solved for at each pixel using background or other object classes indicating that the cnn fea a learned dictionary of pixel grayscale atoms tures are much more discriminative than hog loose localiza the resulting activations are rectified in three ways full and tion likely results from our use of bottom up region proposals and both half waves spatially pooled unit normalized and the positional invariance learned from pre training the cnn for then power transformed x sign x x α whole image classification column three shows how our simple all r cnn variants strongly outperform the three dpm bounding box regression method fixes many localization errors baselines table rows including the two that use feature learning compared to the latest version of dpm bounding box regression which uses only hog features our map is more than percentage points higher vs a rela based on the error analysis we implemented a simple tive improvement the combination of hog and sketch to method to reduce localization errors inspired by the bound kens yields map points over hog alone while hsc ing box regression employed in dpm we train a linear improves over hog by map points when compared regression model to predict a new detection window given internally to their private dpm baselines both use non the features for a selective search region proposal public implementations of dpm that underperform the open full details are given in the supplementary material re source version these methods achieve maps of sults in table table and figure show that this simple and respectively approach fixes a large number of mislocalized detections boosting map by to points detection error analysis semantic segmentation we applied the excellent detection analysis tool from hoiem et al in order to reveal our method error region classification is a standard technique for seman modes understand how fine tuning changes them and to tic segmentation allowing us to easily apply r cnn to the r cnn sensitivity and impact r cnn ft sensitivity and impact r cnn ft bb sensitivity and impact dpm voc sensitivity and impact figure sensitivity to object characteristics each plot shows the mean over classes normalized ap see for the highest and lowest performing subsets within six different object characteristics occlusion truncation bounding box area aspect ratio viewpoint part visibility we show plots for our method r cnn with and without fine tuning ft and bounding box regression bb as well as for dpm voc overall fine tuning does not reduce sensitivity the difference between max and min but does substantially improve both the highest and lowest performing subsets for nearly all characteristics this indicates that fine tuning does more than simply improve the lowest performing subsets for aspect ratio and bounding box area as one might conjecture based on how we warp network inputs instead fine tuning improves robustness for all characteristics including occlusion truncation viewpoint and part visibility pascal voc segmentation challenge to facilitate a di full r cnn fg r cnn full fg r cnn rect comparison with the current leading semantic segmen p tation system called p for second order pooling we work within their open source framework p uses table segmentation mean accuracy on voc vali cpmc to generate region proposals per image and then dation column presents p use our cnn pre trained on predicts the quality of each region for each class using ilsvrc support vector regression svr the high performance of their approach is due to the quality of the cpmc regions always outperforms and the following discussion refers and the powerful second order pooling of multiple feature to the features the fg strategy slightly outperforms full types enriched variants of sift and lbp we also note indicating that the masked region shape provides a stronger that farabet et al recently demonstrated good results signal matching our intuition however full fg achieves on several dense scene labeling datasets not including pas an average accuracy of our best result by a mar cal using a cnn as a multi scale per pixel classifier gin of also modestly outperforming p indicating we follow and extend the pascal segmentation that the context provided by the full features is highly infor training set to include the extra annotations made available mative even given the fg features notably training the by hariharan et al design decisions and hyperparam svrs on our full fg features takes an hour on a single core eters were cross validated on the voc validation set compared to hours for training on p features final test results were evaluated only once in table we present results on the voc test set comparing our best performing method full fg cnn features for segmentation we evaluate three strate against two strong baselines our method achieves the high gies for computing features on cpmc regions all of which est segmentation accuracy for out of categories and begin by warping the rectangular window around the re the highest overall segmentation accuracy of aver gion to the first strategy full ignores the re aged across categories but likely ties with the p result gion shape and computes cnn features directly on the under any reasonable margin of error still better perfor warped window exactly as we did for detection however mance could likely be achieved by fine tuning these features ignore the non rectangular shape of the re gion two regions might have very similar bounding boxes conclusion while having very little overlap therefore the second strat egy fg computes cnn features only on a region fore in recent years object detection performance had stag ground mask we replace the background with the mean nated the best performing systems were complex en input so that background regions are zero after mean sub sembles combining multiple low level image features with traction the third strategy full fg simply concatenates high level context from object detectors and scene classi the full and fg features our experiments validate their com fiers this paper presents a simple and scalable object de plementarity tection algorithm that gives a relative improvement over the best previous results on pascal voc results on voc table shows a summary of our we achieved this performance through two insights the results on the voc validation set compared with p first is to apply high capacity convolutional neural net see supplementary material for complete per category re works to bottom up region proposals in order to localize sults within each feature computation strategy layer and segment objects the second is a paradigm for train table segmentation accuracy on voc test we compare against two strong baselines the regions and parts r p method of and the second order pooling p method of without any fine tuning our cnn achieves top segmentation perfor mance outperforming r p and roughly matching p introduction in the last three years our object classification and detection capabilities have dramatically improved due to advances in deep learning and convolutional networks one encouraging news is that most of this progress is not just the result of more powerful hardware larger datasets and bigger models but mainly a consequence of new ideas algorithms and improved network architectures no new data sources were used for example by the top entries in the ilsvrc competition besides the classification dataset of the same competition for detection purposes our googlenet submission to ilsvrc actually uses times fewer parameters than the winning architecture of krizhevsky et al from two years ago while being significantly more accurate on the object detection front the biggest gains have not come from naive application of bigger and bigger deep networks but from the synergy of deep architectures and classical computer vision like the r cnn algorithm by girshick et al another notable factor is that with the ongoing traction of mobile and embedded computing the efficiency of our algorithms especially their power and memory use gains importance it is noteworthy that the considerations leading to the design of the deep architecture presented in this paper included this factor rather than having a sheer fixation on accuracy numbers for most of the experiments the models were designed to keep a computational budget of billion multiply adds at inference time so that the they do not end up to be a purely academic curiosity but could be put to real world use even on large datasets at a reasonable cost in this paper we will focus on an efficient deep neural network architecture for computer vision codenamed inception which derives its name from the network in network paper by lin et al in conjunction with the famous we need to go deeper internet meme in our case the word deep is used in two different meanings first of all in the sense that we introduce a new level of organization in the form of the inception module and also in the more direct sense of increased network depth in general one can view the inception model as a logical culmination of while taking inspiration and guidance from the theoretical work by arora et al the benefits of the architecture are experimentally verified on the ilsvrc classification and detection challenges where it significantly outperforms the current state of the art related work starting with lenet convolutional neural networks cnn have typically had a standard structure stacked convolutional layers optionally followed by con trast normalization and max pooling are followed by one or more fully connected layers variants of this basic design are prevalent in the image classification literature and have yielded the best results to date on mnist cifar and most notably on the imagenet classification challenge for larger datasets such as imagenet the recent trend has been to increase the number of layers and layer size while using dropout to address the problem of overfitting despite concerns that max pooling layers result in loss of accurate spatial information the same convolutional network architecture as has also been successfully employed for localization object detection and human pose estimation inspired by a neuroscience model of the primate visual cortex serre et al used a series of fixed gabor filters of different sizes to handle multiple scales we use a similar strategy here however contrary to the fixed layer deep model of all filters in the inception architecture are learned furthermore inception layers are repeated many times leading to a layer deep model in the case of the googlenet model network in network is an approach proposed by lin et al in order to increase the representational power of neural networks in their model additional convolutional layers are added to the network increasing its depth we use this approach heavily in our architecture however in our setting convolutions have dual purpose most critically they are used mainly as dimension reduction modules to remove computational bottlenecks that would otherwise limit the size of our networks this allows for not just increasing the depth but also the width of our networks without a significant performance penalty finally the current state of the art for object detection is the regions with convolutional neural networks r cnn method by girshick et al r cnn decomposes the overall detection problem into two subproblems utilizing lowlevel cues such as color and texture in order to generate object location proposals in a category agnostic fashion and using cnn classifiers to identify object categories at those locations such a two stage approach leverages the accuracy of bounding box segmentation with low level cues as well as the highly powerful classification power of state ofthe art cnns we adopted a similar pipeline in our detection submissions but have explored enhancements in both stages such as multi box prediction for higher object bounding box recall and ensemble approaches for better categorization of bounding box proposals motivation and high level considerations the most straightforward way of improving the performance of deep neural networks is by increasing their size this includes both increasing the depth the number of net figure two distinct classes from the classes of the ilsvrc classification challenge domain knowledge is required to distinguish between these classes work levels as well as its width the number of units at each level this is an easy and safe way of training higher quality models especially given the availability of a large amount of labeled training data however this simple solution comes with two major drawbacks bigger size typically means a larger number of parameters which makes the enlarged network more prone to overfitting especially if the number of labeled examples in the training set is limited this is a major bottleneck as strongly labeled datasets are laborious and expensive to obtain often requiring expert human raters to distinguish between various fine grained visual categories such as those in imagenet even in the class ilsvrc subset as shown in figure the other drawback of uniformly increased network size is the dramatically increased use of computational resources for example in a deep vision network if two convolutional layers are chained any uniform increase in the number of their filters results in a quadratic increase of computation if the added capacity is used inefficiently for example if most weights end up to be close to zero then much of the computation is wasted as the computational budget is always finite an efficient distribution of computing resources is preferred to an indiscriminate increase of size even when the main objective is to increase the quality of performance a fundamental way of solving both of these issues would be to introduce sparsity and replace the fully connected layers by the sparse ones even inside the convolutions besides mimicking biological systems this would also have the advantage of firmer theoretical underpinnings due to the groundbreaking work of arora et al their main result states that if the probability distribution of the dataset is representable by a large very sparse deep neural network then the optimal network topology can be constructed layer after layer by analyzing the correlation statistics of the preceding layer activations and clustering neurons with highly correlated outputs although the strict mathematical proof requires very strong conditions the fact that this statement resonates with the well known hebbian principle neurons that fire together wire together suggests that the underlying idea is applicable even under less strict conditions in practice unfortunately today computing infrastructures are very inefficient when it comes to numerical calculation on non uniform sparse data structures even if the number of arithmetic operations is reduced by the overhead of lookups and cache misses would dominate switching to sparse matrices might not pay off the gap is widened yet further by the use of steadily improving and highly tuned numerical libraries that allow for extremely fast dense matrix multiplication exploiting the minute details of the underlying cpu or gpu hardware also non uniform sparse models require more sophisticated engineering and computing infrastructure most current vision oriented machine learning systems utilize sparsity in the spatial domain just by the virtue of employing convolutions however convolutions are implemented as collections of dense connections to the patches in the earlier layer convnets have traditionally used random and sparse connection tables in the feature dimensions since in order to break the symmetry and improve learning yet the trend changed back to full connections with in order to further optimize parallel computation current state of the art architectures for computer vision have uniform structure the large number of filters and greater batch size allows for the efficient use of dense computation this raises the question of whether there is any hope for a next intermediate step an architecture that makes use of filter level sparsity as suggested by the theory but exploits our current hardware by utilizing computations on dense matrices the vast literature on sparse matrix computations e g suggests that clustering sparse matrices into relatively dense submatrices tends to give competitive performance for sparse matrix multiplication it does not seem far fetched to think that similar methods would be utilized for the automated construction of non uniform deeplearning architectures in the near future the inception architecture started out as a case study for assessing the hypothetical output of a sophisticated network topology construction algorithm that tries to approximate a sparse structure implied by for vision networks and covering the hypothesized outcome by dense readily available components despite being a highly speculative undertaking modest gains were observed early on when compared with reference networks based on with a bit of tuning the gap widened and inception proved to be especially useful in the context of localization and object detection as the base network for and interestingly while most of the original architectural choices have been questioned and tested thoroughly in separation they turned out to be close to optimal locally one must be cautious though although the inception architecture has become a success for computer vision it is still questionable whether this can be attributed to the guiding principles that have lead to its construction making sure of this would require a much more thorough analysis and verification architectural details the main idea of the inception architecture is to consider how an optimal local sparse structure of a convolutional vision network can be approximated and covered by readily available dense components note that assuming translation invariance means that our network will be built from convolutional building blocks all we need is to find the optimal local construction and to repeat it spatially arora et al suggests a layer by layer construction where one should analyze the correlation statistics of the last layer and cluster them into groups of units with high correlation these clusters form the units of the next layer and are connected to the units in the previous layer we assume that each unit from an earlier layer corresponds to some region of the input image and these units are grouped into filter banks in the lower layers the ones close to the input correlated units would concentrate in local regions thus we would end up with a lot of clusters concentrated in a single region and they can be covered by a layer of convolutions in the next layer as suggested in however one can also expect that there will be a smaller number of more spatially spread out clusters that can be covered by convolutions over larger patches and there will be a decreasing number of patches over larger and larger regions in order to avoid patch alignment issues current incarnations of the inception architecture are restricted to filter sizes and this decision was based more on convenience rather than necessity it also means that the suggested architecture is a combination of all those layers with their output filter banks concatenated into a single output vector forming the input of the next stage additionally since pooling operations have been essential for the success of current convolutional networks it suggests that adding an alternative parallel pooling path in each such stage should have additional beneficial effect too see figure a as these inception modules are stacked on top of each other their output correlation statistics are bound to vary as features of higher abstraction are captured by higher layers their spatial concentration is expected to decrease this suggests that the ratio of and convolutions should increase as we move to higher layers one big problem with the above modules at least in this na ıve form is that even a modest number of convolutions can be prohibitively expensive on top of a convolutional layer with a large number of filters this problem becomes even more pronounced once pooling units are added to the mix the number of output filters equals to the convolutions convolutions convolutions filter concatenation previous layer max pooling a inception module na ıve version convolutions convolutions convolutions filter concatenation previous layer convolutions convolutions max pooling convolutions b inception module with dimensionality reduction figure inception module ber of filters in the previous stage the merging of output of the pooling layer with outputs of the convolutional layers would lead to an inevitable increase in the number of outputs from stage to stage while this architecture might cover the optimal sparse structure it would do it very inefficiently leading to a computational blow up within a few stages this leads to the second idea of the inception architecture judiciously reducing dimension wherever the computational requirements would increase too much otherwise this is based on the success of embeddings even low dimensional embeddings might contain a lot of information about a relatively large image patch however embeddings represent information in a dense compressed form and compressed information is harder to process the representation should be kept sparse at most places as required by the conditions of and compress the signals only whenever they have to be aggregated en masse that is convolutions are used to compute reductions before the expensive and convolutions besides being used as reductions they also include the use of rectified linear activation making them dual purpose the final result is depicted in figure b in general an inception network is a network consisting of modules of the above type stacked upon each other with occasional max pooling layers with stride to halve the resolution of the grid for technical reasons memory efficiency during training it seemed beneficial to start using inception modules only at higher layers while keeping the lower layers in traditional convolutional fashion this is not strictly necessary simply reflecting some infrastructural inefficiencies in our current implementation a useful aspect of this architecture is that it allows for increasing the number of units at each stage significantly without an uncontrolled blow up in computational complexity at later stages this is achieved by the ubiquitous use of dimensionality reduction prior to expensive convolutions with larger patch sizes furthermore the design follows the practical intuition that visual information should be processed at various scales and then aggregated so that the next stage can abstract features from the different scales simultaneously the improved use of computational resources allows for increasing both the width of each stage as well as the number of stages without getting into computational difficulties one can utilize the inception architecture to create slightly inferior but computationally cheaper versions of it we have found that all the available knobs and levers allow for a controlled balancing of computational resources resulting in networks that are faster than similarly performing networks with non inception architecture however this requires careful manual design at this point googlenet by the googlenet name we refer to the particular incarnation of the inception architecture used in our submission for the ilsvrc competition we also used one deeper and wider inception network with slightly superior quality but adding it to the ensemble seemed to improve the results only marginally we omit the details of that network as empirical evidence suggests that the influence of the exact architectural parameters is relatively minor table illustrates the most common instance of inception used in the competition this network trained with different imagepatch sampling methods was used for out of the models in our ensemble all the convolutions including those inside the inception modules use rectified linear activation the size of the receptive field in our network is in the rgb color space with zero mean reduce and reduce stands for the number of filters in the reduction layer used before the and convolutions one can see the number of filters in the projection layer after the built in max pooling in the pool proj column all these reduction projection layers use rectified linear activation as well the network was designed with computational efficiency and practicality in mind so that inference can be run on individual devices including even those with limited computational resources especially with low memory footprint type patch size stride output size depth reduce reduce pool proj params ops convolution max pool convolution max pool inception inception max pool inception 208 inception inception inception 580k inception 840k max pool inception inception avg pool dropout linear softmax table googlenet incarnation of the inception architecture the network is layers deep when counting only layers with parameters or layers if we also count pooling the overall number of layers independent building blocks used for the construction of the network is about the exact number depends on how layers are counted by the machine learning infrastructure the use of average pooling before the classifier is based on although our implementation has an additional linear layer the linear layer enables us to easily adapt our networks to other label sets however it is used mostly for convenience and we do not expect it to have a major effect we found that a move from fully connected layers to average pooling improved the top accuracy by about however the use of dropout remained essential even after removing the fully connected layers given relatively large depth of the network the ability to propagate gradients back through all the layers in an effective manner was a concern the strong performance of shallower networks on this task suggests that the features produced by the layers in the middle of the network should be very discriminative by adding auxiliary classifiers connected to these intermediate layers discrimination in the lower stages in the classifier was expected this was thought to combat the vanishing gradient problem while providing regularization these classifiers take the form of smaller convolutional networks put on top of the output of the inception and modules during training their loss gets added to the total loss of the network with a discount weight the losses of the auxiliary classifiers were weighted by at inference time these auxiliary networks are discarded later control experiments have shown that the effect of the auxiliary networks is relatively minor around and that it required only one of them to achieve the same effect the exact structure of the extra network on the side including the auxiliary classifier is as follows an average pooling layer with filter size and stride resulting in an output for the and for the stage a convolution with filters for dimension reduction and rectified linear activation a fully connected layer with units and rectified linear activation a dropout layer with ratio of dropped outputs a linear layer with softmax loss as the classifier predicting the same classes as the main classifier but removed at inference time a schematic view of the resulting network is depicted in figure training methodology googlenet networks were trained using the distbelief distributed machine learning system using modest amount of model and data parallelism although we used a cpu based implementation only a rough estimate suggests that the googlenet network could be trained to convergence using few high end gpus within a week the main limitation being the memory usage our training used asynchronous stochastic gradient descent with momentum fixed learning rate schedule decreasing the learning rate by every epochs polyak averaging was used to create the final model used at inference time image sampling methods have changed substantially over the months leading to the competition and already converged models were trained on with other options sometimes in conjunction with changed hyperparameters such as dropout and the learning rate therefore it is hard to give a definitive guidance to the most effective single way to train these networks to complicate matters further some of the models were mainly trained on smaller relative crops others on larger ones inspired by still one prescription that was verified to work very well after the competition includes sampling of various sized patches of the image whose size is distributed evenly between and of the image area with aspect ratio constrained to the interval also we found that the photometric distortions of andrew howard were useful to combat overfitting to the imaging conditions of training data ilsvrc classification challenge setup and results the ilsvrc classification challenge involves the task of classifying the image into one of leaf node categories in the imagenet hierarchy there are about million images for training for validation and images for testing each image is associated with one ground truth category and performance is measured based on the highest scoring classifier predictions two numbers are usually reported the top accuracy rate which compares the ground truth against the first predicted class and the top error rate which compares the ground truth against the first predicted classes an image is deemed correctly classified if the ground truth is among the top regardless of its rank in them the challenge uses the top error rate for ranking purposes input googlenet network with all the bells and whistles we participated in the challenge with no external data used for training in addition to the training techniques aforementioned in this paper we adopted a set of techniques during testing to obtain a higher performance which we describe next we independently trained versions of the same googlenet model including one wider version and performed ensemble prediction with them these models were trained with the same initialization even with the same initial weights due to an oversight and learning rate policies they differed only in sampling methodologies and the randomized input image order during testing we adopted a more aggressive cropping approach than that of krizhevsky et al specifically we resized the image to scales where the shorter dimension height or width is 320 and respectively take the left center and right square of these resized images in the case of portrait images we take the top center and bottom squares for each square we then take the corners and the center crop as well as the square resized to and their mirrored versions this leads to crops per image a similar approach was used by andrew howard in the previous year entry which we empirically verified to perform slightly worse than the proposed scheme we note that such aggressive cropping may not be necessary in real applications as the benefit of more crops becomes marginal after a reasonable number of crops are present as we will show later on the softmax probabilities are averaged over multiple crops and over all the individual classifiers to obtain the final prediction in our experiments we analyzed alternative approaches on the validation data such as max pooling over crops and averaging over classifiers but they lead to inferior performance than the simple averaging in the remainder of this paper we analyze the multiple factors that contribute to the overall performance of the final submission our final submission to the challenge obtains a top error of on both the validation and testing data ranking the first among other participants this is a relative reduction compared to the supervision approach in and about relative reduction compared to the previous year best approach clarifai both of which used external data for training the classifiers table shows the statistics of some of the top performing approaches over the past years we also analyze and report the performance of multiple testing choices by varying the number of models and the team year place error top uses external data supervision no supervision imagenet clarifai no clarifai imagenet msra no vgg no googlenet no table classification performance number of models number of crops cost top error compared to base base table googlenet classification performance break down number of crops used when predicting an image in table when we use one model we chose the one with the lowest top error rate on the validation data all numbers are reported on the validation dataset in order to not overfit to the testing data statistics ilsvrc detection challenge setup and results the ilsvrc detection task is to produce bounding boxes around objects in images among possible classes detected objects count as correct if they match the class of the groundtruth and their bounding boxes overlap by at least using the jaccard index extraneous detections count as false positives and are penalized contrary to the classification task each image may contain many objects or none and their scale may vary results are reported using the mean average precision map the approach taken by googlenet for detection is similar to the r cnn by but is augmented with the inception model as the region classifier additionally the region proposal step is improved by combining the selective search approach with multibox predictions for higher object bounding box recall in order to reduce the number of false positives the superteam year place map external data ensemble approach uva euvision none fisher vectors deep insight imagenet cnn cuhk deepid net imagenet cnn googlenet imagenet cnn table comparison of detection performances unreported values are noted with question marks pixel size was increased by this halves the proposals coming from the selective search algorithm we added back region proposals coming from multi box resulting in total in about of the proposals used by while increasing the coverage from to the overall effect of cutting the number of proposals with increased coverage is a improvement of the mean average precision for the single model case finally we use an ensemble of googlenets when classifying each region this leads to an increase in accuracy from to note that contrary to r cnn we did not use bounding box regression due to lack of time we first report the top detection results and show the progress since the first edition of the detection task compared to the result the accuracy has almost doubled the top performing teams all use convolutional networks we report the official scores in table and common strategies for each team the use of external data ensemble models or contextual models the external data is typically the classification data for pre training a model that is later refined on the detection data some teams also mention the use of the localization data since a good portion of the localization task bounding boxes are not included in the detection dataset one can pre train a general bounding box regressor with this data the same way classification is used for pre training the googlenet entry did not use the localization data for pretraining in table we compare results using a single model only the top performing model is by deep insight and surprisingly only improves by points with an ensemble of models while the googlenet obtains significantly stronger results with the ensemble conclusions our results yield a solid evidence that approximating the expected optimal sparse structure by readily available dense building blocks is a viable method for improving neural networks for computer vision the main advantage of this method is a significant quality gain at a modest increase of computational requirements compared to shallower and narrower architectures our object detection work was competitive despite not team map contextual model bounding box regression trimps soushen no berkeley vision no yes uvaeuvision cuhk no googlenet no no deep insight yes yes table single model performance for detection utilizing context nor performing bounding box regression suggesting yet further evidence of the strengths of the inception architecture for both classification and detection it is expected that similar quality of result can be achieved by much more expensive non inception type networks of similar depth and width still our approach yields solid evidence that moving to sparser architectures is feasible and useful idea in general this suggest future work towards creating sparser and more refined structures in automated ways on the basis of as well as on applying the insights of the inception architecture to other domains superpixel algorithms group pixels into perceptually meaningful atomic regions which can be used to replace the rigid structure of the pixel grid fig they capture image redundancy provide a convenient primitive from which to compute image features and greatly reduce the complexity of subsequent image processing tasks they have become key building blocks of many computer vision algorithms such as top scoring multiclass object segmentation entries to the pascal voc challenge depth estimation segmentation body model estimation and object localization there are many approaches to generating superpixels each with its own advantages and drawbacks that may be better suited to a particular application for example if adherence to image boundaries is of paramount importance the graph based method of may be an ideal choice however if superpixels are to be used to build a graph a method that produces a more regular lattice such as is probably a better choice while it is difficult to define what constitutes an ideal approach for all applications we believe the following properties are generally desirable superpixels should adhere well to image boundaries when used to reduce computational complexity as a preprocessing step superpixels should be fast to compute memory efficient and simple to use when used for segmentation purposes superpixels should both increase the speed and improve the quality of the results we therefore performed an empirical comparison of five stateof the art superpixel methods evaluating their speed ability to adhere to image boundaries and impact on segmentation performance we also provide a qualitative review of these and other superpixel methods our conclusion is that no existing method is satisfactory in all regards to address this we propose a new superpixel algorithm simple linear iterative clustering slic which adapts k means clustering to generate superpixels in a manner similar to while strikingly simple slic is shown to yield state of the art adherence to image boundaries on the berkeley benchmark and outperforms existing methods when used for segmentation on the pascal and msrc data sets furthermore it is faster and more memory efficient than existing methods in addition to these quantifiable benefits slic is easy to use offers flexibility in the compactness and number of the superpixels it generates is straightforward to extend to higher dimensions and is freely available existing superpixel methods algorithms for generating superpixels can be broadly categorized as either graph based or gradient ascent methods below we review popular superpixel methods for each of these categories including some that were not originally designed specifically to generate superpixels table provides a qualitative and quantitative summary of the reviewed methods including their relative performance graph based algorithms graph based approaches to superpixel generation treat each pixel as a node in a graph edge weights between two nodes are proportional to the similarity between neighboring pixels superpixels are created by minimizing a cost function defined over the graph the normalized cuts algorithm recursively partitions a graph of all pixels in the image using contour and texture cues globally minimizing a cost function defined on the edges at the partition boundaries it produces very regular visually pleasing superpixels however the boundary adherence of is relatively poor and it is the slowest among the methods particularly for large images although attempts to speed up the algorithm exist has a complexity of þ where n is the number of pixels felzenszwalb and huttenlocher propose an alternative graph based approach that has been applied to generate superpixels it performs an agglomerative clustering of pixels as nodes on a graph such that each superpixel is the minimum spanning tree of the constituent pixels adheres well to image boundaries in practice but produces superpixels with very irregular sizes and shapes it is oðn lognþ complex and fast in ieee transactions on pattern analysis and machine intelligence vol no november r achanta a shaji and s su sstrunk are with the images and visual representation group school of computer and communication sciences ecole polytechnique fe de rale de lausanne station epfl ch lausanne switzerland e mail radhakrishna achanta appu shaji sabine susstrunk epfl ch k smith is with eth swiss federal institute of technology zurich light microscopy center institute for biochemistry schafmattstrasse ethzhonggerberg ch zurich switzerland e mail kevin smith lmc biol ethz ch a lucchi and p fua are with the computer vision laboratory school of computer and communication sciences ecole polytechnique fe de rale de lausanne station bc epfl ch lausanne switzerland e mail aureline lucchi pascal fua epfl ch manuscript received may revised jan accepted may published online may recommended for acceptance by p felzenszwalb for information on obtaining reprints of this article please send e mail to tpami computer org and reference ieeecs log number tpami digital object identifier no tpami cross platform executables and source code for slic superpixels and supervoxels can be found at http ivrg epfl ch research superpixels ieee published by the ieee computer society practice however it does not offer an explicit control over the amount of superpixels or their compactness moore et al propose a method to generate superpixels that conform to a grid by finding optimal paths or seams that split the image into smaller vertical or horizontal regions optimal paths are found using a graph cuts method similar to seam carving while the complexity of is lognþ according to the authors this does not account for the precomputed boundary maps which strongly influence the quality and speed of the output and in veksler et al use a global optimization approach similar to the texture synthesis work of superpixels are obtained by stitching together overlapping image patches such that each pixel belongs to only one of the overlapping regions they suggest two variants of their method one for generating compact superpixels and one for constantintensity superpixels gradient ascent based algorithms starting from a rough initial clustering of pixels gradient ascent methods iteratively refine the clusters until some convergence criterion is met to form superpixels in mean shift an iterative mode seeking procedure for locating local maxima of a density function is applied to find modes in the color or intensity feature space of an image pixels that converge to the same mode define the superpixels is an older approach producing irregularly shaped superpixels of nonuniform size it is complex making it relatively slow and does not offer direct control over the amount size or compactness of superpixels quick shift also uses a mode seeking segmentation scheme it initializes the segmentation using a medoid shift procedure it then moves each point in the feature space to the nearest neighbor that increases the parzen density estimate while it has relatively good boundary adherence is quite slow with an complexity d is a small constant does not allow for explicit control over the size or number of superpixels previous works have used for object localization and motion segmentation the watershed approach performs a gradient ascent starting from local minima to produce watersheds lines that separate catchment basins the resulting superpixels are often highly irregular in size and shape and do not exhibit good boundary adherence the approach of is relatively fast oðn lognþ complexity but does not offer control over the amount of superpixels or their compactness the turbopixel method progressively dilates a set of seed locations using level set based geometric flow the geometric flow relies on local image gradients aiming to regularly distribute superpixels on the image plane unlike superpixels are constrained to have uniform size compactness and boundary adherence relies on algorithms of varying complexity but in practice as the authors claim has approximately oðnþ behavior however it is among the slowest algorithms examined and exhibits relatively poor boundary adherence slic superpixels we propose a new method for generating superpixels which is faster than existing methods more memory efficient exhibits state of the art boundary adherence and improves the performance of segmentation algorithms simple linear iterative clustering is an adaptation of k means for superpixel generation with two important distinctions the number of distance calculations in the optimization is dramatically reduced by limiting the search space to a region proportional to the superpixel size this reduces the complexity to be linear in the number of pixels n and independent of the number of superpixels k a weighted distance measure combines color and spatial proximity while simultaneously providing control over the size and compactness of the superpixels slic is similar to the approach used as a preprocessing step for depth estimation described in which was not fully explored in the context of superpixel generation algorithm slic is simple to use and understand by default the only parameter of the algorithm is k the desired number of approximately equally sized superpixels for color images in the cielab color space the clustering procedure begins with an initialization step where k initial cluster centers ci ai bi xi yi t are sampled on a regular grid spaced s pixels apart to produce roughly equally sized superpixels the grid interval is s ffiffiffiffiffiffiffiffiffiffi n k p the centers are moved to seed locations corresponding to the lowest gradient position in a neighborhood this is done to avoid centering a superpixel on an edge and to reduce the chance of seeding a superpixel with a noisy pixel next in the assignment step each pixel i is associated with the nearest cluster center whose search region overlaps its location as depicted in fig this is the key to speeding up our algorithm because limiting the size of the search region significantly reduces the number of distance calculations and results in a significant speed advantage over conventional k means clustering where each pixel must be compared with all cluster centers this is only possible through the introduction of a distance measure d which determines the nearest cluster center for each pixel as discussed in section since the expected spatial extent of a superpixel is a region of approximate size s s the search for similar pixels is done in a region around the superpixel center once each pixel has been associated to the nearest cluster center an update step adjusts the cluster centers to be the mean a b x y t vector of all the pixels belonging to the cluster the norm is used to compute a residual error e between the new cluster center locations and previous cluster center locations the assignment and update steps can be repeated iteratively until the error converges but we have found that iterations suffices for most images and report all results in this paper using this criteria finally a postprocessing step enforces connectivity by reassigning disjoint pixels to nearby superpixels the entire algorithm is summarized in algorithm ieee transactions on pattern analysis and machine intelligence vol no november optionally the compactness of the superpixels can be controlled by adjusting m which is discussed in section fig images segmented using slic into superpixels of size and pixels approximately algorithm slic superpixel segmentation initialization initialize cluster centers ck ak bk xk yk t by sampling pixels at regular grid steps s move cluster centers to the lowest gradient position in a neighborhood set label lðiþ for each pixel i set distance dðiþ for each pixel i repeat assignment for each cluster center ck do for each pixel i in a region around ck do compute the distance d between ck and i if d dðiþ then set dðiþ d set lðiþ k end if end for end for update compute new cluster centers compute residual error e until e threshold distance measure slic superpixels correspond to clusters in the labxy color image plane space this presents a problem in defining the distance measure d which may not be immediately obvious d computes the distance between a pixel i and cluster center ck in algorithm a pixel color is represented in the cielab color space a b t whose range of possible values is known the pixel position position y t on the other hand may take a range of values that varies according to the size of the image simply defining d to be the euclidean distance in labxy space will cause inconsistencies in clustering behavior for different superpixel sizes for large superpixels spatial distances outweigh color proximity giving more relative importance to spatial proximity than color this produces compact superpixels that do not adhere well to image boundaries for smaller superpixels the converse is true to combine the two distances into a single measure it is necessary to normalize color proximity and spatial proximity by their respective maximum distances within a cluster ns and nc doing so is written dc ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ðlj þ ðaj þ ðbj q ds ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ðxj þ ðyj q ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi dc nc þ ds ns the maximum spatial distance expected within a given cluster should correspond to the sampling interval ns s ffiffi ð p n kþ determining the maximum color distance nc is not so straightforward as color distances can vary significantly from cluster to cluster and image to image this problem can be avoided by fixing nc to a constant m so that becomes ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi dc m þ ds s ieee transactions on pattern analysis and machine intelligence vol no november fig reducing the superpixel search regions the complexity of slic is linear in the number of pixels in the image oðnþ while the conventional k means algorithm is oðkniþ where i is the number of iterations this is achieved by limiting the search space of each cluster center in the assignment step a in the conventional k means algorithm distances are computed from each cluster center to every pixel in the image b slic only computes distances from each cluster center to pixels within a 2s region note that the expected superpixel size is only s s indicated by the smaller square this approach not only reduces distance computations but also makes slic complexity independent of the number of superpixels table summary of existing superpixel algorithms the ability of a superpixel method to adhere to boundaries found in the berkeley data set is measured according to two standard metrics under segmentation error and boundary recall for superpixels we also report the average time required to segment images using an intel dual core ghz processor with gb ram and the class averaged segmentation accuracy obtained on the msrc data set using the method described in bold entries indicate best performance in each category ability to specify the amount of superpixels control their compactness and ability to generate supervoxels is also provided which simplifies to the distance measure we use in practice d ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi dc þ ds s ð3þ by defining d in this manner m also allows us to weigh the relative importance between color similarity and spatial proximity when m is large spatial proximity is more important and the resulting superpixels are more compact i e they have a lower area to perimeter ratio when m is small the resulting superpixels adhere more tightly to image boundaries but have less regular size and shape when using the cielab color space m can be in the range equation can be adapted for grayscale images by setting dc ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ðlj q it can also be extended to handle supervoxels as depicted in fig by including the depth dimension to the spatial proximity term of ds ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ðxi þ ðyi yjþ þ ðzi q postprocessing like some other superpixel algorithms slic does not explicitly enforce connectivity at the end of the clustering procedure some orphaned pixels that do not belong to the same connected component as their cluster center may remain to correct for this such pixels are assigned the label of the nearest cluster center using a connected components algorithm complexity by localizing the search in the clustering procedure slic avoids performing thousands of redundant distance calculations in practice a pixel falls in the neighborhood of less than eight cluster centers meaning that slic is oðnþ complex in contrast the trivial upper bound for the classical k means algorithm is oðknþ and the practical time complexity is oðnkiþ where i is the number of iterations required for convergence while schemes to reduce the complexity of k means have been proposed using prime number length sampling random sampling local cluster swapping and by setting lower and upper bounds these methods are very general in nature slic is specifically tailored to the problem of superpixel clustering finally unlike most superpixel methods and the aforementioned approaches to speed up k means the complexity of slic is linear in the number of pixels irrespective of k comparison with state of the art we performed a quantitative comparison of slic and five state ofthe art superpixel methods using publicly available source code these algorithms include and two versions of the algorithm proposed in and examples of superpixel segmentations produced by each method appear in fig adherence to boundaries arguably the most important property of a superpixel method is its ability to adhere to image boundaries boundary recall and undersegmentation error are standard measures for boundary adherence in figs and slic and are compared using these measures on the berkeley database in addition a baseline performance obtained by segmenting the image into uniform squares is denoted as squares the berkeley data set contains images and approximately human annotated ground truth segmentations corresponding to each image boundary recall measures what fraction of the ground truth edges fall within at least two pixels of a superpixel boundary the boundary recall of each method is plotted in fig for increasing numbers of superpixels a high boundary recall indicates that very few true edges were missed superpixels generated by slic and demonstrated the best boundary recall performance if we reduce slic compactness m from its default value of slic shows superior performance to under segmentation error shown in fig is another measure of boundary adherence given a region from the ground truth segmentation gi and the set of superpixels required to cover it sjjsj t gi it measures how many pixels from sj leak across the boundary of gi if j j is the size of a segment in pixels m is the number of ground truth segments and b is a minimum number of pixels in sj overlapping gi under segmentation error is expressed as u n xm x sj jsj t gi b jsjj b ca n b is set to percent of jsjj in our experiments to account for ambiguities in the ground truth superpixels that do not tightly fit the ground truth result in a high value of u computational and memory efficiency superpixels are often used to replace the pixel grid to help speed up other algorithms thus it is important that superpixels can be generated efficiently in the first place in fig we compare the time required for the various superpixel methods to segment images of increasing size on an intel dual core ghz processor with gb ram slic with its oðnþ complexity is the fastest superpixel method and its advantage increases with the size of the image while is competitive with oðnþ logn complexity the remaining methods show a significant gap in processing speed it is also important that a superpixel algorithm be memory efficient in order to handle large images slic is the most memory efficient method requiring only n floats to store the distance from ieee transactions on pattern analysis and machine intelligence vol no november fig slic supervoxels computed for a video sequence top frames from a short video sequence of a flag waving bottom left a volume containing the video the last frame appears at the top of the volume bottom right a supervoxel segmentation of the video supervoxels with orange cluster centers are removed for display purposes http people cs uchicago edu pff segment http www cs sfu ca mori research superpixels http www cs toronto edu babalex tar gz http www vlfeat org download html http www csd uwo ca faculty olga code zip each pixel to its nearest cluster center other methods have comparatively high memory requirements and require floats to store edge weights and thresholds for four connectivity or for eight connectivity segmentation performance superpixels are commonly used as a preprocessing step in segmentation algorithms a good superpixel algorithm should improve the performance of the segmentation algorithm that uses it we compared the segmentation resulting from slic and on the msrc data set these results were obtained using the method of which uses superpixels to compute color texture geometry and location features it then trains classifiers for the object classes and learns a crf model the results appearing in table show that slic superpixels yield the best performance slic also reduces the computational time by a factor of over over the method used in example images segmented using slic are shown in fig we also tested on the pascal voc data set using the approach of as shown in table slic provided a boost in segmentation accuracy over and reduced the time spent generating superpixels by an order of magnitude discussion in addition to the properties discussed above other considerations should factor into the quality of a superpixel algorithm one such consideration is the ease of use superpixel methods with many difficult to tune parameters can result in lost time or poor performance another consideration is the ability to specify the amount of superpixels which not all methods provide finally the ability to control the compactness of the superpixels is important compact regular superpixels are often desirable because their ieee transactions on pattern analysis and machine intelligence vol no november fig multiclass segmentation top images from the msrc data set middle ground truth annotations bottom results obtained using slic superpixels in place of following the method proposed in table multiclass object segmentation on the pascal voc data set results for the method of using slic and superpixels fig boundary adherence and segmentation speed a boundary recall measures the fraction of the ground truth edges that fall within at least two pixels of a superpixel boundary while demonstrates the best boundary recall reducing m from the default value increases the boundary recall of slic over that of b under segmentation error measures the amount of superpixel leak for a given ground truth region slic outperforms the other methods showing the lowest under segmentation error for most of the useful operating regime c time required to generate superpixels for images of increasing size slic is the fastest superpixel method followed closely by and then a significant gap is not plotted due to its particularly slow speed bounded size and few neighbors form a more interpretable graph and can extract more locally relevant features however compactness comes at the expense of boundary adherence and the ability to control this tradeoff can be useful in the following we review the performance of each superpixel method with respect to boundary adherence speed memory efficiency segmentation quality parameter tuning ability to specify the amount of superpixels and ability to control superpixel compactness while produced some of the most compact and consistently sized superpixels it fared the worst among all methods in both boundary recall and under segmentation error also suffers from a slow running time and resulted in poor segmentation performance next to it is the slowest superpixel algorithm it is almost times slower than slic for a 536 image taking on the other hand has only one parameter to tune and offers direct control over the number of superpixels normalized cuts showed only a small improvement over the superpixels produced by are even more compact than those of making them attractive for graphbased applications however the boundary adherence is very poor ranking sixth in boundary recall and fifth in undersegmentation error despite this the segmentation quality was surprisingly high the running time of is prohibitively slow and the method failed to segment 536 images producing out of memory errors and these two methods showed similar performance despite their differences in design compact versus constant intensity superpixels compact superpixels are more compact than though much less than and in terms of boundary recall and ranked in the middle of the pack fifth and fourth respectively their standing improved slightly for under segmentation error third and fourth while and are faster than and their slow runtime still limits their usefulness requiring and respectively and they reported one of the worst segmentation performances has three parameters to tune including patch size which can be difficult to set on the positive side allows for control of the number of superpixels and can produce supervoxels quickshift performed well in terms of under segmentation error and boundary recall ranking second and third overall however showed relatively poor segmentation performance and other limitations make it a less than ideal choice it has a slow runtime requires several nonintuitive parameters to be tuned and does not offer control over the amount or compactness of superpixels finally the source code fails to ensure that superpixels are completely connected components which can be problematic for subsequent processing adheres well to image boundaries although the superpixels are very irregular it ranks first in boundary recall outperforming slic by a small margin it is the second fastest method segmenting a 536 image in without performing a parameter search however showed relatively poor segmentation performance and under segmentation error likely because its large irregularly shaped superpixels are not suited to segmentation methods such as last does not allow the number of superpixels or compactness to be controlled with its three input parameters slic among the superpixel methods considered here slic is clearly the best overall performer it is the fastest method segmenting a 536 image in and most memory efficient it boasts excellent boundary adherence outperforming all other methods in under segmentation error and is second only to in boundary recall by a small margin by adjusting m it ranks first when used for segmentation slic showed the best boost in performance on the msrc and pascal data sets slic is simple to use its sole parameter being the number of desired superpixels and it is one of the few methods to produce supervoxels finally among existing methods slic is unique in its ability to control the tradeoff between superpixel compactness and boundary adherence if desired through m more complex distance measures the reader may wonder if more sophisticated distance measures improve slic performance considering the simplicity of the approach described in section we investigated this question by replacing the distance in with an adaptively normalized distance measure aslic and a geodesic distance measure gslic perhaps surprisingly the simple distance measure used in outperforms aslic and gslic in terms of speed memory and boundary adherence adaptive slic or aslic adapts the color and spatial normalizations within each cluster as described in section s and m in are the assumed maximum spatial and color distances within a cluster these constant values are used to normalize color and spatial proximity so they can be combined into a single distance measure for clustering instead of using constant values aslic dynamically normalizes the proximities for each cluster using its maximum observed spatial and color distances ðms mcþ from the previous iteration thus the distance measure becomes d ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi dc mc þ ds ms ieee transactions on pattern analysis and machine intelligence vol no november fig slic applied to segment mitochondria from and em images of neural tissue a slic superpixels from an em slice b the segmentation result from the method of c slic supervoxels on a 024 volume d mitochondria extracted using the method described in e segmentation performance comparing slic supervoxels versus cubes of similar size for the volume in c as before constant normalization factors are used for the first iteration but the algorithm subsequently keeps track of the maximum distances for each cluster the advantage of this approach is that the superpixel compactness is more consistent and it is never necessary to setm this comes at the price of reduced boundary recall performance as shown in fig geodesic slic or gslic replaces the distance of with a geodesic distance the unsigned geodesic distance from one pixel iðpiþ to another iðpjþ is defined as gðiðpiþ iðpjþþ min dðpþ where is the set of all paths between iðpiþ and iðpjþ and dðpþ is the cost associated to path p given by dðpþ xn kiðpiþ iðpi where kiðpiþ iðpi is the euclidean distance between the cielab color vectors of pixels pi and pi this approach has the advantage that the connectivity in the xy plane is guaranteed eliminating the need for the postprocessing step however the computation cost is higher and the boundary adherence performance suffers as seen in fig biomedical applications many popular graph based segmentation approaches such as graph cuts become increasingly expensive as more nodes are added to the graph limiting image size in practice for some applications such as mitochondria segmentation from electron micrographs em the images are large but reducing the resolution is not an option in such cases segmentation on a graph defined over the pixel grid would be intractable in slic superpixels significantly reduce the complexity of the graph making the segmentation tractable segmented mitochondria from are shown in figs and in this approach is extended to image stacks which can contain billions of voxels only the most frugal of algorithms can operate on such large volumes of data without reducing the size of the graph in some manner slic supervoxels reduce the memory requirements and complexity by over three orders of magnitude and significantly increases performance over regular cubes as shown in figs and conclusion superpixels have become an essential tool to the vision community and in this paper we provide the reader with an in depth performance analysis of modern superpixel techniques we performed an empirical comparison of five state of the art algorithms concentrating on their boundary adherence segmentation speed and performance when used as a preprocessing step in a segmentation framework in addition we proposed a new method for generating superpixels based on k means clustering slic which has been shown to outperform existing superpixel methods in nearly every respect although our experiments are thorough they come with a caveat certain superpixel methods specifically and do not consider color information while the other methods do this may adversely impact their performance the imagenet large scale visual recognition challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images the challenge has been run annually from to present attracting participation from more than fifty institutions this paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result we discuss the challenges of collecting large scale ground truth annotation highlight key breakthroughs in categorical object recognition provide a detailed analysis of the current state of the field of large scale image classification and object detection and compare the state ofthe art computer vision accuracy with human accuracy we conclude with lessons learned in the years of the challenge and propose future directions and improvements keywords dataset large scale benchmark object recognition object detection introduction overview the imagenet large scale visual recognition challenge ilsvrc has been running annually for years communicated by m hebert olga russakovsky and jia deng authors contributed equally b olga russakovsky olga cs stanford edu stanford university stanford ca usa university of michigan ann arbor mi usa massachusetts institute of technology cambridge ma usa unc chapel hill chapel hill nc usa since and has become the standard benchmark for large scale object recognition ilsvrc follows in the footsteps of the pascal voc challenge everingham et al established in which set the precedent for standardized evaluation of recognition algorithms in the form of yearly competitions as in pascal voc ilsvrc consists of two components a publically available dataset and an annual competition and corresponding workshop the dataset allows for the development and comparison of categorical object recognition algorithms and the competition andworkshop provide away to track the progress and discuss the lessons learned from the most successful and innovative entries each year the publically released dataset contains a set of manually annotated training images a set of test images is also released with the manual annotations withheld participants train their algorithms using the training images and then automatically annotate the test images these predicted annotations are submitted to the evaluation server results of the evaluation are revealed at the end of the competition period and authors are invited to share insights at the workshop held at the international conference on computer vision iccv or european conference on computer vision eccv in alternate years ilsvrc annotations fall into one of two categories image level annotation of a binary label for the presence or absence of an object class in the image e g there are cars in this image but there are no tigers and object level in this paper we will be using the term object recognition broadly to encompass both image classification a task requiring an algorithm to determine what object classes are present in the image as well as object detection a task requiring an algorithm to localize all objects present in the image in the test annotations were later released publicly since then the test annotation have been kept hidden int j comput vis annotation of a tight bounding box and class label around an object instance in the image e g there is a screwdriver centered at position with width of pixels and height of pixels large scale challenges and innovations in creating the dataset several challenges had to be addressed scaling up from images in pascal voc to in ilsvrc and from object classes to object classes brings with it several challenges it is no longer feasible for a small group of annotators to annotate the data as is done for other datasets fei fei et al criminisi everingham et al xiao et al instead we turn to designing novel crowdsourcing approaches for collecting large scale annotations su et al deng et al some of the object classes may not be as easy to annotate as the categories of pascalvoc e g bananas which appear in bunches may not be as easy to delineate as the basic level categories of aeroplanes or cars having more than a million images makes it infeasible to annotate the locations of all objects much less with object segmentations human body parts and other detailed annotations that subsets of pascal voc contain new evaluation criteria have to be defined to take into account the facts that obtaining perfect manual annotations in this setting may be infeasible once the challenge datasetwas collected its scale allowed for unprecedented opportunities both in evaluation of object recognition algorithms and in developing new techniques novel algorithmic innovations emerge with the availability of large scale training data the broad spectrum of object categoriesmotivated the need for algorithms that are even able to distinguish classes which are visually very similar we highlight the most successful of these algorithms in this paper and compare their performance with human level accuracy finally the large variety of object classes in ilsvrc allows us to perform an analysis of statistical properties of objects and their impact on recognition algorithms this type of analysis allows for a deeper understanding of object recognition and for designing the next generation of general object recognition algorithms goals this paper has three key goals to discuss the challenges of creating this large scale object recognition benchmark dataset to highlight the developments in object classification and detection that have resulted from this effort and to take a closer look at the current state of the field of categorical object recognition the paper may be of interest to researchers working on creating large scale datasets as well as to anybody interested in better understanding the history and the current state of large scale object recognition the collected dataset and additional information about ilsvrc can be found at http image net org challenges lsvrc related work we briefly discuss some prior work in constructing benchmark image datasets image classification datasets caltech fei fei et al was among the first standardized datasets for multicategory image classification with object classes and commonly training images per class caltech griffin et al increased the number of object classes to and added images with greater scale and background variability the tinyimages dataset torralba et al contains million low resolution images collected from the internet using synsets in wordnet miller as queries however since this data has not been manually verified there are many errors making it less suitable for algorithm evaluation datasets such as scenes oliva and torralba fei fei and perona lazebnik et al or recent places zhou et al provide a single scene category label as opposed to an object category the imagenet dataset deng et al is the backbone of ilsvrc imagenet is an image dataset organized according to the wordnet hierarchy miller each concept in wordnet possibly described by multiple words or word phrases is called a synonym set or synset imagenet populates synsets ofwordnet with an average of manually verified and full resolution images as a result imagenet contains annotated images organized by the semantic hierarchy of wordnet as of august imagenet is larger in scale and diversity than the other image classification datasets ilsvrc uses a subset of imagenet images for training the algorithms and some of imagenet image collection protocols for annotating additional images for testing the algorithms image parsingdatasets many datasets aim to provide richer image annotations beyond image category labels labelme russell et al contains general photographs with multiple objects per image it has bounding polygon annotations around objects but the object names are not standardized annotators are free to choose which objects to label and what to name each object the xiao et al dataset contains manually cleaned up and fully annotated images more suitable for standard object detection training and evaluation sift flow liu et al contains images labeled using the labelme system the lotushill dataset yao et al contains very detailed annotations int j comput vis 252 of objects in images and video frames but it is not available for free several datasets provide pixel level segmentations for example msrc dataset criminisi with images and object classes stanford background dataset gould et al with images and classes and the berkeley segmentation dataset arbelaez et al with images annotated with object boundaries open surfaces segments surfaces from consumer photographs and annotates them with surface properties including material texture and contextual information bell et al the closest to ilsvrc is the pascal voc dataset everingham et al which provides a standardized test bed for object detection image classification object segmentation person layout and action classification much of the design choices in ilsvrc have been inspired by pascal voc and the similarities and differences between the datasets are discussed at length throughout the paper ilsvrc scales up pascal voc goal of standardized training and evaluation of recognition algorithms by more than an order of magnitude in number of object classes and images pascal voc has object classes and images compared to with object classes and annotated images the recently released coco dataset lin et al contains more than images with million object instances manually segmented it has fewer object categories than ilsvrc in coco versus in ilsvrc object detection but more instances per category on average compared to about in ilsvrc object detection further it contains object segmentation annotations which are not currently available in ilsvrc coco is likely to become another important large scale benchmark large scale annotation ilsvrc makes extensive use of amazon mechanical turk to obtain accurate annotations sorokin and forsyth works such as welinder et al sheng et al vittayakorn and hays describe quality control mechanisms for this marketplace vondrick et al provides a detailed overview of crowdsourcing video annotation a related line of work is to obtain annotations through well designed games e g von ahn and dabbish our novel approaches to crowdsourcing accurate image annotations are in sects and standardized challenges there are several datasets with standardized online evaluation similar to ilsvrc the aforementioned pascalvoc everingham et al labeled faces in the wild huang et al for unconstrained face recognition reconstruction meets recognition urtasun et al for reconstruction and kitti geiger et al for computer vision in autonomous driving these datasets along with ilsvrc help benchmark progress in different areas of computer vision works such as torralba and efros emphasize the importance of examining the bias inherent in any standardized dataset paper layout we begin with a brief overview of ilsvrc challenge tasks in sect dataset collection and annotation are described at length in sect section discusses the evaluation criteria of algorithms in the large scale recognition setting section provides an overview of the methods developed by ilsvrc participants section contains an in depth analysis ofilsvrcresults sect documents the progress of large scale recognition over the years sect concludes that ilsvrc results are statistically significant sect thoroughly analyzes the current state of the field of object recognition and sect compares state of the art computer vision accuracy with human accuracy we conclude and discuss lessons learned from ilsvrc in sect challenge tasks the goal of ilsvrc is to estimate the content of photographs for the purpose of retrieval and automatic annotation test images are presented with no initial annotation and algorithms have to produce labelings specifying what objects are present in the images new test images are collected and labeled especially for this competition and are not part of the previously published imagenet dataset deng et al ilsvrc over the years has consisted of one or more of the following tasks years in parentheses image classification algorithms produce a list of object categories present in the image single object localization algorithms produce a list of object categories present in the image along with an axis aligned bounding box indicating the position and scale of one instance of each object category object detection algorithms produce a list of object categories present in the image along with an axis aligned bounding box indicating the position and scale of every instance of each object category this section provides an overview and history of each of the three tasks table shows summary statistics in addition ilsvrc in also included a taster fine grained classification task where algorithms would classify dog photographs into one of dog breeds khosla et al fine grained classification has evolved into its own fine grained classification challenge in berg et al which is outside the scope of this paper int j comput vis 252 table overview of the provided annotations for each of the tasks in ilsvrc task image classification single object localization object detection manual labeling on training set number of object classes annotated per image or more locations of annotated classes all instances on some images all instances on all images manual labeling on validation and test sets number of object classes annotated per image all target classes locations of annotated classes all instances on all images all instances on all images image classification task data for the image classification task consists of photographs collected from and other search engines manually labeled with the presence of one of object categories each image contains one ground truth label for each image algorithms produce a list of object categories present in the image the quality of a labeling is evaluated based on the label that best matches the ground truth label for the image see sect constructing imagenet was an effort to scale up an image classification dataset to cover most nouns in english using tens of millions of manually verified photographs deng et al the image classification task of ilsvrc came as a direct extension of this effort a subset of categories and imageswas chosen and fixed to provide a standardized benchmark while the rest of imagenet continued to grow single object localization task the single object localization task introduced in built off of the image classification task to evaluate the ability of algorithms to learn the appearance of the target object itself rather than its image context data for the single object localization task consists of the same photographs collected for the image classification task hand labeled with the presence of one of object categories each image contains one ground truth label additionally every instance of this category is annotated with an axis aligned bounding box for each image algorithms produce a list of object categories present in the image along with a bounding box indicating the position and scale of one instance of each object category the quality of a labeling is evaluated based on the object category label that best matches the ground truth label with the additional requirement that the location of the predicted instance is also accurate see sect www flickr com object detection task the object detection task went a step beyond single object localization and tackled the problem of localizing multiple object categories in the image this task has been a part of the pascal voc for many years on the scale of object categories and tens of thousands of images but scaling it up by an order of magnitude in object categories and in images proved to be very challenging from a dataset collection and annotation point of view see sect data for the detection tasks consists of new photographs collected from flickr using scene level queries the images are annotated with axis aligned bounding boxes indicating the position and scale of every instance of each target object category the training set is additionally supplemented with a data from the single object localization task which contains annotations for all instances of just one object category and b negative images known not to contain any instance of some object categories for each image algorithms produce bounding boxes indicating the position and scale of all instances of all target object categories the quality of labeling is evaluated by recall or number of target object instances detected and precision or the number of spurious detections produced by the algorithm see sect dataset construction at large scale our process of constructing large scale object recognition image datasets consists of three key steps the first step is defining the set of target object categories to do this we select from among the existing imagenet deng et al categories by using wordnet as a backbone miller imagenet already takes care of disambiguating word meanings and of combining together synonyms into the same object category since the selection of object categories needs to be done only once per challenge task we use a combination of automatic heuristics and manual post processing to create the list of target categories appropriate for each task for example for image classifica int j comput vis 252 tion we may include broader scene categories such as a type of beach but for single object localization and object detection we want to focus only on object categories which can be unambiguously localized in images sects the second step is collecting a diverse set of candidate images to represent the selected categories we use both automatic and manual strategies on multiple search engines to do the image collection the process is modified for the different ilsvrc tasks for example for object detection we focus our efforts on collecting scene like images using generic queries such as african safari to find pictures likely to contain multiple animals in one scene sect the third and most challenging step is annotating the millions of collected images to obtain a clean dataset we carefully design crowdsourcing strategies targeted to each individual ilsvrc task for example the bounding box annotation system used for localization and detection tasks consists of three distinct parts in order to include automatic crowdsourced quality control sect annotating images fully with all target object categories on a reasonable budget for object detection requires an additional hierarchical image labeling system sect we describe the data collection and annotation procedure for each of the ilsvrc tasks in order image classification sect single object localization sect and object detection sect focusing on the three key steps for each dataset image classification dataset construction the image classification task tests the ability of an algorithm to name the objects present in the image without necessarily localizing them we describe the choices we made in constructing the ilsvrc image classification dataset selecting the target object categories from imagenet sect collecting a diverse set of candidate images by using multiple search engines and an expanded set of queries inmultiple languages sect and finally filtering the millions of collected images using the carefully designed crowdsourcing strategy of imagenet deng et al sect defining object categories for the image classification dataset the categories used for the image classification task were selected from the imagenet deng et al categories the synsets are selected such that there is no overlap between synsets for any synsets i and j i is not an ancestor of j in the imagenet hierarchy these synsets are part of the larger hierarchy and may have children in imagenet however for ilsvrc we do not consider their child subcategories the synset hierarchy of ilsvrc can be thought of as a trimmed version of the complete imagenet hierarchy figure visualizes the diversity of the object categories the exact synsets used for the image classification and single object localization tasks have changed over the years there are synsets which have been used in all five ilsvrc challenges so far in the first year of the challenge synsets were selected randomly from the available imagenet synsets at the time followed by manual filtering to make sure the object categories were not too obscure with the introduction of the object localization challenge in there were synsets that changed categories such as new zealand beach which were inherently difficult to localize were removed and some new categories from imagenet containing object localization annotations were added in synsets were replaced with categories corresponding to dog breeds to allow for evaluation of more fine grained object classification as shown in fig the synsets have remained consistent since year appendix provides the complete list of object categories used in collecting candidate images for the image classification dataset image collection for ilsvrc classification task is the same as the strategy employed for constructing imagenet deng et al training images are taken directly from imagenet additional images are collected for the ilsvrc using this strategy and randomly partitioned into the validation and test sets we briefly summarize the process deng et al contains further details candidate images are collected from the internet by querying several image search engines for each synset the queries are the set ofwordnet synonyms search engines typically limit the number of retrievable images on the order of a few hundred to a thousand to obtain as many images as possible we expand the query set by appending the queries with the word from parent synsets if the same word appears in the glossary of the target synset for example when querying whippet according towordnet glossary a small slender dog of greyhound type developed in england we also use whippet dog and whippet greyhound to further enlarge and diversify the candidate pool we translate the queries into other languages including chinese spanish dutch and italian we obtain accurate translations using wordnets in those languages image classification dataset annotation annotating images with corresponding object classes follows the strategy employed by imagenet deng et al we summarize it briefly here int j comput vis 252 fig the diversity of data in the ilsvrc image classification and single object localization tasks for each of the eight dimensions we show example object categories along the range of that property object scale number of instances and image clutter for each object category are computed using the metrics defined in sect and in appendix the other properties were computed by asking human subjects to annotate each of the object categories russakovsky et al to collect a highly accurate dataset we rely on humans to verify each candidate image collected in the previous step for a given synset this is achieved by using amazon mechanical turk amt an online platform on which one can put up tasks for users for a monetary reward with a global user base amt is particularly suitable for large scale labeling in each of our labeling tasks we present the users with a set of candidate images and the definition of the target synset including a link to wikipedia we then ask the users to verify whether each image contains objects of the synset we encourage users to select images regardless of occlusions number of objects and clutter in the scene to ensure diversity while users are instructed to make accurate judgment we need to set up a quality control system to ensure this accuracy there are two issues to consider first human users make mistakes and not all users follow the instructions second users do not always agree with each other especially for more subtle or confusing synsets typically at the deeper levels of the tree the solution to these issues is to have multiple users independently label the same image an image is considered positive only if it gets a convincing majority of the votes we observe however that different categories require different levels of consensus among users for example while five users might be necessary for obtaining a good consensus on burmese cat images a much smaller number int j comput vis 252 fig the ilsvrc dataset contains many more fine grained classes compared to the standard pascal voc benchmark for example instead of the pascal dog category there are different breeds of dogs in classification and single object localization tasks is needed for cat images we develop a simple algorithm to dynamically determine the number of agreements needed for different categories of images for each synset we first randomly sample an initial subset of images at least users are asked to vote on each of these images we then obtain a confidence score table indicating the probability of an image being a good image given the consensus among user votes for each of the remaining candidate images in this synset we proceed with the amt user labeling until a pre determined confidence score threshold is reached empirical evaluation evaluation of the accuracy of the large scale crowdsourced image annotation systemwas done on the entire imagenet deng et al a total of synsets were randomly sampled at every tree depth of the mammal and vehicle subtrees an independent group of subjects verified the correctness of each of the images an average of precision is achieved across the synsets we expect similar accuracy on ilsvrc image classification dataset since the image annotation pipeline has remained the same to verify we manually checked image classification test set images the test set has remained unchanged in these years we found annotation errors corresponding as expected to precision image classification dataset statistics using the image collection and annotation procedure described in previous sections we collected a large scale dataset used for ilsvrc classification task there are object classes and approximately training images thousand validation images and thousand test images table documents the size of the dataset over the years of the challenge single object localization dataset construction the single object localization task evaluates the ability of an algorithm to localize one instance of an object category it was introduced as a taster task in ilsvrc and became an official part of ilsvrc in the key challenge was developing a scalable crowdsourcing method for object bounding box annotation our three step self verifying pipeline is described in sect having the dataset collected we perform detailed analysis in sect to ensure that the dataset is sufficiently varied to be suitable for evaluation of object localization algorithms object classes and candidate images the object classes for single object localization task are the same as the object classes for image classification task described above in sect the training images for localization task are a subset of the training images used for image classification task and the validation and test images are the same between both tasks bounding box annotation recall that for the image classification task every image was annotated with one object class label corresponding to one object that is present in an image for the single object localization task every validation and int j comput vis 252 table scale of ilsvrc image classification task minimum per class maximum per class year train images per class val images per class test images per class image classification annotations object classes 3047 281 1300 the numbers in parentheses correspond to minimum per class maximum per class the classes change from year to year but are consistent between image classification and single object localization tasks in the same year all images from the image classification task may be used for single object localization table scale of additional annotations for the ilsvrc single object localization task minimum per class maximum per class year train images with bbox annotations per class train bboxes annotated per class val images with bbox annotations per class val bboxes annotated per class test images with bbox annotations additional annotations for single object localization object classes 344 1268 000 the numbers in parentheses correspond to minimum per class maximum per class the classes change from year to year but are consistent between image classification and single object localization tasks in the same year all images from the image classification task may be used for single object localization test image and a subset of the training images are annotated with axis aligned bounding boxes around every instance of this object every bounding box is required to be as small as possible while including all visible parts of the object instance an alternate annotation procedure could be to annotate the full estimated extent of the object e g if a person legs are occluded and only the torso is visible the bounding box could be drawnto include the likely location of the legs however this alternative procedure is inherently ambiguous and ill defined leading to disagreement among annotators and among researchers what is the true most likely extent of this object we follow the standard protocol of only annotating visible object parts russell et al everingham et al bounding box object annotation system we summarize the crowdsourced bounding box annotation system described in detail in su et al the goal is to build a system that is fully automated highly accurate and cost effective given a collection of images where the some datasets such as pascal voc everingham et al and labelme russell et al are able to provide more detailed annotations for example marking individual object instances as being truncated we chose not to provide this level of detail in favor of annotating more images and more object instances object of interest has been verified to exist for each image the system collects a tight bounding box for every instance of the object there are two requirements quality each bounding box needs to be tight i e the smallest among all bounding boxes that contains all visible parts of the object this facilitates the object detection learning algorithms by providing the precise location of each object instance coverage every object instance needs to have a bounding box this is important for training localization algorithms because it tells the learning algorithms with certainty what is not the object the core challenge of building such a system is effectively controlling the data quality with minimal cost our key observation is that drawing a bounding box is significantly more difficult and time consuming than giving answers to multiple choice questions thus quality control through additional verification tasks is more cost effective than consensus based algorithms this leads to the followingworkflow with simple basic subtasks drawing a worker draws one bounding box around one instance of an object on the given image int j comput vis 252 quality verificationasecondworker checks if the bounding box is correctly drawn coverage verification a third worker checks if all object instances have bounding boxes the sub tasks are designed following two principles first the tasks aremade as simple as possible for example instead of asking the worker to draw all bounding boxes on the same image we ask the worker to draw only one this reduces the complexity of the task second each task has a fixed and predictable amount of work for example assuming that the input images are clean object presence is correctly verified and the coverage verification tasks give correct results the amount of work of the drawing task is always that of providing exactly one bounding box quality control on tasks and is implemented by embedding gold standard images where the correct answer is known worker training for each of these subtasks is described in detail in su et al empirical evaluation the system is evaluated on categories with imagenet deng et al balloon bear bed bench beach bird bookshelf basketball hoop bottle and people a subset of images are randomly sampled from each category on the image level our evaluation shows that images are completely covered with bounding boxes for the remaining some bounding boxes are missing however these are all difficult cases the size is too small the boundary is blurry or there is strong shadow on the bounding box level of all bounding boxes are accurate the bounding boxes are visibly tight the remaining are somewhat off no bounding boxes are found to have less than intersection over union overlap with ground truth additional evaluation of the overall cost and an analysis of quality control can be found in su et al single object localization dataset statistics using the annotation procedure described above we collect a large set of bounding box annotations for the ilsvrc single object classification task all thousand images in the validation set and thousand images in the test set are annotated with bounding boxes around all instances of the ground truth object class one object class per image in addition in of training images are annotated with bounding boxes the same way yielding more than thousand annotated images with more than thousand annotated object instances in of training images are annotated yielding more than thousand annotated images with more than thousand annotated object instances table documents the size of this dataset in addition to the size of the dataset we also analyze the level of difficulty of object localization in these images compared to the pascalvocbenchmark we compute statistics on the single object localization validation set images compared to pascal voc validation images real world scenes are likely to contain multiple instances of some objects and nearby object instances are particularly difficult to delineate the average object category in ilsvrc has target object instances on average per positive image with each instance having on average 47 neighbors adjacent instances of the same object category this is comparable to instances per positive image and 52 neighbors per instance for an average object class in pascal as described in hoiem et al smaller objects tend to be significantly more difficult to localize in the average object category in pascal the object occupies of the image area and in ilsvrc however pascal has only object categories while ilsvrc has the object categories of ilsvrc with the smallest objects on average occupy the same fraction of the image as pascalobjects thus even though on average the object instances tend to be bigger in ilsvrc images there are more than times more object categories than in pascal voc with the same average object scale appendix and russakovsky et al have additional comparisons object detection dataset construction the ilsvrc task of object detection evaluates the ability of an algorithm to name and localize all instances of all target objects present in an image it is much more challenging than object localization because some object instances may be small occluded difficult to accurately localize and the algorithm is expected to locate them all not just the one it finds easiest there are three key challenges in collecting the object detection dataset the first challenge is selecting the set of common objects which tend to appear in cluttered photographs and are well suited for benchmarking object detection performance our approach relies on statistics of the object localization dataset and the tradition of the pascal voc challenge sect the second challenge is obtaining a much more varied set of scene images than those used for the image classification and single object localization datasets section describes the procedure for utilizing as much data from the single object localization dataset as possible and supplementing it with flickr images queried using hundreds of manually designed high level queries int j comput vis 252 the third and biggest challenge is completely annotating this dataset with all the objects this is done in two parts section describes the first part our hierarchical strategy for obtaining the list of all target objects which occur within every image this is necessary since annotating in a straightforwardway by creating a task for every image object class pair is no longer feasible at this scale appendix describes the second part annotating the bounding boxes around these objects using the single object localization bounding box annotation pipeline of sect along with extra verification to ensure that every instance of the object is annotated with exactly one bounding box defining object categories for the object detection dataset there are object classes hand selected for the detection task eacg corresponding to a synset within imagenet these were chosen to be mostly basic level object categories that would be easy for people to identify and label the rationale is that the object detection system developed for this task can later be combined with a fine grained classification model to further classify the objects if a finer subdivision is desired as with the classification classes the synsets are selected such that there is no overlap for any synsets i and j i is not an ancestor of j in the imagenet hierarchy the selection of the object detection classes in was guided by the ilsvrc classification and localization dataset starting with object classes and their bounding box annotations we first eliminated all object classes which tended to be too big in the image on average the object area was greater than of the image area these were classes such as t shirt spiderweb or manhole cover we then manually eliminated all classes which we did not feel were well suited for detection such as hay barbershop or poncho this left object classes which were merged into basic level categories for example different species of birds were merged into just the bird class the classes remained the same in appendix contains the complete list of object categories used in in the context of the hierarchy described in sect staying mindful of the tradition of the pascal voc dataset we also tried to ensure that the set of classes contains as many of the pascal voc classes as possible table shows the correspondences the changes that were done were to ensure more accurate and consistent crowdsourced annotations the object class with the weakest correspondence is potted plant in pascal voc corre some of the training objects are actually annotated with more detailed classes for example one of the object classes is the category dog and some training instances are annotated with the specific dog breed table correspondences between the object classes in the pascal voc everingham et al and the ilsvrc detection task class name in pascal voc classes closest class in ilsvrc det classes average object scale pascal voc ilsvrc det aeroplane airplane bicycle bicycle bird bird boat watercraft bottle wine bottle bus bus car car cat domestic cat chair chair cow cattle dining table table dog dog horse horse motorbike motorcyle person person potted plant flower pot sheep sheep sofa sofa train train tv monitor tv or monitor object scale is the fraction of image area reported in percent occupied by an object instance it is computed on the validation sets of pascal voc and of ilsvrc det the average object scale is across the pascalvoc categories and across the corresponding ilsvrc det categories sect reports additional dataset statistics sponding to flower pot in ilsvrc potted plant was one of the most challenging object classes to annotate consistently among the pascal voc classes and in order to obtain accurate annotations using crowdsourcing we had to restrict the definition to a more concrete object collecting images for the object detection dataset many images for the detection task were collected differently than the images in imagenet and the classification and single object localization tasks figure summarizes the types of images that were collected ideally all of these images would be scene images fully annotated with all target categories however given budget constraints our goal was to provide as much suitable detection data as possible even if the images were drawn from a few different sources and distributions the validation and test detection set images come from two sources percent of images from each source in parentheses the first source is images from int j comput vis 252 fig summary of images collected for the detection task images in green bold boxes have all instances of all detection object classes fully annotated table lists the complete statistics single object localization validation and test sets corresponding to the detection classes or their children in the imagenet hierarchy images where the target object occupied more than of the image area were discarded since they were unlikely to contain other objects of interest the second source is images from flickr collected specifically for detection task we queried flickr using a large set of manually defined queries such as kitchenette or australian zoo to retrieve images of scenes likely to contain several objects of interest appendix contains the full list we also added pairwise queries or queries with two target object names such as tiger lion which also often returned cluttered scenes figure shows a random set of both types of validation images images were randomly split with going into the validation set and into the test set the training set for the detection task comes from three sources of images percent of images from each source in parentheses the first source is all training images from single object localization task corresponding to the detection classes or their children in the imagenet hierarchy we did not filter by object size allowing teams to take advantage of all the positive examples available the second source is negative images which were part of the original imagenet collection process but voted as negative for example some of the images were collected from flickr and search engines for the imagenet synset animals but during the manual verification step did the validation test split is consistent with validation images of remained in the validation set of and test images remained in test set not collect enough votes to be considered as containing an animal these images were manually re verified for the detection task to ensure that they did not in fact contain the target objects the third source is images collected from flickr specifically for the detection task these images were added for following the same protocol as the second type of images in the validation and test set this was done to bring the training and testing distributions closer together complete image object annotation for the object detection dataset the key challenge in annotating images for the object detection task is that all objects in all images need to be labeled suppose there are n inputs images which need to be annotated with the presence or absence of k labels objects a naïve approachwould query humans for each combination of input and label requiring nk queries however nandkcan be very large and the cost of this exhaustive approach quickly becomes prohibitive for example annotating 000 validation and test images with the presence or absence of object classes for the detection task naïvely would take times more effort than annotating 000 validation and test images with object each for the classification task and this is not even counting the additional cost of collecting bounding box annotations around each object instance this quickly becomes infeasible in deng et al we study strategies for scalable multilabel annotation or for efficiently acquiring multiple labels from humans for a collection of items we exploit three key int j comput vis 252 fig random selection of images in ilsvrcdetection validation set the images in the top four rows were taken from single object localization validation set and the images in the bottom four rows were collected from flickr using scene level queries observations for labels in real world applications illustrated in fig correlation subsets of labels are often highly correlated objects such as a computer keyboard mouse and monitor frequently co occur in images similarly some labels tend to all be absent at the same time for example all objects that require electricity are usually absent in pictures taken outdoors this suggests that we could potentially fill in the values of multiple labels by grouping them into only one query for humans instead of checking if dog cat rabbit etc are present in the photo we just check about the animal group if the answer is no then this implies a no for all categories in the group hierarchy the above example of grouping dog cat rabbit etc into animal has implicitly assumed that labels can be grouped together and humans can efficiently answer queries about the group as a whole this brings up our second key observation humans organize semantic concepts into hierarchies and are able to efficiently int j comput vis 252 fig consider the problem of binary multi label annotation for each input e g image and each label e g object the goal is to determine the presence or absense plus or minus of the label e g decide if the object is present in the image multi label annotation becomes much more efficient when considering real world structure of data correlation between labels hierarchical organization of concepts and sparsity of labels fig our algorithm dynamically selects the next query to efficiently determine the presence or absence of every object in every image green denotes a positive annotation and red denotes a negative annotation this toy example illustrates a sample progression of the algorithm for one label cat on a set of images categorize at higher semantic levels thorpe et al e g humans can determine the presence of an animal in an image as fast as every type of animal individually this leads to substantial cost savings sparsity the values of labels for each image tend to be sparse i e an image is unlikely to contain more than a dozen types of objects a small fraction of the hundreds of object categories this enables rapid elimination of many objects by quickly filling in no with a high degree of sparsity an efficient algorithm can have a cost which grows logarithmically with the number of objects instead of linearly we propose algorithmic strategies that exploit the above intuitions the key is to select a sequence of queries for humans such that we achieve the same labeling results with only a fraction of the cost of the naïve approach the main challenges include howtomeasure cost and utility of queries how to construct good queries and how to dynamically order them a detailed description of the generic algorithm along with theoretical analysis and empirical evaluation is presented in deng et al application of the generic multi class labeling algorithm to our setting the generic algorithm automatically selects the most informative queries to ask based on object label statistics learned from the training set in our case of object classes since obtaining the training set was by itself challenging we chose to design the queries by hand we created a hierarchy of queries of the type is there a in the image for example one of the high level questions was is there an animal in the image we ask the crowd workers this question about every image we want to label the children of the animal question would correspond to specific examples of animals for example is there a mammal in the image or is there an animal with no legs to annotate images efficiently these questions are asked only on images determined to contain an animal the leaf node questions correspond to the target objects e g is there a cat in the image a few sample iterations of the algorithm are shown in fig algorithm is the formal algorithm for labeling an image with the presence or absence of each target object category with this algorithm in mind the hierarchy of questions was constructed following the principle that false positives only add extra costwhereas false negatives can significantly affect the quality of the labeling thus it is always better to stick with more general but less ambiguous questions such as is there a mammal in the image as opposed to asking overly specific but potentially ambiguous questions such as is there an animal that can climb trees constructing this hierarchy was a surprisingly time consuming process involving multiple iterations to ensure high accuracy of labeling and avoid question ambiguity appendix shows the constructed hierarchy int j comput vis 252 input image i queries q directed graph g over q output labels l q yes no initialize labels l q q q initialize candidates c q q root g while c not empty do obtain answer a to query q c l q a c c q if a is yes then chldr q children q g l q c c chldr else des q descendants q g l q l q no q des c c des end end algorithm the algorithm for completemulti class annotation this is a special case of the algorithm described in deng et al a hierarchy of questions g is manually constructed all root questions are asked on every image if the answer to query q on image i is no then the answer is assumed to be no for all queries q such that q is a descendant of q in the hierarchy we continue asking the queries until all queries are answered for images taken from the single object localization task we used the known object label to initialize l bounding box annotation once all images are labeled with the presence or absence of all object categories we use the bounding box system described in sect along with some additional modifications of appendix to annotate the location of every instance of every present object category object detection dataset statistics using the procedure described above we collect a largescale dataset for ilsvrcobject detection task there are object classes and approximately training images validation images and test images table documents the size of the dataset over the years of the challenge the major change between and was the addition of 658 fully annotated training images prior to ilsvrc the object detection benchmark was the pascalvoc challenge everingham et al ilsvrc has times more object classes than pascal voc vs times more fully annotated training images 658 vs times more training objects vs times more validation images vs and times more validation objects 501 vs ilsvrc has annotated objects per image on the validation set compared to in pascal voc the average object in ilsvrc takes up of the image area and in pascal voc takes up table contains per class comparisons additionally ilsvrc contains a wide variety of objects including tiny objects such as sunglasses of image area on average ping pong balls of image area on average and basketballs of image area on average evaluation at large scale once the dataset has been collected we need to define a standardized evaluation procedure for algorithms some measures have already been established by datasets such as the caltech fei fei et al for image classification and pascal voc everingham et al for both image classification and object detection to adapt these procedures to the large scale setting we had to address three key challenges first for the image classification and singleobject localization tasks only one object category could be labeled in each image due to the scale of the dataset this created potential ambiguity during evaluation addressed in sect second evaluating localization of object instances is inherently difficult in some images which contain a cluster of objects addressed in sect third evaluating localization of object instances which occupy few pixels in the image is challenging addressed in sect in this sectionwe describe the standardized evaluation criteria for each of the three ilsvrc tasks we elaborate further on these and other more minor challenges with large scale evaluation appendix describes the submission protocol and other details of running the competition itself table scale of ilsvrc object detection task year train images per class train bboxes annotated per class val images per class val bboxes annotated per class test images object detection annotations object classes 561 pos neg 73799 pos rest neg 461 pos neg 74517 pos rest neg numbers in parentheses correspond to minimum per class median per class maximum per class int j comput vis 252 fig tasks in ilsvrc the first column shows the ground truth labeling on an example image and the next three show three sample outputs with the corresponding evaluation score image classification the scale of ilsvrc classification task categories and more than a million of images makes it very expensive to label every instance of every object in every image therefore on this dataset only one object category is labeled in each image this creates ambiguity in evaluation for example an image might be labeled as a strawberry but contain both a strawberry and an apple then an algorithm would not know which one of the two objects to name for the image classification task we allowed an algorithm to identify multiple up to objects in an image and not be penalized as long as one of the objects indeed corresponded to the ground truth label figure top row shows some examples concretely each image i has a single class label ci an algorithm is allowed to return labels and is considered correct if ci j ci for some j let the error of a prediction di j d ci j ci be if ci j ci and otherwise the error of an algorithm is the fraction of test images on which the algorithm makes a mistake error n n i min j di j we used two additional measures of error first we evaluated top error in this case algorithms were penalized if their highest confidence output label did not match ground truth class ci second we evaluated hierarchical error the intuition is that confusing two nearby classes such as two different breeds of dogs is not as harmful as confusing a dog for a container ship for the hierarchical criteria the cost of one misclassification d ci j ci is defined as the height of the lowest common ancestor of ci j and ci in the imagenet hierarchy the height of a node is the length of the longest path to a leaf node leaf nodes have height zero however in practice we found that all three measures of error top top and hierarchical produced the same ordering of results thus since we have been exclusively using the top metric which is the simplest and most suitable to the dataset int j comput vis 252 fig images marked as difficult in the single object localization validation set please refer to sect for details single object localization the evaluation for single object localization is similar to object classification again using a top criteria to allow the algorithm to return unannotated object classes without penalty however now the algorithm is considered correct only if it both correctly identifies the target class ci and accurately localizes one of its instances figure middle row shows some examples concretely an image is associated with object class ci with all instances of this object class annotated with bounding boxes bik an algorithm returns ci j bi j of class labels ci j and associated locations bi j the error of a prediction j is di j max d ci j ci min k d bi j bik here d bi j bik is the error of localization defined as if the area of intersection of boxes bi j and bik divided by the areas of their union is greater than and otherwise everingham et al the error of an algorithm is computed as in eq evaluating localization is inherently difficult in some images consider a picture of a bunch of bananas or a carton of apples it is easy to classify these images as containing bananas or apples and even possible to localize a few instances of each fruit however in order for evaluation to be accurate every instance of banana or apple needs to be annotated and that may be impossible to handle the images where localizing individual object instances is inherently ambiguous we manually discarded of images since some examples of discarded images are shown in fig object detection the criteria for object detection was adopted from pascal voc everingham et al it is designed to penalize the algorithm for missing object instances for duplicate detections of one instance and for false positive detections figure bottom row shows examples for each object class and each image ii an algorithm returns predicted detections bi j si j of predicted locations input bounding box predictions with confidence scores bj j m j and ground truth boxes b on image i for a given object class output binary results z j m j of whether or not prediction j is a true positive detection let u b be the set of unmatched objects order bj j m j in descending order of j for j m do let c bk u iou bk bj thr bk if c then let k argmax k bk c iou bk bj set u u bk set z j since true positive detection else set z j since false positive detection end end algorithm the algorithm for greedily matching object detection outputs to ground truth labels the standard thr bk everingham et al ilsvrc computes thr bk using eq to better handle low resolution objects bi j with confidence scores si j these detections are greedily matched to the ground truth boxes bik using algorithm for every detection j on image i the algorithm returns zi j if the detection is matched to a ground truth box according to the threshold criteria and otherwise for a given object class let n be the total number of ground truth instances across all images given a threshold t define recall as the fraction of the n objects detected by the algorithm and precision as the fraction of correct detections out of the total detections returned by the algorithm concretely recall t i j si j t zi j n precision t i j si j t zi j i j si j t the final metric for evaluating an algorithm on a given object class is average precision over the different levels of recall achieved by varying the threshold t the winner of each object class is then the team with the highest average int j comput vis 252 227 precision and then winner of the challenge is the team that wins on the most object classes difference with pascal voc evaluating localization of object instances which occupy very few pixels in the image is challenging the pascal voc approach was to label such instances as difficult and ignore them during evaluation however since ilsvrc contains a more diverse set of object classes including for example nail and ping pong ball which have many very small instances it is important to include even very small object instances in evaluation in algorithm a predicted bounding box b is considered to have properly localized by a ground truth bounding box b if i ou b b thr b the pascal voc metric uses the threshold thr b however for small objects even deviations of a few pixels would be unacceptable according to this threshold for example consider an object b of size pixels with a detection window of pixels which fully contains that object this would be an error of approximately pixels on each dimension which is average human annotation error however the iouin this casewould be 400 far below the threshold of thus for smaller objects we loosen the threshold in ilsvrc to allow for the annotation to extend up to pixels on average in each direction around the object concretely if the ground truth box b is of dimensions w h then thr b min wh w h in practice this changes the threshold only on objects which are smaller than approximately pixels and affects of objects in the detection validation set practical consideration one additional practical consideration for ilsvrc detection evaluation is subtle and comes directly as a result of the scale of ilsvrc in pascal algorithms would often return many detections per class on the test set including ones with low confidence scores this allowed the algorithms to reach the level of high recall at least in the realm of very low precision on ilsvrc detection test set if an algorithm returns bounding boxes per object per image this would result in detections each detection contains an image index a class index bounding box coordinates and the confidence score so it takes on the order of bytes the full set of detections would then require 24gb to store and submit to the evaluation server which is impractical this means that algorithms in this paper we focus on the mean average precision across all categories as the measure of a team performance this is done for simplicity and is justified since the ordering of teams by mean average precisionwas always the same as the ordering by object categorieswon are implicitly required to limit their predictions to only the most confident locations methods the ilsvrc dataset and the competition has allowed significant algorithmic advances in large scale image recognition and retrieval challenge entries this section is organized chronologically highlighting the particularly innovative and successfulmethods which participated in the ilsvrc each year tables and list all the participating teams we see a turning point in with the development of large scale convolutional neural networks the first year the challenge consisted of just the classification task the winning entry from nec team lin et al used sift lowe and lbp ahonen et al features with two non linear coding representations zhou et al wang et al and a stochastic svm the honorable mention xrce team perronnin et al used an improved fisher vector representation perronnin and dance along with pca dimensionality reduction and data compression followed by a linear svm fisher vector based methods have evolved over years of the challenge and continued performing strongly in every ilsvrc from to the winning classification entry in was the runner up team xrce applying high dimensional image signatures perronnin et al with compression using product quantization sanchez and perronnin and one vs all linear svms the single object localization competitionwas held for the first time with two brave entries the winner was the uva team using a selective search approach to generate class independent object hypothesis regions van de sande et al followed by dense sampling and vector quantization of several color sift features van de sande et al pooling with spatial pyramid matching lazebnik et al and classifying with a histogram intersection kernel svm maji and malik trained on a gpu van de sande et al this was a turning point for large scale object recognition when large scale deep neural networks entered the scene the undisputed winner of both the classification and localization tasks in was the supervision team they trained a large deep convolutional neural network on rgb values with million parameters using an efficient gpu implementation and a novel hidden unit dropout trick int j comput vis 252 table teams participating in ordered alphabetically codename cls loc insitutions contributors and references ilsvrc hminmax massachusetts institute of technology jim mutch sharat chikkerur hristo paskov ruslan salakhutdinov stan bileschi hueihan jhuang ibm ibm research georgia tech lexing xie hua ouyang apostol natsev isil intelligent systems and informatics lab the university of tokyo tatsuya harada hideki nakayama yoshitaka ushiku yuya yamashita jun imura yasuo kuniyoshi itnlp harbin institute of technology deyuan zhang wenfeng xuan xiaolong wang bingquan liu chengjie sun lig laboratoire d informatique de grenoble georges quénot nec nec labs america university of illinois at urbana champaign rutgers yuanqing lin fengjun lv shenghuo zhu ming yang timothee cour kai yu liangliang cao zhen li min hsuan tsai xi zhou thomas huang tong zhang lin et al nii national institute of informatics tokyo japan hefei normal univ heifei china cai zhi zhu xiao zhou shiníchi satoh ntu cemnet sce ntu singapore zhengxiang wang liang tien chia uci university of california irvine hamed pirsiavash deva ramanan charless fowlkes xrce xerox research centre europe jorge sanchez florent perronnin thomas mensink perronnin et al ilsvrc isi intelligent systems and informatics lab university of tokyo tatsuya harada asako kanezaki yoshitaka ushiku yuya yamashita sho inaba hiroshi muraoka yasuo kuniyoshi nii national institute of informatics japan duy dinh le shiníchi satoh uva university of amsterdam university of trento koen e a van de sande jasper r r uijlings arnold w m smeulders theo gevers nicu sebe cees snoek van de sande et al xrce xerox research centre europe ciii florent perronnin jorge sanchez sanchez and perronnin ilsvrc isi university of tokyo jst presto naoyuki gunji takayuki higuchi koki yasumoto hiroshi muraoka yoshitaka ushiku tatsuya harada yasuo kuniyoshi harada and kuniyoshi lear lear inria grenoble tvpa xerox research centre europe thomas mensink jakob verbeek florent perronnin gabriela csurka mensink et al vgg university of oxford karen simonyan yusuf aytar andrea vedaldi andrew zisserman arandjelovic and zisserman sanchez et al supervision university of toronto alex krizhevsky ilya sutskever geoffrey hinton krizhevsky et al uva university of amsterdam koen e a van de sande amir habibian cees g m snoek sanchez and perronnin scheirer et al xrce xerox research centre europe lear inria florent perronnin zeynep akata zaid harchaoui cordelia schmid perronnin et al each method is identified with a codename used in the text we report flat top classification and single object localization error in percents lower is better for teams which submitted multiple entries we report the best score in supervision also submitted entries trained with the extra data from the imagenet fall release and obtained classification error and localization error key references are provided where available more details about the winning entries can be found in sect int j comput vis 252 table teams participating in ordered alphabetically codename cls loc det insitutions contributors and references ilsvrc adobe adobe university of illinois at urbana champaign hailin jin zhe lin jianchao yang tom paine krizhevsky et al ahoward andrew howard consulting andrew howard bupt beijing university of posts and telecommunications orange labs international center beijing chong huang yunlong bian hongliang bai bo liu yanchao feng yuan dong clarifai clarifai matthew zeiler zeiler and fergus zeiler et al cogvision microsoft research harbin institute of technology kuiyuan yang yalong bai yong rui decaf university of california berkeley yangqing jia jeff donahue trevor darrell donahue et al deep punx saint petersburg state university evgeny smirnov denis timoshenko alexey korolev krizhevsky et al wan et al tang delta national tsing hua university che rung lee hwann tzong chen hao ping kang tzu wei huang ci hong deng hao che kao ibm university of illinois at urbana champaign ibm watson research center ibmhaifa research center zhicheng yan liangliang cao john r smith noel codella michele merler sharath pankanti sharon alpert yochay tzur mil university of tokyo masatoshi hidaka chie kamada yusuke mukuta naoyuki gunji yoshitaka ushiku tatsuya harada minerva peking university microsoft research shanghai jiao tong university xidian university harbin institute of technologyς tianjun xiao minjie wang jianpeng li yalong baiς jiaxing zhang kuiyuan yang chuntao hong zheng zhang wang et al nec nec labs america university of missouri xiaoyu wang miao sun tianbao yang yuanqing lin tony x han shenghuo zhu wang et al nus national university of singapore min lin qiang chen jian dong junshi huang wei xia shuicheng yan equal contribution krizhevsky et al orange orange labs international center beijing beijing university of posts and telecommunications hongliang bai lezi wang shusheng cen yinan liu kun tao wei liu peng li yuan dong overfeat new york university pierre sermanet david eigen michael mathieu xiang zhang rob fergus yann lecun sermanet et al quantum self employed student in troy high school fullerton ca henry shu jerry shu batra et al sysu sun yat sen university china xiaolong wang felzenszwalb et al toronto university of toronto yichuan tang nitish srivastava ruslan salakhutdinov equal contribution trimps the third research institute of the ministry of public security p r china jie shao xiaoteng zhang yanfeng shang wenfei wang lin mei chuanping hu ucla university of california los angeles yukun zhu jun zhu alan yuille uiuc university of illinois at urbana champaign thomas paine kevin shih thomas huang krizhevsky et al int j comput vis 252 table continued codename cls loc det insitutions contributors and references uva university of amsterdam euvision technologies koen e a van de sande daniel h f fontijne cees g m snoek harro m g stokman arnold w m smeulders van de sande et al vgg visual geometry group university of oxford karen simonyan andrea vedaldi andrew zisserman simonyan et al zf new york university matthew d zeiler rob fergus zeiler and fergus zeiler et al each method is identified with a codename used in the text for classificaton and single object localization we report flat top error in percents lower is better for detection we report mean average precision in percents higher is better even though the winner of the challenge was determined by the number of object categories won this correlated strongly with map parentheses indicate the team used outside training data and was not part of the official competition some competing teams also submitted entries trained with outside data clarifai with classification error nec with detectionmap key references are provided where available more details about the winning entries can be found in sect table teams participating in ordered alphabetically codename cls clso loc loco det deto insitutions contributors and references ilsvrc adobe adobe uiuc hailin jin zhaowen wang jianchao yang zhe lin ahoward howard vision technologies andrew howard howard bdc institute for infocomm research universit pierre et marie curie olivier morre hanlin goh antoine veillard vijay chandrasekhar krizhevsky et al berkeley uc berkeley ross girshick jeff donahue sergio guadarrama trevor darrell jitendra malik girshick et al breil kaist department of ee jun cheol park yunhun jang hyungwon choi jaeyoung jun chatfield et al jia brno 52 brno university of technology martin koláˇr michal hradiš pavel svoboda krizhevsky et al mikolov et al jia casia chinese academy of science southeast university peihao huang yongzhen huang feng liu zifeng wu fang zhao liang wang tieniu tan girshick et al casiaws cripac casia weiqiang ren chong wang yanhua chen kaiqi huang tieniu tan arbeláez et al cldi kaist cldi inc kyunghyun paeng donggeun yoo sunggyun park jungin lee anthony s paek in so kweon seong dae kim krizhevsky et al perronnin et al cuhk the chinese university of hong kong wanli ouyang ping luo xingyu zeng shi qiu yonglong tian hongsheng li shuo yang zhe wang yuanjun xiong chen qian zhenyao zhu ruohui wang chen change loy xiaogang wang xiaoou tang ouyang et al ouyang and wang deepcnet university of warwick ben graham graham schmidhuber int j comput vis 252 table continued codename cls clso loc loco det deto insitutions contributors and references deepinsight nlpr hkust junjie yan naiyan wang stan z li dit yan yeung girshick et al fengjunlv fengjun lv consulting fengjun lv krizhevsky et al harel et al googlenet google christian szegedy wei liu yangqing jia pierre sermanet scott reed drago anguelov dumitru erhan andrew rabinovich szegedy et al hkust hong kong u of science and tech chinese u of h k stanford u cewu lu hei law hao chen qifeng chen yao xiao chi keung tang uijlings et al girshick et al perronnin et al felzenszwalb et al libccv libccv org liu liu zeiler and fergus mil the university of tokyo iit guwahati senthil purushwalkam yuichiro tsuchiya atsushi kanehira asako kanezaki tatsuya harada kanezaki et al girshick et al the university of tokyo riku togashi keita iwamoto tomoaki iwase hideki nakayama girshick et al msra microsoft research xi an jiaotong u u of science and tech of china kaiming he xiangyu zhang shaoqing ren jian sun he et al nus national university of singapore ibm research australia jian dong yunchao wei min lin qiang chen wei xia shuicheng yan lin et al chen et al nus bst national univ of singapore beijing samsung telecom r d center min lin jian dong hanjiang lai junjun xiong shuicheng yan lin et al howard krizhevsky et al orange orange labs beijing bupt china hongliang bai yinan liu bo liu yanchao feng kun tao yuan dong girshick et al passby lenovo hkust u of macao lin sun zhanghui kuang cong zhao kui jia oscar c au jia krizhevsky et al scut south china univ of technology guo lihua liao qijun ma qianli lin junbin southeast southeast u chinese a of sciences feng liu zifeng wu yongzhen huang sysu sun yat sen university liliang zhang tianshui chen shuye zhang wanglan he liang lin dengguang pang lingbo liu trimps the third research institute of the ministry of public security jie shao xiaoteng zhang jianying zhou jian wang jian chen yanfeng shang wenfei wang lin mei chuanping hu girshick et al manen et al howard ttic toyota technological institute at chicago ecole centrale paris george papandreou iasonas kokkinos papandreou papandreou et al jojic et al krizhevsky et al sermanet et al dubout and fleuret iandola et al int j comput vis 252 table continued codename cls clso loc loco det deto insitutions contributors and references ui university of isfahan fatemeh shafizadegan elham shabaninia yang et al uva u of amsterdam and euvision tech koen van de sande daniel fontijne cees snoek harro stokman arnold smeulders van de sande et al vgg university of oxford karen simonyan andrew zisserman simonyan and zisserman xyz the university of queensland zhongwen xu and yi yang krizhevsky et al jia zeiler and fergus lin et al each method is identified with a codename used in the text for classificaton and single object localization we report flat top error in percents lower is better for detection we report mean average precision in percents higher is better clso loco deto corresponds to entries using outside training data officially allowed in means localization error greater than localization submission was required with every classification submission key references are provided where available more details about the winning entries can be found in sect krizhevsky et al hinton et al the second place in image classification went to the isi team which used fisher vectors sanchez and perronnin and a streamlined version of graphical gaussian vectors harada and kuniyoshi along with linear classifiers using passive aggressive pa algorithm crammer et al the second place in single object localization went to the vgg with an image classification system including dense sift features and color statistics lowe a fisher vector representation sanchez and perronnin and a linear svm classifier plus additional insights from arandjelovic and zisserman sanchez et al both isi and vgg used felzenszwalb et al for object localization supervision used a regression model trained to predict bounding box locations despite the weaker detection model supervision handily won the object localization task a detailed analysis and comparison of the supervision and vgg submissions on the single object localization task can be found in russakovsky et al the influence of the success of the supervision model can be clearly seen in and there were teams participating in the competition compared to in the previous years combined following the success of the deep learningbased method in the vast majority of entries in used deep convolutional neural networks in their submission the winner of the classification task was clarifai with several large deep convolutional networks averaged together the network architectures were chosen using the visualization technique of zeiler and fergus and they were trained on the gpu following zeiler et al using the dropout technique krizhevsky et al the winning single object localization overfeat submissionwas based on an integrated framework for using convolutional networks for classification localization and detection with a multiscale sliding window approach sermanet et al they were the only team tackling all three tasks the winner of object detection task was uva team which utilized a new way of efficient encoding van de sande et al densely sampled color descriptors van de sande et al pooled using a multi level spatial pyramid in a selective search framework uijlings et al the detection results were rescored using a full image convolutional network classifier attracted the most submissions with teams submitting entries compared to just teams in a increase in participation as in almost all teams used convolutional neural networks as the basis for their submission significant progress has been made in just year image classification error was almost halved since and object detection mean average precision almost doubled compared to please refer to sect for details in teamswere allowed to use outside data for training theirmodels in the competition so there were six tracks provided and outside data tracks in each of image classification single object localization and object detection tasks the winning image classification with provided data team was googlenet which explored an improved convolutional neural network architecture combining the multi scale idea with intuitions gained from the hebbian principle additional dimension reduction layers allowed them to increase both the depth and the width of the network significantly without incurring significant computational overhead in the image classificationwith external data track casiawswon by using weakly supervised object localization from only classification labels to improve image classification mcg table omits teams which submitted results but chose not to officially participate in the challenge int j comput vis 252 fig performance of winning entries in the competitions in each of the three tasks details about the entries and numerical results are in sect there is a steady reduction of error every year in object classification and single object localization tasks and a improvement in mean average precision in object detection there are two considerations in making these comparisons the object categories used in islvrc changed between years and and between and however the large scale of the data object categories million training images has remained the same making it possible to compare results image classification and single object localization entries shown here use only provided training data the size of the object detection training data has increased significantly between years and sect section discusses the relative effects of training data increase versus algorithmic improvements region proposals arbeláez et al pretrained on pascal voc data are used to extract region proposals regions are represented using convolutional networks and a multiple instance learning strategy is used to learn weakly supervised object detectors to represent the image in the single object localization with provided data track the winning team was vgg which explored the effect of convolutional neural network depth on its accuracy by using three different architectures with up to weight layers with rectified linear unit non linearity building off of the implementation of caffe jia for localization they used per class bounding box regression similar to overfeat sermanet et al in the single object localization with external data track adobe used additional imagenet classes to train the classifiers in an integrated convolutional neural network framework for both classification and localization with bounding box regression at test time they used k means to find bounding box clusters and rank the clusters according to the classification scores in the object detection with provided data track the winning team nus used the rcnn framework girshick et al with the network in network method lin et al and improvements of howard global context information was incorporated following chen et al in the object detection with external data track the winning team was googlenet which also won image classification with provided data it is truly remarkable that the same team was able to win at both image classification and object detection indicating that their methods are able to not only classify the image based on scene information but also accurately localize multiple object instances just like most teams participating in this track googlenet used the image classification dataset as extra training data large scale algorithmic innovations ilsvrc over the past years has paved the way for several breakthroughs in computer vision the field of categorical object recognition has dramatically evolved in the large scale setting section documents the progress starting from coded sift features and evolving to large scale convolutional neural networks dominating at all three tasks of image classification single object localization and object detection with the availability of so much training data along with an efficient algorithmic implementation and gpu computing resources it became possible to learn neural networks directly from the image data without needing to create multi stage hand tuned pipelines of extracted features and discriminative classifiers the major breakthrough came in with the win of the supervision team on image classification and single object localization tasks krizhevsky et al and by all of the top contestants were relying heavily on convolutional neural networks further over the past few years there has been a lot of focus on large scale recognition in the computer vision community best paper awards at top vision conferences in were awarded to large scale recognition methods at cvpr to fast accurate detection of 000 object classes on a single machine dean et al and at iccv to from large scale image categorization to entry level categories ordonez et al additionally several influential lines of research have emerged such as large scale weakly supervised localization work of kuettel et al which was awarded the best paper award in eccv and large scale zero shot learning e g frome et al int j comput vis 252 results and analysis improvements over the years state of the art accuracy has improved significantly from to showcasing the massive progress that has been made in large scale object recognition over the past years the performance of the winning ilsvrc entries for each task and each year are shown in fig the improvement over the years is clearly visible in this section we quantify and analyze this improvement image classification and single object localization improvement over the years there has been a reduction in image classification error from to and a reduction in single object localization error from to since the beginning of the challenge for consistency here we consider only teams that use the provided training data even though the exact object categories have changed sect the large scale of the dataset has remained the same table making the results comparable across the years the dataset has not changed since and there has been a reduction in image classification error from to and a in single object localization error from to in the past years object detection improvement over the years object detection accuracy as measured by the mean average precision map has increased since the introduction of this task from map in to map in however these results are not directly comparable for two reasons first the size of the object detection training data has increased significantly from to sect second the map result was obtained with the addition of the image classification and single object localization training data here we attempt to understand the relative effects of the training set size increase versus algorithmic improvements all models are evaluated on the same object detection test set first we quantify the effects of increasing detection training data between the two challenges by comparing the same model trained on detection data versus detection data the uva team framework from achieved with data table and with data and no other modifications the absolute increase in mapwas the rcnn model achieved map with detection plus image classification data girshick et al and personal communication with members of the uva team map with detection plus image classification data berkeley team in table the absolute increase in map by expanding detection data to was second we quantify the effects of adding in the external data for training object detection models the nec model in achieved map trained on detection data alone and map trained on detection plus classification data table the absolute increase in map was the uva team best entry in achieved map trained on detection data and map trained on detection plus classification data the absolute increase in map was thus we conclude based on the evidence so far that expanding the set to the set as well as adding in additional training data from the classification task all account for approximately in absolute map improvement for the models for comparison we can also attempt to quantify the effect of algorithmic innovation the uva team framework achieved map on data as mentioned above and their improved method in obtained map table this is absolute increase in map over just year from algorithmic innovation alone in summary we conclude that the absolute increase in map between winning entries of map and of map is the result of impressive algorithmic innovation and not just a consequence of increased training data however increasing the object detection training dataset further is likely to produce additional improvements in detection accuracy for current algorithms statistical significance one important question to ask is whether results of different submissions to ilsvrc are statistically significantly different from each other given the large scale it is no surprise that even minor differences in accuracy are statistically significant we seek to quantify exactly howmuch of a difference is enough following the strategy employed by pascalvoc everingham et al for each method we obtain a confidence interval of its score using bootstrap sampling during each bootstrap round we sample n images with replacement from all the available n test images and evaluate the performance of the algorithm on those sampled images this can be done very efficiently by precomputing the accuracy on each image given the results of all the bootstrapping rounds we discard the lower and the upper α fraction the range of the remaining results represents the 2α confidence interval we run a large number of bootstrapping rounds from 000 until int j comput vis 252 table we use bootstrapping to construct each ilsvrc task in year codename error conf int image classification googlenet vgg 05 msra ahoward deepervision 51 clarifai casiaws trimps adobe clarifai nus zf 51 ahoward 55 overfeat orange supervision supervision isi vgg 43 xrce 52 uva 58 09 single object localization vgg googlenet 25 overfeat adobe 58 sysu supervision 55 05 mil 25 supervision msra trimps orange 43 vgg vgg isi 53 53 54 casiaws 48 object detection googlenet 43 65 cuhk deepinsight 06 nus uva 42 msra table continued year codename ap conf int berkeley 52 36 uva 32 03 49 southeast 48 hkust 03 30 uva 58 00 nec nec overfeat toronto 46 sysu 45 04 32 ucla 48 means the entry used external training data the winners using the provided data for each track and each year are bolded the difference between the winning method and the runner up each year is significant even at the level convergence table shows the results of the top entries to each task of the winning methods are statistically significantly different from the other methods even at the level current state of categorical object recognition besides looking at just the average accuracy across hundreds of object categories and tens of thousands of images we can also delve deeper to understand where mistakes are being made and where researchers efforts should be focused to expedite progress to do so in this section we will be analyzing an optimistic measurement of state of the art recognition performance instead of focusing on the differences in individual algorithms for each task and each object class we compute the best performance of any entry submitted to any including methods using additional training data since the test sets have remained the same we can directly compare all the entries in the past years to obtain the most optimistic measurement of state of the art accuracy on each category for consistency with the object detection metric higher is better in this section we will be using image classification and single object localization accuracy instead of error where accuracy error range of accuracy across object classes figure shows the distribution of accuracy achieved by the optimistic models across the object categories the image classification model achieves accuracy on average int j comput vis 252 fig for each object class we consider the best performance of any entry submitted to including entries using additional training data the plots showthe distribution of these optimistic per class results performance is measured as accuracy for image classification left and for single object localization middle and as average precision for object detection right while the results are very promising in image classification the ilsvrc datasets are far from saturated many object classes continue to be challenging for current algorithms or error but there remains a absolute difference inaccuracy between the most and least accurate object class the single object localization model achieves accuracy on average or error with a range in accuracy across the object classes the object detection model achieves average precision with an range across the object classes it is clear that the ilsvrc dataset is far from saturated performance on many categories has remained poor despite the strong overall performance of the models qualitative examples of easy and hard classes figures and show the easiest and hardest classes for each task i e classeswith the best andworst results obtained with the optimistic models for image classification out of object classes have image classification accuracy according to the optimistic estimate figure top shows a random set of of them they contain a variety of classes such as mammals like red fox and animals with distinctive structures like stingray the hardest classes in the image classification task with accuracy as low as include metallic and see through man made objects such as hook and water bottle thematerial velvet and the highly varied scene class restaurant for single object localization the easiest classes with accuracy are all mammals and birds the hardest classes include metallic man made objects such as letter opener and ladle plus thin structures such as pole and spacebar and highly varied classes such as wing the most challenging class spacebar has a only localization accuracy object detection results are shown in fig the easiest classes are living organisms such as dog and tiger plus basketball and volleyball with distinctive shape and color and a somewhat surprising snowplow the easiest class butterfly is not yet perfectly detected but is very close with ap the hardest classes are as expected small thin objects such as flute and nail and the highly varied lamp and backpack classes with as low as ap per class accuracy as a function of image properties we now take a closer look at the image properties to try to understand why current algorithms perform well on some object classes but not others one hypothesis is that variation in accuracy comes from the fact that instances of some classes tend to be much smaller in images than instances of other classes and smaller objects may be harder for computers to recognize in this section we argue that while accuracy is correlated with object scale in the image not all variation in accuracy can be accounted for by scale alone for every object class we compute its average scale or the average fraction of image area occupied by an instance of the object class on the validation set since the images and object classes in the image classification and single object localization tasks are the same we use the bounding box annotations of the single object localization dataset for both tasks in that dataset the object classes range from swimming trunks with scale of to spider web with scale of in the object detection validation dataset the object classes range from sunglasses with scale of to sofa with scale of figure shows the performance of the optimistic method as a function of the average scale of the object in the image each dot corresponds to one object class we observe a very weak positive correlation between object scale and image classification accuracy ρ for single object localization and object detection the correlation is stronger int j comput vis 252 fig for each object category we take the best performance of any entry submitted to including entries using additional training data given these optimistic results we show the easiest and harder classes for each task the numbers in parentheses indicate classification and localization accuracy for image classification the easiest classes are randomly selected from among object classes with accuracy object detection results are shown in fig at ρ and ρ respectively it is clear that not all variation in accuracy can be accounted for by scale alone nevertheless in the next section we will normalize for object scale to ensure that this factor is not affecting our conclusions per class accuracy as a function of object properties besides considering image level properties we can also observe how accuracy changes as a function of intrinsic int j comput vis 252 fig for each object category we take the best performance of any entry submitted to including entries using additional training data given these optimistic results we show the easiest and harder classes for the object detection task i e classes with best and worst results the numbers in parentheses indicate average precision image classification and single object localization results are shown in fig fig performance of the optimistic method as a function of object scale in the image on each task each dot corresponds to one object class average scale x axis is computed as the average fraction of the image area occupied by an instance of that object class on the validation set optimistic performance y axis corresponds to the best performance on the test set of any entry submitted to including entries with additional training data the test set has remained the same over these years we see that accuracy tends to increase as the objects get bigger in the image however it is clear that far from all the variation in accuracy on these classes can be accounted for by scale alone object properties we define three properties inspired by human vision the real world size of the object whether it deformable within instance and how textured it is for each property the object classes are assigned to one of a few bins listed below these properties are illustrated in fig human subjects annotated each of the image classification and single object localization object classes from with these properties russakovsky et al by construction see sect each of the object detection classes is either also one of object classes or is an ancestor of one or more of the classes in the imagenet hierarchy to compute the values of the properties for each object detection class we simply average the annotated values of the descendant classes int j comput vis 252 in this section we draw the following conclusions about state of the art recognition accuracy as a function of these object properties real world size xs for extra small e g nail small e g fox medium e g bookcase large e g car or xl for extra large e g church the image classification and single object localization optimistic models performs better on large and extra large real world objects than on smaller ones the optimistic object detectionmodel surprisingly performs better on extra small objects than on small or medium ones deformability within instance rigid e g mug or deformable e g water snake the optimistic model on each of the three tasks performs statistically significantly better on deformable objects compared to rigid ones however this effect disappears when analyzing natural objects separately from man made objects amount of texture none e g punching bag low e g horse medium e g sheep or high e g honeycomb the optimistic model on each of the three tasks is significantly better on objects with at least low level of texture compared to untextured objects these and other findings are justified and discussed in detail below experimental setup we observed in sect that objects that occupy a larger area in the image tend to be somewhat easier to recognize to make sure that differences in object scale are not influencing results in this section we normalize each bin by object scale we discard object classes with the largest scales from each bin as needed until the average object scale of object classes in each bin across one property is the same or as close as possible for real world size property for example the resulting average object scale in each of the five bins is in the image classification and single object localization tasks and in the object detection task figure shows the average performance of the optimistic model on the object classes that fall into each bin for each property we analyze the results in detail below unless otherwise specified the reported accuracies below are after the scale normalization step to evaluate statistical significance we compute the confidence interval for accuracy using bootstrapping we repeatedly sample the object classes within the bin with for rigid versus deformable objects the average scale in each bin is for classification and localization and for detection for texture the average scale in each of the four bins is for classification and localization and for detection replacement discard some as needed to normalize by scale and compute the average accuracy of the optimistic model on the remaining classes we report the confidence intervals ci in parentheses real world size in fig top left we observe that in the image classification task the optimistic model tends to perform significantly better on objects which are larger in the real world the classification accuracy is on xs s andmobjects compared to on l and onxl objects since this is after normalizing for scale and thus can t be explained by the objects size in the image we conclude that either larger real world objects are easier for the model to recognize or larger real world objects usually occur in images with very distinctive backgrounds to distinguish between the two cases we look fig top middle we see that in the single object localization task the l objects are easy to localize at localization accuracy xl objects however tend to be the hardest to localize with only localization accuracy we conclude that the appearance of l objectsmust be easier for themodel to learn while xl objects tend to appear in distinctive backgrounds the image background make these xl classes easier for the image level classifier but the individual instances are difficult to accurately localize some examples of l objects are killer whale schooner and lion and some examples of xl objects are boathouse mosque toyshop and steel arch bridge in fig top right corresponding to the object detection task the influence of real world object size is not as apparent one of the key reasons is that many of the xl and l object classes of the image classification and single object localization datasets were removed in constructing the detection dataset sect since they were not basic categories well suited for detection therewere only classes remaining in the dataset train airplane and bus and none after scale normalization we omit them from the analysis the average precision of xs s m objects and map respectively is statistically insignificant from average precision on l objects confidence interval of l objects is this may be due to the fact that there are only l object classes remaining after scale normalization all other real world size bins have at least object classes finally it is interesting that performance on xs objects of map ci 47 is statistically significantly better than performance on s or m objects with and map respectively some examples of xs objects are strawberry bow tie and rugby ball deformability within instance in fig second row it is clear that the optimistic model performs statistically significantly worse on rigid objects than on deformable objects int j comput vis 252 fig performance of the optimistic computer vision model as a function of object properties the x axis corresponds to object properties annotated by human labelers for each object class russakovsky et al and illustrated in fig the y axis is the average accuracy of the optimistic model note that the range of the y axis is different for each task to make the trends more visible the black circle is the average accuracy of the model on all object classes that fall into each bin we control for the effects of object scale by normalizing the object scale within each bin details in sect the color bars show the model accuracy averaged across the remaining classes error bars show the confidence interval obtained with bootstrapping some bins are missing color bars because less than object classes remained in the bin after scale normalization for example the bar for xl real world object detection classes is missing because that bin has only object classes airplane bus train and after normalizing by scale no classes remain int j comput vis 252 image classification accuracy is on rigid objects ci much smaller than on deformable ones single object localization accuracy is on rigid objects ci much smaller than on deformable ones object detection map is on rigid objects ci 42 much smaller than on deformable ones we can further analyze the effects of deformability after separating object classes into natural and man made bins based on the imagenet hierarchy deformability is highly correlated with whether the object is natural or man made correlation for image classification and single object localization classes and 61 for object detection classes figure third row shows the effect of deformability on performance of the model for man made and natural objects separately man made classes are significantly harder than natural classes classification accuracy ci for man made versus for natural localization accuracy ci for man made versus for natural and detection map ci for man made versus for natural however whether the classes are rigid or deformable within this subdivision is no longer significant in most cases for example the image classification accuracy is ci on man made rigid objects and on man made deformable objects not statistically significantly different there are two cases where the differences in performance are statistically significant first for single object localization natural deformable objects are easier than natural rigid objects localization accuracy of ci on natural deformable objects is higher than on natural rigid objects falling slightly outside the confidence interval this difference in performance is likely because deformable natural animals tend to be easier to localize than rigid natural fruit second for object detection man made rigid objects are easier than man made deformable objects map ci on man made rigid objects is higher than map on man made deformable objects this is because man made rigid objects include classes like traffic light or car whereas the man made deformable objects contain challenging classes like plastic bag swimming trunks or stethoscope amount of texture finally we analyze the effect that object texture has on the accuracy of the optimistic model figure fourth row demonstrates that the model performs better as the amount of texture on the object increases the most significant difference is between the performance on untextured objects and the performance on objects with low texture image classification accuracy is on untextured objects ci lower than on low textured objects single object localization accuracy is on untextured objects ci 69 lower than 80 on low textured objects object detection map is on untextured objects ci lower than 42 on low textured objects texture is correlated with whether the object is natural or man made at correlation for image classification and single object localization and 46 correlation for object detection to determine if this is a contributing factor in fig bottom row we break up the object classes into natural and man made and showthe accuracy on objectswith no texture versus objects with low texture we observe that the model is still statistically significantly better on low textured object classes than on untextured ones both on man made and natural object classes independently human accuracy on large scale image classification recent improvements in state of the art accuracy on the ilsvrc dataset are easier to put in perspective when compared to human level accuracy in this section we compare the performance of the leading large scale image classification method with the performance of humans on this task to support this comparison we developed an interface that allowed a human labeler to annotate images with up to five ilsvrc target classes we compare human errors to those of the winning image classificationmodel googlenet sect for this analysis we use a random sample of image classification test set images annotation interface our web based annotation interface consists of one test set image and a list of ilsvrc categories on the side each category is described by its title such as cowboy boot the categories are sorted in the topological order of the imagenet hierarchy which places semantically similar concepts nearby in the list for example all motor vehicle related classes are arranged contiguously in the list every class category is additionally accompanied by a row of examples images from the training set to allow for faster visual scanning the user of the interface selects categories from the list by clicking on the desired items since our interface is web based it allows for natural scrolling through the list and also search by text annotation protocol we found the task of annotating images with one of categories to be an extremely challenging natural object detection classes are removed from this analysis because there are only and natural untextured and low textured classes respectively and none remain after scale normalization all other bins contain at least object classes after scale normalization int j comput vis 252 table human classification results on the classification test set for two expert annotators and relative confusion human succeeds googlenet succeeds human succeeds googlenet fails human fails googlenet succeeds 46 human fails googlenet fails 30 total number of images estimated googlenet classification error estimated human classification error we report top classification error task for an untrained annotator the most common error that an untrained annotator is susceptible to is a failure to consider a relevant class as a possible label because they are unaware of its existence therefore in evaluating the human accuracy we relied primarily on expert annotators who learned to recognize a large portion of the ilsvrc classes during training the annotators labeled a few hundred validation images for practice and later switched to the test set images quantitative comparison of human and computer accuracy on large scale image classification we report results based on experiments with two expert annotators the first annotator trained on images and annotated test images the second annotator trained on images and then annotated test images the average pace of labeling was approximately image per minute but the distribution is strongly bimodal some images are quickly recognized while some images such as those of fine grained breeds of dogs birds or monkeys may require multiple minutes of concentrated effort the results are reported in table annotator annotator evaluated a total of test set images the googlenet classification error on this sample was estimated to be recall that the error on full test set of 000 images is as shown in table the human error was estimated to be thus annotator achieves a performance superior to googlenet by approximately we can analyze the statistical significance of this result under the null hypothesis that they are from the same distribution in particular comparing the two proportions with a z test yields a one sided p value of p 022 thus we can conclude that this result is statistically significant at the confidence level annotator our second annotator trained on a smaller sample of only images and then labeled test set images as seen in table the final classification error is significantly worse at approximately top error the majority of these errors 48 8 can be attributed to the annotator failing to spot and consider the ground truth label as an option thus we conclude that a significant amount of training time is necessary for a human to achieve competitive performance on ilsvrc however with a sufficient amount of training a human annotator is still able to outperform the googlenet result p 022 by approximately annotator comparison we also compare the prediction accuracy of the two annotators of a total of images that both and labeled were correctly labeled by both and were correctly labeled by but not were correctly labeled by but not and were incorrectly labeled by both these include images that we consider to be incorrectly labeled in the ground truth in particular our results suggest that the human annotators do not exhibit strong overlap in their predictions we can approximate the performance of an optimistic human classifier by assuming an image to be correct if at least one of or correctly labeled the image on this sample of images we approximate the error rate of an optimistic human annotator at compared to the googlenet error rate of 9 analysis of human and computer errors on large scale image classification we manually inspected both human and googlenet errors to gain an understanding of common error types and how they compare for purposes of this section we only discuss results based on the larger sample of images that were labeled by annotator examples of representative mistakes are in fig the analysis and insights below were derived specifically from googlenet predictions but we suspect that many of the same errors may be present in other methods types of errors in both computer and human annotations multiple objects both googlenet and humans struggle with images that contain multiple ilsvrc classes usually many more than five with little indication of which object is the focus of the image this error is only present in the classification setting since every image is constrained to have exactly one correct label in total we attribute of googlenet errors and of human errors to this category it is worth noting that humans can have a slight advantage in this error type since it can sometimes be easy to identify the most salient object in the image int j comput vis 252 fig representative validation images that highlight common sources of error for each image we display the ground truth in blue and top predictions from googlenet follow red wrong green right googlenet predictions on the validation set images were graciously provided by members of the googlenet team from left to right images that contain multiple objects images of extreme closeups and uncharacteristic views images with filters images that significantly benefit from the ability to read text images that contain very small and thin objects images with abstract representations and example of a fine grained image that googlenet correctly identifies but a human would have significant difficulty with incorrect annotations we found that approximately out of images were incorrectly annotated in the ground truth this introduces an approximately equal number of errors for both humans and googlenet types of errors that the computer is more susceptible to than the human object small or thin googlenet struggles with recognizing objects that are very small or thin in the image even if that object is the only object present examples of this include an image of a standing person wearing sunglasses a person holding a quill in their hand or a small ant on a stem of a flower we estimate that approximately 21 of googlenet errors fall into this category while none of the human errors do in otherwords in our sample of images no image was mislabeled by a human because they were unable to identify a very small or thin object this discrepancy can be attributed to the fact that a human can very effectively leverage context and affordances to accurately infer the identity of small objects for example a few barely visible feathers near person hand as very likely belonging to amostly occluded quill image filters many people enhance their photos with filters that distort the contrast and color distributions of the image we found that of the images that googlenet incorrectly classified contained a filter thus we posit that googlenet is not very robust to these distortions in comparison only one image among the human errors contained a filter but we do not attribute the source of the error to the filter abstract representations googlenet struggles with images that depict objects of interest in an abstract form such as rendered images paintings sketches plush toys or statues an example is the abstract shape of a bow drawn with a light source in night photography a robotic scorpion or a shadow on the ground of a child on a swing we attribute approximately of googlenet errors to this type of error and believe that humans are significantly more robust with no such errors seen in our sample miscellaneous sources additional sources of error that occur relatively infrequently include extreme closeups of parts of an object unconventional viewpoints such as a rotated image images that can significantly benefit from the ability to read text e g a featureless container identifying itself as face powder objects with heavy occlusions and images that depict a collage of multiple images in general we found that humans are more robust to all of these types of error types of errors that the human is more susceptible to than the computer fine grained recognition we found that humans are noticeably worse at fine grained recognition e g dogs monkeys snakes birds even when they are in clear view to understand the difficulty consider that there are more than species of dogs in the dataset we estimate that of the human errors fall into this category while only of googlenet errors do class unawareness the annotator may sometimes be unaware of the ground truth class present as a label option when pointed out as an ilsvrc class it is usually clear that the label applies to the image these errors get progressively less frequent as the annotator becomes more familiar with ilsvrc classes approximately of the human errors fall into this category insufficient training data recall that the annotator is only presented with examples of a class under every int j comput vis 252 category name however images are not always enough to adequately convey the allowed class variations for example a brown dog can be incorrectly dismissed as a kelpie if all examples of a kelpie feature a dog with black coat however if more than images were listed it would have become clear that a kelpie may have brown coat approximately of human errors fall into this category conclusions from human image classification experiments we investigated the performance of trained human annotators on a sample of ilsvrc test set images our results indicate that a trained human annotator is capable of outperforming the best model googlenet by approximately p 022 we expect that some sources of errormay be relatively easily eliminated e g robustness to filters rotations collages effectively reasoning over multiple scales while others may prove more elusive e g identifying abstract representations of objects on the other hand a large majority of human errors come from fine grained categories and class unawareness we expect that the former can be significantly reduced with fine grained expert annotators while the latter could be reduced with more practice and greater familiarity with ilsvrc classes our results also hint that human errors are not strongly correlated and that human ensembles may further reduce human error rate it is clear that humans will soon outperform state ofthe art ilsvrc image classification models only by use of significant effort expertise and time one interesting followup question for future investigation is how computer level accuracy compares with human level accuracy onmore complex image understanding tasks conclusions in this paper we described the large scale data collection process of ilsvrc provided a summary of the most successful algorithms on this data and analyzed the success and failure modes of these algorithms in this section we discuss some of the key lessons we learned over the years of ilsvrc strive to address the key criticisms of the datasets and the challenges we encountered over the years and conclude by looking forward into the future lessons learned the key lesson of collecting the datasets and running the challenges for years is this all human intelligence tasks need to be exceptionally well designed we learned this lesson both when annotating the dataset using amazon mechanical turk workers sect and even when trying to evaluate human level image classification accuracy using expert labelers sect the first iteration of the labeling interface was always bad generally meaning completely unusable if there was any inherent ambiguity in the questions posed and there almost always was workers found it and accuracy suffered if there is one piece of advice we can offer to future research it is to very carefully design continuously monitor and extensively sanity check all crowdsourcing tasks the other lesson already well known to large scale researchers is this scaling up the dataset always reveals unexpected challenges from designing complicated multistep annotation strategies sect tohaving tomodifythe evaluation procedure sect we had to continuously adjust to the large scale setting on the plus side of course the major breakthroughs in object recognition accuracy sect and the analysis of the strength and weaknesses of current algorithms as a function of object class properties sect would never have been possible on a smaller scale criticism in the past years we encountered three major criticisms of the ilsvrc dataset and the corresponding challenge the ilsvrc dataset is insufficiently challenging the ilsvrc dataset contains annotation errors and the rules of ilsvrc competition are too restrictive we discuss these in order the first criticism is that the objects in the dataset tend to be large and centered in the images making the dataset insufficiently challenging in sect and we tried to put those concerns to rest by analyzing the statistics of the ilsvrc dataset and concluding that it is comparable with and in many cases much more challenging than the long standing pascal voc benchmark everingham et al the second is regarding the errors in ground truth labeling we went through several rounds of in house post processing of the annotations obtained using crowdsourcing and corrected many common sources of errors e g appendix the major remaining source of annotation errors stem from fine grained object classes e g labelers failing to distinguish different species of birds this is a tradeoff that had to be made in order to annotate data at this scale on a reasonable budget we had to rely on non expert crowd labelers however overall the dataset is encouragingly clean by our estimates precision is achieved in the image classification dataset sects 4 and 9 of images that went through the bounding box annotation system have all instances of the target object class labeled with bounding boxes sect int j comput vis 252 the third criticism we encountered is over the rules of the competition regarding using external training data in algorithms had to only use the provided training and validation set images and annotations for training their models with the growth of the field of large scale unsupervised feature learning however questions began to arise about what exactly constitutes outside data for example are image features trained on a large pool of outside images in an unsupervised fashion allowed in the competition after much discussion in we took the first step towards addressing this problem we followed the pascal voc strategy and created two tracks in the competition entries using only provided data and entries using outside data meaning any images or annotations not provided as part of ilsvrctraining or validation sets however in the future this strategywill likely need to be further revised as the computer vision field evolves for example competitions can consider allowing the use of any image features which are publically available even if these features were learned on an external source of data the future given the massive algorithmic breakthroughs over the past years we are very eager to see what will happen in the next years there are many potential directions of improvement and growth for ilsvrcand other large scale image datasets first continuing the trend of moving towards richer image understanding from image classification to single object localization to object detection the next challenge would be to tackle pixel level object segmentation the recently released large scale coco dataset lin et al is already taking a step in that direction second as datasets grow even larger in scale it may become impossible to fully annotate them manually the scale of ilsvrc is already imposing limits on the manual annotations that are feasible to obtain for example we had to restrict the number of objects labeled per image in the image classification and single object localization datasets in the future with billions of images it will become impossible to obtain even one clean label for every image datasets such as yahoo s flickr creative commons released with weak human tags but no centralized annotation will become more common the growth of unlabeled or only partially labeled largescale datasets implies two things first algorithms will have to rely more on weakly supervised training data second even evaluation might have to be done after the algorithms make predictions not before this means that rather than evaluating accuracy how many of the test images or objects http webscope sandbox yahoo com catalog php datatype i did did the algorithm get right or recall howmany of the desired images or objects did the algorithm manage to find both of which require a fully annotated test set we will be focusing more on precision of the predictions that the algorithmmade how many were deemed correct by humans we are eagerly awaiting the future development of object recognition datasets and algorithms and are grateful that ilsvrc served as a stepping stone along this path acknowledgments we thank stanford university unc chapel hill google and facebook for sponsoring the challenges and nvidia for providing computational resources to participants of we thank our advisors over the years lubomir bourdev alexei efros derek hoiem jitendra malik chuck rosenberg and andrew zisserman we thank the pascal voc organizers for partnering with us in running we thank all members of the stanford vision lab for supporting the challenges and putting up with us along the way finally and most importantly we thank all researchers that have made the ilsvrc effort a success by competing in the challenges and by using the datasets to advance computer vision appendix image classification and single object localization object categories abacus abaya academic gown accordion acorn acorn squash acoustic guitar admiral affenpinscher afghan hound african chameleon african crocodile african elephant african grey african hunting dog agama agaric aircraft carrier airedale airliner airship albatross alligator lizard alp altar ambulance american alligator american black bear american chameleon american coot american egret american lobster american staffordshire terrier amphibian analog clock anemone fish angora ant apiary appenzeller apron arabian camel arctic fox armadillo artichoke ashcan assault rifle australian terrier axolotl baboon backpack badger bagel bakery balance beam bald eagle balloon ballplayer ballpoint banana band aid banded gecko banjo bannister barbell barber chair barbershop barn barn spider barometer barracouta barrel barrow baseball basenji basketball basset bassinet bassoon bath towel bathing cap bathtub beach wagon beacon beagle beaker bearskin beaver bedlington terrier bee bee eater beer bottle beer glass bell cote bell pepper bernese mountain dog bib bicycle built for two bighorn bikini binder binoculars birdhouse bison bittern black and gold garden spider black grouse black stork black swan black widow black and tan coonhound black footed ferret blenheim spaniel bloodhound bluetick boa constrictor boathouse bobsled bolete bolo tie bonnet book jacket bookcase bookshop border collie border terrier borzoi boston bull bottlecap bouvier des flandres bow bowtie box turtle boxer brabancon griffon brain coral brambling brass brassiere breakwater breastplate briard brittany spaniel broccoli broom brown bear bubble bucket buckeye buckle bulbul bull mastiff bullet train bulletproof vest bullfrog burrito bustard butcher shop butternut squash cab cabbage butterfly cairn caldron can opener candle cannon canoe capuchin car mirror car wheel carbonara cardigan cardigan cardoon carousel carpenter s kit carton cash machine cassette cassette player castle catamaran cauliflower cd player cello cellular telephone centipede chain chainmail chain saw chainlink fence chambered nautilus cheeseburger cheetah chesapeake bay retriever chest chickadee chiffonier chihuahua chime chimpanzee china cabinet chiton chocolate sauce chow christmas stocking church cicada cinema cleaver cliff cliff dwelling cloak clog clumber cock cocker spaniel cockroach cocktail shaker coffee mug coffeepot coho coil collie colobus combination lock comic book common iguana common newt computer keyboard conch confectionery consomme container ship convertible coral fungus coral reef corkscrew corn cornet coucal cougar cowboy boot cowboy hat coyote cradle crane crane crash helmet crate crayfish crib cricket crock pot croquet ball crossword puzzle crutch cucumber cuirass cup curly coated retriever custard apple daisy dalmatian dam damselfly dandie dinmont desk desktop computer dhole dial telephone diamondback diaper digital clock digital watch dingo dining table dishrag dishwasher disk brake doberman dock dogsled dome doormat dough dowitcher dragonfly drake drilling platform drum drumstick dugong dumbbell dung beetle dungeness crab dutch oven ear earthstar echidna eel eft eggnog egyptian cat electric fan electric guitar electric locomotive electric ray english foxhound english setter english springer entertainment center entlebucher envelope eskimo dog espresso espresso maker european fire salamander european gallinule face powder feather boa fiddler crab fig file fire engine fire screen fireboat flagpole flamingo flat coated retriever flatworm flute fly folding chair football helmet forklift fountain fountain pen four poster fox squirrel freight car french bulldog french horn french loaf frilled lizard frying pan fur coat gar garbage truck garden spider garter snake gas pump gasmask gazelle german shepherd german short haired pointer geyser giant panda giant schnauzer gibbon gila monster go kart goblet golden retriever goldfinch goldfish golf ball golfcart gondola gong goose gordon setter gorilla gown grand piano granny smith grasshopper great dane great grey owl great pyrenees great white shark greater swiss mountain dog green lizard green mamba green snake greenhouse grey fox grey whale grille grocery store groenendael groom ground beetle guacamole guenon guillotine guinea pig gyromitra hair slide hair spray half track hammer hammerhead hamper hamster hand blower hand held computer handkerchief hard disc hare harmonica harp hartebeest harvester harvestman hatchet hay head cabbage hen hen of the woods hermit crab hip hippopotamus hog hognose snake holster home theater honeycomb hook hoopskirt horizontal bar hornbill horned viper horse cart hot pot hotdog hourglass house finch howler monkey hummingbird hyena ibex ibizan hound ice bear ice cream ice lolly impala indian cobra indian elephant indigo bunting indri ipod irish setter irish terrier irish water spaniel irish wolfhound iron isopod italian greyhound jacamar jack o lantern jackfruit jaguar japanese spaniel jay jean jeep jellyfish jersey jigsaw puzzle jinrikisha joystick junco keeshond kelpie kerry blue terrier killer whale kimono king crab king penguin king snake kit fox kite knee pad knot koala komodo dragon komondor kuvasz lab coat labrador retriever lacewing ladle ladybug lakeland terrier lakeside lampshade langur laptop lawn mower leaf beetle leafhopper leatherback turtle lemon lens cap leonberg leopard lesser panda letter opener lhasa library lifeboat lighter limousine limpkin liner lion lionfish lipstick little blue heron llama loafer loggerhead long horned beetle lorikeet lotion loudspeaker loupe lumbermill lycaenid lynx macaque macaw madagascar cat magnetic compass magpie mailbag mailbox maillot maillot malamute malinois maltese dog manhole cover mantis maraca marimba marmoset marmot mashed potato mask matchstick maypole maze measuring cup meat loaf medicine chest meerkat megalith menu mexican hairless microphone microwave military uniform milk can miniature pinscher miniature poodle miniature schnauzer minibus miniskirt minivan mink missile mitten mixing bowl mobile home model t modem monarch monastery mongoose monitor moped mortar mortarboard mosque mosquito net motor scooter mountain bike mountain tent 246 int j comput vis 252 mouse mousetrap moving van mud turtle mushroom muzzle nail neck brace necklace nematode newfoundland night snake nipple norfolk terrier norwegian elkhound norwich terrier notebook obelisk oboe ocarina odometer oil filter old english sheepdog orange orangutan organ oscilloscope ostrich otter otterhound overskirt ox oxcart oxygen mask oystercatcher packet paddle paddlewheel padlock paintbrush pajama palace panpipe paper towel papillon parachute parallel bars park bench parking meter partridge passenger car patas patio pay phone peacock pedestal pekinese pelican pembroke pencil box pencil sharpener perfume persian cat petri dish photocopier pick pickelhaube picket fence pickup pier piggy bank pill bottle pillow pineapple ping pong ball pinwheel pirate pitcher pizza plane planetarium plastic bag plate plate rack platypus plow plunger polaroid camera pole polecat police van pomegranate pomeranian poncho pool table pop bottle porcupine pot potpie potter s wheel power drill prairie chicken prayer rug pretzel printer prison proboscis monkey projectile projector promontory ptarmigan puck puffer pug punching bag purse quail quill quilt racer racket radiator radio radio telescope rain barrel ram rapeseed recreational vehicle red fox red wine red wolf red backed sandpiper red breasted merganser redbone redshank reel reflex camera refrigerator remote control restaurant revolver rhinoceros beetle rhodesian ridgeback rifle ringlet ringneck snake robin rock beauty rock crab rock python rocking chair rotisserie rottweiler rubber eraser ruddy turnstone ruffed grouse rugby ball rule running shoe safe safety pin saint bernard saltshaker saluki samoyed sandal sandbar sarong sax scabbard scale schipperke school bus schooner scoreboard scorpion scotch terrier scottish deerhound screen screw screwdriver scuba diver sea anemone sea cucumber sea lion sea slug sea snake sea urchin sealyham terrier seashore seat belt sewing machine shetland sheepdog shield shih tzu shoe shop shoji shopping basket shopping cart shovel shower cap shower curtain siamang siamese cat siberian husky sidewinder silky terrier ski ski mask skunk sleeping bag slide rule sliding door slot sloth bear slug snail snorkel snow leopard snowmobile snowplow soap dispenser soccer ball sock soft coated wheaten terrier solar dish sombrero sorrel soup bowl space bar space heater space shuttle spaghetti squash spatula speedboat spider monkey spider web spindle spiny lobster spoonbill sports car spotlight spotted salamander squirrel monkey staffordshire bullterrier stage standard poodle standard schnauzer starfish steam locomotive steel arch bridge steel drum stethoscope stingray stinkhorn stole stone wall stopwatch stove strainer strawberry street sign streetcar stretcher studio couch stupa sturgeon submarine suit sulphur butterfly sulphur crested cockatoo sundial sunglass sunglasses sunscreen suspension bridge sussex spaniel swab sweatshirt swimming trunks swing switch syringe tabby table lamp tailed frog tank tape player tarantula teapot teddy television tench tennis ball terrapin thatch theater curtain thimble three toed sloth thresher throne thunder snake tibetan mastiff tibetan terrier tick tiger tiger beetle tiger cat tiger shark tile roof timber wolf titi toaster tobacco shop toilet seat toilet tissue torch totem pole toucan tow truck toy poodle toy terrier toyshop tractor traffic light trailer truck tray tree frog trench coat triceratops tricycle trifle trilobite trimaran tripod triumphal arch trolleybus trombone tub turnstile tusker typewriter keyboard umbrella unicycle upright vacuum valley vase vault velvet vending machine vestment viaduct vine snake violin vizsla volcano volleyball vulture waffle iron walker hound walking stick wall clock wallaby wallet wardrobe warplane warthog washbasin washer water bottle water buffalo water jug water ouzel water snake water tower weasel web site weevil weimaraner welsh springer spaniel west highland white terrier whippet whiptail whiskey jug whistle white stork white wolf wig wild boar window screen window shade windsor tie wine bottle wing wire haired fox terrier wok wolf spider wombat wood rabbit wooden spoon wool worm fence wreck yawl yellow lady s slipper yorkshire terrier yurt zebra zucchini appendix additional single object localization dataset statistics we consider two additional metrics of object localization difficulty chance performance of localization and the level of clutter we use these metrics to compare single object localization dataset to the pascal voc object detection benchmark the measures of localization difficulty are computed on the validation set of both datasets according to both of these measures of difficulty there is a subset of ilsvrc which is as challenging as pascal but more than an order of magnitude greater in size figure shows the distributions of different properties object scale chance performance of localization and level of clutter across the different classes in the two datasets chance performance of localization cpl chance performance on a dataset is a commonmetric to consider we define thecplmeasure as the expected accuracy of a detector which first randomly samples an object instance of that class and then uses its bounding box directly as the proposed localization window on all other images after rescaling the images to the same size concretely let bn be all the bounding boxes of the object instances within a class then cpl i j i i ou bi bj n n some of the most difficult ilsvrc categories to localize according to this metric are basketball swimming trunks ping pong ball and rubber eraser all with less than cpl this measure correlates strongly ρ 9 with the average scale of the object fraction of image occupied by object the average cpl across the ilsvrc categories is 8 the pascal categories have an average cpl of 8 which is the same as the cpl of the most difficult categories of ilsvrc clutter intuitively even small objects are easy to localize on a plain background to quantify clutter we employ the objectness measure of alexe et al which is a classgeneric object detector evaluating howlikely a windowin the image contains a coherent object of any class as opposed to background sky water grass for every image m containing target object instances at positions bm bm we use the publicly available objectness software to sample windows wm wm wm in order of decreasing probability of the window containing any generic object let obj m be the number of generic object looking windows sampled before localizing an instance of the target category i e obj m min k maxi iou wm k bm i 0 for a category containing m images we compute the average number of such windows per image and define clutter m m obj m the higher the clutter of a category the harder the objects are to localize according to generic cues if an object can t be localized with the first windows as is the case for of images on average per category in ilsvrc and in pascal we set obj m the fact that more than of objects can be localized with these windows imply that the objectness cue is already quite strong so objects that require many windows on average will be extremely difficult to detect e g ping pong ball clutter of 9 57 or windows on average basketball clutter of 9 21 puck clutter of 9 17 in ilsvrc the most difficult object in pascal is bottle with clutter score of 8 47 on average ilsvrc has clutter score of the most difficult subset of ilsvrc with object categories has an order of magnitude more categories and the same average amount of clutter of 90 as the pascal dataset appendix manually curated queries for obtaining object detection scene images in sect we discussed three types of queries we used for collecting the object detection images single object category name or a synonym a pair of object category names a manual query typically targetting one or more int j comput vis 252 fig distribution of various measures of localization difficulty on the ilsvrc2012 single object localization dark green and pascal voc light blue validation sets object scale is fraction of image area occupied by an average object instance chance performance of localization and level of clutter are defined inappendix the plots on top contain the full ilsvrc validation set with classes the plots on the bottom contain ilsvrc classes with the lowest chance performance of localization all plots contain all classes of pascal voc object categories with insufficient data here we provide a list of the manually curated queries afternoon tea ant bridge building armadillo race armadillo yard artist studio auscultation baby room banjo orchestra banjo rehersal banjo show califone headphones media player sets camel dessert camel tourist carpenter drilling carpentry centipede wild coffee shop continental breakfast toaster continental breakfast waffles crutch walking desert scorpion diner dining room dining table dinner dragonfly friendly dragonfly kid dragonfly pond dragonfly wild drying hair dumbbell curl fan blow wind fast food fast food restaurant firewood chopping flu shot goldfish aquarium goldfish tank golf cart on golf course gym dumbbell hamster drinking water harmonica orchestra harmonica rehersal harmonica show harp ensemble harp orchestra harp rehersal harp show hedgehog cute hedgehog floor hedgehog hidden hippo bird hippo friendly home improvement diy drill horseback riding hotel coffee machine hotel coffee maker hotel waffle maker jellyfish scuba jellyfish snorkling kitchen kitchen counter coffee maker kitchen counter toaster kitchenette koala feed koala tree ladybug flower ladybug yard laundromat lion zebra friendly lunch mailman making breakfast making waffles mexican food motorcycle racing office office fan opossum on tree branch orchestra panda play panda tree pizzeria pomegranate tree porcupine climbing trees power drill carpenter purse shop red panda tree riding competition riding motor scooters school supplies scuba starfish sea lion beach sea otter sea urchin habitat shopping for school supplies sitting in front of a fan skunk and cat skunk park skunk wild skunk yard snail flower snorkling starfish snowplow cleanup snowplow pile snowplow winter soccer game south american zoo starfish sea world starts shopping steamed artichoke stethoscope doctor strainer pasta strainer tea syringe doctor table with food tape player tiger circus tiger pet using a can opener using power drill waffle iron breakfast wild lion savana wildlife preserve animals wiping dishes wombat petting zoo zebra savana zoo feeding zoo in australia appendix 4 hierarchy of questions for full image annotation the following is a hierarchy of questions manually constructed for crowdsourcing full annotation of images with the presence or absence of object detection categories in and all questions are of the form is there a in the image questions marked with are asked on every image if the answer to a question is determined to be no then the answer to all descendant questions is assumed to be no the numbered leaf nodes correspond to the object detection categories the goal in the hierarchy construction is to save cost by asking as few questions as possible on every image while avoiding any ambiguity in questions which would lead to false negatives during annotation this hierarchy is not treestructured some questions have multiple parents hierarchy of questions first aid medical items stethoscope syringe neck brace 4 crutch stretcher band aid an adhesive bandage to cover small cuts or blisters musical instruments accordion a portable box shaped free reed instrument the reeds are made to vibrate by air from the bellows controlled by the player 8 piano pianoforte forte piano percussion instruments chimes maraccas drums etc 9 chime a percussion instrument consisting of a set of tuned bells that are struck with a hammer used as an orchestral instrument maraca drum stringed instrument banjo the body of a banjo is round please do not confuse with guitar cello a large stringed instrument seated player holds it upright while playing violin bowed stringed instrument that has four strings a hollow body an unfretted fingerboard and is played with a bow please do not confuse with cello which is held upright while playing harp guitar please do not confuse with banjo the body of a banjo is round 248 int j comput vis 252 wind instrument a musical instrument in which the sound is produced by an enclosed column of air that is moved by the breath such as trumpet french horn harmonica flute etc 17 trumpet a brass musical instrument with a narrow tube and a flared bell which is played by means of valves often has keys on top french horn a brass musical instrument consisting of a conical tube that is coiled into a spiral with a flared bell at the end trombone a brass instrument consisting of a long tube whose length can be varied by a u shaped slide harmonica 21 flute a high pitched musical instrument that looks like a straight tube and is usually played sideways please do not confuse with oboes which have a distinctive straw like mouth piece and a slightly flared end oboe a slender musical instrument roughly long with metal keys a distinctive straw like mouthpiece and often a slightly flared end please do not confuse with flutes saxophone a musical instrument consisting of a brass conical tube often with a u bend at the end food something you can eat or drink includes growing fruit vegetables and mushrooms but does not include living animals food with bread or crust pretzel bagel pizza hotdog hamburgers etc pretzel 25 bagel beigel pizza pizza pie hotdog hot dog red hot hamburger beefburger burger guacamole 30 burrito 31 popsicle ice cream or water ice on a small wooden stick fruit 32 fig pineapple ananas banana pomegranate 36 apple strawberry orange lemon vegetables 40 cucumber cuke 41 artichoke globe artichoke 42 bell pepper 43 head cabbage mushroom items that run on electricity plugged in or using batteries including clocks microphones traffic lights computers etc 45 remote control remote electronics that blow air 46 hair dryer blow dryer 47 electric fan a device for creating a current of air by movement of a surface or surfaces please do not consider hair dryers electronics that can play music or amplify sound 48 tape player 49 ipod microphone mike computer and computer peripherals mouse laptop printer keyboard etc 51 computer mouse 52 laptop laptop computer 53 printer please do not consider typewriters to be printers 54 computer keyboard 55 lamp electric cooking appliance an appliance which generates heat to cook food or boil water microwave microwave oven 57 toaster 58 waffle iron coffee maker a kitchen appliance used for brewing coffee automatically 60 vacuum vacuum cleaner 61 dishwasher dish washer dishwashing machine 62 washer washing machine an electric appliance for washing clothes 63 traffic light traffic signal stoplight tv or monitor an electronic device that represents information in visual form 65 digital clock a clock that displays the time of day digitally kitchen items tools utensils and appliances usually found in the kitchen electric cooking appliance an appliance which generates heat to cook food or boil water 56 microwave microwave oven 57 toaster 58 waffle iron 59 coffee maker a kitchen appliance used for brewing coffee automatically 61 dishwasher dish washer dishwashing machine stove things used to open cans bottles can opener or corkscrew 67 can opener tin opener corkscrew 69 cocktail shaker non electric item commonly found in the kitchen pot pan utensil bowl etc strainer frying pan skillet bowl a dish for serving food that is round open at the top and has no handles please do not confuse with a cup which usually has a handle and is used for serving drinks salt or pepper shaker a shaker with a perforated top for sprinkling salt or pepper plate rack spatula a turner with a narrow flexible blade ladle a spoon shaped vessel with a long handle frequently used to transfer liquids from one container to another refrigerator icebox furniture including benches 78 bookshelf a shelf on which to keep books baby bed small bed for babies enclosed by sides to prevent baby from falling 80 filing cabinet office furniture consisting of a container for keeping papers in order bench a long seat for several people typically made of wood or stone 82 chair a raised piece of furniture for one person to sit on please do not confuse with benches or sofas which are made for more people 83 sofa couch upholstered seat for more than one person please do not confuse with benches which are made of wood or stone or with chairs which are for just one person table clothing article of clothing a covering designed to be worn on a person s body diaper garment consisting of a folded cloth drawn up between the legs and fastened at the waist worn by infants to catch excrement swimming attire clothes used for swimming or bathing swim suits swim trunks bathing caps swimming trunks swimsuit worn by men while swimming bathing cap swimming cap a cap worn to keep hair dry while swimming or showering maillot a woman s one piece bathing suit necktie a man s formal article of clothing worn around the neck including bow ties bow tie a man s tie that ties in a bow 90 tie a long piece of cloth worn for decorative purposes around the neck or shoulders resting under the shirt collar and knotted at the throat not a bow tie headdress headgear clothing for the head hats helmets bathing caps etc bathing cap swimming cap a cap worn to keep hair dry while swimming or showering 91 hat with a wide brim helmet protective headgear made of hard material to resist blows 93 miniskirt mini a very short skirt brassiere bra an undergarment worn by women to support their breasts sunglasses living organism other than people dogs snakes fish insects sea urchins starfish etc living organism which can fly bee dragonfly 98 ladybug butterfly 100 bird living organism which cannot fly please don t include humans living organism with or 4 legs please don t include humans mammals but please do not include humans feline cat like animal cat tiger or lion domestic cat tiger lion canine dog like animal dog hyena fox or wolf dog domestic dog canis familiaris fox wild carnivorous mammal with pointed muzzle and ears and a bushy tail please do not confuse with dogs animals with hooves camels elephants hippos pigs sheep etc elephant hippopotamus hippo camel swine pig or boar sheep woolly animal males have large spiraling horns please do not confuse with antelope which have long legs cattle cows or oxen domestic bovine animals zebra horse antelope a graceful animal with long legs and horns directed upward and backward squirrel hamster short tailed burrowing rodent with large cheek pouches otter monkey koala bear bear other than pandas skunk mammal known for its ability fo spray a liquid with a strong odor they may have a single thick stripe across back and tail two thinner stripes or a series of white spots and broken stripes rabbit giant panda an animal characterized by its distinct black and white markings red panda reddish brown old world raccoon like carnivore frog toad lizard please do not confuse with snake lizards have legs turtle armadillo porcupine hedgehog living organism with or more legs lobster scorpion insects etc lobster large marine crustaceans with long bodies and muscular tails three of their five pairs of legs have claws scorpion centipede an arthropod having a flattened body of to segments each with a pair of legs the foremost pair being modified as prehensors tick a small creature with 4 pairs of legs which lives on the blood of mammals and birds isopod a small crustacean with seven pairs of legs adapted for crawling ant living organism without legs fish snake seal etc please don t include plants living organism that lives in water seal whale fish sea cucumber etc jellyfish starfish sea star seal whale ray a marine animal with a horizontally flattened body and enlarged winglike pectoral fins with gills on the underside goldfish small golden or orange red fishes living organism that slides on land worm snail snake snail snake please do not confuse with lizard snakes do not have legs vehicle any object used to move people or objects from place to place a vehicle with wheels golfcart golf cart snowplow a vehicle used to push snow from roads motorcycle or moped car automobile not a golf cart or a bus bus a vehicle carrying many passengers used for public transport train cart a heavy open wagon usually having two wheels and drawn by an animal bicycle bike a two wheeled vehicle moved by foot pedals unicycle monocycle a vehicle without wheels snowmobile sleighs snowmobile tracked vehicle for travel on snow watercraft such as ship or boat a craft designed for water transportation airplane an aircraft powered by propellers or jets cosmetics toiletry designed to beautify the body face powder perfume essence usually comes in a smaller bottle than hair spray hair spray cream ointment lotion lipstick lip rouge carpentry items items used in carpentry including nails hammers axes screwdrivers drills chain saws etc chain saw chainsaw nail pin shaped with a head on one end and a point on the other axe a sharp tool often used to cut trees logs hammer a blunt hand tool used to drive nails in or break things apart please do not confuse with axe which is sharp screwdriver power drill a power tool for drilling holes into hard materials school supplies rulers erasers pencil sharpeners pencil boxes binders ruler rule measuring stick consisting of a strip of wood or metal or plastic with a straight edge that is used for drawing straight lines and measuring lengths rubber eraser rubber pencil eraser int j comput vis 252 pencil sharpener pencil box pencil case binder ring binder sports items items used to play sports or in the gym such as skis raquets gymnastics bars bows punching bags balls bow weapon for shooting arrows composed of a curved piece of resilient wood with a taut cord to propel the arrow puck hockey puck vulcanized rubber disk inches in diameter that is used instead of a ball in ice hockey ski racket racquet gymnastic equipment parallel bars high beam etc balance beam a horizontal bar used for gymnastics which is raised from the floor and wide enough to walk on horizontal bar high bar used for gymnastics gymnasts grip it with their hands please do not confuse with balance beam which is wide enough to walk on ball golf ball baseball basketball croquet ball soccer ball ping pong ball rugby ball volleyball tennis ball punching bag punch bag punching ball punchball dumbbell an exercising weight two spheres connected by a short bar that serves as a handle liquid container vessels which commonly contain liquids such as bottles cans etc pitcher a vessel with a handle and a spout for pouring beaker a flatbottomed jar made of glass or plastic used for chemistry milk can 192 soap dispenser wine bottle water bottle cup or mug usually with a handle and usually cylindrical bag backpack a bag carried by a strap on your back or shoulder purse a small bag for carrying money plastic bag person flower pot a container in which plants are cultivated appendix modification to bounding box system for object detection the bounding box annotation system described in sect is used for annotating images for both the single object localization dataset and the object detection dataset however two additional manual post processing are needed to ensure accuracy in the object detection scenario ambiguous objects the first common source of error was that workers were not able to accurately differentiate some object classes during annotation some commonly confused labels were seal and sea otter backpack and purse banjo and guitar violin and cello brass instruments trumpet trombone french horn and brass flute and oboe ladle and spatula despite our best efforts providing positive and negative example images in the annotation task adding text explanations to alert the user to the distinction between these categories these errors persisted in the single object localization setting this problem was not as prominent for two reasons first the way the data was collected imposed a strong prior on the object class which was present second since only one object category needed to be annotated per image ambiguous images could be discarded for example if workers couldn t agree on whether or not a trumpet was in fact present this image could simply be removed in contrast for the object detection setting consensus had to be reached for all target categories on all images to fix this problem once bounding box annotations were collected we manually looked through all cases where the bounding boxes for two different object classes had significant overlap with each other about of the collected boxes about a quarter of these boxes were found to correspond to incorrect objects and were removed crowdsourcing this post processing step with very stringent accuracy constraints would be possible but it occurred in few enough cases that itwas faster and more accurate to do this in house duplicate annotations the second common source of error were duplicate bounding boxes drawn on the same object instance despite instructions not to draw more than one bounding box around the same object instance and constraints in the annotation ui enforcing at least a pixel difference between different bounding boxes these errors persisted one reasonwas that sometimes the initial bounding box was not perfect and subsequent labelers drew a slightly improved alternative this type of error was also present in the single object localization scenario but was not a major cause for concern a duplicate bounding box is a slightly perturbed but still correct positive example and single object localization is only concerned with correctly localizing one object instance for the detection task algorithms are evaluated on the ability to localize every object instance and penalized for duplicate detections so it is imperative that these labeling errors are corrected even if they only appear in about 0 of cases approximately of bounding boxes were found to have significant overlap of more than 50 with another bounding box of the same object class we again manually verified all of these cases in house in approximately 40 of the cases the two bounding boxes correctly corresponded to different people in a crowd to stacked plates or tomusical instruments nearby in an orchestra in the other 60 of cases one of the boxes was randomly removed these verification steps complete the annotation procedure of bounding boxes around every instance of every object class in validation test and a subset of training images for the detection task training set annotation with the optimized algorithm of sect we fully annotated the validation and test sets however annotating all training images with all target object classes was still a budget challenge positive training images taken from the single object localization dataset already had bounding box annotations of all instances of one object class on each image we extended the existing annotations to the detection dataset by making two modification first we corrected any bounding box omissions resulting from merging fine grained categories i e if an image belonged to the dalmatian category and all instances of dalmatian were annotated with bounding boxes for single object localization we ensured that all remaining dog instances are also annotated for the object detection task second we collected significantly more training data for the person class because 250 int j comput vis 252 the existing annotation set was not diverse enough to be representative the only people categories in the single object localization task are scuba diver groom and ballplayer to compensate we additionally annotated people in a large fraction of the existing training set images appendix competition protocol competition format at the beginning of the competition period each year we release the new training validation test images training validation annotations and competition specification for the year we then specify a deadline for submission usually approximately 4 months after the release of data teams are asked to upload a text file of their predicted annotations on test images by this deadline to a provided server we then evaluate all submissions and release the results for every task we released code that takes a text file of automatically generated image annotations and compares it with the ground truth annotations to return a quantitative measure of algorithm accuracy teams can use this code to evaluate their performance on the validation data as described in everingham et al there are three options for measuring performance on test data i release test images and annotations and allow participants to assess performance themselves ii release test images but not test annotations participants submit results and organizers assess performance iii neither test images nor annotations are released participants submit software and organizers run it on new data and assess performance in line with the pascal voc choice we opted for option ii option i allows too much leeway in overfitting to the test data option iii is infeasible especially given the scale of our test set images we released test annotations for the image classification task but all other test annotations have remained hidden to discourage fine tuning results on the test data evaluation protocol after the challenge after the challenge period we set up an automatic evaluation server that researchers can use throughout the year to continue evaluating their algorithms against the ground truth test annotations we limit teams to submissions per week to discourage parameter tuning on the test data and in practice we have never had a problem with researchers abusing the system in this work we investigate the effect of the convolutional network depth on its accuracy in the large scale image recognition setting our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small convolution filters which shows that a significant improvement on the prior art configurations can be achieved by pushing the depth to weight layers these findings were the basis of our imagenet challenge submission where our team secured the first and the second places in the localisa tion and classification tracks respectively we also show that our representations generalise well to other datasets where they achieve state of the art results we have made our two best performing convnet models publicly available to facili tate further research on the use of deep visual representations in computer vision i ntroduction convolutional networks convnets have recently enjoyed a great success in large scale im age and video recognition krizhevsky et al zeiler fergus sermanet et al simonyan zisserman which has become possible due to the large public image reposito ries such as imagenet deng et al and high performance computing systems such as gpus or large scale distributed clusters dean et al in particular an important role in the advance of deep visual recognition architectures has been played by the imagenet large scale visual recog nition challenge ilsvrc russakovsky et al which has served as a testbed for a few generations of large scale image classification systems from high dimensional shallow feature en codings perronnin et al the winner of ilsvrc to deep convnets krizhevsky et al the winner of ilsvrc with convnets becoming more of a commodity in the computer vision field a number of at tempts have been made to improve the original architecture of krizhevsky et al in a bid to achieve better accuracy for instance the best performing submissions to the ilsvrc zeiler fergus sermanet et al utilised smaller receptive window size and smaller stride of the first convolutional layer another line of improvements dealt with training and testing the networks densely over the whole image and over multiple scales sermanet et al howard in this paper we address another important aspect of convnet architecture design its depth to this end we fix other parameters of the architecture and steadily increase the depth of the network by adding more convolutional layers which is feasible due to the use of very small 3 3 convolution filters in all layers as a result we come up with significantly more accurate convnet architectures which not only achieve the state of the art accuracy on ilsvrc classification and localisation tasks but are also applicable to other image recognition datasets where they achieve excellent performance even when used as a part of a relatively simple pipelines e g deep features classified by a linear svm without fine tuning we have released our two best performing to facilitate further research the rest of the paper is organised as follows in sect we describe our convnet configurations the details of the image classification training and evaluation are then presented in sect 3 and the current affiliation google deepmind current affiliation university of oxford and google deepmind http www robots ox ac uk vgg research published as a conference paper at iclr configurations are compared on the ilsvrc classification task in sect 4 sect concludes the paper for completeness we also describe and assess our ilsvrc object localisation system in appendix a and discuss the generalisation of very deep features to other datasets in appendix b finally appendix c contains the list of major paper revisions c onv n et c onfigurations to measure the improvement brought by the increased convnet depth in a fair setting all our convnet layer configurations are designed using the same principles inspired by ciresan et al krizhevsky et al in this section we first describe a generic layout of our convnet configurations sect and then detail the specific configurations used in the evaluation sect our design choices are then discussed and compared to the prior art in sect 3 a rchitecture during training the input to our convnets is a fixed size rgb image the only pre processing we do is subtracting the mean rgb value computed on the training set from each pixel the image is passed through a stack of convolutional conv layers where we use filters with a very small receptive field 3 3 which is the smallest size to capture the notion of left right up down center in one of the configurations we also utilise convolution filters which can be seen as a linear transformation of the input channels followed by non linearity the convolution stride is fixed to pixel the spatial padding of conv layer input is such that the spatial resolution is preserved after convolution i e the padding is pixel for 3 3 conv layers spatial pooling is carried out by five max pooling layers which follow some of the conv layers not all the conv layers are followed by max pooling max pooling is performed over a 2 pixel window with stride 2 a stack of convolutional layers which has a different depth in different architectures is followed by three fully connected fc layers the first two have channels each the third performs way ilsvrc classification and thus contains channels one for each class the final layer is the soft max layer the configuration of the fully connected layers is the same in all networks all hidden layers are equipped with the rectification relu krizhevsky et al non linearity we note that none of our networks except for one contain local response normalisation lrn normalisation krizhevsky et al as will be shown in sect 4 such normalisation does not improve the performance on the ilsvrc dataset but leads to increased memory con sumption and computation time where applicable the parameters for the lrn layer are those of krizhevsky et al 2 2 c onfigurations the convnet configurations evaluated in this paper are outlined in table one per column in the following we will refer to the nets by their names a e all configurations follow the generic design presented in sect 2 and differ only in the depth from weight layers in the network a 8 conv and 3 fc layers to weight layers in the network e conv and 3 fc layers the width of conv layers the number of channels is rather small starting from in the first layer and then increasing by a factor of 2 after each max pooling layer until it reaches in table 2 we report the number of parameters for each configuration in spite of a large depth the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv layer widths and receptive fields weights in sermanet et al 2 3 d iscussion our convnet configurations are quite different from the ones used in the top performing entries of the ilsvrc krizhevsky et al and ilsvrc competitions zeiler fergus sermanet et al rather than using relatively large receptive fields in the first conv lay ers e g with stride 4 in krizhevsky et al or with stride 2 in zeiler fergus sermanet et al we use very small 3 3 receptive fields throughout the whole net which are convolved with the input at every pixel with stride it is easy to see that a stack of two 3 3 conv layers without spatial pooling in between has an effective receptive field of three 2 published as a conference paper at iclr table 1 convnet configurations shown in columns the depth of the configurations increases from the left a to the right e as more layers are added the added layers are shown in bold the convolutional layer parameters are denoted as convhreceptive field sizei hnumber of channelsi the relu activation function is not shown for brevity convnet configuration a a lrn b c d e weight weight weight weight weight weight layers layers layers layers layers layers input rgb image 64 64 64 64 64 lrn 64 64 64 64 maxpool 128 128 128 128 128 128 128 maxpool maxpool maxpool maxpool fc fc fc soft max table 2 number of parameters in millions network a a lrn b c d e number of parameters 138 such layers have a effective receptive field so what have we gained by using for instance a stack of three 3 3 conv layers instead of a single layer first we incorporate three non linear rectification layers instead of a single one which makes the decision function more discriminative second we decrease the number of parameters assuming that both the input and the output of a three layer 3 3 convolution stack has c channels the stack is parametrised by 3 32 c 2 27c 2 weights at the same time a single conv layer would require c 2 49c 2 parameters i e more this can be seen as imposing a regularisation on the conv filters forcing them to have a decomposition through the 3 3 filters with non linearity injected in between the incorporation of 1 1 conv layers configuration c table 1 is a way to increase the non linearity of the decision function without affecting the receptive fields of the conv layers even though in our case the 1 1 convolution is essentially a linear projection onto the space of the same dimensionality the number of input and output channels is the same an additional non linearity is introduced by the rectification function it should be noted that 1 1 conv layers have recently been utilised in the network in network architecture of lin et al small size convolution filters have been previously used by ciresan et al but their nets are significantly less deep than ours and they did not evaluate on the large scale ilsvrc dataset goodfellow et al applied deep convnets weight layers to the task of street number recognition and showed that the increased depth led to better performance googlenet szegedy et al a top performing entry of the ilsvrc classification task was developed independently of our work but is similar in that it is based on very deep convnets 3 published as a conference paper at iclr weight layers and small convolution filters apart from 3 3 they also use 1 1 and convolutions their network topology is however more complex than ours and the spatial reso lution of the feature maps is reduced more aggressively in the first layers to decrease the amount of computation as will be shown in sect 4 our model is outperforming that of szegedy et al in terms of the single network classification accuracy 3 c lassification f ramework in the previous section we presented the details of our network configurations in this section we describe the details of classification convnet training and evaluation 3 1 t raining the convnet training procedure generally follows krizhevsky et al except for sampling the input crops from multi scale training images as explained later namely the training is carried out by optimising the multinomial logistic regression objective using mini batch gradient descent based on back propagation lecun et al with momentum the batch size was set to momentum to 0 9 the training was regularised by weight decay the penalty multiplier set to 4 and dropout regularisation for the first two fully connected layers dropout ratio set to 0 the learning rate was initially set to 2 and then decreased by a factor of when the validation set accuracy stopped improving in total the learning rate was decreased 3 times and the learning was stopped after iterations epochs we conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to krizhevsky et al the nets required less epochs to converge due to a implicit regularisation imposed by greater depth and smaller conv filter sizes b pre initialisation of certain layers the initialisation of the network weights is important since bad initialisation can stall learning due to the instability of gradient in deep nets to circumvent this problem we began with training the configuration a table 1 shallow enough to be trained with random initialisation then when training deeper architectures we initialised the first four convolutional layers and the last three fully connected layers with the layers of net a the intermediate layers were initialised randomly we did not decrease the learning rate for the pre initialised layers allowing them to change during learning for random initialisation where applicable we sampled the weights from a normal distribution with the zero mean and 2 variance the biases were initialised with zero it is worth noting that after the paper submission we found that it is possible to initialise the weights without pre training by using the random initialisation procedure of glorot bengio to obtain the fixed size convnet input images they were randomly cropped from rescaled training images one crop per image per sgd iteration to further augment the training set the crops underwent random horizontal flipping and random rgb colour shift krizhevsky et al training image rescaling is explained below training image size let s be the smallest side of an isotropically rescaled training image from which the convnet input is cropped we also refer to s as the training scale while the crop size is fixed to in principle s can take on any value not less than for s the crop will capture whole image statistics completely spanning the smallest side of a training image for s the crop will correspond to a small part of the image containing a small object or an object part we consider two approaches for setting the training scale s the first is to fix s which corresponds to single scale training note that image content within the sampled crops can still represent multi scale image statistics in our experiments we evaluated models trained at two fixed scales s which has been widely used in the prior art krizhevsky et al zeiler fergus sermanet et al and s given a convnet configuration we first trained the network using s to speed up training of the s network it was initialised with the weights pre trained with s and we used a smaller initial learning rate of 3 the second approach to setting s is multi scale training where each training image is individually rescaled by randomly sampling s from a certain range smin smax we used smin and smax since objects in images can be of different size it is beneficial to take this into account during training this can also be seen as training set augmentation by scale jittering where a single 4 published as a conference paper at iclr model is trained to recognise objects over a wide range of scales for speed reasons we trained multi scale models by fine tuning all layers of a single scale model with the same configuration pre trained with fixed s 3 2 t esting at test time given a trained convnet and an input image it is classified in the following way first it is isotropically rescaled to a pre defined smallest image side denoted as q we also refer to it as the test scale we note that q is not necessarily equal to the training scale s as we will show in sect 4 using several values of q for each s leads to improved performance then the network is applied densely over the rescaled test image in a way similar to sermanet et al namely the fully connected layers are first converted to convolutional layers the first fc layer to a conv layer the last two fc layers to 1 1 conv layers the resulting fully convolutional net is then applied to the whole uncropped image the result is a class score map with the number of channels equal to the number of classes and a variable spatial resolution dependent on the input image size finally to obtain a fixed size vector of class scores for the image the class score map is spatially averaged sum pooled we also augment the test set by horizontal flipping of the images the soft max class posteriors of the original and flipped images are averaged to obtain the final scores for the image since the fully convolutional network is applied over the whole image there is no need to sample multiple crops at test time krizhevsky et al which is less efficient as it requires network re computation for each crop at the same time using a large set of crops as done by szegedy et al can lead to improved accuracy as it results in a finer sampling of the input image compared to the fully convolutional net also multi crop evaluation is complementary to dense evaluation due to different convolution boundary conditions when applying a convnet to a crop the convolved feature maps are padded with zeros while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image due to both the convolutions and spatial pooling which substantially increases the overall network receptive field so more context is captured while we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy for reference we also evaluate our networks using 50 crops per scale regular grid with 2 flips for a total of crops over 3 scales which is comparable to crops over 4 scales used by szegedy et al 3 3 i mplementation d etails our implementation is derived from the publicly available c caffe toolbox jia branched out in december but contains a number of significant modifications allowing us to perform training and evaluation on multiple gpus installed in a single system as well as train and evaluate on full size uncropped images at multiple scales as described above multi gpu training exploits data parallelism and is carried out by splitting each batch of training images into several gpu batches processed in parallel on each gpu after the gpu batch gradients are computed they are averaged to obtain the gradient of the full batch gradient computation is synchronous across the gpus so the result is exactly the same as when training on a single gpu while more sophisticated methods of speeding up convnet training have been recently pro posed krizhevsky which employ model and data parallelism for different layers of the net we have found that our conceptually much simpler scheme already provides a speedup of 3 75 times on an off the shelf 4 gpu system as compared to using a single gpu on a system equipped with four nvidia titan black gpus training a single net took 2 3 weeks depending on the architecture 4 c lassification e xperiments dataset in this section we present the image classification results achieved by the described convnet architectures on the ilsvrc dataset which was used for ilsvrc chal lenges the dataset includes images of classes and is split into three sets training 1 3m images validation images and testing images with held out class labels the clas sification performance is evaluated using two measures the top 1 and top error the former is a multi class classification error i e the proportion of incorrectly classified images the latter is the 5 published as a conference paper at iclr main evaluation criterion used in ilsvrc and is computed as the proportion of images such that the ground truth category is outside the top 5 predicted categories for the majority of experiments we used the validation set as the test set certain experiments were also carried out on the test set and submitted to the official ilsvrc server as a vgg team entry to the ilsvrc competition russakovsky et al 4 1 s ingle s cale e valuation we begin with evaluating the performance of individual convnet models at a single scale with the layer configurations described in sect 2 2 the test image size was set as follows q s for fixed s and q 0 5 smin smax for jittered s smin smax the results of are shown in table 3 first we note that using local response normalisation a lrn network does not improve on the model a without any normalisation layers we thus do not employ normalisation in the deeper architectures b e second we observe that the classification error decreases with the increased convnet depth from layers in a to layers in e notably in spite of the same depth the configuration c which contains three 1 1 conv layers performs worse than the configuration d which uses 3 3 conv layers throughout the network this indicates that while the additional non linearity does help c is better than b it is also important to capture spatial context by using conv filters with non trivial receptive fields d is better than c the error rate of our architecture saturates when the depth reaches layers but even deeper models might be beneficial for larger datasets we also compared the net b with a shallow net with five 5 5 conv layers which was derived from b by replacing each pair of 3 3 conv layers with a single 5 5 conv layer which has the same receptive field as explained in sect 2 3 the top 1 error of the shallow net was measured to be higher than that of b on a center crop which confirms that a deep net with small filters outperforms a shallow net with larger filters finally scale jittering at training time s leads to significantly better results than training on images with fixed smallest side s or s even though a single scale is used at test time this confirms that training set augmentation by scale jittering is indeed helpful for capturing multi scale image statistics table 3 convnet performance at a single test scale convnet config table 1 smallest image side top 1 val error top 5 val error train s test q a 4 a lrn 10 5 b 9 9 1 9 4 c 1 9 3 3 8 8 0 8 8 d 8 8 25 8 1 3 9 0 e 9 8 25 5 8 0 4 2 m ulti s cale e valuation having evaluated the convnet models at a single scale we now assess the effect of scale jittering at test time it consists of running a model over several rescaled versions of a test image corresponding to different values of q followed by averaging the resulting class posteriors considering that a large discrepancy between training and testing scales leads to a drop in performance the models trained with fixed s were evaluated over three test image sizes close to the training one q s 32 s s 32 at the same time scale jittering at training time allows the network to be applied to a wider range of scales at test time so the model trained with variable s smin smax was evaluated over a larger range of sizes q smin 0 5 smin smax smax published as a conference paper at iclr the results presented in table 4 indicate that scale jittering at test time leads to better performance as compared to evaluating the same model at a single scale shown in table 3 as before the deepest configurations d and e perform the best and scale jittering is better than training with a fixed smallest side s our best single network performance on the validation set is 8 7 5 top 1 top 5 error highlighted in bold in table 4 on the test set the configuration e achieves 7 3 top 5 error table 4 convnet performance at multiple test scales convnet config table 1 smallest image side top 1 val error top 5 val error train s test q b 28 2 9 27 7 9 2 c 27 8 9 2 512 512 26 3 8 2 224 288 26 8 d 416 26 5 8 6 512 512 8 7 5 224 288 26 9 8 7 e 416 26 7 8 6 512 512 8 7 5 4 3 m ulti crop evaluation in table 5 we compare dense convnet evaluation with mult crop evaluation see sect 3 2 for de tails we also assess the complementarity of the two evaluation techniques by averaging their soft max outputs as can be seen using multiple crops performs slightly better than dense evaluation and the two approaches are indeed complementary as their combination outperforms each of them as noted above we hypothesize that this is due to a different treatment of convolution boundary conditions table 5 convnet evaluation techniques comparison in all experiments the training scale s was sampled from 512 and three test scales q were considered 512 convnet config table 1 evaluation method top 1 val error top 5 val error dense 8 7 5 d multi crop 6 7 5 multi crop dense 4 7 2 dense 8 7 5 e multi crop 6 7 4 multi crop dense 4 7 1 4 4 c onv n et f usion up until now we evaluated the performance of individual convnet models in this part of the exper iments we combine the outputs of several models by averaging their soft max class posteriors this improves the performance due to complementarity of the models and was used in the top ilsvrc submissions in krizhevsky et al and zeiler fergus sermanet et al the results are shown in table 6 by the time of ilsvrc submission we had only trained the single scale networks as well as a multi scale model d by fine tuning only the fully connected layers rather than all layers the resulting ensemble of 7 networks has 7 3 ilsvrc test error after the submission we considered an ensemble of only two best performing multi scale models configurations d and e which reduced the test error to 7 0 using dense evaluation and 6 8 using combined dense and multi crop evaluation for reference our best performing single model achieves 7 1 error model e table 5 4 5 c omparison with the s tate of the a rt finally we compare our results with the state of the art in table 7 in the classification task of ilsvrc challenge russakovsky et al our vgg team secured the place with 7 published as a conference paper at iclr table 6 multiple convnet fusion results error combined convnet models top 1 val top 5 val top 5 test ilsvrc submission d 224 288 d 352 416 d 512 512 c 224 288 c 352 384 416 7 7 5 7 3 e 224 288 e 384 352 384 416 post submission d 512 384 512 e 512 384 512 dense eval 0 7 1 7 0 d 512 256 384 512 e 256 512 256 384 512 multi crop 9 7 2 d 256 512 256 384 512 e 256 512 256 384 512 multi crop dense eval 7 6 8 6 8 7 3 test error using an ensemble of 7 models after the submission we decreased the error rate to 6 8 using an ensemble of 2 models as can be seen from table 7 our very deep convnets significantly outperform the previous gener ation of models which achieved the best results in the ilsvrc and ilsvrc competi tions our result is also competitive with respect to the classification task winner googlenet with 6 7 error and substantially outperforms the ilsvrc winning submission clarifai which achieved 2 with outside training data and 7 without it this is remarkable considering that our best result is achieved by combining just two models significantly less than used in most ilsvrc submissions in terms of the single net performance our architecture achieves the best result 7 0 test error outperforming a single googlenet by 0 9 notably we did not depart from the classical convnet architecture of lecun et al but improved it by substantially increasing the depth table 7 comparison with the state of the art in ilsvrc classification our method is denoted as vgg only the results obtained without outside training data are reported method top 1 val error top 5 val error top 5 test error vgg 2 nets multi crop dense eval 7 6 8 6 8 vgg 1 net multi crop dense eval 4 7 1 7 0 vgg ilsvrc submission 7 nets dense eval 7 7 5 7 3 googlenet szegedy et al 1 net 7 9 googlenet szegedy et al 7 nets 6 7 msra he et al nets 8 1 msra he et al 1 net 27 9 9 1 9 1 clarifai russakovsky et al multiple nets 11 7 clarifai russakovsky et al 1 net 12 5 zeiler fergus zeiler fergus 6 nets 36 0 14 7 14 8 zeiler fergus zeiler fergus 1 net 37 5 0 1 overfeat sermanet et al 7 nets 34 0 2 6 overfeat sermanet et al 1 net 35 7 14 2 krizhevsky et al krizhevsky et al 5 nets 1 4 4 krizhevsky et al krizhevsky et al 1 net 40 7 2 we propose a natural scene statistic based expanded to ensure that the end user is presented with a distortion generic blind no reference nr image quality satisfactory quality of experience qoe while traditional assessment iqa model that operates in the spatial domain qoe methods have focused on optimizing delivery networks the new model dubbed blind referenceless image spatial quality evaluator brisque does not compute distortion specific with respect to throughput buffer lengths and capacity per features such as ringing blur or blocking but instead uses scene ceptually optimized delivery of multimedia services is also statistics of locally normalized luminance coefficients to quantify fast gaining importance this is especially timely given the possible losses of naturalness in the image due to the presence explosive growth in especially wireless video traffic and of distortions thereby leading to a holistic measure of quality the expected shortfalls in bandwidth these perceptual approaches underlying features used derive from the empirical distribution of locally normalized luminances and products of locally normalized attempt to deliver an optimized qoe to the end user by luminances under a spatial natural scene statistic model no utilizing objective measures of visual quality transformation to another coordinate frame dct wavelet etc objective blind or no reference nr image quality assess is required distinguishing it from prior nr iqa approaches ment iqa refers to automatic quality assessment of an image despite its simplicity we are able to show that brisque using an algorithm such that the only information that the is statistically better than the full reference peak signal to noise ratio and the structural similarity index and is highly algorithm receives before it makes a prediction on quality is competitive with respect to all present day distortion generic the distorted image whose quality is being assessed on the nr iqa algorithms brisque has very low computational other end of the spectrum lie full reference fr algorithms complexity making it well suited for real time applications that require as input not only the distorted image but also brisque features may be used for distortion identification as a clean pristine reference image with respect to which the well to illustrate a new practical application of brisque we describe how a nonblind image denoising algorithm can quality of the distorted image is assessed somewhere between be augmented with brisque in order to perform blind these two extremes lie reduced reference rr approaches that image denoising results show that brisque augmentation possess some information regarding the reference image eg leads to performance improvements over state of the art a watermark but not the actual reference image itself apart methods a software release of brisque is available online from the distorted image http live ece utexas edu research quality zip for public use and evaluation our approach to nr iqa is based on the principle that natural possess certain regular statistical properties index terms blind quality assessment denoising natural that are measurably modified by the presence of distortions scene statistics no reference image quality assessment spatial domain figure a and b shows examples of natural and artificial images from the tid database respectively the normalized luminance coefficients explained later of the natural image i i ntroduction closely follow gaussian like distribution as shown in fig c w ith the launch of networked handheld devices which can capture store compress send and display a variety of audiovisual stimuli high definition television hdtv while the same doesnot hold for the empirical distribution of the artificial image shown in fig d deviations from the regularity of natural statistics when streaming internet protocol tv iptv and websites such as quantified appropriately enable the design of algorithms capa youtube facebook and flickr etc an enormous amount of ble of assessing the perceptual quality of an image without visual data of visual data is making its way to consumers the need for any reference image by quantifying natural because of this considerable time and resources are being image statistics and refraining from an explicit character manuscript received january revised july accepted ization of distortions our approach to quality assessment august date of publication august date of current is not limited by the type of distortions that afflict the version november this work was supported by the national image such approaches to nr iqa are significant since most science foundation under grant ccf and grant iis and by intel corporation and cisco systems inc under the video aware wireless current approaches are distortion specific i e they networks vawn program the associate editor coordinating the review of are capable of performing blind iqa only if the distortion this manuscript and approving it for publication was prof alex chichung kot the authors are with the laboratory for image and video engineering department of electrical and computer engineering university of texas natural images are not necessarily images of natural environments such at austin austin tx usa e mail mittal anish gmail com as trees or skies any natural light image that is captured by an optical camera anushmoorthy gmail com bovik ece utexas edu and is not subjected to artificial processing on a computer is regarded as a color versions of one or more of the figures in this paper are available natural image of course image sensors may capture natural radiation other online at http ieeexplore ieee org than visible light but the images formed may obey different nss than those digital object identifier tip considered here ieee ieee transactions on image processing vol no december a b empirical empirical gaussian fit gaussian fit probability probability 06 normalized luminance mscn i j c d fig underlying gaussianity of natural images examples of a natural images and b artificial images from the tid database c shows that normalized luminance coefficients follow a nearly gaussian distribution for the natural image a d shows that this property does not hold true for the empirical distribution of the artificial image b that afflicts the image is known beforehand e g blur or to as subjective quality assessment where human observers noise or compression and so on see below previously we rate a large number of distorted and possibly reference have proposed other nss based distortion generic approaches signals when the individual opinions are averaged across the to nr iqa that statistically model images in the wavelet subjects a mean opinion score mos or differential mean domain and in the dct domain our contribution opinion score dmos is obtained for each of the visual here is a new nr iqa model that is purely spatial that relies signals in the study where the mos dmos is representative on a spatial nss model which does not require a mapping of the perceptual quality of the visual signal the goal of to a different co ordinate domain wavelet dct etc and so an objective quality assessment qa algorithm is to predict is transform free that demonstrates better ability to predict quality scores for these signals such that the scores produced human judgments of quality than other popular fr and nr by the algorithm correlate well with human opinions of iqa models that is highly efficient and that is useful for signal quality mos dmos practical application of qa perceptually optimizing image processing algorithms such as algorithms requires that these algorithms compute perceptual denoising quality efficiently while the presence of a reference image or information the regularity of natural scene statistics nss has been regarding the reference simplifies the problem of quality well established in the visual science literature where regu assessment practical applications of such algorithms are lim larity has been demonstrated in the spatial domain and ited in real world scenarios where reference information is in the wavelet domain for example it is well known generally unavailable at nodes where quality computation is that the power spectrum of natural images is a function of undertaken further it can be argued that fr and to a large frequency and takes the form f γ where γ is an exponent extent rr approaches are not quality measures in the true that varies over a small range across natural images sense since these approaches measure fidelity relative to a the product of our research is the blind referenceless reference image moreover the assumption of a pristine nature image spatial quality evaluator brisque which utilizes of any reference is questionable since all images are ostensibly an nss model framework of locally normalized luminance distorted coefficients and quantifies naturalness using the parame the performance of any iqa model is best gauged by its ters of the model brisque introduces a new model of correlation with human subjective judgements of quality the statistics of pair wise products of neighboring locally since the human is the ultimate receiver of the visual normalized luminance values the parameters of this model signal such human opinions of visual quality are generally further quantify the naturalness of the image our claim is obtained by conducting large scale human studies referred that characterizing locally normalized luminance coefficients mittal et al nr iqa in the spatial domain in this way is sufficient not only to quantify naturalness but second caters only to certain kinds of distortion processes also to quantify quality in the presence of distortion this limits the applicability of their framework to new in this article we detail the statistical model of locally distortions normalized luminance coefficients in the spatial domain as we have also developed previous nr qa models in the well as the model for pairwise products of these coefficients past following our philosophy first fully developed in we describe the statistical features that are used from the that nss models provide powerful tools for probing human model and demonstrate that these features correlate well with judgements of visual distortions our work on nss based human judgements of quality we then describe how we learn fr qa algorithms more recent rr models a mapping from features to quality space to produce an and very recent work on nss based nr qa automatic blind measure of perceptual quality we thoroughly have led us to the conclusion that visual features evaluate the performance of brisque and statistically com derived from nss lead to particularly potent and simple qa pare brisque performance to state of the art fr and nr models iqa approaches we demonstrate that brisque is highly our recently proposed nss based nr iqa model dubbed competitive to these nr iqa approaches and also statistically the distortion identification based image integrity and ver better than the popular full reference peak signal to noise ity evaluation diivine index deploys summary statistics ratio psnr and structural similarity index ssim we derived from an nss wavelet coefficient model using a two show that brisque performs well on independent databases stage framework for qa distortion identification followed by analyze its complexity and compare it with other nr iqa distortion specific qa the diivine index performs approaches finally to further illustrate the practical relevance quite well on the live iqa database achieving statistical of brisque we describe how a non blind image denoising parity with the full reference structural similarity ssim algorithm can be augmented with brisque in order to index improve blind image denoising results show that brisque a complementary approach developed at the same time augmentation leads to significant performance improvements named blind image notator using dct statistics bliinds over the state of the art before we describe brisque in ii index is a pragmatic approach to nr iqa that operates detail we first briefly review relevant prior work in the area in the dct domain where a small number of features are of blind iqa computed from an nss model of block dct coefficients efficient nss features are calculated and fed to a regression function that delivers accurate qa predictions bliinds ii is ii p revious w ork a single stage algorithm that also delivers highly competitive most existing blind iqa models proposed in the past qa prediction power although bliinds ii index is multi assume that the image whose quality is being assessed is scale the small number of feature types allow for efficient afflicted by a particular kind of distortion computation of visual quality and hence the index is attractive these approaches extract distortion specific features that relate for practical applications to loss of visual quality such as edge strength at block while both diivine and bliinds ii deliver top nr iqa boundaries however a few general purpose approaches for performance to date each of them has certain limitations nr iqa have been proposed recently the large number of features that diivine computes implies li devised a set of heuristic measures to characterize visual that it may be difficult to compute in real time although quality in terms of edge sharpness random noise and structural bliinds ii is more efficient than diivine it requires non noise while gabarda and cristobal modeled anisotropies linear sorting of block based nss features which slows it in images using renyi entropy the authors in considerably use gabor filter based local appearance descriptors to form in our continued search for fast and efficient high perfor a visual codebook and learn dmos score vector associating mance nss based nr qa indices we have recently stud each word with a quality score however in the process of ied the possibility of developing transform free models that visual codebook formation each feature vector associated with operate directly on the spatial pixel data our inspiration for an image patch is labeled by dmos asigned to the entire thinking we may succeed is the pioneering work by ruderman image this is questionable as each image patch can present a on spatial natural scene modeling and the success of the different level of quality depending on the distortion process spatial multi scale ssim index which competes well with the image is afflicted with in particular local distortions transform domain iqa models such as packet loss might afflict only a few image patches also the approach is computationally expensive limiting its iii b lind s patial i mage q uality a ssessment applicability in real time applications tang et al proposed an approach which learns an much recent work has focused on modeling the statistics ensemble of regressors trained on three different groups of of responses of natural images using multiscale transforms features natural image statistics distortion texture statistics eg gabor filters wavelets etc given that neuronal and blur noise statistics another approach is based on a responses in area of visual cortex perform scale space hybrid of curvelet wavelet and cosine transforms although orientation decompositions of visual data transform domain these approaches work on a variety of distortions each set models seem like natural approaches particularly in view of of features in the first approach and transforms in the the energy compaction sparsity and decorrelating properties ieee transactions on image processing vol no december of these transforms when combined with divisive normal decorrelated exhibits a largely homogeneous appearance with ization strategies however successful models of a few low energy residual object boundaries spatial luminance statistics have also received attention from our hypothesis is that the mscn coefficients have charac vision researchers teristic statistical properties that are changed by the presence of distortion and that quantifying these changes will make it possible to predict the type of distortion affecting an image a natural scene statistics in the spatial domain as well as its perceptual quality in order to visualize how the spatial approach to nr iqa that we have developed the mscn coefficient distributions vary as a function of can be summarized as follows given a possibly distorted distortion fig plots a histogram of mscn coefficients for image first compute locally normalized luminances via local a natural undistorted image and for various distorted versions mean subtraction and divisive normalization ruderman of it notice how the reference image exhibits a gaussian observed that applying a local non linear operation to log like appearance as observed by ruderman while each contrast luminances to remove local mean displacements from distortion modifies the statistics in its own characteristic way zero log contrast and to normalize the local variance of the log for example blur creates a more laplacian appearance while contrast has a decorrelating effect such an operation may white noise distortion appears to reduce the weight of the tail be applied to a given intensity image i i j to produce of the histogram we have found that a generalized gaussian i i j μ i j distribution ggd can be used to effectively capture a broader iˆ i j spectrum of distorted image statistics which often exhibit σ i j c changes in the tail behaviour i e kurtosis of the empirical where i m j n are spatial indices m n coefficient distributions where the ggd with zero mean are the image height and width respectively c is a is given by constant that prevents instabilities from occurring when the α denominator tends to zero eg in the case of an image patch α x f x α σ exp corresponding to the plain sky and α β k l where μ i j wk l ik l i j α β σ k k l l α k l and is the gamma function σ i j wk l ik l i j μ i j t a e t dt k k l l a a where w wk l k k k l l l is a circularly symmetric gaussian weighting function sampled out the shape parameter α controls the shape of the distribu to standard deviations and rescaled to unit volume in our tion while σ control the variance we choose the zero mean implementation k l we show how performance distribution since generally mscn coefficient distributions varies with changes in the window size in the performance are symmetric the parameters of the ggd α σ are esti evaluation section mated using the moment matching based approach proposed ruderman also observed that these normalized luminance in values strongly tend towards a unit normal gaussian charac we deploy this parametric model to fit the mscn empirical teristic for natural images such an operation can be used distributions from distorted images as well as undistorted ones to model the contrast gain masking process in early human for each image we estimate parameters α σ from a vision we utilize the pre processing model ggd fit of the mscn coefficients these form the first set in our qa model development and refer to the transformed of features that will be used to capture image distortion to luminances i i j as mean subtracted contrast normalized show that pristine and distorted images are well separated mscn coefficients as illustrated in the left column of in ggd parameter space we took a set of pristine images fig there is high correlation between surrounding pixels from the berkeley image segmentation database similar because image functions are generally piecewise smooth aside kinds of distortions as present in the live iqa database from sparse edge discontinuities hence we observe a diagonal jpeg jpeg white noise gaussian blur and fast kind of structure in the plots shown in the left column fading channel errors were introduced in each image at varying the normalization procedure greatly reduces dependencies degrees of severity to form the distorted image set as shown between neighboring coefficients as is apparent in the plots in fig a pristine and distorted images occupy different shown in the right column regions in this parameter space white noise is very clearly in order to help the reader visualize what the non linear separated from the pristine image set making it one of the transformation does to an image figure plots an image from easiest to gauge the quality of and fast fading have a the live iqa database its local mean field μ i j and high degree of overlap as fast fading images in live database local variance field σ i j and the mscn field the variance are actually multidistorted first compressed into a bitstream field highlights object boundaries and other local high contrast using a codec then passed through a rayleigh fast phenomenon the mscn field while clearly not entirely fading channel to simulate packet loss mittal et al nr iqa in the spatial domain a b fig scatter plot between neighboring values of a original luminance coefficients and b mscn coefficients rows from top to bottom illustrate horizontal vertical main diagonal and secondary diagonal neighbors notice a high correlation between surrounding pixels with a diagonal structure in the plots shown in a the normalization procedure greatly reduces these dependencies as is apparent in the plots shown in b we also model the statistical relationships between neigh i j iˆ i j iˆ i j boring pixels while mscn coefficients are definitely more homogenous for pristine images the signs of adjacent coef for i m and j n ficients also exhibit a regular structure which gets disturbed under the gaussian coefficient model and assuming the in the presence of distortion we model this structure using mscn coeffficients are zero mean and unit variance these the empirical distributions of pairwise products of neighboring products obey the following distribution in the absence of mscn coefficients along four orientations horizon distortion tal h vertical v main diagonal and secondary x ρ x diagonal as illustrated in fig specifically exp ρ ρ f x ρ π ρ h i j iˆ i j iˆ i j where f is an asymmetric probability density function v i j iˆ i j iˆ i j ρ denotes the correlation coefficient of adjacent coefficents i j iˆ i j iˆ i j and k is the modified bessel function of the second kind ieee transactions on image processing vol no december vary in the presence of distortion in fig we plot histograms of paired products along each of four orientations for a reference image and for distorted versions of it the aggd with zero mode is given by ν ν x β β exp βl x l r ν f x ν ν a ν β βr exp βr x x l ν where βl σl ν b c βr σr ν the shape parameter ν controls the shape of the distri bution while and are scale parameters that control the spread on each side of the mode respectively the aggd further generalizes the generalized gaussian distrib d e ution ggd and subsumes it by allowing for asymmetry in the distribution the skew of the distribution is a function of fig effect of the normalization procedure a original image i b local mean field μ c i μ d local variance field σ e mscn coefficients the left and right scale parameters if then the aggd i μ σ reduces to the ggd although the aggd is infrequently used it has been deployed to model skewed heavy tailed distributions of image texture the parameters of the number of coefficients normalized org aggd ν are estimated using the moment matching based approach proposed in figure b shows the d jpeg scatter plot between ν for horizontal paired products wn using the same set of images as used for showing separation blur in ggd parameter space it can be visualized that different ff distortions occupy different parts of the space also we expect images to have a better separation when modeled in the high dimensional space of parameters obtained by fitting aggd distributions to paired products from different orientations and scales together this figure also motives the use of to better capture the finite empirical density function the parameters η ν of the best aggd fit are extracted where η is given by η βr βl mscn ν thus for each paired product parameters para fig histogram of mscn coefficients for a natural undistorted image and its various distorted versions distortions from the live iqa database meters orientation orientations are computed yielding jpeg jpeg compression wn additive white gaussian the next set of features table i summarizes the features noise blur gaussian blur ff rayleigh fast fading channel simulation utilized images are naturally multiscale and distortions affect image structure across scales further as research in quality assess while we have found that this density function is a good model ment has demonstrated incorporating multiscale information of the empirical histograms of products of adjacent normalized when assessing quality produces qa algorithms that per coefficients it has only a single parameter and as such does form better in terms of correlation with human perception not provide a good fit to the empirical histograms of coefficient hence we extract all features listed in table i products fig from distorted images further it is not at two scales the original image scale and at a reduced finite at the origin hence as a practical alternative we adopt resolution low pass filtered and downsampled by a factor the very general asymmetric generalized gaussian distribution of we observed that increasing the number of scales beyond aggd model in order to visualize how paired products did not contribute to performance much thus a total of mittal et al nr iqa in the spatial domain org jpeg wn org blur ff log ν jpeg log wn blur ff log σ log log α r l a b fig a d scatter plot between shape and scale parameters obtained by fitting ggd to the empirical distributions of mscn coefficients of pristine images of berkeley image segmentation database and simulated distorted images where similar kinds of distortions as those present in the live iqa database jpeg jpeg white noise gaussian blur and fast fading channel errors were introduced in each image at varying degrees of severity b d scatter plot between shape left scale and right scale obtained by fitting aggd to horizontal paired products using the same set of images as a table i s ummary of f eatures e xtracted in o rder to c lassify and q uantify d istortions feature id feature description computation procedure f shape and variance fit ggd to mscn coefficients f shape mean left variance right variance fit aggd to h pairwise products f f shape mean left variance right variance fit aggd to v pairwise products f f shape mean left variance right variance fit aggd to pairwise products f f shape mean left variance right variance fit aggd to pairwise products b quality evaluation a mapping is learned from feature space to quality scores using a regression module yielding a measure of image qual ity the framework is generic enough to allow for the use of any regressor in our implementation a support vector machine svm regressor svr is used svr has previously been applied to image quality assessment problems for example a learning driven feature pooling approach using fig various paired products computed in order to quantify neigh svr was proposed in wavelet domain nss and singular boring statistical relationships pairwise products are computed along four value decomposition features have been used to map quality orientations horizontal vertical main diagonal and secondary diagonal at a distance of pixel to human ratings via svr in and respectively svr is generally noted for being able to handle high dimensional data we utilize the libsvm package to implement features at each scale are used to identify distortions the svr with a radial basis function rbf kernel and to perform distortion specific quality assessment in fig we plot the spearman rank ordered correlation coefficient iv p erformance e valuation srocc between each of these features and human dmos from the live iqa database for each of the distortions in a correlation with human opinions the database jpeg and compression additive we used the live iqa database to test the perfor white gaussian noise gaussian blur and a rayleigh fast fading mance of brisque which consists of reference images channel distortion to ascertain how well the features correlate with distorted images spanning five different distortion with human judgments of quality note that no training is categories and jpeg compression addi undertaken here the plot is simply to illustrate that each tive white gaussian noise wn gaussian blur blur and feature captures quality information and to show that images a rayleigh fast fading channel simulation ff each of the are affected differently by different distortions distorted images has an associated difference mean opinion ieee transactions on image processing vol no december number of coefficients normalized number of coefficients normalized org org jpeg jpeg wn wn blur blur ff ff paired product coefficients paired product coefficients a b number of coefficients normalized number of coefficients normalized org org jpeg jpeg wn wn blur blur ff ff paired product coefficients paired product coefficients c d fig histograms of paired products of mscn coefficients of a natural undistorted image and various distorted versions of it a horizontal b vertical c main diagonal d secondary diagonal distortions from the live iqa database jpeg jpeg compression wn additive white gaussian noise blur gaussian blur ff rayleigh fast fading channel simulation score dmos which represents the subjective quality of the and lcc indicate good performance in terms of correlation image with human opinion these performance indices are tabulated since the brisque approach requires a training procedure in tables ii and iii to calibrate the regressor module we divide the live database we also tabulated the performance of three full reference into two randomly chosen subsets training and indices peak signal to noise ratio psnr structural similar testing such that no overlap between train and test content ity index ssim and multi scale structural similarity occurs we do this to ensure that the reported results do index ms ssim although psnr is a poor mea not depend on features extracted from known spatial content sure of perceptual quality it is often used to benchmark which can artifically improve performance further we repeat for qa algorithms the ssim and ms ssim this random train test procedure times and report the indices are popular owing to their performance and simplicity median of the performance across these iterations in we also include the performance of the previously summa order to eliminate performance bias rized general purpose no reference algorithms cbiq the spearman rank ordered correlation coefficient lbiq bliinds ii and diivine index we srocc and pearson linear correlation coefficient lcc requested quality scores from authors for cbiq and between the predicted score from the algorithm and dmos further note that due to randomness of the trials there may be a were used to access qa performance before computing lcc slight discrepancy between results reported here and elsewhere however these the algorithm scores were passed through a logistic non differences in correlations are not statistically significant and are simply an linearity as described in a value close to for srocc artifact of the random train test sampling mittal et al nr iqa in the spatial domain table iii m edian l inear c orrelation c oefficient a cross t rain t est c ombinations on the live iqa d atabase i talics i ndicate srocc of feature with dmos n o r eference a lgorithms jpeg wn blur ff all psnr 9029 7801 8592 ssim 9824 9514 ms ssim 9793 9645 9511 cbiq 9454 9338 8955 lbiq 9345 9104 9087 bliinds ii 9426 8994 9164 jp2k diivine 9347 9370 9270 jpeg wn pointwise 8447 8670 8258 blur ff pairwise 9571 8670 brisque 9734 9506 9424 feature number table iv fig correlation of features with human judgments of quality dmos m edian s pearman r ank o rdered c orrelation c oefficient for different distortions srocc a cross t rain t est c ombinations on the table ii live iqa d atabase for d ifferent w indow s izes m edian s pearman r ank o rdered c orrelation c oefficient i talics i ndicate n o r eference a lgorithms srocc a cross t rain t est c ombinations on white gaussian fast the live iqa d atabase i talics i ndicate k l jpeg overall noise blur fading n o r eference a lgorithms 9510 9497 9360 jpeg wn blur ff all psnr 8831 7515 8636 ssim 9466 9046 9129 ms ssim 9785 9542 9535 cbiq 9418 9324 8954 lbiq 9291 8983 9063 bliinds ii 9331 8912 9124 diivine 9208 9373 9250 pointwise 8593 8759 8297 pairwise 9510 8759 9302 srocc value brisque 9647 9511 9395 lbiq implementations of other indices are available online we also reported the correlations obtained by modeling empirical distributions of mscn coefficients pointwise alone and pairwise products alone to compare their relative importance b variation with window size psnr ssim ms ssim cb lbiq bliinds ii diivine brisque as observed from the table iv the performance of fig mean srocc and standard error bars for various algorithms across brisque remains relatively stable with respect to variation in the train test trials on live iqa database the window size used to compute the local mean and variances however the performance starts to decrease when it becomes fairly large as the computations become non local although there exist differences in the median correlations between the different algorithms see table ii these differ ences may not be statistically relevant hence to evaluate c statistical significance and hypothesis testing the statistical significance of performance of each of the figure plots the mean srocc across the trials and algorithms considered we performed hypothesis testing based the standard deviations of performance across these trials on the t test on the srocc values obtained from the for each of the algorithms considered here train test trials and we tabulated the results in table v ieee transactions on image processing vol no december table v jpeg wn blur ff r esults of o ne s ided t t est p erformed b etween srocc values of various iqa a lgorithms a value of i ndicates t hat the row a lgorithm is s tatically s uperior to the c olumn a lgorithm i ndicates t hat the row is w orse t han the c olumn a value of g ives i ndicates jpeg t hat the t wo a lgorithms a re s tatistically i ndistinguishable i talics i ndicate n o r eference a lgorithms 02 wn bliinds psnr ssim msssim cbiq lbiq diivine brisque ii psnr 02 02 ssim blur msssim cbiq lbiq bliinds ii ff 00 03 diivine brisque table vi fig mean confusion matrix for classifier across trials illustrates which row distortion is confused with which column distortion higher m edian c lassification a ccuracy a cross number indicates greater confusion t rain t est t rials jpeg wn blur ff all table vii m edian s pearman r ank o rdered c orrelation c oefficient classification accuracy srocc a cross t rain t est c ombinations on the live iqa database i talics i ndicate n o r eference a lgorithms the null hypothesis is that the mean correlation for the jpeg wn blur ff all row algorithm is equal to mean correlation for the column brisque 9647 9511 9395 algorithm with a confidence of the alternate hypothesis brisque is that the mean correlation of row is greater than or lesser 9439 9479 9315 stage than the mean correlation of the column a value of in the table indicates that the row algorithm is statically superior to the column algorithm whereas a indicates that the row is statistically worse than the column a value of indicates each of the distortions where the sum of each row in the that the row and column are statistically indistinguishable confusion matrix is and actual values represent the mean or equivalent i e we could not reject the null hypothesis confusion percentage across the train test trials we at the confidence level see from fig that ff and are most confused with from table v we conclude that brisque is highly com each other which is not surprising since ff distortion is a petitive with all no reference algorithms tested and statistically combination of followed by packet loss errors better than the full reference algorithms psnr and ssim and jpeg are also confused sometimes wn and blur are given that these measures require additional information in generally not confused with other distortions the form of the reference image this is by no means a small achievement this result suggests that to the extent distortions can be trained on one can replace full reference algorithms e two stage performance such as ssim with the proposed brisque without any loss of performance we note that brisque remains slightly inferior we also investigated the possibility of replacing the one to the fr ms ssim indicating that there may still be some stage framework where features are directly mapped to room for improvement in performance quality with a two stage framework similar to that proposed in in this approach the same set of features are used to identify the distortion afflicting the image as are then used d classification accuracy for distortion specific qa such a two stage approach was in order to demonstrate that brisque features can also used with recent success for nss based blind iqa in be used for explicit distortion identification we report table vii we tabulate the median srocc value across the median classification accuracy of the classifier for each trials for the two stage realization of brisque we also list of the distortions in the live database as well as across all the performances of brisque for comparison purposes the distortions in table vi slight dip in the performance can be attributed to imperfect further in order to visualize which distortions are distortion identification in the first stage of the two stage confused the most fig plots the confusion matrix for framework mittal et al nr iqa in the spatial domain table viii table x s pearman s r ank o rdered c orrelation c oefficient srocc on c omplexity a nalysis of brisque a c omparison of the a mount the d atabase i talicized a lgorithms a re nr iqa of t ime taken to c ompute various q uality m easures for a lgorithms o thers a re fr iqa a lgorithms a i mage on a gh z s ingle c ore pc w ith gb of ram jpeg wn gblur all psnr 918 870 algorithm time seconds ssim 935 960 psnr brisque 881 diivine bliinds ii table ix brisque i nformal c omplexity a nalysis of brisque tabulated values r eflect the p ercentage of t ime d evoted to e ach of the s teps in brisque and diivine and in table x we list the time taken in step percentage of time seconds to compute each quality measure on an image of mscn resolution on a ghz single core pc with gb ggd of ram we use unoptimized matlab code for all of pairwise products and aggd these algorithms in order to ensure a fair comparison we also list the efficiency as a fraction of the time taken to compute f database independence psnr to allow for a machine independent comparison having evaluated brisque on the live iqa database across algorithms as table x demonstrates brisque is we now demonstrate that the performance of brisque is not quite efficient outperforming the diivine index and the bound by the database on which it is tested to show this we bliinds ii index by a large amount this suggests that the trained brisque on the entire live iqa database and then spatial domain brisque an ideal candidate for real time applied brisque to the database blind assessment of visual quality the tid database consists of reference images and distorted images over distortion categories since there v a pplication to b lind i mage d enoising are only natural images and our algorithm is based on the computational efficiency and excellent quality predic the statistics of natural images we test our approach only on tion performance makes brisque an attractive option for these images further although there exist distortion practical applications one such application could be using a categories we tested brisque only on these distortions quality measure to augment the performance of image repair that it is trained for jpeg compression algorithms in this section we describe one such approach additive white noise wn and gaussian blur blur ff where the brisque features are used to transform a non distortion does not exist in the tid database the results blind image denoising algorithm into a blind image denoising of applying brisque on tid are tabulated in table viii algorithm where we also list the performance of psnr and ssim blind image denoising algorithms seek to reduce the amount for comparison purposes it should be clear that brisque of noise present in corrupted images without any additional performs well in terms of correlation with human perception information such as the noise variance although image of quality and that the performance does not depend on the denoising is a well studied problem in image processing database blind image denoising remains relatively underex plored the proposed algorithms typically address g computational complexity parameter estimation in an ad hoc fashion without regard to our description of brisque focused on the relationship natural scene statistics here we demonstrate a systematic of the statistical features to natural scene statistics and the perception based parameter estimation approach that results effect that distortions have on such statistics however given in better denoising performance we augment a state of the art the small number of features that are extracted per scale image denoising algorithm by using brisque feature based and the fact that parameter estimation needs to be performed parameter prediction to improve performance only times for an entire image in comparison to parameter the work closest in concept to this approach is the one estimation for each block as in bliinds ii the reader proposed in where image content measures were used to will appreciate the fact that brisque is extremely efficient predict the noise variance in the image which was then used having demonstrated that brisque performs well in terms for image denoising however the approach is computationally of correlation with human perception we also now show that intensive and the measure of content in the image may brisque has low complexity in table ix we list the relative not be the ideal measure to predict noise variance in percentage of time each of the stages of brisque uses as a the noisy image is denoised multiple times and quality is percentage of the time taken to compute the quality of an estimated using their proposed no reference content evaluation image once trained algorithm amongst the large set of denoised images produced we also compare the overall computational complexity the image with the best content quality is selected as the of brisque with the fr psnr and the nr bliinds ii denoised image as an alternative we propose a learning based ieee transactions on image processing vol no december quality of denoised image using ms ssim a b predicted default c amount of noise variance on scale of fig accurate noise variance as input to the algorithm in produces poorer quality denoised images a noisy image σ 0158 ms ssim 9063 b denoised with σ 0158 ms ssim 9176 and fig mean quality and associated errors at each noise level across c denoised with σ 0040 ms ssim 9480 test images for our approach as well as the reference implementation of framework where noise parameters are estimated using natural scene statistics based on brisque features non blind algorithm could be improved by using brisque the denoising algorithm that we use is the one proposed natural scene features to produce a blind image denoiser in which requires as input the noise variance in the image to show the effectiveness of our algorithm and to however our experiments suggest that when the algorithm demonstrate its robustness across a large variety of images is fed the true accurate noise variance the performance of and distortion levels we created a noisy image dataset from the denoiser is sub par the performance of the algorithm the images present in the berkeley image segmentation drastically improves if a systematically different parameter database we introduced different levels of gaussian selected based on perceptual quality is fed as input to the noise to each image yielding a total of noisy images algorithm in order to demonstrate this in fig we plot an the noise variance ranged from 001 to uniformly image denoised using the true noise variance and that arrived at sampled on a logarithmic scale images were then used using the noise variance from our approach described below for training and for testing thereby ensuring no content notice that our approach produces better visual quality and overlap between the two sets the regression model described better objective quality as gauged by the multi scale structural above was trained on training images and then used to similarity index ms ssim predict the input parameter on the test images we design our training framework to account for this once denoised images are obtained we compare their discrepency and to ensure that the denoised image attains quality using ms ssim using our approach as well for the highest visual quality our approach proceeds as follows the default implementation of the algorithm and in given a large set of noisy images afflicted with different fig we plot the mean quality and the associated standard levels of noise we denoise each image using the denoising errors at each noise level across the test images for both algorithm by providing as input images these approaches it is clear that brisque augmented distorted with various values of noise variance the denoised produces much higher quality images than the baseline images so obtained are judged for their quality using ms we also analyzed whether the differences observed in the ssim and the noise parameter corresponding to the image quality of the denoised images between our approach and the with the maximum denoised quality is set as the input to the reference implementation are statistically significant algorithm these noise variances are then used in a training using the t test our analysis indicates that for all noise phase where brisque features are mapped on to the noise variances simulated in the present data our approach is sta prediction parameter using svm regression as before tistically superior to the reference implementation in once trained the automatic parameter prediction approach is terms of perceived visual quality at the confidence level capable of predicting the level of input noise to so excepting when the noise variance is a tiny 0316 where that the output denoised image has the highest visual quality the two approaches become statistically indistinguishable we note that our training approach resembles that of given a new unseen test noisy image the brisque aug vi c onclusion mented approach predicts the accurate input to we proposed a natural scene statistic based distortion and denoises the image with as we shall soon see much generic blind no reference nr quality assessment algorithm higher visual quality than the baseline notice that brisque the blind referenceless image spatial quality evaluator augmentation is not limited to the algorithm and any brisque which operates in the spatial domain the success of machine learning algorithms generally depends on data representation and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data although specific domain knowledge can be used to help design representations learning with generic priors can also be used and the quest for ai is motivating the design of more powerful representation learning algorithms implementing such priors this paper reviews recent work in the area of unsupervised feature learning and deep learning covering advances in probabilistic models autoencoders manifold learning and deep networks this motivates longer term unanswered questions about the appropriate objectives for learning good representations for computing representations i e inference and the geometrical connections between representation learning density estimation and manifold learning index terms deep learning representation learning feature learning unsupervised learning boltzmann machine autoencoder neural nets ç introduction t he performance of machine learning methods is heavily dependent on the choice of data representation or features on which they are applied for that reason much distribution of the underlying explanatory factors for the observed input a good representation is also one that is useful as input to a supervised predictor among the of the actual effort in deploying machine learning algo various ways of learning representations this paper focuses rithms goes into the design of preprocessing pipelines and on deep learning methods those that are formed by the data transformations that result in a representation of the composition of multiple nonlinear transformations with data that can support effective machine learning such the goal of yielding more abstract and ultimately more feature engineering is important but labor intensive and useful representations here we survey this rapidly highlights the weakness of current learning algorithms developing area with special emphasis on recent progress their inability to extract and organize the discriminative we consider some of the fundamental questions that have information from the data feature engineering is a way to been driving research in this area specifically what makes take advantage of human ingenuity and prior knowledge to one representation better than another given an example compensate for that weakness to expand the scope and how should we compute its representation i e perform ease of applicability of machine learning it would be highly feature extraction also what are appropriate objectives for desirable to make learning algorithms less dependent on learning good representations feature engineering so that novel applications could be constructed faster and more importantly to make progress why should we care about learning toward artificial intelligence ai an ai must fundamen representations tally understand the world around us and we argue that this can only be achieved if it can learn to identify and representation learning has become a field in itself in the disentangle the underlying explanatory factors hidden in machine learning community with regular workshops at the observed milieu of low level sensory data the leading conferences such as nips and icml and a new this paper is about representation learning i e learning conference dedicated to it iclr sometimes under the representations of the data that make it easier to extract header of deep learning or feature learning although depth useful information when building classifiers or other is an important part of the story many other priors are predictors in the case of probabilistic models a good interesting and can be conveniently captured when the representation is often one that captures the posterior problem is cast as one of learning a representation as discussed in the next section the rapid increase in scientific activity on representation learning has been accompanied the authors are with the department of computer science and operations research universite de montre al po box succ centre ville and nourished by a remarkable string of empirical successes montreal quebec canada both in academia and in industry below we briefly manuscript received apr revised oct accepted feb highlight some of these high points published online feb recommended for acceptance by s bengio l deng h larochelle h lee and speech recognition and signal processing r salakhutdinov speech was one of the early applications of neural for information on obtaining reprints of this article please send e mail to tpami computer org and reference ieeecs log number networks in particular convolutional or time delay neural tpamisi digital object identifier no tpami international conference on learning representations 00 ß ieee published by the ieee computer society bengio et al representation learning a review and new perspectives networks the recent revival of interest in neural networks deep learning and representation learning has had a strong impact in the area of speech recognition with breakthrough results 148 obtained by several academics as well as researchers at industrial labs bringing these algorithms to a larger scale and into products for example microsoft released in a new version of their microsoft audio video indexing service speech system based on deep learning these authors managed to reduce the word error rate on four major benchmarks by about percent e g from to percent on compared to state of the art models based on gaussian mixtures for the acoustic modeling and trained on the same amount of data hours of speech the relative fig illustration of representation learning discovering explanatory improvement in error rate obtained by dahl et al on factors middle hidden layer in red some explaining the input semi a smaller large vocabulary speech recognition benchmark supervised setting and some explaining target for each task because bing mobile business search dataset with hours of these subsets overlap sharing of statistical strength helps generalization speech is between and percent representation learning algorithms have also been ap approaches or surpasses the state of the art on these tasks plied to music substantially beating the state of the art in but is simpler and much faster than traditional predictors polyphonic transcription with relative error improve learning word embeddings can be combined with learning ment between and percent on a standard benchmark of image representations in a way that allows associating text four datasets deep learning also helped to win mirex and images this approach has been used successfully to music information retrieval competitions for example in build google image search exploiting huge quantities of on audio tagging data to map images and queries in the same space and it has recently been extended to deeper multimodal object recognition representations the beginnings of deep learning in focused on the the neural net language model was also improved by mnist digit image classification problem break adding recurrence to the hidden layers allowing it to ing the supremacy of svms percent error on this beat the state of the art smoothed n gram models not dataset the latest records are still held by deep networks only in terms of perplexity exponential of the average ciresan et al currently claim the title of state of the art negative log likelihood of predicting the right next word for the unconstrained version of the task e g using a going down from to but also in terms of word convolutional architecture with percent error and error rate in speech recognition since the language model rifai et al is state of the art for the knowledge free is an important component of a speech recognition system decreasing it from percent baseline version of mnist with 81 percent error or percent discriminative language model to in the last few years deep learning has moved from percent on the wall street journal benchmark task digits to object recognition in natural images and the latest similar models have been applied in statistical machine breakthrough has been achieved on the imagenet dataset translation improving perplexity and bleu bringing down the state of the art error rate from to scores recursive autoencoders which generalize recurrent percent networks have also been used to beat the state of the art natural language processing nlp in full sentence paraphrase detection almost dou bling the score for paraphrase detection representation besides speech recognition there are many other nlp learning can also be used to perform word sense applications of representation learning distributed represen disambiguation bringing up the accuracy from tations for symbolic data were introduced by hinton to percent on the subset of senseval where the and first developed in the context of statistical language system could be applied with subject verb object sen modeling by bengio et al in the so called neural net tences finally it has also been successfully used to language models they are all based on learning a surpass the state of the art in sentiment analysis distributed representation for each word called a word embedding adding a convolutional architecture collobert multitask and transfer learning domain et al developed the senna that shares adaptation representations across the tasks of language modeling transfer learning is the ability of a learning algorithm to part of speech tagging chunking named entity recognition exploit commonalities between different learning tasks to semantic role labeling and syntactic parsing senna share statistical strength and transfer knowledge across tasks as discussed below we hypothesize that representation see for a review of early work in this area learning algorithms have an advantage for such tasks for the knowledge free version of the task where no image specific because they learn representations that capture underlying prior is used such as image deformations or convolutions factors a subset of which may be relevant for each the class imagenet benchmark whose results are detailed here http www image net org challenges lsvrc results html particular task as illustrated in fig this hypothesis downloadable from http ml nec labs com senna seems confirmed by a number of empirical results showing ieee transactions on pattern analysis and machine intelligence vol no august the strengths of representation learning algorithms in discussed in the previous section multitask and transfer learning scenarios transfer learning domain adaptation most impressive are the two transfer learning challenges manifolds probability mass concentrates near re held in and won by representation learning algo gions that have a much smaller dimensionality than rithms first the transfer learning challenge presented at the original space where the data live this is an icml workshop of the same name was won using explicitly exploited in some of the autoencoder unsupervised layerwise pretraining a second algorithms and other manifold inspired algorithms transfer learning challenge was held the same year and described respectively in sections and won by goodfellow et al results were presented at natural clustering different values of categorical nips challenges in learning hierarchical models variables such as object classes are associated with workshop in the related domain adaptation setup the target separate manifolds more precisely the local varia remains the same but the input distribution changes tions on the manifold tend to preserve the value of a in the multitask learning setup representation learning category and a linear interpolation between exam has also been found to be advantageous because ples of different classes in general involves going of shared factors across tasks through a low density region i e p ðx j y iþ for different i tend to be well separated and not overlap what makes a representation good much for example this is exploited in the manifold priors for representation learning in ai tangent classifier mtc discussed in section this hypothesis is consistent with the idea that in one of us introduced the notion of ai tasks which humans have named categories and classes because are challenging for current machine learning algorithms of such statistical structure discovered by their and involve complex but highly structured dependencies brain and propagated by their culture and machine one reason why explicitly dealing with representations is learning tasks often involve predicting such catego interesting is because they can be convenient to express rical variables many general priors about the world around us i e priors temporal and spatial coherence consecutive from a that are not task specific but would be likely to be useful for sequence or spatially nearby observations tend to be a learning machine to solve ai tasks examples of such associated with the same value of relevant catego general purpose priors are the following rical concepts or result in a small move on the smoothness assumes that the function to be learned surface of the high density manifold more gener f is t x y generally implies fðxþ fðyþ this ally different factors change at different temporal most basic prior is present in most machine learning and spatial scales and many categorical concepts of but is insufficient to get around the curse of interest change slowly when attempting to capture dimensionality see section such categorical variables this prior can be enforced multiple explanatory factors the data generating by making the associated representations slowly distribution is generated by different underlying changing i e penalizing changes in values over factors and for the most part what one learns about time or space this prior was introduced in and is one factor generalizes in many configurations of the discussed in section other factors the objective to recover or at least sparsity for any given observation x only a small disentangle these underlying factors of variation is fraction of the possible factors are relevant in terms discussed in section this assumption is behind of representation this could be represented by the idea of distributed representations discussed in features that are often zero as initially proposed section by olshausen and field or by the fact that a hierarchical organization of explanatory factors the most of the extracted features are insensitive to small concepts that are useful for describing the world variations of x this can be achieved with certain around us can be defined in terms of other concepts forms of priors on latent variables peaked at or in a hierarchy with more abstract concepts higher in by using a nonlinearity whose value is often flat at the hierarchy defined in terms of less abstract ones i e and with a derivative or simply by this assumption is exploited with deep representa penalizing the magnitude of the jacobian matrix of tions elaborated in section derivatives of the function mapping input to semi supervised learning with inputs x and target y representation this is discussed in sections to predict a subset of the factors explaining x and distribution explain much of y given x hence simplicity of factor dependencies in good high level representations that are useful for p ðxþ tend to be representations the factors are related to each other useful when learning p ðy j xþ allowing sharing of through simple typically linear dependencies this statistical strength between the unsupervised and can be seen in many laws of physics and is assumed supervised learning tasks see section when plugging a linear predictor on top of a learned shared factors across tasks with many y of interest or representation many learning tasks in general tasks e g the we can view many of the above priors as ways to help the corresponding p ðy j x taskþ are explained by learner discover and disentangle some of the underlying and factors that are shared with other tasks allowing a priori unknown factors of variation that the data may sharing of statistical strengths across tasks as reveal this idea is pursued further in sections and bengio et al representation learning a review and new perspectives smoothness and the curse of dimensionality sparse representation and k n in nonsparse rbms and for ai tasks such as vision and nlp it seems hopeless to other dense representations these are all or rely only on simple parametric models such as linear representations the generalization of clustering to models because they cannot capture enough of the distributed representations is multiclustering where either complexity of interest unless provided with the appropriate several clusterings take place in parallel or the same feature space conversely machine learning researchers clustering is applied on different parts of the input such have sought flexibility in learners such as as in the very popular hierarchical feature extraction for kernel machines with a fixed generic local response kernel object recognition based on a histogram of cluster categories such as the gaussian kernel unfortunately as argued at detected in different patches of an image the length by bengio and monperrus bengio et al exponential gain from distributed or sparse representations bengio and lecun bengio and bengio et al is discussed further in section fig it comes most of these algorithms only exploit the principle of local about because each parameter e g the parameters of one of generalization i e the assumption that the target function to the units in a sparse code or one of the units in an rbm can be learned is smooth enough so they rely on examples to be reused in many examples that are not simply near explicitly map out the wrinkles of the target function general neighbors of each other whereas with local generalization ization is mostly achieved by a form of local interpolation different regions in input space are basically associated with between neighboring training examples although smooth their own private set of parameters for example as in ness can be a useful assumption it is insufficient to deal decision trees nearest neighbors gaussian svms and so with the curse of dimensionality because the number of such on in a distributed representation an exponentially large wrinkles ups and downs of the target function may grow number of possible subsets of features or hidden units can be exponentially with the number of relevant interacting activated in response to a given input in a single layer factors when the data are represented in raw input space model each feature is typically associated with a preferred we advocate learning algorithms that are flexible and input direction corresponding to a hyperplane in input nonparametric but do not rely exclusively on the smooth space and the code or representation associated with that ness assumption instead we propose to incorporate generic input is precisely the pattern of activation which features priors such as those enumerated above into representation respond to the input and how much this is in contrast learning algorithms smoothness based learners such as with a nondistributed representation such as the one kernel machines and linear models can still be useful on top learned by most clustering algorithms for example of such learned representations in fact the combination of k means in which the representation of a given input learning a representation and kernel machine is equivalent vector is a one hot code identifying which one of a small to learning the kernel i e the feature space kernel machines number of cluster centroids best represents the input are useful but they depend on a prior definition of a suitable depth and abstraction similarity metric or a feature space in which naive similarity metrics suffice we would like to use the data along with depth is a key aspect to the representation learning strategies very generic priors to discover those features or equiva we consider in this paper as we will discuss deep lently a similarity function architectures are often challenging to train effectively and this has been the subject of much recent research and distributed representations progress however despite these challenges they carry two good representations are expressive meaning that a reason significant advantages that motivate our long term interest in ably sized learned representation can capture a huge discovering successful training strategies for deep architec number of possible input configurations a simple counting tures these advantages are deep architectures promote argument helps us to assess the expressiveness of a model the reuse of features and deep architectures can potentially producing a representation how many parameters does it lead to progressively more abstract features at higher layers of require compared to the number of input regions or representations more removed from the data configurations it can distinguish learners of one hot feature reuse the notion of reuse which explains the representations such as traditional clustering algorithms power of distributed representations is also at the heart of gaussian mixtures nearest neighbor algorithms decision the theoretical advantages behind deep learning i e trees or gaussian svms all require oðnþ parameters and distributed representations where k out of n representation or oðnþ examples to distinguish oðnþ input regions one elements or feature values can be independently varied for example could naively believe that one cannot do better however they are not mutually exclusive each concept is represented by having k features being turned on or active while each feature is involved in restricted boltzmann machines rbms sparse coding representing many concepts autoencoders or multilayer neural networks can all sparse representations distributed representations where only a few represent up to þ input regions using only oðnþ of the elements can be varied at a time i e k n as discussed in things are only slightly better when allowing parameters with k the number of nonzero elements in a continuous valued membership values for example in ordinary mixture models with separate parameters for each mixture component but the local in the sense that the value of the learned function at x depends difference in representational power is still exponential the situation mostly on training examples xðtþ close to x may also seem better with a decision tree where each given input is we understand nonparametric as including all learning algorithms associated with a one hot code over the tree leaves which deterministically whose capacity can be increased appropriately as the amount of data selects associated ancestors the path from root to node unfortunately the and its complexity demands it for example including mixture models number of different regions represented equal to the number of leaves of and neural networks where the number of parameters is a data the tree still only grows linearly with the number of parameters used to selected hyperparameter specify it ieee transactions on pattern analysis and machine intelligence vol no august constructing multiple levels of representation or learning a examples to learn representations that separate the various hierarchy of features the depth of a circuit is the length of explanatory sources doing so should give rise to a the longest path from an input node of the circuit to an representation significantly more robust to the complex output node of the circuit the crucial property of a deep and richly structured variations extant in natural data circuit is that its number of paths i e ways to reuse different sources for ai related tasks parts can grow exponentially with its depth formally one it is important to distinguish between the related but can change the depth of a given circuit by changing the distinct goals of learning invariant features and learning to definition of what each node can compute but only by a disentangle explanatory factors the central difference is constant factor the typical computations we allow in each the preservation of information invariant features by node include weighted sum product artificial neuron definition have reduced sensitivity in the direction of model such as a monotone nonlinearity on top of an invariance this is the goal of building features that are affine transformation computation of a kernel or logic insensitive to variation in the data that are uninformative gates theoretical results clearly show families of functions to the task at hand unfortunately it is often difficult to where a deep representation can be exponentially more determine a priori which set of features and variations will efficient than one that is insufficiently deep 83 ultimately be relevant to the task at hand further as is if the same family of functions can be often the case in the context of deep learning methods the represented with fewer parameters or more precisely feature set being trained may be destined to be used in with a smaller vc dimension learning theory would multiple tasks that may have distinct subsets of relevant suggest that it can be learned with fewer examples features considerations such as these lead us to the yielding improvements in both computational efficiency conclusion that the most robust approach to feature less nodes to visit and statistical efficiency fewer learning is to disentangle as many factors as possible discarding parameters to learn and reuse of these parameters over as little information about the data as is practical if some form many different kinds of inputs of dimensionality reduction is desirable then we hypothe abstraction and invariance deep architectures can lead to size that the local directions of variation least represented abstract representations because more abstract concepts can in the training data should be first to be pruned out as in often be constructed in terms of less abstract ones in some principal components analysis pca for example which cases such as in the convolutional neural network we does it globally instead of around each example explicitly build this abstraction in via a pooling mechanism see section more abstract concepts are generally good criteria for learning representations invariant to most local changes of the input that makes the one of the challenges of representation learning that representations that capture these concepts generally highly distinguishes it from other machine learning tasks such nonlinear functions of the raw input this is obviously true as classification is the difficulty in establishing a clear of categorical concepts where more abstract representations objective or target for training in the case of classification detect categories that cover more varied phenomena e g the objective is at least conceptually obvious we want to larger manifolds with more wrinkles and thus they minimize the number of misclassifications on the training potentially have greater predictive power abstraction can dataset in the case of representation learning our objective also appear in high level continuous valued attributes that is far removed from the ultimate objective which typically are only sensitive to some very specific types of changes in is learning a classifier or some other predictor our the input learning these sorts of invariant features has problem is reminiscent of the credit assignment problem been a long standing goal in pattern recognition encountered in reinforcement learning we have proposed that a good representation is one that disentangles the disentangling factors of variation underlying factors of variation but how do we translate beyond being distributed and invariant we would like our that into appropriate training criteria is it even necessary representations to disentangle the factors of variation different to do anything but maximize likelihood under a good explanatory factors of the data tend to change indepen model or can we introduce priors such as those enumer dently of each other in the input distribution and only a ated above possibly data dependent ones that help the few at a time tend to change when one considers a sequence representation better do this disentangling this question of consecutive real world inputs remains clearly open but is discussed in more detail in complex data arise from the rich interaction of many sections and sources these factors interact in a complex web that can complicate ai related tasks such as object classification for example an image is composed of the interaction between building deep representations one or more light sources the object shapes and the in a breakthrough in feature learning and deep material properties of the various surfaces present in the learning was initiated by geoff hinton and quickly image shadows from objects in the scene can fall on each followed up in the same year and soon other in complex patterns creating the illusion of object after by lee et al and many more later it has been boundaries where there are none and can dramatically extensively reviewed and discussed in a central idea effect the perceived object shape how can we cope with referred to as greedy layerwise unsupervised pretraining was to these complex interactions how can we disentangle the learn a hierarchy of features one level at a time using objects and their shadows ultimately we believe the unsupervised feature learning to learn a new transforma approach we adopt for overcoming these challenges must tion at each level to be composed with the previously leverage the data itself using vast quantities of unlabeled learned transformations essentially each iteration of bengio et al representation learning a review and new perspectives unsupervised feature learning adds one layer of weights to another early approach was to stack rbms or autoenco a deep neural network finally the set of layers could be ders into a deep autoencoder 92 if we have a series of encoder combined to initialize a deep supervised predictor such as decoder pairs ðf ðiþ ð þ gðiþ ð þþ then the overall encoder is the a neural network classifier or a deep generative model composition of the encoders f ðnþ ð f ðf ð þþþ and the such as a deep boltzmann machine dbm overall decoder is its transpose often with transposed this paper is mostly about feature learning algorithms weight matrices as well ð f ðnþ ð þþþ the deep that can be used to form deep architectures in particular it autoencoder or its regularized version as discussed in was empirically observed that layerwise stacking of feature section can then be jointly trained with all the extraction often yielded better representations for example parameters optimized with respect to a global reconstruction in terms of classification error quality of the error criterion more work on this avenue clearly needs to be samples generated by a probabilistic model or in done and it was probably avoided by fear of the challenges in terms of the invariance properties of the learned features training deep feedforward networks discussed in section whereas this section focuses on the idea of stacking along with very encouraging recent results single layer models section follows up with a discussion yet another recently proposed approach to training deep on joint training of all the layers architectures is to consider the iterative construction of after greedy layerwise unsupervised pretraining the a free energy function i e with no explicit latent variables resulting deep features can be used either as input to a except possibly for a top level layer of hidden units for a standard supervised machine learning predictor such as an deep architecture as the composition of transformations svm or as initialization for a deep supervised neural associated with lower layers followed by top level hidden network e g by appending a logistic regression layer or units the question is then how to train a model defined by purely supervised layers of a multilayer neural network an arbitrary parameterized free energy function ngiam the layerwise procedure can also be applied in a purely et al have used hybrid monte carlo but other supervised setting called the greedy layerwise supervised options include contrastive divergence cd score pretraining for example after the first one hidden matching denoising score matching layer multilayer perceptrons mlp is trained its output ratio matching and noise contrastive estimation layer is discarded and another one hidden layer mlp can be stacked on top of it and so on although results reported single layer learning modules in were not as good as for unsupervised pretraining within the community of researchers interested in repre they were nonetheless better than without pretraining at all sentation learning there has developed two broad parallel alternatively the outputs of the previous layer can be fed as lines of inquiry one rooted in probabilistic graphical extra inputs for the next layer in addition to the raw input models and one rooted in neural networks fundamentally as successfully done in another variant the difference between these two paradigms is whether the pretrains in a supervised way all the previously added layered architecture of a deep learning model is to be layers at each step of the iteration and in their experiments interpreted as describing a probabilistic graphical model or this discriminant variant yielded better results than un as describing a computation graph in short are hidden supervised pretraining units considered latent random variables or as computa whereas combining single layers into a supervised tional nodes model is straightforward it is less clear how layers to date the dichotomy between these two paradigms pretrained by unsupervised learning should be combined has remained in the background perhaps because they to form a better unsupervised model we cover here some of appear to have more characteristics in common than the approaches to do so but no clear winner emerges and separating them we suggest that this is likely a function much work has to be done to validate existing proposals or of the fact that much recent progress in both of these areas improve them has focused on single layer greedy learning modules and the the first proposal was to stack pretrained rbms into a similarities between the types of single layer models that deep belief network or dbn where the top layer is have been explored mainly the rbm on the probabilistic interpreted as an rbm and the lower layers as a directed side and the autoencoder variants on the neural network sigmoid belief network however it is not clear how to side indeed as shown by one of us and others approximate maximum likelihood training to further in the case of the rbm training the model via an inductive optimize this generative model one option is the wake principle known as score matching to be discussed in sleep algorithm but more work should be done to section is essentially identical to applying a regular assess the efficiency of this procedure in terms of improving ized reconstruction objective to an autoencoder another the generative model strong link between pairs of models on both sides of this the second approach that has been put forward is to divide is when the computational graph for computing combine the rbm parameters into a dbm by basically representation in the neural network model corresponds halving the rbm weights to obtain the dbm weights exactly to the computational graph that corresponds to the dbm can then be trained by approximate maximum inference in the probabilistic model and this happens to likelihood as discussed in more detail later section also correspond to the structure of graphical model itself this joint training has brought substantial improvements e g as in the rbm both in terms of likelihood and in terms of classification the connection between these two paradigms becomes performance of the resulting deep feature learner more tenuous when we consider deeper models where in ieee transactions on pattern analysis and machine intelligence vol no august the case of a probabilistic model exact inference typically constraints is independent component analysis or ica becomes intractable in the case of deep models the instead we refer the reader to note computational graph diverges from the structure of the that while in the simplest case complete noise free ica model for example in the case of a dbm unrolling yields linear features in the more general case it can be variational approximate inference into a computational equated with a linear generative model with non gaussian graph results in a recurrent graph structure we have independent latent variables similar to sparse coding performed preliminary exploration of deterministic section which result in nonlinear features therefore variants of deep autoencoders whose computational graph ica and its variants like independent and topographic ica is similar to that of a dbm in fact very close to the mean can and have been used to build deep networks field variational approximations associated with the see section the notion of obtaining indepen boltzmann machine and that is one interesting inter dent components also appears similar to our stated goal of mediate point to explore between the deterministic disentangling underlying explanatory factors through deep approaches and the graphical model approaches networks however for complex real world distributions it in the next few sections we will review the major is doubtful that the relationship between truly independent developments in single layer training modules used to underlying factors and the observed high dimensional data support feature learning and particularly deep learning we can be adequately characterized by a linear transformation divide these sections between section the probabilistic models with inference and training schemes that directly parameterize the generative or decoding pathway and probabilistic models section the typically neural network based models that from the probabilistic modeling perspective the question directly parametrize the encoding pathway interestingly of feature learning can be interpreted as an attempt to some models like predictive sparse decomposition psd recover a parsimonious set of latent random variables that inherit both properties and will also be discussed describe a distribution over the observed data we can section we then present a different view of express as pðx hþ a probabilistic model over the joint space representation learning based on the associated geometry of the latent variables h and observed data or visible and the manifold assumption in section variables x feature values are conceived as the result of an first let us consider an unsupervised single layer inference process to determine the probability distribution representation learning algorithm spanning all three views of the latent variables given the data i e pðh j xþ often probabilistic autoencoder and manifold learning referred to as the posterior probability learning is conceived in terms of estimating a set of model parameters that pca locally maximize the regularized likelihood of the training we will use probably the oldest feature extraction algo data the probabilistic graphical model formalism gives us rithm pca to illustrate the probabilistic autoencoder and two possible modeling paradigms in which we can consider manifold views of representation learning pca learns a the question of inferring latent variables directed and linear transformation h fðxþ w t x þ b of input x irdx undirected graphical models which differ in their para where the columns of dx dh matrix w form an orthogonal meterization of the joint distribution pðx hþ yielding major basis for the dh orthogonal directions of greatest variance in impact on the nature and computational costs of both the training data the result is dh features the components inference and learning of representation h that are decorrelated the three interpretations of pca are the following it is related to directed graphical models probabilistic models section such as probabilistic pca directed latent factor models separately parameterize the factor analysis and the traditional multivariate gaussian conditional likelihood pðx j hþ and the prior pðhþ to construct distribution the leading eigenvectors of the covariance the joint distribution pðx hþ pðx j hþpðhþ examples of matrix are the principal components the representation this decomposition include pca sparse coding it learns is essentially the same as that learned by a basic sigmoid belief networks and the newly linear autoencoder section and it can be viewed as a introduced spike and slab sparse coding model simple linear form of linear manifold learning section i e characterizing a lower dimensional region in input space explaining away near which the data density is peaked thus pca may be in directed models often lead to one important property the back of the reader mind as a common thread relating explaining away i e a priori independent causes of an event these various viewpoints unfortunately the expressive can become nonindependent given the observation of the power of linear features is very limited they cannot be event latent factor models can generally be interpreted as stacked to form deeper more abstract representations since latent cause models where the h activations cause the the composition of linear operations yields another linear observed x this renders the a priori independent h to be operation here we focus on recent algorithms that have nonindependent as a consequence recovering the posterior been developed to extract nonlinear features which can be distribution of h pðh j xþ which we use as a basis for feature stacked in the construction of deep networks although representation is often computationally challenging and some authors simply insert a nonlinearity between learned can be entirely intractable especially when h is discrete single layer linear projections a classic example that illustrates the phenomenon is to another rich family of feature extraction techniques that imagine you are on vacation away from home and you this review does not cover in any detail due to space receive a phone call from the security system company bengio et al representation learning a review and new perspectives ðtþ telling you that the alarm has been activated you begin exchange scaling of column i with scaling of hi such a worrying your home has been burglarized but then you constraint is necessary for the penalty to have any effect hear on the radio that a minor earthquake has been reported the probabilistic interpretation of sparse coding differs in the area of your home if you happen to know from prior from that of pca in that instead of a gaussian prior on the experience that earthquakes sometimes cause your home latent random variable h we use a sparsity inducing alarm system to activate then suddenly you relax con laplace prior corresponding to an penalty fident that your home has very likely not been burglarized y dh the example illustrates how the alarm activation rendered pðhþ expð jhi jþ two otherwise entirely independent causes burglarized and i earthquake to become dependent in this case the depen pðx j hþ n x w h þ x i dency is one of mutual exclusivity since both burglarized and earthquake are very rare events and both can cause alarm in the case of sparse coding because we will seek a sparse activation the observation of one explains away the other representation i e one with many features set to exactly despite the computational obstacles we face when attempt zero we will be interested in recovering the maximum ing to recover the posterior over h explaining away a posteriori value map value of h i e h argmaxh pðh j promises to provide a parsimonious pðh j xþ which can be xþ rather than its expected value j x under this an extremely useful characteristic of a feature encoding interpretation dictionary learning proceeds as maximizing scheme if one thinks of a representation as being composed the likelihood q of the data given these map values of h of various feature detectors and estimated attributes of the argmaxw t pðxðtþ j h ðtþ þ subject to the norm constraint on observed input it is useful to allow the different features to w note that this parameter learning scheme subject to the compete and collaborate with each other to explain the map values of the latent h is not standard practice in input this is naturally achieved with directed graphical p literature typically the the probabilistic graphical model models but can also be achieved with undirected models likelihood of the data pðxþ h pðx j hþpðhþ is maximized see section such as boltzmann machines if there are directly in the presence of latent variables expectation lateral connections between the corresponding units or maximization is employed where the parameters are optimized with respect to the marginal likelihood i e corresponding interaction terms in the energy function that summing or integrating the joint log likelihood over all defines the probability model values of the latent variables under their posterior p ðh j xþ probabilistic interpretation of pca pca can be given a rather than considering only the single map value of h natural probabilistic interpretation as factor the theoretical properties of this form of parameter analysis learning are not yet well understood but seem to work well in practice e g k means versus gaussian mixture pðhþ n h 2h i models and viterbi training for hmms note also that the pðx j hþ n x w h þ x i interpretation of sparse coding as a map estimation can where x irdx h irdh n ðv þ is the multivariate be questioned because even though the interpretation normal density of v with mean and covariance and of the penalty as a log prior is a possible interpretation columns of w span the same space as leading dh principal there can be other bayesian interpretations compatible with components but are not constrained to be orthonormal the training criterion sparse coding like pca sparse coding has both a sparse coding is an excellent example of the power of probabilistic and nonprobabilistic interpretation sparse explaining away even with a very overcomplete diction coding also relates a latent representation h either a vector ary the map inference process used in sparse coding to of random variables or a feature vector depending on the find h can pick out the most appropriate bases and zero interpretation to the data x through a linear mapping w the others despite them having a high degree of correla tion with the input this property arises naturally in which we refer to as the dictionary the difference between directed graphical models such as sparse coding and is sparse coding and pca is that sparse coding includes a entirely due to the explaining away effect it is not seen in penalty to ensure a sparse activation of h is used to encode commonly used undirected probabilistic models such as each input x from a nonprobabilistic perspective sparse the rbm nor is it seen in parametric feature encoding coding can be seen as recovering the code or feature vector methods such as autoencoders the tradeoff is that associated with a new input x via compared to methods such as rbms and autoencoders inference in sparse coding involves an extra inner loop of h fðxþ argmin kx w h þ h optimization to find h with a corresponding increase in the computational cost of feature extraction compared to learning the dictionary w can be accomplished by optimiz autoencoders and rbms the code in sparse coding is a free ing the following training criterion with respect to w variable for each example and in that sense the implicit x encoder is nonparametric j sc kxðtþ w h ðtþ t one might expect that the parsimony of the sparse coding representation and its explaining away effect would ðtþ where x is the tth example and h ðtþ is the corresponding be advantageous and indeed it seems to be the case coates sparse code determined by w is usually constrained to have unit norm columns because one can arbitrarily overcomplete with more dimensions of h than dimensions of x ieee transactions on pattern analysis and machine intelligence vol no august and ng demonstrated on the cifar object or units these stochastic units can be divided into two classification task with a patch base feature extraction groups the visible units x that represent the pipeline that in the regime with few labeled data and the hidden or latent units h that training examples per class the sparse coding representa mediate dependencies between the visible units through tion significantly outperformed other highly competitive their mutual interactions the pattern of interaction is encoding schemes possibly because of these properties and specified through the energy function because of the very computationally efficient algorithms t t that have been proposed for it in comparison with the e bm t t t ðx hþ x ux h v h x w h b x d h general case of inference in the presence of explaining away sparse coding enjoys considerable popularity as a where fu v w b dg are the model parameters that feature learning and encoding paradigm there are numer respectively encode the visible to visible interactions the ous examples of its successful application as a feature hidden to hidden interactions the visible to hidden inter representation scheme including natural image modeling actions the visible self connections and the hidden self audio classification nlp as connections called biases to avoid overparameterization well as being a very successful model of the early visual the diagonals of u and v are set to zero cortex sparsity criteria can also be generalized the boltzmann machine energy function specifies the successfully to yield groups of features that prefer to all probability distribution over h via the boltzmann be zero but if one or a few of them are active then the distribution with the partition function z given by penalty for activating others in the group is small different hdh xx dx h xx x x group sparsity patterns can incorporate different forms of z exp e bm ðx h þ prior knowledge xdx hdh is one example of a promising variation on sparse coding for feature learning the model this joint probability distribution gives rise to the set of possesses a set of latent binary spike variables together with conditional distributions of the form a set of latent real valued slab variables the activation of x the spike variables dictates the sparsity pattern has x p ðhi j x hni þ sigmoid wji xj þ þ di been applied to the cifar and cifar object j classification tasks and shows the same pattern as sparse coding of superior performance in the regime of x relatively few labeled examples per class in x p ðxj j h xnj þ sigmoid wji xj þ þ bj fact in both the cifar dataset with examples per i class and the cifar dataset when the number of examples is reduced to a similar range the representa in general inference in the boltzmann machine is intract tion actually outperforms sparse coding representations able for example computing the conditional probability of this advantage was revealed clearly with winning the hi given the visibles p ðhi j xþ requires marginalizing over nips transfer learning challenge the rest of the hiddens which implies evaluating a sum undirected graphical models with terms undirected graphical models also called markov random hx hx i hx hdh x fields mrfs parameterize the joint pðx hþ through a p ðhi j xþ p ðh j xþ product of unnormalized nonnegative clique potentials hi hdh ¼0 y y y however with some judicious choices in the pattern of pðx hþ i ðxþ j ðhþ k ðx hþ z i j interactions between the visible and hidden units more k tractable subsets of the model family are possible as we where i ðxþ j ðhþ and k ðx hþ are the clique potentials discuss next describing the interactions between the visible elements rbms the rbm is likely the most popular subclass of between the hidden variables and those interaction boltzmann machine it is defined by restricting the between the visible and hidden variables respectively interactions in the boltzmann energy function in to only the partition function z ensures that the distribution is those between h and x i e e rbm is e bm with u and normalized within the context of unsupervised feature v as such the rbm can be said to form a bipartite learning we generally see a particular form of mrf called a graph with the visibles and the hiddens forming two layers boltzmann distribution with clique potentials constrained of vertices in the graph and no connection between units of to be positive the same layer with this restriction the rbm possesses the useful property that the conditional distribution over the pðx hþ expð e ðx hþþ hidden units factorizes given the visibles z y where e ðx hþ is the energy function and contains the p ðh j xþ p ðhi j xþ interactions described by the mrf clique potentials and are i x the model parameters that characterize these interactions p ðhi j xþ sigmoid wji xj þ di the boltzmann machine was originally defined as a j network of symmetrically coupled binary random variables bengio et al representation learning a review and new perspectives likewise the conditional distribution over the visible units absolute values this point is supported by the common use given the hiddens also factorizes of preprocessing methods that standardize the global y scaling of the pixel values across images in a dataset or p ðx j hþ p ðxj j hþ across the pixel values within each image j x these kinds of concerns about the ability of the grbm to model natural image data has led to the development of p ðxj j hþ sigmoid wji hi þ bj i alternative rbm based models that each attempt to take on this objective of better modeling nondiagonal conditional this makes inferences readily tractable in rbms for covariances ranzato and hinton introduced the mean example the rbm feature representation is taken to be and covariance rbm mcrbm like the grbm the mcrbm the set of posterior marginals p ðhi j xþ which given the is a two layer boltzmann machine that explicitly models conditional independence described in are immedi the visible units as gaussian distributed quantities how ately available note that this is in stark contrast to the ever unlike the grbm the mcrbm uses its hidden layer to situation with popular directed graphical models for independently parametrize both the mean and covariance unsupervised feature extraction where computing the of the data through two sets of hidden units the mcrbm posterior probability is intractable is a combination of the covariance rbm crbm importantly the tractability of the rbm does not extend which models the conditional covariance and the grbm to its partition function which still involves summing an which captures the conditional mean while the grbm has exponential number of terms it does imply however that shown considerable potential as the basis of a highly we can limit the number of terms to g usually successful phoneme recognition system it seems that this is still an unmanageable number of terms and therefore due to difficulties in training the mcrbm the model has we must resort to approximate methods to deal with its been largely superseded by the mpot model the mpot estimation model mean product of student t distributions model it is difficult to overstate the impact the rbm has had to is a combination of the grbm and the product of student the fields of unsupervised feature learning and deep t distributions model it is an energy based model learning it has been used in a truly impressive variety of where the conditional distribution over the visible units applications including fmri image classification conditioned on the hidden variables is a multivariate motion and spatial transformations collabora gaussian nondiagonal covariance and the complementary tive filtering and natural image modeling conditional distribution over the hidden variables given the visibles are a set of independent gamma distributions generalizations of the rbm to real valued data the pot model has recently been generalized to the important progress has been made in the last few years in mpot model to include nonzero gaussian means by defining generalizations of the rbm that better capture real the addition of grbm like hidden units similarly to how valued data in particular real valued image data by better the mcrbm generalizes the crbm the mpot model has modeling the conditional covariance of the input pixels the been used to synthesize large scale natural images that standard rbm as discussed above is defined with both show large scale features and shadowing structure it has binary visible variables v and binary latent vari been used to model natural textures in a tiled ables h the tractability of inference and learning in convolution configuration see section the rbm has inspired many authors to extend it via another recently introduced rbm based model with the modifications of its energy function to model other kinds of objective of having the hidden units encode both the mean data distributions in particular there have been multiple and covariance information is the spike and slab restricted attempts to develop rbm type models of real valued data boltzmann machine ssrbm the ssrbm is where x irdx the most straightforward approach to defined as having both a real valued slab variable and a modeling real valued observations within the rbm frame binary spike variable associated with each unit in the work is the so called gaussian rbm grbm where the only hidden layer the ssrbm has been demonstrated as a change in the rbm energy function is to the visible units feature learning and extraction scheme in the context of biases by adding a bias term that is quadratic in the visible cifar object classification from natural images units x while it probably remains the most popular way to and has performed well in the role when trained model real valued data within the rbm framework convolutionally see section on full cifar natural ranzato and hinton suggest that the grbm has images the model demonstrated the ability to generate proven to be a somewhat unsatisfactory model of natural natural image samples that seem to capture the broad images the trained features typically do not represent statistical structure of natural images better than previous sharp edges that occur at object boundaries and lead to parametric generative models as illustrated with the latent representations that are not particularly useful samples of fig features for classification tasks ranzato and hinton the mcrbm mpot and ssrbm each set out to model argue that the failure of the grbm to adequately capture real valued data such that the hidden units encode not only the statistical structure of natural images stems from the the conditional mean of the data but also its conditional exclusive use of the model capacity to capture the covariance other than differences in the training schemes conditional mean at the expense of the conditional the most significant difference between these models is how covariance natural images they argue are chiefly char they encode their conditional covariance while the mcrbm acterized by the covariance of the pixel values not by their and the mpot use the activation of the hidden units to ieee transactions on pattern analysis and machine intelligence vol no august x t xt x log p xðtþ log p xðtþ h gradient based optimization requires its gradient which for boltzmann machines is given by x t xt bm ðtþ log pðxðtþ þ iepðhjxðtþ þ e ðx hþ i i xt bm þ iepðx hþ e ðx hþ i where we have the expectations with respect to pðhðtþ j xðtþ þ under the clamped condition also called the positive phase and over the full joint pðx hþ under the unclamped condition also called the negative phase intuitively the gradient acts to locally move the model distribution the negative phase distribution toward the data distribution positive phase distribution by pushing down the energy of ðh xðtþ þ pairs for h p ðh j xðtþ þ while pushing up the energy of ðh xþ pairs for ðh xþ p ðh xþ until the two forces are in equilibrium at which point the sufficient fig top samples from convolutionally trained ssrbm from statistics gradient of the energy function have equal bottom images in the cifar training set closest expectations with x sampled from the training distribution distance with contrast normalized training images to correspond ing model samples on top the model does not appear to be or with x sampled from the model overfitting particular training examples the rbm conditional independence properties imply that the expectation in the positive phase of is enforce constraints on the covariance of x the ssrbm uses tractable the negative phase term arising from the the hidden unit to pinch the precision matrix along the partition function contribution to the log likelihood direction specified by the corresponding weight vector gradient is more problematic because the computation these two ways of modeling conditional covariance of the expectation over the joint is not tractable the diverge when the dimensionality of the hidden layer is various ways of dealing with the partition function significantly different from that of the input in the contribution to the gradient have brought about a number overcomplete setting sparse activation with the ssrbm of different training algorithms many trying to approx parametrization permits variance only in the select direc imate the log likelihood gradient tions of the sparsely activated hidden units this is a to approximate the expectation of the joint distribution property the ssrbm shares with sparse coding models in the negative phase contribution to the gradient it is on the other hand in the case of the mpot or natural to again consider exploiting the conditional in mcrbm an overcomplete set of constraints on the covar dependence of the rbm to specify a monte carlo approx iance implies that capturing arbitrary covariance along a imation of the expectation over the joint particular direction of the input requires decreasing potentially all constraints with positive projection in that rbm l rbm ðlþ ðlþ direction this perspective would suggest that the mpot iepðx hþ e ðx hþ e x h i l i and mcrbm do not appear to be well suited to provide a sparse representation in the overcomplete setting xðlþ h ðlþ þ drawn by a block gibbs markov with the samples ð chain monte carlo mcmc sampling procedure rbm parameter estimation many of the rbm training methods we discuss here are x ðlþ p ðx j h ðl þ applicable to more general undirected graphical models h ðlþ p ðh j x ðlþ þ but are particularly practical in the rbm setting freund and haussler proposed a learning algorithm for naively for each gradient update step one would start a harmoniums rbms based on projection pursuit cd gibbs sampling chain wait until the chain converges to the has been used most often to train rbms and many equilibrium distribution and then draw a sufficient number recent papers use stochastic maximum likelihood sml of samples to approximate the expected gradient with respect to the model joint distribution in then restart as discussed in section in training probabilistic the process for the next step of approximate gradient ascent models parameters are typically adapted to maximize the on the log likelihood this procedure has the obvious flaw likelihood of the training data or equivalently the log that waiting for the gibbs chain to burn in and reach likelihood or its penalized version which adds a regular equilibrium anew for each gradient update cannot form the ization term with t training examples the log likelihood basis of a practical training algorithm cd sml is given by and fast weights persistent cd or fast weights bengio et al representation learning a review and new perspectives persistent contrastive divergence fpcd are all ways which implies that the learning update will slightly to avoid or reduce the need for burn in increase the energy decrease the probability of those samples making the region in the neighborhood of those cd samples less likely to be resampled and therefore making it cd estimation estimates the negative phase more likely that the samples will move somewhere else expectation with a very short gibbs chain often just typically going near another mode rather than drawing one step initialized at the training data used in the positive samples from the distribution of the current model with phase this reduces the variance of the gradient estimator parameters fpcd exaggerates this effect by drawing and still moves in a direction that pulls the negative chain samples from a local perturbation of the model with samples toward the associated positive chain samples parameters and an update much has been written about the properties and alternative x t interpretations of cd and its similarity to autoencoder þ þ t þ log pðxðtþ þ training for example 225 i sml where is the relatively large fast weight learning rate the sml algorithm also known as persistent contrastive and but near is a forgetting factor that divergence or pcd is an alternative way to keeps the perturbed model close to the current model sidestep an extended burn in of the negative phase gibbs unlike tempering fpcd does not converge to the model sampler at each gradient update rather than initializing distribution as and go to and further work is the gibbs chain at the positive phase sample as in cd sml necessary to characterize the nature of its approximation to initializes the chain at the last state of the chain used for the the model distribution nevertheless fpcd is a popular previous update in other words sml uses a continually and apparently effective means of drawing approximate running gibbs chain or often a number of gibbs chains samples from the model distribution that faithfully repre run in parallel from which samples are drawn to estimate sent its diversity at the price of sometimes generating the negative phase expectation despite the model para spurious samples in between two modes because the fast meters changing between updates these changes should be weights roughly correspond to a smoothed view of the small enough that only a few steps of gibbs in practice current model energy function it has been applied in a often one step is used are required to maintain samples variety of applications and it has been from the equilibrium distribution of the gibbs chain i e transformed into a sampling algorithm that also shares the model distribution this fast mixing property with herding for the same a troublesome aspect of sml is that it relies on the gibbs reason i e introducing negative correlations between con chain to mix well especially between modes for learning to secutive samples of the chain to promote faster mixing succeed typically as learning progresses and the weights of the rbm grow the ergodicity of the gibbs sample begins to pseudolikelihood ratio matching and more break down if the learning rate associated with gradient while cd sml and fpcd are by far the most popular ascent þ g with g log p ðxþ is not reduced to methods for training rbms and rbm based models all of compensate then the gibbs sampler will diverge from the these methods are perhaps most naturally described as model distribution and learning will fail desjardins et al offering different approximations to maximum likelihood cho et al and salakhutdinov have all training there exist other inductive principles that are considered various forms of tempered transitions to address alternatives to maximum likelihood that can also be used to the failure of gibbs chain mixing and convincing solutions train rbms in particular these include pseudolikelihood have not yet been clearly demonstrated a recently intro and ratio matching both of these inductive duced promising avenue relies on depth itself relying on the principles attempt to avoid explicitly dealing with the observation that mixing between modes is much easier on partition function and their asymptotic efficiency has been deeper layers section analyzed pseudolikelihood seeks to maximize the tieleman and hinton have proposed quite a product of all conditional distributions of the form different approach to addressing potential mixing problems p ðxd j xnd þ while ratio matching can be interpreted as an of sml with their fpcd and it has also been exploited to extension of score matching to discrete data types both train dbms and construct a pure sampling algorithm methods amount to weighted differences of the gradient of for rbms fpcd builds on the surprising but robust the rbm free evaluated at a data point and at tendency of gibbs chains to mix better during sml neighboring points one potential drawback of these learning than when the model parameters are fixed the methods is that depending on the parameterization of the phenomenon is rooted in the form of the likelihood energy function their computational requirements may gradient itself the samples drawn from the sml scale up to oðnd þ worse than cd sml fpcd or denoising gibbs chain are used in the negative phase of the gradient score matching discussed below marlin et al empirically compared all of these methods except when weights become large the estimated distribution is more peaky and the chain takes a very long time to mix to move from mode to denoising score matching on a range of classification mode so that practically the gradient estimator can be very poor this is a serious chicken and egg problem because if sampling is not effective then the free energy f ðx þ is the energy associated with the data neither is the training procedure which may seem to stall and yield even marginal probability f ðx þ log p ðxþ log z and is tractable for larger weights the rbm ieee transactions on pattern analysis and machine intelligence vol no august reconstruction and density modeling tasks and found that direct encoding methods that do not require a decoder in general sml provided the best combination of overall such as semi supervised embedding and slow feature performance and computational tractability however in a analysis later study the same authors found denoising score matching to be a competitive inductive principle both in autoencoders terms of classification performance with respect to sml in the autoencoder framework one starts by and in terms of computational efficiency with respect to explicitly defining a feature extracting function in a specific analytically obtained score matching denoising score parameterized closed form this function which we will matching is a special case of the denoising autoencoder denote f is called the encoder and will allow the dae training criterion section when the reconstruc straightforward and efficient computation of a feature tion error residual equals a gradient i e the score function vector h f ðxþ from an input x for each example xðtþ associated with an energy function as shown in from a dataset xðt þ g we define in the spirit of the boltzmann machine gradient hðtþ f ðxðtþ þ several approaches have been proposed to train energy based models one is noise contrastive estimation in where hðtþ is the feature vector or representation or code which the training criterion is transformed into a probabil computed from xðtþ another closed form parameterized istic classification problem distinguish between positive function g called the decoder maps from feature space back training examples and negative noise samples generated into input space producing a reconstruction r g ðhþ by a broad distribution such as the gaussian another whereas probabilistic models are defined from an explicit family of approaches more in the spirit of cd relies on probability function and are trained to maximize often distinguishing positive examples of the training distribu approximately the data likelihood or a proxy autoenco tion and negative examples obtained by perturbations of ders are parameterized through their encoder and decoder the positive examples and are trained using a different training principle the set of parameters of the encoder and decoder are learned simultaneously on the task of reconstructing as well as directly learning a parametric map from possibly the original input i e attempting to incur the input to representation lowest possible reconstruction error lðx rþ a measure of the within the framework of probabilistic models adopted in discrepancy between x and its reconstruction r over section the learned representation is always associated training examples good generalization means low recon with latent variables specifically with their posterior struction error at test examples while having high distribution given an observed input x unfortunately this reconstruction error for most other x configurations to posterior distribution tends to become very complicated capture the structure of the data generating distribution it and intractable if the model has more than a couple of is therefore important that something in the training interconnected layers whether in the directed or undir criterion or the parameterization prevents the autoencoder ected graphical model frameworks it then becomes from learning the identity function which has zero necessary to resort to sampling or approximate inference reconstruction error everywhere this is achieved through techniques and to pay the associated computational and various means in the different forms of autoencoders as approximation error price if the true posterior has a large described below in more detail and we call these regularized number of modes that matter then current inference autoencoders a particular form of regularization consists of techniques may face an unsurmountable challenge or constraining the code to have a low dimension and this is endure a potentially serious approximation this is in what the classical autoencoder or pca do addition to the difficulties raised by the intractable partition in summary basic autoencoder training consists in function in undirected graphical models moreover finding a value of parameter vector minimizing recon a posterior distribution over latent variables is not yet a struction error simple usable feature vector that can for example be fed to a x j ae ð þ lðxðtþ g ðf ðxðtþ þþþ classifier so actual feature values are typically derived from t that distribution taking the latent variable expectation as is typically done with rbms their marginal probability where xðtþ is a training example this minimization is or finding their most likely value as in sparse coding if usually carried out by stochastic gradient descent as in the we are to extract stable deterministic numerical feature training of mlps since autoencoders were primarily values in the end anyway an alternative apparently developed as mlps predicting their input the most commonly used forms for the encoder and decoder are nonprobabilistic feature learning paradigm that focuses on affine mappings optionally followed by a nonlinearity carrying out this part of the computation very efficiently is that of autoencoders and other directly parameterized f ðxþ sf ðb þ w xþ feature or representation functions the commonality between these methods is that they learn a direct encoding g ðhþ sg ðd þ w hþ i e a parametric map from inputs to their representation regularized autoencoders discussed next also involve where sf and sg are the encoder and decoder activation learning a decoding function that maps back from functions typically the elementwise sigmoid or hyperbolic representation to input space sections and discuss tangent nonlinearity or the identity function if staying bengio et al representation learning a review and new perspectives linear the set of parameters of such a model is the representation even when it is overcomplete the effect fw b w dg where b and d are called encoder and of a bottleneck or of this regularization is that the decoder bias vectors and w and w are the encoder and autoencoder cannot reconstruct everything well that it is decoder weight matrices trained to reconstruct well the training examples and the choice of sg and l depends largely on the input generalization mean that reconstruction error is also small domain range and nature and is usually chosen so that l on test examples an interesting justification for the returns a negative log likelihood for the observed value of x sparsity penalty or any penalty that restricts in a soft way a natural choice for an unbounded domain is a linear the volume of hidden configurations easily accessible by the decoder with a squared reconstruction error i e sg ðaþ a learner is that it acts in spirit like the partition function of and lðx rþ kx if inputs are bounded between and rbms by making sure that only few input configurations however ensuring a similarly bounded reconstruction can have a low reconstruction error can be achieved by using sg sigmoid in addition if the alternatively one can view the objective of the regular inputs are of a binary nature a binary cross entropy is ization applied to an autoencoder as making the represen sometimes used tation as constant insensitive as possible with respect to if both encoder and decoder use a sigmoid nonlinearity changes in input this view immediately justifies two then f ðxþ and g ðhþ have the exact same form as the variants of regularized autoencoders described below conditionals p ðh j vþ and p ðv j hþ of binary rbms see contractive autoencoders caes reduce the number of section this similarity motivated an initial study of effective degrees of freedom of the representation around each point by making the encoder contractive i e making the possibility of replacing rbms with autoencoders as the the derivative of the encoder small thus making the basic pretraining strategy for building deep networks as hidden units saturate while the dae makes the whole well as the comparative analysis of autoencoder reconstruc mapping robust i e insensitive to small random tion error gradient and cd updates perturbations or contractive making sure that the recon one notable difference in the parameterization is that struction cannot stay good when moving in most directions rbms use a single weight matrix which follows naturally around a training example from their energy function whereas the autoencoder framework allows for a different matrix in the encoder sparse autoencoders and decoder in practice however weight tying in which one the earliest uses of single layer autoencoders for building defines w w t may be and is most often used deep architectures by stacking them considered the rendering the parameterizations identical the usual train idea of tying the encoder weights and decoder weights to ing procedures however differ greatly between the two approaches a practical advantage of training autoencoder restrict capacity as well as the idea of introducing a form of variants is that they define a simple tractable optimization sparsity regularization sparsity in the representation objective that can be used to monitor progress can be achieved by penalizing the hidden unit biases in the case of a linear autoencoder linear encoder and making these additive offset parameters more negative decoder with squared reconstruction error minimizing 71 or by directly penalizing the output learns the same as pca this is also true of the hidden unit activations making them closer to their when using a sigmoid nonlinearity in the encoder but saturating value at 162 penalizing the bias not if the weights w and w are tied w w t because w runs the danger that the weights could compensate for the cannot be forced into being small and w large to achieve a bias which could hurt numerical optimization when linear encoder directly penalizing the hidden unit outputs several variants similarly le et al recently can be found in the literature but a clear comparative p pshowed that adding a analysis is still lacking although the penalty i e simply regularization term of the form t j ðwj xðtþ þ to a linear autoencoder with tied weights where is a nonlinear the sum of output elements hj in the case of sigmoid convex function yields an efficient algorithm for learning nonlinearity would seem the most natural because of its linear ica use in sparse coding it is used in few papers involving sparse autoencoders a close cousin of the penalty is the regularized autoencoders student t penalty þ þ originally proposed for like pca autoencoders were originally seen as a dimen sparse coding several papers penalize the average sionality reduction technique and thus used a bottleneck i e output h j e g over a minibatch and instead of pushing it dh dx on the other hand successful uses of sparse coding to encourage it to approach a fixed target either through a and rbm approaches tend to favor overcomplete representa mean square error penalty or maybe more sensibly tions i e dh dx this can allow the autoencoder to simply because hj behaves like a probability a kullback liebler duplicate the input in the features with perfect reconstruc divergence with respect to the binomial distribution with tion without having extracted more meaningful features probability log h j þ h j þ þ constant for recent research has demonstrated very successful alter example with 05 native ways called regularized autoencoders to constrain daes pdx lðx rþ xi logðri þ þ ri þ ri þ vincent et al proposed altering the training contrary to traditional pca loading factors but similarly to the objective in from mere reconstruction to that of parameters learned by probabilistic pca the weight vectors learned by a linear autoencoder are not constrained to form an orthonormal basis nor to denoising an artificially corrupted input i e learning to have a meaningful ordering they will however span the same subspace reconstruct the clean input from a corrupted version ieee transactions on pattern analysis and machine intelligence vol no august score matching efficiently finally alain and bengio generalize and prove that daes of arbitrary para meterization with small gaussian corruption noise are general estimators of the score caes caes proposed by rifai et al follow up on daes and share a similar motivation of learning robust representa tions caes achieve this by adding an analytic contractive penalty to the frobenius norm of the encoder jacobian and results in penalizing the sensitivity of learned fig when data concentrate near a lower dimensional manifold the features to infinitesimal input variations let jðxþ f x ðxþ corruption vector is typically almost orthogonal to the manifold and the be the jacobian matrix of the encoder at x the cae reconstruction function learns to denoise map from low probability training objective is configurations corrupted inputs to high probability ones original inputs creating a vector field aligned with the score derivative of the x estimated density j cae l xðtþ g f ðxðtþ þ þ kjðxðtþ t learning the identity is no longer enough the learner must where is a hyperparameter controlling the strength of the capture the structure of the input distribution to optimally regularization for an affine sigmoid encoder the contrac undo the effect of the corruption process with the tive penalty term is easy to compute reconstruction essentially being a nearby but higher density point than the corrupted input fig illustrates that the jj ðxþ f ðxþj f ðxþj þwj x dae is learning a reconstruction function that corresponds ðf ðxþj f ðxþj kwj to a vector field pointing toward high density regions j the manifold where examples concentrate there are at least three notable differences with daes formally the objective optimized by a dae is which may be partly responsible for the better performance x j dae ieqð xjxðtþ þ lðxðtþ g ðf ð xþþþ that cae features seem to empirically demonstrate the t sensitivity of the features is rather than that of the reconstruction the penalty is analytic rather than where ieqð xjxðtþ þ averages over corrupted examples x stochastic an efficiently computable expression replaces drawn from corruption process qð x j xðtþ þ in practice this what might otherwise require dx corrupted samples to size is optimized by stochastic gradient descent where the up i e the sensitivity in dx directions a hyperpara stochastic gradient is estimated by drawing one or a few meter allows fine control of the tradeoff between corrupted versions of xðtþ each time xðtþ is considered reconstruction and robustness while the two are mingled corruptions considered in vincent et al include in a dae note however that there is a tight connection additive isotropic gaussian noise salt and pepper noise for between the dae and the cae as shown in a dae gray scale images and masking noise salt or pepper only with small corruption noise can be seen through a taylor for example setting some randomly chosen inputs to expansion as a type of contractive autoencoder where the independently per example masking noise has been used contractive penalty is on the whole reconstruction function in most of the simulations qualitatively better features are rather than just on the encoder reported with denoising resulting in improved classifica a potential disadvantage of the cae analytic penalty is tion and dae features performed similarly or better than that it amounts to only encouraging robustness to infinitesimal rbm features chen et al show that a simpler input variations this is remedied in with the alternative with a closed form solution can be obtained cae h which penalizes all higher order derivatives in when restricting to a linear autoencoder and have success an efficient stochastic manner by adding a term that fully applied it to domain adaptation encourages jðxþ and jðx þ þ to be close vincent relates daes to energy based probabilistic models daes basically learn in rð xþ x a vector pointing x j caeþh l xðtþ g ðxðtþ þ þ kjðxðtþ in the direction of the estimated score log x pð xþ fig in the t special case of linear reconstruction and squared error þ ie kjðxþ jðx þ vincent shows that training an affine sigmoid affine dae amounts to learning an energy based model whose where n iþ and is the associated regularization energy function is very close to that of a grbm training strength hyperparameter uses a regularized variant of the score matching parameter the dae and cae have been successfully used to win estimation technique termed denoising score the final phase of the unsupervised and transfer learning matching swersky had shown that training challenge the representation learned by the cae grbms with score matching is equivalent to training a regular autoencoder with an additional regularization term that is the robustness of the representation is encouraged but note that in the cae the decoder weights are tied to the encoder while follow up on the theoretical results in weights to avoid degenerate solutions and this should also make the showed the practical advantage of denoising to implement decoder contractive bengio et al representation learning a review and new perspectives tends to be saturated rather than sparse i e most hidden so a sparse coding stage could be jointly optimized units are near the extremes of their range e g or and along with following stages of a deep architecture i ðxþ their derivative h x is near the nonsaturated units are few and sensitive to the inputs with their associated filters weight vectors together forming a basis explaining the representation learning as manifold local changes around x as discussed in section learning another way to get saturated nearly binary units is another important perspective on representation learning is semantic hashing based on the geometric notion of manifold its premise is the manifold hypothesis according to which real world data psd presented in high dimensional spaces are expected to sparse coding may be viewed as a kind of auto concentrate in the vicinity of a manifold m of much lower encoder that uses a linear decoder with a squared dimensionality dm embedded in high dimensional input reconstruction error but whose nonparametric encoder f space irdx this prior seems particularly well suited for ai performs the comparatively nontrivial and relatively costly tasks such as those involving images sounds or text for iterative minimization of a practically successful which most uniformly sampled input configurations are variant of sparse coding and autoencoders named psd unlike natural stimuli as soon as there is a notion of replaces that costly and highly nonlinear encoding representation one can think of a manifold by consider step by a fast noniterative approximation during recogni ing the variations in input space which are captured by or tion computing the learned features psd has been applied reflected by corresponding changes in the learned repre to object recognition in images and video sentation to first approximation some directions are well but also to audio mostly within the framework of preserved the tangent directions of the manifold while multistage convolutional deep architectures section others are not directions orthogonal to the manifolds with the main idea can be summarized by the following this perspective the primary unsupervised learning task is equation for the training criterion which is simultaneously then seen as modeling the structure of the data supporting optimized with respect to hidden codes representation hðtþ manifold the associated representation being learned can and with respect to parameters ðw þ be associated with an intrinsic coordinate system on the x embedded manifold the archetypal manifold modeling j psd khðtþ þ kxðtþ w hðtþ þ khðtþ f ðxðtþ algorithm is not surprisingly also the archetypal low t dimensional representation learning algorithm pca which models a linear manifold it was initially devised with the objective of finding the closest linear manifold to a cloud of where xðtþ is the input vector for example t hðtþ is the data points the principal components i e the representa optimized hidden code for that example and f ð þ is the tion f ðxþ that pca yields for an input point x uniquely encoding function the simplest variant being locates its projection on that manifold it corresponds to f ðxðtþ þ tanhðb þ w t xðtþ þ intrinsic coordinates on the manifold data manifolds for complex real world domains are however expected to be where encoding weights are the transpose of decoding strongly nonlinear their modeling is sometimes approached weights many variants have been proposed including the as patchworks of locally linear tangent spaces use of a shrinkage operation instead of the hyperbolic the large majority of algorithms built on this geometric tangent note that the penalty on h tends to make perspective adopt a nonparametric approach based on a them sparse and how this is the same criterion as sparse training set nearest neighbor graph coding with dictionary learning except for the additional in these nonparametric ap constraint that one should be able to approximate the proaches each high dimensional training point has its sparse codes h with a parameterized encoder f ðxþ one own set of free low dimensional embedding coordinates can thus view psd as an approximation to sparse coding which are optimized so that certain properties of the where we obtain a fast approximate encoder once psd is neighborhood graph computed in original high dimen trained object representations f ðxþ are used to feed a sional input space are best preserved these methods classifier they are computed quickly and can be further however do not directly learn a parameterized feature fine tuned the encoder can be viewed as one stage or one extraction function f ðxþ applicable to new test points layer of a trainable multistage system such as a feedforward which seriously limits their use as feature extractors except neural network in a transductive setting comparatively few nonlinear psd can also be seen as a kind of autoencoder where the manifold learning methods have been proposed that learn a codes h are given some freedom that can help to further parametric map that can directly compute a representation improve reconstruction one can also view the encoding for new points we will focus on these penalty added on top of sparse coding as a kind of regularizer that forces the sparse codes to be nearly actually data points need not strictly lie on the manifold but the probability density is expected to fall off sharply as one moves away from it computable by a smooth and efficient encoder this is in and it may actually be constituted of several possibly disconnected contrast with the codes obtained by complete optimization manifolds with different intrinsic dimensionality of the sparse coding criterion which are highly nonsmooth for several of these techniques representations for new points can be computed using the nystro m approximation as has been or even nondifferentiable a problem that motivated other proposed as an extension in but this remains cumbersome and approaches to smooth the inferred codes of sparse coding computationally expensive ieee transactions on pattern analysis and machine intelligence vol no august learning a parametric mapping based on a to be stuck at until a significant displacement has taken neighborhood graph place in input space some of the above nonparametric manifold learning the local coordinate coding lcc algorithm is very algorithms can be modified to learn a parametric mapping similar to sparse coding but is explicitly derived from a f i e applicable to new points instead of having free low manifold perspective using the same notation as that of dimensional embedding coordinate parameters for each p ðtþin lcc replaces regularization term sparse coding training point these coordinates are obtained through an khðtþ j jhj j yielding objective explicitly parameterized function as with the parametric x x ðtþ variant of t sne j lcc kxðtþ w hðtþ þ jhj jkw j xðtþ instead semi supervised embedding learns a direct t j encoding while taking into account the manifold hypothesis through a neighborhood graph a parameterized neural network architecture simultaneously learns a manifold this is identical to sparse coding when p but with embedding and a classifier the training criterion encourages larger p it encourages the active anchor points for xðtþ i e the ðtþ training set neigbhors to have similar representations codebook vectors w j with nonnegligible jhj j that are the reduced and tightly controlled number of free combined to reconstruct xðtþ to be not too far from xðtþ parameters in such parametric methods compared to their hence the local aspect of the algorithm an important pure nonparametric counterparts forces models to general theoretical contribution of yu et al is to show that that ize the manifold shape nonlocally which can translate any lipschitz smooth function m ir defined on a into better features and final performance however smooth nonlinear manifold m embedded in irdx can be basing the modeling of manifolds on training set neighbor well approximated by a globally linear function with respect hood relationships might be risky statistically in high to the resulting coding scheme i e linear in h where the dimensional spaces sparsely populated due to the curse of accuracy of the approximation and required number dh of dimensionality as for example most euclidean nearest anchor points depend on dm rather than dx this result has neighbors risk having too little in common semantically been further extended with the use of local tangent the nearest neighbor graph is simply not sufficiently directions as well as to multiple layers densely populated to map out satisfyingly the wrinkles of let us now consider the efficient noniterative feedfor the target manifold it can also become ward encoders f used by psd and the autoencoders reviewed in section which are in the form of or problematic computationally to consider all pairs of data the computed representation for x will only be significantly points which scales quadratically with training set size sensitive to input space directions associated with non learning to represent nonlinear manifolds saturated hidden units see e g for the jacobian of a can we learn a manifold without requiring nearest sigmoid layer these directions to which the representation neighbor searches yes for example with regularized is significantly sensitive like in the case of pca or sparse autoencoders or pca in pca the sensitivity of the coding may be viewed as spanning the tangent space of the extracted components the code to input changes is the manifold at point x same regardless of position x the tangent space is the same rifai et al empirically analyze in this light the everywhere along the linear manifold by contrast for a singular value spectrum of the jacobian derivatives of nonlinear manifold the tangent of the manifold changes as representation vector with respect to input vector of a we move on the manifold as illustrated in fig in trained cae here the svd provides an ordered ortho nonlinear representation learning algorithms it is conveni normal basis of most sensitive directions the spectrum is ent to think about the local variations in the representation as sharply decreasing indicating a relatively small number of the input x is varied on the manifold i e as we move among significantly sensitive directions this is taken as empirical high probability configurations as we discuss below the evidence that the cae indeed modeled the tangent space of first derivative of the encoder therefore specifies the shape a low dimensional manifold the leading singular vectors of the manifold its tangent plane around an example x form a basis for the tangent plane of the estimated lying on it if the density was really concentrated on the manifold as illustrated in fig the cae criterion is manifold and the encoder had captured that we would find believed to achieve this thanks to its two opposing terms the encoder derivatives to be nonzero only in the directions the isotropic contractive penalty that encourages the representation to be equally insensitive to changes in any spanned by the tangent plane input directions and the reconstruction term that pushes let us consider sparse coding in this light parameter different training points in particular neighbors to have a matrix w may be interpreted as a dictionary of input different representation so they may be reconstructed directions from which a different subset will be picked to accurately thus counteracting the isotropic contractive model the local tangent space at an x on the manifold that pressure only in directions tangent to the manifold subset corresponds to the active i e nonzero features for analyzing learned representations through the lens of input x nonzero component hi will be sensitive to small the spectrum of the jacobian and relating it to the notion of changes of the input in the direction of the associated tangent space of a manifold is feasible whenever the weight vector w i whereas inactive features are more likely mapping is differentiable and regardless of how it was even if pairs are picked stochastically many must be considered learned whether as direct encoding as in autoencoder before obtaining one that weighs significantly on the optimization objective variants or derived from latent variable inference as in bengio et al representation learning a review and new perspectives p ðx j hþ of sparse coding section which only accounts for the decoder the encoder is viewed as an approximate inference mechanism to guess p ðh j xþ and initialize a map iterative inference where the sparse prior p ðhþ is taken into account however in psd the encoder is trained jointly with the decoder rather than simply taking the end result of iterative inference as a target to approximate fig the tangent vectors to the high density manifold as estimated by an interesting to reconcile these facts is that the a contractive autoencoder the original input is shown on the top encoder is a parametric approximation for the map solution of a left each tangent vector images on right side of first row corresponds variational lower bound on the joint log likelihood when map to a plausible additive deformation of the original input as illustrated on the second row where a bit of the third singular vector is added to the learning is viewed as a special case of variational learning original to form a translated and deformed image unlike in pca the where the approximation of the joint log likelihood is tangent vectors are different for different inputs because the estimated with a dirac distribution located at the map solution manifold is highly nonlinear the variational recipe tells us to simultaneously improve the likelihood reduce reconstruction error and improve the sparse coding or rbms exact low dimensional manifold variational approximation reduce the discrepancy between models like pca would yield nonzero singular values the encoder output and the latent variable value hence associated to directions along the manifold and exact zeros psd sits at the intersection of probabilistic models with for directions orthogonal to the manifold but in smooth latent variables and direct encoding methods which models like the cae or the rbm we will instead have large directly parameterize the mapping from input to represen versus relatively small singular values as opposed to tation rbms also sit at the intersection because their nonzero versus exactly zero particular parameterization includes an explicit mapping leveraging the modeled tangent spaces from input to representation thanks to the restricted the local tangent space at a point along the manifold can connectivity between hidden units however this nice be thought of as capturing locally valid transformations property does not extend to their natural deep general that were prominent in the training data for example izations i e dbms discussed in section rifai et al examine the tangent directions extracted regularized autoencoders capture local with an svd of the jacobian of caes trained on digits structure of the density images or text document data they appear to correspond can we also say something about the probabilistic inter to small translations or rotations for images or digits and pretation of regularized autoencoders their training to substitutions of words within the same theme for criterion does not fit the standard likelihood framework documents such very local transformations along a data because this would involve a data dependent prior an manifold are not expected to change class identity to interesting hypothesis emerges to answer that question out build their mtc rifai et al then apply techniques of recent theoretical results the training criterion such as tangent distance and tangent propagation of regularized autoencoders instead of being a form of which were initially developed to build classifiers that are maximum likelihood corresponds to a different inductive insensitive to input deformations provided as prior principle such as score matching the score matching domain knowledge now these techniques are applied connection is discussed in section and has been shown using the local leading tangent directions extracted by a for a particular parametrization of dae and equivalent cae i e not using any prior domain knowledge except grbm the work in generalizes this idea to a the broad prior about the existence of a manifold this broader class of parameterizations arbitrary encoders and approach set a new record for mnist digit classification decoders and shows that by regularizing the autoencoder among prior knowledge free approaches so that it be contractive one obtains that the reconstruction function and its derivative estimate first and second derivatives connections between probabilistic and of the underlying data generative density this view can be direct encoding models exploited to successfully sample from autoencoders as shown in the proposed sampling algorithms the standard likelihood framework for probabilistic models are mcmcs similar to langevin mcmc using not just the decomposes the training criterion for models with para estimated first derivative of the density but also the meters in two parts the log likelihood log p ðx j þ or estimated manifold tangents so as to stay close to manifolds log p ðx j h þ with latent variables h and the prior log p ð þ of high density or log p ðh j þ þ log p ð þ with latent variables this interpretation connects well with the geometric psd a probabilistic interpretation perspective introduced in section the regularization in the case of the psd algorithm a connection can be made effects e g due to a sparsity regularizer a contractive between the above standard probabilistic view and the regularizer or the denoising criterion ask the learned direct encoding computation graph the probabilistic representation to be as insensitive as possible to the input model of psd is the same directed generative model while minimizing reconstruction error on the training examples forces the representation to contain just enough it yielded 81 percent error rate using the full mnist training set with no prior deformations and no convolution suggested by ian goodfellow personal communication ieee transactions on pattern analysis and machine intelligence vol no august fig top early during training mcmc mixes easily between modes because the estimated distribution has high entropy and puts enough mass everywhere for small steps movements mcmc to go from mode to mode bottom later on training relying on good mixing can stall because estimated modes are separated by wide low density deserts leading singular vectors of the reconstruction or encoder fig reconstruction function rðxþ green learned by a high capacity jacobian corresponding to those associated with smallest autoencoder on input minimizing reconstruction error at training examples xðtþ rðxðtþ þ in red while trying to be as constant as possible second derivative of the log density otherwise the dotted line is the identity reconstruction which might be obtained without the regularizer the blue arrows show the vector field learning approximate inference of rðxþ x pointing toward high density peaks estimated by the model let us now consider from closer how a representation is and estimating the score log density derivative computed in probabilistic models with latent variables when iterative inference is required there is a computation information to distinguish them the solution is that graph possibly with random number generation in some of variations along the high density manifolds are preserved the nodes in the case of mcmc that maps inputs to while other variations are compressed the reconstruction representation and in the case of deterministic inference function should be as constant as possible while reprodu e g map inference or variational inference that function cing training examples i e points near a training example could be optimized directly this is a way to generalize psd should be mapped to that training example fig the that has been explored in recent work on probabilistic reconstruction function should map an input toward the models at the intersection of inference and learning nearest point manifold i e the difference between where a central idea is that instead of reconstruction and input is a vector aligned with the using a generic inference mechanism one can use one that is estimated score the derivative of the log density with learned and is more efficient taking advantage of the respect to the input the score can be zero on the specifics of the type of data on which it is applied manifold where reconstruction error is also zero at local maxima of the log density but it can also be zero at local sampling challenges minima it means that we cannot equate low reconstruction a troubling challenge with many probabilistic models with error with high estimated probability the second deriva latent variables like most boltzmann machine variants is tives of the log density correspond to the first derivatives that good mcmc sampling is required as part of the of the reconstruction function and on the manifold where learning procedure but that sampling becomes extremely the first derivative is they indicate the tangent directions inefficient or unreliable as training progresses because the of the manifold where the first derivative remains near modes of the learned distribution become sharper making as illustrated in fig the basic idea of the auto mixing between modes very slow whereas initially during encoder sampling algorithms in is to make training a learner assigns mass almost uniformly as mcmc moves where one moves toward the manifold training progresse its entropy decreases approaching the by following the density gradient i e applying a entropy of the target distribution as more examples and reconstruction and adds noise in the directions of the more computation are provided according to our manifold and natural clustering priors of section the target distribution has sharp modes manifolds separated by extremely low density areas mixing then becomes more difficult because mcmc methods by their very nature tend to make small steps to nearby high probability configurations this is illustrated in fig bengio et al suggest that deep representations could help mixing between such well separated modes based on both theoretical arguments and on empirical evidence the idea is that if higher level representations disentangle the underlying abstract factors better then fig sampling from regularized autoencoders each mcmc small steps in this abstract space e g swapping from step adds to current state x the noise mostly in the direction of the estimated manifold tangent plane h and projects back toward the one category to another can easily be done by mcmc manifold high density regions by performing a reconstruction step the high level representations can then be mapped back to bengio et al representation learning a review and new perspectives the input space to obtain input level samples as in the global training of deep models dbns sampling algorithm 94 this has been demon one of the most interesting challenges raised by deep strated both with dbns and with the newly proposed architectures is how should we jointly train all the levels in algorithm for sampling from contracting and daes the previous section and in section we have only this observation alone does not suffice to solve the discussed how single layer models could be combined to problem of training a dbn or a dbm but it may provide a form a deep model here we consider joint training of all crucial ingredient and it makes it possible to consider the levels and the difficulties that may arise successfully sampling from deep models trained by procedures that do not require an mcmc like the stacked the challenge of training deep architectures regularized autoencoders used in higher level abstraction means more nonlinearity it means evaluating and monitoring performance that two nearby input configurations may be interpreted very differently because a few surface details change the it is always possible to evaluate a feature learning underlying semantics whereas most other changes in the algorithm in terms of its usefulness with respect to a surface details would not change the underlying semantics particular task e g object classification with a predictor that is fed or initialized with the learned features in the representations associated with input manifolds may practice we do this by saving the features learned e g at be complex because the mapping from input to representa regular intervals during training to perform early stop tion may have to unfold and distort input manifolds that ping and training a cheap classifier on top such as a linear generally have complicated shapes into spaces where classifier however training the final classifier can be a distributions are much simpler where relations between substantial computational overhead e g supervised fine factors are simpler maybe even linear or involving many tuning a deep neural network usually takes more training conditional independencies our expectation is that iterations than the feature learning itself so we may want modeling the joint distribution between high level abstrac to avoid having to train a classifier for every training tions and concepts should be much easier in the sense of iteration of the unsupervised learner and every hyperpara requiring much less data to learn the hard part is learning meter setting more importantly this may give an incom a good representation that does this unfolding and plete evaluation of the features what would happen for disentangling this may be at the price of a more difficult other tasks all these issues motivate the use of methods training problem possibly involving ill conditioning and to monitor and evaluate purely unsupervised performance local minima this is rather easy with all the autoencoder variants with it is only since that researchers have seriously some caution outlined below and rather difficult with the investigated ways to train deep architectures to the excep undirected graphical models such as the rbm and tion of the convolutional networks the first realization boltzmann machines section was that unsupervised or supervised layerwise for autoencoder and sparse coding variants test set training was easier and that this could be taken advantage of reconstruction error can be readily computed but by itself by stacking single layer models into deeper ones may be misleading because larger capacity e g more it is interesting to ask why does the layerwise unsupervised features more training time tends to systematically lead pretraining procedure sometimes help a supervised learner to lower reconstruction error even on the test set hence it there seems to be a more general principle at of cannot be used reliably for selecting most hyperparameters guiding the training of intermediate representations which may on the other hand denoising reconstruction error is clearly be easier than trying to learn it all in one go this is nicely immune to this problem so that solves the problem for related to the curriculum learning idea that it may be daes based on the connection between daes and caes much easier to learn simpler concepts first and then build uncovered in this immunity can be extended to higher level ones on top of simpler ones this is also daes but not to the hyperparameter controlling the coherent with the success of several deep learning algo amount of noise or of contraction rithms that provide some such guidance for intermediate for rbms and some not too deep boltzmann ma representations like semi supervised embedding chines one option is the use of annealed importance the question of why unsupervised pretraining could be sampling to estimate the partition function and thus helpful was extensively studied trying to dissect the the test log likelihood note that this estimator can have answer into a regularization effect and an optimization effect high variance and that it becomes less reliable variance the regularization effect is clear from the experiments becomes too large as the model becomes more interesting where the stacked rbms or daes are used to initialize a with larger weights more nonlinearity sharper modes and supervised classification neural network it may simply a sharper probability density function see our previous come from the use of unsupervised learning to bias the discussion in section another interesting and recently learning dynamics and initialize it in the basin of attraction of proposed option for rbms is to track the partition function a good local minimum of the training criterion where during training which could be useful for early good is in terms of generalization error the underlying stopping and reducing the cost of ordinary ais for toy hypothesis exploited by this procedure is that some of the rbms e g hidden units or less or inputs or less features or latent factors that are good at capturing the the exact log likelihood can also be computed analytically leading variations in the input distribution are also good at and this can be a good way to debug and verify some properties of interest first suggested to us by leon bottou ieee transactions on pattern analysis and machine intelligence vol no august capturing the variations in the target output random neural network training is to nullify the average value and variables of interest e g classes the optimization effect slope of each hidden unit output and possibly locally is more difficult to tease out because the top two layers of a normalize magnitude as well the debate still rages deep neural net can just overfit the training set whether the between using online methods such as stochastic gradient lower layers compute useful features or not but there are descent and using second order methods on large mini several indications that optimizing the lower levels with batches of several thousand examples with a respect to a supervised training criterion can be challenging variant of stochastic gradient descent recently winning an one such indication is that changing the numerical optimization challenge conditions of the optimization procedure can have a finally several recent results exploiting large quantities of profound impact on the joint training of a deep architec labeled data suggest that with proper initialization and ture for example by changing the initialization range and choice of nonlinearity very deep purely supervised net changing the type of nonlinearity used much more so works can be trained successfully without any layerwise than with shallow architectures one hypothesis to explain pretraining researchers report that in some of the difficulty in the optimization of deep such conditions layerwise unsupervised pretraining architectures is centered on the singular values of the brought little or no improvement over pure supervised jacobian matrix associated with the transformation from the learning from scratch when training for long enough this features at one level into the features at the next level if reinforces the hypothesis that unsupervised pretraining acts these singular values are all small less than then the as a prior which may be less necessary when very large mapping is contractive in every direction and gradients quantities of labeled data are available but which begs would vanish when propagated backward through many the question of why this had not been discovered earlier layers this is a problem already discussed for recurrent the latest results reported in this respect are neural networks which can be seen as very deep particularly interesting because they allowed drastically networks with shared parameters at each layer when reducing the error rate of object recognition on a benchmark unfolded in time this optimization difficulty has moti the class imagenet task where many more tradi vated the exploration of second order methods for deep tional computer vision approaches had been evaluated architectures and recurrent networks in particular hessian http www image net org challenges lsvrc free second order methods unsupervised results html the main techniques that allowed this success pretraining has also been proposed to help training include the following efficient gpu training allowing one to recurrent networks and temporal rbms i e at each train longer more than million visits of examples an time step there is a local signal to guide the discovery of aspect first reported by lee et al and ciresan et al good features to capture in the state variables model with a large number of labeled examples artificially transformed the current state as hidden units the joint distribution of examples see section a large number of tasks or the previous state and the current input natural gradient 000 classes for imagenet convolutional architecture with methods that can be applied to networks with millions max pooling see section for these latter two techniques of parameters i e with good scaling properties have also rectifying nonlinearities discussed above careful initialization been proposed cho et al propose using discussed above careful parameter update and adaptive adaptive learning rates for rbm training along with a learning rate heuristics layerwise feature normalization across novel and interesting idea for a gradient estimator that features and a new dropout trick based on injecting strong takes into account the invariance of the model to flipping binary multiplicative noise on hidden units this trick is hidden unit bits and inverting signs of corresponding similar to the binary noise injection used at each layer of a weight vectors at least one study indicates that the choice stack of daes future work is hopefully going to help of initialization to make the jacobian of each layer closer to identify which of these elements matter most how to across all its singular values could substantially reduce generalize them across a large variety of tasks and the training difficulty of deep networks and this is architectures and in particular contexts where most coherent with the success of the initialization procedure of examples are unlabeled i e including an unsupervised echo state networks as recently studied by sutskever component in the training criterion there are also several experimental results showing that the choice of hidden units nonlinearity joint training of dbms could influence both training and generalization perfor we now consider the problem of joint training of all layers mance with particularly interesting results obtained with of a specific unsupervised model the dbm whereas much sparse rectifying units 69 an old idea progress albeit with many unanswered questions has been regarding the ill conditioning issue with neural networks is made on jointly training all the layers of deep architectures that of symmetry breaking part of the slowness of conver using back propagated gradients i e mostly in the gence may be due to many units moving together like supervised setting much less work has been done on their sheep and all trying to reduce the output error for the same purely unsupervised counterpart for example with examples by initializing with sparse weights or by dbms note however that one could hope that the using often saturated nonlinearities such as rectifiers as max pooling units gradients only flow along a few paths https sites google com site optimization challenges which may help hidden units to specialize more quickly joint training of all the layers of a deep belief net is much more another promising idea to improve the conditioning of challenging because of the much harder inference problem involved bengio et al representation learning a review and new perspectives successful techniques described in the previous section lðqv þ ieqv log p ðv þ log qv þ could be applied to unsupervised learning algorithms ieqv e dbm ðv þ log qv þ like the rbm the dbm is another particular subset of log z the boltzmann machine family of models where the units dbm are again arranged in layers however unlike the rbm the lðqv þ e ðv þ ieqv dbm possesses multiple layers of hidden units with units dbm in odd numbered layers being conditionally independent e ðv þ þ iep given even numbered layers and vice versa with respect to the boltzmann energy function of the dbm corresponds to setting u and a sparse connectivity this variational learning procedure leaves the negative structure in both v and w we can make the structure of the phase untouched which can thus be estimated through dbm more explicit by specifying its energy function for sml or cd as in the rbm case the model with two hidden layers it is given as training dbms t the major difference between training a dbm and an rbm e dbm ðv þ vt w h v ð29þ is that instead of maximizing the likelihood directly we t t bt v instead choose parameters to maximize the lower bound on the likelihood given in the sml based algorithm for with fw v bg the dbm can also be charac maximizing this lower bound is as follows terized as a bipartite graph between two sets of vertices formed by odd and even numbered layers with v hð0þ clamp the visible units to a training example iterate over until convergence mean field approximate inference generate negative phase samples v and a key point of departure from the rbm is that the posterior through sml distribution over the hidden units given the visibles is no compute lðqv þ using the values obtained in longer tractable due to the interactions between the hidden steps and units salakhutdinov and hinton resort to a mean finally update the model parameters with a step of field approximation to the posterior specifically in the case approximate stochastic gradient ascent of a model with two hidden layers we wish to approximate while the above procedure appears to be a simple p j vþq q with the factored distribution qv þ extension of the highly effective sml scheme for training qv ðhj þ qv ðhi þ such that the kl divergence rbms as we demonstrate in desjardins et al this klðp ðh h j vþkqv þþ is minimized or equiva procedure seems vulnerable to falling in poor local lently that a lower bound to the log likelihood is maximized minima that leave many hidden units effectively dead xx not significantly different from its random initialization log p ðvþ lðqv þ qv þ with small norm the failure of the sml joint training strategy was noted p ðv þ by salakhutdinov and hinton as an alternative they log qv hð2þ þ proposed a greedy layerwise training strategy this procedure consists of pretraining the layers of the dbm maximizing this lower bound with respect to the mean in much the same way as the dbn i e by stacking rbms field distribution qv þ by setting derivatives to zero and training each layer to independently model the output yields the following mean field update equations of the previous layer a final joint fine tuning is done x x following the above sml based procedure ð1þ h i sigmoid wji vj þ vik h k þ di j k building in invariance x ð1þ it is well understood that incorporating prior domain h k sigmoid vik h i þ dk knowledge helps machine learning exploring good strate i gies for doing so is a very important research avenue note how the above equations ostensibly look like a fixed however if we are to advance our understanding of core point recurrent neural network i e with constant input in the machine learning principles it is important that we keep same way that an rbm can be associated with a simple comparisons between predictors fair and maintain a clear autoencoder the above mean field update equations for the awareness of the prior domain knowledge used by different dbm can be associated with a recurrent autoencoder in that learning algorithms especially when comparing their case the training criterion involves the reconstruction error performance on benchmark problems we have so far only at the last or at consecutive time steps this type of model presented algorithms that exploited only generic inductive has been explored by savard and seung and biases for high dimensional problems thus making them shown to do a better job at denoising than ordinary potentially applicable to any high dimensional problem autoencoders the most prevalent approach to incorporating prior knowl iterating 32 until convergence yields the q para edge is to hand design better features to feed a generic meters of the variational positive phase of classifier and this has been used extensively in computer ieee transactions on pattern analysis and machine intelligence vol no august vision e g here we rather focus on how basic typically taking their max or their sum this confers upon domain knowledge of the input in particular its topological the resulting pooled feature layer some degree of invariance structure e g bitmap images having a structure may to input translations and this style of architecture alter be used to learn better features nating selective feature extraction and invariance creating pooling has been the basis of convolutional networks the generating transformed examples neocognitron and hmax models and argued to generalization performance is usually improved by be the architecture used by mammalian brains for object providing a larger quantity of representative data this recognition the output of a pooling unit can be achieved by generating new examples by applying will be the same irrespective of where a specific feature is small random deformations to the original training located inside its pooling region empirically the use of examples using deformations that are known not to pooling seems to contribute significantly to improved change the target variables of interest for example an classification accuracy in object classification tasks object class is invariant to small transformations of images a successful variant of pooling connected to such as translations rotations scaling or shearing this sparse coding is pooling for which the old approach has recently been applied with great pool output is the square root of the possibly weighted sum success in the work of ciresan et al who used an of squares of filter outputs ideally we would like to efficient gpu implementation speedup to train a generalize feature pooling so as to learn what features should standard but large deep mlp on deformed mnist digits be pooled together for example as successfully done in using both affine and elastic deformations with several papers 53 in this plain old stochastic gradient descent they reach a record way the pool output learns to be invariant to the variations 32 percent classification error rate captured by the span of the features pooled convolution and pooling patch based training another powerful approach is based on even more basic the simplest approach for learning a convolutional layer in knowledge of merely the topological structure of the input an unsupervised fashion is patch based training simply dimensions by this we mean for example the layout of feeding a generic unsupervised feature learning algorithm pixels in images or audio spectrograms the structure of with local patches extracted at random positions of the videos the sequential structure of text or of temporal inputs the resulting feature extractor can then be swiped sequences in general based on such structure one can over the input to produce the convolutional feature maps define local receptive fields so that each low level feature that map may be used as a new input for the next layer will be computed from only a subset of the input a and the operation repeated to thus learn and stack several neighborhood in the topology e g a subimage at a given layers such an approach was recently used with indepen position this topological locality constraint corresponds to dent subspace analysis on video blocks reaching a layer having a very sparse weight matrix with nonzeros the state of the art on the ucf kth and only allowed for topologically local connections comput youtube action recognition datasets similarly coates and ing the associated matrix products can of course be made ng compared several feature learners with patch based much more efficient than having to handle a dense matrix training and reached state of the art results on several in addition to the statistical gain from a much smaller classification benchmarks interestingly in this work number of free parameters in domains with such topolo performance was almost as good with very simple k means gical structure similar input patterns are likely to appear at clustering as with more sophisticated feature learners we different positions and nearby values e g consecutive however conjecture that this is the case only because frames or nearby pixels are likely to have stronger patches are rather low dimensional compared to the dependencies that are also important to model the data dimension of a whole image a large dataset might in fact these dependencies can be exploited to discover the provide sufficient coverage of the space of for example topology i e recover a regular grid of pixels out of a edges prevalent in patches so that a distributed set of vectors without any order information for example representation is not absolutely necessary another plau after the elements have been arbitrarily shuffled in the same sible explanation for this success is that the clusters way for all examples thus the same local feature identified in each image patch are then pooled into a computation is likely to be relevant at all translated histogram of cluster counts associated with a larger positions of the receptive field hence the idea of sweeping subimage whereas the output of a regular clustering is a such a local feature extractor over the topology this one hot nondistributed code this histogram is itself a corresponds to a convolution and transforms an input into distributed representation and the soft k means a similarly shaped feature map equivalently to sweeping representation allows not only the nearest filter but also its this may be seen as static but differently positioned neighbors to be active replicated feature extractors that all share the same parameters this is at the heart of convolutional networks convolutional and tiled convolutional training that have been applied both to object recognition it is possible to directly train large convolutional layers and to image segmentation another hallmark of the using an unsupervised criterion an early approach convolutional architecture is that values computed by the trained a standard but deep convolutional mlp on the task same feature detector applied at several neighboring input of denoising images i e as a deep convolutional dae locations are then summarized through a pooling operation convolutional versions of the rbm or its extensions have bengio et al representation learning a review and new perspectives also been developed as well as a surround us second one would expect that instead of just probabilistic max pooling operation built into convolutional being slowly changing different factors could be associated deep networks 136 other unsupervised with their own different time scale the specificity of their feature learning approaches that were adapted to the time scale could thus become a hint to disentangle convolutional setting include psd 111 explanatory factors third one would expect that some a convolutional version of sparse coding called deconvolu factors should really be represented by a group of numbers tional networks topographic ica and mpot such as the x y and z position of some object in space and which kivinen and williams applied to modeling the pose parameters of hinton et al rather than by a natural textures gregor and lecun and le et al single scalar and that these groups tend to move together also demonstrated the technique of tiled convolution where structured sparsity penalties 107 could be parameters are shared only between feature extractors used for this purpose whose receptive fields are k steps away so the ones looking at immediate neighbor locations are not shared this algorithms to disentangle factors of variation allows pooling units to be invariant to more than just the goal of building invariant features is to remove translations and is a hybrid between convolutional net sensitivity of the representation to directions of variance works and earlier neural networks with local connections in the data that are uninformative to the task at hand but no weight sharing however it is often the case that the goal of feature extraction is the disentangling or separation of many distinct alternatives to pooling but informative factors in the data for example in a video alternatively one can also use explicit knowledge of the of people subject identity action performed subject pose expected invariants expressed mathematically to define relative to the camera and so on in this situation the transformations that are robust to a known family of input methods of generating invariant features such as feature deformations using the so called scattering operators pooling may be inadequate which can be computed in a way interestingly the process of building invariant features can be seen as analogous to deep convolutional networks and wavelets consisting of two steps first low level features are like convolutional networks the scattering operators recovered that account for the data second subsets of alternate two types of operations convolution and pooling these low level features are pooled together to form higher as a norm unlike convolutional networks the proposed level invariant features exemplified by the pooling and approach keeps at each level all of the information about the subsampling layers of convolutional neural networks the input in a way that can be inverted and automatically invariant representation formed by the pooling features yields a very sparse but very high dimensional represen offers an incomplete window on the data as the detailed tation another difference is that the filters are not learned representation of the lower level features is abstracted away but instead are set so as to guarantee that a priori specified in the pooling procedure while we would like higher level invariances are robustly achieved just a few levels were features to be more abstract and exhibit greater invariance sufficient to achieve impressive results on several bench we have little control over what information is lost through mark datasets pooling what we really would like is for a particular feature set to be invariant to the irrelevant features and temporal coherence and slow features disentangle the relevant features unfortunately it is often the principle of identifying slowly moving changing difficult to determine a priori which set of features will factors in temporal spatial data has been investigated by ultimately be relevant to the task at hand many 219 as a principle for finding an interesting approach to taking advantage of some of useful representations in particular this idea has been the factors of variation known to exist in the data is the applied to image sequences and as an explanation for transforming autoencoder instead of a scalar pattern why simple and complex cells behave the way they do detector e g corresponding to the probability of the a good overview can be found in presence of a particular form in the input one can think more recently temporal coherence has been successfully of the features as organized in groups that include both a exploited in deep architectures to model video it was pattern detector and pose parameters that specify attributes of also found that temporal coherence discovered visual the detected pattern in what is assumed a priori is that features similar to those obtained by ordinary unsuper pairs of examples or consecutive ones are observed with vised feature learning and a temporal coherence an associated value for the corresponding change in the pose penalty has been combined with a training criterion for parameters for example an animal that controls its eyes unsupervised feature learning sparse autoencoders knows what changes to its ocular motor system were applied with regularization in this case yielding improved when going from one image on its retina to the next in that classification performance work it is also assumed that the pose changes are the same the temporal coherence prior can be expressed in for all the pattern detectors and this makes sense for global several ways the simplest being the squared difference changes such as image translation and camera geometry between feature values at times t and t þ other plausible changes instead we would like to discover the pose temporal coherence priors include the following first parameters and attributes that should be associated with instead of penalizing the squared change penalizing the each feature detector without having to specify ahead of absolute value or a similar sparsity penalty would state time what they should be force them to be the same for all that most of the time the change should be exactly which features and having to necessarily observe the changes in would intuitively make sense for the real life factors that all of the pose parameters or attributes ieee transactions on pattern analysis and machine intelligence vol no august the approach taken recently in the mtc discussed in approaches the probabilistic models both the directed section is interesting in this respect without any kind such as sparse coding and the undirected kind such as supervision or prior knowledge it finds prominent local boltzmann machines the reconstruction based algorithms factors of variation tangent vectors to the manifold related to autoencoders and the geometrically motivated extracted from a cae interpreted as locally valid input manifold learning approaches drawing connections be deformations higher level features are subsequently tween these approaches is currently a very active area of encouraged to be invariant to these factors of variation so research and is likely to continue to produce models and that they must depend on other characteristics in a sense methods that take advantage of the relative strengths of this approach is disentangling valid local deformations each paradigm along the data manifold from other more drastic changes practical concerns and guidelines one of the criticisms associated to other factors of variation such as those that addressed to artificial neural networks and deep learning affect class identity algorithms is that they have many hyperparameters and one solution to the problem of information loss that variants and that exploring their configurations and would fit within the feature pooling paradigm is to architectures is an art this has motivated an earlier book consider many overlapping pools of features based on the on the tricks of the trade of which is still same low level feature set such a structure would have the relevant for training deep architectures in particular what potential to learn a redundant set of invariant features that concerns initialization ill conditioning and stochastic may not cause significant loss of information however it gradient descent a good and more modern compendium is not obvious what learning principle could be applied of good training practice particularly adapted to training that can ensure that the features are invariant while rbms is provided in while a similar guide oriented maintaining as much information as possible while a more toward deep neural networks can be found in dbn or a dbm as discussed in sections and both of which are part of a novel version of the above respectively with two hidden layers would in principle be book recent work on automating hyperparameter search able to preserve information into the pooling second is also making it more convenient efficient hidden layer there is no guarantee that the second layer and reproducible features are more invariant than the low level first layer incorporating generic ai level priors we have covered features however there is some empirical evidence that many high level generic priors that we believe could bring the second layer of the dbn tends to display more machine learning closer to ai by improving representation invariance than the first layer learning many of these priors relate to the assumed a more principled approach from the perspective of existence of multiple underlying factors of variation whose ensuring a more robust compact feature representation can variations are in some sense orthogonal to each other they be conceived by reconsidering the disentangling of features are expected to be organized at multiple levels of abstrac through the lens of its generative equivalent feature tion hence the need for deep architectures which also have composition since many unsupervised learning algorithms statistical advantages because they allow to reuse para have a generative interpretation or a way to reconstruct meters in a combinatorially efficient way only a few of inputs from their high level representation the generative these factors would typically be relevant for any particular perspective can provide insight into how to think about example justifying sparsity of representation these factors disentangling factors the majority of the models currently are expected to be related to simple e g linear depen used to construct invariant features have the interpretation dencies with subsets of these explaining different random that their low level features linearly combine to construct variables of interest inputs tasks and varying in struc the data this is a fairly rudimentary form of feature tured ways in time and space temporal and spatial composition with significant limitations for example it is coherence we expect future successful applications of not possible to linearly combine a feature with a generic representation learning to refine and increase that list of transformation such as translation to generate a trans priors and to incorporate most of them instead of focusing on only one research in training criteria that better take formed version of the feature nor can we even consider a these priors into account is likely to move us closer to the generic color feature being linearly combined with a long term objective of discovering learning algorithms that grayscale stimulus pattern to generate a colored pattern it can disentangle the underlying explanatory factors would seem that if we are to take the notion of disen inference we anticipate that methods based on directly tangling seriously we require a richer interaction of parameterizing a representation function will incorporate features than that offered by simple linear combinations more and more of the iterative type of computation one it is a challenging task to develop eﬀective and eﬃcient ap pearance models for robust object tracking due to factors such as pose variation illumination change occlusion and motion blur existing on line tracking algorithms often update models with samples from obser vations in recent frames while much success has been demonstrated numerous issues remain to be addressed first while these adaptive appearance models are data dependent there does not exist suﬃcient amount of data for online algorithms to learn at the outset second online tracking algorithms often encounter the drift problems as a re sult of self taught learning these mis aligned samples are likely to be added and degrade the appearance models in this paper we propose a simple yet eﬀective and eﬃcient tracking algorithm with an appearance model based on features extracted from the multi scale image feature space with data independent basis our appearance model employs non adaptive random projections that preserve the structure of the image feature space of objects a very sparse measurement matrix is adopted to eﬃciently extract the features for the appearance model we com press samples of foreground targets and the background using the same sparse measurement matrix the tracking task is formulated as a binary classiﬁcation via a naive bayes classiﬁer with online update in the com pressed domain the proposed compressive tracking algorithm runs in real time and performs favorably against state of the art algorithms on challenging sequences in terms of eﬃciency accuracy and robustness introduction despite that numerous algorithms have been proposed in the literature object tracking remains a challenging problem due to appearance change caused by pose illumination occlusion and motion among others an eﬀective appearance model is of prime importance for the success of a tracking algorithm that has been attracting much attention in recent years tracking algorithms can be generally categorized as either generative or discriminative based on their appearance models generative tracking algorithms typically learn a model to represent the target object and then use it to search for the image region with minimal reconstruction error black et al learn an oﬀ line subspace model to represent the object of interest for tracking the ivt method utilizes an incremental subspace model a fitzgibbon et al eds eccv part iii lncs pp c springer verlag berlin heidelberg real time compressive tracking to adapt appearance changes recently sparse representation has been used in the tracker where an object is modeled by a sparse linear combination of target and trivial templates however the computational complexity of this tracker is rather high thereby limiting its applications in real time scenarios li et al further extend the tracker by using the orthogonal matching pursuit algorithm for solving the optimization problems eﬃciently despite much demonstrated success of these online generative tracking algorithms several problems remain to be solved first numerous training samples cropped from consecutive frames are required in order to learn an appearance model online since there are only a few samples at the outset most tracking algorithms often assume that the target appearance does not change much during this period however if the appearance of the target changes signiﬁcantly at the beginning the drift problem is likely to occur second when multiple samples are drawn at the current target location it is likely to cause drift as the appearance model needs to adapt to these potentially mis aligned examples third these generative algorithms do not use the background information which is likely to improve tracking stability and accuracy discriminative algorithms pose the tracking problem as a binary classiﬁcation task in order to ﬁnd the decision boundary for separating the target object from the background avidan extends the optical ﬂow approach with a support vector machine classiﬁer for object tracking collins et al demonstrate that the most discriminative features can be learned online to separate the target object from the background grabner et al propose an online boosting algo rithm to select features for tracking however these trackers only use one positive sample i e the current tracker location and a few negative samples when updating the classiﬁer as the appearance model is updated with noisy and potentially misaligned examples this often leads to the tracking drift problem grabner et al propose an online semi supervised boosting method to allevi ate the drift problem in which only the samples in the ﬁrst frame are labeled and all the other samples are unlabeled babenko et al introduce multiple instance learning into online tracking where samples are considered within posi tive and negative bags or sets recently a semi supervised learning approach is developed in which positive and negative samples are selected via an online classiﬁer with structural constraints in this paper we propose an eﬀective and eﬃcient tracking algorithm with an appearance model based on features extracted in the compressed domain the main components of our compressive tracking algorithm are shown by fig ure our appearance model is generative as the object can be well represented based on the features extracted in the compressive domain it is also discrimi native because we use these features to separate the target from the surround ing background via a naive bayes classiﬁer in our appearance model features are selected by an information preserving and non adaptive dimensionality re duction from the multi scale image feature space based on compressive sensing theories it has been demonstrated that a small number of randomly generated linear measurements can preserve most of the salient information and allow almost perfect reconstruction of the signal if the signal is compressible such as natural images or audio we use a very sparse measurement matrix that satisﬁes the restricted isometry property rip thereby facilitating ef ﬁcient projection from the image feature space to a low dimensional compressed subspace for tracking the positive and negative samples are projected i e compressed with the same sparse measurement matrix and discriminated by a simple naive bayes classiﬁer learned online the proposed compressive track ing algorithm runs at real time and performs favorably against state of the art trackers on challenging sequences in terms of eﬃciency accuracy and robustness ideally we expect r provides a stable embedding that approxi mately preserves the distance between all pairs of original signals the johnson lindenstrauss lemma states that with high probability the distances be tween the points in a vector space are preserved if they are projected onto a randomly selected subspace with suitably high dimensions baraniuk et al proved that the random matrix satisfying the johnson lindenstrauss lemma also holds true for the restricted isometry property in compressive sensing there fore if the random matrix r in satisﬁes the johnson lindenstrauss lemma we can reconstruct x with minimum error from v with high probability if x is compressive such as audio or image we can ensure that v preserves almost all the information in x this very strong theoretical support motivates us to ana lyze the high dimensional signals via its low dimensional random projections in the proposed algorithm we use a very sparse matrix that not only satisﬁes the johnson lindenstrauss lemma but also can be eﬃciently computed for real time tracking random measurement matrix a typical measurement matrix satisfying the restricted isometry property is the random gaussian matrix r rn m where rij n as used in numerous works recently however as the matrix is dense the memory and computational loads are still large when m is large in this paper we adopt a very sparse random measurement matrix with entries deﬁned as achlioptas proved that this type of matrix with or satisﬁes the johnson lindenstrauss lemma this matrix is very easy to compute which re quires only a uniform random generator more importantly when it is very sparse where two thirds of the computation can be avoided in addition li et al showed that for o m x rm this matrix is asymptotically normal even when m log m the random projections are almost as accu rate as the conventional random projections where rij n in this work we set m which makes a very sparse random matrix for each row of r only about c c entries need to be computed therefore the computational complexity is only o cn which is very low furthermore we only need to store the nonzero entries of r which makes the memory requirement also very light in this section we present our tracking algorithm in details the tracking prob lem is formulated as a detection task and our algorithm is shown in figure we assume that the tracking window in the ﬁrst frame has been determined at each frame we sample some positive samples near the current target location and negative samples far away from the object center to update the classiﬁer to predict the object location in the next frame we draw some samples around the current target location and determine the one with the maximal classiﬁcation score graphical representation of compressing a high dimensional vector x to a low dimensional vector v in the matrix r dark gray and white rectangles represent neg ative positive and zero entries respectively the blue arrows illustrate that one of nonzero entries of one row of r sensing an element in x is equivalent to a rectangle ﬁlter convolving the intensity at a ﬁxed position of an input image eﬃcient dimensionality reduction for each sample z rw h to deal with the scale problem we represent it by convolving z with a set of rectangle ﬁlters at multiple scales hw h deﬁned as where i and j are the width and height of a rectangle ﬁlter respectively then we represent each ﬁltered image as a column vector in rwh and then concatenate these vectors as a very high dimensional multi scale image feature vector x xm rm where m wh the dimensionality m is typically in the order of to we adopt a sparse random matrix r in with m to project x onto a vector v rn in a low dimensional space the random matrix r needs to be computed only once oﬀ line and remains ﬁxed throughout the tracking process for the sparse matrix r in the computational load is very light as shown by figure we only need to store the nonzero entries in r and the positions of rectangle ﬁlters in an input image corresponding to the nonzero entries in each row of r then v can be eﬃciently computed by using r to sparsely measure the rectangular features which can be eﬃciently computed using the integral image method analysis of low dimensional compressive features as shown in figure each element vi in the low dimensional feature v rn is a linear combination of spatially distributed rectangle features at diﬀerent scales as the coeﬃcients in the measurement matrix can be positive or negative via the compressive features compute the relative intensity diﬀerence in a way similar to the generalized haar like features see also figure the haar like features have been widely used for object detection with demonstrated success the basic types of these haar like features are typically de signed for diﬀerent tasks there often exist a very large number of ideally we expect r provides a stable embedding that approxi mately preserves the distance between all pairs of original signals the johnson lindenstrauss lemma states that with high probability the distances be tween the points in a vector space are preserved if they are projected onto a randomly selected subspace with suitably high dimensions baraniuk et al proved that the random matrix satisfying the johnson lindenstrauss lemma also holds true for the restricted isometry property in compressive sensing there fore if the random matrix r in satisﬁes the johnson lindenstrauss lemma we can reconstruct x with minimum error from v with high probability if x is compressive such as audio or image we can ensure that v preserves almost all the information in x this very strong theoretical support motivates us to ana lyze the high dimensional signals via its low dimensional random projections in the proposed algorithm we use a very sparse matrix that not only satisﬁes the johnson lindenstrauss lemma but also can be eﬃciently computed for real time tracking random measurement matrix a typical measurement matrix satisfying the restricted isometry property is the random gaussian matrix r rn m where rij n as used in numerous works recently however as the matrix is dense the memory and computational loads are still large when m is large in this paper we adopt a very sparse random measurement matrix with entries deﬁned as achlioptas proved that this type of matrix with or satisﬁes the johnson lindenstrauss lemma this matrix is very easy to compute which re quires only a uniform random generator more importantly when it is very sparse where two thirds of the computation can be avoided in addition li et al showed that for o m x rm this matrix is asymptotically normal even when m log m the random projections are almost as accu rate as the conventional random projections where rij n in this work we set m which makes a very sparse random matrix for each row of r only about c c entries need to be computed therefore the computational complexity is only o cn which is very low furthermore we only need to store the nonzero entries of r which makes the memory requirement also very light in this section we present our tracking algorithm in details the tracking prob lem is formulated as a detection task and our algorithm is shown in figure we assume that the tracking window in the ﬁrst frame has been determined at each frame we sample some positive samples near the current target location and negative samples far away from the object center to update the classiﬁer to predict the object location in the next frame we draw some samples around the current target location and determine the one with the maximal classiﬁcation score graphical representation of compressing a high dimensional vector x to a low dimensional vector v in the matrix r dark gray and white rectangles represent neg ative positive and zero entries respectively the blue arrows illustrate that one of nonzero entries of one row of r sensing an element in x is equivalent to a rectangle ﬁlter convolving the intensity at a ﬁxed position of an input image eﬃcient dimensionality reduction for each sample z rw h to deal with the scale problem we represent it by convolving z with a set of rectangle ﬁlters at multiple scales hw h deﬁned as where i and j are the width and height of a rectangle ﬁlter respectively then we represent each ﬁltered image as a column vector in rwh and then concatenate these vectors as a very high dimensional multi scale image feature vector x xm rm where m wh the dimensionality m is typically in the order of to we adopt a sparse random matrix r in with m to project x onto a vector v rn in a low dimensional space the random matrix r needs to be computed only once oﬀ line and remains ﬁxed throughout the tracking process for the sparse matrix r in the computational load is very light as shown by figure we only need to store the nonzero entries in r and the positions of rectangle ﬁlters in an input image corresponding to the nonzero entries in each row of r then v can be eﬃciently computed by using r to sparsely measure the rectangular features which can be eﬃciently computed using the integral image method analysis of low dimensional compressive features as shown in figure each element vi in the low dimensional feature v rn is a linear combination of spatially distributed rectangle features at diﬀerent scales as the coeﬃcients in the measurement matrix can be positive or negative via the compressive features compute the relative intensity diﬀerence in a way similar to the generalized haar like features see also figure the haar like features have been widely used for object detection with demonstrated success the basic types of these haar like features are typically de signed for diﬀerent tasks there often exist a very large number of probability distributions of three diﬀerent features in a low dimensional space the red stair represents the histogram of positive samples while the blue one represents the histogram of negative samples the red and blue lines denote the corresponding estimated distributions by our incremental update method haar like features which make the computational load very heavy this problem is alleviated by boosting algorithms for selecting important features re cently babenko et al adopted the generalized haar like features where each one is a linear combination of randomly generated rectangle features and use online boosting to select a small set of them for object tracking in our work the large set of haar like features are compressively sensed with a very sparse measurement matrix the compressive sensing theories ensure that the extracted features of our algorithm preserve almost all the information of the original im age therefore we can classify the projected features in the compressed domain eﬃciently without curse of dimensionality classiﬁer construction and update for each sample z rm its low dimensional representation is v vn rn with m n we assume all elements in v are independently distributed and model them with a naive bayes classiﬁer figure shows the probability distributions for three dif ferent features of the positive and negative samples cropped from a few frames of a sequence for clarity of presentation it shows that a gaussian distribution with online update using is a good approximation of the features in the projected space the main steps of our algorithm are summarized in algorithm discussion we note that simplicity is the prime characteristic of our algorithm in which the proposed sparse measurement matrix r is independent of any training samples thereby resulting in a very eﬃcient method in addition our algorithm achieves robust performance as discussed below diﬀerence with related work it should be noted that our algorithm is diﬀerent from the recently proposed tracker and compressive sensing tracker first both algorithms are generative models that encode an ob ject sample by sparse representation of templates using minimization thus the training samples cropped from the previous frames are stored and updated but this is not required in our algorithm due to the use of a data independent measurement matrix second our algorithm extracts a linear combination of gen eralized haar like features but these trackers use the holistic templates for sparse representation which are less robust as demonstrated in our experi ments in an orthogonal matching pursuit algorithm is applied to solve the minimization problems third both of these tracking algorithms need to solve numerous time consuming minimization problems but our algorithm is eﬃcient as only matrix multiplications are required random projection vs principal component analysis for visual track ing dimensionality reduction algorithms such as principal component analysis illustration of robustness of our algorithm to ambiguity in detection top row three positive samples the sample in red rectangle is the most correct positive sam ple while other two in yellow rectangles are less correct positive samples bottom row the probability distributions for a feature extracted from positive and negative samples the red markers denote the feature extracted from the most correct pos itive sample while the yellow markers denote the feature extracted from the two less correct positive samples the red and blue stairs as well as lines denote the estimated distributions of positive and negative samples as shown in figure and its variations have been widely used in generative tracking methods these methods need to update the appearance models frequently for robust tracking however these methods are sensitive to occlusion due to the holis tic representation schemes furthermore it is not clear whether the appearance models can be updated correctly with new observations e g without alignment errors to avoid tracking drift in contrast our algorithm does not suﬀer from the problems with online self taught learning approaches as the proposed model with the measurement matrix is data independent it has been shown that for image and text applications favorable results can be achieved by methods with random projection than principal component analysis robustness to ambiguity in detection the tracking by detection methods often encounter the inherent ambiguity problems as shown in figure recently babenko et al introduced multiple instance learning schemes to alleviate the tracking ambiguity problem our algorithm is robust to the ambiguity problem as illustrated in figure while the target appearance changes over time the most correct positive samples e g the sample in the red rectangle in figure are similar in most frames however the less correct positive samples e g samples in yellow rectangles of figure are much more diﬀerent as they involve some background information which vary much more than those within the tar get object thus the distributions for the features extracted from the most correct positive samples are more concentrated than those from the less cor rect positive samples this in turn makes the features from the most correct positive samples much more stable than those from the less correct positive samples e g bottom row in figure the features denoted by red markers are more stable than those denoted by yellow markers thus our algorithm is able to select the most correct positive sample because its probability is larger than k zhang l zhang and m h yang those of the less correct positive samples see the markers in figure in ad dition our measurement matrix is data independent and no noise is introduced by mis aligned samples robustness to occlusion each feature in our algorithm is spatially localized figure which is less sensitive to occlusion than holistic representations simi lar representations e g local binary patterns and generalized haar like fea tures have been shown to be more eﬀective in handling occlusion moreover features are randomly sampled at multiple scales by our algorithm in a way similar to which have demonstrated robust results for dealing with occlusion dimensionality of projected space assume there exist d input points in rm given as well as β and let r rn m be a random matrix projecting data from rm to rn the theoretical bound for the dimension n that satisﬁes the johnson lindenstrauss lemma is n 2β ln d in practice bingham and mannila pointed out that this bound is much higher than that suﬃces to achieve good results on image and text data in their applications the lower bound for n when is but n is suﬃcient to generate good results in our experiments with samples i e d and β the lower bound for n is approximately another bound derived from the restricted isometry property in compressive sensing is much tighter than that from johnson lindenstrauss lemma where n κβ log m β and κ and β are constants for m κ and β it is expected that n we ﬁnd that good results can be obtained when n in our experiments experiments we evaluate our tracking algorithm with state or the art methods on chal lenging sequences among which are publicly available and are our own the animal shaking and soccer sequences are provided in and the box and jumping are from the trackers we compare with are the fragment tracker frag the online adaboost method oab the semi supervised tracker semib the miltrack algorithm the tracker the tld tracker and the struck method we note that the source code of is not available for evaluation and the implementation requires some technical details and parameters not discussed therein it is worth noticing that we use the most challenging sequences from the existing works for fair comparison we use the source or binary codes provided by the authors with tuned parameters for best performance for our compared trackers we either use the tuned pa rameters from the source codes or empirically set them for best results since all of the trackers except for frag involve randomness we run them times and report the average result for each video clip our tracker is implemented in matlab which runs at frames per second fps on a pentium dual core ghz cpu with gb ram the source codes and datasets are available at experimental setup given a target location at the current frame the search radius for drawing positive samples is set to α which generates positive samples the inner and outer radii for the set x ζ β that generates negative samples are set to ζ and β respectively we randomly select negative samples from set x ζ β the search radius for set dγ to detect the object location is set to γ and about samples are generated the dimensionality of projected space is set to n and the learning parameter λ is set to 85 experimental results all of the video frames are in gray scale and we use two metrics to evaluate the proposed algorithm with state of the art trackers the ﬁrst metric is the success rate score area roi area roit t roig roig where roit is the tracking bounding box and roig is the ground truth bounding box if the score is larger than in one frame the tracking result is considered as a success the other is the center location error measured with manually labeled ground truth data table and table show the quantitative results averaged over times as described above we note that although tld tracker is able to relocate on the target during tracking it is easy to lose the target completely for some frames in most of the test sequences thus we only show the center location errors for the sequences that tld can keep track all the time the proposed compressive tracking algorithm achieves the best or second best results in most sequences in terms of both success rate and center location error furthermore our tracker runs faster than all the other algorithms although they except for the tld method and tracker are implemented in c or c which is intrinsically more eﬃcient than matlab figure shows screenshots of some tracking results k zhang l zhang and m h yang table center location error cle in pixels and average frame per second fps bold fonts indicate the best performance while the italic fonts indicate the second best ones the total number of evaluated frames is scale pose and illumination change for the david indoor sequence shown in figure a the illumination and pose of the object both change gradually the miltrack tld and struck methods perform well on this sequence for the shaking sequence shown in figure b when the stage light changes dras tically and the pose of the subject changes rapidly as he performs all the other trackers fail to track the object reliably the proposed tracker is robust to pose and illumination changes as object appearance can be modeled well by random projections based on the johnson lindenstrauss lemma and the classiﬁer with online update is used to separate foreground and background samples moreover the proposed tracker is a discriminative model with local features that has been demonstrated to handle pose variation well e g miltrack the generative subspace tracker e g ivt has been shown to be eﬀective in dealing with large illumination changes while the discriminative tracking method with local features i e miltrack has been demonstrated to handle pose variation ad equately furthermore the features we use are similar to generalized haar like features which have been shown to be robust to scale and orientation change as illustrated in the david indoor sequence in addition our tracker performs well on the sylvester and panda sequences in which the target objects undergo signiﬁcant pose changes see the supplementary material for details occlusion and pose variation the target object in occluded face sequence in figure c undergoes large pose variation and heavy occlusion only the mil track and struck methods as well as the proposed algorithm perform well on this sequence in addition our tracker achieves the best performance in terms of success rate center location error and frame rate the target player in soc cer sequence is heavily occluded by others many times when he is holding up the trophy as shown in figure d in some frames the object is almost fully moreover the object undergoes drastic motion blur and illumination change and all the other trackers lose track of the targets in numerous frames due to drastic scene change it is unlikely that on line appearance models are able to adapt fast and correctly our tracker can handle occlusion and pose variations well as its appearance model is discrimi natively learned from target and background with a data independent measure ment thereby alleviating the inﬂuence from background see also figure furthermore our tracker performs well for objects with non rigid pose variation and camera view change in the bolt sequence figure j because the appear ance model of our tracker is based on local features which are insensitive to non rigid shape deformation out of plane rotation and abrupt motion the object in the kitesurf sequence figure e undergoes acrobat movements with degrees out of plane rotation only the miltrack semib and the proposed trackers perform well on this sequence both our tracker and the miltrack method are designed to handle object location ambiguity in tracking with classiﬁers and discrimina tive features the object in the animal sequence figure f exhibits abrupt k zhang l zhang and m h yang motion both the struck and the proposed methods perform well on this se quence however when the out of plane rotation and abrupt motion both occur in the biker and tiger sequences figure g h all the other algorithms fail to track the target objects well our tracker outperforms the other methods in all the metrics accuracy success rate and speed the kitesurf skiing twin ings girl and tiger sequences all contain out of plane rotation while jumping and box sequences include abrupt motion similarly our tracker performs well in terms of all metrics background clutters the object in the cliﬀ bar sequence changes in scale orientation and the surrounding background has similar texture as the tracker uses a generative appearance model that does not take background in formation into account it is diﬃcult to keep track of the objects correctly the object in the coupon book sequence undergoes signiﬁcant appearance change at the th frame and then the other coupon book appears both the frag and semib methods are distracted to track the other coupon book in fig ure i while our tracker successfully tracks the correct one because the tld tracker relies heavily on the visual information in the ﬁrst frame to re detect the object it also suﬀers from the same problem our algorithm is able to track the right objects accurately in these two sequences because it extracts discriminative features for the most correct positive sample i e the target object online see figure with classiﬁer update for foreground background separation concluding remarks in this paper we proposed a simple yet robust tracking algorithm with an ap pearance model based on non adaptive random projections that preserve the structure of original image space a very sparse measurement matrix was adopted to eﬃciently compress features from the foreground targets and background ones the tracking task was formulated as a binary classiﬁcation problem with online update in the compressed domain our algorithm combines the merits of generative and discriminative appearance models to account for scene changes numerous experiments with state of the art algorithms on challenging sequences demonstrated that the proposed algorithm performs well in terms of accuracy robustness and speed everal salient object detection approaches have been pub lished which have been assessed using diﬀerent evaluation scores and datasets resulting in discrepancy in model comparison this calls for a methodological framework to compare existing models and evaluate their pros and cons we analyze benchmark datasets and scoring techniques and for the ﬁrst time provide a quantitative comparison of state of the art saliency detection models we ﬁnd that some models perform consistently better than the others saliency models that intend to pre dict eye ﬁxations perform lower on segmentation datasets compared to salient object detection algorithms further we propose combined mod els which show that integration of the few best models outperforms all models over other datasets by analyzing the consistency among the best models and among humans for each scene we identify the scenes where models or humans fail to detect the most salient object we highlight the current issues and propose future research directions introduction visual saliency is the ability of a vision system human or machine to select a certain subset of visual information for further processing this mechanism serves as a ﬁlter to select only the interesting information related to current behaviors or tasks to be processed while ignoring irrelevant information recently salient object detection has attracted a lot of interest in computer vision as it provides fast solutions to several complex processes firstly it detects the most salient and attention grabbing object in a scene and then it segments the whole extent of that object the output usually is a map where the intensity of each pixel represents the probability of that pixel belonging to the salient object this problem in its essence is a segmentation problem but slightly diﬀers from the traditional general image segmentation while salient object detection models segment only the salient foreground object from the background general segmentation algorithms partition an image into regions of coherent properties salient object detection methods also diﬀer from other saliency models that aim to predict scene locations where a human observer may ﬁxate since saliency models whether they address segmentation or ﬁxation prediction both generate saliency maps they are interchangeably applicable the value of saliency detection methods lies in their applications in many ﬁelds including object detection and recognition image compression video a fitzgibbon et al eds eccv part ii lncs pp c springer verlag berlin heidelberg salient object detection a benchmark summarization and photo collage a comparison of some image re targeting techniques some based on salient object detection is available at http people csail mit edu mrub retargetme some work has been published on quantitative evaluation of general segmen tation algorithms to the authors best knowledge such attempt for benchmarking salient object segmentation methods has not been reported un fortunately these methods have often been evaluated on diﬀerent datasets which in some cases are small and not easily accessible the lack of published bench marks causes discrepancy in quantitative comparison of competing models not only does a benchmark allow researchers to compare their models with other al gorithms but it also helps identify the chief factors aﬀecting performance this could result in an even faster performance improvement related works here we provide a short summary of the main trends in saliency detection the interested reader can refer to the extensive reviews for more details as a pioneer itti et al derived bottom up visual saliency using center surround diﬀerences across multi scale image features ma and zhang pro posed an alternative local contrast method using a fuzzy growth model harel et al used graph algorithms and a measure of dissimilarity to achieve eﬃcient saliency computation with their graph based visual saliency gbvs model liu et al used conditional random ﬁeld to learn regions of interest us ing three features multi scale contrast center surround histogram and color spatial distribution more recently goferman et al simultaneously modeled local low level clues global considerations visual organization rules and high level features to highlight salient objects along with their contexts zhai and shah deﬁned pixel level saliency by contrast to all other pixels however for eﬃciency they used only luminance information thus ignoring distinctive clues in other channels achanta et al proposed a frequency tuned method that directly deﬁnes pixel saliency using the color diﬀerences from the average image color visual saliency is equated to discrimination in and extended to bottom up mechanism in the pre attentive biological vision spectral com ponents in an image have been explored to detect visual saliency 56 in hou and zhang the gist of the scene is represented with the average fourier envelope and the diﬀerential spectral components are used to extract salient re gions this is replaced by the phase spectrum of the fourier transform in because it is more eﬀective and computationally eﬃcient some researchers in cluding bruce and tsotsos and zhang et al attempted to deﬁne visual saliency based on information theory some others have further used graph cut or grab cub algorithms to reﬁne borders of their saliency maps and count for salient object contours while some methods deﬁne visual saliency in a local way e g itti et al seo gbvs aws and daklein http www wisdom weizmann ac il sim vision a borji d n sihite and l itti some others are based on global rarity of image regions over the entire scene e g aim sun hounips houcvpr and rc object based theories of attention propose that humans attend to objects and high level concepts inspired by these cognitive ﬁndings some models e g judd et al have used object detectors such as faces humans animals and text to detect salient locations some models address saliency detection in the spatio temporal domain by employing motion ﬂicker optical ﬂow e g or interest points learned from the image regions at ﬁxated locations e g since the research in this area is rather new and the few existing models are in their early phases thus we leave their quantitative evaluation for the future recently a new trend called active visual segmentation has emerged with the intention to segment a region that contains a ﬁxation point mishra et al their framework combines monocular cues color intensity texture with stereo and or motion in a cue independent manner similarly siagian and koch also proposed a new approach for active segmentation by combining boundary and region information we consider adding them to promote this new trend the salient object detection benchmark saliency detection models in this work as the initial seed we focus on those models that are easily accessible attained good accuracies or have been highly referred software for some models was already available online for others we contacted their creators for the code the authors then either sent us the source code to compile or the executables some authors however preferred to run their models on our stimuli and hence send us back the saliency maps in order to achieve a thorough model comparison we intend to open an online challenge where modelers could contribute by submitting their results we compare three categories of models those aiming to detect and segment the most salient object in a scene emphasized more here active segmentation approaches and models that address ﬁxation prediction table shows the list of models from the ﬁrst two categories and table shows category datasets we choose benchmark datasets based on the following criteria being widely used having size and stimulus variety and containing diﬀerent biases such as number of annotators number of salient objects and center bias due to specialty of various datasets it is likely that model rankings may diﬀer across datasets hence to come up with a fair comparison it is recommended to run models over several datasets and draw objective conclusions a model is considered to be good if it performs well over almost all datasets fig provides explanation of the datasets used here as well as sample images of the ﬁve smallest and largest objects vfrom each fig shows mean u annotation position m ap u v u v suv averaged over u images and annotated bounding boxes of v subjects suv there is a strong center bias in the single object datasets most probably due to the tendency of photogra phers to frame interesting objects at the image center similarly there are two peaks at the left and the right in the images with two salient objects salient object detection a benchmark table compared salient object detection models checked sorted chronologically abbreviations m matlab c c c s sent saliency maps w and h image width height db shows the datasets that we have results over them jialisal is ap plied to and images of asd and msra respectively max x preserve the aspect ratio while resizing the bigger dimension to x fig a shows entropy of images asd and msra contain more clut tered scenes histogram of normalized object sizes object size image size is plotted in fig b it shows there are few images with large objects in these datasets objects usually range from small to medium size about of the whole image with msra containing larger objects on average fig c shows subject agreement for an image which is deﬁned as where si and sj are annotations of the i th and j th subjects of n subjects above score has the well deﬁned lower bound of when there is no overlap in segmentations of users and upper bound of when they have perfect overlap as fig shows subjects have higher agreement over msra and datasets about of r values are above compared to the sod dataset proposed combined models since diﬀerent models are based on diﬀer ent hypotheses and algorithms it is likely that combining evidences from them may enhance the saliency detection accuracy here we investigate such an idea let p xf represent the probability of an image pixel x being part of the salient foreground object i e normalized saliency map let p xf mi be such evidence from the i th model assuming independence among models i e p xf mi mj p xf mi p xf mj then a naive bayesian evidence accumula tion would be where k is the number of models and z is chosen in a way that the ﬁnal map is a probability density function pdf since a very small value by only a single model suppresses all evidences from the other models in the multiplication case eq we also consider another combination scheme using linear summation where si and sj are annotations of the i th and j th subjects of n subjects above score has the well deﬁned lower bound of when there is no overlap in segmentations of users and upper bound of when they have perfect overlap as fig shows subjects have higher agreement over msra and datasets about of r values are above compared to the sod dataset proposed combined models since diﬀerent models are based on diﬀer ent hypotheses and algorithms it is likely that combining evidences from them may enhance the saliency detection accuracy here we investigate such an idea let p xf represent the probability of an image pixel x being part of the salient foreground object i e normalized saliency map let p xf mi be such evidence from the i th model assuming independence among models experiments and results baseline models we implemented two simple yet powerful baseline models map explained in sec and fig and human inter observer io model which is the aggregated map of annotations by other subjects excluding the one under test for each image the io model provides an upper bound for other models since humans usually agree in annotating the most salient object sample images from the datasets top row shows the ﬁve smallest ob jects and bottom row shows the ﬁve largest objects from each dataset asd this dataset contains 000 images from the msra dataset authors have manually segmented the salient object contour within the user drawn rect angle to obtain binary masks link http ivrgwww epfl ch sim achanta msra this dataset part b of the original dataset includes 000 im ages containing labeled rectangles from nine users drawing a rectangular shape around what they consider the most salient object there is a large variation among images including natural scenes animals indoor outdoor resolution etc link http research microsoft com en us um people jiansun salientobject salient object htm sed this dataset contains two parts the ﬁrst one single object database has images containing only one salient object similar to the asd but in the second one two objects database there are two salient objects in each image images our purpose in employing this dataset is to evaluate the accuracy of models when there is more than one ob ject in a scene is to evaluate accuracy of models over more complex stimuli it is still not clear how the models developed over single object datasets will scale up in more general cases each of one object and two object datasets contain im ages link http www wisdom weizmann ac il sim vision sod this dataset is a collection of salient object boundaries based on the berkeley segmentation dataset bsd seven subjects are asked to choose the salient object in images this dataset con tains many images with several objects making it challenging for models link http elderlab yorku ca sim vida sod index html map model ranks in the middle among ﬁxation prediction models on sod map model works very well right below the best model repeatedly indicating high center bias in this dataset fig using f measure aim consistently ranks above other models itti and pqft rank at the bottom over all datasets the model is based on the same principles of the itti model but uses maxnorm normalization for each feature map ﬁnd the global max m and the average m of all other local maxima then weigh the map by m m please refer to for a full investigation of ﬁxation prediction models models built originally for ﬁxation prediction on average perform lower than models speciﬁcally built to detect and segment the most salient object in a scene best ﬁxation prediction models perform better than poor saliency detection mod els why does performance accuracy of the two categories of approaches diﬀer over segmentation datasets the reason lies on the amount of true positives vs false positives segmentation approaches try to generate white salient regions to capture more of the true positives on the other hand ﬁxation prediction models are very selective and generate few false positives there are not many ﬁxations on the image in a separate study we noticed that ﬁxation prediction mod els perform better than the saliency detection models over eye ﬁxation datasets active segmentation algorithms score consistently below the other two categories the main reason is the dependency of these models on the initial seed which sometimes may not happen on the most salient object due to the spatial outliers in the image accuracy of combined models our combined models using cbsal svo and rc score the highest in many cases supporting our claim in evidence in tegration overall our combined models rank the best in the following order identity log exp and mult over and sod datasets combined models perform lower compared to single object datasets but still outperform many mod els according precision recall curves our models stand on top except this is because of cbsal model perform poorly on these datasets causing the per formance to drop for combined models note that our selection of which models to combine was purely based on the asd dataset and not by over ﬁtting re sults to all datasets it is possible that combining best models over each dataset will outperform all models over that dataset we found that the combination of the best two models cbsal and svo still works as good as slightly below combining the three best models supplement the role of object size it is more challenging to obtain high precision recall for small objects than large objects an algorithm that selects the whole image obtains precision with recall if an object occupies of the image we compare accuracy of models over images of asd for with the smallest and for images with the largest objects fig shows samples on average the object occupied for 38 for of the image area respectively for small and large objects the resulting pr curves for asd and datasets are shown in fig see supplement for other dbs models specially map score higher on large objects io scores higher on large objects thus showing higher subject agreement combined models still perform higher than other models in both cases diﬀerence is more pronounced over small objects good models e g cbsal svo and liuietip still perform well with the exception of svo that shows a noticeable performance drop over large objects fixation prediction model rankings diﬀer over both cases while gbvs is the best over small objects hounips over asd and aim over are the best over large objects map model outperforms all ﬁxation prediction models over large objects the role of annotation consistency to check whether annotation con sistency aﬀects accuracy we selected according to eq most and least consistent images for sed datasets of all datasets and calculated the scores shown in fig as expected io and map models perform very high over most consistent images on average for all other models combined models are at the top in both cases except in the least consistent case easy diﬃcult stimuli for models here we study the easiest and most diﬃcult scenes for models that did well in all cases for each model we sorted the stimuli based on their auc score and chose the top easiest and bottom hardest ﬁve images supplement we noticed that models have many easy and diﬃcult scenes in common fig the top ﬁve stimuli usually have one a borji d n sihite and l itti fig five images with least and most annotation consistency from datasets vivid salient object at the center with a distinctive color from the background the bottom ﬁve stimuli often contain objects in a textured background objects composed of several diﬀerent parts or objects that attract top down attention e g text faces human social interactions gaze direction or animals analysis of map smoothing and center bias here we investigate the role of map smoothing blurring as it has been shown to aﬀect scoring for ﬁx ation prediction in the past we convolve saliency maps of models with variable sized gaussian kernels and calculate the scores we also add sepa rately central gaussian kernels to the saliency maps auc scores are shown in fig with smoothing scores slightly change but qualitative trends and model rankings stay the same hence not aﬀecting our conclusions the reason why smoothing changes ﬁxation prediction but not salient object detection accuracy is because there is uncertainty in ﬁxations such that they often do not land on the exact intended locations and in salient object detection scores are calculated using image regions while in ﬁxation prediction they are calculated by sampling maps from eye positions shown in fig all datasets have center bias similar to the eye movement datasets from fig right side we conclude that adding center bias raises the accuracy of low performing models while it decreases the accuracy of good models however this change in accuracy is not signiﬁcant and does not alter model rankings large convolutional network models have recently demon strated impressive classiﬁcation performance on the imagenet bench mark krizhevsky et al however there is no clear understanding of why they perform so well or how they might be improved in this paper we explore both issues we introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the oper ation of the classiﬁer used in a diagnostic role these visualizations allow us to ﬁnd model architectures that outperform krizhevsky et al on the imagenet classiﬁcation benchmark we also perform an ablation study to discover the performance contribution from diﬀerent model layers we show our imagenet model generalizes well to other datasets when the softmax classiﬁer is retrained it convincingly beats the current state of the art results on caltech and caltech datasets introduction since their introduction by lecun et al in the early convolutional networks convnets have demonstrated excellent performance at tasks such as hand written digit classiﬁcation and face detection in the last months sev eral papers have shown that they can also deliver outstanding performance on more challenging visual classiﬁcation tasks ciresan et al demonstrate state of the art performance on norb and cifar datasets most notably krizhevsky et al show record beating performance on the imagenet classiﬁcation benchmark with their convnet model achieving an error rate of compared to the place result of following on from this work girshick et al have shown leading detection performance on the pascal voc dataset sev eral factors are responsible for this dramatic improvement in performance i the availability of much larger training sets with millions of labeled examples ii powerful gpu implementations making the training of very large models practi cal and iii better model regularization strategies such as dropout despite this encouraging progress there is still little insight into the internal operation and behavior of these complex models or how they achieve such good performance from a scientiﬁc standpoint this is deeply unsatisfactory with out clear understanding of how and why they work the development of better models is reduced to trial and error in this paper we introduce a visualization d fleet et al eds eccv part i lncs pp c springer international publishing switzerland visualizing and understanding convolutional networks technique that reveals the input stimuli that excite individual feature maps at any layer in the model it also allows us to observe the evolution of features during training and to diagnose potential problems with the model the visu alization technique we propose uses a multi layered deconvolutional network deconvnet as proposed by zeiler et al to project the feature activations back to the input pixel space we also perform a sensitivity analysis of the clas siﬁer output by occluding portions of the input image revealing which parts of the scene are important for classiﬁcation using these tools we start with the architecture of krizhevsky et al and explore diﬀerent architectures discovering ones that outperform their results on imagenet we then explore the generalization ability of the model to other datasets just retraining the softmax classiﬁer on top as such this is a form of supervised pre training which contrasts with the unsupervised pre training methods popularized by hinton et al 13 and others 26 related work visualization visualizing features to gain intuition about the network is com mon practice but mostly limited to the layer where projections to pixel space are possible in higher layers alternate methods must be used 8 ﬁnd the optimal stimulus for each unit by performing gradient descent in image space to maximize the unit activation this requires a careful initialization and does not give any information about the unit invariances motivated by the latter short coming extending an idea by show how the hessian of a given unit may be computed numerically around the optimal response giving some insight into invariances the problem is that for higher layers the invariances are extremely complex so are poorly captured by a simple quadratic approximation our approach by contrast provides a non parametric view of invariance show ing which patterns from the training set activate the feature map our approach is similar to contemporary work by simonyan et al who demonstrate how saliency maps can be obtained from a convnet by projecting back from the fully connected layers of the network instead of the convolutional features that we use girshick et al show visualizations that identify patches within a dataset that are responsible for strong activations at higher layers in the model our vi sualizations diﬀer in that they are not just crops of input images but rather top down projections that reveal structures within each patch that stimulate a particular feature map feature generalization our demonstration of the generalization ability of convnet features is also explored in concurrent work by donahue et al and girshick et al they use the convnet features to obtain state of the art performance on caltech and the sun scenes dataset in the former case and for object detection on the pascal voc dataset in the latter approach we use standard fully supervised convnet models throughout the paper as de ﬁned by lecun et al and krizhevsky et al these models map a color m d zeiler and r fergus input image xi via a series of layers to a probability vector yˆi over the c dif ferent classes each layer consists of i convolution of the previous layer output or in the case of the layer the input image with a set of learned ﬁlters ii passing the responses through a rectiﬁed linear function relu x max x iii optionally max pooling over local neighborhoods and iv optionally a local contrast operation that normalizes the responses across feature maps for more details of these operations see and the top few layers of the net work are conventional fully connected networks and the ﬁnal layer is a softmax classiﬁer fig shows the model used in many of our experiments we train these models using a large set of n labeled images x y where label yi is a discrete variable indicating the true class a cross entropy loss function suitable for image classiﬁcation is used to compare yˆi and yi the parameters of the network ﬁlters in the convolutional layers weight matrices in the fully connected layers and biases are trained by back propagating the derivative of the loss with respect to the parameters throughout the network and updating the parameters via stochastic gradient descent details of training are given in section visualization with a deconvnet understanding the operation of a convnet requires interpreting the feature activ ity in intermediate layers we present a novel way to map these activities back to the input pixel space showing what input pattern originally caused a given activation in the feature maps we perform this mapping with a deconvolutional network deconvnet zeiler et al a deconvnet can be thought of as a convnet model that uses the same components ﬁltering pooling but in reverse so instead of mapping pixels to features does the opposite in zeiler et al deconvnets were proposed as a way of performing unsupervised learning here they are not used in any learning capacity just as a probe of an already trained convnet to examine a convnet a deconvnet is attached to each of its layers as illus trated in fig top providing a continuous path back to image pixels to start an input image is presented to the convnet and features computed throughout the layers to examine a given convnet activation we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer then we successively i unpool ii rectify and iii ﬁlter to reconstruct the activity in the layer beneath that gave rise to the chosen activation this is then repeated until input pixel space is reached unpooling in the convnet the max pooling operation is non invertible how ever we can obtain an approximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables in the decon vnet the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations preserving the structure of the stimulus see fig bottom for an illustration of the procedure rectification the convnet uses relu non linearities which rectify the fea ture maps thus ensuring the feature maps are always positive to obtain valid visualizing and understanding convolutional networks feature reconstructions at each layer which also should be positive we pass the reconstructed signal through a relu non filtering the convnet uses learned ﬁlters to convolve the feature maps from the previous layer to approximately invert this the deconvnet uses transposed versions of the same ﬁlters as other autoencoder models such as rbms but applied to the rectiﬁed maps not the output of the layer beneath in practice this means ﬂipping each ﬁlter vertically and horizontally note that we do not use any contrast normalization operations when in this reconstruction path projecting down from higher layers uses the switch settings generated by the max pooling in the convnet on the way up as these switch settings are peculiar to a given input image the reconstruction obtained from a single activation thus resembles a small piece of the original input image with structures weighted according to their contribution toward to the feature acti vation since the model is trained discriminatively they implicitly show which parts of the input image are discriminative note that these projections are not samples from the model since there is no generative process involved the whole procedure is similar to backpropping a single strong activation rather than the h usual gradients i e computing x n where h is the element of the feature map with the strong activation and xn is the input image however it diﬀers in that i the the relu is imposed independently and ii contrast normalization operations are not used a general shortcoming of our approach is that it only visualizes a single activation not the joint activity present in a layer neverthe less as we show in fig these visualizations are accurate representations of the input pattern that stimulates the given feature map in the model when the parts of the original input image corresponding to the pattern are occluded we see a distinct drop in activity within the feature map training details we now describe the large convnet model that will be visualized in section the architecture shown in fig is similar to that used by krizhevsky et al for imagenet classiﬁcation one diﬀerence is that the sparse connections used in krizhevsky layers due to the model being split across gpus are replaced with dense connections in our model other important diﬀerences re lating to layers and were made following inspection of the visualizations in fig as described in section the model was trained on the imagenet training set million images spread over diﬀerent classes each rgb image was preprocessed by resiz ing the smallest dimension to cropping the center region subtract ing the per pixel mean across all images and then using diﬀerent sub crops of size corners center with out horizontal ﬂips stochastic gradient descent with a mini batch size of was used to update the parameters starting with a learning rate of in conjunction with a momentum term of a deconvnet layer left attached to a convnet layer right the deconvnet will reconstruct an approximate version of the convnet features from the layer beneath bottom an illustration of the unpooling operation in the deconvnet using switches which record the location of the local max in each pooling region colored zones during pooling in the convnet the black white bars are negative positive activations within the feature map anneal the learning rate throughout training manually when the validation error plateaus dropout is used in the fully connected layers and with a rate of 0 all weights are initialized to and biases are set to 0 visualization of the ﬁrst layer ﬁlters during training reveals that a few of them dominate to combat this we renormalize each ﬁlter in the convolutional layers whose rms value exceeds a ﬁxed radius of 1 to this ﬁxed radius this is crucial especially in the ﬁrst layer of the model where the input images are roughly in the range as in krizhevsky et al we produce multiple diﬀerent crops and ﬂips of each training example to boost training set size we stopped training after epochs which took around days on a single gpu using an implementation based on convnet visualization using the model described in section we now use the deconvnet to visualize the feature activations on the imagenet validation set feature visualization fig shows feature visualizations from our model once training is complete for a given feature map we show the top acti vations each projected separately down to pixel space revealing the diﬀerent visualizing and understanding convolutional networks structures that excite that map and showing its invariance to input deformations alongside these visualizations we show the corresponding image patches these have greater variation than visualizations which solely focus on the discriminant structure within each patch for example in layer row 1 col the patches appear to have little in common but the visualizations reveal that this particular feature map focuses on the grass in the background not the foreground objects the projections from each layer show the hierarchical nature of the features in the network layer 2 responds to corners and other edge color conjunctions layer has more complex invariances capturing similar textures e g mesh patterns row 1 col 1 text layer shows signiﬁcant variation and is more class speciﬁc dog faces bird legs layer shows entire objects with signiﬁcant pose variation e g keyboards and dogs feature evolution during training fig visualizes the progression during training of the strongest activation across all training examples within a given feature map projected back to pixel space sudden jumps in appearance result from a change in the image from which the strongest activation originates the lower layers of the model can be seen to converge within a few epochs however the upper layers only develop develop after a considerable number of epochs demonstrating the need to let the models train until fully converged 1 architecture selection while visualization of a trained model gives insight into its operation it can also assist with selecting good architectures in the ﬁrst place by visualizing the ﬁrst and second layers of krizhevsky et al architecture fig a c various problems are apparent the ﬁrst layer ﬁlters are a mix of extremely high and low frequency information with little coverage of the mid frequencies additionally the layer visualization shows aliasing artifacts caused by the large stride used in the layer convolutions to remedy these problems we i reduced the layer ﬁlter size from to and ii made the stride of the convolution 2 rather than this new architecture retains much more information in the and layer features as shown in fig b d more importantly it also improves the classiﬁcation performance as shown in section 1 2 occlusion sensitivity with image classiﬁcation approaches a natural question is if the model is truly identifying the location of the object in the image or just using the surrounding context fig attempts to answer this question by systematically occluding diﬀerent portions of the input image with a grey square and monitoring the output of the classiﬁer the examples clearly show the model is localizing the objects within the scene as the probability of the correct class drops signiﬁcantly when the object is occluded fig also shows visualizations from the strongest feature map of the top convolution layer in addition to activity in this map summed over spatial locations as a function of occluder position visualization of features in a fully trained model for layers 2 we show the top 9 activations in a random subset of feature maps across the validation data projected down to pixel space using our deconvolutional network approach our reconstructions are not samples from the model they are reconstructed patterns from the validation set that cause high activations in a given feature map for each feature map we also show the corresponding image patches note i the the strong grouping within each feature map ii greater invariance at higher layers and iii exaggeration of discriminative parts of the image e g eyes and noses of dogs layer row 1 cols 1 best viewed in electronic form the compression artifacts are a consequence of the submission limit not the reconstruction algorithm itself architecture of our 8 layer convnet model a by crop of an image with color planes is presented as the input this is convolved with diﬀerent layer ﬁlters red each of size by using a stride of 2 in both x and y the resulting feature maps are then i passed through a rectiﬁed linear function not shown ii pooled max within regions using stride 2 and iii contrast normalized across feature maps to give diﬀerent by element feature maps similar operations are repeated in layers 2 the last two layers are fully connected taking features from the top convolutional layer as input in vector form 6 6 dimensions the ﬁnal layer is a c way softmax function c being the number of classes all ﬁlters and feature maps are square in shape evolution of a randomly chosen subset of model features through training each layer features are displayed in a diﬀerent block within each block we show a randomly chosen subset of features at epochs 1 2 the visualiza tion shows the strongest activation across all training examples for a given feature map projected down to pixel space using our deconvnet approach color contrast is artiﬁcially enhanced and the ﬁgure is best viewed in electronic form occluder covers the image region that appears in the visualization we see a strong drop in activity in the feature map this shows that the visualization genuinely corresponds to the image structure that stimulates that feature map hence validating the other visualizations shown in fig and fig 2 this dataset consists of 1 3m training validation test examples spread over categories table 1 shows our results on this dataset using the exact architecture speciﬁed in krizhevsky et al we attempt to replicate their result on the validation set we achieve an error rate within 0 1 of their reported value on the imagenet validation set next we analyze the performance of our model with the architectural changes outlined in section this model shown in fig signiﬁcantly outperforms the architecture of krizhevsky et al beating their single model result by 1 test top when we combine multiple models we obtain a test error of 8 an improve ment of 1 6 this result is close to that produced by the data augmentation approaches of howard which could easily be combined with our architec ture however our model is some way short of the winner of the imagenet classiﬁcation competition three test examples where we systematically cover up diﬀerent portions of the scene with a gray square column and see how the top layer feature maps b c and classiﬁer output d e changes b for each position of the gray scale we record the total activation in one layer feature map the one with the strongest response in the unoccluded image c a visualization of this feature map projected down into the input image black square along with visualizations of this map from other images the ﬁrst row example shows the strongest feature to be the dog face when this is covered up the activity in the feature map decreases blue area in b d a map of correct class probability as a function of the position of the gray square e g when the dog face is obscured the probability for pomeranian drops signiﬁcantly e the most probable label as a function of occluder position e g in the row for most locations it is pomeranian but if the dog face is obscured but not the ball then it predicts tennis ball in the example text on the car is the strongest feature in layer but the classiﬁer is most sensitive to the wheel the example contains multiple objects the strongest feature in layer picks out the faces but the classiﬁer is sensitive to the dog blue region in d since it uses multiple feature maps the following we refer to top validation error this is surprising given that they contain the majority of model parameters removing two of the middle convolutional layers also makes a relatively small diﬀerence to the error rate however removing both the middle convolution layers and the fully connected layers yields a model with only layers whose performance is dramatically worse this would suggest that the overall depth of the model is important for obtaining good performance we then modify our model shown in fig changing the size of the fully connected layers makes little diﬀerence to performance same for model of krizhevsky et al however increasing the size of the middle convolution layers goes give a useful gain in performance the experiments above show the importance of the convolutional part of our imagenet model in obtaining state of the art performance this is supported by the visualizations of fig 2 which show the complex invariances learned in the convolutional layers we now explore the ability of these feature extraction layers to generalize to other datasets namely caltech 9 caltech and pascal voc to do this we keep layers 1 of our imagenet trained model ﬁxed and train a new softmax classiﬁer on top for the appropriate num ber of classes using the training images of the new dataset since the softmax contains relatively few parameters it can be trained quickly from a relatively small number of examples as is the case for certain datasets the experiments compare our feature representation obtained from ima genet with the hand crafted features used by other methods in both our ap proach and existing ones the caltech pascal training data is only used to train the classiﬁer as they are of similar complexity ours softmax others lin ear svm the feature representation is crucial to performance it is important to note that both representations were built using images beyond the caltech and pascal training sets for example the hyper parameters in hog descriptors were determined through systematic experiments on a pedestrian dataset 5 we also try a second strategy of training a model from scratch i e resetting layers 1 7 to random values and train them as well as the softmax on the training images of the pascal caltech dataset one complication is that some of the caltech datasets have some images that are also in the imagenet training data using normalized correlation we visualizing and understanding convolutional networks identiﬁed these few overlap and removed them from our imagenet training set and then retrained our imagenet models so avoiding the possibility of train test contamination caltech we follow the procedure of 9 and randomly select or im ages per class for training and test on up to images per class reporting the average of the per class accuracies in table 3 using 5 train test folds training took minutes for images class the pre trained model beats the best re ported result for images class from 3 by 2 2 our result agrees with the recently published result of donahue et al 7 who obtain 1 accuracy imgs class the convnet model trained from scratch however does terribly only achieving 5 showing the impossibility of training a large convnet on such a small dataset we follow the procedure of selecting 30 or training images per class reporting the average of the per class accuracies in table 4 our imagenet pretrained model beats the current state of the art results ob tained by bo et al 3 by a signiﬁcant margin 2 vs 2 for training images class however as with caltech the model trained from scratch does poorly in fig 7 we explore the one shot learning 9 regime with our pre trained model just 6 caltech training images are needed to beat the leading method using 10 times as many images this shows the power of the imagenet feature extractor pascal we used the standard training and validation images to train a way softmax on top of the imagenet pretrained convnet this is not ideal as pascal images can contain multiple objects and our model just provides a single exclusive prediction for each image table 5 shows the results on the test set comparing to the leading methods the top 2 entries in the competition and concurrent work from oquab et al who use a convnet with a more appropriate classiﬁer the pascal and imagenet images are quite diﬀerent in nature the former being full scenes unlike the latter we explored large convolutional neural network models trained for image clas siﬁcation in a number ways first we presented a novel way to visualize the activity within the model this reveals the features to be far from random un interpretable patterns rather they show many intuitively desirable properties such as compositionality increasing invariance and class discrimination as we ascend the layers we also show how these visualization can be used to identify problems with the model and so obtain better results for example improving on krizhevsky et al s impressive imagenet result we then demon strated through a series of occlusion experiments that the model while trained for classiﬁcation is highly sensitive to local structure in the image and is not just using broad scene context an ablation study on the model revealed that having a minimum depth to the network rather than any individual section is vital to the model s performance finally we showed how the imagenet trained model can generalize well to other datasets for caltech and caltech the datasets are similar enough that we can beat the best reported results in the latter case by a signiﬁcant margin our convnet model generalized less well to the pascal data perhaps m d zeiler and r fergus suﬀering from dataset bias although it was still within 3 2 of the best reported result despite no tuning for the task for example our performance might improve if a diﬀerent loss function was used that permitted multiple ob jects per image this would naturally enable the networks to tackle the object detection as well catalogue description advanced topics in computer graphics concentrating on image formation and modelling issues the impli cations of the data driven approach to computer graphics simulation and non parametric methods contrasted the course will involve a project investigating and implementing some current algorithms from the literature learning objectives cmpt exposes students to a comprehensive introduction to the fundamentals of computer graphics pro gramming the learning outcomes for this course include describe the basic graphics pipeline and how forward and backward rendering factor in this create a program to display models of simple graphics images derive linear perspective from similar triangles by converting points x y z to points x z y z obtain dimensional and dimensional points by applying affine transformations apply dimensional coordinate system and the changes required to extend dimensional transformation operations to handle transformations in explain the concept and applications of texture mapping sampling and anti aliasing implement simple procedures that perform transformation and clipping operations on simple images implement a simple real time renderer using the opengl api with vertex buffers and shaders compare and contrast the different rendering techniques represent curves and surfaces using both implicit and parametric forms create simple polyhedral models by surface tessellation and using modeling tools e g blender describe the tradeoffs in different representations of rotations implement the spline interpolation method for producing in between positions and orientations implement basic physics based animation algorithms for particle dynamics using simple newtonian me chanics with euler methods student evaluation grading scheme presentation midterm exam feb in class class project final exam total project the project will involve graphics programming modeling and animation in modern opengl final project videos will be presented in class on tuesday april attendance is mandatory the tentative project schedule is as follows part due jan basic opengl application part due feb camera controls part due feb shading part due march texture mapping part due march creating a model of a campus or city building part due april creating a animation of cityscape presentation students will deliver in class presentations that describe and demonstrate a useful computer graphics tool or software package student presentations will be done in groups of and will take place during a couple of the thursday lecture slots throughout the term attendance is mandatory and students will be marked on their participation in discussions presentations will be prepared in groups of two each group will present for minutes show live demo for minutes and have a minute q a discussion questions from the class presentation groups and topics will be set in the first week of the course criteria that must be met to pass students must achieve an total mark of or greater to pass the course attendance expectation regular attendance is expected attendance is mandatory for all student presentations final exam scheduling the registrar schedules all final examinations including deferred and supplemental exams students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information required text interactive computer graphics edition by edward angel pearson recommended texts opengl programming guide edition by dave shreiner et al addison wesley lecture schedule topic subtopics computer graphics overview images models pipeline architecture c programming review c vs c build processes conventions modern opengl programming opengl pipeline libraries conventions basic shaders linear algebra vectors matrices coordinate systems geometry curves surfaces implicit vs parametric forms polygons transformations points vs vectors homogeneous transformations viewing projections opengl camera control shading lighting models surface normals phong shading texture mapping mapping methods texture coordinates buffers and blending translucency compositing images anti aliasing clipping cohen sutherland bounding boxes hidden surface removal rasterization line drawing polygon fill line scanning environment maps reflection models sphere map cube map shadow maps projective shadows depth maps shadow test parametric animation camera motion splines quaternions physics based animation dynamics particles rigid bodies fluid simulation displays visual cues stereoscopic displays head coupled rendering policies late assignments assignment are due on mondays at late assignments may be submitted by the following thursday before but will be deducted assignments submitted later than this will receive a mark of zero missed assignments missed assignments submitted later than thursday at following the assignment deadline will be given a grade of zero missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible a doctor note is required for misses due to illness arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the appli cation must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the course outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations subsection of the calendar university of saskatchewan calendar academic courses policy academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals subsection of the university secretary website and avoid any behaviour that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offense academic dishonesty is a serious offense and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of com plaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca undergrad honesty php for more information on what academic integrity means for students see the student conduct appeals subsection of the university secretary website at http www usask ca secretariat student conduct appeals forms integritydefined pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to regis ter with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports students must follow dss policy and procedures for more information check http www students usask ca disability or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss catalogue description an introduction to computer graphics that includes real time and off line realistic image synthesis tech niques and basic animation techniques such as key framing and physics based methods programmable rater graphics ray tracing and efficient data structures for both are also introduced learning objectives cmpt exposes students to a comprehensive introduction to the fundamentals of computer graphics pro gramming the learning outcomes for this course include describe the basic graphics pipeline and how forward and backward rendering factor in this create a program to display models of simple graphics images derive linear perspective from similar triangles by converting points x y z to points x z y z obtain dimensional and dimensional points by applying affine transformations apply dimensional coordinate system and the changes required to extend dimensional transformation operations to handle transformations in explain the concept and applications of texture mapping sampling and anti aliasing implement simple procedures that perform transformation and clipping operations on simple images implement a simple real time renderer using the opengl api with vertex buffers and shaders compare and contrast the different rendering techniques represent curves and surfaces using both implicit and parametric forms create simple polyhedral models by surface tessellation and using modeling tools e g blender describe the tradeoffs in different representations of rotations implement the spline interpolation method for producing in between positions and orientations implement basic physics based animation algorithms for particle dynamics using simple newtonian me chanics with euler methods student evaluation grading scheme assignments midterm exam feb in class final exam class project total assignments the assignments will involve graphics programming and modeling if an undergraduate course in com puter graphics has already been taken these assignments can be omitted by permission of the instructor if omitted the of the final grade will be re distributed to the midterm and final exams part due jan basic opengl application part due feb camera controls part due feb shading part due march texture mapping project the project will assigned on an individual basis criteria that must be met to pass students must achieve an total mark of or greater to pass the course attendance expectation regular attendance is expected attendance is mandatory for all student presentations final exam scheduling the registrar schedules all final examinations including deferred and supplemental exams students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information required text interactive computer graphics edition by edward angel pearson recommended texts opengl programming guide edition by dave shreiner et al addison wesley lecture schedule topic subtopics computer graphics overview images models pipeline architecture c programming review c vs c build processes conventions modern opengl programming opengl pipeline libraries conventions basic shaders linear algebra vectors matrices coordinate systems geometry curves surfaces implicit vs parametric forms polygons transformations points vs vectors homogeneous transformations viewing projections opengl camera control shading lighting models surface normals phong shading texture mapping mapping methods texture coordinates buffers and blending translucency compositing images anti aliasing clipping cohen sutherland bounding boxes hidden surface removal rasterization line drawing polygon fill line scanning environment maps reflection models sphere map cube map shadow maps projective shadows depth maps shadow test parametric animation camera motion splines quaternions physics based animation dynamics particles rigid bodies fluid simulation displays visual cues stereoscopic displays head coupled rendering policies late assignments assignment are due on mondays at late assignments may be submitted by the following thursday before but will be deducted assignments submitted later than this will receive a mark of zero missed assignments missed assignments submitted later than thursday at following the assignment deadline will be given a grade of zero missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible a doctor note is required for misses due to illness arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the appli cation must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the course outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations subsection of the calendar university of saskatchewan calendar academic courses policy academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals subsection of the university secretary website and avoid any behaviour that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offense academic dishonesty is a serious offense and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of com plaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca undergrad honesty php for more information on what academic integrity means for students see the student conduct appeals subsection of the university secretary website at http www usask ca secretariat student conduct appeals forms integritydefined pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to regis ter with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports students must follow dss policy and procedures for more information check http www students usask ca disability or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss catalogue description an advanced course in computer graphics concentrating on techniques for synthesizing images and anima tions physical simulation for animation procedural modeling and texture synthesis data driven computer graphics including motion capture image based rendering and model acquisition further alternatives to traditional image formation methods such as non photorealistic rendering and point based rendering course objectives this course exposes students to modeling and simulation techniques for human applications students taking this course will survey different approaches to image synthesis computer graphics survey different approaches to animation synthesis computer animation learn the detailed principles of physics based simulation use modeling tools to create models develop computer programs with existing modeling simulation apis student evaluation grading scheme seminar presentations programming assignment research project total seminar presentation students will select a siggraph course on a topic of interest in consultation with the instructor and present three short min lectures based on topics from that course students are encouraged to use the slides and other examples directly from the course materials but should be well versed in the material to make the concepts easy to understand for the rest of the class presentation jan first topic presentation feb second topic presentation march third topic programming assignments assignment jan modeling in artisynth assignment march programming assignment related to research project research project deliverable feb proposal presentation deliverable feb proposal report deliverable feb technical report deliverable april final presentation deliverable april final report attendance expectation regular attendance is required please inform the instructor if you have to miss a lecture students will also be asked to attend select lectures from cmpt unless previously taken reading list relevant research papers selected from books and conference and journal publications including but not limited to the following venues acm siggraph courses acm siggraph asia soft tissue biomechanical modeling for computer assisted surgery computer methods in biomechanics and biomedical engineering papers and excerpts will be posted to moodle topics topic subtopics computer graphics modeling geometry meshes vertices homogenous coordinates physics based simulation select readings from soft tissue biomechanical modeling for computer assisted surgery on rigid body mechanics fem ap proaches for human tissue modeling human simulation modeling basic principles tools artisynth ansys zygote datasets and more medical imaging different imaging modalities mri ct us vf fmri pet from imaging to models image processing segmentation meshing tools amira slicer turtleseg applications graphics and animation in education medicine entertainment ergonomics seminars select topics taken from acm siggraph courses to be deter mined policies incomplete course work and final grades when a student has not completed the required course work which includes any assignment by the time of submission of the final grades they may be granted an extension to permit completion of an assignment extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the course outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations subsection of the calendar university of saskatchewan calendar academic courses policy academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals subsection of the university secretary website and avoid any behaviour that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offense academic dishonesty is a serious offense and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of com plaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca undergrad honesty php for more information on what academic integrity means for students see the student conduct appeals subsection of the university secretary website at http www usask ca secretariat student conduct appeals forms integritydefined pdf e have completed our discussion of the ﬁrst half of the synthetic camera model specifying objects in three dimensions we now investigate the multi tude of ways in which we can describe our virtual camera along the way we examine related topics such as the relationship between classical viewing techniques and com puter viewing and how projection is implemented using projective transformations there are three parts to our approach first we look at the types of views that we can create and why we need more than one type of view then we examine how an application program can specify a particular view within opengl we will see that the viewing process has two parts in the ﬁrst we use the model view matrix to switch vertex representations from the object frame in which we deﬁned our objects to their representation in the eye frame in which the camera is at the origin this represen tation of the geometry will allow us to use canonical viewing procedures the second part of the process deals with the type of projection we prefer parallel or perspec tive and the part of the world we wish to image the clipping or view volume these speciﬁcations will allow us to form a projection matrix that is concatenated with the model view matrix finally we derive the projection matrices that describe the most important parallel and perspective views and investigate how to carry out these pro jections in opengl classical and computer viewing before looking at the interface between computer graphics systems and application programs for three dimensional viewing we take a slight diversion to consider classi cal viewing there are two reasons for examining classical viewing first many of the jobs that were formerly done by hand drawing such as animation in movies archi tectural rendering drafting and mechanical parts design are now routinely done with the aid of computer graphics practitioners of these ﬁelds need to be able to pro duce classical views such as isometrics elevations and various perspectives and thus must be able to use the computer system to produce such renderings second the relationships between classical and computer viewing show many advantages of and a few difﬁculties with the approach used by most apis cop figure viewing figure movement of the center of projection cop to infinity when we introduced the synthetic camera model in chapter we pointed out the similarities between classical and computer viewing the basic elements in both cases are the same we have objects a viewer projectors and a projection plane figure the projectors meet at the center of projection cop the cop corresponds to the center of the lens in the camera or in the eye and in a computer graphics system it is the origin of the camera frame for perspective views all standard graphics systems follow the model that we described in chapter which is based on geometric optics the projection surface is a plane and the projectors are straight lines this situation is the one we usually encounter and is straightforward to implement especially with our pipeline model both classical and computer graphics allow the viewer to be an inﬁnite distance from the objects note that as we move the cop to inﬁnity the projectors become parallel and the cop can be replaced by a direction of projection dop as shown in figure note also that as the cop moves to inﬁnity we can leave the projection plane ﬁxed and the size of the image remains about the same even though the cop is inﬁnitely far from the objects views with a ﬁnite cop are called perspective views views with a cop at inﬁnity are called parallel views for parallel views the origin of the camera frame usually lies in the projection plane color plates and show a parallel and a perspective rendering respectively these plates illustrate the importance of having both types of view available in appli cations such as architecture in an api that supports both types of viewing the user can switch easily between various viewing modes most modern apis support both parallel and perspective viewing the class of projections produced by these systems is known as planar geometric projections because the projection surface is a plane and the projectors are lines both perspective and parallel projections preserve lines they do not in general preserve angles although the parallel views are the limiting case of perspective viewing both classical and computer viewing usually treat them as separate cases for classical views the techniques that people use to construct the two types by hand are different as anyone who has taken a drafting class surely knows from the computer perspective there are differences in how we specify the two types of views rather than looking at a parallel view as the limit of the perspective view we derive the limiting equations and use those equations directly to form the corre sponding projection matrix in modern pipeline architectures the projection matrix corresponding to either type of view can be loaded into the pipeline although computer graphics systems have two fundamental types of viewing parallel and perspective classical graphics appears to permit a host of different views ranging from multiview orthographic projections to one two and three point perspectives this seeming discrepancy arises in classical graphics as a result of the desire to show a speciﬁc relationship among an object the viewer and the projec tion plane as opposed to the computer graphics approach of complete independence of all speciﬁcations classical viewing when an architect draws an image of a building she knows which side she wishes to display and thus where she should place the viewer in relationship to the building each classical view is determined by a speciﬁc relationship between the objects and the viewer in classical viewing there is the underlying notion of a principal face the types of objects viewed in real world applications such as architecture tend to be composed of a number of planar faces each of which can be thought of as a principal face for a rectangular object such as a building there are natural notions of the front back top bottom right and left faces in addition many real world objects have faces that meet at right angles thus such objects often have three orthogonal directions associated with them figure shows some of the main types of views we start with the most re strictive view for each of the parallel and perspective types and then move to the less restrictive conditions orthographic projections our ﬁrst classical view is the orthographic projection shown in figure in all or thographic or orthogonal views the projectors are perpendicular to the projection plane in a multiview orthographic projection we make multiple projections in front elevation elevation oblique plan oblique isometric one point perspective three point perspective figure classical views figure orthographic projections each case with the projection plane parallel to one of the principal faces of the object usually we use three views such as the front top and right to display the object the reason that we produce multiple views should be clear from figure for a box like object only the faces parallel to the projection plane appear in the image a viewer usually needs more than two views to visualize what an object looks like from its multiview orthographic projections visualization from these images can require skill on the part of the viewer the importance of this type of view is that it preserves both distances and angles and because there is no distortion of either distance or shape multiview orthographic projections are well suited for working drawings axonometric projections if we want to see more principal faces of our box like object in a single view we must remove one of our restrictions in axonometric views the projectors are still figure temple and three multiview orthographic projections projection plane projection plane projection plane a b c figure axonometric projections a construction of trimetric view projections b top view c side view orthogonal to the projection plane as shown in figure but the projection plane can have any orientation with respect to the object if the projection plane is placed symmetrically with respect to the three principal faces that meet at a corner of our rectangular object then we have an isometric view if the projection plane is placed symmetrically with respect to two of the principal faces then the view is dimetric the general case is a trimetric view these views are shown in figure note that in an isometric view a line segment length in the image space is shorter than its length measured in the object space this foreshortening of distances is the same dimetric trimetric isometric figure axonometric views projection plane projection plane projection plane a b c figure oblique view a construction b top view c side view in the three principal directions so we can still make distance measurements in the dimetric view however there are two different foreshortening ratios in the trimetric view there are three also although parallel lines are preserved in the image angles are not a circle is projected into an ellipse this distortion is the price we pay for the ability to see more than one principal face in a view that can be produced easily either by hand or by computer axonometric views are used extensively in architectural and mechanical design oblique projections the oblique views are the most general parallel views we obtain an oblique projec tion by allowing the projectors to make an arbitrary angle with the projection plane as shown in figure consequently angles in planes parallel to the projection plane are preserved a circle in a plane parallel to the projection plane is projected into a cir cle yet we can see more than one principal face of the object oblique views are the most difﬁcult to construct by hand they are also somewhat unnatural most physi cal viewing devices including the human visual system have a lens that is in a ﬁxed relationship with the image plane usually the lens is parallel to the plane although these devices produce perspective views if the viewer is far from the object the views are approximately parallel but orthogonal because the projection plane is parallel to the lens the bellows camera that we used to develop the synthetic camera model in section has the ﬂexibility to produce approximations to parallel oblique views one use of such a camera is to create images of buildings in which the sides of the building are parallel rather than converging as they would be in an image created with an orthogonal view with the camera on the ground from the application programmer point of view there is no signiﬁcant differ ence among the different parallel views the application programmer speciﬁes a type of view parallel or perspective and a set of parameters that describe the camera the problem for the application programmer is how to specify these parameters in the viewing procedures so as best to view an object or to produce a speciﬁc classical view perspective viewing all perspective views are characterized by diminution of size when objects are moved farther from the viewer their images become smaller this size change gives perspective views their natural appearance however because the amount by which a line is foreshortened depends on how far the line is from the viewer we cannot make measurements from a perspective view hence the major use of perspective views is in applications such as architecture and animation where it is important to achieve natural looking images in the classical perspective views the viewer is located symmetrically with respect to the projection plane as shown in figure thus the pyramid determined by the window in the projection plane and the center of projection is a symmetric or right figure perspective viewing a b c figure classical perspective views a three point b two point c one point pyramid this symmetry is caused by the ﬁxed relationship between the back retina and lens of the eye for human viewing or between the back and lens of a camera for standard cameras and by similar ﬁxed relationships in most physical situations some cameras such as the bellows camera have movable ﬁlm backs and can produce general perspective views the model used in computer graphics includes this general case the classical perspective views are usually known as one two and three point perspectives the differences among the three cases are based on how many of the three principal directions in the object are parallel to the projection plane consider the three perspective projections of the building shown in figure any corner of the building includes the three principal directions in the most general case the three point perspective parallel lines in each of the three principal directions converges to a ﬁnite vanishing point figure a if we allow one of the principal directions to become parallel to the projection plane we have a two point projection figure b in which lines in only two of the principal directions converge finally in the one point perspective figure c two of the principal directions are parallel to the projection plane and we have only a single vanishing point as with parallel viewing it should be apparent from the programmer point of view that the three situations are merely special cases of general perspective viewing which we implement in section viewing with a computer we can now return to three dimensional graphics from a computer perspective be cause viewing in computer graphics is based on the synthetic camera model we should be able to construct any of the classical views however there is a fundamen tal difference all the classical views are based on a particular relationship among the objects the viewer and the projectors in computer graphics we stress the indepen dence of the object speciﬁcations and camera parameters hence to create one of the classical views the application program must use information about the objects to create and place the proper camera using opengl we will have many options on how and where we carry out viewing all our approaches will use the powerful transformation capabilities of the gpu because every transformation is equivalent to a change of frames we can develop viewing in terms of the frames and coordinate systems we introduced in chapter in particular we will work with object coordinates camera coordinates and clip coordinates a good starting point is the output of the vertex shader in chapters and we used the fact that as long as the vertices output by the vertex shader were within the clipping volume they continued onto the rasterizer hence in chapter we were able to specify vertex positions inside the default viewing cube in chapter we learned how to scale positions using afﬁne transformations so they would be mapped inside the cube we also relied on the fact that objects that are sent to the rasterizer are projected with a simple orthographic projection hidden surface removal however occurs after the fragment shader conse quently although an object might be blocked from the camera by other objects even with hidden surface removal enabled the rasterizer will still generate fragments for blocked objects within the clipping volume however we need more ﬂexibility in both how we specify objects and how we view them there are four major additions to address we need the ability to work in the units of the application we need to position the camera independently of the objects we want to be able to specify a clipping volume in units related to the appli cation we want to be able to do either parallel or perspective projections we can accomplish all these additions by careful use of transformations the ﬁrst three using afﬁne transformations and the last using a process called perspective normal ization all of these transformations must be carried out either in the application code or in the vertex shader we approach all these tasks through the transformation capabilities we devel oped in chapter of the frames that are used in opengl three are important in the viewing process the object frame the camera frame and the clip coordinate frame in chapters and we were able to avoid explicitly specifying the ﬁrst two by using a default in which all three frames were identical we either directly speciﬁed vertex po sitions in clip coordinates or used an afﬁne transformation to scale objects we wanted to be visible to lie within the clipping cube in clip coordinates the camera was ﬁxed to be at the origin and pointing in the negative z direction in clip coordinates to get a more ﬂexible way to do viewing we will separate the process into two fundamental operations first we must position and orient the camera this oper ation is the job of the model view transformation after vertices pass through this the default camera can see objects behind it if they are in the clipping volume object coordinates vertices camera coordinates clip coordinates vertices figure viewing transformations transformation they will be represented in eye or camera coordinates the second step is the application of the projection transformation this step applies the speciﬁed projection orthographic or perspective to the vertices and puts objects within the speciﬁed clipping volume into the same clipping cube in clip coordinates one of the functions of either projection will be to allow us to specify a view volume in camera coordinates rather than having to scale our object to ﬁt into the default view volume these transformations are shown in figure what we have called the current transformation matrix will be the product of two matrices the model view matrix and the projection matrix the model view matrix will take vertices in object coordinates and convert them to a representation in camera coordinates and thus must encapsulate the positioning and orientation of the camera the projection matrix will both carry out the desired projection either orthogonal or perspective and convert a viewing volume speciﬁed in camera coordinates to ﬁt inside the viewing cube in clip coordinates positioning of the camera in this section we deal with positioning and orientation of the camera in section we discuss how we specify the desired projection although we will focus on an api that will work well with opengl we also will examine brieﬂy a few other apis to specify a camera y positioning of the camera frame as we saw in chapter we can specify vertices in any units we choose and we can deﬁne a model view matrix by a sequence of afﬁne transformations that repositions these vertices the model view transformation is the concatenation of a modeling x transformation that takes instances of objects in object coordinates and brings them into the world frame the second part transforms world coordinates to eye coordi nates because we usually do not need to access world coordinates we can use the model view matrix rather than separate modeling and viewing matrices z initially we set the model view matrix to an identity matrix so the camera frame and the object frame are identical hence the camera is initially pointing in figure initial camera position the negative z direction figure in most applications we model our objects as being located around the origin so a camera located at the default position with the default orientation does not see all the objects in the scene thus either we must y y yc x x xc z zc a b figure movement of the camera and object frames a initial configuration b configuration after change in the model view matrix move the camera away from the objects that we wish to have in our image or the objects must be moved in front of the camera these are equivalent operations as either can be looked at as positioning the frame of the camera with respect to the frame of the objects it might help to think of a scene in which we have initially speciﬁed several ob jects by specifying all vertices and putting their positions into an array we start with the model view matrix set to an identity matrix changes to the model view matrix move the object frame relative to the camera and affect the camera view of all ob jects deﬁned afterward because their vertices are speciﬁed relative to the repositioned object frame equivalently in terms of the ﬂow of an application program the pro jection and model view matrices are part of its state we will either apply them to the vertex positions in the application or more likely we will send them to the vertex shader where they will be applied automatically whenever vertex data is sent to the shader in either case the sequence illustrated in figure shows the process in part a we have the initial conﬁguration a vertex speciﬁed at p has the same represen tation in both frames in part b we have changed the model view matrix to c by a sequence of transformations the two frames are no longer the same although c contains the information to move from the camera frame to the object frame or equivalently contains the information that moves the camera away from its initial position at the origin of the object frame a vertex speciﬁed at q after the change to the model view matrix is at q in the object frame however its position in the camera frame is cq and can be stored internally within the application or sent to the gpu where it will be converted to camera coordinates the viewing transformation will assume that vertex data it starts with is in camera coordinates an equivalent view is that the camera is still at the origin of its own frame and the model view matrix is applied to primitives speciﬁed in this system in practice you can use either view but be sure to take great care regarding where in your program the primitives are speciﬁed relative to changes in the model view matrix at any given time the model view matrix encapsulates the relationship between the camera frame and the object frame although combining the modeling and view ing transformations into a single matrix may initially cause confusion on closer ex amination this approach is a good one if we regard the camera as an object with geometric properties then transformations that alter the position and orientation of objects should also affect the position and orientation of the camera relative to these objects the next problem is how we specify the desired position of the camera and then implement camera positioning in opengl we outline three approaches one in this section and two in section two others are given as exercises exercises and our ﬁrst approach is to specify the position indirectly by applying a sequence of rotations and translations to the model view matrix this approach is a direct application of the instance transformation that we presented in chapter but we must be careful for two reasons first we usually want to specify the camera position and orientation before we position any objects in the scene second the order of transformations on the camera may appear to be backward from what you might expect consider an object centered at the origin the camera is in its initial position also at the origin pointing down the negative z axis suppose that we want an image of the faces of the object that point in the positive z direction we must move the camera away from the origin if we allow the camera to remain pointing in the negative z direction then we want to move the camera backward along the positive z axis and the proper transformation is t where d is a positive number many people ﬁnd it helpful to interpret this operation as moving the camera frame relative to the object frame this point of view has a basis in classical viewing in computer graphics we usually think of objects as being positioned in a ﬁxed frame in an animation where in the program we specify the position of the camera depends on whether we wish to attach the camera to a particular object or to place the camera in a ﬁxed position in the scene see exercise and it is the viewer who must move to the right position to achieve the desired view in classical viewing the viewer dominates conceptually we do viewing by picking up the object orienting it as desired and bringing it to the desired location one con sequence of the classical approach is that distances are measured from the viewer to the object rather than as in most physically based systems from the object to the viewer classical viewing often resulted in a left handed camera frame early graphics systems followed the classical approach by having modeling in right handed coordi y nates and viewing in left handed coordinates a decision that although technically correct caused confusion among users when we are working in camera coordinates we will measure distances from the camera which is consistent with classical viewing in opengl the internal frames are right handed fortunately because the applica tion program works primarily in object coordinates the application programmer usually does not see any of the internal representations and thus does not have to worry about these alternate perspectives on viewing suppose that we want to look at the same object from the positive x axis now not only do we have to move away from the object but we also have to rotate the z camera about the y axis as shown in figure we must do the translation after we rotate the camera by degrees about the y axis in the program the calls must be in the reverse order as we discussed in section so we expect to see code like the following translate d rotatex in terms of the two frames ﬁrst we rotate the object frame relative to the camera frame and then we move the two frames apart in chapters and we were able to show simple three dimensional examples by using an identity matrix as the default projection matrix that default setting has the effect of creating an orthographic projection with the camera at the origin pointed in the negative z direction in our cube example in chapter we rotated the cube to see the desired faces as we just discussed rotating the cube is equivalent to rotating the frame of the cube with respect to the frame of the camera we could have achieved the same view by rotating the camera relative to the cube we can extend this strategy of translating and rotating the camera to create other orthographic views perspective views require changes to the default projection consider creating an isometric view of the cube suppose that again we start with a cube centered at the origin and aligned with the axes because the default camera is in the middle of the cube we want to move the camera away from the cube by a translation we obtain an isometric view when the camera is located symmetrically with respect to three adjacent faces of the cube for example anywhere along the line from the origin through the point we can move the cube away from the camera and then rotate the cube to achieve the desired view or equivalently move the camera away from the cube and then rotate it to point at the cube starting with the default camera suppose that we are now looking at the cube from somewhere on the positive z axis we can obtain one of the eight isometric figure positioning of the camera y y x z a b figure cube after rotation about x axis a view from positive z axis b view from positive y axis views there is one for each vertex by ﬁrst rotating the cube about the x axis until we see the two faces symmetrically as shown in figure a clearly we obtain this view by rotating the cube by degrees the second rotation is about the y axis we rotate the cube until we get the desired isometric the required angle of rotation is degrees about the y axis this second angle of rotation may not seem obvious consider what happens to the cube after the ﬁrst rotation from our position on the positive z axis the cube appears as shown in fig ure a the original corner vertex at has been transformed to if we look at the cube from the x axis as in figure b we see that we want to rotate the right ve rtex to the y axis the right triangle that determines this angle has sides of and which correspond to an angle of degrees however we need a clockwise rotation so the angle must be negative finally we move the camera away from the origin thus our strategy is ﬁrst to rotate the frame of the camera relative to the frame of the object and then to separate the two frames the model view matrix is of the form m trxry we obtain this model view matrix for an isometric by multiplying the matrices in homogeneous coordinates the concatenation of the rotation matrices yields r r r it is si mple to verify that the original vertex is correctly transformed to by this matrix if we concatenate in the translation by d the matrix becomes tr d in opengl the code for setting the model view matrix is as follows translate d rotatex rotatey we have gone from a representation of our objects in object coordinates to one in camera coordinates rotation and translation do not affect the size of an object nor equivalently the size of its orthographic projection however these transforma tions can affect whether or not objects are clipped because the clipping volume is measured relative to the camera if for example we translate the object away from the camera it may no longer lie within the clipping volume hence even though the projection of the object is unchanged and the camera still points at it the object would not be in the image two viewing apis the construction of the model view matrix for an isometric view is a little unsatis fying although the approach was intuitive an interface that requires us to compute the individual angles before specifying the transformations is a poor one for an ap plication program we can take a different approach to positioning the camera an approach that is similar to that used by phigs one of the original standard apis for three dimensional graphics our starting point is again the object frame we describe the camera position and orientation in this frame the precise type of image that we wish to obtain perspective or parallel is determined separately by the speciﬁ cation of the projection matrix this second part of the viewing process is often called the normalization transformation we approach this problem as one of a change in frames again we think of the camera as positioned initially at the origin pointed in the negative z direction its desired location is centered at a point called the view vrp n vup v u reference point vrp figure whose position is given in the object frame the user executes a function such as x y z to specify this position next we want to specify the orientation of the camera we can divide this speciﬁcation into two parts speciﬁcation of the view plane normal vpn and speciﬁcation of the view up vector vup the vpn n in figure gives the orientation of the projection plane or back of the camera the orientation figure camera frame of a plane is determined by that plane normal and thus part of the api is a function such as nx ny nz the orientation of the plane does not specify what direction is up from the camera perspective given only the vpn we can rotate the camera with its back in this plane the speciﬁcation of the vup ﬁxes the camera and is performed by a function such as vup_z figure determination of the view up vector we project the vup vector on the view plane to obtain the up direction vector v figure use of the projection allows the user to specify any vector not parallel to v rather than being forced to compute a vector lying in the projection plane the vector v is orthogonal to n we can use the cross product to obtain a third orthogonal direction u this new orthogonal coordinate system usually is referred to as either the viewing coordinate system or the u v n system with the addition of the vrp we have the desired camera frame the matrix that does the change of frames is the view orientation matrix and is equivalent to the viewing component of the model view matrix we can derive this matrix using rotations and translations in homogeneous coordinates we start with the speciﬁcations of the view reference point x p z the view plane normal nx ny z and the view up vector vupx v up vup we construct a new frame with the view reference point as its origin the view plane normal as one coordinate direction and two other orthogonal directions that we call u and v our default is that the original x y z axes become u v n re spectively the view reference point can be handled through a simple translation t x y z from the viewing frame to the original origin the rest of the model view matrix is determined by a rotation so that the model view matrix v is of the form v tr the direction v must be orthogonal to n hence n v figure shows that v is the projection of vup into the plane formed by n and vup and thus must be a linear combination of these two vectors v αn βvup if we temporarily ignore the length of the vectors then we can set β and solve for vup n α n n and v vup vup n n n n we can ﬁnd the third orthogonal direction by taking the cross product u v n these vectors do not generally have unit length we can normalize each indepen dently obtaining three unit length vectors ut vt and nt the matrix uxt vxt ntx a uyt uzt vyt vzt nty ntz is a rotation matrix that orients a vector in the utvtnt system with respect to the original system however we really want to go in the opposite direction to obtain the representation of vectors in the original system in the utvtnt system we want a but because a is a rotation matrix the desired matrix r is r a at finally multiplying by the translation matrix t we have uxt vxt v rt nt uyt vyt nt uzt vzt nt xuxt yuyt zuzt xvxt yvyt zvzt xnt ynt znt note that in this case the translation matrix is on the right whereas in our ﬁrst derivation it was on the left one way to interpret this difference is that in our ﬁrst derivation we ﬁrst rotated one of the frames and then pushed the frames apart in a direction represented in the camera frame in the second derivation the camera position was speciﬁed in the object frame another way to understand this difference is to note that the matrices rt and tr have similar forms the rotation parts of the product the upper left submatrices are identical as are the bottom rows the top three elements in the right column differ because the frame of the rotation affects the translation coefﬁcients in rt and does not affect them in tr for our isometric example n vup the camera position must be along a diagonal in the original frame if we use d p d we obtain the same model view matrix that we derived in section the look at function the use of the vrp vpn and vup is but one way to provide an api for specifying the position of a camera in many situations a more direct method is appropriate consider the situation illustrated in figure here a camera is located at a point e called the eye point speciﬁed in the object frame and it is pointed at a second point a called the at point these points determine a vpn and a vrp the vpn is given by upx upy upz y atx aty atz x z eyex eyey eyez figure look at positioning the vector formed by point subtraction between the eyepoint and the at point vpn a e and normalizing it n vpn vpn the view reference point is the eye point hence we need only to add the desired up direction for the camera and a function to construct the desired matrix lookat could be of the lookat eye at up or the equivalent form lookat glfloat eyex glfloat eyey glfloat eyez glfloat atx glfloat aty glfloat atz glfloat upx glfloat upy glfloat upz note that once we have computed the vector vpn we can proceed as we did with forming the transformation in the previous section a slightly simpler computation would be to form a vector perpendicular to n and vup by taking their cross product and normalizing it u vup n vup n because we are working in homogeneous coordinates vup can be a type if the fourth component is a zero y y y x x x z z z roll pitch yaw figure roll pitch and yaw finally we get the normalized projection of the up vector onto the camera plane by taking a second cross product v n u n u note that we can use the standard rotations translations and scalings as part of deﬁning our objects although these transformations will also alter the model view matrix it is often helpful conceptually to consider the use of lookat as positioning the objects and subsequent operations that affect the model view matrix as position ing the camera note that whereas functions such as lookat that position the camera alter the model view matrix and are speciﬁed in object coordinates the functions that we introduce to form the projection matrix will be speciﬁed in eye coordinates figure elevation and azimuth other viewing apis in many applications neither of the viewing interfaces that we have presented is appropriate consider a ﬂight simulation application the pilot using the simulator usually uses three angles roll pitch and yaw to specify her orientation these angles are speciﬁed relative to the center of mass of the vehicle and to a coordinate system aligned along the axes of the vehicle as shown in figure hence the pilot sees an object in terms of the three angles and of the distance from the object to the center of mass of her vehicle a viewing transformation can be constructed exercise from these speciﬁcations from a translation and three simple rotations viewing in many applications is most naturally speciﬁed in polar rather than rectilinear coordinates applications involving objects that rotate about other ob jects ﬁt this category for example consider the speciﬁcation of a star in the sky its direction from a viewer is given by its elevation and azimuth figure the eleva tion is the angle above the plane of the viewer at which the star appears by deﬁning a normal at the point that the viewer is located and using this normal to deﬁne a plane we deﬁne the elevation regardless of whether or not the viewer is actually standing on a plane we can form two other axes in this plane creating a viewing coordinate system the azimuth is the angle measured from an axis in this plane to the projec tion onto the plane of the line between the viewer and the star the camera can still be rotated about the direction it is pointed by a twist angle parallel projections a parallel projection is the limit of a perspective projection in which the center of projection is inﬁnitely far from the objects being viewed resulting in projectors that are parallel rather than converging at the center of projection equivalently a parallel projection is what we would get if we had a telephoto lens with an inﬁnite focal length rather than ﬁrst deriving the equations for a perspective projection and computing their limiting behavior we will derive the equations for parallel projections directly using the fact that we know in advance that the projectors are parallel and point in a y direction of projection orthogonal projections orthogonal or orthographic projections are a special case of parallel projections in which the projectors are perpendicular to the view plane in terms of a camera orthogonal projections correspond to a camera with a back plane parallel to the lens z x y z x which has an inﬁnite focal length figure shows an orthogonal projection with the projection plane z as points are projected into this plane they retain their x and y values and the equations of projection are xp x yp y zp we can write this result using our original homogeneous coordinates figure orthogonal projection xp x zp z to prepare ourselves for a more general orthogonal projection we can write this expression as q mip where x z i identity matrix and m the projection described by m is carried out by the hardware after the vertex shader hence only those objects inside the cube of side length centered at the origin will be projected and possibly visible if we want to change which objects are visible we can replace the identity matrix by a transformation n that we can carry out either in the application or in the vertex shader which will give us control over the clipping volume for example if we replace i with a scaling matrix we can see more or fewer objects parallel viewing with opengl we will focus on a single orthogonal viewing function in which the view volume is a right parallelepiped as shown in figure the sides of the clipping volume are the four planes x right x left y top y bottom the near front clipping plane is located a distance near from the origin and the far back clipping plane is at a distance far from the origin all these values are in camera coordinates we will derive a function ortho glfloat left glfloat right glfloat bottom glfloat top glfloat near glfloat far which will form the projection matrix n although mathematically we get a parallel view by moving the camera to inﬁn ity because the projectors are parallel we can slide this camera in the direction of projection without changing the projection consequently it is helpful to think of an orthogonal camera located initially at the origin in camera coordinates with the view volume determined by users of microsoft windows may have to change the identiﬁers near and far because far is a reserved word in visual c y right top far z far view volume z near x left bottom near z figure orthographic viewing x y z as the default behavior equivalently we are applying an identity projection matrix for n we will derive a nonidentity matrix n using translation and scaling that will transform vertices in camera coordinates to ﬁt inside the default view volume a process called projection normalization this matrix is what will be produced by ortho note that we are forced to take this approach because the ﬁnal projection carried out by the gpu is ﬁxed nevertheless the normalization process is efﬁcient and will allow us to carry out parallel and perspective projections with the same pipeline projection normalization when we introduced projection in chapter and looked at classical projection earlier in this chapter we viewed it as a technique that took the speciﬁcation of points in three dimensions and mapped them to points on a two dimensional projection surface such a transformation is not invertible because all points along a projector map into the same point on the projection surface in computer graphics systems we adopt a slightly different approach first we work in four dimensions using homogeneous coordinates second we retain depth information distance along a projector as long as possible so that we can do hidden surface removal later in the pipeline third we use projection normalization to convert all projections into orthogonal projections by ﬁrst distorting the objects such that the orthogonal projection of the distorted objects is the same as the de sired projection of the original objects this technique is shown in figure the concatenation of the normalization matrix which carries out the distortion and the a b figure predistortion of objects a perspective view b ortho graphic projection of distorted object figure normalization transformation simple orthogonal projection matrix from section as shown in figure yields a homogeneous coordinate matrix that produces the desired projection one advantage of this approach is that we can design the normalization matrix so that view volume is distorted into the canonical view volume which is the cube deﬁned by the planes x y z besides the advantage of having both perspective and parallel views supported by the same pipeline by loading in the proper normalization matrix the canonical view vol ume simpliﬁes the clipping process because the sides are aligned with the coordinate axes the normalization process deﬁnes what most systems call the projection matrix the projection matrix brings objects into four dimensional clip coordinates and the subsequent perspective division converts vertices to a representation in three dimensional normalized device coordinates values in normalized device coordinates are later mapped to window coordinates by the viewport transformation here we are concerned with the ﬁrst step deriving the projection matrix orthogonal projection matrices although parallel viewing is a special case of perspective viewing we start with or thogonal parallel viewing and later extend the normalization technique to perspective viewing in opengl the default projection matrix is an identity matrix or equivalently what we would get from the following code n ortho the view volume is in fact the canonical view volume points within the cube deﬁned by the sides x y and z are mapped to the same cube points outside this cube remain outside the cube as trivial as this observation may seem it indicates that we can get the desired projection matrix for the general orthogonal view by ﬁnding a matrix that maps the right parallelepiped speciﬁed by ortho to this same cube before we do so recall that the last two parameters in ortho are distances to the near and far planes measured from a camera at the origin pointed in the negative z direction thus the near plane is at z which is behind the camera and the far plane is at z which is in front of the camera although the projectors are parallel and an orthographic projection is conceptually akin to having a camera with a long telephoto lens located far from the objects the importance of the near and far distances in ortho is that they determine which objects are clipped out now suppose that instead we set the ortho parameters by the following func tion call n ortho left right bottom top near far we now have speciﬁed a right parallelepiped view volume whose right side relative to the camera is the plane x left whose left side is the plane x right whose top is the plane y top and whose bottom is the plane y bottom the front is the near clipping plane z near and the back is the far clipping plane z far the pro jection matrix that opengl sets up is the matrix that transforms this volume to the cube centered at the origin with sides of length which is shown in figure this matrix converts the vertices that specify our objects to vertices within this canonical view volume by scaling and translating them consequently vertices are transformed such that vertices within the speciﬁed view volume are transformed to vertices within the canonical view volume and vertices outside the speciﬁed view volume are trans formed to vertices outside the canonical view volume putting everything together we see that the projection matrix is determined by the type of view and the view vol ume speciﬁed in ortho and that these speciﬁcations are relative to the camera the positioning and orientation of the camera are determined by the model view matrix these two matrices are concatenated together and objects have their vertices trans formed by this matrix product we can use our knowledge of afﬁne transformations to ﬁnd this projection ma trix there are two tasks that we need to do first we must move the center of the right top far left bottom near figure mapping a view volume to the canonical view volume figure affine transformations for normalization speciﬁed view volume to the center of the canonical view volume the origin by do ing a translation second we must scale the sides of the speciﬁed view volume to each have a length of see figure hence the two transformations are t t right left top bottom far near and s s right left top bottom near far and they can be concatenated together figure to form the projection matrix right left left right right left top bottom n st top bottom top bottom this matrix maps the near clipping plane z near to the plane z and the far clipping plane z far to the plane z because the camera is pointing in the negative z direction the projectors are directed from inﬁnity on the negative z axis toward the origin oblique projections using ortho we have only a limited class of parallel projections namely only those for which the projectors are orthogonal to the projection plane as we saw earlier in y object back clipping plane front clipping plane dop projection plane x z figure oblique clipping volume this chapter oblique parallel projections are useful in many ﬁelds we could develop an oblique projection matrix directly instead however we follow the process that we used for the general orthogonal projection we convert the desired projection to a canonical orthogonal projection of distorted objects an oblique projection can be characterized by the angle that the projectors make with the projection plane as shown in figure in apis that support general par allel viewing the view volume for an oblique projection has the near and far clipping planes parallel to the view plane and the right left top and bottom planes parallel to the direction of projection as shown in figure we can derive the equations for oblique projections by considering the top and side views in figure which shows a projector and the projection plane z the angles θ and φ characterize the degree of obliqueness in drafting projections such as the cavalier and cabinet pro jections are determined by speciﬁc values of these angles however these angles are not the only possible interface see exercises and if we consider the top view we can ﬁnd xp by noting that tan θ z xp x and thus xp x z cot θ likewise yp y z cot φ x z figure oblique projection note that without oblique projections we cannot draw coordinate axes in the way that we have been doing in this book see exercise x z z y x z a b figure oblique projection a top view b side view using the equation for the projection plane zp we can write these results in terms of a homogeneous coordinate matrix p following our strategy of the previous example we can break p into the product p morthh θ φ where h θ φ is a shearing matrix thus we can implement an oblique projection by ﬁrst doing a shear of the objects by h θ φ and then doing an orthographic projection figure shows the effect of h θ φ on an object a cube inside an oblique view volume the sides of the clipping volume become orthogonal to the view plane but the sides of the cube become oblique as they are affected by the same shear transformation however the orthographic projection of the distorted cube is identical to the oblique projection of the undistorted cube we are not ﬁnished because the view volume created by the shear is not our canonical view volume we have to apply the same scaling and translation matrices that we used in section hence the transformation y z figure effect of shear transformation right left right left right left top bottom st top bottom top bottom must be inserted after the shear and before the ﬁnal orthographic projection so the ﬁnal matrix is n morthsth the values of left right bottom and top are the vertices of the right parallelepiped view volume created by the shear these values depend on how the sides of the original view volume are communicated through the application program they may have to be determined from the results of the shear to the corners of the original view volume one way to do this calculation is shown in figure the speciﬁcation for an oblique projection can be through the angles θ and ψ that projectors make with the projection plane the parameters near and far are not changed by the shear however the x and y values of where the sides of the view volume intersect the near plane are changed by the shear and become left right top and bottom if these points of intersection are xmin near xmax near ymin near and ymax near then our derivation of shear in chapter yields the relationships left xmin near cot θ right xmax near cot θ top ymax near cot φ bottom ymin near cot φ an interactive viewer in this section we extend the rotating cube program to include both the model view matrix and an orthogonal projection matrix whose parameters can be set interac tively as in our previous examples with the cube we have choices as to where to apply our transformations in this example we will send the model view and pro jection matrices to the vertex shader because the model view matrix can be used to both transform an object and position the camera in this example we will not use the mouse function and instead focus on camera position and the orthogonal projection it will be straightforward to bring back the mouse and idle callbacks later to restart the rotation of the cube the colored cube is centered at the origin in object coordinates so wherever we place the camera the at point is at the origin let position the camera in polar coordinates so the eye point has coordinates r cos θ eye r sin θ cos φ r sin θ sin φ where the radius r is the distance from the origin we can let the up direction be the y direction in object coordinates these values specify a model view matrix through the lookat function in this example we will send both a model view and a projection matrix to the vertex shader with the following display callback void display glclear eye radius cos theta eye radius sin theta cos phi eye radius sin theta sin phi lookat eye at up projection ortho left right bottom top near far projection gldrawarrays n glutswapbuffers the corresponding vertex shader is in vposition in vcolor out color uniform uniform projection void main projection vposition color vcolor we can use the keyboard callback to alter the projection matrix and the camera position the position of the camera is controlled by the r o and p keys and the sides of the viewing parallelpiped by the x y and z keys void mykey unsigned char key int mousex int mousey float dr degrees in radians if key q key q exit if key x left right if key x left right if key y bottom top if key y bottom top if key z near far if key z near far if key r radius if key r radius if key o theta dr if key o theta dr if key p phi dr if key p phi dr glutpostredisplay note that as we move the camera around the size of the image of the cube does not change which is a consequence of using an orthogonal projection however depending on the radius some or even all of the cube can be clipped out this behavior is a consequence of the parameters in ortho being measured relative to the camera hence if we move the camera back by increasing the radius but leave the near and far distances unchanged ﬁrst the back of the cube will be clipped out eventually as the radius becomes larger the entire cube will be clipped out now consider what happens as we change the parameters in ortho as we in crease right and left the cube elongates in the x direction a similar phenomenon occurs when we increase bottom and top in the y direction although this distortion of the cube image may be annoying it is a consequence of using an x y rectan gle in ortho that is not square this rectangle is mapped to the full viewport which has been unchanged we can alter the program so that we increase or decrease all of left right bottom and top simultaneously or we can alter the viewport as part of any change to ortho see exercise perspective projections we now turn to perspective projections which are what we get with a camera whose lens has a ﬁnite focal length or in terms of our synthetic camera model the center of projection is ﬁnite as with parallel projections we will separate perspective viewing into two parts the positioning of the camera and the projection positioning will be done the same way and we can use the lookat function the projection part is equivalent to select ing a lens for the camera as we saw in chapter it is the combination of the lens and the size of the ﬁlm or of the back of the camera that determines how much of the world in front of a camera appears in the image in computer graphics we make an equivalent choice when we select the type of projection and the viewing parameters with a physical camera a wide angle lens gives the most dramatic perspectives with objects near the camera appearing large compared to objects far from the lens a telephoto lens gives an image that appears ﬂat and is close to a parallel view first we consider the mathematics for a simple projection we can extend our use of homogeneous coordinates to the projection process which allows us to char acterize a particular projection with a matrix simple perspective projections suppose that we are in the camera frame with the camera located at the origin pointed in the negative z direction figure shows two possibilities in fig ure a the back of the camera is orthogonal to the z direction and is parallel to the lens this conﬁguration corresponds to most physical situations including those of the human visual system and of simple cameras the situation shown in figure b is more general the back of the camera can have any orientation with respect to the front we consider the ﬁrst case in detail because it is simpler how ever the derivation of the general result follows the same steps and should be a direct exercise exercise as we saw in chapter we can place the projection plane in front of the center of projection if we do so for the conﬁguration of figure a we get the views shown in figure a point in space x y z is projected along a projector into the point xp yp zp all projectors pass through the origin and because the projection plane is perpendicular to the z axis zp d because the camera is pointing in the negative z direction the projection plane is in the negative half space z and the value of d is negative z z x x a b figure two cameras a back parallel to front b back not parallel to front y xp d x z y z d x z z a z b c figure three views of perspective projection a three dimensional view b top view c side view from the top view shown in figure b we see two similar triangles whose tangents must be the same hence x xp z d and x xp z d using the side view shown in figure c we obtain a similar result for yp y yp z d these equations are nonlinear the division by z describes nonuniform foreshort ening the images of objects farther from the center of projection are reduced in size diminution compared to the images of objects closer to the cop figure projection pipeline we can look at the projection process as a transformation that takes points x y z to other points xp yp zp although this perspective transformation pre serves lines it is not afﬁne it is also irreversible because all points along a projector project into the same point we cannot recover a point from its projection in sec tions and we will develop an invertible variant of the projection transforma tion that preserves distances that are needed for hidden surface removal we can extend our use of homogeneous coordinates to handle projections when we introduced homogeneous coordinates we represented a point in three dimensions x y z by the point x y z in four dimensions suppose that instead we replace x y z by the four dimensional point wx p wz as long as w we can recover the three dimensional point from its four dimensional representation by dividing the ﬁrst three components by w in this new homogeneous coordinate form points in three dimensions become lines through the origin in four dimensions transformations are again represented by ma trices but now the ﬁnal row of the matrix can be altered and thus w can be changed by such a transformation obviously we would prefer to keep w to avoid the divisions otherwise nec essary to recover the three dimensional point however by allowing w to change we can represent a larger class of transformations including perspective projections consider the matrix m the matrix m transforms the point x z to the point q x y z z d at ﬁrst glance q may not seem sensible however when we remember that we have to divide the ﬁrst three components by the fourth to return to our original three dimensional space we obtain the results x xp z d y yp z d z zp z d d which are the equations for a simple perspective projection in homogeneous coor dinates dividing q by its w component replaces q by the equivalent point x xp t y y p we have shown that we can do at least a simple perspective projection by deﬁn ing a projection matrix that we apply after the model view matrix however we must perform a perspective division at the end this division can be made a part of the pipeline as shown in figure perspective projections with opengl the projections that we developed in section did not take into account the proper ties of the camera the focal length of its lens or the size of the ﬁlm plane figure shows the angle of view for a simple pinhole camera like the one that we discussed in chapter only those objects that ﬁt within the angle of view of the camera appear in the image if the back of the camera is rectangular only objects within a semi inﬁnite pyramid the view volume whose apex is at the cop can appear in the image ob jects not within the view volume are said to be clipped out of the scene hence our description of simple projections has been incomplete we did not include the effects of clipping with most graphics apis the application program speciﬁes clipping parameters through the speciﬁcation of a projection the inﬁnite pyramid in figure becomes figure specification of a view volume back clipping plane cop figure front and back clipping planes a ﬁnite clipping volume by adding front and back clipping planes in addition to the angle of view as shown in figure the resulting view volume is a frustum a truncated pyramid we have ﬁxed only one parameter by specifying that the cop is at the origin in the camera frame in principle we should be able to specify each of the six sides of the frustum to have almost any orientation if we did so however we would make it difﬁcult to specify a view in the application and complicate the implementation in practice we rarely need this ﬂexibility and usually we can get by with only two perspective viewing functions other apis differ in their function calls but incorporate similar restrictions perspective functions we will develop two functions for specifying perspective views and one for specifying parallel views alternatively we can form the projection matrix directly either by top near z figure specification of a frustum loading it or by applying rotations translations and scalings to an initial identity matrix we can specify a perspective camera view by the function frustum glfloat left glfloat right glfloat bottom glfloat top glfloat near glfloat far whose parameters are similar to those in ortho these parameters are shown in fig ure in the camera frame the near and far distances are measured from the cop the origin in eye coordinates to front and back clipping planes both of which are parallel to the plane z because the camera is pointing in the negative z direction the front near clipping plane is the plane z near and the back far clipping plane is the plane z far the left right top and bottom values are measured in the near front clipping plane the plane x left is to the left of the camera as viewed from the cop in the direction the camera is pointing similar statements hold for right bottom and top although in virtually all applications far near as long as near far the resulting projection matrix is valid although objects behind the center of projection the origin will be inverted in the image if they lie between the near and far planes note that these speciﬁcations do not have to be symmetric with respect to the z axis and that the resulting frustum also does not have to be symmetric a right frustum in section we show how the projection matrix for this projection can be derived from the simple perspective projection matrix in many applications it is natural to specify the angle of view or ﬁeld of view however if the projection plane is rectangular rather than square then we see a different angle of view in the top and side views figure the angle fovy is the angle between the top and bottom planes of the clipping volume the function perspective glfloat fovy glfloat aspect glfloat near glfloat far w y h fovy x z figure specification using the field of view allows us to specify the angle of view in the up y direction as well as the aspect ratio width divided by height of the projection plane the near and far planes are speciﬁed as in frustum perspective projection matrices for perspective projections we follow a path similar to the one that we used for parallel projections we ﬁnd a transformation that allows us by distorting the vertices of our objects to do a simple canonical projection to obtain the desired image our ﬁrst step is to decide what this canonical viewing volume should be we then introduce a new transformation the perspective normalization transformation that converts a perspective projection to an orthogonal projection finally we derive the perspective projection matrix we will use in opengl perspective normalization in section we introduced a simple perspective projection matrix for the pro jection plane at z and the center of the projection at the origin the projection matrix is to form an image we also need to specify a clipping volume suppose that we ﬁx the angle of view at degrees by making the sides of the viewing volume intersect the projection plane at a degree angle equivalently the view volume is the semi inﬁnite view pyramid formed by the planes y x z zmin z figure simple perspective projection x z y z shown in figure we can make the volume ﬁnite by specifying the near plane to be z near and the far plane to be z far where both near and far the distances from the center of projection to the near and far planes satisfy near far consider the matrix n which is similar to m but is nonsingular for now we leave α and β unspeciﬁed but nonzero if we apply n to the homogeneous coordinate point p x y z t we obtain the new point q xt yt zt wt t where xt x yt y zt αz β wt z after dividing by wt we have the three dimensional point xtt ytt x z y z ztt α β if we apply an orthographic projection along the z axis to n we obtain the matrix orth which is a simple perspective projection matrix and the projection of the arbitrary point p is pt m orth x np z after we do the perspective division we obtain the desired values for xp and yp x x p z y y p z we have shown that we can apply a transformation n to points and after an or thogonal projection we obtain the same result as we would have for a perspective projection this process is similar to how we converted oblique projections to or thogonal projections by ﬁrst shearing the objects the matrix n is nonsingular and transforms the original viewing volume into a new volume we choose α and β such that the new volume is the canonical clipping volume consider the sides x z they are transformed by xtt x z to the planes xtt likewise the sides y z are transformed to ytt z far z x x cop figure perspective normalization of view volume z the front clipping plane z near is transformed to the plane ztt α β finally the far plane z far is transformed to the plane ztt α β if we select α near far near far near far β near far then the plane z near is mapped to the plane ztt the plane z far is mapped to the plane ztt and we have our canonical clipping volume figure shows this transformation and the distortion to a cube within the volume thus n has transformed the viewing frustum to a right parallelepiped and an orthographic projection in the transformed volume yields the same image as does the perspective projection the matrix n is called the perspective normalization matrix the map ping ztt α β is nonlinear but preserves the ordering of depths thus if and are the depths of two points within the original viewing volume and z z far left bottom near right top near cop figure opengl perspective then their transformations satisfy consequently hidden surface removal works in the normalized volume although the nonlinearity of the transformation can cause numerical problems because the depth buffer has a limited depth resolution note that although the original projec tion plane we placed at z has been transformed by n to the plane ztt β α there is little consequence to this result because we follow n by an orthographic pro jection although we have shown that both perspective and parallel transformations can be converted to orthographic transformations the effects of this conversion are greatest in implementation as long as we can put a carefully chosen projection matrix in the pipeline before the vertices are deﬁned we need only one viewing pipeline for all possible views in chapter where we discuss implementation in detail we will see how converting all view volumes to right parallelepipeds by our normalization process simpliﬁes both clipping and hidden surface removal opengl perspective transformations the function frustum does not restrict the view volume to a symmetric or right frustum the parameters are as shown in figure we can form the perspective matrix by ﬁrst converting this frustum to the symmetric frustum with degree sides see figure the process is similar to the conversion of an oblique parallel view to an orthogonal view first we do a shear to convert the asymmetric frustum to a symmetric one figure shows the desired transformation the shear an gle is determined by our desire to skew shear the point left right top bottom near to near the required shear matrix is h θ φ h cot left right cot top bottom the resulting frustum is described by the planes right left x near top bottom y near z near z far the next step is to scale the sides of this frustum to x z y z without changing either the near plane or the far plane the required scaling matrix is s near right left near top bottom note that this transfor mation is determined uniquely without reference to the location of the far plane z far because in three dimensions an afﬁne transformation is determined by the results of the transformation on four points in this case these points are the four vertices where the sides of the frustum intersect the near plane to get the far plane to the plane z and the near plane to z after applying a projection normalization we use the projection normalization matrix n n with α and β as in section the resulting projection matrix is in terms of the near and far distances near right left right left p nsh near top bottom top bottom top bottom we obtain the projection matrix corresponding to persective fovy aspect near far by using symmetry in p so left right bottom top and simple trigonometry to determine top near tan fovy right top aspect simplifying p to near right near p nsh top perspective example we have to make almost no changes to our previous example to go from an orthogo nal projection to a perspective projection we can substitute frustum for ortho and the parameters are the same however for a perspective view we should have far near note that if we want to see the foreshortening we associate with perspective views we can either move the cube off the z axis or add additional cubes to the right or left we can add the perspective division to our vertex shader so it becomes in vposition in vcolor out color uniform uniform projection void main projection vposition vposition w color vcolor the full program is in appendix a hidden surface removal before introducing a few additional examples and extensions of viewing we need to deepen our understanding of the hidden surface removal process let start with the cube we have been using in our examples when we look at a cube that has opaque sides depending on its orientation we see only one two or three front facing sides from the perspective of our basic viewing model we can say that we see only these faces because they block the projectors from reaching any other surfaces from the perspective of computer graphics however all six faces of the cube have been speciﬁed and travel down the graphics pipeline thus the graphics sys tem must be careful about which surfaces it displays conceptually we seek algo rithms that either remove those surfaces that should not be visible to the viewer called hidden surface removal algorithms or ﬁnd which surfaces are visible called visible surface algorithms there are many approaches to the problem several of which we investigate in chapter opengl has a particular algorithm associated with it the z buffer algorithm to which we can interface through three function calls hence we introduce that algorithm here and we return to the topic in chapter hidden surface removal algorithms can be divided into two broad classes object space algorithms attempt to order the surfaces of the objects in the scene such that rendering surfaces in a particular order provides the correct image for example for our cube if we were to render the back facing surfaces ﬁrst we could paint over them with the front surfaces and would produce the correct image this class of algorithms does not work well with pipeline architectures in which objects are passed down the pipeline in an arbitrary order in order to decide on a proper order in which to render the objects the graphics system must have all the objects available so it can sort them into the desired back to front order image space algorithms work as part of the projection process and seek to deter mine the relationship among object points on each projector the z buffer algorithm is of the latter type and ﬁts in well with the rendering pipeline in most graphics sys tems because we can save partial information as each object is rendered the basic idea of the z buffer algorithm is shown in figure a projector from the cop passes through two surfaces because the circle is closer to the viewer than to the triangle it is the circle color that determines the color placed in the color buffer at the location corresponding to where the projector pierces the projection plane the difﬁculty is determining how we can make this idea work regardless of the order in which the triangle and the circle pass through the pipeline projection plane cop z figure the z buffer algorithm let assume that all the objects are polygons if as the polygons are rasterized we can keep track of the distance from the cop or the projection plane to the closest point on each projector that already has been rendered then we can update this information as successive polygons are projected and ﬁlled ultimately we display only the closest point on each projector the algorithm requires a depth buffer or z buffer to store the necessary depth information as polygons are rasterized because we must keep depth information for each pixel in the color buffer the z buffer has the same spatial resolution as the color buffers its depth resolution is usually bits with recent graphics cards that store this information as ﬂoating point numbers the z buffer is one of the buffers that constitute the frame buffer and is usually part of the memory on the graphics card the depth buffer is initialized to a value that corresponds to the farthest distance from the viewer when each polygon inside the clipping volume is rasterized the depth of each fragment how far the corresponding point on the polygon is from the viewer is calculated if this depth is greater than the value at that fragment location in the depth buffer then a polygon that has already been rasterized is closer to the viewer along the projector corresponding to the fragment hence for this fragment we ignore the color of the polygon and go on to the next fragment for this polygon where we make the same test if however the depth is less than what is already in the z buffer then along this projector the polygon being rendered is closer than any one we have seen so far thus we use the color of the polygon to replace the color of the pixel in the color buffer and update the depth in the z buffer for the example shown in figure we see that if the triangle passes through the pipeline ﬁrst its colors and depths will be placed in the color and z buffers when the circle passes through the pipeline its colors and depths will replace the colors and depths of the triangle where they overlap if the circle is rendered ﬁrst its colors and depths will be placed in the buffers when the triangle is rendered in the areas where there is overlap the depths of the triangle are greater than the depth of the circle and at the corresponding pixels no changes will be made to the color or depth buffers major advantages of this algorithm are that its complexity is proportional to the number of fragments generated by the rasterizer and that it can be implemented with a small number of additional calculations over what we have to do to project and display polygons without hidden surface removal we will return to this issue in chapter from the application programmer perspective she must initialize the depth buffer and enable hidden surface removal by using glutinitdisplaymode glenable the color of the polygon is determined by shading chapter and texture mapping chapter if these features are enabled here we use the glut library for the initialization and specify a depth buffer in addition to our usual rgb color and double buffering the programmer can clear the color and depth buffers as necessary for a new rendering by using glclear culling for a convex object such as the cube faces whose normals point away from the viewer are never visible and can be eliminated or culled before the rasterizer we can turn on culling in opengl by enabling it as follows glenable however culling is guaranteed to produce a correct image only if we have a single convex object often we can use culling in addition to the z buffer algorithm which works with any collection of polygons for example suppose that we have a scene composed of a collection of n cubes if we use only the z buffer algorithm we pass polygons through the pipeline if we enable culling half the polygons can be eliminated early in the pipeline and thus only polygons pass through all stages of the pipeline we consider culling further in chapter displaying meshes we now have the tools to walk through a scene interactively by having the camera parameters change in response to user input before introducing a simple interface let consider another example of data display mesh plots a mesh is a set of polygons that share vertices and edges a general mesh as shown in figure may contain polygons with any number of vertices and require a moderately sophisticated data structure to store and display efﬁciently rectangular and triangular meshes such as we introduced in chapter for modeling a sphere are much simpler to work with and are useful for a wide variety of applications here we introduce rectangular meshes for the display of height data height data determine a surface such as terrain through either a function that gives the heights above a reference value such as elevations above sea level or through samples taken at various points on the surface suppose that the heights are given by y through a function y f x z where x and z are the points on a two dimensional surface such as a rectangle thus for each x z we get exactly one y as shown in figure such surfaces are sometimes called dimensional surfaces or height ﬁelds although all surfaces cannot be represented this way they have many applications for example if we use an x z coordinate system to give positions on the surface of the earth then we can use such a function to represent the height or altitude at each location in many situations figure mesh figure height field the function f is known only discretely and we have a set of samples or measurements of experimental data of the form yij f xi zj we assume that these data points are equally spaced such that xi i n zj j m where and are the spacing between the samples in the x and z directions respectively if f is known analytically then we can sample it to obtain a set of discrete data with which to work probably the simplest way to display the data is to draw a line strip for each value of x and another for each value of z thus generating n m line strips suppose that the height data are in an array data we can form a single array with the data converted to vertices arranged by rows with the code float data n m vertices n m int k for int i i n i for int j j m j vertices k i data i j j k we can form an array for the vertices by column by switching roles of i and j as follows vertices n m int k for int i i m i for int j j n j vertices k j data j i i k we usually will want to scale the data to be over a convenient range such as and scale the x and z values to make them easier to display as part of the model view matrix or equivalently by adjusting the size of the view volume we can display these vertices by sending both arrays to the vertex shader so in the initialization we set up the vertex buffer object with the correct size but without sending any data gluint buffer glbindvertexarray buffer loc glgetattriblocation program vposition glenablevertexattribarray loc glvertexattribpointer loc glgenbuffers buffer glbindbuffer buffer glbufferdata sizeof vertices null in the display callback we load the two vertex arrays successively and display them form array of vertices by row here glbufferdata sizeof vertices vertices gldrawarrays n m form array of vertices by column here glbufferdata sizeof vertices vertices gldrawarrays n m you should now be able to complete a program to display the data figure shows a rectangular mesh from height data for a part of honolulu hawaii these data are available on the web site for the book there are a few problems with this simple figure mesh plot of honolulu data using line strips approach one is that we have to load data onto the gpu twice every time we want to display the mesh and thus are wasting a lot of time moving data from the cpu to the gpu a second problem is that we are not doing any hidden surface removal so we see the lines from parts of the original surface that should be hidden from the viewer third there are some annoying extra lines that appear from the end of one row or column to the next row or column these lines are a consequence of putting all rows and columns into a single line strip we can get around the last problem by assigning vertex colors carefully the ﬁrst two problems can be avoided by displaying the data as a surface using polygons an added advantage in using polygons is that we will be able to extend our code to displaying the mesh with lights and material properties in chapter displaying meshes as a surface one simple way to generate a surface is through a triangular mesh we can use the four points yij yi j yi j and yi j to generate two triangles thus the height data specify a mesh of triangles the basic opengl program is straightforward we can form the array of triangle data in the main function or as part of initialization as in the following code which normalizes the data to be in the range the x values to be over and the z values to be over float data n m all values assumed non negative float fmax maximum of data triangles n m vertex positions float fn float n float fm float m int k for i i n i for j j m j nm quads triangles quad triangles k i fn data i j fmax j fm k triangles k i fn data i j fmax j fm k triangles k i fn data i j fmax j fm k triangles k i fn data i j fmax j fm k triangles k i fn data i j fmax j fm k triangles k i fn data i j fmax j fm k we initialize the vertex array as before glbindvertexarray abuffer loc glgetattriblocation program vposition glenablevertexattribarray loc glvertexattribpointer loc glgenbuffers buffers glbindbuffer buffers glbufferdata sizeof triangles triangles and display it as glpolygonmode gldrawarrays n m if we integrate this code with our previous example using line strips the output will look almost identical although we have designed a surface by choosing to display only the edges by using a polygon mode of we do not generate any fragments corresponding to the inside of the polygon and thus we see the edges of polygons that would be hidden if the mode were we can ﬁx this problem by rendering the data twice ﬁrst as a ﬁlled white surface and second as black lines because the data are already on the gpu we do not have to send any vertex data to the gpu for the second rendering we can specify two colors in the initialization that we will send to the fragment shader white black glgetuniformlocation program fcolor and then modify to the display callback to have the code glpolygonmode white gldrawarrays n m glpolygonmode black gldrawarrays n m the modiﬁed fragment shader is uniform fcolor void main fcolor polygon offset there are interesting aspects to this opengl program and we can make various modiﬁcations first if we use all the data the resulting plot may contain many small polygons the resulting density of lines in the display may be annoying and can contain moire patterns hence we might prefer to subsample the data either by using every kth point for some k or by averaging groups of data points to obtain a new set of samples with smaller n and m there is one additional trick that we used in the display of figure if we draw both a polygon and a line loop with the code in the previous section then each triangle is rendered twice in the same plane once ﬁlled and once by its edges even though the second rendering of just the edges is done with ﬁlled rendering numer ical inaccuracies in the renderer often cause parts of second rendering to lie behind the corresponding fragments in the ﬁrst rendering we can avoid this problem by enabling the polygon offset mode and setting the offset parameters using glpoly gonoffset polygon ﬁll offset moves fragments slightly away from the viewer so all the desired lines should be visible in initialization we can set up polygon offset by glenable glpolygonoffset the two parameters in polygonoffset are combined with the slope of the polygon and an implementation dependent constant consequently you may have to do a little experimentation to ﬁnd the best values perhaps the greatest weakness of our code is that we are sending too much data to the gpu and not using the most efﬁcient rendering method consider a mesh consisting of a single row of n quadrilaterals if we render it as triangles using vertices n gldrawarrays n we send vertices to the gpu if we instead set our vertices as a triangle strip then these two lines of code become vertices n gldrawarrays n not only are we sending less data and requiring less of the gpu memory but the gpu will render the triangles much faster as a triangle strip as opposed to individual triangles for a m mesh we can easily construct the array for a triangle strip for an n m mesh the process is more complex although it would be simple to repeat the process for the m mesh n times setting up n triangle strips this approach would have us repeatedly sending data to the gpu what we need is a single triangle strip for the entire mesh exercises and outline two approaches walking through a scene the next step is to specify the camera and add interactivity in this version we use perspective viewing and we allow the viewer to move the camera by pressing the x x y y z and z keys on the keyboard but we have the camera always pointing at the center of the cube the lookat function provides a simple way to reposition and reorient the camera the changes that we have to make to our previous program in section are minor we deﬁne an array viewer to hold the camera position its contents are altered by the simple keyboard callback function keys as follows void keys unsigned char key int x int y if key x viewer if key x viewer if key y viewer if key y viewer if key z viewer if key z viewer glutpostredisplay the display function calls lookat using viewer for the camera position and uses the origin for the at position the cube is rotated as before based on the mouse input note the order of the function calls in display that alter the model view matrix void display glclear lookat viewer viewer viewer rotatex theta rotatey theta rotatez theta draw mesh or other objects here glutswapbuffers we can invoke frustum from the reshape callback to specify the camera lens through the following code projection void myreshape int w int h glviewport w h glfloat left right bottom top glfloat aspect glfloat w h if aspect bottom aspect top aspect else left aspect right aspect projection frustum left right bottom top projection note that we chose the values of the parameters in frustum based on the aspect ratio of the window other than the added speciﬁcation of a keyboard callback function in main the rest of the program is the same as the program in section if you run this program you should note the effects of moving the camera the lens and the sides of the viewing frustum note what happens as you move toward the mesh you should also consider the effect of always having the viewer look at the center of the mesh as she is moving note that we could have used the mouse buttons to move the viewer we could use the mouse buttons to move the user forward or to turn her right or left see exercise however by using the keyboard for moving the viewer we can use the mouse to move the object as with the rotating cube in chapter in this example we are using direct positioning of the camera through lookat there are other possibilities one is to use rotation and translation matrices to alter the model view matrix incrementally if we want to move the viewer through the scene without having her looking at a ﬁxed point this option may be more appealing we could also keep a position variable in the program and change it as the viewer moves in this case the model view matrix would be computed from scratch rather than changed incrementally which option we choose depends on the particular application and often on other factors as well such as the possibility that numerical errors might accumulate if we were to change the model view matrix incrementally many times the basic mesh rendering can be extended in many ways in chapter we will learn to add lights and surface properties to create a more realistic image in chapter we will learn to add a texture to the surface the texture map might be an image of the terrain from a photograph or other data that might be obtained by digitization of a map if we combine these techniques we can generate a display in which we can make the image depend on the time of day by changing the position of the light source it is also possible to obtain smoother surfaces by using the data to deﬁne a smoother surface with the aid of one of the surface types that we will introduce in chapter projections and shadows the creation of simple shadows is an interesting application of projection matrices although shadows are not geometric objects they are important components of realistic images and give many visual clues to the spatial relationships among the objects in a scene starting from a physical point of view shadows require a light source to be present a point is in shadow if it is not illuminated by any light source or equivalently if a viewer at that point cannot see any light sources however if the only light source is at the center of projection there are no visible shadows because any shadows are behind visible objects this lighting strategy has been called the ﬂashlight in the eye model and corresponds to the simple lighting we have used thus far to add physically correct shadows we must understand the interaction between light and materials a topic that we investigate in chapter there we show that global calculations are difﬁcult normally they cannot be done in real time nevertheless the importance of shadows in applications such as ﬂight simulators led to a number of special approaches that can be used in many circumstances consider the shadow generated by the point source in figure we assume for simplicity that the shadow falls on the ground or the surface y xl yl zl x z figure shadow from a single polygon not only is the shadow a ﬂat polygon called a shadow polygon but it also is the projection of the original polygon onto the surface speciﬁcally the shadow polygon is the projection of the polygon onto the surface with the center of projection at the light source thus if we do a projection onto the plane of a surface in a frame in which the light source is at the origin we obtain the vertices of the shadow polygon these vertices must then be converted back to a representation in the object frame rather than do the work as part of an application program we can ﬁnd a suitable projection matrix and use it to compute the vertices of the shadow polygon suppose that we start with a light source at xl yl zl as shown in figure a if we reorient the ﬁgure such that the light source is at the origin as shown in figure b by a translation matrix t xl yl zl then we have a simple perspective projection through the origin the projection matrix is m finally we translate everything back with t xl yl zl the concatenation of this matrix and the two translation matrices projects the vertex x y z to x x x xl y yl yl yp z z z zl y yl yl y z a b figure shadow polygon projection a from a light source b with source moved to the origin however with an opengl program we can alter the model view matrix to form the desired polygon as follows if the light source is ﬁxed we can compute the shadow projection matrix once as part of initialization otherwise we need to recompute it perhaps in the idle callback function if the light source is moving the code for setting up the matrix is as follows float light location of light m shadow projection matrix initially an identity matrix m light let project a single square polygon parallel onto the plane y we can specify the square through the vertices square note that the vertices are ordered so that we can render them using a triangle strip we initialize a red color for the square and a black color for its shadow which we will send to the fragment shader we initialize a vertex array and a buffer object as we did in our previous examples gluint abuffer buffer glgenvertexarrays abuffer glbindvertexarray abuffer int loc glgetattriblocation program vposition glenablevertexattribarray loc glvertexattribpointer loc color_loc glgetuniformlocation program fcolor glgenbuffers buffer glbindbuffer buffer glbufferdata sizeof square square if the data do not change we can also set the projection matrix and model view matrix as part of initialization and send them to the vertex shader lookat eye at up projection ortho left right bottom top near far matrix_loc projection_loc projection the core of the display callback is void display mm clear the window glclear render red square color_loc red gldrawarrays matrix to compute vertices of shadow polygon mm translate light light light m translate light light light matrix_loc mm render shadow polygon color_loc black gldrawarrays glutswapbuffers note that although we are performing a projection with respect to the light source the matrix that we use is the model view matrix we render the same polygon twice the ﬁrst time as usual and the second time with an altered model view matrix that transforms the vertices the same viewing conditions are applied to both the polygon and its shadow polygon the results of computing shadows for the colored cube are shown in color plate for a simple environment such as an airplane ﬂying over ﬂat terrain casting a single shadow this technique works well it is also easy to convert from point sources to distant parallel light sources see exercise however when objects can cast shadows on other objects this method becomes impractical in chapter we address more general but slower rendering methods that will create shadows automatically as part of the rendering process summary and notes we have come a long way we can now write complete nontrivial three dimensional applications probably the most instructive activity that you can do now is to write such an application developing skill with manipulating the model view and projec tion functions takes practice we have presented the mathematics of the standard projections although most apis obviate the application programmer from writing projection functions under standing the mathematics leads to understanding a pipeline implementation based on concatenation of matrices until recently application programs had to do the projections within the applications and most hardware systems did not support perspective projections there are three major themes in the remainder of this book first we explore modeling further by expanding our basic set of primitives in chapter we incorpo rate more complex relationships between simple objects through hierarchical models in chapter we explore approaches to modeling that allow us to describe objects through procedures rather than as geometric objects this approach allows us to model objects with only as much detail as is needed to incorporate physical laws into our models and to model natural phenomena that cannot be described by polygons in chapter we leave the world of ﬂat objects adding curves and curved surfaces these objects are deﬁned by vertices and we can implement them by breaking them into small ﬂat primitives so we can use the same viewing pipeline the second major theme is realism although more complex objects allow us to build more realistic models we also explore more complex rendering options in chapter we consider the interaction of light with the materials that characterize our objects we look more deeply at hidden surface removal methods at shading models and in chapter at techniques such as texture mapping that allow us to create complex images from simple objects using advanced rendering techniques third we look more deeply at implementation in chapter at this point we have introduced the major functional units of the graphics pipeline we discuss the details of the algorithms used in each unit we will also see additional possibilities for creating images by working directly in the frame buffer suggested readings carlbom and paciorek discuss the relationships between classical and com puter viewing rogers and adams give many examples of the projection matrices corresponding to the standard views used in drafting foley et al watt and hearn and baker derive canonical projection transfor mations all follow a phigs orientation so the api is slightly different from the one used here although foley derives the most general case the references differ in whether they use column or row matrices in where the cop is located and in whether the projection is in the positive or negative z direction see the opengl pro gramming guide for a further discussion of the use of the model view and projection matrices in opengl exercises not all projections are planar geometric projections give an example of a projection in which the projection surface is not a plane and another in which the projectors are not lines consider an airplane whose position is speciﬁed by the roll pitch and yaw and by the distance from an object find a model view matrix in terms of these parameters consider a satellite orbiting the earth its position above the earth is speciﬁed in polar coordinates find a model view matrix that keeps the viewer looking at the earth such a matrix could be used to show the earth as it rotates show how to compute u and v directions from the vpn vrp and vup using only cross products can we obtain an isometric of the cube by a single rotation about a suitably chosen axis explain your answer derive the perspective projection matrix when the cop can be at any point and the projection plane can be at any orientation show that perspective projection preserves lines any attempt to take the projection of a point in the same plane as the cop will lead to a division by zero what is the projection of a line segment that has endpoints on either side of the projection plane deﬁne one or more apis to specify oblique projections you do not need to write the functions just decide which parameters the user must specify derive an oblique projection matrix from speciﬁcation of front and back clip ping planes and top right and bottom left intersections of the sides of the clipping volume with the front clipping plane our approach of normalizing all projections seems to imply that we could predistort all objects and support only orthographic projections explain any problems we would face if we took this approach to building a graphics system how do the opengl projection matrices change if the cop is not at the origin assume that the cop is at d and the projection plane is z we can create an interesting class of three dimensional objects by extending two dimensional objects into the third dimension by extrusion for example a circle becomes a cylinder a line becomes a quadrilateral and a quadrilateral in the plane becomes a parallelepiped use this technique to convert the two dimensional maze from exercise to a three dimensional maze extend the maze program of exercise to allow the user to walk through the maze a click on the middle mouse button should move the user forward a click on the right or left button should turn the user degrees to the right or left respectively if we were to use orthogonal projections to draw the coordinate axes the x and y axes would lie in the plane of the paper but the z axis would point out of the page instead we can draw the x and y axes meeting at a degree angle with the z axis going off at degrees from the x axis find the matrix that projects the original orthogonal coordinate axes to this view write a program to display a rotating cube in a box with three light sources each light source should project the cube onto one of the three visible sides of the box find the projection of a point onto the plane ax by cz d from a light source located at inﬁnity in the direction dx dy dz using one of the three dimensional interfaces discussed in chapter write a program to move the camera through a scene composed of simple objects write a program to ﬂy through the three dimensional sierpinski gasket formed by subdividing tetrahedra can you prevent the user from ﬂying through walls in animation often we can save effort by working with two dimensional pat terns that are mapped onto ﬂat polygons that are always parallel to the camera a technique known as billboarding write a program that will keep a simple polygon facing the camera as the camera moves stereo images are produced by creating two images with the viewer in two slightly different positions consider a viewer who is at the origin but whose eyes are separated by units what are the appropriate viewing speciﬁcations to create the two images in section we displayed a mesh by drawing two line strips how would you alter this approach to not draw the extra line from the end of one row or column to the begining of the next row or column derive a method for displaying a mesh using a single triangle strip construct a fragment shader that does polygon offset during a perspective projection write a shader that modiﬁes the height of a mesh in the shader render a rectangular mesh as a single triangle strip by creating a degenerate triangle at the end of each row write a program that will ﬂy around above a mesh your program should allow the user to look around at the hills and valleys rather than always looking at a single point write a reshape callback that does not distort the shape of objects as the win dow is altered e have learned to build three dimensional graphical models and to display them however if you render one of our models you might be disappointed to see images that look ﬂat and thus fail to show the three dimensional nature of the model this appearance is a consequence of our unnatural assumption that each surface is lit such that it appears to a viewer in a single color under this assumption the orthographic projection of a sphere is a uniformly colored circle and a cube appears as a ﬂat hexagon if we look at a photograph of a lit sphere we see not a uniformly colored circle but rather a circular shape with many gradations or shades of color it is these gradations that give two dimensional images the appearance of being three dimensional what we have left out is the interaction between light and the surfaces in our models this chapter begins to ﬁll that gap we develop separate models of light sources and of the most common light material interactions our aim is to add shading to a fast pipeline graphics architecture consequently we develop only local lighting models such models as opposed to global lighting models allow us to compute the shade to assign to a point on a surface independent of any other surfaces in the scene the calculations depend only on the material properties assigned to the surface the local geometry of the surface and the locations and properties of the light sources in this chapter we introduce the lighting models used most often in opengl applications we shall see that we have choices as to where to apply a given lighting model in the application in the vertex shader or in the fragment shader following our previous development we investigate how we can apply shading to polygonal models we develop a recursive approximation to a sphere that will allow us to test our shading algorithms we then discuss how light and material properties are speciﬁed in opengl applications and can be added to our sphere approximating program we conclude the chapter with a short discussion of the two most important methods for handling global lighting effects ray tracing and radiosity b a figure reflecting surfaces light and matter in chapters and we presented the rudiments of human color vision delaying until now any discussion of the interaction between light and surfaces perhaps the most general approach to rendering is based on physics where we use principles such as conservation of energy to derive equations that describe how light is reﬂected from surfaces from a physical perspective a surface can either emit light by self emission as a light bulb does or reﬂect light from other surfaces that illuminate it some surfaces may both reﬂect light and emit light from internal physical processes when we look at a point on an object the color that we see is determined by multiple interactions among light sources and reﬂective surfaces these interactions can be viewed as a recursive process consider the simple scene in figure some light from the source that reaches surface a is scattered some of this reﬂected light reaches surface b and some of it is then scattered back to a where some of it is again reﬂected back to b and so on this recursive scattering of light between surfaces accounts for subtle shading effects such as the bleeding of colors between adjacent surfaces mathematically the limit of this recursive process can be described using an integral equation the rendering equation which in principle we could use to ﬁnd the shading of all surfaces in a scene unfortunately this equation generally cannot be solved analytically numerical methods for computing a solution are not fast enough for real time rendering there are various approximate approaches such as radiosity and ray tracing each of which is an excellent approximation to the rendering equation for particular types of surfaces although ray tracing can render moderately complex scenes in real time these methods cannot render scenes at the rate at which we can pass polygons through the modeling projection pipeline consequently we focus on a simpler rendering model based on the phong reﬂection model that provides a compromise between physical correctness and efﬁcient calculation we will introduce global methods in section and then consider the rendering equation radiosity and ray tracing in greater detail in chapter figure light and surfaces rather than looking at a global energy balance we follow rays of light from light emitting or self luminous surfaces that we call light sources we then model what happens to these rays as they interact with reﬂecting surfaces in the scene this approach is similar to ray tracing but we consider only single interactions between light sources and surfaces there are two independent parts of the problem first we must model the light sources in the scene then we must build a reﬂection model that deals with the interactions between materials and light to get an overview of the process we can start following rays of light from a point source as shown in figure as we noted in chapter our viewer sees only the light that leaves the source and reaches her eyes perhaps through a complex path and multiple interactions with objects in the scene if a ray of light enters her eye directly from the source she sees the color of the source if the ray of light hits a surface visible to our viewer the color she sees is based on the interaction between the source and the surface material she sees the color of the light reﬂected from the surface toward her eyes in terms of computer graphics we replace the viewer by the projection plane as shown in figure conceptually the clipping window in this plane is mapped to the screen thus we can think of the projection plane as ruled into rectangles each corresponding to a pixel the color of the light source and of the surfaces determines the color of one or more pixels in the frame buffer we need to consider only those rays that leave the source and reach the viewer eye either directly or through interactions with objects in the case of computer viewing these are the rays that reach the center of projection cop after passing through the clipping rectangle note that in scenes for which the image shows a lot cop figure light surfaces and computer imaging a b c figure light material interactions a specular surface b diffuse surface c translucent surface of the background most rays leaving a source do not contribute to the image and are thus of no interest to us we make use of this observation in section figure shows both single and multiple interactions between rays and objects it is the nature of these interactions that determines whether an object appears red or brown light or dark dull or shiny when light strikes a surface some of it is absorbed and some of it is reﬂected if the surface is opaque reﬂection and absorption account for all the light striking the surface if the surface is translucent some of the light is transmitted through the material and emerges to interact with other objects these interactions depend on wavelength an object illuminated by white light appears red because it absorbs most of the incident light but reﬂects light in the red range of frequencies a shiny object appears so because its surface is smooth conversely a dull object has a rough surface the shading of objects also depends on the orientation of their surfaces a factor that we shall see is characterized by the normal vector at each point these interactions between light and materials can be classiﬁed into the three groups depicted in figure specular surfaces appear shiny because most of the light that is reﬂected or scattered is in a narrow range of angles close to the angle of reﬂection mirrors are perfectly specular surfaces the light from an incoming light ray may be partially absorbed but all reﬂected light from a given angle emerges at a single angle obeying the rule that the angle of incidence is equal to the angle of reﬂection diffuse surfaces are characterized by reﬂected light being scattered in all directions walls painted with matte or ﬂat paint are diffuse reﬂectors as are many natural materials such as terrain viewed from an airplane or a satellite perfectly diffuse surfaces scatter light equally in all directions and thus a ﬂat perfectly diffuse surface appears the same to all viewers translucent surfaces allow some light to penetrate the surface and to emerge from another location on the object this process of refraction characterizes glass and water some incident light may also be reﬂected at the surface we shall model all these surfaces in section first we consider light sources y i e light sources light can leave a surface through two fundamental processes self emission and re ﬂection we usually think of a light source as an object that emits light only through p x internal energy sources however a light source such as a light bulb can also reﬂect some light that is incident on it from the surrounding environment we will usually omit the emissive term in our simple models when we discuss lighting in section we will see that we can easily add a self emission term z if we consider a source such as the one in figure we can look at it as an object with a surface each point x y z on the surface can emit light that is char acterized by the direction of emission θ φ and the intensity of energy emitted at each wavelength λ thus a general light source can be characterized by a six variable illumination function i x y z θ φ λ note that we need two angles to specify a direction and we are assuming that each frequency can be considered independently from the perspective of a surface illuminated by this source we can obtain the total contribution of the source figure by integrating over its surface a process that accounts for the emission angles that reach this surface and must also account for the distance between the source and the surface for a distributed light source such as a light bulb the evaluation of this integral is difﬁcult whether we use analytic or nu merical methods often it is easier to model the distributed source with polygons each of which is a simple source or with an approximating set of point sources we consider four basic types of sources ambient lighting point sources spot lights and distant light these four lighting types are sufﬁcient for rendering most simple scenes figure light source figure adding the contribution from a source color sources not only do light sources emit different amounts of light at different frequencies but their directional properties can vary with frequency as well consequently a physically correct model can be complex however our model of the human visual system is based on three color theory which tells us we perceive three tristimulus values rather than a full color distribution for most applications we can thus model light sources as having three components red green and blue and can use each of the three color sources to obtain the corresponding color components that a human observer sees we describe a source through a three component intensity or luminance func tion ir ib each of whose components is the intensity of the independent red green and blue components thus we use the red component of a light source for the calculation of the red component of the image because light material computations involve three similar but independent calculations we will tend to present a single scalar equation with the understanding that it can represent any of the three color components ambient light in many rooms such as classrooms or kitchens the lights have been designed and positioned to provide uniform illumination throughout the room such illumination is often achieved through large sources that have diffusers whose purpose is to scatter light in all directions we could create an accurate simulation of such illumination at least in principle by modeling all the distributed sources and then integrating the illumination from these sources at each point on a reﬂecting surface making such a model and rendering a scene with it would be a daunting task for a graphics system especially one for which real time performance is desirable alternatively we can look at the desired effect of the sources to achieve a uniform light level in the room this uniform lighting is called ambient light if we follow this second approach we can postulate an ambient intensity at each point in the environment thus ambient figure point source illuminating a surface illumination is characterized by an intensity ia that is identical at every point in the scene our ambient source has three color components iar iab we will use the scalar ia to denote any one of the red green or blue components of ia although every point in our scene receives the same illumination from ia each surface can reﬂect this light differently point sources an ideal point source emits light equally in all directions we can characterize a point source located at a point by a three component color matrix ir ib the intensity of illumination received from a point source is proportional to the inverse square of the distance between the source and surface hence at a point p figure the intensity of light received from the point source is given by the matrix i p p i p p as with ambient light we will use i to denote any of the components of i the use of point sources in most applications is determined more by their ease of use than by their resemblance to physical reality scenes rendered with only point sources tend to have high contrast objects appear either bright or dark in the real world it is the large size of most light sources that contributes to softer scenes as we can see from figure which shows the shadows created by a source of ﬁnite size some areas are fully in shadow or in the umbra whereas others are in partial figure shadows created by finite size light source shadow or in the penumbra we can mitigate the high contrast effect from point source illumination by adding ambient light to a scene the distance term also contributes to the harsh renderings with point sources although the inverse square distance term is correct for point sources in practice it p is usually replaced by a term of the form a bd where d is the distance between p and the constants a b and c can be chosen to soften the lighting note that if the light source is far from the surfaces in the scene the intensity of the light from the source is sufﬁciently uniform that the distance term is constant over each surface is figure spotlight intensity e e figure attenuation of a spotlight intensity e e figure spotlight expo nent figure parallel light source spotlights spotlights are characterized by a narrow range of angles through which light is emit ted we can construct a simple spotlight from a point source by limiting the angles at which light from the source can be seen we can use a cone whose apex is at ps which points in the direction ls and whose width is determined by an angle θ as shown in figure if θ the spotlight becomes a point source more realistic spotlights are characterized by the distribution of light within the cone usually with most of the light concentrated in the center of the cone thus the intensity is a function of the angle φ between the direction of the source and a vector to a point on the surface as long as this angle is less than θ figure although this function could be deﬁned in many ways it is usually deﬁned by cose φ where the exponent e figure determines how rapidly the light intensity drops off as we shall see throughout this chapter cosines are convenient functions for lighting calculations if u and v are any unit length vectors we can compute the cosine of the angle θ between them with the dot product cos θ u v a calculation that requires only three multiplications and two additions distant light sources most shading calculations require the direction from the point on the surface to the light source position as we move across a surface calculating the intensity at each point we should recompute this vector repeatedly a computation that is a signiﬁcant part of the shading calculation however if the light source is far from the surface the vector does not change much as we move from point to point just as the light from the sun strikes all objects that are in close proximity to one another at the same angle figure illustrates that we are effectively replacing a point source of light with a source that illuminates objects with parallel rays of light a parallel source in practice the calculations for distant light sources are similar to the calculations for parallel projections they replace the location of the light source with the direction of the light source hence in homogeneous coordinates a point light source at is represented internally as a four dimensional column matrix x z in contrast the distant light source is described by a direction vector whose represen tation in homogeneous coordinates is the matrix x y z the graphics system can carry out rendering calculations more efﬁciently for distant light sources than for near ones of course a scene rendered with distant light sources looks different from a scene rendered with near sources fortunately our models will allow both types of sources the phong reflection model n v although we could approach light material interactions through physical models we have chosen to use a model that leads to efﬁcient computations especially when we i use it with our pipeline rendering model the reﬂection model that we present was r introduced by phong and later modiﬁed by blinn it has proved to be efﬁcient and to be a close enough approximation to physical reality to produce good renderings under a variety of lighting conditions and material properties the phong model uses the four vectors shown in figure to calculate a color for an arbitrary point p on a surface if the surface is curved all four vectors can change as we move from point to point the vector n is the normal at p we discuss its calculation in section the vector v is in the direction from p to the viewer or cop the vector l is in the direction of a line from p to an arbitrary point on the source for a distributed light source or as we are assuming for now to the point light source finally the vector r is in the direction that a perfectly reﬂected ray from l would take note that r is determined by n and l we calculate it in section the phong model supports the three types of material light interactions ambient diffuse and specular that we introduced in section suppose that we have a set of point sources we assume that each source can have separate ambient diffuse and specular components for each of the three primary colors although this assumption may appear unnatural remember that our goal is to create realistic shad ing effects in as close to real time as possible we use a local model to simulate effects that can be global in nature thus our light source model has ambient diffuse and specular terms we need nine coefﬁcients to characterize these terms at any point p on the surface we can place these nine coefﬁcients in a illumination matrix for the ith light source figure vectors used by the phong model lira liga liba lirs ligs libs the ﬁrst row of the matrix contains the ambient intensities for the red green and blue terms from source i the second row contains the diffuse terms the third con tains the specular terms we assume that any distance attenuation terms have not yet been applied this matrix is only a simple way of storing the nine lighting terms we need in practice we will use constructs such as light_i_specular or light_i_specular for each source in our code the four dimensional form will be useful when we consider lighting with materials that are not opaque we construct the model by assuming that we can compute how much of each of the incident lights is reﬂected at the point of interest for example for the red diffuse term from source i lird we can compute a reﬂection term rird and the latter contribution to the intensity at p is rirdlird the value of rird depends on the material properties the orientation of the surface the direction of the light source and the distance between the light source and the viewer thus for each point we have nine coefﬁcients that we can place in a matrix of reﬂection terms of the form rira riga riba rirs rigs ribs we can then compute the contribution for each color source by adding the ambient diffuse and specular components for example the red intensity that we see at p from source i is iir riralira rirdlird rirslirs iira iird iirs we obtain the total intensity by adding the contributions of all sources and possibly a global ambient term thus the red term is ir iira iird iirs iar i where iar is the red component of the global ambient light we can simplify our notation by noting that the necessary computations are the same for each source and for each primary color they differ depending on whether we are considering the ambient diffuse or specular terms hence we can omit the subscripts i r g and b we write i ia id is lara ldrd lsrs with the understanding that the computation will be done for each of the primaries and each source the global ambient term can be added at the end as with the lighting terms when we get to code we will use forms such as reflect_i_specular note that these terms are all for a single surface and in general we will have different reﬂectivity properties for each material ambient reflection the intensity of ambient light ia is the same at every point on the surface some of this light is absorbed and some is reﬂected the amount reﬂected is given by the ambient reﬂection coefﬁcient ra ka because only a positive fraction of the light is reﬂected we must have ka and thus ia kala here la can be any of the individual light sources or it can be a global ambient term a surface has of course three ambient coefﬁcients kar kag and kab and they can differ hence for example a sphere appears yellow under white ambient light if its blue ambient coefﬁcient is small and its red and green coefﬁcients are large diffuse reflection a perfectly diffuse reﬂector scatters the light that it reﬂects equally in all directions hence such a surface appears the same to all viewers however the amount of light reﬂected depends both on the material because some of the incoming light is absorbed and on the position of the light source relative to the surface diffuse reﬂections are characterized by rough surfaces if we were to magnify a cross section of a diffuse surface we might see an image like that shown in figure rays of light that hit the surface at only slightly different angles are reﬂected back at markedly different angles perfectly diffuse surfaces are so rough that there is no preferred angle of reﬂection such surfaces sometimes called lambertian surfaces can be modeled mathematically with lambert law consider a diffuse planar surface as shown in figure illuminated by the sun the surface is brightest at noon and dimmest at dawn and dusk because ac cording to lambert law we see only the vertical component of the incoming light figure rough surface a b figure illumination of a diffuse surface a at noon b in the afternoon d n d a b figure vertical contributions by lambert law a at noon b in the afternoon one way to understand this law is to consider a small parallel light source striking a plane as shown in figure as the source is lowered in the artiﬁcial sky the same amount of light is spread over a larger area and the surface appears dimmer returning to the point source of figure we can characterize diffuse reﬂections mathematically lambert law states that rd cos θ where θ is the angle between the normal at the point of interest n and the direction of the light source l if both l and n are unit length vectors then cos θ l n if we add in a reﬂection coefﬁcient kd representing the fraction of incoming diffuse light that is reﬂected we have the diffuse reﬂection term id kd l n ld if we wish to incorporate a distance term to account for attenuation as the light travels a distance d from the source to the surface we can again use the quadratic attenuation term i kd a bd l n ld direction vectors such as l and n are used repeatedly in shading calculations through the dot product in practice both the programmer and the graphics software should seek to normalize all such vectors as soon as possible there is a potential problem with this expression because l n ld will be neg ative if the light source is below the horizon in this case we want to use zero rather than a negative value hence in practice we use max l n ld specular reflection if we employ only ambient and diffuse reﬂections our images will be shaded and will appear three dimensional but all the surfaces will look dull somewhat like chalk what we are missing are the highlights that we see reﬂected from shiny objects these highlights usually show a color different from the color of the reﬂected ambient and diffuse light for example a red plastic ball viewed under white light has a white highlight that is the reﬂection of some of the light from the source in the direction of the viewer figure whereas a diffuse surface is rough a specular surface is smooth the smoother the surface is the more it resembles a mirror figure shows that as the surface gets smoother the reﬂected light is concentrated in a smaller range of angles centered about the angle of a perfect reﬂector a mirror or a perfectly specular surface mod eling specular surfaces realistically can be complex because the pattern by which the light is scattered is not symmetric it depends on the wavelength of the incident light and it changes with the reﬂection angle phong proposed an approximate model that can be computed with only a slight increase over the work done for diffuse surfaces the model adds a term for specular reﬂection hence we consider the surface as being rough for the diffuse term and smooth for the specular term the amount of light that the viewer sees depends on the angle φ between r the direction of a perfect reﬂector and v the direction of the viewer the phong model uses the equation is ksls cosα φ the coefﬁcient ks ks is the fraction of the incoming specular light that is reﬂected the exponent α is a shininess coefﬁcient figure shows how as α is increased the reﬂected light is concentrated in a narrower region centered on the angle of a perfect reﬂector in the limit as α goes to inﬁnity we get a mirror values in the range to correspond to most metallic surfaces and smaller values correspond to materials that show broad highlights the computational advantage of the phong model is that if we have normalized r and n to unit length we can again use the dot product and the specular term becomes is kslsmax r v α we can add a distance term as we did with diffuse reﬂections what is referred to as the phong model including the distance term is written i k l max l n k l max r v α k l figure specular high lights figure specular sur face a bd this formula is computed for each light source and for each primary figure effect of shininess coefficient it might seem to make little sense either to associate a different amount of ambi ent light with each source or to allow the components for specular and diffuse lighting to be different because we cannot solve the full rendering equation we must use var ious tricks in an attempt to obtain realistic renderings consider for example an environment with many objects when we turn on a light some of that light hits a surface directly these contributions to the image can be modeled with specular and diffuse components of the source however much of the rest of the light from the source is scattered from multiple reﬂections from other objects and makes a contribution to the light received at the surface under considera tion we can approximate this term by having an ambient component associated with the source the shade that we should assign to this term depends on both the color of the source and the color of the objects in the room an unfortunate consequence of our use of approximate models to some extent the same analysis holds for diffuse light diffuse light reﬂects among the surfaces and the color that we see on a partic ular surface depends on other surfaces in the environment again by using carefully chosen diffuse and specular components with our light sources we can approximate a global effect with local calculations we have developed the phong model in object space the actual shading how ever is not done until the objects have passed through the model view and projection transformations these transformations can affect the cosine terms in the model see exercise consequently to make a correct shading calculation we must either preserve spatial relationships as vertices and vectors pass through the pipeline per haps by sending additional information through the pipeline from object space or go backward through the pipeline to obtain the required shading information the modified phong model if we use the phong model with specular reﬂections in our rendering the dot product r v should be recalculated at every point on the surface we can obtain an interesting approximation by using the unit vector halfway between the viewer vector and the light source vector h l v l v figure shows all ﬁve vectors here we have deﬁned ψ as the angle between n and l v h the halfway angle when v lies in the same plane as do l n and r we can show see exercise that φ if we replace r v with n h we avoid calculation of r however the halfway angle ψ is smaller than φ and if we use the same exponent e in n h e that we used in r v e then the size of the specular highlights will be smaller we can mitigate this problem by replacing the value of the exponent e with a value et so that n h et is closer to r v e it is clear that avoiding recalculation of r is desirable however to appreciate fully where savings can be made you should consider all the cases of ﬂat and curved surfaces near and far light sources and near and far viewers see exercise when we use the halfway vector in the calculation of the specular term we are using the blinn phong or modiﬁed phong lighting model this model is the default in systems with a ﬁxed function pipeline and is the one we will use in our ﬁrst shaders that carry out lighting color plate shows a group of utah teapots section that have been ren dered in opengl using the modiﬁed phong model note that it is only our ability to control material properties that makes the teapots appear different from one another the various teapots demonstrate how the modiﬁed phong model can create a variety of surface effects ranging from dull surfaces to highly reﬂective surfaces that look like metal figure determination of the halfway vector computation of vectors the illumination and reﬂection models that we have derived are sufﬁciently general that they can be applied to either curved or ﬂat surfaces to parallel or perspective views and to distant or near surfaces most of the calculations for rendering a scene involve the determination of the required vectors and dot products for each special case simpliﬁcations are possible for example if the surface is a ﬂat polygon the normal is the same at all points on the surface if the light source is far from the surface the light direction is the same at all points in this section we examine how the vectors are computed for the general case in section we see what additional techniques can be applied when our objects are composed of ﬂat polygons this case is especially important because most renderers including opengl render curved surfaces by approximating those surfaces with many small ﬂat polygons normal vectors for smooth surfaces the vector normal to the surface exists at every point and gives the local orientation of the surface its calculation depends on how the surface is represented mathematically two simple cases the plane and the sphere illustrate both how we compute normals and where the difﬁculties lie a plane can be described by the equation ax by cz d as we saw in chapter this equation could also be written in terms of the normal to the plane n and a point known to be on the plane as n p where p is any point x y z on the plane comparing the two forms we see that the vector n is given by a c or in homogeneous coordinates a c however suppose that instead we are given three noncollinear points that are in this plane and thus are sufﬁcient to determine it uniquely the vectors and are parallel to the plane and we can use their cross product to ﬁnd the normal n we must be careful about the order of the vectors in the cross product reversing the order changes the surface from outward pointing to inward pointing and that reversal can affect the lighting calculations some graphics systems use the ﬁrst three vertices in the speciﬁcation of a polygon to determine the normal automatically opengl does not do so but as we shall see in section forcing users to compute normals creates more ﬂexibility in how we apply our lighting model for curved surfaces how we compute normals depends on how we represent the surface in chapter we discuss three different methods for representing curves and surfaces we can see a few of the possibilities by considering how we represent a unit sphere centered at the origin the usual equation for this sphere is the implicit equation f x y z or in vector form f p p p the normal is given by the gradient vector which is deﬁned by the column matrix f x n the sphere could also be represented in parametric form in this form the x y and z values of a point on the sphere are represented independently in terms of two parameters u and v x x u v y y u v z z u v as we shall see in chapter this form is preferable in computer graphics especially for representing curves and surfaces although for a particular surface there may be multiple parametric representations one parametric representation for the sphere is x u v cos u sin v y u v cos u cos v z u v sin u as u and v vary in the range π u π π v π we get all the points on the sphere when we are using the parametric form we can obtain the normal from the tangent plane shown in figure at a point p u v x u v y u v z u v t on the surface the tangent plane gives the local orientation of the surface at a point we can derive it by taking the linear terms of the taylor series expansion of the surface at p the result is that at p lines in the directions of the vectors represented by figure tangent plane to sphere x u v p y u u z u p y v v z v lie in the tangent plane we can use their cross product to obtain the normal n p p u v for our sphere we ﬁnd that cos u sin v sin u we are interested in only the direction of n thus we can divide by cos u to obtain the unit normal to the sphere n p in section we use this result to shade a polygonal approximation to a sphere within a graphics system we usually work with a collection of vertices and the normal vector must be approximated from some set of points close to the point where the normal is needed the pipeline architecture of real time graphics systems makes this calculation difﬁcult because we process one vertex at a time and thus the graphics system may not have the information available to compute the approximate normal at a given point consequently graphics systems often leave the computation of normals to the user program in opengl we will usually set up a normal as a vertex attribute by a mechanism such as typedef normal figure a mirror normal n nx ny nz and then send the normal as needed to a vertex shader as an attribute qualiﬁed variable angle of reflection once we have calculated the normal at a point we can use this normal and the direction of the light source to compute the direction of a perfect reﬂection an ideal mirror is characterized by the following statement the angle of incidence is equal to the angle of reﬂection these angles are as pictured in figure the angle of incidence is the angle between the normal and the light source assumed to be a point source the angle of reﬂection is the angle between the normal and the direction in which the light is reﬂected in two dimensions there is but a single angle satisfying the angle condition in three dimensions however our statement is insufﬁcient to compute the required angle there is an inﬁnite number of angles satisfying our condition we must add the following statement at a point p on the surface the incoming light ray the reﬂected light ray and the normal at the point must all lie in the same plane these two conditions are sufﬁcient for us to determine r from n and l our primary interest is the direction rather than the magnitude of r however many of our rendering calculations will be easier if we deal with unit length vectors hence we assume that both l and n have been normalized such that l n we also want r if θi θr then cos θi cos θr using the dot product the angle condition is cos θi l n cos θr n r the coplanar condition implies that we can write r as a linear combination of l and n r αl βn taking the dot product with n we ﬁnd that n r αl n β l n we can get a second condition between α and β from our requirement that r also be of unit length thus r r n solving these two equations we ﬁnd that r l n n l some of the shaders we develop will use this calculation to compute a reﬂection vector for use in the application others that need the reﬂection vector only in a shader can use the glsl reflect function to compute it methods such as environment maps will use the reﬂected view vector see exercise that is used to determine what a viewer would see if she looked at a reﬂecting surface such as a highly polished sphere polygonal shading assuming that we can compute normal vectors given a set of light sources and a viewer the lighting models that we have developed can be applied at every point on a surface unfortunately even if we have simple equations to determine normal vec tors as we did in our example of a sphere section the amount of computation required can be large we have already seen many of the advantages of using polyg onal models for our objects a further advantage is that for ﬂat polygons we can signiﬁcantly reduce the work required for shading most graphics systems including figure polygonal mesh figure distant source and viewer opengl exploit the efﬁciencies possible for rendering ﬂat polygons by decomposing curved surfaces into many small ﬂat polygons consider a polygonal mesh such as that shown in figure where each poly gon is ﬂat and thus has a well deﬁned normal vector we consider three ways to shade the polygons ﬂat shading smooth or gouraud shading and phong shading flat shading the three vectors l n and v can vary as we move from point to point on a surface for a ﬂat polygon however n is constant if we assume a distant viewer v is constant over the polygon finally if the light source is distant l is constant here distant could be interpreted in the strict sense of meaning that the source is at inﬁnity the necessary adjustments such as changing the location of the source to the direction of the source could then be made to the shading equations and to their implementation distant could also be interpreted in terms of the size of the polygon relative to how far the polygon is from the source or viewer as shown in figure graphics systems or user programs often exploit this deﬁnition if the three vectors are constant then the shading calculation needs to be carried out only once for each polygon and each point on the polygon is assigned the same shade this technique is known as ﬂat or constant shading flat shading will show differences in shading among the polygons in our mesh if the light sources and viewer are near the polygon the vectors l and v will be dif figure flat shading of polygonal mesh ferent for each polygon however if our polygonal mesh has been designed to model a smooth surface ﬂat shading will almost always be disappointing because we can see even small differences in shading between adjacent polygons as shown in fig ure the human visual system has a remarkable sensitivity to small differences in light intensity due to a property known as lateral inhibition if we see an increas ing sequence of intensities as is shown in figure we perceive the increases in brightness as overshooting on one side of an intensity step and undershooting on the other as shown in figure we see stripes known as mach bands along the edges this phenomenon is a consequence of how the cones in the eye are connected to the optic nerve and there is little that we can do to avoid it other than to look for smoother shading techniques that do not produce large differences in shades at the edges of polygons smooth and gouraud shading in our rotating cube example of section we saw that the rasterizer interpolates colors assigned to vertices across a polygon suppose that the lighting calculation is made at each vertex using the material properties and the vectors n v and l computed for each vertex thus each vertex will have its own color that the rasterizer can use to interpolate a shade for each fragment note that if the light source is distant and either the viewer is distant or there are no specular reﬂections then smooth or interpolative shading shades a polygon in a constant color figure step chart perceived intensity actual intensity figure perceived and actual intensities at an edge n figure normals near interior vertex if we consider our mesh the idea of a normal existing at a vertex should cause concern to anyone worried about mathematical correctness because multiple poly gons meet at interior vertices of the mesh each of which has its own normal the normal at the vertex is discontinuous although this situation might complicate the mathematics gouraud realized that the normal at the vertex could be deﬁned in such a way as to achieve smoother shading through interpolation consider an interior ver tex as shown in figure where four polygons meet each has its own normal in gouraud shading we deﬁne the normal at a vertex to be the normalized average of the normals of the polygons that share the vertex for our example the vertex normal is given by n from an opengl perspective gouraud shading is deceptively simple we need only to set the vertex normals correctly often the literature makes no distinction between smooth and gouraud shading however the lack of a distinction causes a problem how do we ﬁnd the normals that we should average together if our program is lin ear specifying a list of vertices and other properties we do not have the necessary information about which polygons share a vertex what we need of course is a data structure for representing the mesh traversing this data structure can generate the vertices with the averaged normals such a data structure should contain at a mini mum polygons vertices normals and material properties one possible structure is the one shown in figure the key information that must be represented in the data structure is which polygons meet at each vertex color plates and show the shading effects available in opengl in color plate there is a single light source but each polygon has been rendered with a single shade constant shading computed using the phong model in color plate figure mesh data structure na figure edge normals normals have been assigned to all the vertices opengl has then computed shades for the vertices and has interpolated these shades over the faces of the polygons color plate contains another illustration of the smooth shading provided by opengl we used this color cube as an example in both chapters and and the programs are in appendix a the eight vertices are colored black white red green blue cyan magenta and yellow once smooth shading is enabled opengl interpolates the colors across the faces of the polygons automatically phong shading even the smoothness introduced by gouraud shading may not prevent the appear ance of mach bands phong proposed that instead of interpolating vertex intensities as we do in gouraud shading we interpolate normals across each polygon consider a polygon that shares edges and vertices with other polygons in the mesh as shown in figure we can compute vertex normals by interpolating over the normals of the polygons that share the vertex next we can use interpolation as we did in chap ter to interpolate the normals over the polygon consider figure we can use the interpolated normals at vertices a and b to interpolate normals along the edge between them nc α α na αnb we can do a similar interpolation on all the edges the normal at any interior point can be obtained from points on the edges by n α β β nc βnd once we have the normal at each point we can make an independent shading calcu lation usually this process can be combined with rasterization of the polygon until recently phong shading could only be carried out off line because it requires the in terpolation of normals across each polygon in terms of the pipeline phong shading na figure interpolation of normals in phong shading requires that the lighting model be applied to each fragment hence the name per fragment shading we will implement phong shading through a fragment shader approximation of a sphere by recursive subdivision we have used the sphere as an example curved surface to illustrate shading calcu lations however the sphere is not an object supported within opengl so we will generate approximations to a sphere using triangles through a process known as re cursive subdivision a technique we introduced in chapter for constructing the sierpinski gasket recursive subdivision is a powerful technique for generating ap proximations to curves and surfaces to any desired level of accuracy the sphere approximation provides a basis for us to write simple programs that illustrate the interactions between shading parameters and polygonal approximations to curved surfaces our starting point is a tetrahedron although we could start with any regular polyhedron whose facets could be divided initially into triangles the regular tetra hedron is composed of four equilateral triangles determ ined by four vertices we st art with the four vertices and all four lie on the unit sphere centered at the origin exer cise suggests one method for ﬁnding these points we get a ﬁrst approximation by drawing a wireframe for the tetrahedron we specify the four vertices as follows v 471405 we then put the vertices into an array data so we can display the triangles using line loops each triangle adds three points to this array using the function static int k void triangle a b c data k a the regular icosahedron is composed of equilateral triangles it makes a nice starting point for generating spheres see k data k b k data k c k all the data for the tetrahedron is put into data as follows void tetrahedron triangle v v v triangle v v v triangle v v v triangle v v v the order of vertices obeys the right hand rule so we can convert the code to draw shaded polygons with little difﬁculty if we add the usual code for initialization set ting up a vertex buffer object and drawing the array our program will generate an image such as that in figure a simple regular polyhedron but a poor approxi mation to a sphere we can get a closer approximation to the sphere by subdividing each facet of the tetrahedron into smaller triangles subdividing into triangles will ensure that all the new facets will be ﬂat there are at least three ways to do the subdivision as shown in figure we can bisect each of the angles of the triangle and draw the three bisectors which meet at a common point thus generating three new triangles we can also compute the center of mass centroid of the vertices by simply averaging them and then draw lines from this point to the three vertices again generating three triangles however these techniques do not preserve the equilateral triangles that make up the regular tetrahedron instead recalling a construction for the sierpinski gasket of chapter we can connect the bisectors of the sides of the triangle forming four equilateral triangles as shown in figure c we use this technique for our example after we have subdivided a facet as just described the four new triangles will still be in the same plane as the original triangle we can move the new vertices that figure tetrahedron a b c figure subdivision of a triangle by a bisecting angles b computing the centroid and c bisecting sides we created by bisection to the unit sphere by normalizing each bisected vertex using the normalization function normalize in vec h we can now subdivide a single triangle deﬁned by the vertices a b and c normalize a b normalize a c normalize b c triangle a triangle c triangle b triangle we can use this code in our tetrahedron routine to generate triangles rather than but we would rather be able to repeat the subdivision process n times to generate successively closer approximations to the sphere by calling the subdivision routine recursively we can control the number of subdivisions first we make the tetrahedron routine depend on the depth of recursion by adding an argument n void tetrahedron int n v v v n v v v n v v v n v v v n the function calls itself to subdivide further if n is greater than zero but generates triangles if n has been reduced to zero here is the code void a b c int n if n normalize v a v b normalize v a v c normalize v b v c a n c n b n v3 n else triangle a b c figure shows an approximation to the sphere drawn with this code we now turn to adding lighting and shading to our sphere approximation specifying lighting parameters for many years the blinn phong lighting model was the standard in computer graphics it was implemented in hardware and was speciﬁed as part of the opengl ﬁxed functionality pipeline with the present emphasis on shaders we are free to im plement other lighting models with no loss of efﬁciency we can also choose where to apply a light model consequently we must specify a group of lighting and mate rial parameters and then either use them in the application code or send them to the shaders light sources in section we introduced four types of light sources ambient point spotlight and distant however because spotlights and distant light sources can be derived from a point source we will focus on point sources and ambient light an ideal point source emits light uniformly in all directions to get a spotlight from a point source we need only limit the directions of the point source and make the light emissions follow a desired proﬁle to get a distant source from a point source we need to allow the location of the source to go to inﬁnity so the position of the source becomes the direction of the source note that this argument is similar to the argument that parallel viewing is the limit of perspective viewing as the center of projection moves to inﬁnity as we argued in deriving the equations for parallel projections we will ﬁnd it easier to derive the equations for lighting with distant sources directly rather than by taking limits although the contribution of ambient light is the same everywhere in a scene ambient light is dependent on the sources in the environment for example consider a closed room with a single white point source when the light is turned off there is no light in the room of any kind when the light is turned on at any point in the room that can see the light source there is a contribution from the light hitting surfaces directly contributing to the diffuse or specular reﬂection we see at that point there is also a contribution from the white light bouncing off of multiple surfaces in the room and giving a contribution that is almost the same at every point in the room it is this latter contribution that we call ambient light its color depends not only on the color of the source but also on the reﬂective properties of the surfaces in the room thus if the room has red walls we would expect the ambient component to have a dominant red component however the existence of an ambient component to the shade we see on a surface is ultimately tied to the light sources in the environment and hence becomes part of the speciﬁcation of the sources for every light source we must specify its color and either its location or its direction as in section the color of a source will have three components diffuse specular and ambient that we can specify for a single light as figure sphere approx imations using subdivision we can specify the position of the light as follows for a point source its position will be in homogeneous coordinates so a light might be speciﬁed as if the fourth component is changed to zero as in the source becomes a directional source in the direction for positional light sources we may also want to account for the attenuation of light received due to its distance from the source although for an ideal source the attenuation is inversely proportional to the square of the distance d we can gain more ﬂexibility by using the distance attenuation model f d a bd which contains constant linear and quadratic terms we can use three ﬂoats for these values float and use them in the application or send them to the shaders as uniform variables we can also convert a positional source to a spotlight by setting its direction the angle of the cone or the spotlight cutoff and the drop off rate or spotlight exponent these three parameters can be speciﬁed by three ﬂoats materials material properties should match up directly with the supported light sources and with the chosen reﬂection model we may also want the ﬂexibility to specify different material properties for the front and back faces of a surface for example we might specify ambient diffuse and specular reﬂectivity coef ﬁcients ka kd ks for each primary color through three colors using either rgb or rgba colors as ambient diffuse specular or assuming the surface is opaque ambient diffuse specular here we have deﬁned a small amount of white ambient reﬂectivity yellow diffuse properties and white specular reﬂections note that often the diffuse and specular reﬂectivity are the same for the specular component we also need to specify its shininess float shininess if we have different reﬂectivity properties for the front and back faces we can also specify three additional parameters back_specular that can be used to render the back faces we also want to allow for scenes in which a light source is within the view volume and thus might be visible for example for an outdoor night scene we might see the moon in an image we could model the moon with a simple polygonal approximation to a circle however when we render the moon its color should be constant and not be affected by other light sources we can create such effects by including an emissive component that models self luminous sources this term is unaffected by any of the light sources and it does not affect any other surfaces it adds a ﬁxed color to the surfaces and is speciﬁed in a manner similar to other material properties for example emission specﬁes a small amount of blue green cyan emission from an application programmer perspective we would like to have material properties that we can specify with a single function call we can achieve this goal by deﬁning material objects in the application using structs or classes for example consider the typedef typedef struct materialstruct ambient diffuse specular emission float shininess materialstruct we can now deﬁne materials by code such as materialstruct brassmaterials 78 99 81 and access this code through a pointer currentmaterial brassmaterials implementing a lighting model thus far we have only looked at parameters that we might use in a light model we have yet to build a particular model nor have we worried about where to apply a lighting model we focus on a simple version of the blinn phong model using a single point source because light from multiple sources is additive we can repeat the calculation for each source and add up the individual contributions we have three choices as to where we do the calculation in the application in the vertex shader or in the fragment shader although the basic model can be the same for each there will major differences both in efﬁciency and appearance depending on where the calculation is done applying the lighting model in the application we have used two methods to assign colors to ﬁlled triangles in the ﬁrst we sent a single color for each polygon to the shaders as a uniform variable and used this color for each fragment in the second we assigned a color to each vertex as a vertex attribute the rasterizer then interpolated these vertex colors across the polygon both these approaches can be applied to lighting in constant or ﬂat shading we apply a lighting model once for each polygon and use the computed color for the entire polygon in the interpolative shading we apply the model at each vertex to compute a vertex color attribute the vertex shader can then output these colors and the rasterizer will interpolate them to determine a color for each fragment let do a simple example with ambient diffuse and specular lighting assume that the following parameters have been speciﬁed for a single point light source also assume that there is a single material whose parameters are reflect_specular the color we need to compute is the sum of the ambient diffuse and specular contributions ambient diffuse specular ambient diffuse specular each component of the ambient term is the product of the corresponding terms from the ambient light source and the material reﬂectivity we can use the function product a b return a b a b a b a b hence ambient product we need the normal to compute the diffuse term because we are working with triangles we have the three vertices and these vertices determine a unique plane and its normal suppose that we have three vertices and the cross product of and is perpendicular to the plane determined by the three vertices thus we get the desired unit normal as follows n normalize cross note that the direction of the normal depends on the order of the vertices and assumes we are using the right hand rule to determine an outward face next we need to take the dot product of the unit normal with the vector in the direction of the light source there are four cases we must consider constant shading with a distant source interpolative shading with a distant source constant shading with a ﬁnite source interpolative shading with a ﬁnite source for constant shading we only need to compute a single diffuse color for each triangle for a distant source we have the direction of the source that is the same for all points on the triangle hence we can simply take the dot product of the unit normal with a normalized source direction the diffuse contribution is then diffuse product dot n normalize there is one additional step we should take the diffuse term only makes sense if the dot product is nonnegative so we must modify the calculation to diffuse float d dot n normalize if d diffuse product reflect_ambient d else diffuse for a distant light source the diffuse contribution at each vertex is identical so we need do only one diffuse calculation per polygon and thus interpolative and constant diffuse shading are the same for a ﬁnite or near source we have two choices we either compute a single diffuse contribution for the entire polygon and use constant shading or we compute the diffuse term at each vertex and use interpolative shading because we are working with triangles the normal is the same at each vertex but with a near source the vector from any point on the polygon to the light source will be different if we use a single color we can use the point at the center of the triangle to compute the direction v v2 v float d dot n normalize if d diffuse product reflect_ambient d else diffuse the calculation for the specular term appears to be similar to the calculation for the diffuse but there is one tricky issue we need to compute the halfway vector for a distance source the light position becomes a direction so half normalize light_position the view direction is a vector from a point on the surface to the eye the default is that the camera is at the origin in object space so for a vertex v the vector is origin v origin thus even though each triangle has a single normal there is a different halfway vector for each vertex and consequently the specular term will vary across the surface as the rasterizer interpolates the vertex shades the specular term for vertex v can be computed as specular float dot half n if specular pow product material_specular else specular where the expression exp log evaluates to the power efficiency for a static scene the lighting computation is done once so we can send the vertex positions and vertex colors to the gpu once consider what happens if we add lighting to our rotating cube program each time the cube rotates about a coordinate axis the normal to four of the faces changes as does the position of each of the six vertices hence we must recompute the diffuse and specular components at each of the vertices if we do all the calculations in the cpu both the vertex positions and colors must then be sent to the gpu for large data sets this process is extremely inefﬁcient not only are we doing a lot of computation on the cpu but we are also causing a potential bottleneck by sending so much vertex data to the gpu consequently we will almost always want to do lighting calculation in the shaders before examining shaders for lighting there are a few other efﬁciency measures we can employ either in the application or in a shader we can obtain many efﬁcien cies if we assume that either or both of the viewer and the light source are far from the polygon we are rendering hence even if the source is a point source with a ﬁnite lo cation it might be far enough away that the distances from the vertices to the source are all about the same in this case the diffuse term at each vertex would be identical and we would need do only one calculation per polygon note that a deﬁnition of far and near in this context depends both on the distance to the light source and the size of the polygon a small polygon will not show much variation in the diffuse compo nent even if the source is fairly close to the polygon the same argument holds for the specular term when we consider the distance between vertices and the viewer we can add parameters that allow the application to specify if it wants to use these simpliﬁed calculations in chapter we saw that a surface has both a front face and a back face for polygons we determine front and back by the order in which the vertices are spec iﬁed using the right hand rule for most objects we see only the front faces so we are not concerned with how opengl shades the back facing surfaces for example for convex objects such as a sphere or a parallelepiped figure the viewer can never see a back face regardless of where she is positioned however if we remove a side from a cube or slice the sphere as shown in figure a properly placed viewer may see a back face thus we must shade both the front and back faces correctly in many situations we can ignore all back faces by either culling them out in the appli cation or by not rendering any face whose normal does not point toward the viewer if we render back faces they may have different material properties than the front faces so we must specify a set of back face properties figure shading of con vex objects figure visible back surfaces light sources are special types of geometric objects and have geometric at tributes such as position just like polygons and points hence light sources can be affected by transformations we can specify them at the desired position or specify them in a convenient position and move them to the desired position by the model view transformation the basic rule governing object placement is that vertices are converted to eye coordinates by the model view transformation in effect at the time the vertices are deﬁned thus by careful placement of the light source speciﬁcations relative to the deﬁnition of other geometric objects we can create light sources that remain stationary while the objects move light sources that move while objects re main stationary and light sources that move with the objects we also have choices as to which coordinate system to use for lighting computa tions for now we will do our lighting calculations in object coordinates depending on whether or not the light source or objects are moving it may be more efﬁcient to use eye coordinates later when we add texture mapping to our skills we will intro duce lighting methods that will use local coordinate systems lighting in the vertex shader when we presented transformations we saw that a transformation such as the model view transformation could be carried out either in the application or the vertex shader but for most applications it was far more efﬁcient to implement the transformation in the shader the same is true for lighting to implement lighting in the vertex shader we must carry out three steps first we must choose a lighting model do we use the blinn phong or some other model do we include distance attenuation do we want two sided lighting once we make these decisions we can write a vertex shader to implement the model finally we have to transfer the necessary data to the shader some data can be trans ferred using uniform variables and other data can be transferred as vertex attributes let go through the process for the model we just developed the blinn phong model without distance attenuation with a single point light source we can transfer the ambient diffuse and specular components of the source plus its position as uniform variables we can do likewise for the material properties rather than writing the application code ﬁrst because we know how to transfer information to a shader ﬁrst we will write the vertex shader the vertex shader must output a vertex position in clip coordinates and a vertex color to the rasterizer if we send a model view matrix and a projection matrix to the shader then the computation of the vertex position is identical to our examples from chapters and hence this part of the code will look something like in vposition uniform modelview uniform projection void main projection modelview vposition the output color is the sum of the ambient diffuse and specular contributions out color ambient diffuse specular color ambient diffuse specular so the part we must address is computation of these three terms rather than sending all the reﬂectivity and light colors separately to the shader we send only the product term for each contribution thus in the ambient computa tion we use the products of the red green and blue ambient light with the red green and blue ambient reﬂectivities we can compute these products and send them to the shader as the uniform vector in ambientproduct we can do the same for the diffuse and specular products uniform diffuseproduct specularproduct the ambient term is then simply ambient ambientproduct the diffuse term requires a normal for each vertex because triangles are ﬂat the normal is the same for each vertex in a triangle so we can send the normal to the shader as a uniform variable we can use the normalize function to get a unit length normal from the type we used in the application uniform normal n normalize normal xyz the unit vector in the direction of the light source is given by l normalize lightposition vposition xyz the diffuse term is then diffuse max dot l n diffuseproduct the specular term is computed in a similar manner because the viewer is at the origin in object coordinates the normalized vector in the direction of the viewer is in section we will consider methods that assign a different normal to each vertex of a polygon e normalize vposition xyz and the halfway vector is h normalize l e the specular term is then specular pow max dot n h shininess specularproduct however if the light source is behind the surface there cannot be a specular term so we add a simple test specular max pow max dot n h shininess specularproduct here is the full shader in vposition in normal out color uniform ambientproduct diffuseproduct specularproduct uniform modelview uniform projection uniform lightposition uniform float shininess void main ambient diffuse specular projection modelview vposition n normalize normal xyz l normalize lightposition xyz modelview vposition xyz e normalize modelview vposition xyz h normalize l e float kd max dot l n float ks pow max dot n h shininess ambient ambientproduct diffuse kd diffuseproduct specular max pow max dot n h shininess specularproduct color ambient diffuse specular xyz because the colors are set in the vertex shader the simple fragment shader that we have used previously in color void main color will take the interpolated colors from the rasterizer and assign them to fragments let return to the cube shading example the main change we have to make is to set up uniform variables for the light and material parameters thus for the ambient component we might have an ambient light term and an ambient reﬂectivity given as we compute the ambient product color4 ambient_product product light_ambient material_ambient we get these values to the shader as follows gluint glgetuniformlocation program ambientproduct ambient_product we can do the same for the rest of the uniform variables in the vertex shader note that the normal vector depends on more than one vertex and so cannot be computed in the shader because the shader has the position information only for the vertex that initiated its execution there is an additional issue that has to do with the fact that the cube is rotating as the cube rotates the positions of all the vertices and all the normals to the surface change when we ﬁrst developed the program we applied the rotation transforma tion in the application and each time that the rotation matrix was updated we resent the vertices to the gpu later we argued that it was far more efﬁcient to send the rotation matrix to the shader and let the transformation be carried out in the gpu the same is true with this example we can send a projection matrix and a model view matrix as uniform variables this example has only one object the cube and thus the rotation matrix and the model view matrix are identical if we want to apply the rotation to the normal vector in the gpu then we need to make the following change to the shader nn modelview normal n normalize nn xyz the complete program is in appendix a however if there are multiple objects in the scene or the viewing parameters change we have to be a little careful with this construct suppose that there is a sec ond nonrotating cube in the scene and we also use nondefault viewing parameters now we have two different model view matrices one for each cube one is constant and the other is changing as one of the cubes rotates what is really changing is the modeling part of the model view matrix and not the viewing part that positions the camera we can handle this complication in a number of ways we could compute the two model view matrices in the application and send them to the vertex shader each time there is a rotation we could also use separate modeling and viewing trans formations and send only the modeling matrix the rotation matrix to the shader after initialization we would then form the model view matrix in the shader we could also just send the rotation angles to the shader and do all the work there if the light source is also changing its position we have even more options figure shaded sphere model shading of the sphere model the rotating cube is a simple example to demonstrate lighting but because there are only six faces and they meet at right angles it is not a good example for testing the smoothness of a lighting model consider instead the sphere model that we developed in section although the model comprises many small triangles unlike the cube we do not want to see the edges rather we want to shade the polygons so that we cannot see the edges where triangles meet and the smoother the shading the fewer polygons we need to model the sphere to shade the sphere model we can start with the same shaders we used for the rotating cube the differences are in the application program we replace the generation of the cube with the tetrahedron subdivision from section adding the computation of the normals which are sent to the vertex shader as attribute qualiﬁed variables the result is shown in figure note that even as we increase the number of subdivisions so that the interiors of the spheres appear smooth we can still see edges of polygons around the outside of the sphere image this type of outline is called a silhouette edge the differences in this example between constant shading and smooth shading are minor because each triangle is ﬂat the normal is the same at each vertex if the source is far from the object the diffuse component will be constant for each triangle likewise if the camera is far from the viewer the specular term will be constant for each triangle however because two adjacent triangles will have different normals and thus are shaded with different colors we still can see the lack of smoothness even if we create many triangles one way to get an idea of how smooth a display we can get with relatively few triangles is to use the actual normals of the sphere for each vertex in the approxima tion in section we found that for the sphere centered at the origin the normal at a point p is simply p hence in the triangle function the position of a vertex gives the normal void triangle a b c normals k a data k a k normals k b data k b k normals k c data k c k the results of this deﬁnition of the normals are shown in figure and color plate although using the true normals produces a rendering more realistic than ﬂat shading the example is not a general one because we have used normals that are known analytically we also have not provided a true gouraud shaded image sup pose we want a gouraud shaded image of our approximate sphere at each vertex we need to know the normals of all polygons incident at the vertex our code does not have a data structure that contains the required information try exercises and in which you construct such a structure note that six polygons meet at a vertex created by subdivision whereas only three polygons meet at the original vertices of the tetrahedron figure shading of the sphere with the true normals per fragment lighting there is another option we can use to obtain a smoother shading we can do the light ing calculation on a per fragment basis rather than on a per vertex basis when we did all our lighting calculations in the vertex shader visually there was no advantage over doing the same computation in the application and then sending the computed vertex colors to the vertex shader which would then pass them on to the rasterizer thus whether we did lighting in the vertex shader or in the application the rasterizer interpolated the same vertex colors to obtain fragment colors with a fragment shader we can do an independent lighting calculation for each fragment the fragment shader needs to get the interpolated values of the normal vector light source position and eye position from the rasterizer the vertex shader can compute these values and output them to the rasterizer in addition the vertex shader must output the vertex position in clip coordinates here is the vertex shader in vposition in normal uniform modelview uniform lightposition uniform projection out n out l out e void main projection modelview vposition n normal xyz l lightposition xyz vposition xyz if lightposition w l lightposition xyz e vposition xyz the fragment shader can now apply the blinn phong lighting model to each fragment using the light and material parameters passed in from the application as uniform variables and the interpolated vectors from the rasterizer the following shader cor responds to the vertex shader we used in the previous example uniform ambientproduct diffuseproduct specularproduct uniform modelview uniform lightposition uniform float shininess in n in l in e void main nn normalize n ee normalize e ll normalize l ambient diffuse specular h normalize ll ee float kd max dot ll nn kd dot ll nn float ks pow max dot nn h shininess ambient ambientproduct diffuse kd diffuseproduct if dot ll nn specular else specular ks specularproduct ambient diffuse specular xyz note that we normalize vectors in the fragment shader rather than in the vertex shaders if we were to normalize a variable such as the normals in the vertex shader it would not guarantee that the interpolated normals produced by the rasterizer would have the unit magnitude needed for the lighting computation nonphotorealistic shading programmable shaders make it possible to not only incorporate more realistic light ing models in real time but also to create interesting nonphotorealistic effects two such examples are the use of only a few colors and emphasizing the edges in objects both these effects are techniques that we might want to use to obtain a cartoonlike effect in an image suppose that we use only two colors in a vertex shader yellow red we could then switch between the colors based for example on the magnitude of the diffuse color using the light and normal vectors we could assign colors as if dot lightv norm else although we could have used two colors in simpler ways by using the diffuse color to determine a threshold the color of the object changes with its shape and the position of the light source we can also try to draw the silhouette edge of an object one way to identify such edges is to look at sign changes in dot lightv norm this value should be positive for any vertex facing the viewer and negative for a vertex pointed away from the viewer thus we can test for small values of this value and assign a color such as black to the vertex black if abs dot viewv norm glfrontcolor global illumination there are limitations imposed by the local lighting model that we have used con sider for example an array of spheres illuminated by a distant source as shown in figure a the spheres close to the source block some of the light from the source from reaching the other spheres however if we use our local model each sphere is shaded independently all appear the same to the viewer figure b in addi tion if these spheres are specular some light is scattered among spheres thus if the spheres were very shiny we should see the reﬂection of multiple spheres in some of the spheres and possibly even the multiple reﬂections of some spheres in themselves our lighting model cannot handle this situation nor can it produce shadows except by using the tricks for some special cases as we saw in chapter all of these phenomena shadows reﬂections blockage of light are global effects and require a global lighting model although such models exist and can be quite elegant in practice they are incompatible with the pipeline model with the l a l b figure array of shaded spheres a global lighting model b local lighting model cop figure polygon blocked from light source pipeline model we must render each polygon independently of the other polygons and we want our image to be the same regardless of the order in which the application produces the polygons although this restriction limits the lighting effects that we can simulate we can render scenes very rapidly there are alternative rendering strategies including ray tracing and radiosity that can handle global effects each is best at different lighting conditions ray trac ing starts with the synthetic camera model but determines for each projector that strikes a polygon if that point is indeed illuminated by one or more sources before computing the local shading at each point thus in figure we see three poly gons and a light source the projector shown intersects one of the polygons a local renderer might use the modiﬁed phong model to compute the shade at the point of intersection the ray tracer would ﬁnd that the light source cannot strike the point of intersection directly but that light from the source is reﬂected from the third polygon and this reﬂected light illuminates the point of intersection in chapter we shall show how to ﬁnd this information and make the required calculations a radiosity renderer is based upon energy considerations from a physical point of view all the light energy in a scene is conserved consequently there is an energy balance that accounts for all the light that radiates from sources and is reﬂected by various surfaces in the scene a radiosity calculation thus requires the solution of a large set of equations involving all the surfaces as we shall see in chapter a ray tracer is best suited to a scene consisting of highly reﬂective surfaces whereas a radiosity renderer is best suited for a scene in which all the surfaces are perfectly diffuse although a pipeline renderer cannot take into account many global phenomena exactly this observation does not mean we cannot produce realistic imagery with opengl or another api that is based upon a pipeline architecture what we can do is use our knowledge of opengl and of the effects that global lighting produces to approximate what a global renderer would do for example our use of projective shadows in chapter shows that we can produce simple shadows many of the most exciting advances in computer graphics over the past few years have been in the use of pipeline renderers for global effects we will study many such techniques in the next few chapters including mapping methods multipass rendering and transparency summary and notes we have developed a lighting model that ﬁts well with our pipeline approach to graphics with it we can create a variety of lighting effects and we can employ different types of light sources although we cannot create the global effects of a ray tracer a typical graphics workstation can render a polygonal scene using the modiﬁed phong reﬂection model and smooth shading in the same amount of time as it can render a scene without shading from the perspective of an application program adding shading requires setting parameters that describe the light sources and materials and can be implemented with programmable shaders in spite of the limitations of the local lighting model that we have introduced our simple renderer performs remarkably well it is the basis of the reﬂection model supported by most apis programmable shaders have changed the picture considerably not only can we create new methods of shading each vertex we can use fragment shaders to do the lighting calculation for each fragment thus avoiding the need to interpolate colors across each polygon methods such as phong shading that were not possible within the standard pipeline can now be programmed by the user and will execute in about the same amount of time as the modiﬁed phong shader it is also possible to create a myriad of new shading effects the recursive subdivision technique that we used to generate an approximation to a sphere is a powerful one that will reappear in various guises in chapter where we use variants of this technique to render curves and surfaces it will also arise when we introduce modeling techniques that rely on the self similarity of many natural objects this chapter concludes our development of polygonal based graphics you should now be able to generate scenes with lighting and shading techniques for creating even more sophisticated images such as texture mapping and compositing involve using the pixel level capabilities of graphics systems topics that we consider in chapter now is a good time for you to write an application program experiment with various lighting and shading parameters try to create light sources that move either independently or with the objects in the scene you will probably face difﬁculties in producing shaded images that do not have small defects such as cracks between polygons through which light can enter many of these problems are artifacts of small numerical errors in rendering calculations there are many tricks of the trade for mitigating the effects of these errors some you will discover on your own others are given in the suggested readings for this chapter we turn to rasterization issues in chapter although we have seen some of the ways in which the different modules in the rendering pipeline function we have not yet seen the details as we develop these details you will see how the pieces ﬁt together such that each successive step in the pipeline requires only a small increment of work suggested readings the use of lighting and reﬂection in computer graphics has followed two parallel paths the physical and the computational from the physical perspective kajiya rendering equation describes the overall energy balance in an environment and requires knowledge of the reﬂectivity function for each surface reﬂection mod els such as the torrance sparrow model and cook torrance model are based on modeling a surface with small planar facets see hall and foley for discussions of such models phong is credited with putting together a computational model that in cluded ambient diffuse and specular terms the use of the halfway vector was ﬁrst suggested by blinn the basic model of transmitted light was used by whit ted it was later modiﬁed by heckbert and hanrahan gouraud introduced interpolative shading the opengl programming guide contains many good hints on effec tive use of opengl rendering capabilities and discusses the ﬁxed function lighting pipeline that uses functions that have been deprecated in shader based opengl exercises most graphics systems and apis use the simple lighting and reﬂection models that we introduced for polygon rendering describe the ways in which each of these models is incorrect for each defect give an example of a scene in which you would notice the problem often when a large polygon that we expect to have relatively uniform shading is shaded by opengl it is rendered brightly in one area and more dimly in others explain why the image is uneven describe how you can avoid this problem in the development of the phong reﬂection model why do we not consider light sources being obscured from the surface by other surfaces in our reﬂec tion model how should the distance between the viewer and the surface enter the render ing calculations we have postulated an rgb model for the material properties of surfaces give an argument for using a subtractive color model instead find four points equidistant from one another on a unit sphere these points determine a tetrahedron hint you can arbitrarily let one of the points be at and let the other three be in the plane y d for some positive value of d show that if v lies in the same plane as l n and r then the halfway angle satisﬁes φ what relationship is there between the angles if v is not coplanar with the other vectors consider all the combinations of near or far viewers near or far light sources ﬂat or curved surfaces and diffuse and specular reﬂections for which cases can you simplify the shading calculations in which cases does the use of the halfway vector help explain your answers construct a data structure for representing the subdivided tetrahedron tra verse the data structure such that you can gouraud shade the approximation to the sphere based on subdividing the tetrahedron repeat exercise but start with an icosahedron instead of a tetrahedron construct a data structure for representing meshes of quadrilaterals write a program to shade the meshes represented by your data structure write a program that does recursive subdivisions on quadrilaterals and quadri lateral meshes consider two materials that meet along a planar boundary suppose that the speed of light in the two materials are and v2 show that snell law is a statement that light travels from a point in one material to a point in the second material in the minimum time show that the halfway vector h is at the angle at which a surface must be oriented so that the maximum amount of reﬂected light reaches the viewer although we have yet to discuss frame buffer operations you can start con structing a ray tracer using a single routine of the form x y color that places the value of color either an rgb color or an intensity at the pixel located at x y in the frame buffer write a pseudocode routine ray that recursively traces a cast ray you can assume that you have a function available that will intersect a ray with an object consider how to limit how far the original ray will be traced if you have a pixel writing routine available on your system write a ray tracer that will ray trace a scene composed of only spheres use the mathematical equations for the spheres rather than a polygonal approximation add light sources and shading to the maze program in exercise using the sphere generation program in appendix a as a starting point con struct an interactive program that will allow you to position one or more light sources and to alter material properties use your program to try to generate images of surfaces that match familiar materials such as various metals plas tic and carbon as geometric data pass through the viewing pipeline a sequence of rotations translations scalings and a projection transformation is applied to the vectors that determine the cosine terms in the phong reﬂection model which if any of these operations preserve the angles between the vectors what are the implications of your answer for implementation of shading estimate the amount of extra calculations required for phong shading as com pared to gouraud shading take into account the results of exercise if the light position is altered by an afﬁne transformation such as a modeling transformation how must the normal vector be transformed so that the angle between the normal and the light vector remains unchanged redo the implementation of the blinn phong shading model so the calcula tions are carried out in eye coordinates generalize the shadow generation algorithm section to handle ﬂat sur faces at arbitrary orientations convert the shadow generation algorithm section to an algorithm for distant sources hint the perspective projection should become a parallel projection compare the shadow generation algorithm of section to the generation of shadows by a global rendering method what types of shadows can be generated by one method but not the other consider a highly reﬂective sphere centered at the origin with a unit radius if a viewer is located at p describe the points she would see reﬂected in the sphere at a point on its surface from vertices to fragments chapter e now turn to the next steps in the pipeline clipping rasterization and hidden surface removal although we have yet to consider some major parts of opengl that are available to the application programmer including discrete primitives texture mapping and curves and surfaces there are several reasons for considering these topics at this point first you may be wondering how your pro grams are processed by the system that you are using how lines are drawn on the screen how polygons are ﬁlled and what happens to primitives that lie outside the viewing volumes deﬁned in your program second our contention is that if we are to use a graphics system efﬁciently we need to have a deeper understanding of the implementation process which steps are easy and which tax our hardware and soft ware third our discussion of implementation will open the door to new capabilities that are supported by the latest hardware learning implementation involves studying algorithms as when we study any algorithm we must be careful to consider such issues as theoretical versus practical performance hardware versus software implementations and the speciﬁc character istics of an application although we can test whether an opengl implementation works correctly in the sense that it produces the correct pixels on the screen there are many choices for the algorithms employed we focus on the basic operations that are both necessary to implement a standard api and required whether the rendering is done by a pipeline architecture or by another method such as ray tracing conse quently we present a variety of the basic algorithms for each of the principal tasks in an implementation in this chapter we are concerned with the basic algorithms that are used to im plement the rendering pipeline employed by opengl we shall focus on three issues clipping rasterization and hidden surface removal clipping involves eliminating objects that lie outside the viewing volume and thus cannot be visible in the image rasterization produces fragments from the remaining objects these fragments can contribute to the ﬁnal image hidden surface removal determines which fragments correspond to objects that are visible namely those that are in the view volume and are not blocked from view by other objects closer to the camera vertices pixels figure high level view of the graphics process basic implementation strategies let us begin with a high level view of the implementation process in computer graphics we start with an application program and we end with an image we can again consider this process as a black box figure whose inputs are the vertices and states deﬁned in the program geometric objects attributes camera speciﬁcations and whose output is an array of colored pixels in the frame buffer within the black box we must do many tasks including transformations clip ping shading hidden surface removal and rasterization of the primitives that can appear on the display these tasks can be organized in a variety of ways but regard less of the strategy that we adopt we must always do two things we must pass every geometric object through the system and we must assign a color to every pixel in the color buffer that is displayed suppose that we think of what goes into the black box in terms of a single program that carries out the entire process this program takes as input a set of vertices specifying geometric objects and produces as output pixels in the frame buffer because this program must assign a value to every pixel and must process every geometric primitive and every light source we expect this program to contain at least two loops that iterate over these basic variables if we wish to write such a program then we must immediately address the following question which variable controls the outer loop the answer we choose determines the ﬂow of the entire implementation process there are two fundamental strategies often called the image oriented and the object oriented approaches in the object oriented approach the outer loop is over the objects we can think of the program as controlled by a loop of this form for render object a pipeline renderer ﬁts this description vertices are deﬁned by the program and ﬂow through a sequence of modules that transforms them colors them and deter mines whether they are visible a polygon might ﬂow through the steps illustrated in figure note that after a polygon passes through geometric processing the ras terization of this polygon can potentially affect any pixels in the frame buffer most implementations that follow this approach are based on construction of a rendering pipeline containing hardware or software modules for each of the tasks data ver tices ﬂow forward through the system in the past the major limitations of the object oriented approach were the large amount of memory required and the high cost of processing each object indepen y y y projection rasterizing x x frame buffer z z figure object oriented approach dently any geometric primitive that emerges from the geometric processing can po tentially affect any set of pixels in the frame buffer thus the entire color buffer and various other buffers such as the depth buffer used for hidden surface removal must be of the size of the display and must be available at all times before memory became both inexpensive and dense this requirement was considered to be a serious problem now various pipelined geometric processors are available that can process tens of millions of polygons per second in fact precisely because we are doing the same operations on every primitive the hardware to build an object based system is fast and relatively inexpensive with many of the functions implemented with special purpose chips today the main limitation of object oriented implementations is that they can not handle most global calculations because each geometric primitive is processed independently and in an arbitrary order complex shading effects that involve multiple geometric objects such as reﬂections cannot be handled except by approx imate methods the major exception is hidden surface removal where the z buffer is used to store global information image oriented approaches loop over pixels or rows of pixels called scanlines that constitute the frame buffer in pseudocode the outer loop of such a program is of the following form for pixel for each pixel we work backward trying to determine which geometric prim itives can contribute to its color the advantages of this approach are that we need only limited display memory at any time and that we can hope to generate pixels at the rate and in the order required to refresh the display because the results of most calculations do not differ greatly from pixel to pixel or scanline to scanline we can use this coherence in our algorithms by developing incremental forms for many of the steps in the implementation the main disadvantage of this approach is that unless we ﬁrst build a data structure from the geometric data we do not know which primitives affect which pixels such a data structure can be complex and may imply that all the geometric data must be available at all times during the rendering process for problems with very large databases even having a good data representation may figure implementation tasks not avoid memory problems however because image space approaches have access to all objects for each pixel they are well suited to handle global effects such as shadows and reﬂections ray tracing chapter is an example of the image based approach we lean toward the object based approach although we look at examples of algorithms suited for both approaches four major tasks we start by reviewing the blocks in the pipeline focusing on those blocks that we have yet to discuss in detail there are four major tasks that any graphics system must perform to render a geometric entity such as a three dimensional polygon as that entity passes from deﬁnition in a user program to possible display on an output device modeling geometry processing rasterization fragment processing figure shows how these tasks might be organized in a pipeline implementation regardless of the approach all four tasks must be carried out modeling the usual results of the modeling process are sets of vertices that specify a group of geometric objects supported by the rest of the system we have seen a few examples that required some modeling by the user such as the approximation of spheres in chapter in chapters and we explore other modeling techniques we can look at the modeler as a black box that produces geometric objects and is usually a user program yet there are other tasks that the modeler might perform consider for example clipping the process of eliminating parts of objects that cannot appear on the display because they lie outside the viewing volume a user can generate geometric objects in her program and she can hope that the rest of the system can process these objects at the rate at which they are produced or the modeler can attempt to ease the burden on the rest of the system by minimizing the number of objects that it passes on the latter approach often means that the modeler may do some of the same jobs as the rest of the system albeit with different algorithms in the case of clipping the modeler knowing more about the speciﬁcs of the application can often use a good heuristic to eliminate many if not most primitives before they are sent on through the standard viewing process geometry processing geometry processing works with vertices the goals of the geometry processor are to determine which geometric objects can appear on the display and to assign shades or colors to the vertices of these objects four processes are required projection primitive assembly clipping and shading usually the ﬁrst step in geometry processing is to change representations from object coordinates to camera or eye coordinates using the model view transforma tion as we saw in chapter the conversion to camera coordinates is only the ﬁrst part of the viewing process the second step is to transform vertices using the pro jection transformation to a normalized view volume in which objects that might be visible are contained in a cube centered at the origin vertices are now represented in clip coordinates not only does this normalization convert both parallel and ortho graphic projections to a simple orthographic projection in a simple volume but in addition we simplify the clipping process as we shall see in section geometric objects are transformed by a sequence of transformations that may reshape and move them modeling or may change their representations viewing eventually only those primitives that ﬁt within a speciﬁed volume the view volume can appear on the display after rasterization we cannot however simply allow all objects to be rasterized hoping that the hardware will take care of primitives that lie wholly or partially outside the view volume the implementation must carry out this task before rasterization one reason is that rasterizing objects that lie outside the view volume is inefﬁcient because such objects cannot be visible another reason is that when vertices reach the rasterizer they can no longer be processed individually and ﬁrst must be assembled into primitives primitives that lie partially in the viewing volume can generate new primitives with new vertices for which we must carry out shading calculations before clipping can take place vertices must be grouped into objects a process known as primitive assembly note that even though an object lies inside the view volume it will not be vis ible if it is obscured by other objects algorithms for hidden surface removal or visible surface determination are based on the three dimensional spatial relation ships among objects this step is normally carried out as part of fragment processing as we saw in chapter colors can be determined on either a per vertex or per fragment basis if they are assigned on a per vertex basis they can be sent from the application as vertex attributes or computed in the vertex shader if lighting is enabled vertex colors are computed using a lighting model that can be implemented in the application or in the vertex shader after clipping takes place the remaining vertices are still in four dimensional homogeneous coordinates perspective division converts them to three dimensional representation in normalized device coordinates collectively these operations constitute what has been called front end process ing all involve three dimensional calculations and all require ﬂoating point arith metic all generate similar hardware and software requirements all are carried out on a vertex by vertex basis we will discuss clipping the only geometric step that we have yet to discuss in section rasterization even after geometric processing has taken place we still need to retain depth infor mation for hidden surface removal however only the x y values of the vertices are needed to determine which pixels in the frame buffer can be affected by the primitive for example after perspective division a line segment that was speciﬁed originally in three dimensions by two vertices becomes a line segment speciﬁed by a pair of three dimensional vertices in normalized device coordinates to generate a set of fragments that give the locations of the pixels in the frame buffer corresponding to these vertices we only need their x y components or equivalently the results of the orthogonal projection of these vertices we determine these fragments through a process called rasterization or scan conversion for line segments rasterization determines which fragments should be used to approximate a line segment between the projected vertices for polygons rasterization determines which pixels lie inside the two dimensional polygon determined by the projected vertices the colors that we assign to these fragments can be determined by the vertex at tributes or obtained by interpolating the shades at the vertices that are computed as in chapter objects more complex than line segments and polygons are usually ap proximated by multiple line segments and polygons and thus most graphics systems do not have special rasterization algorithms for them we shall see exceptions to this rule for some special curves and surfaces in chapter the rasterizer starts with vertices in normalized device coordinates but outputs fragments whose locations are in units of the display window coordinates as we saw in chapters and the projection of the clipping volume must appear in the assigned viewport in opengl this ﬁnal transformation is done after projection and is two dimensional the preceding transformations have normalized the view volume such that its sides are of length and line up with the sides of the viewport figure so this transformation is simply xv x vmin x x vmax xvmin y y y y y v z z vmin z z vmax vmin z v vmin vmax vmin recall that for perspective viewing these z values have been scaled nonlinearly by perspective normalization however they retain their original depth order so they xmax ymax xvmax yvmax x y xmin ymin xv yv xvmin yvmin figure viewport transformation can be used for hidden surface removal we shall use the term screen coordinates to refer to the two dimensional system that is the same as window coordinates but lacks the depth coordinate fragment processing in the simplest situations each fragment is assigned a color by the rasterizer and this color is placed in the frame buffer at the locations corresponding to the fragment location however there are many other possibilities the separate pixel pipeline chapter supported by architectures such as opengl merges with the results of the geometric pipeline at the rasterization stage consider what happens when a shaded and texture mapped polygon is processed vertex lighting is computed as part of the geometric processing the texture values are not needed until after rasterization when the renderer has generated fragments that correspond to locations inside a polygon at this point interpolation of per vertex colors and texture coordinates takes place and the texture parameters determine how to combine texture colors and fragment colors to determine ﬁnal colors in the color buffer as we have noted objects that are in the view volume will not be visible if they are blocked by any opaque objects closer to the viewer the required hidden surface removal process is typically carried out on a fragment by fragment basis until now we have assumed that all objects are opaque and thus an object located behind another object is not visible we can also assume that objects are translucent and allow some light to pass through in this case fragment colors may have to be blended with the colors of pixels already in the color buffer we consider this possibility in chapter in most displays the process of taking the image from the frame buffer and displaying it on a monitor happens automatically and is not of concern to the appli cation program however there are numerous problems with the quality of display such as the jaggedness associated with images on raster displays in chapter we in troduce algorithms for reducing this jaggedness or aliasing and we discuss problems with color reproduction on displays clipping we can now turn to clipping the process of determining which primitives or parts of primitives ﬁt within the clipping or view volume deﬁned by the application program clipping is done before the perspective division that is necessary if the w component of a clipped vertex is not equal to the portions of all primitives that can possibly be displayed we have yet to apply hidden surface removal lie within the cube w x w w y w w z w this coordinate system is called normalized device coordinates because it depends on neither the original application units nor the particulars of the display device although the information to produce the correct image is retained in this coordinate system note also that projection has been carried out only partially we still must do the perspective division and the ﬁnal orthographic projection we shall concentrate on clipping of line segments and polygons because they are the most common primitives to pass down the pipeline although the opengl pipeline does clipping on three dimensional objects there are other systems in which the objects are ﬁrst projected into the x y plane fortunately many of the most efﬁcient algorithms are almost identical in two and three dimensions and we will focus on these algorithms line segment clipping a clipper decides which primitives or parts of primitives can possibly appear on the display and be passed on to the rasterizer primitives that ﬁt within the speciﬁed view volume pass through the clipper or are accepted primitives that cannot appear on the display are eliminated or rejected or culled primitives that are only partially within the view volume must be clipped such that any part lying outside the volume is removed clipping can occur at one or more places in the viewing pipeline the modeler may clip to limit the primitives that the hardware must handle the primitives may be clipped after they have been projected from three to two dimensional objects in opengl primitives are clipped against a three dimensional view volume before rasterization we shall develop a sequence of clippers for both pedagogic and his toric reasons we start with two two dimensional line segment clippers both extend directly to three dimensions and to clipping of polygons cohen sutherland clipping the two dimensional clipping problem for line segments is shown in figure we can assume for now that this problem arises after three dimensional line segments have been projected onto the projection plane and that the window is part of the d g figure two dimensional clipping projection plane mapped to the viewport on the display all values are speciﬁed as real numbers we can see that the entire line segment ab appears on the display whereas none of cd appears ef and gh have to be shortened before being displayed although a line segment is completely determined by its endpoints gh shows that even if both endpoints lie outside the clipping window part of the line segment may still appear on the display we could compute the intersections of the lines of which the segments are parts with the sides of the window and could thus determine the necessary information for clipping however we want to avoid intersection calculations if possible because each intersection requires a ﬂoating point division the cohen sutherland algorithm was the ﬁrst to seek to replace most of the expensive ﬂoating point multiplications and divisions with a combination of ﬂoating point subtractions and bit operations the algorithm starts by extending the sides of the window to inﬁnity thus break x xmin x xmax y ymax y ymin ing up space into the nine regions shown in figure each region can be assigned a unique bit binary number or outcode as follows suppose that x y is a point in the region then figure breaking up of space and outcodes b if y ymax otherwise likewise if y ymin and and are determined by the relationship between x and the left and right sides of the window the resulting codes are indicated in figure for each endpoint of a line segment we ﬁrst compute the endpoint outcode a step that can require eight ﬂoating point subtractions per line segment consider a line segment whose outcodes are given by outcode and outcode we can now reason on the basis of these outcodes there are four cases both endpoints are inside the clipping window as is true for segment ab in figure the entire line segment is inside and the segment can be sent on to be rasterized or vice versa one endpoint is inside the clipping window one is outside see segment cd in figure the line segment must be shortened the nonzero outcode indicates which edge or edges of the window are crossed by the segment one or two intersections must be computed note that after one intersection is computed we can compute the outcode of the h j g b d f i a c e figure cases of outcodes in cohen sutherland algorithm point of intersection to determine whether another intersection calculation is required by taking the bitwise and of the outcodes we determine whether or not the two endpoints lie on the same outside side of the window if so the line segment can be discarded see segment ef in figure o1 both endpoints are outside but they are on the outside of different edges of the window as we can see from segments gh and ij in figure we cannot tell from just the outcodes whether the segment can be discarded or must be shortened the best we can do is to intersect with one of the sides of the window and to check the outcode of the resulting point all our checking of outcodes requires only boolean operations we do intersection calculations only when they are needed as in the second case or where the outcodes did not contain enough information as in the fourth case the cohen sutherland algorithm works best when there are many line segments but few are actually displayed in this case most of the line segments lie fully outside one or two of the extended sides of the clipping rectangle and can thus be eliminated on the basis of their outcodes the other advantage is that this algorithm can be extended to three dimensions the main disadvantage of the algorithm is that it must be used recursively consider line segment gh in figure it must be clipped against both the left and top sides of the clipping window generally the simplest way to do so is to use the initial outcodes to determine the ﬁrst side of the clipping window to clip against after this ﬁrst shortening of the original line segment a new outcode is computed for the new endpoint created by shortening and the algorithm is reexecuted we have not discussed how to compute any required intersections the form this calculation takes depends on how we choose to represent the line segment although only a single division should be required in any case if we use the standard explicit form of a line y mx h where m is the slope of the line and h is the line y intercept then we can compute m and h from the endpoints however vertical lines cannot be represented in this form a critical weakness of the explicit form if we were interested in only the cohen sutherland algorithm it would be fairly simple to program all cases directly because the sides of the clipping rectangle are parallel to the axes however we are interested in more than just clipping consequently other representations of the line and line segment are of importance in particular parametric representations are almost always used in computer graphics we have already seen the parametric form of the line in chapter the parametric representation of other types of curves is considered in chapter liang barsky clipping if we use the parametric form for lines we can approach the clipping of line segments in a different and ultimately more efﬁcient manner suppose that we have a line segment deﬁned by the two endpoints t and t we can use these endpoints to deﬁne a unique line that we can express parametrically either in matrix form p α α or as two scalar equations x α α y α α note that this form is robust and needs no changes for horizontal or vertical lines as the parameter α varies from to we move along the segment from to negative values of α yield points on the line on the other side of from similarly values of α give points on the line past going off to inﬁnity consider a line segment and the line of which it is part as shown in figure a as long as the line is not parallel to a side of the window if it is we can handle that situation with ease there are four points where the line intersects the extended sides of the window these points correspond to the four values of the parameter and one of these values corresponds to the line entering the window another corresponds to the line leaving the window leaving aside for the moment how we a b figure two cases of a parametric line and a clipping window compute these intersections we can order them and determine which correspond to intersections that we need for clipping for the given example hence all four intersections are inside the original line segment with the two in nermost and determining the clipped line segment we can distinguish this case from the case in figure b which also has the four intersections between the endpoints of the line segment by noting that the order for this case is α3 the line intersects both the top and the bottom of the window before it intersects either the left or the right thus the entire line segment must be rejected other cases of the ordering of the points of intersection can be argued in a similar way efﬁcient implementation of this strategy requires that we avoid computing inter sections until they are needed many lines can be rejected before all four intersections are known we also want to avoid ﬂoating point divisions where possible if we use the parametric form to determine the intersection with the top of the window we ﬁnd the intersection at the value α ymax similar equations hold for the other three sides of the window rather than com puting these intersections at the cost of a division for each we instead write the equation as α ymax all the tests required by the algorithm can be restated in terms of and similar terms can be computed for the other sides of the windows thus all decisions about clipping can be made without ﬂoating point division only if an intersection is needed because a segment has to be shortened is the division done the efﬁciency of this approach compared to that of the cohen sutherland algorithm is that we avoid multiple shortening of line segments and the related reexecutions of the clipping algorithm we forgo discussion of other efﬁcient two dimensional line clipping al gorithms because unlike the cohen sutherland and liang barsky algorithms these algorithms do not extend to three dimensions figure polygon clipping in shadow generation polygon clipping polygon clipping arises in a number of ways certainly we want to be able to clip polygons against rectangular windows for display however we may at times want windows that are not rectangular other parts of an implementation such as shadow generation and hidden surface removal can require clipping of polygons against other polygons for example figure shows the shadow of a polygon that we create by clipping a polygon that is closer to the light source against polygons that are farther away many antialiasing methods rely on our ability to clip polygons against other polygons we can generate polygon clipping algorithms directly from line clipping algo rithms by clipping the edges of the polygon successively however we must be careful to remember that a polygon is a two dimensional object with an interior and de pending on the form of the polygon we can generate more than one polygonal object by clipping consider the nonconvex or concave polygon in figure a if we clip it against a rectangular window we get the result shown in figure b most viewers looking at this ﬁgure would conclude that we have generated three polygons by clipping unfortunately implementing a clipper that can increase the number of objects can be a problem we could treat the result of the clipper as a single polygon as shown in figure with edges that overlap along the sides of the window but this choice might cause difﬁculties in other parts of the implementation convex polygons do not present such problems clipping a convex polygon against a rectangular window can leave at most a single convex polygon see exer cise a graphics system might then either forbid the use of concave polygons or divide tessellate a given polygon into a set of convex polygons as shown in fig ure opengl includes tessellation functions for rectangular clipping regions both the cohen sutherland and the liang barsky algorithms can be applied to polygons on an edge by edge basis there is another approach developed by sutherland and hodgeman that ﬁts well with the pipeline architectures that we have discussed a line segment clipper can be envisioned as a black box whose input is the pair of vertices from the segment to be tested and clipped and whose output either is a pair of vertices corresponding to the clipped line segment or is nothing if the input line segment lies outside the window figure rather than considering the clipping window as four line segments we can con sider it as the object created by the intersection of four inﬁnite lines that determine the top bottom right and left sides of the window we can then subdivide our clip per into a pipeline of simpler clippers each of which clips against a single line that is the extension of an edge of the window we can use the black box view on each of the individual clippers a b figure clipping of a concave polygon a before clipping b after clipping figure creation of a single polygon figure tessellation of a concave polygon y4 y4 a b figure two views of clipping a clipping against a rectangle b clipper as a black box xi ymax ymax a b figure clipping against top a graphically b black box view suppose that we consider clipping against only the top of the window we can think of this operation as a black box whose input and output are pairs of vertices with the value of ymax as a parameter known to the clipper figure using the similar triangles in figure we see that if there is an intersection it lies at y max figure intersection with the top of the window ymax thus the clipper returns one of three pairs xi ymax or xi ymax we can clip against the bottom right and left lines independently using the same equations with the roles of x and y exchanged as nec essary and the values for the sides of the window inserted the four clippers can now be arranged in the pipeline of figure if we build this conﬁguration in hardware we have a clipper that is working on four vertices concurrently figure shows a simple example of the effect of successive clippers on a polygon a y5 y5 y4 b figure pipeline clipping a clipping problem b pipeline clippers figure example of pipeline clipping clipping of other primitives our emphasis in chapters through was on writing programs in which the objects are built from line segments and triangles we often render the curved objects that we discuss in chapter by subdividing them into small approximately ﬂat poly gons in pipeline architectures we usually ﬁnd some variant of the clippers that we have presented nevertheless there are situations in which we want either to clip ob jects before they reach the hardware or to employ algorithms optimized for other primitives a b figure using bounding boxes a polygon and clipping window b polygon bounding box and clipping window figure clipping with bounding boxes bounding boxes and volumes suppose that we have a many sided polygon as shown in figure a we could apply one of our clipping algorithms which would clip the polygon by individually clipping all that polygon edges however we can see that the entire polygon lies outside the clipping window we can exploit this observation through the use of the axis aligned bounding box or the extent of the polygon figure b the smallest rectangle aligned with the window that contains the polygon calculating the bounding box requires merely going through the vertices of the polygon to ﬁnd the minimum and maximum of both the x and y values once we have the bounding box we can often avoid detailed clipping consider the three cases in figure for the polygon above the window no clipping is necessary because the minimum y for the bounding box is above the top of the window for the polygon inside the window we can determine that it is inside by comparing the bounding box with the window only when we discover that the bounding box straddles the window do we have to carry out detailed clipping using all the edges of the polygon the use of extents is such a powerful technique in both two and three dimensions that modeling systems often compute a bounding box for each object automatically and store the bounding box with the object axis aligned bounding boxes work in both two and three dimensions in three dimensions they can be used in the application to perform clipping to reduce the burden on the pipeline other volumes such as spheres can also work well one of the other applications of bounding volumes is in collision detection chapter one of the fundamental operations in animating computer games is to determine if two moving entities have collided for example consider two animated characters moving in a sequence of images we need to know when they collide so that we can alter their paths this problem has many similarities to the clipping problem because we want to determine when the volume of one intersects the volume of the other the complexity of the objects and the need to do these calculations very quickly make this problem difﬁcult a common approach is to place each object in a bounding volume either an axis aligned bounding box or a sphere and to determine if the volumes intersect if they do then detailed calculations can be done a b figure curve clipping curves surfaces and text the variety of curves and surfaces that we can deﬁne mathematically makes it difﬁcult to ﬁnd general algorithms for processing these objects the potential difﬁculties can be seen from the two dimensional curves in figure for a simple curve such as a quadric we can compute intersections although at a cost higher than that for lines for more complex curves such as the spiral not only must intersection calculations be computed with numerical techniques but even determining how many intersections we must compute may be difﬁcult we can avoid such problems by approximating curves with line segments and surfaces with planar polygons the use of bounding boxes can also prove helpful especially in cases such as quadratics where we can compute intersections exactly but would prefer to make sure that the calculation is necessary before carrying it out the handling of text differs from api to api with many apis allowing the user to specify how detailed a rendering of text is required there are two extremes on one end text is stored as bit patterns and is rendered directly by the hardware without any geometric processing any required clipping is done in the frame buffer at the other extreme text is deﬁned like any other geometric object and is then processed through the standard viewing pipeline opengl allows both these cases by not having a separate text primitive the user can choose which mode she prefers by deﬁning either bitmapped characters using pixel operations or stroke characters using the standard primitives clipping in the frame buffer we might also consider delaying clipping until after objects have been projected and converted into screen coordinates clipping can be done in the frame buffer through a technique called scissoring however it is usually better to clip geometric entities before the vertices reach the frame buffer thus clipping within the frame buffer generally is required only for raster objects such as blocks of pixels clipping in three dimensions in three dimensions we clip against a bounded volume rather than against a bounded region in the plane the simplest extension of two dimensional clipping to three xmax ymax zmax z figure three dimensional clipping against a right parallelepiped dimensions is for the right parallelepiped clipping region figure xmin x xmax ymin y ymax zmin z zmax or in clip space w x w w y w w z w our three clipping algorithms cohen sutherland liang barsky and sutherland hodgeman and the use of extents can be extended to three dimensions for the cohen sutherland algorithm we replace the bit outcode with a bit out code the additional bits are set if the point lies either in front of or behind the clipping volume figure the testing strategy is virtually identical for the two and three dimensional cases for the liang barsky algorithm we add the equation z α α to obtain a three dimensional parametric representation of the line segment we have to consider six intersections with the surfaces that form the clipping volume but we can use the same logic as we did in the two dimensional case pipeline clippers add two modules to clip against the front and back of the clipping volume the major difference between two and three dimensional clippers is that in three dimensions we are clipping either lines against planes or polygons against figure cohen sutherland regions in three dimensions planes instead of clipping lines against lines as we do in two dimensions conse quently our intersection calculations must be changed a typical intersection calcu lation can be posed in terms of a parametric line in three dimensions intersecting a plane figure if we write the line and plane equations in matrix form where n is the normal to the plane and is a point on the plane we must solve the equations n p α α p p n p α for the α corresponding to the point of intersection this value is α n n p1 and computation of an intersection requires six multiplications and a division how ever if we look at the standard viewing volumes we see that simpliﬁcations are possible for orthographic viewing figure the view volume is a right paral lelepiped and each intersection calculation reduces to a single division as it did for two dimensional clipping when we consider an oblique view figure we see that the clipping volume no longer is a right parallelepiped although you might think that we have to compute dot products to clip against the sides of the volume here is where the normalization process that we introduced in chapter pays dividends we showed that an oblique projection is equivalent to a shearing of the data followed by an orthographic projec tion although the shear transformation distorts objects they are distorted such that they project correctly by an orthographic projection the shear also distorts the clip ping volume from a general parallelepiped to a right parallelepiped figure a shows a top view of an oblique volume with a cube inside the volume figure b figure plane line inter section y ymax x xmin y ymin x xmax z zmax z zmin figure clipping for orthographic viewing z zmax figure clipping for oblique viewing projection plane projection plane distorted object new clipping volume a b figure distortion of view volume by shear a top view before shear b top view after shear shows the volume and object after they have been distorted by the shear as far as pro jection is concerned carrying out the oblique transformation directly or replacing it by a shear transformation and an orthographic projection requires the same amount of computation when we add in clipping it is clear that the second approach has a deﬁnite advantage because we can clip against a right parallelepiped this example illustrates the importance of considering the incremental nature of the steps in an im plementation analysis of either projection or clipping in isolation fails to show the importance of the normalization process for perspective projections the argument for normalization is just as strong by carrying out the perspective normalization transformation from chapter but not the orthographic projection we again create a rectangular clipping volume and simplify all subsequent intersection calculations rasterization we are now ready to take the ﬁnal step in the journey from the speciﬁcation of geo metric entities in an application program to the formation of fragments rasterization of primitives in this chapter we are concerned with only line segments and polygons both of which are deﬁned by vertices we can assume that we have clipped the prim itives such that each remaining primitive is inside the view volume fragments are potential pixels each fragment has a color attribute and a location in screen coordinates that corresponds to a location in the color buffer fragments also carry depth information that can be used for hidden surface removal to clarify the discussion we will ignore hidden surface removal until section and thus we can work directly in screen coordinates because we are not considering hidden surface removal translucent fragments or antialiasing we can develop rasterization algorithms in terms of the pixels that they color we further assume that the color buffer is an n m array of pixels with corresponding to the lower left corner pixels can be set to a given color by a single function inside the graphics implementation of the form int ix int iy int value the argument value can be either an index in color index mode or a pointer to an rgba color on the one hand a color buffer is inherently discrete it does not make sense to talk about pixels located at places other than integer values of ix and iy on the other hand screen coordinates which range over the same values as do ix and iy are real numbers for example we can compute a fragment location such as in screen coordinates but must realize that the nearest pixel is centered either at or at depending on whether pixels are considered to be centered at whole or half integer values pixels have attributes that are colors in the color buffer pixels can be displayed in multiple shapes and sizes that depend on the characteristics of the display we address this matter in section for now we can assume that a pixel is displayed as a square whose center is at the location associated with the pixel and whose side is equal to the distance between pixels in opengl the centers of pixels are located at values halfway between integers there are some advantages to this choice see exercise we also assume that a concurrent process reads the contents of the color buffer and creates the display at the required rate this assumption which holds in many systems that have dual ported memory allows us to treat the rasterization process independently of the display of the contents of the frame buffer the simplest scan conversion algorithm for line segments has become known as the dda algorithm after the digital differential analyzer an early electromechanical device for digital simulation of differential equations because a line satisﬁes the differential equation dy dx m where m is the slope generating a line segment is equivalent to solving a simple differential equation numerically suppose that we have a line segment deﬁned by the endpoints and because we are working in a color buffer we assume that these values have been rounded to have integer values so the line segment starts and ends at a known pixel the slope is given by m figure line segment in we assume that m window coordinates figure pixels gener ated by dda algorithm we can handle other values of m using symmetry our algorithm is based on writing a pixel for each value of ix in as x goes from to if we are on the line segment as shown in figure for any change in x equal to the corresponding changes in y must be as we move from to we increase x by in each iteration thus we must increase y by m although each x is an integer each y is not because m is a ﬂoating point number and we must round it to ﬁnd the appropriate pixel as shown in figure our algorithm in pseudocode is for ix ix ix y m x round y figure pixels gener ated by high and low slope lines where round is a function that rounds a real number to an integer the reason that we limited the maximum slope to can be seen from figure our algorithm is of this form for each x ﬁnd the best y for large slopes the separation between pixels that are colored can be large generating an unacceptable approximation to the line segment if however for slopes greater than we swap the roles of x and y the algorithm becomes this for each y ﬁnd the best x for the same line segments we get the approximations in figure note that the use of symmetry removes any potential problems from either vertical or horizontal line segments you may want to derive the parts of the algorithm for negative slopes because line segments are determined by vertices we can use interpolation to assign a different color to each pixel that we generate we can also generate various figure pixels gener ated by revised dda algo rithm this assumption is not necessary to derive an algorithm if we use a ﬁxed point representation for the endpoints and do our calculations using ﬁxed point arithmetic then we retain the compu tational advantages of the algorithm and produce a more accurate rasterization dash and dot patterns by changing the color that we use as we generate pixels neither of these effects has much to do with the basic rasterization algorithm as the latter job is to determine only which pixels to color rather than to determine the color that is used bresenham s algorithm the dda algorithm appears efﬁcient certainly it can be coded easily but it re quires a ﬂoating point addition for each pixel generated bresenham derived a line rasterization algorithm that remarkably avoids all ﬂoating point calculations and has become the standard algorithm used in hardware and software rasterizers we assume as we did with the dda algorithm that the line segment goes between the integer points y1 and y2 and that the slope satisﬁes m this slope condition is crucial for the algorithm as we can see with the aid of fig ure suppose that we are somewhere in the middle of the scan conversion of our line segment and have just placed a pixel at i j we know that the line of which the segment is part can be represented as y mx h at x i this line must pass within one half the length of the pixel at i j otherwise the rounding operation would not have generated this pixel if we move ahead to x i the slope condition indicates that we must set the color of one of only two possible pixels either the pixel at i j or the pixel at i j having reduced our choices to two pixels we can pose the problem anew in terms of the decision variable d a b where a and b are the distances between the line and the upper and lower candidate pixels at x i as shown in figure if d is positive the line passes closer to the lower pixel so we choose the pixel at i j otherwise we choose the pixel at i j although we could compute d by computing y mx b we hesitate to do so because m is a ﬂoating point number we obtain the computational advantages of bresenham algorithm through two further steps first we replace ﬂoating point operations with ﬁxed point operations second we apply the algorithm incrementally we start by replacing d with the new decision variable d a b a b we are assuming that the pixels centers are located halfway between integers j j j i i y mx h figure conditions for bresenham algorithm j j j i i y mx h figure decision variable for bresenham algorithm a change that cannot affect which pixels are drawn because it is only the sign of the decision variable that matters if we substitute for a and b using the equation of the line and noting that m y2 y1 h y2 mx2 then we can see that d is an integer we have eliminated ﬂoating point calculations but the direct computation of d requires a fair amount of ﬁxed point arithmetic let us take a slightly different approach suppose that dk is the value of d at x k we would like to compute dk incrementally from dk there are two situations depending on whether or not we incremented the y location of the pixel at the previous step these situations are shown in figure by observing that a is the distance between the location of the upper candidate location and the line we see that a increases by m only if x was increased by the previous decision otherwise it decreases by m likewise b either decreases by m or increases by m when we increment x multiplying by we ﬁnd that the possible changes in d are either or we can state this result in the form j j j i i i j b j j i i m i figure incrementing of the values of a and b dk dk if dk otherwise the calculation of each successive pixel in the color buffer requires only an addition and a sign test this algorithm is so efﬁcient that it has been incorporated as a single instruction on graphics chips see exercise for calculation of the initial value polygon rasterization one of the major advantages that the ﬁrst raster systems brought to users was the ability to display ﬁlled polygons at that time coloring each point in the interior of a polygon with a different shade was not possible in real time and the phrases rasterizing polygons and polygon scan conversion came to mean ﬁlling a polygon with a single color unlike rasterization of lines where a single algorithm dominates there are many viable methods for rasterizing polygons the choice depends heavily on the implementation architecture we concentrate on methods that ﬁt with our pipeline approach and can also support shading in sections through we survey a number of other approaches inside outside testing flat simple polygons have well deﬁned interiors if they are also convex they are guaranteed to be rendered correctly by opengl and by other graphics systems more general polygons arise in practice however and we can render them in multiple ways for nonﬂat polygons we can work with their projections section or we can use the ﬁrst three vertices to determine a plane to use for the interior for ﬂat nonsimple polygons we must decide how to determine whether a given point is strictly speaking there is no such thing as a nonﬂat polygon because the interior is not deﬁned unless it is ﬂat however from a programming perspective we can deﬁne a polygon by simply giving a list of vertices regardless of whether or not they lie in the same plane figure filling with the odd even test inside or outside of the polygon conceptually the process of ﬁlling the inside of a polygon with a color or pattern is equivalent to deciding which points in the plane of the polygon are interior inside points the crossing or odd even test is the most widely used test for making inside outside decisions suppose that p is a point inside a polygon any ray emanating from p and going off to inﬁnity must cross an odd number of edges any ray emanating from a point outside the polygon and entering the polygon crosses an even number of edges before reaching inﬁnity hence a point can be deﬁned as being inside if after drawing a line through it and following this line starting on the outside we cross an odd number of edges before reaching it for the star shaped polygon in figure we obtain the inside coloring shown odd even testing is easy to implement and inte grates well with the standard rendering algorithms usually we replace rays through points with scanlines and we count the crossing of polygon edges to determine inside and outside however we might want our ﬁll algorithm to color the star polygon as shown in figure rather than as shown in figure the winding test allows us to make that happen this test considers the polygon as a knot being wrapped around a point or a line to implement the test we consider traversing the edges of the polygon from any starting vertex and going around the edge in a particular direction which direction does not matter until we reach the starting point we illustrate the path by labeling the edges as shown in figure b next we consider an arbitrary point the winding number for this point is the number of times it is encircled by the edges of the polygon we count clockwise encirclements as positive and counterclockwise encirclements as negative or vice versa thus points outside the star in figure are not encircled and have a winding number of points that were ﬁlled in figure all have a winding number of and points in the center that were not ﬁlled by the odd even test have a winding number of if we change our ﬁll rule to be that a point is inside the polygon if its winding number is not zero then we ﬁll the inside of the polygon as shown in figure a a b figure fill using the winding number test opengl and concave polygons because opengl only renders triangles which are always ﬂat and convex we still have the problem of what to do with more general polygons one approach is to work with the application to ensure that they only generate triangles another is to provide software that can tessellate a given polygon into ﬂat convex polygons usually triangles there are many ways to divide a given polygon into triangles a good tessellation should not produce triangles that are long and thin it should if possible produce sets of triangles that can use supported features such as triangle strips and triangle fans let consider one approach to tessellating or triangularizing an arbitrary simple polygon with n vertices from the construction it will be clear that we always obtain a triangularization using exactly n triangles we assume our polygon is speciﬁed by an ordered list of vertices vn thus there is an edge from to from v1 to v2 and ﬁnally from vn to the ﬁrst step is to ﬁnd the left most vertex vi a calculation that requires a simple scan of the x components of the vertices let vi and vi be the two neighbors of vi where the indices are computed modulo n these three vertices form the triangle vi vi vi if the situation is as in figure then we can proceed recursively by removing vi from the original list and we will have a triangle and a polygon with n vertices however because the polygon may not be convex the line segment from vi to vi can cross other edges as shown in figure we can test for this case by checking if any of the other vertices lie to the left of the line segment and inside the triangle determined by vi vi vi if we connect vi to the left most of these vertices we split the original triangle into two polygons as in figure each of which has at least two vertices fewer than the original triangle using the leftmost vertex ensures that the two polygons are simple hence we can proceed recursively with these two triangles knowing that in the end we will have all triangles note that the worst case performance of this method occurs when there are no vertices in the triangle formed by vi vi vi we require o n tests to make sure that this is the case and if it is we then remove only one vertex from the original polygon consequently the worst case performance is o n2 however if we know in advance that the polygon is convex these tests are not needed and the method is o n the best performance in general occurs when the splitting results in two polygons with an equal number of vertices if such a split occurs on each step the method would be o n log n the suggested readings at the end of the chapter include methods that are guaranteed to be o n log n but they are more complex than the method outlined here in practice we rarely work with polygons with so many vertices that we need the more complex methods fill and sort a different approach to rasterization of polygons starts with the idea of a polygon processor a black box whose inputs are the vertices for a set of two dimensional polygons and whose output is a frame buffer with the correct pixels set suppose vi vi figure removal of a triangle from a polygon vi vi figure vertex inside triangle vi vi vi figure splitting into two polygons that we consider ﬁlling each polygon with a constant color a choice we make only to clarify the discussion first consider a single polygon the basic rule for ﬁlling a polygon is as follows if a point is inside the polygon color it with the inside ﬁll color this conceptual algorithm indicates that polygon ﬁll is a sorting problem where we sort all the pixels in the frame buffer into those that are inside the polygon and those that are not from this perspective we obtain different polygon ﬁll algorithms using different ways of sorting the points we introduce three possibilities flood ﬁll scanline ﬁll odd even ﬁll figure polygon dis played by edges flood fill we can display an unﬁlled polygon by rasterizing its edges into the frame buffer using bresenham algorithm suppose that we have only two colors a background color white and a foreground or drawing color black we can use the foreground color to rasterize the edges resulting in a frame buffer colored as shown in figure for a simple polygon if we can ﬁnd an initial point x y inside the polygon a seed point then we can look at its neighbors recursively coloring them with the foreground color if they are not edge points the ﬂood ﬁll algorithm can be expressed in pseudocode assuming that there is a function that returns the color of a pixel int x int y if x y white x y black x y x y x y x y a we can obtain a number of variants of ﬂood ﬁll by removing the recursion one way to do so is to work one scanline at a time b figure singularities a zero or two edge crossings b one edge crossing singularities we can extend most polygon ﬁll algorithms to other shapes if we use care see exer cise polygons have the distinct advantage that the locations of their edges are known exactly even polygons can present problems however when vertices lie on scanlines consider the two cases in figure if we are using an odd even ﬁll def inition we have to treat these two cases differently for case a we can count the intersection of the scanline with the vertex as either zero or two edge crossings for case b the vertex scanline intersection must be counted as one edge crossing we can ﬁx our algorithm in one of two ways we can check to see which of the two situations we have and then count the edge crossings appropriately or we can prevent the special case of a vertex lying on an edge a singularity from ever arising we rule it out by ensuring that no vertex has an integer y value if we ﬁnd one that does we perturb its location slightly another method one that is especially valuable if we are working in the frame buffer is to consider a virtual frame buffer of twice the resolution of the real frame buffer in the virtual frame buffer pixels are located only at even values of y and all vertices are located only at odd values of y placing pixel centers halfway between integers as opengl does is equivalent to using this approach hidden surface removal although every fragment generated by rasterization corresponds to a location in a color buffer we do not want to display the fragment by coloring the corresponding pixel if the fragment is from an object behind another opaque object hidden surface removal or visible surface determination is done to discover what part if any of each object in the view volume is visible to the viewer or is obscured from the viewer by other objects we describe a number of techniques for a scene composed purely of planar polygons because most renderers will have subdivided surfaces into polygons at this point this choice is appropriate line segments can be handled by slight modiﬁcations see exercise object space and image space approaches the study of hidden surface removal algorithms clearly illustrates the variety of available algorithms the differences between working with objects and working with images and the importance of evaluating the incremental effects of successive algo rithms in the implementation process consider a scene composed of k three dimensional opaque ﬂat polygons each of which we can consider to be an individual object we can derive a generic object space approach by considering the objects pairwise as seen from the center of projec tion consider two such polygons a and b there are four possibilities figure a completely obscures b from the camera we display only a b obscures a we display only b a and b both are completely visible we display both a and b a and b partially obscure each other we must calculate the visible parts of each polygon for complexity considerations we can regard the determination of which case we have and any required calculation of the visible part of a polygon as a single opera tion we proceed iteratively we pick one of the k polygons and compare it pairwise a b c d figure two polygons a b partially obscures a b a partially obscures b c both a and b are visible d b totally obscures a with the remaining k polygons after this procedure we know which part if any of this polygon is visible and we render the visible part we are now done with this polygon so we repeat the process with any of the other k polygons each step in volves comparing one polygon pairwise with the other remaining polygons until we have only two polygons remaining and we compare these to each other we can eas ily determine that the complexity of this calculation is o thus without deriving any of the details of any particular object space algorithm we should suspect that the object space approach works best for scenes that contain relatively few polygons the image space approach follows our viewing and ray casting model as shown in figure consider a ray that leaves the center of projection and passes through a pixel we can intersect this ray with each of the planes determined by our k polygons determine for which planes the ray passes through a polygon and ﬁnally for those rays ﬁnd the intersection closest to the center of projection we color this pixel with the shade of the polygon at the point of intersection our fundamental operation is the intersection of rays with polygons for an n m display we have to carry out this operation nmk times giving o k complexity again without looking at the details of the operations we were able to get an upper bound in general the o k bound accounts for the dominance of image space methods the o k bound is a worst case bound in practice image space algorithms perform much better see exercise however because image space approaches work at the fragment or pixel level their accuracy is limited by the resolution of the frame buffer sorting and hidden surface removal the o upper bound for object oriented hidden surface removal might remind you of the poorer sorting algorithms such as bubble sort any method that involves brute force comparison of objects by pairs has o complexity but there is a more direct connection which we exploited in the object oriented sorting algorithms in section if we could organize objects by their distances from the camera we should be able to come up with a direct method of rendering them but if we follow the analogy we know that the complexity of good sorting al gorithms is o k log k we should expect the same to be true for object oriented we can use more than one ray for each pixel to increase the accuracy of the rendering cop figure image space hidden surface removal hidden surface removal and in fact such is the case as with sorting there are multiple algorithms that meet these bounds in addition there are related problems involving comparison of objects such as collision detection that start off looking as if they are o when in fact they can be reduced to o k log k scanline algorithms the attraction of a scanline algorithm is that such a method has the potential to generate pixels as they are displayed consider the polygon in figure with one scanline shown if we use our odd even rule for deﬁning the inside of the polygon we can see three groups of pixels or spans on this scanline that are inside the polygon note that each span can be processed independently for lighting or depth calculations a strategy that has been employed in some hardware that has parallel span processors for our simple example of constant ﬁll after we have identiﬁed the spans we can color the interior pixels of each span with the ﬁll color the spans are determined by the set of intersections of polygons with scanlines the vertices contain all the information that we need to determine these intersec tions but the method that we use to represent the polygon determines the order in which these intersections are generated for example consider the polygon in fig ure which has been represented by an ordered list of vertices the most obvious way to generate scanline edge intersections is to process edges deﬁned by successive vertices figure shows these intersections indexed in the order in which this method would generate them note that this calculation can be done incrementally see exercise however as far as ﬁll is concerned this order is far from the one we want if we are to ﬁll one scanline at a time we would like the intersections sorted ﬁrst by scanlines and then by order of x on each scanline as shown in figure a brute force approach might be to sort all the intersections into the desired order however a large or jagged polygon might intersect so many edges that the n inter sections can be large enough that the o n log n complexity of the sort makes the figure polygon with spans b figure polygon gener ated by vertex list b c a figure desired order of vertices figure data structure for y x algorithm calculation too slow for real time implementations consider for example a polygon that spans one half of the scan lines a number of methods avoid the general search one originally known as the y x algorithm creates a bucket for each scanline as edges are processed the in tersections with scanlines are placed in the proper buckets within each bucket an insertion sort orders the x values along each scanline the data structure is shown in figure once again we see that a properly chosen data structure can speed up the algorithm we can go even further by reconsidering how to represent polygons if we do so we arrive at the scanline method that was introduced in section figure back face test back face removal in chapter we noted that in opengl we can choose to render only front facing polygons for situations where we cannot see back faces such as scenes composed of convex polyhedra we can reduce the work required for hidden surface removal by eliminating all back facing polygons before we apply any other hidden surface removal algorithm the test for culling a back facing polygon can be derived from figure we see the front of a polygon if the normal which comes out of the front face is pointed toward the viewer if θ is the angle between the normal and the viewer then the polygon is facing forward if and only if θ or equivalently cos θ the second condition is much easier to test because instead of computing the cosine we can use the dot product n v figure computing the area of a polygon we can simplify this test even further if we note that usually it is applied after transfor mation to normalized device coordinates in this system all views are orthographic with the direction of projection along the z axis hence in homogeneous coordi nates v thus if the polygon is on the surface ax by cz d in normalized device coordinates we need only to check the sign of c to determine whether we have a front or back facing polygon this test can be implemented easily in either hardware or software we must simply be careful to ensure that removing back facing polygons is correct for our application there is another interesting approach to determining back faces the algorithm is based on computing the area of the polygon in screen coordinates consider the polygon in figure with n vertices its area a is given by a y i i yi x i xi where the indices are taken modulo n see exercise a negative area indicates a back facing polygon the z buffer algorithm the z buffer algorithm is the most widely used hidden surface removal algorithm it has the advantages of being easy to implement in either hardware or software and of being compatible with pipeline architectures where it can execute at the speed at a z figure the z buffer algorithm which fragments are passing through the pipeline although the algorithm works in image space it loops over the polygons rather than over pixels and can be regarded as part of the scan conversion process that we discussed in section suppose that we are in the process of rasterizing one of the two polygons shown in figure we can compute a color for each point of intersection between a ray from the center of projection and a pixel using interpolated values of the vertex shades computed as in chapter in addition we must check whether this point is visible it will be visible if it is the closest point of intersection along the ray hence if we are rasterizing b its shade will appear on the screen if the distance is less than the distance to polygon a conversely if we are rasterizing a the pixel that corresponds to the point of intersection will not appear on the display because we are proceeding polygon by polygon however we do not have the information on all other polygons as we rasterize any given polygon however if we keep depth information with each fragment then we can store and update depth information for each location in the frame buffer as fragments are processed suppose that we have a buffer the z buffer with the same resolution as the frame buffer and with depth consistent with the resolution that we wish to use for distance for example if we have a display and we use standard integers for the depth calculation we can use a z buffer with bit elements initially each element in the depth buffer is initialized to a depth corresponding to the maximum distance away from the center of projection the color buffer is initialized to the background color at any time during rasterization and fragment processing if we have already done perspective normalization we should replace the center of projection with the direction of projection because all rays are parallel however this change does not affect the z buffer algorithm because we can measure distances from any arbitrary plane such as the plane z rather than from the cop each location in the z buffer contains the distance along the ray corresponding to the location of the closest intersection point on any polygon found so far the calculation proceeds as follows we rasterize polygon by polygon using one of the methods from section for each fragment on the polygon correspond ing to the intersection of the polygon with a ray through a pixel we compute the depth from the center of projection we compare this depth to the value in the z buffer corresponding to this fragment if this depth is greater than the depth in the z buffer then we have already processed a polygon with a corresponding fragment closer to the viewer and this fragment is not visible if the depth is less than the depth in the z buffer then we have found a fragment closer to the viewer we up date the depth in the z buffer and place the shade computed for this fragment at the corresponding location in the color buffer note that for perspective views the depth we are using in the z buffer algorithm is the distance that has been altered by the normalization transformation that we discussed in chapter although this trans formation is nonlinear it preserves relative distances however this nonlinearity can introduce numerical inaccuracies especially when the distance to the near clipping plane is small unlike other aspects of rendering where the particular implementation algo rithms may be unknown to the user for hidden surface removal opengl uses the z buffer algorithm this exception arises because the application program must ini tialize the z buffer explicitly every time a new image is to be generated the z buffer algorithm works well with image oriented approaches to imple mentation because the amount of incremental work is small suppose that we are rasterizing a polygon scanline by scanline an option we examined in section the polygon is part of a plane figure that can be represented as ax by cz d suppose that y1 and y2 are two points on the polygon and the plane if y2 y1 then the equation for the plane can be written in differential form as in opengl we can use the function gldepthfunc to decide what to do when the distances are equal y2 y1 z1 ax by cz d figure incremental z buffer algorithm this equation is in window coordinates so each scanline corresponds to a line of constant y and as we move across a scanline on a scanline we increase x in unit steps corresponding to moving one pixel in the frame buffer and is constant thus as we move from point to point across a scanline a c this value is a constant that needs to be computed only once for each polygon although the worst case performance of an image space algorithm is propor tional to the number of primitives the performance of the z buffer algorithm is proportional to the number of fragments generated by rasterization which depends on the area of the rasterized polygons scan conversion with the z buffer we have already presented most of the essentials of polygon rasterization in sec tion we discussed the odd even and winding tests for determining whether a point is inside a polygon in chapter we learned to shade polygons by interpolation here we have only to put together the pieces and to consider efﬁciency suppose that we follow the pipeline once more concentrating on what happens to a single polygon the vertices and normals pass through the geometric transforma tions one at a time the vertices must be assembled into a polygon before the clipping stage if our polygon is not clipped out its vertices and normals can be passed on for shading and hidden surface removal at this point although projection normaliza tion has taken place we still have depth information if we wish to use an interpolative shading method we can compute the lighting at each vertex three tasks remain computation of the ﬁnal orthographic projection hidden surface removal and shading careful use of the z buffer algorithm can accomplish y ys x xs z a b figure dual representations of a polygon a normalized device coordinates b screen coordinates y ys ys j x xs z a b figure dual representations of a scanline a in normalized device coordinates b in screen coordinates all three tasks simultaneously consider the dual representations of a polygon illus trated in figure in a the polygon is represented in three dimensional normal ized device coordinates in b it is shown after projection in screen coordinates the strategy is to process each polygon one scanline at a time if we work again in terms of these dual representations we can see that a scanline projected backward from screen coordinates corresponds to a line of constant y in normalized device coordinates figure suppose that we simultaneously march across this scan line and its back projection for the scanline in screen coordinates we move one pixel width with each step we use the normalized device coordinate line to deter mine depths incrementally and to see whether or not the pixel in screen coordinates corresponds to a visible point on the polygon having computed shading for the ver tices of the original polygon we can use interpolation to obtain the correct color for visible pixels this process requires little extra effort over the individual steps that we have already discussed it is controlled and thus limited by the rate at which we can send polygons through the pipeline modiﬁcations such as applying bit patterns called stipple patterns or texture to polygons require only slight changes a b figure painter algorithm a two polygons and a viewer are shown b polygon a partially obscures b when viewed depth sort and the painter algorithm although image space methods are dominant in hardware due to the efﬁciency and ease of implementation of the z buffer algorithm often object space methods are used within the application to lower the polygon count depth sort is a direct imple mentation of the object space approach to hidden surface removal we present the algorithm for a scene composed of planar polygons extensions to other classes of ob jects are possible depth sort is a variant of an even simpler algorithm known as the painter algorithm suppose that we have a collection of polygons that is sorted based on how far from the viewer the polygons are for the example in figure a we have two polygons to a viewer they appear as shown in figure b with the polygon in front partially obscuring the other to render the scene correctly we could ﬁnd the part of the rear polygon that is visible and render that part into the frame buffer a calculation that requires clipping one polygon against the other or we could use an approach analogous to the way a painter might render the scene she probably would paint the rear polygon in its entirety and then the front polygon painting over that part of the rear polygon not visible to the viewer in the process both polygons would be rendered completely with the hidden surface removal being done as a consequence of the back to front rendering of the polygons the two questions related to this algorithm are how to do the sort and what to do if polygons overlap depth sort addresses both although in many applications more efﬁciencies can be found see for example exercise suppose we have already computed the extent of each polygon the next step of depth sort is to order all the polygons by how far away from the viewer their maximum z value is this step gives the algorithm the name depth sort suppose that the order is as shown in figure which depicts the z extents of the polygons after the sort if the minimum depth the z value of a given polygon is greater than the maximum depth of the polygon behind the one of interest we can paint the polygons back to front and we are done for example polygon a in figure is behind all in ray tracing and scientiﬁc visualization we often use front to back rendering of polygons polygons figure the z extents of sorted polygons a b figure test for overlap in x and y extents a nonoverlapping x extents b nonoverlapping y extents the other polygons and can be painted ﬁrst however the others cannot be painted based solely on the z extents if the z extents of two polygons overlap we still may be able to ﬁnd an order to paint render the polygons individually and yield the correct image the depth sort algorithm runs a number of increasingly more difﬁcult tests attempting to ﬁnd such an ordering consider a pair of polygons whose z extents overlap the simplest test is to check their x and y extents figure if either of the x or y extents do not overlap neither polygon can obscure the other and they can be painted in either order even if these tests fail it may still be possible to ﬁnd an order in which we can paint the polygons individually figure shows such a case all the vertices of one polygon lie on the same side of the plane determined by the other we can process the vertices see exercise of the two polygons to determine whether this case exists figure polygons with overlapping extents the x and y extent tests apply to only a parallel view here is another example of the advantage of working in normalized device coordinates after perspective normalization figure cyclic overlap two troublesome situations remain if three or more polygons overlap cyclically as shown in figure there is no correct order for painting the best we can do is to divide at least one of the polygons into two parts and attempt to ﬁnd an order to paint the new set of polygons the second problematic case arises if a polygon can pierce another polygon as shown in figure if we want to continue with depth sort we must derive the details of the intersection a calculation equivalent to clip ping one polygon against the other if the intersecting polygons have many vertices we may want to try another algorithm that requires less computation a performance analysis of depth sort is difﬁcult because the particulars of the application determine how often the more difﬁcult cases arise for example if we are working with poly gons that describe the surfaces of solid objects then no two polygons can intersect nevertheless it should be clear that because of the initial sort the complexity must be at least o k log k where k is the number of objects figure piercing poly gons figure ideal raster line antialiasing rasterized line segments and edges of polygons look jagged even on a display device that has a resolution as high as we can notice these defects in the display this type of error arises whenever we attempt to go from the continuous representa tion of an object which has inﬁnite resolution to a sampled approximation which has limited resolution the name aliasing has been given to this effect because of the tie with aliasing in digital signal processing aliasing errors are caused by three related problems with the discrete nature of the frame buffer first if we have an n m frame buffer the number of pixels is ﬁxed and we can generate only certain patterns to approximate a line segment many different continuous line segments may be approximated by the same pattern of pixels we can say that all these segments are aliased as the same sequence of pixels given the sequence of pixels we cannot tell which line segment generated the sequence second pixel locations are ﬁxed on a uniform grid regardless of where we would like to place pixels we cannot place them at other than evenly spaced locations third pixels have a ﬁxed size and shape at ﬁrst glance it might appear that there is little we can do about such prob lems algorithms such as bresenham algorithm are optimal in that they choose the closest set of pixels to approximate lines and polygons however if we have a display that supports more than two colors there are other possibilities although mathe matical lines are one dimensional entities that have length but not width rasterized lines must have a width in order to be visible suppose that each pixel is displayed as a square of width unit and can occupy a box of unit height and width on the dis play our basic frame buffer can work only in multiples of one pixel we can think of an idealized line segment in the frame buffer as being one pixel wide as shown in fig ure of course we cannot draw this line because it does not consist of our square some frame buffers permit operations in units of less than one pixel through multisampling methods a b c d figure aliased versus antialiased line segments a aliased line segment b antialiased line segment c magnified aliased line segment d magnified antialiased line segment pixels we can view bresenham algorithm as a method for approximating the ideal one pixel wide line with our real pixels if we look at the ideal one pixel wide line we can see that it partially covers many pixel sized boxes it is our scan conversion algo rithm that forces us for lines of slope less than to choose exactly one pixel value for each value of x if instead we shade each box by the percentage of the ideal line that crosses it we get the smoother appearing image shown in figure b this technique is known as antialiasing by area averaging the calculation is similar to polygon clipping there are other approaches to antialiasing as well as antialiasing algorithms that can be applied to other primitives such as polygons color plate shows aliased and antialiased versions of a small area of the object in color plate a related problem arises because of the simple way that we are using the z buffer algorithm as we have speciﬁed that algorithm the color of a given pixel is determined by the shade of a single primitive consider the pixel shared by the three polygons shown in figure if each polygon has a different color the color assigned to the pixel is the one associated with the polygon closest to the viewer we could obtain a much more accurate image if we could assign a color based on an area weighted average of the colors of the three triangles such algorithms can be implemented with fragment shaders on hardware with ﬂoating point frame buffers we have discussed only one type of aliasing spatial domain aliasing when we generate sequences of images such as for animations we also must be concerned with time domain aliasing consider a small object moving in front of the projection plane that has been ruled into pixel sized units as shown in figure if our rendering process sends a ray through the center of each pixel and determines what it hits then sometimes we intersect the object and sometimes if the projection of the object is small we miss the object the viewer will have the unpleasant experience of seeing the object ﬂash on and off the display as the animation progresses there are several ways to deal with this problem for example we can use more than one ray per pixel a technique common in ray tracing what is common to all antialiasing techniques is that they require considerably more computation than does rendering without antialiasing in practice for high resolution images antialiasing is done off line and only when a ﬁnal image is needed figure polygons that share a pixel figure time domain aliasing display considerations in most interactive applications the application programmer need not worry about how the contents of the frame buffer are displayed from the application program mer perspective as long as she uses double buffering the process of writing into the frame buffer is decoupled from the process of reading the frame buffer contents for display the hardware redisplays the present contents of the frame buffer at a rate sufﬁcient to avoid ﬂicker usually to hz and the application programmer worries only about whether or not her program can execute and ﬁll the frame buffer fast enough as we saw in chapter the use of double buffering allows the display to change smoothly even if we cannot push our primitives through the system as fast as we would like numerous other problems affect the quality of the display and often cause users to be unhappy with the output of their programs for example the displays of two monitors may have the same nominal resolution but may display pixels of different sizes see exercises and perhaps the greatest source of problems with displays concerns the basic phys ical properties of displays the range of colors they can display and how they map software deﬁned colors to the values of the primaries for the display the color gamuts of different displays can differ greatly in addition because the primaries on different systems are different even when two different monitors can produce the same visible color they may require different values of the primaries to be sent to the displays from the graphics system in addition the mapping between brightness values deﬁned by the program and what is displayed is nonlinear opengl does not address these issues directly because colors are speciﬁed as rgb values that are independent of any display properties in addition because rgb primaries are limited to the range from to it is often difﬁcult to account for the full range of color and brightness detectable by the human visual system however if we expand on our discussion of color and the human visual system from chapter we can gain some additional control over color in opengl g r b figure color cube color systems our basic assumption about color supported by the three color theory of human vision is that the three color values that we determine for each pixel correspond to the tristimulus values that we introduced in chapter thus a given color is a point in a color cube as in figure and can be written symbolically as c however there are signiﬁcant differences across rgb systems for example suppose that we have a yellow color that opengl has represented with the rgb triplet if we use these values to drive both a crt and a ﬁlm image recorder we will see different colors even though in both cases the red is percent of maximum the green is percent of maximum and there is no blue the reason is that the ﬁlm dyes and the crt phosphors have different color distributions consequently the range of displayable colors or the color gamut is different for each the emphasis in the graphics community has been on device independent graphics consequently the real differences among display properties are not ad dressed by most apis fortunately the colorimetry literature contains the infor mation we need the standards for many of the common color systems exist for example crts are based on the national television systems committee ntsc rgb system we can look at differences in color systems as being equivalent to differ ent coordinate systems for representing our tristimulus values if b1 t and b2 t are the representations of the same color in two different systems then there is a color conversion matrix m such that whether we determine this matrix from the literature or by experimentation it allows us to produce similar displays on different output devices there are numerous potential problems even with this approach the color gamuts of the two systems may not be the same hence even after the conversion of tristimulus values a color may not be producible on one of the systems second the printing and graphic arts industries use a four color subtractive system cmyk that adds black k as a fourth primary conversion between rgb and cmyk often requires a great deal of human expertise third there are limitations to our linear color theory the distance between colors in the color cube is not a measure of how far apart the colors are perceptually for example humans are particularly sensitive to color shifts in blue color systems such as yuv and cie lab have been created to address such issues most rgb color systems are based on the primaries in real systems such as crt phosphors and ﬁlm dyes none can produce all the colors that we can see most color standards are based on a theoretical three primary system called the xyz color system here the y primary is the luminance of the color in the xyz system all colors can be speciﬁed with positive tristimulus values we use matrices to convert from an xyz color representation to representations in the standard systems color specialists often prefer to work with chromaticity coordinates rather than tristimulus values the chromaticity of a color consists of the three fractions of the color in the three primaries thus if we have the tristimulus values and for a particular rgb color its chromaticity coordinates are t t t adding the three equations we have and thus we can work in the two dimensional space ﬁnding only when its value is needed the information that is missing from chromaticity coordinates which was contained in the original tristimulus values is the sum t1 t2 t3 a value related to the intensity of the color when working with color systems this intensity is often not important to issues related to producing colors or matching colors across different systems because each color fraction must be nonnegative the chromaticity values are limited by ti figure triangle of producible colors in chromaticity coordinates nm y spectral colors crt colors nm nm x figure visible colors and color gamut of a display all producible colors must lie inside the triangle in figure figure shows this triangle for the xyz system and a curve of the representation for each visible spectral line for the xyz system this curve must lie inside the triangle figure also shows the range of colors in x y chromaticity coordinates that are producible on a typical color printer or crt if we compare the two ﬁgures we see that the colors inside the curve of pure spectral lines but outside the gamut of the physical display cannot be displayed on the physical device one defect of our development of color is that rgb color is based on how color is produced and measured rather than on how we perceive a color when we see a given g r b a b figure hue lightness saturation color a using the rgb color cube b using a single cone color we describe it not by three primaries but based on other properties such as the name we give the color and how bright a shade we see the hue saturation lightness hls system is used by artists and some display manufacturers the hue is the name we give to a color red yellow gold the lightness is how bright the color appears saturation is the color attribute that distinguishes a pure shade of a color from a shade of the same hue that has been mixed with white forming a pastel shade we can relate these attributes to a typical rgb color as shown in figure a given a color in the color cube the lightness is a measure of how far the point is from the origin black if we note that all the colors on the principal diagonal of the cube going from black to white are shades of gray and are totally unsaturated then the saturation is a measure of how far the given color is from this diagonal finally the hue is a measure of where the color vector is pointing hls colors are usually described in terms of a color cone as shown in figure b or a double cone that also converges at the top from our perspective we can look at the hls system as providing a representation of an rgb color in polar coordinates the color matrix rgb colors and rgba colors can be manipulated as any other vector type in par ticular we can alter their components by multiplying by a matrix we call the color matrix for example if we use an rgba color representation the matrix multiplica tion converts a color rgba toa new color rtg tbtat by the matrix multiplication rt r bt b thus if we are dealing with opaque surfaces for which a the matrix c y converts the additive representation of a color to its subtractive representation gamma correction in chapter we deﬁned brightness as perceived intensity and observed that the human visual system perceives intensity in a logarithmic manner as depicted in figure one consequence of this property is that if we want the brightness steps to appear to be uniformly spaced the intensities that we assign to pixels should increase exponentially these steps can be calculated from the measured minimum and maximum intensities that a display can generate in addition the intensity i of a crt is related to the voltage v applied by i vγ or log i γ log v where the constants γ and are properties of the particular crt one implication of these two results is that two monitors may generate different brightnesses for the same values in the frame buffer one way to correct for this problem is to have a lookup table in the display whose values can be adjusted for the particular character istics of the monitor the gamma correction there is an additional problem with crts it is not possible to have a crt whose display is totally black when no signal is applied the minimum displayed intensity is called the dark ﬁeld value and can be problematic especially when crt technology is used to project images the contrast ratio of a display is the ratio of the maximum to minimum brightness newer display technologies have contrast ratios in the thousands dithering and halftoning we have speciﬁed a color buffer by its spatial resolution the number of pixels and by its precision the number of colors it can display if we view these separate numbers as ﬁxed we say that a high resolution black and white laser printer can display only bit pixels this argument also seems to imply that any black and white medium such as a book cannot display images with multiple shades we know from experience that that is not the case the trick is to trade spatial resolution for grayscale or color precision halftoning techniques in the printing industry use photographic means to simulate gray levels by creating patterns of black dots of varying size the x intensity figure logarithmic brightness figure digital halftone patterns human visual system tends to merge small dots together and sees not the dots but rather an intensity proportional to the ratio of white to black in a small area digital halftones differ because the size and location of displayed pixels are ﬁxed consider a group of bit pixels as shown in figure if we look at this pattern from far away we see not the individual pixels but rather a gray level based on the number of black pixels for our example although there are different patterns of black and white pixels there are only possible shades corresponding to to black pixels in the array there are many algorithms for generating halftone or dither patterns the simplest picks patterns for our example and uses them to create a display with rather than two gray levels although at the cost of decreasing the spatial resolution by a factor of the simple algorithm always using the same array to simulate a shade can generate beat or moire patterns when displaying anything regular such patterns arise whenever we image two regular phenomena because we see the sum and dif ferences of their frequencies such effects are closely related to the aliasing problems we shall discuss in chapter many dithering techniques are based on simply randomizing the least signiﬁcant bit of the luminance or of each color component more sophisticated dither algo rithms use randomization to create patterns with the correct average properties but avoid the repetition that can lead to moire effects see exercise halftoning or dithering is often used with color especially with hard copy dis plays such as ink jet printers that can produce only fully on or off colors each pri mary can be dithered to produce more visual colors opengl supports such displays and allows the user to enable dithering glenable color dithering allows color monitors to produce smooth color displays and normally dithering is enabled because dithering is so effective displays can work well with a limited num ber of bits per color allowing frame buffers to have a limited amount of memory in many applications we need to use the opengl query function glgetintegerv to ﬁnd out how many bits are being used for each color since this information is impor tant when we use some of the techniques from chapter that read pixels from the frame buffer if dithering is enabled and we read pixels out of the frame buffer pixels that were written with the same rgb values may return different values when read if these small differences are important dithering should be disabled before reading from the frame buffer the geometry engine that was the basis of many silicon graphics workstations is a vlsi chip that performed geometric transformations and clipping through a hardware pipeline gl the predecessor of opengl was developed as an api for users of these workstations much of the opengl literature also follows the pipeline approach we should keep in mind however that opengl is an api it does not say anything about the underlying implementation in principle an image deﬁned by an opengl program could be obtained from a ray tracer we should carry away two lessons from our emphasis on pipeline architectures first this architecture provides an aid to the applications programmer in understanding the process of creating images second at present the pipeline view can lead to efﬁcient hardware and software implementations the example of the z buffer algorithm is illustrative of the relationship between hardware and software fifteen years ago many hidden surface removal algorithms were used of which the z buffer algorithm was only one the availability of fast dense inexpensive memory has made the z buffer algorithm the dominant method for hidden surface removal a related example is that of workstation architectures where special purpose graphics chips have made remarkable advances in just the past few years not only has graphics performance increased at a rate that exceeds moore law but many new features have become available in the graphics processors the whole approach we have taken in this book is based on these architectures so what does the future hold certainly graphics systems will get faster and less expensive more than any other factor advances in hardware probably will dictate what future graphics systems will look like at the present hardware development is being driven by the video game industry for less than we can purchase a graph ics card that exceeds the performance of graphics workstations that a few years ago would have cost more than the features and performance of these cards are optimized for the needs of the computer game industry thus we do not see uniform speedups in the various graphics functions that we have presented in addition new hardware features are appearing far faster than they can be incorporated into stan dard apis however the speed at which these processors operate has challenged both the graphics and scientiﬁc communities to discover new algorithms to solve problems that until now had always been solved using conventional architectures on the software side the low cost and speed of recent hardware has enabled soft ware developers to produce rendering software that allows users to balance rendering time and quality of rendering hence a user can add some ray traced objects to a scene the number depending on how long she is willing to wait for the rendering the future of standard apis is much less clear on one hand users in the scientiﬁc community prefer stable apis so that application codes will have a long lifetime on the other hand users want to exploit new hardware features that are not supported on all systems opengl has tried to take a middle road until opengl all releases were backward compatible so applications developed on earlier versions were guar anteed to run on new releases opengl and later versions deprecated many core features of earlier versions including immediate mode rendering and most of the default behavior of the ﬁxed function pipeline this major change in philosophy has allowed opengl to rapidly incorporate new hardware features for those who need to run older code almost all implementations support a compatibility extension with all the deprecated functions numerous advanced architectures under exploration use massive parallelism how parallelism can be exploited most effectively for computer graphics is still an open issue our two approaches to rendering object oriented and image oriented lead to two entirely different ways to develop a parallel renderer which we shall explore further in chapter we have barely scratched the surface of implementation the literature is rich with algorithms for every aspect of the implementation process the references should help you to explore this topic further suggested readings the books by rogers and by foley and colleagues contain many more algorithms than we can present here also see the series graphic gems gra94 and gpu gems books such as mo ller and haines and eberly cover the inﬂuence of recent advances in hardware the cohen sutherland clipping algorithm goes back to the early years of computer graphics as does bresenham algorithm which was originally proposed for pen plotters see and for the liang barsky and sutherland hogman clippers algorithms for triangulation can be found in references on computational ge ometry see for example de berg which also discusses delaunay triangula tion which we discuss in chapter the z buffer algorithm was developed by catmull see sutherland for a discussion of various approaches to hidden surface removal our decision to avoid details of the hardware does not imply that the hardware is either simple or uninteresting the rate at which a modern graphics processor can display graphical entities requires sophisticated and clever hardware designs the discussion by molnar and fuchs in shows a variety of approaches pratt provides matrices to convert among various color systems half tone and dithering are discussed by jarvis and by knuth exercises consider two line segments represented in parametric form p α α p1 αp2 q β β find a procedure for determining whether the segments intersect and if they do for ﬁnding the point of intersection extend the argument of exercise to ﬁnd a method for determining whether two ﬂat polygons intersect prove that clipping a convex object against another convex object results in at most one convex object in what ways can you parallelize the image and object oriented approaches to implementation because both normals and vertices can be represented in homogeneous coor dinates both can be operated on by the model view transformation show that normals may not be preserved by the transformation derive the viewport transformation express it in terms of the three dimensional scaling and translation matrices used to represent afﬁne trans formations in two dimensions pre raster graphics systems were able to display only lines programmers pro duced three dimensional images using hidden line removal techniques many current apis allow us to produce wireframe images composed of only lines in which the hidden lines that deﬁne nonvisible surfaces have been removed how does this problem differ from that of the polygon hidden surface removal that we have considered derive a hidden line removal algorithm for objects that consist of the edges of planar polygons often we display functions of the form y f x z by displaying a rectangular mesh generated by the set of values f xi zj evaluated at regular intervals in x and z hidden surface removal should be applied because parts of the surface can be obscured from view by other parts derive two algorithms one using hidden surface removal and the other using hidden line removal to display such a mesh although we argued that the complexity of the image space approach to hidden surface removal is proportional to the number of polygons perfor mance studies have shown almost constant performance explain this result consider a scene composed of only solid three dimensional polyhedra can you devise an object space hidden surface removal algorithm for this case how much does it help if you know that all the polyhedra are convex we can look at object space approaches to hidden surface removal as analo gous to sorting algorithms however we argued that the former complexity is o we know that only the worst performing sorting algorithms have such poor performance and most are o k log k does it follow that object space hidden surface removal algorithms have similar complexity explain your answer devise a method for testing whether one planar polygon is fully on one side of another planar polygon what are the differences between our image space approaches to hidden surface removal and to ray tracing can we use ray tracing as an alternate technique to hidden surface removal what are the advantages and disadvan tages of such an approach write a program to generate the locations of pixels along a rasterized line segment using bresenham algorithm check that your program works for all slopes and all possible locations of the endpoints what is the initial value of the decision variable bresenham algorithm can be extended to circles convince yourself of this statement by considering a circle centered at the origin which parts of the circle must be generated by an algorithm and which parts can be found by sym metry can you ﬁnd a part of the circle such that if we know a point generated by a scan conversion algorithm we can reduce the number of candidates for the next pixel show how to use ﬂood ﬁll to generate a maze like the one you created in exercise suppose that you try to extend ﬂood ﬁll to arbitrary closed curves by scan converting the curve and then applying the same ﬁll algorithm that we used for polygons what problems can arise if you use this approach consider the edge of a polygon between vertices at x1 y1 and y2 derive an efﬁcient algorithm for computing the intersection of all scan lines with this edge assume that you are working in window coordinates vertical and horizontal edges are potentially problematic for polygon ﬁll al gorithms how would you handle these cases for the algorithms that we have presented in two dimensional graphics if two polygons overlap we can ensure that they are rendered in the same order by all implementations by associating a priority attribute with each polygon polygons are rendered in reverse priority order that is the highest priority polygon is rendered last how should we modify our polygon ﬁll algorithms to take priority into account a standard antialiasing technique used in ray tracing is to cast rays not only through the center of each pixel but also through the pixel four corners what is the increase in work compared to casting a single ray through the center although an ideal pixel is a square of unit per side most crt systems generate round pixels that can be approximated as circles of uniform intensity if a completely full unit square has intensity and an empty square has intensity how does the intensity of a displayed pixel vary with the radius of the circle consider a bilevel display with round pixels do you think it is wiser to use small circles or large circles for foreground colored pixels explain your an swer why is defocusing the beam of a crt sometimes called the poor person antialiasing suppose that a monochrome display has a minimum intensity output of imin a crt display is never completely black and a maximum output of imax given that we perceive intensities in a logarithmic manner how should we assign k intensity levels such that the steps appear uniform generate a halftone algorithm based on the following idea suppose that gray levels vary from to and that we have a random number generator that produces random numbers that are uniformly distributed over this interval if we picka gray level g g percent of the random numbers generated will be less than g images produced on displays that support only a few colors or gray levels tend to show contour effects because the viewer can detect the differences between adjacent shades one technique for avoiding this visual effect is to add a little noise jitter to the pixel values why does this technique work how much noise should you add does it make sense to conclude that the degraded image created by the addition of noise is of higher quality than that of the original image show that the area of a two dimensional polygon speciﬁed by the vertices x1 yi is given by i yi yi xi xi what is the signiﬁcance of a negative area hint consider the areas of the trapezoids formed by two successive vertices and corresponding values on the x axis this page intentionally left blank hus far we have worked directly with geometric objects such as lines polygons and polyhedra although we understood that if visible these entities would eventually be rasterized into pixels in the frame buffer we did not have to concern ourselves with working with pixels directly over the last years the major advances in hardware and software have evolved to allow the application program to access the frame buffer both directly and indirectly many of the most exciting methods that have evolved over the past two decades rely on interactions between the application program and various buffers texture mapping antialiasing compositing and alpha blending are only a few of the techniques that become possible when the api allows us to work with discrete buffers at the same time gpus have evolved to include a large amount of memory to support discrete techniques this chapter introduces these techniques focusing on those that are supported by opengl and by similar apis we start by looking at the frame buffer in more detail and the basis for working with arrays of pixels we then consider mapping methods these techniques are applied during the rendering process and they enable us to give the illusion of a surface of great complexity although the surface might be a single polygon all these techniques use arrays of pixels to deﬁne how the shading process that we studied in chapter is augmented to create these illusions we shall then look at some of the other buffers that are supported by the opengl api and how these buffers can be used for new applications in particular we examine techniques for combining or compositing images here we use the fourth color in rgba mode and we shall see that we can use this channel to blend images and to create effects such as transparency we conclude with a discussion of the aliasing problems that arise whenever we work with discrete elements buffers we have already used two types of standard buffers color buffers and depth buffers there may be others supported by the hardware and software for special purposes what all buffers have in common is that they are inherently discrete they have m figure buffer limited resolution both spatially and in depth we can deﬁne a two dimensional buffer as a block of memory with n m k bit elements figure we have used the term frame buffer to mean the set of buffers that the graphics system uses for rendering including the front and back color buffers the depth buffer and other buffers the hardware may provide these buffers generally reside on the graphics card later in this chapter we will extend the notion of a frame buffer to include other buffers that a system might provide for off screen rendering operations for now we will work with just the standard frame buffer at a given spatial location in the frame buffer the k bits can include bit rgba colors integers representing depths or bits that can be used for masks groups of bits can store byte representations of color components integers for depths or ﬂoating point numbers for colors or depths figure shows the opengl frame buffer and some of its constituent parts if we consider the entire frame buffer the values of n and m match the spatial resolution of the display the depth of the frame buffer the value of k can exceed a few hundred bits even for the simple cases that we have seen so far we have bits for the front and back color buffers and bits for the depth buffer the numerical accuracy or precision of a given buffer is determined by its depth thus if a frame buffer has bits each for its front and back color buffers each rgba color component is stored with a precision of bits when we work with the frame buffer we usually work with one constituent buffer at a time thus we shall use the term buffer in what follows to mean a partic ular buffer within the frame buffer each of these buffers is n m and is k bits deep however k can be different for each buffer for a color buffer its k is determined by how many colors the system can display usually for rgb displays and for rgba displays for the depth buffer its k is determined by the depth precision that the system can support often bits to match the size of a ﬂoating point number or an integer we use the term bitplane to refer to any of the k n m planes in a buffer and pixel to refer to all k of the bits at a particular spatial location with this deﬁni tion a pixel can be a byte an integer or even a ﬂoating point number depending on which buffer is used and how data are stored in the buffer the applications programmer generally will not know how information is stored in the frame buffer because the frame buffer is inside the implementation which the programmer sees as a black box thus the application program sends writes or draws information into the frame buffer or obtains reads information from the frame buffer through opengl functions when the application program reads or writes pixels not only are data transferred between ordinary processor memory and graphics memory on the graphics card but usually these data must be reformatted to be compatible with the frame buffer consequently what are ordinarily thought of as digital images for example jpeg png or tiff images exist only on the application side of the process not only must the application programmer worry how to decode particular images so they can be sent to the frame buffer through opengl functions but the programmer also must be aware of the time that is spent in the movement we can also have one three and four dimensional buffers stencil buffer depth buffer m back buffer n front buffer figure opengl frame buffer of digital data between processor memory and the frame buffer if the application programmer also knows the internal format of how data are stored in any of the buffers she can often write application programs that execute more efﬁciently digital images before we look at how the graphics system can work with digital images through pixel and bit operations let ﬁrst examine what we mean by a digital image within our programs we generally work with images that are arrays of pixels these images can be of a variety of sizes and data types depending on the type of image with which we are working for example if we are working with rgb images we usually represent each of the color components with byte whose values range from to thus we might declare a image in our application program as glubyte myimage or if we are using a ﬂoating point representation typedef myimage if we are working with monochromatic or luminance images each pixel represents a gray level from black to white so we would use glubyte myimage most references often use the term image instead of digital image this terminology can be confused with using the term image to refer to the result of combining geometric objects and a camera through the projection process to obtain what we have called an image in this chapter the context should be clear so that there should not be any confusion one way to form digital images is through code in the application program for example suppose that we want to create a image that consists of an checkerboard of alternating red and black squares such as we might use for a game the following code will work check red black for int i i i for int j j j check i j i j red black usually writing code to form images is limited to those that contain regular patterns more often we obtain images directly from data for example if we have an array of real numbers that we have obtained from an experiment or a simulation we can scale them to go over the range to and then convert these data to form an unsigned byte luminance image or over to for a ﬂoating point image there is a third method of obtaining images that has become much more preva lent because of the inﬂuence of the internet images are produced by scanning con tinuous images such as photographs or produced directly using digital cameras each image is in one of many possible standard formats some of the most popu lar formats are gif tiff png pdf and jpeg these formats include direct coding of the values in some order compressed but lossless coding and compressed lossy coding each format arose from the particular needs of a group of applications for example postscript ps images are deﬁned by the postscript language used to con trol printers these images are an exact encoding of the image data either rgb or luminance into the bit ascii character set consequently postscript images can be understood by a large class of printers and other devices but tend to be very large encapsulated postscript eps are similar but include additional information that is useful for previewing images gif images are color index images and thus store a color table and an array of indices for the image tiff images can have two forms in one form all the image data are coded di rectly a header describes how the data are arranged in the second form the data are compressed compression is possible because most images contain much redundant data for example large areas of most images show very little variation in color or intensity this redundancy can be removed by algorithms that result in a compressed version of the original image that requires less storage compressed tiff images are formed by the lempel ziv algorithm that provides optimal lossless compression allowing the original image to be compressed and recovered exactly jpeg images are compressed by an algorithm that allows small errors in the compression and re construction of the image consequently jpeg images have very high compression ratios that is the ratio of the number of bits in the original ﬁle to the number of bits in the compressed data ﬁle with little or no visible distortion figure shows a b c figure a original tiff luminance image b jpeg image com pressed by a factor of c jpeg image compressed by a factor of three versions of a single luminance image uncompressed as a tiff image figure a and as two jpeg images compressed with different ratios fig ure b and figure c the corresponding ﬁle sizes are 80 and 962 bytes respectively thus the tiff image has byte for each pixel plus bytes of header and trailer information for the jpeg images the compression ra tios are approximately and even with the higher compression ratio there is little visible distortion in the image if we store the original image as a postscript im age the ﬁle will be approximately twice as large as the tiff image because each byte will be converted into two bit ascii characters each pair requiring bytes of stor age if we store the image as a compressed tiff ﬁle we need only about one half of the storage using a zip ﬁle a popular format used for compressing arbitrary ﬁles would give about the same result this amount of compression is image dependent although this compression method is lossless the compression ratio is far worse than is obtainable with lossy jpeg images which are visibly almost indistinguishable from the original this closeness accounts for the popularity of the jpeg format for send ing images over the internet most digital cameras produce images in jpeg and raw formats the raw format gives the unprocessed rgb data plus a large amount of header information including the date the resolution and the distribution of the colors the large number of image formats poses problems for a graphics api although some image formats are simple others are quite complex the opengl api avoids the problem by supporting only blocks of pixels as compared to images formatted for ﬁles most of these opengl formats correspond to internal formats that differ in the number of bits for each color component and the order of the components in memory there is limited support for compressed texture images but not for the standard formats such as jpeg hence although opengl can work with images that are arrays of standard data types in memory it is the application programmer responsibility to read any formatted images into processor memory and write them out as formatted ﬁles we will not deal with these issues here as any discussion would require us to discuss the details of particular image formats the necessary information can be found in the suggested readings at the end of the chapter we can also obtain digital images directly from our graphics system by forming images of three dimensional scenes using the geometric pipeline and then reading these images back we will see how to do the required operations later in the chapter writing into buffers in a modern graphics system a user program can both write into and read from the buffers there are two factors that make these operations different from the usual reading and writing into computer memory first we only occasionally want to read or write a single pixel or bit rather we tend to read and write rectangular blocks of pixels or bits known as bit blocks for example we rasterize an entire scan line at a time when we ﬁll a polygon we write a small block of pixels when we display a raster character we change the values of all pixels in a buffer when we do a clear operation hence it is important to have both the hardware and software support a set of operations that work on rectangular blocks of pixels known as bit block transfer bitblt operations as efﬁciently as possible these operations are also known as raster operations raster ops suppose that we want to take an n m block of pixels from one of our buffers the source buffer and to copy it into either the same buffer or another buffer the destination buffer this transfer is shown in figure a typical form for a bitblt write operation is source n m x y destination u v where source and destination are the buffers the operation writes an n m source block whose lower left corner is at x y to the destination buffer starting at source figure writing of a block a location u v although there are numerous details that we must consider such as what happens if the source block goes over the boundary of the destination block the essence of bitblt is that a single function call alters the entire destination block note that from the hardware perspective the type of processing involved has none of the characteristics of the processing of geometric objects consequently the hardware that optimizes bitblt operations has a completely different architecture from the ge ometric pipeline thus the opengl architecture contains both a geometry pipeline and a pixel pipeline each of which usually is implemented separately writing modes a second difference between normal writing into memory and bitblt operations is the variety of ways we can write into the buffers opengl supports different modes writing modes for putting pixel data into a buffer to understand the full range of possibilities let consider how we might write into a buffer the usual concept of a write to memory is replacement the execution of a statement inac program such as y x results in the value at the location where y is stored being replaced with the value at the location of x there are other possibilities suppose that we can work one bit at a time in our buffers consider the writing model in figure the bit that we wish to place in memory perhaps in an altered form is called the source bit the place in memory where we want to put it is called the destination bit d if as in chapter we are allowed to read before writing as depicted in figure then writing can be described by a replacement function f such that d f d for a bit source and destination there are only possible ways to deﬁne the function f namely the logical operations between two bits these operations are shown in figure where each of the columns on the right corresponds to one possible f we can use the binary number represented by each column to d d figure writing model d figure writing modes denote a writing mode equivalently we can denote writing modes by the logical operation deﬁned by the column suppose that we think of the logical value as corresponding to a background color say white and as corresponding to a foreground color say black we can examine the effects of various choices of f writing modes and are clear operations that change the value of the destination to either the foreground or the background color the new value of the destination bit is independent of both the source and the destination values modes and are the normal writing modes mode is the function d it simply replaces the value of the destination bit with the source mode is the logical or operation d d figure shows that these two writing modes can have different effects on the contents of the frame buffer in this example we write a dashed line into a frame buffer that already had a black foreground colored rectangle rendered into it both modes write the foreground color over the background color but they differ if we try to write the background color over the foreground color which mode should be used depends on what effect the application programmer wishes to create mode mode figure writing in modes and writing with xor mode is the exclusive or operation xor denoted by it is the most interesting of the writing modes unlike modes and mode cannot be implemented without a read of the destination bit the power of the xor write mode comes from the property that if and d are binary variables then d d thus if we apply xor twice to a bit we return that bit to its original state the most important applications of this mode involve interaction consider what happens when we use menus in an interactive application such as in a painting program in response to a mouse click a menu appears covering a portion of the screen after the user indicates an action from the menu the menu disappears and the area of the screen that it covered is returned to that area original state what has transpired involves the use of off screen memory known as backing store suppose that the menu has been stored off screen as an array of bits m and that the area of the screen where the menu appears is an array of bits s consider the sequence of operations s s m m s m s s m where we assume that the xor operation is applied to corresponding bits in s and m if we substitute the result of the ﬁrst equation in the second and the result of the second in the third we ﬁnd that at the end of the three operations the menu appears on the screen the original contents of the screen where the menu is now located are now off screen where the menu was originally we have swapped the menu with an area of the screen using three bitblt operations this method of swapping is considerably different from the normal mode of swapping which uses replacement mode writing but requires temporary storage to effect the swap there are numerous variants of this technique one is to move a cursor around the screen without affecting the area under it another is ﬁlling polygons with a solid color as part of scan conversion note that because many apis trace their backgrounds to the days before raster displays became the norm the xor write mode was not always available in opengl the standard mode of writing into the frame buffer is mode fragments are copied into the frame buffer we can change the mode by enabling logic operations gllogicop mode glenable where mode can be any of the modes the usual ones are the the default and mapping methods one of the most powerful uses of discrete data is for surface rendering the process of modeling an object by a set of geometric primitives and then rendering these primitives has its limitations consider for example the task of creating a virtual orange by computer our ﬁrst attempt might be to start with a sphere from our discussion in chapter we know that we can build an approximation to a sphere out of triangles and can render these triangles using material properties that match those of a real orange unfortunately such a rendering would be far too regular to look much like an orange we could instead follow the path that we shall explore in chapter we could try to model the orange with some sort of curved surface and then render the surface this procedure would give us more control over the shape of our virtual orange but the image that we would produce still would not look right although it might have the correct overall properties such as shape and color it would lack the ﬁne surface detail of the real orange if we attempt to add this detail by adding more polygons to our model even with hardware capable of rendering tens of millions of polygons per second we can still overwhelm the pipeline an alternative is not to attempt to build increasingly more complex models but rather to build a simple model and to add detail as part of the rendering process as we saw in chapter as the implementation renders a surface be it a polygon or a curved surface it generates sets of fragments each of which corresponds to a pixel in the frame buffer fragments carry color depth and other information that can be used to determine how they contribute to the pixels to which they correspond as part of the rasterization process we must assign a shade or color to each fragment we started in chapter by using the modiﬁed phong model to determine vertex colors that could be interpolated across surfaces however these colors can be modiﬁed dur ing fragment processing after rasterization the mapping algorithms can be thought of as either modifying the shading algorithm based on a two dimensional array the map or as modifying the shading by using the map to alter surface parameters such as material properties and normals there are three major techniques figure texture mapping a pattern to a surface texture mapping bump mapping environment mapping texture mapping uses an image or texture to inﬂuence the color of a fragment textures can be speciﬁed using a ﬁxed pattern such as the regular patterns often used to ﬁll polygons by a procedural texture generation method or through a digitized image in all cases we can characterize the resulting image as the mapping of a texture to a surface as shown in figure as part of the rendering of the surface whereas texture maps give detail by painting patterns onto smooth surfaces bump maps distort the normal vectors during the shading process to make the sur face appear to have small variations in shape such as the bumps on a real orange reﬂection maps or environment maps allow us to create images that have the ap pearance of reﬂected materials without our having to trace reﬂected rays in this technique an image of the environment is painted onto the surface as that surface is being rendered the three methods have much in common all three alter the shading of indi vidual fragments as part of fragment processing all rely on the map being stored as a one two or three dimensional digital image all keep the geometric complexity low while creating the illusion of complex geometry however all are also subject to aliasing errors there are various examples of two dimensional mappings in the color plates color plate was created using an opengl environment map and shows how a single texture map can create the illusion of a highly reﬂective surface while avoiding global calculations color plate uses a texture map for the surface of the table color plate uses texture mapping to create a brick pattern in virtual reality visualization simulations and interactive games real time performance is required hardware support for texture mapping in modern systems allows the detail to be added without signiﬁcantly degrading the rendering time however in terms of the standard pipeline there are signiﬁcant differences among the three techniques standard texture mapping is supported by the basic opengl pipeline and makes use of both the geometric and pixel pipelines environ ment maps are a special case of standard texture mapping but can be altered to create a variety of new effects if we can alter fragment processing bump mapping requires us to process each fragment independently something we can do with a fragment shader texture mapping textures are patterns they can range from regular patterns such as stripes and checkerboards to the complex patterns that characterize natural materials in the real world we can distinguish among objects of similar size and shape by their textures thus if we want to create detailed virtual objects we can extend our present capabil ities by mapping a texture to the objects that we create textures can be one two three or four dimensional for example a one dimensional texture might be used to create a pattern for coloring a curve a three dimensional texture might describe a solid block of material from which we could sculpt an object because the use of surfaces is so important in computer graphics mapping two dimensional textures to surfaces is by far the most common use of tex ture mapping and will be the only form of texture mapping that we shall consider in detail however the processes by which we map these entities is much the same regardless of the dimensionality of the texture and we lose little by concentrating on two dimensional texture mapping two dimensional texture mapping although there are multiple approaches to texture mapping all require a sequence of steps that involve mappings among three or four different coordinate systems at var ious stages in the process we shall be working with screen coordinates where the ﬁnal image is produced object coordinates where we describe the objects upon which the textures will be mapped texture coordinates which we use to locate positions in the texture and parametric coordinates which we use to help us deﬁne curved surfaces methods differ according to the types of surfaces we are using and the type of render ing architecture we have our approach will be to start with a fairly general discussion of texture introducing the various mappings and then to show how texture mapping is handled by a real time pipeline architecture such as that employed by opengl in most applications textures start out as two dimensional images of the sorts we introduced in section thus they might be formed by application programs or scanned in from a photograph but regardless of their origin they are eventu ally brought into processor memory as arrays we call the elements of these arrays texels or texture elements rather than pixels to emphasize how they will be used however at this point we prefer to think of this array as a continuous rectangu lar two dimensional texture pattern t t the independent variables and t are known as texture coordinates with no loss of generality we can scale our texture coordinates to vary over the interval in four dimensions the coordinates are in t r q space a texture map associates a texel with each point on a geometric object that is itself mapped to screen coordinates for display if the object is represented in homogeneous or x y z w coordinates then there are functions such that x x t y y t z z t w w t one of the difﬁculties we must confront is that although these functions exist con ceptually ﬁnding them may not be possible in practice in addition we are worried about the inverse problem having been given a point x y z or x y z w on an object how do we ﬁnd the corresponding texture coordinates or equivalently how do we ﬁnd the inverse functions x y z w t t x y z w to use to ﬁnd the texel t t if we deﬁne the geometric object using parametric u v surfaces such as we did for the sphere in section there is an additional mapping function that gives object coordinate values x y z or x y z w in terms of u and v although this mapping is known for simple surfaces such as spheres and triangles and for the surfaces that we shall discuss in chapter we also need the mapping from parametric coordinates u v to texture coordinates and sometimes the inverse mapping from texture coordinates to parametric coordinates we also have to consider the projection process that take us from object coor dinates to screen coordinates going through eye coordinates clip coordinates and window coordinates along the way we can abstract this process through a function that takes a texture coordinate pair t and tells us where in the color buffer the corresponding value of t t will make its contribution to the ﬁnal image thus there is a mapping of the form xs xs t ys ys t into coordinates where xs ys is a location in the color buffer depending on the algorithm and the rendering architecture we might also want the function that takes us from a pixel in the color buffer to the texel that makes a contribution to the color of that pixel one way to think about texture mapping is in terms of two concurrent map pings the ﬁrst from texture coordinates to object coordinates and the second from parametric coordinates to object coordinates as shown in figure a third map ping takes us from object coordinates to screen coordinates v u y xs t z figure texture maps for a parametric surface conceptually the texture mapping process is simple a small area of the texture pattern maps to the area of the geometric surface corresponding to a pixel in the ﬁnal image if we assume that the values of t are rgb color values we can use these values either to modify the color of the surface that might have been determined by a lighting model or to assign a color to the surface based on only the texture value this color assignment is carried out as part of the assignment of fragment colors on closer examination we face a number of difﬁculties first we must determine the map from texture coordinates to object coordinates a two dimensional texture usually is deﬁned over a rectangular region in texture space the mapping from this rectangle to an arbitrary region in three dimensional space may be a complex func tion or may have undesirable properties for example if we wish to map a rectangle to a sphere we cannot do so without distortion of shapes and distances second owing to the nature of the rendering process which works on a pixel by pixel basis we are more interested in the inverse map from screen coordinates to texture coordinates it is when we are determining the shade of a pixel that we must determine what point in the texture image to use a calculation that requires us to go from screen coordinates to texture coordinates third because each pixel corresponds to a small rectangle on the display we are interested in mapping not points to points but rather areas to ar eas here again is a potential aliasing problem that we must treat carefully if we are to avoid artifacts such as wavy sinusoidal or moire patterns figure shows several of the difﬁculties suppose that we are computing a color for the square pixel centered at screen coordinates xs ys the center xs ys corresponds to a point x y z in object space but if the object is curved the projection of the corners of the pixel backward into object space yields a curved y xs t x ys z figure preimages of a pixel t v u figure aliasing in texture generation preimage of the pixel in terms of the texture image t t projecting the pixel back yields a preimage in texture space that is the area of the texture that ideally should contribute to the shading of the pixel let put aside for a moment the problem of how we ﬁnd the inverse map and let us look at the determination of colors one possibility is to use the location that we get by back projection of the pixel center to ﬁnd a texture value although this technique is simple it is subject to serious aliasing problems which are especially visible if the texture is periodic figure illustrates the aliasing problem here we have a repeated texture and a ﬂat surface the back projection of the center of each pixel happens to fall in between the dark lines and the texture value is always the lighter color more generally not taking into account the ﬁnite size of a pixel can lead to moire patterns in the image a better strategy but one more difﬁcult to implement is to assign a texture value based on averaging of the texture map over the preimage note that this method is imperfect too for the example in figure we would assign an average shade but we would still not get the striped pattern of the texture ultimately we still have aliasing defects due to the limited resolution of both the frame buffer and the texture map these problems are most visible when there are regular high frequency components in the texture now we can turn to the mapping problem in computer graphics most curved surfaces are represented parametrically a point p on the surface is a function of two t xs smax tmax smin tmin umax vmax umin vmin ys figure linear texture mapping parameters u and v for each pair of values we generate the point x u v z u v in chapter we study in detail the derivation of such surfaces given a parametric surface we can often map a point in the texture map t t to a point on the surface p u v by a linear map of the form u as bt c v ds et f as long as ae bd this mapping is invertible linear mapping makes it easy to map a texture to a group of parametric surface patches for example if as shown in figure the patch determined by the corners smin tmin and smax tmax corresponds to the surface patch with corners umin vmin and umax vmax then the mapping is u u min smin smax smin umax umin v v min t tmin tmax tmin vmax vmin this mapping is easy to apply but it does not take into account the curvature of the surface equal sized texture patches must be stretched to ﬁt over the surface patch another approach to the mapping problem is to use a two part mapping the ﬁrst step maps the texture to a simple three dimensional intermediate surface such as a sphere cylinder or cube in the second step the intermediate surface containing the mapped texture is mapped to the surface being rendered this two step mapping process can be applied to surfaces deﬁned in either geometric or parametric coordi nates the following example is essentially the same in either system suppose that our texture coordinates vary over the unit square and that we use the surface of a cylinder of height h and radius r as our intermediate object as shown t figure texture mapping with a cylinder in figure points on the cylinder are given by the parametric equations x r cos u y r sin z v h as u and v vary over hence we can use the mapping u t v by using only the curved part of the cylinder and not the top and bottom we were able to map the texture without distorting its shape however if we map to a closed object such as a sphere we must introduce shape distortion this problem is similar to the problem of creating a two dimensional image of the earth for a map if you look at the various maps of the earth in an atlas all distort shapes and distances both texture mapping and map design techniques must choose among a variety of representations based on where we wish to place the distortion for example the familiar mercator projection puts the most distortion at the poles if we use a sphere of radius r as the intermediate surface a possible mapping is x r cos u y r sin cos v z r sin sin we can also use a rectangular box as shown in figure here we map the texture to a box that can be unraveled like a cardboard packing box this mapping often is used with environment maps section the second step is to map the texture values on the intermediate object to the desired surface figure shows three possible strategies in figure a we take the texture value at a point on the intermediate object go from this point in the direction of the normal until we intersect the object and then place the texture value at the point of intersection we could also reverse this method starting at a point on the surface of the object and going in the direction of the normal at this point until t figure texture mapping with a box intermediate object a b c figure second mapping a using the normal from the interme diate surface b using the normal from the object surface c using the center of the object we intersect the intermediate object where we obtain the texture value as shown in figure b a third option if we know the center of the object is to draw a line from the center through a point on the object and to calculate the intersection of this line with the intermediate surface as shown in figure c the texture at the point of intersection with the intermediate object is assigned to the corresponding point on the desired object texture mapping in opengl opengl supports a variety of texture mapping options the ﬁrst version of opengl contained the functionality to map one and two dimensional textures to one through four dimensional graphical objects mapping of three dimensional textures now is part of opengl and is supported by most hardware we will focus on mapping two dimensional textures to surfaces opengl texture maps rely on its pipeline architecture we have seen that there are actually two parallel pipelines the geometric pipeline and the pixel pipeline for texture mapping the pixel pipeline merges with fragment processing after raster ization as shown in figure this architecture determines the type of texture mapping that is supported in particular texture mapping is done as part of frag ment processing each fragment that is generated is then tested for visibility with the z buffer we can think of texture mapping as a part of the shading process but a part vertices pixels figure pixel and geometry pipelines that is done on a fragment by fragment basis texture coordinates are handled much like normals and colors they are associated with vertices through the opengl state and the required texture values can then be obtained by interpolating the texture co ordinates at the vertices across polygons two dimensional texture mapping texture mapping requires interaction among the application program the vertex shader and the fragment shader there are three basic steps first we must form a texture image and place it in texture memory on the gpu second we must assign texture coordinates to each fragment finally we must apply the texture to each fragment each of these steps can be accomplished in multiple ways and there are many parameters that we can use to control the process as texture mapping has become more important and gpus have evolved to support more texture mapping options apis have added more and more texture mapping functions texture objects in early versions of opengl there was only a single texture the current texture that existed at any time each time that a different texture was needed for example if we wanted to apply different textures to different surfaces in the same scene we had to set up a new texture map this process was very inefﬁcient each time another texture image was needed it had to be loaded into texture memory replacing the texels that were already there in a manner analogous to having multiple program objects textureobjects allow the application program to deﬁne objects that consist of the texture array and the various texture parameters that control its application to surfaces as long as there is sufﬁcient memory to retain them these objects reside in texture memory we create a two dimensional texture object by ﬁrst getting some unused iden tiﬁers by calling glgentextures for a single texture we could use the following code gluint mytex glgentextures mytex we start forming a new texture object with the function glbindtexture as in the following code glbindtexture mytex subsequent texture functions specify the texture image and its parameters which become part of the texture object another call to glbindtexture with a new name starts a new texture object a later execution of glbindtexture with an existing name makes that texture object the current texture object we can delete unused texture objects by gldeletetextures the texture array two dimensional texture mapping starts with an array of texels which is a two dimensional pixel rectangle suppose that we have a image that was generated by our program or perhaps was read in from a ﬁle into an array glubyte we specify that this array is to be used as a two dimensional texture after a call to glbindtexture by more generally two dimensional textures are speciﬁed through the functions glenum target glint level glint iformat glsizei width glsizei height glint border glenum format glenum type glvoid tarray the target parameter lets us choose a single image as in our example set up a cube map section or test if there is sufﬁcient texture memory for a texture image the level parameter is used for mipmapping section where denotes the highest level or resolution or that we are not using mipmapping the third param eter speciﬁes how we would like the texture stored in texture memory the fourth and ﬁfth parameters width and height specify the size of the image in memory the border parameter is no longer used and should be set to the format and type parameters describe how the pixels in the image in processor memory are stored so that opengl can read those pixels and store them in texture memory texture coordinates and samplers the key element in applying a texture in the fragment shader is the mapping between the location of a fragment and the corresponding location within the texture image where we will get the texture color for that fragment because each fragment has a t figure mapping to texture coordinates location in the frame buffer that is one of its attributes we need not refer to this position explicitly in the fragment shader the potential difﬁculty is identifying the desired location in the texture image in many applications we could compute this location from a mathematical model of the objects in others we might use some sort of approximation opengl does not have any preferred method and simply requires that we provide the location to the fragment shader or compute it within the shader rather than having to use integer texel locations that depend on the dimensions of the texture image we use two ﬂoating point texture coordinates and t both of which range over the interval as we traverse the texture image for our example of a 512 512 two dimensional texture image the value corresponds to the texel and corresponds to the texel 511 as shown in figure any values of and t in the unit interval correspond to a unique texel it is up to the application and the shaders to determine the appropriate texture coordinates for a fragment the most common method is to treat texture coordinates as a vertex attribute thus we could provide texture coordinates just as we provide vertex colors in the application we then would pass these coordinates to the vertex shader and let the rasterizer interpolate the vertex texture coordinates to fragment texture coordinates let consider a simple example of using our checkerboard texture image for each side of the cube example the example is particularly simple because we have an obvious mapping between each face of the cube and the texture coordinates for each vertex namely we assign texture coordinates and to the four corners of each face recall that we form triangles for the six faces we add an array to hold the texture coordinates define n glfloat n here is the code of the quad function typedef void quad int a int b int c int d static int i vertex and color index i colors a points i vertices a i i i colors a points i vertices b i i i colors a points i vertices c i i i colors a points i vertices a i i i colors a points i vertices c i i i colors a points i vertices d i i we also need to do our initialization so we can pass the texture coordinates as a vertex attribute with the identiﬁer texcoord in the vertex shader gluint glgetattriblocation program texcoord glenablevertexattribarray glvertexattribpointer next we need to initialize the buffer object to store all of the data since we have three separate arrays of data we will need to load them into an appropriately sized buffer in three operations using glbuffersubdata glintptr offset glsizeiptr size sizeof points sizeof sizeof glgenbuffers buffer glbindbuffer buffers glbufferdata size null offset glbufferdata offset sizeof points points offset sizeof points glbufferdata offset sizeof offset sizeof glbufferdata offset sizeof turning to the vertex shader we add the texture coordinate attribute and output the texture coordinates here is the vertex shader for the rotating cube with texture coordinates in texcoord in vposition in vcolor out color out st uniform theta void main rx ry rz c cos theta sin theta rz c z z z c z ry c y y y c y rx c x x x c x rz ry rx vposition color vcolor st texcoord the output texture coordinates st are interpolated by the rasterizer and can be inputs to the fragment shader note that the vertex shader is only concerned with the texture coordinates and has nothing to do with the texture object we created earlier we should not be sur prised because the texture itself is not needed until we are ready to assign a color to a fragment that is in the fragment shader note also that many of the complexities of how we can apply the texture many of which we have yet to discuss are inside the texture object and thus will allow us to use very simple fragment shaders the key to putting everything together is a new type of variable called a sampler which most often appears only in a fragment shader a sampler variable provides access to a texture object including all its parameters there are sampler variables for the types of textures supported by opengl including one dimensional sam two dimensional and three dimensional tex tures and special types such as cube maps samplercube we link the texture object mytex we created in the application with the shader using a uniform variable gluint glgetuniformlocation program texmap where texmap is the name of the sampler in the fragment shader and the second parameter in refers to the default texture unit we will discuss mutliple texture units in section the fragment shader is almost trivial the interpolated vertex colors and the texture coordinates are input variables if we want the texture values to multiply the colors as if we were using the checkerboard texture to simulate glass that alternates between clear and opaque we could multiply the colors from the application by the values in the texture image as in the following fragment shader in st in color uniform texmap void main color texmap st in the example shown in figure a we use the whole texture on a rectangle if we used only part of the range of and t for example we would use only part of for the texture map and would get an image like that in figure b opengl interpolates and t across the quadrilateral then maps these values back to the appropriate texel in the quadrilateral example is simple because there is an obvious mapping of texture coordinates to vertices for general polygons the application programmer must decide how to assign the texture a b figure mapping of a checkerboard texture to a quadrilateral a using the entire texel array b using part of the texel array a b c figure mapping of texture to polygons a and b mapping of a checkerboard texture to a triangle c mapping of a checkerboard texture to a trapezoid coordinates figure shows a few of the possibilities with the same texture map figures a and b use the same triangle but different texture coordinates note the artifacts of the interpolation and how quadrilaterals are treated as two triangles as they are rendered in figure c the basics of opengl texture mapping are simple specify an array of colors for the texture values then assign texture coordinates unfortunately there are a few nasty details that we must discuss before we can use texture effectively solving the resulting problems involves making trade offs between quality of the images and efﬁciency one problem is how to interpret a value of or t outside of the range generally we want the texture either to repeat if we specify values outside this range or to clamp the values to or that is we want to use the values at and for values below and above the interval respectively for repeated textures we set these parameters via gltexparameteri for t we use for clamping we use by executing these functions after the glbindtexture the parameters become part of the texture object point sample figure texels used with linear filtering texture sampling aliasing of textures is a major problem when we map texture coordinates to the array of texels we rarely get a point that corresponds to the center of a texel one option is to use the value of the texel that is closest to the texture coordinate output by the rasterizer this option is known as point sampling but it is the one most subject to visible aliasing errors a better strategy although one that requires more work is to use a weighted average of a group of texels in the neighborhood of the texel determined by point sampling this option is known as linear ﬁltering thus in figure 20 we see the location within a texel that is given by bilinear interpolation from the texture coordinates at the vertices and the four texels that would be used to obtain a smoother value if we are using linear ﬁltering there is a problem at the edges of the texel array as we need additional texel values outside the array there is a further complication however in deciding how to use the texel values to obtain a texture value the size of the pixel that we are trying to color on the screen may be smaller or larger than one texel as shown in figure 21 in the ﬁrst case the texel is larger than one pixel magniﬁcation in the second it is smaller miniﬁcation in both cases the fastest strategy is to use the value of the nearest point sampling we can specify this option for both magniﬁcation and miniﬁcation of textures as follows gltexparameteri gltexparameteri alternatively we can use ﬁltering to obtain a smoother less aliased image if we specify instead of opengl has another way to deal with the miniﬁcation problem it is called mipmapping for objects that project to an area of screen space that is small com pared with the size of the texel array we do not need the resolution of the original t xs t xs ys a ys b figure 21 mapping texels to pixels a minification b magnification texel array opengl allows us to create a series of texture arrays at reduced sizes it will then automatically use the appropriate size for a 64 64 original array we can set up and arrays for the current texture object by executing the function call glgeneratemipmap we can also set up the maps directly using the level parameter in gltexim this parameter is the level in the mipmap hierarchy for the speciﬁed texture array thus level refers to the original image level to the image at half resolution and so on however we can give a pointer to any image in different calls to gltex and thus can have entirely different images used at different levels of the mipmap hierarchy these mipmaps are invoked automatically if we specify gltexparameteri this option asks opengl to use point sampling with the best mipmap we can also do ﬁltering within the best mipmap point sampling using linear ﬁltering between mipmaps or both ﬁlters figure shows the differences in mapping a texture using the nearest texel linear ﬁltering and mipmapping both with using the nearest texel and with linear ﬁltering the object is a quadrilateral that appears almost as a triangle when shown in perspective the texture is a series of black and white lines that is applied so that the lines converge at the far side of the quadrilateral note that this texture map because of its regularity shows dramatic aliasing effects the use of the nearest texel shows moire patterns and jaggedness in the lines using linear ﬁltering makes the lines smoother but there are still clear moire patterns the texels between the black and white stripes are gray because of the ﬁltering mipmapping also replaces many of the blacks and whites of the two color patterns with grays that are the average of the two color values for the parts of the object that are farthest from the viewer the texels are gray and blend with the background the mipmapped texture using the nearest texel in the proper mipmap still shows the jaggedness that is smoothed out when we use linear ﬁltering with the mipmap advances in the speed of graphics processors gpus and the inclusion of large amounts of texture memory in these gpus often allows applications to use ﬁltering and mipmapping without a performance penalty a ﬁnal issue in using textures in opengl is the interaction between texture and shading for rgb colors there are multiple options the texture can modulate the shade that we would have assigned without texture mapping by multiplying the color components of the texture by the color components from the shader we could let the color of the texture totally determine the color of a fragment a technique called decaling these and other options are easily implemented in the fragment shader a b c d figure 22 texture mapping to a quadrilateral a point sampling b linear filtering c mipmapping point sampling d mipmapping linear filtering working with texture coordinates our examples so far have assumed implicitly that we know how to assign texture coordinates if we work with rectangular polygons of the same size then it is fairly easy to assign coordinates we can also use the fact that texture coordinates can be stored as one two three or four dimensional arrays just as are vertices thus texture coordinates can be transformed by matrices and manipulated in the same manner as we transformed positions with the model view and projection matrices we can use a texture matrix to scale and orient textures and to create effects in which the texture moves with the object the camera or the lights however if the set of polygons is an approximation to a curved object then as signing texture coordinates is far more difﬁcult consider the polygonal approxima figure polygonal model of utah teapot figure texture mapped utah teapot tion of the utah in figure although the model uses only quadrilaterals these quadrilaterals differ in size with smaller quadrilaterals in areas where the ob ject has higher curvature and larger quadrilaterals in ﬂatter areas figure shows our checkerboard texture mapped to the teapot without making any adjustment for the different sizes of the polygons as we can see by assigning the same set of texture coordinates to each polygon the texture mapping process adjusts to the individual sizes of the polygons by scaling the texture map as needed hence in areas such as the handle where many small polygons are needed to give a good approximation to the curved surface the black and white boxes are small compared to those on the body of the teapot in some applications these patterns are acceptable however if all surfaces of the teapot were made from the same material we would expect to see the same pattern on all its parts in principle we could use the texture matrix to scale texture coordinates to achieve the desired display however in practice it is almost impossible to determine the necessary information from the model to form the matrix one solution to this problem is to generate texture coordinates for each vertex in terms of the distance from a plane in either eye coordinates or object coordinates mathematically each texture coordinate is given as a linear combination of the ho mogeneous coordinate values thus for and t we shall discuss the utah teapot in detail in chapter a b figure teapot using texture coordinate generation a in object coordinates b in eye coordinates asx bsy csz dsw t atx bty ctz dtw figure 25 a shows the teapot with texture coordinate generation in object space figure 25 b uses the same equations but with the calculations in eye space by doing the calculation in object space the texture is ﬁxed to the object and thus will rotate with the object using eye space the texture pattern changes as we apply transformations to the object and give the illusion of the object moving through a texture ﬁeld one of the important applications of this technique is in terrain generation and mapping we can map surface features as textures directly onto a three dimensional mesh multitexturing thus far we have looked at applying a single texture to an object however there are many surface rendering effects that can best be implemented by more than a single application of a texture for example suppose that we want to apply a shadow to an object whose surface shades are themselves determined by a texture map we could use a texture map for the shadow but if there were only a single texture application this method would not work fragment figure sequence of texture units if instead we have multiple texture units as in figure 26 then we can accom plish this task each unit acts as an independent texturing stage starting with the results of the previous stage this facility is supported in recent versions of opengl suppose that we want to use two texture units we can deﬁne two texture objects as part of our initialization we then activate each in turn and decide how its texture should be applied the usual code is of the form glactivetexture unit glbindtexture how to apply texture glactivetexture unit glbindtexture how to apply texture each texture unit can use different texture coordinates and the application needs to provide those texture coordinates for each unit frame buffer texture generation one of the most powerful uses of texture mapping is to provide detail without generating numerous geometric objects high end graphics systems can do two dimensional texture mapping in real time for every frame the texture is mapped to objects as part of the rendering process at almost the same rate as non texture mapped objects are processed graphics boards for personal computers now contain a signiﬁcant amount of texture memory and allow game developers to use texture mapping to create complex animated environments if for example we want to simulate grass in a scene we can texture map an image of grass that we might have obtained by say scanning a photograph faster than we can generate two or three dimensional objects that look like grass in mapping applications rather than generating realistic surface detail for terrain we can digitize a real map and paint it on a three dimensional surface model by texture mapping we can also look for procedural methods for determining texture patterns of particular interest are patterns that we see in nature such as the textures of sand grass or minerals these textures show both structure regular patterns and consid erable randomness most approaches to generating such textures algorithmically start with a random number generator and process its output as shown in figure we shall study procedural noise in detail in chapter noise texture figure texture generation the generation of a three dimensional texture ﬁeld t t r is a direct extension of two dimensional texture generation techniques there are some practical advan tages to using three dimensional textures most important is that by associating each t r value directly with an x y z point we can avoid the mapping problem en tirely the user needs only to deﬁne a function t t r with the desired properties conceptually this process is similar to sculpting the three dimensional object from a solid block whose volume is colored by the speciﬁed texture this technique has been used to generate objects that look as if they have been carved from solid rock the texture generation process deﬁnes a function t t r that displays the graininess we associate with materials such as marble and granite there are other advantages to using three dimensional textures suppose that we have a two dimensional texture that we obtained by photographing or modeling some natural material such as stone now suppose that we want to create a cube that looks like it was formed from this stone if we use two dimensional texture mapping we have to map the same pattern to the six sides of the cube to make the cube look real we must try to make the texture map appear continuous at the edges of the cube where two texture maps meet when we work with natural patterns it is virtually impossible to ensure that we can do this matching note that the problem is even more serious at the vertices of the cube where three texture maps meet see exercise 26 often we can use ﬁltering and texture borders to give visually acceptable results however if we use three dimensional textures this problem does not arise environment maps highly reﬂective surfaces are characterized by specular reﬂections that mirror the en vironment consider for example a shiny metal ball in the middle of a room we can see the contents of the room in a distorted form on the surface of the ball obviously this effect requires global information as we cannot shade the ball correctly without knowing about the rest of the scene a physically based rendering method such as a ray tracer can produce this kind of image although in practice ray tracing calcula tions usually are too time consuming to be practical for real time applications we can however use variants of texture mapping that can give approximate results that are visually acceptable through environment maps or reﬂection maps the basic idea is simple consider the mirror in figure which we can look at as a polygon whose surface is a highly specular material from the point of view of a renderer the position of the viewer and the normal to the polygon are known so that the angle of reﬂection is determined as in chapter if we follow along this figure 28 scene with a mirror angle until we intersect the environment we obtain the shade that is reﬂected in the mirror of course this shade is the result of a shading process that involves the light sources and materials in the scene we can obtain an approximately correct value of this shade as part of a two step rendering pass similar in some respects to the two step texture mapping process that we outlined in section in the ﬁrst pass we render the scene without the mirror polygon with the camera placed at the center of the mirror pointed in the direction of the normal of the mirror thus we obtain an image of the objects in the environment as seen by the mirror this image is not quite correct exercise but is usually good enough we can then use this image to obtain the shades texture values to place on the mirror polygon for the second normal rendering with the mirror placed back in the scene there are two difﬁculties with this approach first the images that we obtain in the ﬁrst pass are not quite correct because they have been formed without one of the objects the mirror in the environment second we must confront the mapping issue onto what surface should we project the scene in the ﬁrst pass and where should we place the camera potentially we want all the information in the scene as we may want to do something like have our mirror move so that we should see different parts of the environment on successive frames and thus a simple projection will not sufﬁce there have been a variety of approaches to this projection problem the clas sic approach is to project the environment onto a sphere centered at the center of projection in figure we see some polygons that are outside the sphere and their projections on the sphere note that a viewer located at the center of the sphere cannot tell whether she is seeing the polygons in their original positions or their pro jections on the sphere this illusion is similar to what we see in a planetarium the stars that appear to be an inﬁnite distance away are actually the projection of lights onto the hemisphere which encloses the audience in the original version of environment mapping the surface of the sphere was then converted to a rectangle using lines of longitude and latitude for the mapping although conceptually simple there are problems at the poles where the shape dis tortion becomes inﬁnite computationally this mapping does not preserve areas very well and requires evaluating a large number of trigonometric functions object in environment intermediate surface projected object figure mapping of the environment x figure reflection map opengl supports a variation of this method called sphere mapping the appli cation program supplies a circular image that is the orthographic projection of the sphere onto which the environment has been mapped the advantage of this method is that the mapping from the reﬂection vector to two dimensional texture coordi nates on this circle is simple and can be implemented in either hardware or software the difﬁcult part is obtaining the required circular image it can be approximated by taking a perspective projection with a very wide angle lens or by remapping some other type of projection such as the cube projection that we discuss next we load the texture image in texture memory through the equations for generating the texture coordinates can be understood with the help of figure it is probably easiest if we work backward from the viewer to the image suppose that the texture map is in the plane z d where d is positive and we project backward orthogonally toward a unit sphere centered at the origin thus if the textu re coordinates in the plane are t then the projector intersects the sphere at t for the unit sphere centered at the origin the coordinates of any point on the sphere are also the components of the unit normal at that point we can then compute the direction of reﬂection as in chapter by r n v n v where n t the vector r points into the environment thus any object that r intersects has texture coordinates t however this argument is backward because we start with an object deﬁned by vertices given r we can solve for and t and ﬁnd that if rx rz then rx f ry t f where f r if we put everything into eye coordinates we compute r using the unit vector from the origin to the vertex for v and the vertex normal for n this process reveals some issues that show that this method is only approximate the reﬂection map is only correct for the vertex at the origin in principle each vertex should have its own reﬂection map actually each point on the object should have its own map and not an approximate value computed by interpolation the errors are most signiﬁcant the farther the object is from the origin nevertheless reﬂection reflective object v figure reflective cube map mapping gives visually acceptable results in most situations especially when there is animation as in ﬁlms and games if we want to compute an environment map using the graphics system we prefer to use the standard projections that are supported by the graphics systems for an environment such as a room the natural intermediate object is a box we compute six projections corresponding to the walls ﬂoor and ceiling using six virtual cameras located at the center of the box each pointing in a different direction at this point we can treat the six images as a single environment map and derive the textures from it as in figure color plate 23 shows one frame from pixar animation studio geri game a reﬂection map was computed on a box color plate 24 and then mapped to geri glasses we could also compute the six images in our program and use them to compute the circular image required by opengl spherical maps note that all these methods can suffer from geometric distortions and aliasing problems in addition unless the application recomputes the environment maps they are not correct if the viewer moves regardless of how the images are computing once we have them we can specify a cube map in opengl with six function calls one for each face of a cube centered at the origin thus if we have a 512 512 rgba image imagexp for the positive x face we have the following 512 512 imagexp for reﬂection maps the calculation of texture coordinates can be done auto matically however cube maps are fundamentally different from sphere maps which are much like standard two dimensional texture maps with special coordinate cal culations here we must use three dimensional texture coordinates which are often computed in the shader these techniques are examples of multipass rendering or multirendering where in order to compute a single image we compute multiple images each using the rendering pipeline multipass methods are becoming increasingly more impor tant as the power of graphics cards has increased to the point that we can render a scene multiple times from different perspectives in less time than is needed for rea sonable refresh rates equivalently most of these techniques can be done within the fragment shader reflection map example let look at a simple example of a reﬂection map based on our rotating cube example in this example we will use a cube map in which each of the six texture maps is a single texel our rotating cube will be totally reﬂective and placed inside a box each of whose sides is one of the six colors red green blue cyan magenta and yellow here how we can set up the cube map as part of initialization using texture unit gluint tex glubyte red glubyte green glubyte blue glubyte cyan glubyte magenta 255 255 glubyte yellow 255 255 glactivetexture glgentextures tex glbindtexture tex red green blue cyan magenta gl_rgb gl_rgb yellow gltexparameteri figure reflection cube map the texture map will be applied using a sampler in the fragment shader we set up the required uniform variables as in our other examples gluint texmaplocation texmaplocation glgetuniformlocation program texmap texmaplocation corresponding to unit now that we have set up the cube map we can turn to the determination of the texture coordinates the required computations for a reﬂection or environment map are shown in figure 32 we assume that the environment has already been mapped to the cube the difference between a reﬂection map and a simple cube texture map is that we use the reﬂection vector to access the texture for a reﬂection map rather than the view vector we can compute the reﬂection vector at each vertex in our vertex program and then let the fragment program interpolate these values over the primitive however to compute the reﬂection vector we need the normal to each side of the rotating cube we can compute normals in the application and send them to the vertex shader as a vertex attribute through the quad function normals n normal void quad int a int b int c int d static int i normal normalize cross vertices b vertices a vertices c vertices b normals i normal points i vertices a i normals i normal points i vertices b i normals i normal points i vertices c i normals i normal points i vertices a i normals i normal points i vertices c i normals i normal points i vertices d i and combining the normal data in a vertex array glbindbuffer buffer glbufferdata sizeof points sizeof normals null glbuffersubdata sizeof points points glbuffersubdata sizeof points sizeof normals normals that is aligned with the shader glgetattriblocation program normal glenablevertexattribarray we will assume that the rotation to the cube is applied in the application and its effect is incorporated in the model view matrix we also assume that the camera location is ﬁxed the normals must then be rotated in the vertex shader before we can use the reflect function to compute the direction of reﬂection our vertex shader is in vposition in normal out r uniform modelview uniform projection void main projection modelview vposition eyepos vposition xyz nn modelview normal n normalize nn xyz r reflect eyepos n it computes the reﬂection vector in eye coordinates as a varying variable if we want the color to be totally determined by the texture the fragment shader is simply as follows in r uniform samplercube texmap void main texcolor texturecube texmap r texcolor we can create more complex lighting by having the color determined in part by the specular diffuse and ambient lighting as we did for the modiﬁed phong lighting model however we must be careful as to which frame we want to use in our shaders the difference between this example and previous ones is that the environment map usually is computed in world coordinates object positions and normals are speciﬁed in object coordinates and are brought into the world frame by modeling transforma tions in the application we usually never see the object coordinate representation of objects because the model view transformation converts object coordinates directly to eye coordinates in many applications we deﬁne our objects directly without mod eling transformations so that model and object coordinates are the same however we want to write our program in a manner that allows for modeling transformations when we do reﬂection mapping one way to accomplish this task is to compute the modeling matrix in the application and pass it to the fragment program as a uni form variable also note that we need the inverse transpose of the modeling matrix to transform the normal however if we pass in the inverse matrix as another uni form variable we can postmultiply the normal to obtain the desired result color plate shows the use of a reﬂection map to determine the colors on the teapot the teapot is set inside a cube each of whose sides is one of the colors red green blue cyan magenta or yellow bump mapping bump mapping is a texture mapping technique that can give the appearance of great complexity in an image without increasing the geometric complexity unlike simple texture mapping bump mapping will show changes in shading as the light source or object moves making the object appear to have variations in surface smoothness let start by returning to our example of creating an image of an orange if we take a photograph of a real orange we can apply this image as a texture map to a surface however if we move the lights or rotate the object we immediately notice that we have the image of a model of an orange rather than the image of a real orange the problem is that a real orange is characterized primarily by small variations in its surface rather than by variations in its color and the former are not captured by texture mapping the technique of bump mapping varies the apparent shape of the surface by perturbing the normal vectors as the surface is rendered the colors that are generated by shading then show a variation in the surface properties unlike techniques such as environment mapping that can be implemented without programmable shaders bump mapping cannot be done in real time without them finding bump maps we start with the observation that the normal at any point on a surface characterizes the orientation of the surface at that point if we perturb the normal at each point on the surface by a small amount then we create a surface with small variations in its shape if this perturbation to the normal can be applied only during the shading process we can use a smooth model of the surface which must have a smooth normal but we can shade it in a way that gives the appearance of a complex surface because the perturbations are to the normal vectors the rendering calculations are correct for the altered surface even though the more complex surface deﬁned by the perturbed normals need never be created we can perturb the normals in many ways the following procedure for para metric surfaces is an efﬁcient one let p u v be a point on a parametric surface the partial derivatives at the point x x u v pu y pv y lie in the plane tangent to the surface at the point their cross product can be nor malized to give the unit normal at that point n pu pv pu pv suppose that we displace the surface in the normal direction by a function called the bump or displacement function d u v which we can assume is known and small d u v the displaced surface is given by pt p d u v n we would prefer not to create the displaced surface because such a surface would have a higher geometric complexity than the undisplaced surface and would thus slow down the rendering process we just want to make it look as though we have displaced the original surface we can achieve the desired look by altering the normal n instead of p and using the perturbed normal in our shading calculations the normal at the perturbed point pt is given by the cross product nt ptu ptv we can compute the two partial derivatives by differentiating the equation for pt obtaining pt p d n d u v n u u u u pt p d n d u v n v v v v if d is small we can neglect the term on the right of these two equations and take their cross product noting that n n to obtain the approximate perturbed normal nt n d n p d n p u v v u the two terms on the right are the displacement the difference between the original and perturbed normals the cross product of two vectors is orthogonal to both of them consequently both cross products yield vectors that lie in the tangent plane at p and their sum must also be in the tangent plane although ptu and ptv lie in the tangent plane perpendicular to nt they are not necessarily orthogonal to each other we can obtain an orthogonal basis and a corresponding rotation matrix using the cross product first we normalize nt and ptu obtaining the vectors m nt nt t ptu ptu we obtain the third orthogonal vector b by b m t the vector t is called the tangent vector at p and b is called the binormal vector at p the matrix m t b m t is the rotation matrix that will convert representations in the original space to repre sentations in terms of the three vectors the new space is sometimes called tangent space because the tangent and binormal vectors can change for each point on the surface tangent space is a local coordinate system to better understand the implication of having introduced another frame one that is local to the point on the surface let consider a bump from the plane z 0 the surface can be written in implicit form as f x y ax by c 0 if we let u x and v y then if a 0 we have u c p u v av a the vectors p u and p v can be normalized to give the orthogonal vectors ptu ptu ptv ptv 0 0 t 0 0 t because these vectors turned out to be orthogonal they serve as the tangent binormal vectors the unit normal is n 0 0 t for this case the displacement function is a function d x y to specify the bump map we need two functions that give the values of d x and d y if these functions are known analytically we can evaluate them either in the application or in the shader more often however we have a sampled version of d x y as an array of pixels d dij the required partial derivatives can be approximated by the difference between adjacent elements in the following array d x dij di j d y dij di j these arrays can be precomputed in the application and stored as a texture called a normal map the fragment shader can obtain the values using a sampler before we develop the necessary shaders consider what is different for the gen eral case when the surface is not described by the plane z 0 in our simple case the tangent space axes aligned with the object or world axes in general the normal from a surface will not point in the z direction nor along any particular axis in ad dition the tangent and binormal vectors although orthogonal to each other and the normal will have no particular orientation with respect to the world or object axes the displacement function is measured along the normal so its partial derivatives are in an arbitrary plane however in tangent space this displacement is along the z coordinate axis hence the importance of the matrix m composed of the normal tangent and binormal vectors is that it allows us to go to the local coordinate system in which the bump map calculations match what we just did the usual implemen tation of bump mapping is to ﬁnd this matrix and transform object space vectors into vectors in a tangent space local coordinate system because tangent space is lo cal the change in representation can be different for every fragment with polygonal meshes the calculation can be simpler if we use the same normal across each polygon and the application can send the tangent and binormal to the vertex shader once for each polygon we are almost ready to write vertex and fragment shaders for bump mapping the entities that we need for lighting the surface normal the light vector the half angle vector the vertex location are usually in eye or object coordinates at the point in the process when we do lighting whether we use a normal map or compute the perturbation of the normal procedurally in a fragment shader the displacements are in texture space coordinates for correct shading we have to convert either the normal map to object space coordinates or the object space coordinates to texture space coordinates in general the latter requires less work because it can be carried out on a per vertex basis in the vertex shader rather than on a per fragment basis as we have seen the matrix needed to convert from object space to texture space is precisely the matrix composed of the normal tangent and binormal we can send the normal to the vertex shader as a vertex attribute if it changes at each vertex or if we are working with a single ﬂat polygon at a time we can use a uniform variable the application can also provide tangent vectors in a similar manner the binormal can then be computed in the shader using the cross product function these computations are done in the vertex shader which produces a light vector and view vector in tangent coordinates for use in the fragment shader because the normal vector in tangent coordinates always points in the positive z direction the view and light vectors are sufﬁcient for doing lighting in tangent space bump map example our example is a single square in the plane y 0 with a light source above the plane that rotates in the plane y 0 we will include only diffuse lighting to minimize the amount of code we need our displacement is a small square in the center of the original square before developing the code the output is shown in figure the image on the left is with the light source in its original position the image on the left is with the light source rotated degrees in the x z plane at the same height above the surface first let look at the application program we will use two triangles for the square polygon and each of the six vertices will have a texture so that part of the code will be much as in previous examples figure bump mapping of a square displacement points point2 void mesh vertices 0 0 0 0 0 0 0 0 0 0 0 0 0 point4 0 0 0 0 0 point4 0 0 0 0 0 0 points 0 vertices 0 tex_coord 0 point2 0 0 0 0 points vertices tex_coord point2 0 0 0 points vertices tex_coord point2 0 0 points vertices tex_coord point2 0 0 points vertices tex_coord point2 0 0 0 points vertices 0 tex_coord point2 0 0 0 0 we send these data to the gpu as vertex attributes the displacement map is generated as an array in the application the displace ment data are in the array data the normal map is computing by taking the differ ences to approximate the partial derivatives for two of the components and using 0 for the third to form the array normals because these values are stored as colors in a texture image the components are scaled to the interval 0 0 0 const int n float data n n normals n n for int i 0 i n i for int j 0 j n j data i j 0 0 for int i n i n i for int j n j n 4 j data i j 0 for int i 0 i n i for int j 0 j n j n data i j data i j 0 0 data i j data i j normals i j 0 normalize n 0 the array normals is then sent to the gpu by building a texture object we send a projection matrix a model view matrix the light position and the diffuse lighting parameters to the shaders as uniform variables because the surface is ﬂat the normal is constant and can be sent to the shader as a uniform variable likewise the tangent vector is constant and can be any vector in the same plane as the polygon and can also be sent to the vertex shader as a uniform variable we now turn to the vertex shader in this simple example we want to do the calculations in texture space hence we must transform both the light vector and eye vector to this space the required transformation matrix is composed of the normal tangent and binormal vectors the normal and tangent are speciﬁed in object coordinates and must ﬁrst be converted to eye coordinates the required transformation matrix is the normal matrix which is the inverse transpose of the upper left submatrix of the model view matrix see exercise we assume this matrix is computed in the application and sent to the shader as another uniform variable we can then use the transformed normal and tangent to give the binormal in eye coordinates finally we use these three vectors to transform the view vector and light vector to texture space here is the vertex shader bump map vertex shader out l light vector in texture space coordinates out v view vector in texture space coordinates out st texture coordinates in texcoord in vposition uniform normal uniform lightposition uniform modelview uniform projection uniform normalmatrix uniform objtangent void main projection modelview vposition st texcoord eyeposition modelview vposition eyelightpos lightposition xyz normal tangent and binormal in eye coordinates n normalize normalmatrix normal t normalize normalmatrix objtangent b cross n t light vector in texture space l x dot t eyelightpos eyeposition l y dot b eyelightpos eyeposition l z dot n eyelightpos eyeposition l normalize l view vector in texture space v x dot t eyeposition v y dot b eyeposition v z dot n eyeposition v normalize v our strategy for the fragment shader is to provide the normalized perturbed normals as a texture map from the application as a normal map the fragment shader is given by the following code in l in v in st uniform diffuseproduct uniform texmap void main n texmap st nn normalize 0 n xyz 0 ll normalize l float kd max dot nn xyz ll 0 0 kd diffuseproduct the values in the texture map are scaled back to the interval 0 0 the diffuse product is a vector computed in the application each of whose components is the product of a diffuse light component and a diffuse material component note that this example does not use the texture space view vectors computed in the vertex shader these vectors would be necessary if we wanted to add a specular term we have only touched the surface so to speak of bump mapping many of its most powerful applications are when it is combined with procedural texture generation which we explore further in chapter compositing techniques thus far we have assumed that we want to form a single image and that the ob jects that form this image have surfaces that are opaque opengl provides a mecha nism through alpha α blending that can among other effects create images with translucent objects the alpha channel is the fourth color in rgba or rgbα color mode like the other colors the application program can control the value of a or α for each pixel however in rgba mode if blending is enabled the value of α con trols how the rgb values are written into the frame buffer because fragments from multiple objects can contribute to the color of the same pixel we say that these ob jects are blended or composited together we can use a similar mechanism to blend together images opacity and blending the opacity of a surface is a measure of how much light penetrates through that surface an opacity of α corresponds to a completely opaque surface that blocks all light incident on it a surface with an opacity of 0 is transparent all light passes through it the transparency or translucency of a surface with opacity α is given α consider the three uniformly lit polygons shown in figure assume that the middle polygon is opaque and the front polygon nearest to the viewer is trans parent if the front polygon were perfectly transparent the viewer would see only the middle polygon however if the front polygon is only partially opaque partially transparent similar to colored glass the color that viewer sees is a blending of the colors of the front and middle polygon because the middle polygon is opaque the figure translucent and opaque polygons viewer does not see the back polygon if the front polygon is red and the middle is blue she sees magenta due to the blending of the colors if we let the middle polygon be only partially opaque she sees the blending of the colors of all three polygons in computer graphics we usually render polygons one at a time into the frame buffer consequently if we want to use blending or compositing we need a way to apply opacity as part of fragment processing we can use the notion of source and destination pixels just as we used source and destination bits in section as a polygon is processed pixel sized fragments are computed and if they are visible are assigned colors based on the shading model in use until now we have used the color of a fragment as computed by the shading model and by any mapping techniques to determine the color of the pixel in the frame buffer at the location in screen coordinates of the fragment if we regard the fragment as the source pixel and the frame buffer pixel as the destination we can combine these values in various ways using α values is one way of controlling the blending on a fragment by fragment basis combining the colors of polygons is similar to joining two pieces of colored glass into a single piece of glass that has a higher opacity and a color different from either of the original pieces if we represent the source and destination pixels with the four element rgbα arrays sr sg sb sa d dr dg db da then a compositing operation replaces d with dt brsr crdr bgsg cgdg bbsb cbdb basa cada the arrays of constants b br bg bb ba and c cr cg cb ca are the source and destination blending factors respectively as occurs with rgb colors a value of α over 0 is limited or clamped to the maximum of 0 and negative values are clamped to 0 0 we can choose both the values of α and the method of combining source and destination values to achieve a variety of effects image compositing the most straightforward use of α blending is to combine and display several images that exist as pixel maps or equivalently as sets of data that have been rendered inde pendently in this case we can regard each image as a radiant object that contributes equally to the ﬁnal image usually we wish to keep our rgb colors between 0 and in the ﬁnal image without having to clamp those values greater than hence we can either scale the values of each image or use the source and destination blending factors suppose that we have n images that should contribute equally to the ﬁnal display at a given pixel image i has components ciαi here we are using ci to denote the color triplet ri gi bi if we replace ci by and αi by then we can simply add n n each image into the frame buffer assuming the frame buffer is initialized to black with an α 0 alternately we can use a source blending factor of by setting the α value for each pixel in each image to be and using for the destination blending factor and α for the source blending factor although these two methods produce the same image if the hardware supports compositing the second may be more efﬁcient note that if n is large blending factors of the form can lead to signiﬁcant loss of color resolution recent frame buffers support ﬂoating point arithmetic and thus can avoid this problem blending and compositing in opengl the mechanics of blending in opengl are straightforward we enable blending by glenable then we set up the desired source and destination factors by glblendfunc opengl has a number of blending factors deﬁned including the values gl_one and 0 gl_zero the source α and α and and the destination α and α and the application program speciﬁes the desired op tions and then uses rgba color the major difﬁculty with compositing is that for most choices of the blending factors the order in which we render the polygons affects the ﬁnal image for example many applications use the source α as the source blending factor and α for the destination factor the resulting color and opacity are rdt gdt bdt αdt αsrs αs rd αsg αs gd αsbs αs bd αsαd αs αd this formula ensures that neither colors nor opacities can saturate however the resulting color and α values depend on the order in which the polygons are rendered consequently unlike in most opengl programs where the user does not have to worry about the order in which polygons are rasterized to get a desired effect we must now control this order within the application a more subtle but visibly apparent problem occurs when we combine opaque and translucent objects in a scene normally when we use blending we do not en able hidden surface removal because polygons behind any polygon already rendered would not be rasterized and thus would not contribute to the ﬁnal image in a scene with both opaque and transparent polygons any polygon behind an opaque polygon should not be rendered but translucent polygons in front of opaque polygons should be composited there is a simple solution to this problem that does not require the application program to order the polygons we can enable hidden surface removal as usual and can make the z buffer read only for any polygon that is translucent we do so by calling gldepthmask when the depth buffer is read only a translucent polygon that lies behind any opaque polygon already rendered is discarded a translucent polygon that lies in front of any polygon that has already been rendered is blended with the color of the polygons it is in front of however because the z buffer is read only for this polygon the depth values in the buffer are unchanged opaque polygons set the depth mask to true and are rendered normally note that because the result of compositing depends on the order in which we composite individual elements we may notice defects in images in which we render translucent polygons in an arbitrary order if we are willing to sort the translucent polygons then we can render all the opaque polygons ﬁrst and then render the translucent polygons in a back to front order with the z buffer in a read only mode 4 antialiasing revisited one of the major uses of the α channel is for antialiasing because a line must have a ﬁnite width to be visible the default width of a line that is rendered should be one pixel wide we cannot produce a thinner line unless the line is horizontal or vertical such a line partially covers a number of pixels in the frame buffer as shown in figure suppose that as part of the geometric processing stage of the rendering process as we process a fragment we set the α value for the corresponding pixel to be a number between 0 and that is the amount of that pixel covered by the fragment we can then use this α value to modulate the color as we render the fragment to the frame buffer we can use a destination blending factor of α and a source destination factor of α however if there is overlap of fragments within a pixel then there are numerous possibilities as we can see from figure in figure a the fragments do not overlap in figure b they do overlap consider the problem from the perspective of a renderer that works one polygon a time for our simple example suppose that we start with an opaque background and that the frame buffer starts with the background color we can set 0 because no part of the pixel figure raster line a b figure fragments a nonoverlapping b overlapping has yet been covered with fragments from polygons the ﬁrst polygon is rendered the color of the destination pixel is set to cd and its α value is set to αd thus a fragment that covers the entire pixel will have its color assigned to the destination pixel and the destination pixel will be opaque if the background is black the destination color will be now consider the fragment from the second polygon that subtends the same pixel how we add in its color and α value depends on how we wish to interpret the overlap if there is no overlap we can assign the new color by blending the color of the destination with the color of the fragment resulting in the color and α cd α1c1 αd this color is a blending of the two colors and does not need to be clamped the resulting value of α represents the new fraction of the pixel that is covered however the resulting color is affected by the order in which the polygons are rendered the more difﬁcult questions are what to do if the fragments overlap and how to tell whether there is an overlap one tactic is to take a probabilistic view if fragment occupies a fraction of the pixel fragment occupies a fraction of the same pixel and we have no other information about the location of the fragments within the pixel then the average area of overlap is we can represent the average case as shown in figure hence the new destination α should be αd α2 e z distance figure average overlap how we should assign the color is a more complex problem because we have to decide whether the second fragment is in front of the ﬁrst or the ﬁrst is in front of the second or even whether the two should be blended we can deﬁne an appro priate blending for whichever assumption we wish to make note that in a pipeline renderer polygons can be generated in an order that has nothing to do with their distances from the viewer however if we couple α blending with hidden surface re moval we can use the depth information to make front versus back decisions in opengl we can invoke antialiasing without having the user program com bine α values explicitly if we enable blending and smoothing for lines or polygons for example we can use glenable glenable glenable glblendfunc to enable antialiasing there may be a considerable performance penalty associated with antialiasing color plate shows opengl antialiasing of polygons back to front and front to back rendering although using the α channel gives us a way of creating the appearance of translu cency it is difﬁcult to handle transparency in a physically correct manner without taking into account how an object is lit and what happens to rays and projectors that pass through translucent objects in figure we can see several of the difﬁcul ties we ignore refraction of light through translucent surfaces an effect than cannot be handled easily with a pipeline polygon renderer suppose that the rear polygon is opaque but reﬂective and that the two polygons closer to the viewer are translucent by following various rays from the light source we can see a number of possibili ties some rays strike the rear polygon and the corresponding pixels can be colored with the shade at the intersection of the projector and the polygon for these rays we figure scene with translucent objects should also distinguish between points illuminated directly by the light source and points for which the incident light passes through one or both translucent polygons for rays that pass through only one translucent surface we have to adjust the color based on the color and opacity of the polygon we should also add a term that ac counts for the light striking the front polygon that is reﬂected toward the viewer for rays passing through both translucent polygons we have to consider their combined effect for a pipeline renderer the task is even more difﬁcult if not impossible because we have to determine the contribution that each polygon makes as it is passed through the pipeline rather than considering the contributions of all polygons to a given pixel at the same time in applications where handling of translucency must be done in a consistent and realistic manner we often must sort the polygons from front to back within the application then depending on the application we can do a front to back or back to front rendering using opengl blending functionality see exercise scene antialiasing and multisampling rather than antialiasing individual lines and polygons as we discussed in sec tion 4 we can antialias the entire scene using a technique called multisampling in this mode every pixel in the frame buffer contains a number of samples each sample is capable of storing a color depth and other values when a scene is ren dered it is as if the scene is rendered at an enhanced resolution however when the image must be displayed in the frame buffer all of the samples for each pixel are combined to produce the ﬁnal pixel color in opengl the number of samples per pixel is a function of how the frame buffer is created when the application initializes in our programs since we use glut you would add the additional option to the glutinitdis playmode this will request that the pixels in the frame buffer have multiple samples just as line and polygon antialiasing can be enabled and disabled during the ren dering of a frame so too with multisampling to turn on multisampling and begin antialiasing all of the primitives rendered in the frame simply call glenable multisample likewise calling gldisable will stop the multisampled rendering generally speaking an application will almost always either multisample all the time or never aij figure filtering and convolution image processing we can use pixel mapping to perform various image processing operations suppose that we start with a discrete image perhaps this image was generated by a rendering or perhaps we obtained it by digitizing a continuous image using a scanner we can represent the image with an n m matrix a aij of scalar levels if we process each color component of a color image independently we can regard the entries in a as either individual color components or gray lumi nance levels a linear ﬁlter produces a ﬁltered matrix b whose elements are bij hkl ai k j l k m l n we say that b is the result of convolving a with a ﬁlter matrix h in general the values of m and n are small and we can represent h by a small convolution matrix we can view the ﬁltering operation as shown in figure for m n for each pixel in a we place the convolution matrix over aij and take a weighted average of the surrounding points the values in the matrix are the weights for example for n m we can average each pixel with its four surrounding neighbors using the matrix 0 0 0 0 this ﬁlter can be used for antialiasing we can use more points and can weight the center more heavily with h 4 note that we must deﬁne a border around a if we want b to have the same dimen sions other operations are possible with small matrices for example we can use the matrix 0 0 h 4 0 0 to detect changes in value or edges in the image if the matrix h is k k we can implement a ﬁlter by accumulating images in the frame buffer each time adding in a shifted version of a other multipass methods we can also use blending for ﬁltering in time and depth for example if we jitter an object and render it multiple times leaving the positions of the other objects unchanged we get dimmer copies of the jittered object in the ﬁnal image if the object is moved along a path rather than randomly jittered we see the trail of the object this motion blur effect is similar to the result of taking a photograph of a moving object using a long exposure time we can adjust the object α value so as to render the ﬁnal position of the object with greater opacity or to create the impression of speed differences we can use ﬁltering in depth to create focusing effects a real camera cannot produce an image with all objects in focus objects within a certain distance from the camera the camera depth of ﬁeld are in focus objects outside it are out of fo cus and appear blurred computer graphics produces images with an inﬁnite depth of ﬁeld because we do not have to worry about the limitations of real lenses occa sionally however we want to create an image that looks as though it were produced by a real camera or to defocus part of a scene so as to emphasize the objects within a desired depth of ﬁeld this time the trick is to move the viewer in a manner that leaves a particular plane ﬁxed as shown in figure suppose that we wish to keep the plane at z zf in focus and to leave the near z zmin and far z zmax clip ping distances unchanged if we use frustum we specify the near clipping rectangle xmin xmax ymin ymax if we move the viewer from the origin in the x direction by xmin ymin zmin z zmax zf i x figure depth of field jitter we must change xmin to xmt in xmin z zf znear similar equations hold for xmax ymin and ymax as we increase and we create a narrower depth of ﬁeld sampling and aliasing we have seen a variety of applications in which the conversion from a continuous representation of an entity to a discrete approximation of that entity leads to visible errors in the display we have used the term aliasing to characterize these errors when we work with buffers we are always working with digital images and if we are not careful these errors can be extreme in this section we examine the nature of digital images and gather facts that will help us to understand where aliasing errors arise and how the effects of these errors can be mitigated we start with a continuous two dimensional image f x y we can regard the value of f as either a gray level in a monochromatic image or the value of one of the primaries in a color image in the computer we work with a digital image that is an array of nm pixels arranged as n rows of m pixels each pixel has k bits there are two processes involved in going from a continuous image to a discrete image first we must sample the continuous image at nm points on some grid to obtain a set of values fij each of these samples of the continuous image is the value of f measured over a small area in the continuous image then we must convert each of these samples into a k bit pixel by a process known as quantization sampling theory suppose that we have a rectangular grid of locations where we wish to obtain our samples of f as in figure if we assume that the grid is equally spaced then an ideal sampler would produce a value fij f ihx jhy y hy f hx x figure sampling grid a b figure one dimensional decomposition a function b components where hx and hy are the distances between the grid points in the x and y directions respectively leaving aside for now the fact that no real sampler can make such a precise measurement there are two important questions first what errors have we made in this idealized sampling process that is how much of the information in the original image is included in the sampled image second can we go back from the digital image to a continuous image without incurring additional errors this latter step is called reconstruction and describes display processes such as are required in displaying the contents of a frame buffer on a monitor the mathematical analysis of these issues uses fourier analysis a branch of ap plied mathematics particularly well suited for explaining problems of digital signal processing the essence of fourier theory is that a function of either space or time can be decomposed into a set of sinusoids at possibly an inﬁnite number of fre quencies this concept is most familiar with sound where we routinely think of a particular sound in terms of its frequency components or spectrum for a two dimensional image we can think of it as being composed of sinusoidal patterns in two spatial frequencies that when added together produce the image figure a shows a one dimensional function figure b shows the two sinusoids that form it fig ure shows two dimensional periodic functions thus every two dimensional spatial function f x y has two equivalent representations one is its spatial form f x y the other is a representation in terms of its spectrum the frequency domain representation g ξ η the value of g is the contribution to f at the two dimensional spatial frequency ξ η by using these alternate representations of functions we ﬁnd that many phenomena including sampling can be explained much more easily in the frequency domain we can explain the consequences of sampling without being overwhelmed by the mathematics if we accept without proof the fundamental theorem known as the nyquist sampling theorem there are two parts to the theorem the ﬁrst allows us to discuss sampling errors whereas the second governs reconstruction we examine the second in section a figure two dimensional periodic functions b nyquist sampling theorem part the ideal samples of a continuous function contain all the information in the original function if and only if the continuous function is sampled at a frequency greater than twice the highest frequency in the function thus if we are to have any chance of not losing information we must restrict ourselves to functions that are zero in the frequency domain except in a window of width less than the sampling frequency centered at the origin the lowest fre quency that cannot be in the data so as to avoid aliasing one half of the sampling frequency is called the nyquist frequency functions whose spectra are zero out side of some window are known as band limited functions for a two dimensional image the sampling frequencies are determined by the spacing of a two dimensional grid with x and y spacing of hx and hy respectively the theorem assumes an ideal sampling process that gathers an inﬁnite number of samples each of which is the exact value at the grid point in practice we can take only a ﬁnite number of samples the number matching the resolution of our buffer consequently we can not produce a truly band limited function although this result is a mathematical consequence of fourier theory we can observe that there will always be some am biguity inherent in a ﬁnite collection of sampled points simply because we do not know the function outside the region from which we obtained the samples the consequences of violating the nyquist criteria are aliasing errors we can see from where the name aliasing comes by considering an ideal sampling process both the original function and its set of samples have frequency domain representa tions the spectral components of the sampled function are replicas of the spectrum of the original function with their centers separated by the sampling frequency con sider the one dimensional function in figure a with the samples indicated figure b shows its spectrum in figure c we have the spectrum of the sampled function showing the replications of the spectrum in figure b be cause we have sampled at a rate higher than the nyquist frequency there is a separa tion between the replicas now consider the case in figure here we have violated the nyquist crite rion and the replicas overlap consider the central part of the plot which is magni ﬁed in figure and shows only the central replica centered at the origin and the replica to its right centered at ξs the frequency is above the nyquist frequency ξs there is however a replica of generated by the sampling process from the replica on the right at ξs a frequency less than the nyquist frequency the en ergy at this frequency can be heard if we are dealing with digital sound or seen if we are considering two dimensional images we say that the frequency has an alias at this statement assumes no knowledge of the underlying function f other than a set of its samples if we have additional information such as knowledge that the function is periodic knowledge of the function over a ﬁnite interval can be sufﬁcient to determine the entire function we show the magnitude of the spectrum because the fourier transform produces complex num bers for the frequency domain components f x g gs x a b c figure band limited function a function and its samples in the spatial domain b spectrum of the function c spectrum of the samples g figure overlapping replicas g 0 0 figure aliasing ξs note that once aliasing has occurred we cannot distinguish between infor mation that was at a frequency in the original data and information that was placed at this frequency by the sampling process we can demonstrate aliasing and ambiguity without using fourier analysis by looking at a single sinusoid as shown in figure if we sample this sinusoid at twice its frequency we can recover it from two samples however these same two samples are samples of a sinusoid of twice this frequency and they can also be samples of sinusoids of other multiples of the basic frequency all these frequencies are aliases of the same original frequency if we know that the data were band limited however then the samples can describe only the original sinusoid if we were to do an analysis of the frequency content of real world images we would ﬁnd that the spectral components of most images are concentrated in the lower frequencies consequently although it is impossible to construct a ﬁnite sized image that is band limited the aliasing errors often are minimal because there is little content in frequencies above the nyquist frequency and little content is aliased into figure aliasing of sinusoid f x y a figure scanning of an image b a point sampling b area averaging frequencies below the nyquist frequency the exceptions to this statement arise when there is regular periodic information in the continuous image in the frequency representation regularity places most of the information at a few frequencies if any of these frequencies is above the nyquist limit the aliasing effect is noticeable as beat or moire patterns examples that you might have noticed include the patterns that appear on video displays when people in the images wear striped shirts or plaid ties and wavy patterns that arise both in printed halftoned ﬁgures derived from computer displays and in digital images of farmland with plowed ﬁelds often we can minimize aliasing by preﬁltering before we scan an image or by controlling the area of the data that the scanner uses to measure a sample figure shows two possible ways to scan an image in figure a we see an ideal scanner it measures the value of a continuous image at a point so the samples are given by fij f xi yi in figure b we have a more realistic scanner that obtains samples by taking a weighted average over a small interval to produce samples of the form fij xi yi f x y w x y dydx 1 figure sinc function by selecting the size of the window and the weighting function w we can attenuate high frequency components in the image and thus we can reduce aliasing fortu nately real scanners must take measurements over a ﬁnite region called the sampling aperture thus some antialiasing takes place even if the user has no understanding of the aliasing problem reconstruction suppose that we have an inﬁnite set of samples the members of which have been sampled at a rate greater than the nyquist frequency the reconstruction of a contin uous function from the samples is based on part of the nyquist sampling theorem nyquist sampling theorem part 2 we can reconstruct a continuous function f x from its samples fi by the formula f x i fi sinc x xi the function sinc x see figure is deﬁned as sinc x sin πx πx the two dimensional version of the reconstruction formula for a function f x y with ideal samples fij is f x y fijsinc x xi sinc y yj i j these formulas follow from the fact that we can recover an unaliased func tion in the frequency domain by using a ﬁlter that is zero except in the interval ξs 2 ξs 2 a low pass ﬁlter to obtain a single replica from the inﬁnite number f 0 figure one dimensional reconstruction figure two dimensional sinc function of replicas generated by the sampling process shown in figure the reconstruc tion of a one dimensional function is shown in figure in two dimensions the reconstruction involves use of a two dimensional sinc as shown in figure un fortunately the sinc function cannot be produced in a physical display because of its negative side lobes consider the display problem for a crt display we start with a digital image that is a set of samples for each sample we can place a spot of light centered at a grid point on the display surface as shown in figure the value of the sample controls the intensity of the spot or modulates the beam we can control the shape of the spot by using techniques such as focusing the beam the reconstruc tion formula tells us that the beam should have the shape of a two dimensional sinc but because the beam puts out energy the spot must be nonnegative at all points consequently the display process must make errors we can evaluate a real display by considering how well its spot approximates the desired sinc figure shows the sinc and several one dimensional approximations the gaussian shaped spot corre sponds to the shape of many crt spots whereas the rectangular spot might corre figure display of a point on crt a b c d figure display spots a ideal spot b rectangular approxima tion c piecewise linear approximation d gaussian approximation q g q1 g2 g figure 54 quantizer spond to an lcd display with square pixels note that we can make either approxi mation wider or narrower if we analyze the spot proﬁles in the frequency domain we ﬁnd that the wider spots are more accurate at low frequencies but are less accurate at higher frequencies in practice the spot size that we choose is a compromise visible differences across monitors often can be traced to different spot proﬁles 12 quantization the mathematical analysis of sampling explains a number of important effects how ever we have not included the effect of each sample being quantized into k discrete levels given a scalar function g with values in the range gmin g gmax a quantizer is a function q such that if gi g gi 1 q g qi thus for each value of g we assign it one of k values as shown in figure 54 in general designing a quantizer involves choosing the qi the quantization levels and the gi the threshold values if we know the probability distribution for g p g we can solve for the values that minimize the mean square error e r g q g g dg however we often design quantizers based on the perceptual issues that we discussed in chapter 1 a simple rule of thumb is that we should not be able to detect one level changes but should be able to detect all two level changes given the threshold for the visual system to detect a change in luminance we usually need at least or bits or to levels we should also consider the logarithmic intensity brightness response of humans to do so we usually distribute the levels exponentially to give approximately equal perceptual errors as we go from one level to the next summary and notes in the early days of computer graphics practitioners worked with only two and three dimensional geometric objects whereas those practitioners who were involved with only two dimensional images were considered to be working in image process ing advances in hardware have made graphics and image processing systems practi cally indistinguishable for those practitioners involved with synthesizing images certainly a major part of computer graphics this merging of ﬁelds has brought forth a multitude of new techniques the idea that a two dimensional image or texture can be mapped to a three dimensional surface in no more time than it takes to render the surface with constant shading would have been unthinkable years ago now these techniques are routine techniques such as texture mapping have had an enormous effect on real time graphics in ﬁelds such as animation virtual reality and scientiﬁc visualization we use hardware texture mapping to add detail to images without burdening the geo metric pipeline the use of compositing techniques through the alpha channel allows the application programmer to perform tasks such as antialiasing and to create ef fects such as fog and depth of ﬁeld that until recently were done on different types of architectures after the graphics had been created mapping methods provide some of the best examples of the interactions among graphics hardware software and applications consider texture mapping although it was ﬁrst described and implemented purely as a software algorithm once people saw its ability to create scenes with great visual complexity hardware developers started putting large amounts of texture memory in graphics systems once texture mapping was implemented in hardware it could be done in real time a development that led to the redesign of many applications notably computer games recent advances in gpus provide many new possibilities one is that the pipeline is now programmable the programmability of the fragment processor makes possi ble many new texture manipulation techniques while preserving interactive speeds second the inclusion of large amounts of memory on the gpu removes one of the major bottlenecks in discrete methods namely many of the transfers of image data between processor memory and the gpu third gpu architectures are designed for rapid processing of discrete data by incorporating a high degree of parallelism for fragment processing finally the availability of ﬂoating point frame buffers elimi nates many of the precision issues that plagued techniques that manipulated image data in this chapter we have concentrated on techniques that are supported by re cently available hardware and apis many of the techniques introduced here are recent many more have appeared in the recent literature and are available only for programmable processors suggested readings environment mapping was developed by blinn and newell texture mapping was ﬁrst used by catmull see the review by heckbert hardware support for texture mapping came with the sgi reality engine see akeley perlin and hoffert designed a noise function to generate two and three dimensional texture maps many texture synthesis techniques are discussed in ebert et al the aliasing problem in computer graphics has been of importance since the advent of raster graphics see crow the ﬁrst concerns were with rasteri zation of lines but later other forms of aliasing arose with animations and ray tracing the image processing books provide an introduction to signal processing and aliasing in two dimensions the books by glass ner and watt and policarpo are aimed at practitioners of computer graphics many of the compositing techniques including use of the α channel were sug gested by porter and duff the opengl programming guide contains many examples of how buffers can be used the recent literature includes many new examples of the use of buffers see the recent issues of the journals computer graphics and ieee computer graphics and applications technical details on most of the standard image formats can be found in exercises 1 show how you can use the xor writing mode to implement an odd even ﬁll algorithm 2 what are the visual effects of using xor to move a cursor around on the screen how is an image produced with an environment map different from a ray traced image of the same scene 4 in the movies and television the wheels of cars and wagons often appear to be spinning in the wrong direction what causes the effect can anything be done to ﬁx this problem explain your answer we can attempt to display sampled data by simply plotting the points and letting the human visual system merge the points into shapes why is this technique dangerous if the data are close to the nyquist limit 6 why do the patterns of striped shirts and ties change as an actor moves across the screen of your television why should we do antialiasing by preprocessing the data rather than by post processing them suppose that we have two translucent surfaces characterized by opacities and α2 what is the opacity of the translucent material that we create by using the two in series give an expression for the transparency of the combined material assume that we view translucent surfaces as ﬁlters of the light passing through them develop a blending model based on the complementary colors cmy in section we used 1 α and α for the destination and source blending factors respectively what would be the visual difference if we used 1 for the destination factor and kept α for the source factor create interactive paintbrushes that add color gradually to image also use blending to add erasers that gradually remove images from the screen 12 devise a method of using texture mapping for the display of arrays of three dimensional pixels voxels show how to use the luminance histogram of an image to derive a lookup table that will make the altered image have a ﬂat histogram 14 when we supersample a scene using jitter why should we use a random jitter pattern suppose that a set of objects is texture mapped with regular patterns such as stripes and checkerboards what is the difference in aliasing patterns that we would see when we switch from parallel to perspective views consider a scene composed of simple objects such as parallelepipeds that are instanced at different sizes suppose that you have a single texture map and you are asked to map this texture to all the objects how would you map the texture so that the pattern would be the same size on each face of each object write a program using mipmaps in which each mipmap is constructed from a different image is there a practical application for such a program 18 using either your own image processing code for convolution or the imaging extensions of opengl implement a general ﬁltering program for lumi nance images take an image from a digital camera or from some other source and apply smoothing and sharpening ﬁlters repetitively pay special attention to what happens at the edges of the ﬁltered images 20 repeat exercise 19 but ﬁrst add a small amount of random noise to the image describe the differences between the results of the two exercises 21 if your system supports the imaging extensions compare the performance of ﬁltering using the extensions with ﬁltering done by your own code using processor memory 22 one of the most effective methods of altering the contrast of an image is to allow the user to design a lookup interactively consider a graph in which a curve is approximated with three connected line segments write a program that displays an image allows the user to specify the line segments interactively and shows the image after it has been altered by the curve 23 in a similar vein to exercise 22 write an interactive program that allows users to design pseudocolor maps 24 devise a method to convert the values obtained from a cube map to values for a spherical map 25 write an interactive program that will return the colors of pixels on the display 26 suppose we want to create a cube that has a black and white checkerboard pattern texture mapped to its faces can we texture map the cube so that the colors alternate as we traverse the cube from face to face in what types of applications might you prefer a front to back rendering in stead of a back to front rendering 28 the color gamut in chromaticity coordinates is equivalent to the triangle in rgb space that is deﬁned by the primaries write a program that will display this triangle and the edges of the cube in which it lies each point on the triangle should have the color determined by its coordinates in rgb space this triangle is called the maxwell triangle 29 find the matrix that converts ntsc rgb and use it to redisplay the color gamut of your display in xy chromaticity coordinates 30 show that the normal matrix is the inverse transpose of the upper left submatrix of the model view matrix modeling and hierarchy chapter odels are abstractions of the world both of the real world in which we live and of virtual worlds that we create with computers we are all familiar with mathematical models that are used in all areas of science and engineering these models use equations to model the physical phenomena that we wish to study in computer science we use abstract data types to model organizations of objects in computer graphics we model our worlds with geometric objects when we build a mathematical model we must choose carefully which type of mathematics ﬁts the phenomena that we wish to model although ordinary differential equations may be appropriate for modeling the dynamic behavior of a system of springs and masses we would probably use partial differential equations to model turbulent ﬂuid ﬂow we go through analogous processes in computer graphics choosing which primitives to use in our models and how to show relationships among them often as is true of choosing a mathematical model there are multiple approaches so we seek models that can take advantage of the capabilities of our graphics systems in this chapter we explore multiple approaches to developing and working with models of geometric objects we consider models that use as building blocks a set of simple geometric objects either the primitives supported by our graphics systems or a set of user deﬁned objects built from these primitives we extend the use of trans formations from chapter to include hierarchical relationships among the objects the techniques that we develop are appropriate for applications such as robotics and ﬁgure animation where the dynamic behavior of the objects is characterized by rela tionships among the parts of the model the notion of hierarchy is a powerful one and is an integral part of object oriented methodologies we extend our hierarchical models of objects to hierarchical models of whole scenes including cameras lights and material properties such models allow us to extend our graphics apis to more object oriented systems and also give us insight into using graphics over networks and distributed environments such as the world wide web 1 symbols and instances our ﬁrst concern is how we can store a model that may include many sophisticated objects there are two immediate issues how we deﬁne an object more complex than the ones we have dealt with until now and how we represent a collection of these y objects most apis take a minimalist attitude toward primitives they contain only a few primitives leaving it to the user to construct more complex objects from these primitives sometimes additional libraries provide objects built on top of the basic primitives we assume that we have available a collection of basic three dimensional objects provided by these options x we can take a nonhierarchical approach to modeling by regarding these objects as symbols and by modeling our world as a collection of symbols symbols can include geometric objects fonts and application speciﬁc sets of graphical objects z symbols are usually represented at a convenient size and orientation for example a figure 1 cylinder symbol cylinder is usually oriented parallel to one of the axes as shown in figure 1 often with a unit height a unit radius and its bottom centered at the origin most apis including opengl make a distinction between the frame in which the symbol is deﬁned sometimes called the model frame and the object or world frame this distinction can be helpful when the symbols are purely shapes such as the symbols that we might use for circuit elements in a cad application and have no physical units associated with them in opengl we have to set up the transformation from the frame of the symbol to the object coordinate frame within the application thus the model view matrix for a given symbol is the concatenation of an instance transformation that brings the symbol into object coordinates and a matrix that brings the symbol into the eye frame the instance transformation that we introduced in chapter allows us to place instances of each symbol in the model at the desired size orientation and location thus the instance transformation m trs is a concatenation of a translation a rotation and a scale and possibly a shear as shown in figure 2 consequently opengl programs often contain repetitions of code in the following form instance instance translate dx dy dz rotatez rz rotatey ry rotatex rx scale sx sy sz instance cylinder or some other symbol in this example the instance matrix is computed and alters the model view matrix the resulting model view matrix is sent to the vertex shader using gluniform the y y y y x x x z z z z figure 2 instance transformation symbol scale rotate translate 1 2 1 1 sx sy sz ex ey ez dx dy dz figure symbol instance transformation table code for cylinder generates vertices and can send them to the vertex shader using gldrawarrays alternately we can apply the model view matrix in the application as we generate the vertices we can also think of such a model in the form of a table as shown in figure here each symbol is assumed to have a unique numerical identiﬁer the table stores this identiﬁer and the parameters necessary to build the instance transformation matrix the table shows that this modeling technique contains no information about relationships among objects however the table contains all the information that we require to draw the objects and is thus a simple data structure or model for a group of geometric objects we could search the table for an object change the instance transformation for an object and add or delete objects however the ﬂatness of the representation limits us 2 hierarchical models suppose that we wish to build a model of an automobile that we can animate we can compose the model from ﬁve parts the chassis and the four wheels figure 4 each of which we can describe by using our standard graphics primitives two frames of a simple animation of the model are shown in figure we could write a program figure 4 automobile model figure 5 two frames of animation to generate this animation by noting that if each wheel has a radius r then a degree rotation of a wheel must correspond to the car moving forward or backward a distance of the program could then contain one function to generate each wheel and another to generate the chassis all these functions could use the same input such as the desired speed and direction of the automobile in pseudocode our program might look like this float speed float d direction float t time determine speed and direction at time t d d d d d this program is just the kind that we do not want to write it is linear and shows none of the relationships among the components of the automobile there are two types of relationships that we would like to exploit first we cannot separate the movement of the car from the movement of the wheels if the car moves forward the wheels must turn 1 second we would like to use the fact that all the wheels of the automobile are identical they are merely located in different places with different orientations we can represent the relationships among parts of the models both abstractly and visually with graphs mathematically a graph consists of a set of nodes or vertices and a set of edges edges connect pairs of nodes or possibly connect a node to itself edges can have a direction associated with them the graphs we use here are all directed graphs which are graphs that have their edges leaving one node and entering another 1 it is not clear whether we should say the wheels move the chassis or the chassis moves the wheels from a graphics perspective the latter view is probably more useful figure 6 tree structure for an automobile the most important type of graph we use is a tree a connected tree is a directed graph without closed paths or loops in addition each node but one the root node has one edge entering it thus every node except the root has a parent node the node from which an edge enters and can have one or more child nodes nodes to which edges are connected a node without children is called a terminal node or leaf figure 6 shows a tree that represents the relationships in our car model the chassis is the root node and all four wheels are its children although the mathematical graph is a collection of set elements in practice both the edges and nodes can contain additional information for our car example each node can contain information deﬁning the geometric objects associated with it the information about the location and orientation of the wheels can be stored either in their nodes or in the edges connecting them with their parent in most cars the four wheels are identical so storing the same information on how to draw each one at four nodes is inefﬁcient we can use the ideas behind the instance transformation to allow us to use a single prototype wheel in our model if we do so we can replace the tree structure by the directed acyclic graph dag in figure in a dag although there are loops we cannot follow directed edges around any loop thus if we follow any path of directed edges from a node the path terminates at another node and in practice working with dags is no more difﬁcult than working with trees for our car we can store the information that positions each instance of the single prototype wheel in the chassis node in the wheel node or with the edges both forms trees and dags are hierarchical methods of expressing the re lationships in the physical model in each form various elements of a model can be related to other parts their parents and their children we will explore how to ex press these hierarchies in a graphics program figure directed acyclic graph dag model of an auto mobile a robot arm robotics provides many opportunities for developing hierarchical models consider the simple robot arm illustrated in figure a we can model it with three simple objects or symbols perhaps using only two parallelepipeds and a cylinder each of the symbols can be built up from our basic primitives y z a y x x x z z b figure robot arm a total model b components figure 9 movement of robot components and frames the robot arm consists of the three parts shown in figure b the mechanism has three degrees of freedom two of which can be described by joint angles between components and the third by the angle the base makes with respect to a ﬁxed point on the ground in our model each joint angle determines how to position a component with respect to the component to which it is attached or in the case of the base the joint angle positions it relative to the surrounding environment each joint angle is measured in each component own frame we can rotate the base about its vertical axis by an angle θ this angle is measured from the x axis to some ﬁxed point on the bottom of the base the lower arm of the robot is attached to the base by a joint that allows the arm to rotate in the plane z 0 in the arm frame this rotation is speciﬁed by an angle φ that is measured from the x axis to the arm the upper arm is attached to the lower arm by a similar joint and it can rotate by an angle ψ measured like that for the lower arm in its own frame as the angles vary we can think of the frames of the upper and lower arms as moving relative to the base by controlling the three angles we can position the tip of the upper arm in three dimensions suppose that we wish to write a program to render our simple robot model rather than specifying each part of the robot and its motion independently we take an incremental approach the base of the robot can rotate about the y axis in its frame by the angle θ thus we can describe the motion of any point p on the base by applying a rotation matrix ry θ to it the lower arm is rotated about the z axis in its own frame but this frame must be shifted to the top of the base by a translation matrix t 0 0 where is the height above the base to the point where the joint between the base and the lower arm is located however if the base has rotated then we must also rotate the lower arm using ry θ we can accomplish the positioning of the lower arm by applying ry θ t 0 h1 0 rz φ to the arm vertices we can interpret the matrix ry θ t 0 h1 0 as the matrix that positions the lower arm relative to the object or world frame and rz φ as the matrix that positions the lower arm relative to the base equivalently we can interpret these matrices as positioning the frames of the lower arm and base relative to some world frame as shown in figure 9 when we apply similar reasoning to the upper arm we ﬁnd that this arm has to be translated by a matrix t 0 h2 0 relative to the lower arm and then rotated by rz ψ the matrix that controls the upper arm is thus ry θ t 0 h1 0 rz φ t 0 h2 0 rz ψ the form of the display function for an opengl program to display the robot as a function of the joint angles using the array theta for θ φ and ψ shows how we can alter the model view matrix incrementally to display the various parts of the model efﬁciently void display glclear rotatey theta 0 base translate 0 0 0 0 rotatez theta 1 translate 0 0 0 0 rotatez theta 2 glutswapbuffers note that we have described the positioning of the arm independently of the details of the individual parts as long as the positions of the joints do not change we can alter the form of the robot by changing only the functions that draw the three parts this separation makes it possible to write separate functions to describe the components and to animate the robot figure shows the relationships among the parts of the robot arm as a tree the complete program implements the structure and uses the mouse to animate the robot through a menu it uses three parallelepipeds for the base and arms if we use our cube code from chapter with an instance transformation for the parts then the robot is rendered by the code instance void base figure tree structure for the robot arm in figure instance translate 0 0 0 5 0 0 scale instance gldrawarrays 0 n void instance translate 0 0 0 5 0 0 scale 16 instance gldrawarrays 0 n void instance translate 0 0 0 5 0 0 scale 16 instance gldrawarrays 0 n figure 11 node representation in each case the instance transformation must scale the cube to the desired size and because the cube vertices are centered at the origin each cube must be raised to have its bottom in the place y 0 the product of the model view and instance transformations is sent to the vertex shader followed by the vertices and colors if desired for each part of the robot because the model view matrix is different for each part of the robot we render each part once its data have been sent to the gpu note that in this example because we are using cubes for all the parts we need to send the points to the gpu only once however if the parts were using different symbols then we would need to use gldrawarrays in each drawing function returning to the tree in figure we can look at it as a tree data structure of nodes and edges as a graph if we store all the necessary information in the nodes rather than in the edges then each node figure 11 must store at least three items 1 a pointer to a function that draws the object represented by the node 2 a homogeneous coordinate matrix that positions scales and orients this node and its children relative to the node parent 3 pointers to children of the node certainly we can include other information in a node such as a set of attributes color texture material properties that applies to the node drawing an object described by such a tree requires performing a tree traversal that is we must visit every node at each node we must compute the matrix that applies to the primitives pointed to by the node and must display these primitives our opengl program shows an incremental approach to this traversal this example is a simple one there is only a single child for each of the parent nodes in the tree the next example shows how we handle more complex models figure 12 a humanoid figure 4 trees and traversal figure 12 shows a boxlike representation of a humanoid that might be used for a robot model or in a virtual reality application if we take the torso as the root element we can represent this ﬁgure with the tree shown in figure 13 once we have positioned the torso the position and orientation of the other parts of the model are determined by the set of joint angles we can animate the ﬁgure by deﬁning the motion of its joints in a basic model the knee and elbow joints might each have only figure 13 tree representation of figure 12 a single degree of freedom like the robot arm whereas the joint at the neck might have two or three degrees of freedom let assume that we have functions such as head and that draw the individual parts symbols in their own frames we can now build a set of nodes for our tree by deﬁning matrices that position each part relative to its parent exactly as we did for the robot arm if we assume that each body part has been deﬁned at the desired size each of these matrices is the concatenation of a translation matrix with a rotation matrix we can show these matrices as we do in figure 14 by using the matrices to label the edges of the tree remember that each matrix represents the incremental change when we go from the parent to the child the interesting part of this example is how we do the traversal of the tree to draw the ﬁgure in principle we could use any tree traversal algorithm such as a depth ﬁrst or breadth ﬁrst search although in many applications it is insigniﬁcant which traversal algorithm is used we will see that there are good reasons for always using the same algorithm for traversing our graphs we will always traverse our trees left to right depth ﬁrst that is we start with the left branch follow it to the left as deep mh mlla mrla mlll mrll figure 14 tree with matrices as we can go then go back up to the ﬁrst right branch and proceed recursively this order of traversal is called a pre order traversal we can write a tree traversal function in one of two ways we can do the traversal explicitly in the application code using stacks to store the required matrices and attributes as we move through the tree we can also do the traversal recursively in this second approach the code is simpler because the storage of matrices and attributes is done implicitly we develop both approaches because both are useful and because their development yields further insights into how we can build graphics systems 4 1 a stack based traversal consider the drawing of the ﬁgure by a function figure this function might be called from the display callback or from a mouse callback in an animation that uses the mouse to control the joint angles the model view matrix m in effect when this function is invoked determines the position of the ﬁgure relative to the rest of the scene and to the camera the ﬁrst node that we encounter results in the torso being drawn with m applied to all the torso primitives we then trace the leftmost branch of the tree to the node for the head there we invoke the function head with the model view matrix updated to mmh next we back up to the torso node then go down the subtree deﬁning the left arm this part looks just like the code for the robot arm we draw the left upper arm with the matrix mmlua and the left lower arm with matrix mmluamlla then we move on to the right arm left leg and right leg each time we switch limbs we must back up to the root and recover m it is probably easiest to think in terms of the current transformation matrix of chapter 3 the model view matrix c that is applied to the primitives deﬁned at a node 2 the matrix c starts out as m is updated to mmh for the head and later to mmlulmlll and so on the application program must manipulate c before each call to a function deﬁning a part of the ﬁgure note that as we back up the tree to start the right upper arm we need m again rather than reforming it or any other matrix we might need to reuse in a more complex model we can store push it on a stack and recover it with pop here is a simple stack class with a capacity of matrices class public static const int max index 0 void push const matrix pop private matrices max int index 2 we can ignore the projection matrix for now void push const matrix matrices index matrix index pop index return matrices index our traversal code will have translations and rotations intermixed with pushes and pops of the model view matrix consider the code without parameter values for the beginning of the function figure mvstack figure mvstack push torso translate rotate head mvstack pop mvstack push translate rotate mvstack pop mvstack push translate rotate mvstack pop mvstack push translate rotate mvstack pop mvstack push the ﬁrst push duplicates the current model view matrix putting the copy on the top of the model view matrix stack this method of pushing allows us to work immedi ately with the other transformations that alter the model view matrix knowing that we have preserved a copy on the stack the following calls to translate and ro tate determine mh and concatenate it with the initial model view matrix we can then generate the primitives for the head the subsequent pop recovers the original model view matrix note that we must do another push to leave a copy of the original model view matrix that we can recover when we come back to draw the right leg the functions for the individual parts are similar to the previous example here is the torso function void torso mvstack push instance translate 0 0 0 5 0 0 scale 16 instance colorcube gldrawarrays 0 n mvstack pop note the use of a push at the beginning and a pop at the end of the function these serve to isolate the code and protect other parts of the program from being affected by state changes in this function you should be able to complete this function by continuing in a similar manner appendix a contains a complete program that implements this ﬁgure with a menu that will allow you to change the various joint angles the individual parts are implemented using parallelepipeds and the entire model can be shaded as we discussed in chapter 5 we have not considered how attributes such as color and material properties are handled by our traversal of a hierarchical model attributes are state variables once set they remain in place until changed again hence we must be careful as we traverse our tree for example suppose that within the code for torso we set the color to red and then within the code for head set the color to blue if there are no other color changes the color will still be blue as we traverse the rest of the tree and may remain blue after we leave the code for figure here is an example in which the particular traversal algorithm can make a difference because the current state can be affected differently depending on the order in which the nodes are visited this situation may be disconcerting but there is a solution we can create other stacks that allow us to deal with attributes in a manner similar to our use of the model view matrix if we push the attributes on the attribute stack on entrance to the function figure and pop on exit we have restored the attributes to their original state moreover we can use additional pushes and pops within figure to control how attributes are handled in greater detail in a more complex model we can apply these ideas recursively if for example we want to use a more detailed model of the head one incorporating eyes ears a nose and a mouth then we could model these parts separately the head would then itself be modeled hierarchically and its code would include the pushing and popping of matrices and attributes although we have discussed only trees if two or more nodes call the same function we really have a dag but dags present no additional difﬁculties color plates 22 and 27 show hierarchical models of robots and ﬁgures used in simulations these objects were created with high level interactive software that relies on our ability to traverse hierarchical structures to render the models the approach that we used to describe hierarchical objects is workable but has limitations the code is explicit and relies on the application programmer to push and pop the required matrices and attributes in reality we implemented a stack based representation of a tree the code was hardwired for the particular example and thus would be difﬁcult to extend or use dynamically the code also does not make a clear distinction between building a model and rendering it although many application programmers write code in this form we prefer to use it primarily to illustrate the ﬂow of an opengl program that implements tree hierarchies we now turn to a more general and powerful approach to working with tree structured hierarchies 5 use of tree data structures our second approach is to use a standard tree data structure to represent our hierar chy and then to render it with a traversal algorithm that is independent of the model we use a left child right sibling structure consider the alternate representation of a tree in figure it is arranged such that all the elements at the same level are linked left to right the children of a given node are represented as a second list arranged from the leftmost child to the rightmost this second list points downward in figure 15 this representation describes the structure of our hierarchical ﬁgure but the structure still lacks the graphical information at each node we must store the information necessary to draw the object a func tion that deﬁnes the object and the homogeneous coordinate matrix that positions the object relative to its parent consider the following node structure typedef struct treenode root a b siblings mat m void f struct treenode sibling struct treenode child treenode figure 15 a tree b left child right sibling representation the array m stores a 4 4 homogeneous coordinate matrix when we render the node this matrix must ﬁrst multiply the current model view matrix then the func tion f which includes the graphics primitives is executed we also store a pointer to the sibling node on the right and a pointer to the leftmost child if one or the other does not exist then we store the null pointer null for our ﬁgure we specify 10 nodes corresponding to the 10 parts of our model treenode rla_node we can specify the nodes either in the main function or in myinit for example consider the root of the ﬁgure tree the torso node it can be oriented by a rotation about the y axis we can form the required rotation matrix using our matrix func tions and the function to be executed after forming the matrix is torso the torso node has no siblings and its leftmost child is the head node so the torso node is given as follows m rotatey theta 0 f torso sibling null child if we use a cube as the basis for the torso the drawing function might look as follows void torso mvstack push instance translate 0 0 0 5 0 0 scale gluniformmatrix4fv 16 instance colorcube gldrawarrays 0 n mvstack pop the instance transformation ﬁrst scales the cube to the desired size and then trans lates it so its bottom lies in the plane y 0 the torso is the root node of the ﬁgure so its code is a little different from the other nodes consider the speciﬁcation for the left upper arm node m translate 0 9 0 0 rotatex theta 3 f sibling child and the function void mvstack push instance translate 0 0 0 5 0 0 scale gluniformmatrix4fv 16 model_view instance colorcube gldrawarrays 0 n model_view mvstack pop the upper arm must be translated relative to the torso and its own width to get the center of rotation in the correct place the node for the upper arm has both a sibling the upper right arm and a child the lower left arm to render the left upper arm we ﬁrst compute an instance transformation that gives it the desired size and positions so its bottom is also on the plane y 0 this instance matrix is concatenated with the current model view matrix to position the upper left arm correctly in object coordinates the other nodes are speciﬁed in a similar manner traversing the tree in the same order preorder traversal as in section 4 can be accomplished by the recursive code as follows void traverse treenode root if root null return mvstack push model_view model_view model_view root m root f if root child null traverse root child model_view mvstack pop if root sibling null traverse root sibling to render a nonnull node we ﬁrst save the graphics state with mvpush view we then use the matrix at the node to modify the model view matrix we then draw the objects at the node with the function pointed to by f finally we traverse all the children recursively note that because we have multiplied the model view matrix by the local matrix we are passing this altered matrix to the children for the siblings however we do not want to use this matrix because each has its own local matrix hence we must return to the original state mvstack pop before traversing the children if we are changing attributes within nodes either we can push and pop attributes within the rendering functions or we can push the attributes when we push the model view matrix one of the nice aspects of this traversal method is that it is completely indepen dent of the particular tree thus we can use a generic display callback such as the following void display void glclear traverse glutswapbuffers we again animate the ﬁgure by controlling individual joint angles which are selected from a menu and are incremented and decremented through the mouse buttons thus the dynamics of the program are in the mouse callback which changes an angle recomputes the appropriate node matrix and then posts a redisplay mymouse int button int state int x int y if button state theta angle 5 0 if theta angle 0 theta angle 0 if button state theta angle 5 0 if theta angle 0 theta angle 0 mvstack push model_view switch angle case 0 torso_node m rotatey theta 0 break case 1 case 2 head_node m translate 0 0 0 5 0 0 rotatex theta 1 rotatey theta 2 translate 0 0 0 5 0 0 break rest of cases there is one more feature that we can add to show the ﬂexibility of this approach as the program executes we can add or remove dynamic nodes rather than static nodes we can create dynamic nodes with the following code typedef treenode new treenode nodes are deﬁned as before for example m translate upper_arm_width 0 9 torso_height 0 0 rotatex theta 3 f lua_node sibling lua_node child with the traversal by traverse for our ﬁgure example there is no particular advantage to the dynamic ap proach in a more general setting however we can use the dynamic approach to create structures that change interactively for example we can use this form to write an application that will let us edit ﬁgures adding and removing parts as desired this type of implementation is the basis for the scene trees that we discuss in section color plate 27 shows one frame of an animation with the ﬁgure using cylinders for most of the parts and adding lighting note that as we have coded our examples there is a ﬁxed traversal order for the graph if we had applied some other traversal algorithm we could have produced a different image if we made any state changes within the graph such as changing transformations or attributes we can avoid some of these potential problems if we are careful to isolate parts of our code by pushing and popping attributes and matrices in each node although there is a performance penalty for doing so too often 6 animation the models that we developed for our two examples the robot arm and the ﬁgure are articulated the models consist of rigid parts connected by joints we can make such models change their positions in time animate them by altering the values of a small set of parameters hierarchical models allow us to model the compound motions incorporating the physical relationships among the parts of the model what we have not discussed is how to alter the parameters over time so as to achieve the desired motion of the many approaches to animation a few basic techniques are of particular importance when we work with articulated ﬁgures these techniques arise both from traditional hand animation and from robotics in the case of our robot model consider the problem of moving the tip of the upper arm from one position to another the model has three degrees of freedom the three angles that we can specify although each set of angles has a unique position for the tip the converse is not true given a desired position of the tip of the arm there may be no set of angles that place the tip as desired a single set of angles that yields the speciﬁed position or multiple sets of angles that place the tip at the desired position studying kinematics involves describing the position of the parts of the model based on only the joint angles we can use our hierarchical modeling methods either to determine positions numerically or to ﬁnd explicit equations that give the position of any desired set of points in the model in terms of the joint angles thus if θ is an array of the joint angles and p is an array whose elements are the vertices in our model a kinematic model is of the form p f θ likewise if we specify the rates of change of the joint angles the joint velocities then we can obtain velocities of points on the model the kinematic model neglects matters such as the effects of inertia and friction we could derive more complex differential equations that describe the dynamic be havior of the model in terms of applied forces a topic that is studied in robotics whereas both kinematics and dynamics are ways of describing the forward be havior of the model in animation we are more concerned with inverse kinematics and inverse dynamics given a desired state of the model how can we adjust the joint angles so as to achieve this position there are two major concerns first given an en vironment including the robot and other objects we must determine whether there exists a sequence of angles that achieves the desired state there may be no single valued function of the form θ f 1 p for a given p in general we cannot tell if there is any θ that corresponds to this position or if there are multiple values of θ that satisfy the equation even if we can ﬁnd a sequence of joint angles we must ensure that as we go through this sequence our model does not hit any obstacles or violate any physical constraints although for a model as simple as our robot we might be able to ﬁnd equations that give the joint angles in terms of the position we cannot do so in general because the forward equations do not have unique inverses the ﬁgure model which has 11 degrees of freedom should give you an idea of how difﬁcult it is to solve this problem a basic approach to overcoming these difﬁculties comes from traditional hand animation techniques in key frame animation the animator positions the objects at a set of times the key frames in hand animation animators then can ﬁll in the remaining frames a process called in betweening in computer graphics we can automate in betweening by interpolating the joint angles between the key frames or equivalently using simple approximations to obtain the required dynamic equations between key frames using gpus much of the work required for in betweening can now be automated as part of the pipeline often using the programmability of recent gpus we can also use the spline curves that we develop in chapter 10 to give smooth methods of going between key frames although we can develop code for the interpolation both a skillful human animator and good interactive methods are crucial if we are to choose the key frames and the positions of objects in these frames graphical objects although we have introduced multiple graphics paradigms our development has been heavily based on a pipeline implementation of the synthetic camera model we made this choice because we want to support interactive three dimensional ap plications with currently available hardware and software consequently we have emphasized a mode of graphics in which geometric data are placed on the gpu and rendered to the screen almost immediately afterward we have not made full use of the fact that once data are on the gpu they can be reused without regenerating them in the application in addition our desire to present the basics of implementation has led us to de velop graphics in a manner that was never far from the details of the implementation for all its beneﬁts this approach has not let us exploit many high level alternatives to developing graphical applications now we move to a higher level of abstraction and introduce two major concepts first we expand our notion of objects from geometric objects such as polygons and vectors to include most of the elements within a graphics program such as viewers lights and material properties second we focus on objects that exist even after their images have been drawn and even if we never display them we investigate other approaches such as the use of classes in c or structures in c although the opengl api does not support this approach directly we do not have to abandon opengl we still use opengl for rendering and we regard what we develop as a software layer on top of opengl 1 methods attributes and messages our programs manipulate data the data may be in many forms ranging from numbers to strings to the geometric entities that we build in our applications in traditional imperative programming the programmer writes code to manipulate the data usually through functions the data are passed to a function through the function s parameters data are returned in a similar manner to manipulate the data sent to it the function must be aware of how those data are organized consider for example the cube that we have used in many of our previous examples we have seen that we can model it in various ways including with vertex pointers edge lists and lists of polygon vertices the application programmer may care little about which model is used and may prefer to regard the cube as an atomic entity or an object in addition she may care little about the details of how the cube is rendered to the screen which shading model or which polygon ﬁll algorithm is used she can assume that the cube knows how to render itself and that conceptually the rendering algorithm is tied to the object itself in some ways opengl supports this view by using the state of the graphics system to control rendering for example the color of the cube its orientation and the lights that are applied to its surfaces can all be part of the state of the graphics system and may not depend on how the cube is modeled figure 16 imperative programming paradigm figure object oriented paradigm however if we are working with a physical cube we might ﬁnd this view a bit strange the location of a physical cube is tied to the physical object as are its color size and orientation although we could use opengl to tie some properties to a virtual cube through pushing and popping various attributes and matrices the underlying programming model does not support these ideas well for example a function that transforms the cube would have to know exactly how the cube is represented and would work as shown in figure 16 the application programmer would write a function that would take as its inputs a pointer to the cube s data and the parameters of the transformation it would then manipulate the data for the cube and return control to the application program perhaps also returning some values object oriented design and object oriented programming look at manipulation of objects in a fundamentally different manner even in the early days of object oriented programming languages such as smalltalk recognized that computer graph ics provides excellent examples of the power of the object oriented approach recent trends within the software community indicate that we can combine our pipeline orientation with an object orientation to build even more expressive and high performance graphics systems object oriented programming languages deﬁne objects as modules with which we build programs these modules include the data that deﬁne the module such as the vertices for our cube properties of the module attributes and the functions methods that manipulate the module and its attributes we send messages to objects to invoke a method this model is shown in figure 17 the advantage to the writer of the application program is that she now does not need to know how the cube is represented she needs to know only what functionality the cube object supports what messages she can send to it although the c struct has some of the properties of objects the c language does not support the full power of an object oriented approach in c the struct is replaced with the class c classes have two important properties that we can exploit to get the ﬂavor of the object oriented approach c programmers should have no trouble understanding these concepts 2 a cube object suppose that we wish to create a cube object in c that has a color attribute and a homogeneous coordinate transformation associated with it in c we could use a struct of the following form struct cube float color 3 float matrix 4 4 implementation goes here the implementation part of the structure contains the information on how a cube is actually represented typically an application programmer does not need this information and needs to change only the color or matrix associated with the cube once the struct has been deﬁned instances of the cube object can be created as are other basic data types cube a b attributes that are part of the class deﬁnition can be changed for each instance of the cube thus we can set the color of cube a to red as follows a color 0 1 0 a color 1 a color 2 0 0 it should be clear how such a struct can be implemented within an opengl system although we have created a retained cube object we are limited in how we can manipulate it or render it we could write a function that would render the cube through code such as a or we could rotate the cube by invoking the method a theta d where d is the vector about which we wish to rotate this approach is workable but has limitations one is that we need separate rendering and rotation functions for each type of object a second is that the imple mentation part of the code is accessible to application programs c classes solve both of these problems a c class can have public private and protected mem bers the public members are similar to the members of c struct and can be altered by any function the private members neither are visible to nor can be altered by a function that uses the class the protected members are visible to classes within the same hierarchy a programmer can also declare classes to be friends to give access to speciﬁc classes thus in c we deﬁne the cube as class cube public color model private implementation goes here thereby protecting and hiding the details of the implementation furthermore c classes allow us to have members that are functions or methods once we add such functions the object orientation becomes clearer suppose that we add member functions to the public part of the cube class as follows void render void translate float x float y float z void rotate float theta float float float now an application program could create translate rotate and render a cube through the following code cube a a rotate 0 1 0 0 0 0 0 a translate 1 0 2 0 3 0 a render conceptually this code assumes that an instance of the cube knows how to rotate itself and that by executing the code a rotate we are sending a message to a cube object that we would like it to carry out such a rotation we could easily write an implementation of the rotation and translation methods that would use our rotation and translation functions to change the matrix member of the cube object it is less clear what a function call such as a render really does each instance of an object persists and can be altered by further code in the application program what is most important is that we have created an object that continues to exist somewhere in our system in a form that is not visible to the application program the attributes of the object are also in the system and can be altered by functions such as rotate the render function causes the object to be redrawn using the object s state rather than the system s current state hence the render step involves sending data to the gpu with the necessary vertex attribute data coming from the implementation part of the object and using a function such as gldrawarrays to display the object 3 implementing the cube object as an example of the choices that go into developing the private part of an object let s consider the cube one basic implementation would be similar to what we did for our rotating cube examples we look at the cube as comprised of six faces each of which consists of two triangles thus the private part of the cube might be of the form private points colors note that we are allowing for different colors at each vertex the constructor for a cube would set values for the points with a default of a unit cube we can do far better if we include more information in the implementation of particular interest is information that might help in the rendering thus whereas opengl will do hidden surface removal correctly through the z buffer algorithm we can often do much better by eliminating objects earlier through a separate visibility test as we will discuss in section 11 to support this functionality we might want to include information that determines a bounding volume for the object for example we can include the axis aligned bounding box for objects within the private part of the code for polygonal objects we need simply save the minimum and maximum of the x y and z of the vertices after they have been transformed by any transformation matrix stored with the object 4 objects and hierarchy one of the major advantages of object oriented design is the ability to reuse code and to build more sophisticated objects from a small set of simple objects as in section 4 we can build a ﬁgure object from cubes and have multiple instances of this new object each with its own color size location and orientation a class for the humanoid ﬁgure could refer to the classes for arms and legs the class for a car could refer to classes for wheels and a chassis thus we would once more have tree like representations similar to those that we developed in section 5 often in object oriented design we want the representations to show relation ships more complex than the parent child relationship that characterizes trees as we have used trees the structure is such that the highest level of complexity is at the root and the relationship between a parent and child is a has a relationship thus the stick ﬁgure has two arms and two legs whereas the car has four wheels and a chassis we can look at hierarchy in a different manner with the simplest objects being the top of the hierarchy and the relationship between parents and children being an is a relationship this type of hierarchy is typical of taxonomies a mammal is an animal a human is a mammal we used this relationship in describing projections a parallel projection is a planar geometric projection an oblique projection is a parallel projection has a relationships allow us to deﬁne multiple complex objects from simpler objects and also allow the more complex object to inherit properties from the simpler object thus if we write the code for a parallel projection the oblique projection code can use this code and reﬁne only the parts that are necessary to convert the general parallel projection to an oblique one for geometric objects we can deﬁne base objects with a default set of properties such as their color and material properties an application programmer could then use these properties or change them in subobjects these concepts are supported by languages such as c that allow for subclasses and inheritance 5 geometric objects suppose that we now want to build an object oriented graphics system what objects should we include although it is clear that we want to have objects such as points vectors polygons rectangles and triangles possibly using subclasses it is less clear how we should deal with attributes light sources and viewers for example should a material property be associated with an object such as a cube or is it a separate object the answer can be either or both we could create a cube class in which there is a member for each of the ambient diffuse and specular material properties that we introduced with the phong model in chapter 5 we could also deﬁne a material class using code such as the following class material public specular float shininess diffuse ambient we could then assign the material to a geometric object through a member function of the cube class as follows cube a material b a setmaterial b light sources are geometric objects they have position and orientation among their features and we can easily add a light source object class light public boolean type boolean near position orientation specular diffuse ambient once we have built up a collection of geometric objects we can use it to describe a scene to take advantage of the hierarchical relationships that we have introduced we develop a new tree structure called a scene graph scene graphs if we think about what goes into describing a scene we can see that in addition to our graphical primitives and geometric objects derived from these primitives we have other objects such as lights and a camera these objects may also be deﬁned by vertices and vectors and may have attributes such as color that are similar to the attributes associated with geometric primitives it is the totality of these objects that describes a scene and there may be hierarchical relationships among these objects for example when a primitive is deﬁned in a program the camera parameters that exist at that time are used to form the image if we alter the camera lens between the deﬁnition of two geometric objects we may produce an image in which each object is viewed differently although we cannot create such an image with a real camera the example points out the power of our graphics systems we can extend our use of tree data structures to describe these relationships among geometric objects cameras lights and attributes knowing that we can write a graphical application program to traverse a graph we can expand our notion of the contents of a graph to describe an entire scene one possibility is to use a tree data structure and to include various attributes at each node in addition to the instance matrix and a pointer to the drawing function another possibility is to allow new types of nodes such as attribute deﬁnition nodes and matrix transformation nodes consider the tree in figure 18 here we have set up individual nodes for the colors and for the model view matrices the place where there are branches at the top can be considered a special type of node a group node whose function is to isolate the two children the group node allows us to preserve the state that exists at the time that we enter a node and thus isolates the state of the subtree beginning at a group node from the rest of the tree using our preorder traversal algorithm the corresponding application code is of the following form figure 18 scene tree pushattrib pushmatrix color translate rotate translate popmatrix pushmatrix translate rotate popmatrix popattrib the group nodes correspond to the opengl push and pop functions this code preserves and restores both the attributes and the model view matrix before exiting it sets a drawing color that applies to the rest of the tree and traverses the tree in a manner similar to the ﬁgure example we can go further and note that we can use the attribute and matrix stacks to store the viewing conditions thus we can create a camera node in the tree although we probably do not want a scene in which individual objects are viewed with different cameras we may want to view the same set of objects with multiple cameras produc ing for example the multiview orthographic projections and isometric view that are used by architects and engineers such images can be created with a scene graph that has multiple cameras the scene graph we have just described is equivalent to an opengl program in the sense that we can use the tree to generate the program in a totally mechanical fashion this approach was taken by open inventor and later by open scene graph osg both object oriented apis that were built on top of opengl open inventor and osg programs build manipulate and render a scene graph execution of a pro gram causes traversal of the scene graph which in turn executes graphics functions that are implemented in opengl the notion of scene graphs couples nicely with the object oriented paradigm introduced in section we can regard all primitives attributes and transforma tions as software objects and we can deﬁne classes to manipulate these entities from this perspective we can make use of concepts such as data encapsulation to build up scenes of great complexity with simple programs that use predeﬁned software ob jects we can even support animations through software objects that appear as nodes in the scene graph but cause parameters to change and the scene to be redisplayed although in open inventor the software objects are rendered using opengl the scene graph itself is a database that includes all the elements of the scene opengl is the rendering engine that allows the database to be converted to an image but it is not used in the speciﬁcation of the scene game engines employ a similar strategy in which the game play modiﬁes a scene graph that can be traversed and rendered at an interactive rate graphics software systems are evolving to the conﬁguration shown in fig ure 19 opengl is the rendering engine it usually sits on top of another layer known as the hardware abstraction layer hal which is a virtual machine that communicates with the physical hardware above opengl is an object oriented layer that supports scene graphs and a storage mechanism user programs can be written for any of the layers depending on what facilities are required by the application figure 19 modern graphics architecture 9 open scene graph open scene graph is probably the most popular of the full scene graph apis and provides much of the functionality lacking in our example in addition to supporting a wider variety of nodes there are two additional concepts that are key to osg first one of the beneﬁts of a higher level of software than opengl is that such software can balance the workload between the cpu and the graphics processor consider how we process geometry in opengl an application produces primitives that are speciﬁed through sets of vertices as we have seen opengl s main concern is rendering all geometric primitives pass down at least part of the pipeline it is only at the end of vertex processing that primitives that lie outside the view volume are clipped out if a primitive is blocked from the viewer by another opaque geometric object and cannot appear in the ﬁnal image it nevertheless passes through most of the pipeline and only during hidden surface removal will it be eliminated although present gpus can process millions of vertices per second many applications have such complex geometry that even these gpus cannot render the geometry at a sufﬁ ciently high frame rate osg uses two strategies occlusion culling and level of detail rendering to lower the rendering load occlusion culling seeks to eliminate objects that cannot be visible because they are blocked by other objects before they enter the rendering pipeline in figure 20 region occluded by square view volume figure 20 occlusion we see that the square lies in the view volume and blocks the triangle from view although the z buffer algorithm would yield a correct rendering because opengl processes each object independently it cannot discover the occlusion however all the geometry is stored in the osg scene graph as is the information on the viewer hence osg can use one of many algorithms to go through the scene graph and cull objects we will examine an approach to occlusion culling that uses binary spatial partitioning in section 11 the second strategy is based on an argument similar to the one that we used to justify mipmaps for texture mapping if we can tell that a geometric object will render to a small area of the display we do not need to render all the detail that might be its geometry once more the necessary information can be placed in the scene graph osg has a level of detail node whose children are the models of an object with different levels of geometric complexity the application program sets up these nodes during the traversal of the scene graph osg determines which level of detail to use level of detail rendering is important not only to osg but also to real time ap plications such as interactive games that are built using proprietary game engines game engines are very large complex software objects that may comprise millions of lines of code although a game engine may use opengl or directx to render the graphics and make extensive use of programmable shaders a game engine also has to handle the game play and manage complex interactions that might involve multi ple players game engines use scene graphs to maintain all the needed information including the geometry and texture maps and use level of detail extensively in pro cessing their scene graphs in the next section we will examine some related issues involving graphics over the internet the second major concept is how the scene graph is processed for each frame osg uses three traversals rather than the single traversal in our simple scene graph the goal of the traversal process is to create a list of the geometry that can be rendered this list contains the geometry at the best level of detail and only the geometry that has survived occlusion culling in addition the geometry has been sorted so that translucent surfaces will be rendered correctly the ﬁrst traversal deals with updates to the scene graph that might be generated by callbacks that handle interaction or changes to the geometry from the application program the second traversal builds a list of the geometry that has to be rendered this traversal uses occlusion culling translucency level of detail and bounding vol umes the ﬁnal traversal goes through the geometry list and issues the necessary opengl calls to render the geometry 10 graphics and the internet before leaving the subject of scene graphs we consider some of the issues that govern how graphics can be conveyed over the internet of particular interest are multiplayer games that can involve thousands of concurrent participants each of whom poten tially can affect the scene graph of any other participant and applications with large models that may be distributed over multiple sites the internet has had an enormous effect on virtually all communications and computer applications it allows us to communicate information in a multitude of forms and makes possible new methods of interaction in order to use the internet to its full potential we need to move graphical information efﬁciently to build ap plications that are viewable from many locations and to access resources distributed over many sites opengl and its extensions have had a major inﬂuence on the de velopment of net based three dimensional applications and standards we will take a graphics oriented approach and see what extensions we need to develop internet applications some of the concepts will be familiar we use the client server model to allow efﬁcient rendering we also look at how we can implement graphical applica tions that are independent of the api 10 1 hypermedia and html as the internet evolved a series of standard high level protocols became widely accepted for transferring mail ﬁles and other types of information systems such as the x window system allowed users to open windows on remote systems and to transfer basic graphical information as the internet grew however more and more information became publicly available and users needed more sophisticated methods to share information that was stored in a distributed way in diverse formats there are three key elements necessary for sharing such information 1 an addressing scheme that allows users to identify resources over the network 2 a method of encoding information in addition to simple text such as pictures and references or links to other resources and 3 a method of searching for resources interactively the ﬁrst two needs were addressed by researchers at the european particle phys ics center cern who created the world wide web which is essentially a net worked hypertext system resources ﬁles are identiﬁed by a unique uniform re source locator url that consists of three parts the protocol for transferring the document the server where the document is located and the location on the server where the document is to be found for example the url for support for this text can be found at http www cs unm edu angel book the ﬁrst part http indi cates that the information will be transferred using the hypertext transfer protocol http the second part www cs unm edu identiﬁes the server as the world wide web site of the computer science department at the university of new mexico the ﬁnal part indicates that the information is stored in user account angel under the directory book because no document is indicated a default document the home page will be displayed the second contribution of cern was the hypertext markup language html which provided a simple way to describe a document consisting of text references to other documents links and images html documents are text docu ments typically in ascii code or one of the standard extended character sets the combination of url addressing and html documents provided a way of making resources available but until the national center for supercomputer applications ncsa came up with its browser mosaic it was not easy for a user to ﬁnd resources and to search the web browsers are interactive programs that allow the user to search for and download documents on the web mosaic and later netscape navigator opened the door to surﬁng the web 10 2 java and applets one issue with interactive computer graphics on the internet is the heterogenous collection of computer hardware and operating systems an application programmer cannot create an application that will execute on any machine connected to the web even if they use an industry standard api like opengl java solves a portion of this problem by creating a machine in software that can be implemented on any computer java programs are compiled into byte code that can be run on any java machine regardless of what the underlying hardware is thus the client and server exchange byte code over the web small programs in byte code called applets are understood by the standard web browsers and have added a tremendous amount of dynamic behavior to the web 10 3 interactive graphics and the web while html is useful for structuring information for layout on a rendered web page until html version 5 it lacked any rendering capabilities most often any interactive rendering that was done in a browser was originally done using a java applet however the requirement of downloading an applet combined with support of java virtual ma chines dimished java s promise of application portability particularly when related to three dimensional computer graphics as web browsers became more capable of supporting various formats other technologies evolved adobe s flash technology uses a plugin to a web browser to provide interactivity but it lacks at the time of this writing a comprehensive solu tion for doing three dimensional graphics with most of its focus on rendering video or simple user interfaces another technology that evolved was javascript javascript is a derivative lan guage of java that is interpreted and executed by the web browser as compared to a virtual machine plugin this allows tighter integration of normal web page ren dering as interactive user controlled rendering once again however javascript s rendering capabilities were mostly focused on two dimensional operations and most three dimensional renderings were done through software renderers implemented in javascript until recently there wasn t a widely adopted standard for interactive three dimensional graphics 10 4 webgl webgl is a derivative of opengl or more speciﬁcally opengl es version 2 0 the embedded system version of opengl it provides javascript bindings for opengl functions and allows an html page using webgl to render using any gpu resources available in the system where the web browser is running webgl is currently under development by the khronos group the same indus try consortium that develops opengl at the time of this writing it integrates the rendering capabilities of s canvas element as with modern opengl appli cations all rendering is controlled by vertex and fragment shaders 11 other tree structures tree and dag structures provide powerful tools to describe scenes trees are also used in a variety of other ways in computer graphics of which we consider three the ﬁrst is the use of expression trees to describe an object hierarchy for solid objects the other two describe spatial hierarchies that we can use to increase the efﬁciency of many rendering algorithms 11 1 csg trees the polygonal representation of objects that we have used has many strengths and a few weaknesses the most serious weakness is that polygons describe only the surfaces that enclose the interior of a three dimensional object such as a polyhedron in cad applications this limitation causes difﬁculties whenever we must employ any volu metric properties of the graphical object such as its weight or its moment of inertia in addition because we display an object by drawing its edges or surfaces there can be ambiguities in the display for example the wireframe shown in figure 21 can be interpreted either as a cube with a hole through it created by removal of a cylinder or as a solid cube composed of two different materials constructive solid geometry csg addresses these difﬁculties assume that we start with a set of atomic solid geometric entities such as parallelepipeds cylinders and spheres the attributes of these objects can include surface properties such as color or reﬂectivity but also volumetric properties such as size and density in describing scenes of such objects we consider those points in space that constitute each object equivalently each object is a set of points and we can use set algebra to form new objects from these solid primitives figure 21 wireframe that has two possible interpreta tions a b a b a b a b figure 22 set operations a b c d a b c d figure 23 csg object csg modeling uses three set operations union intersection and set difference the union of two sets a and b written a b consists of all points that are either in a or in b the intersection of a and b a b is the set of all points that are in both a and b the set difference a b is the set of points that are in a and are not in b figure 22 shows two objects and possible objects created by the three set operations objects are described by algebraic expressions the expression a b c d might describe an object such as the one illustrated in figure 23 typically we store and parse algebraic expressions using expression trees where internal nodes store operations and terminal nodes store operands for example the tree in figure 24 is a csg tree that represents the object a b c d in figure 23 we can evaluate or render the csg tree by a postorder traversal that is we recursively evaluate the tree to the left of a node and the tree on the right of the node and ﬁnally use these values to evaluate the node itself rendering of objects in csg often is done with a variant of ray tracing see exercise 10 and chapter 11 figure 24 csg tree figure 25 collection of polygons and a viewer 11 2 bsp trees scene graphs and csg trees describe hierarchical relationships among the parts of an object we can also use trees to describe the world object space and encapsulate the spatial relationships among groups of objects these relationships can lead to fast methods of visibility testing to determine which objects might be seen by a camera thus avoiding processing all objects with tests such as the z buffer algorithm these techniques have become very important in real time animations for computer games one approach to spatial hierarchy starts with the observation that a plane divides or partitions three dimensional space into two parts half spaces successive planes subdivide space into increasingly smaller partitions in two dimensions we can use lines to partition space consider the polygons shown in figure 25 with the viewer located as indicated arguing as we did in chapter 7 there is an order in which to paint these polygons so that the image will be correct rather than using a method such as depth sort each time we want to render these polygons we can store the relative positioning information in a tree we start the construction of the tree using the plane of one polygon to separate groups of polygons that are in front of it from those that are behind it for example consider a simple world in which all the polygons are parallel and are oriented with their normals parallel to the z axis this assumption makes it easier to illustrate the algorithm but does not affect the algorithm as long as the plane figure 26 top view of polygons figure 27 binary space partitioning bsp tree of any polygon separates the other polygons into two groups in this world the view from the z direction is as shown in figure 26 plane a separates the polygons into two groups one containing b and c which are in front of a and the second containing d e and f which are behind a we use this plane to start a binary space partitioning tree bsp tree that stores the separating planes and the order in which they are applied thus in the bsp tree in figure 27 a is at the root b and c are in the left subtree and d e and f are in the right subtree proceeding recursively c is behind the plane of b so we can complete the left subtree the plane of d separates e and f thus completing the right subtree note that for a given set of polygons there are multiple possible bsp trees corresponding to the order in which we choose to make our partitions in the general case if a separating plane intersects a polygon then we can break up the polygon into two polygons one in front of the plane and one behind it similar to what we did with overlapping polygons in the depth sort algorithm in chapter 6 figure 28 movement of the viewer to back we can use this tree to paint the polygons by doing a backward in order traver sal that is we traverse the tree recursively drawing the right subtree ﬁrst followed by the root and ﬁnally by the left subtree one of the advantages of bsp trees is that we can use the same tree even if the viewer moves by changing the traversal algo rithm if the viewer moves to the back as shown in figure 28 then we can paint the polygons using a standard in order traversal left subtree root right subtree also note that we can use the algorithm recursively wherever planes separate sets of poly gons or other objects into groups called clusters thus we might group polygons into polyhedral objects then group these polyhedra into clusters we can then ap ply the algorithm within each cluster in applications such as ﬂight simulators where the world model does not change but the viewer s position does the use of bsp trees can be efﬁcient for doing visible surface determination during rendering the tree contains all the required information to paint the polygons the viewer s position de termines the traversal algorithm bsp trees are but one form of hierarchy to divide space another is the use of bounding volumes such as spheres the root of a tree of bounding spheres would be the sphere that contains all the objects in a scene subtrees would then correspond to groups of objects within the larger sphere and the root nodes would be the bounding spheres for each object we could use the same idea with other types of bounding volumes such as the bounding boxes that we discussed in chapter 6 spheres are particularly good for interactive games because we can quickly determine if an object is potentially visible or whether two objects might collide 11 3 quadtrees and octrees one limitation of bsp trees is that the planes that separate polygons can have an arbitrary orientation so that construction of the tree can be costly involving ordering and often splitting of polygons octrees and quadtrees avoid this problem by using separating planes and lines parallel to the coordinate axes consider the two dimensional picture in figure 29 we assume that this pic ture is composed of black and white pixels perhaps formed by the rendering of a three dimensional scene if we wish to store the scene we can save it as a binary array but notice the great deal of coherence in the picture pixels of each color are clustered figure 29 two dimensional space of pixels figure 30 first subdivision of space together we can draw two lines as in figure 30 dividing the region into quadrants noting that one quadrant is all white we can assign a single color to it for the other three we can subdivide again and continue subdividing any quadrant that contains pixels of more than a single color this information can be stored in a tree called a quadtree in which each level corresponds to a subdivision and each node has four children thus the quadtree for our original simple picture is as shown in figure 31 because we construct the quadtree by subdividing space with lines parallel to the coordinate axes formation and traversal of the tree are simpler than are the corresponding operations for a bsp tree one of the most important advantages of quadtrees is that they can reduce the amount of memory needed to store images quadtrees partition two dimensional space they can also be used to partition object space in a manner similar to bsp trees and thus can be traversed in an order depending on the position of the viewer so as to render correctly the objects in each region in three dimensions quadtrees extend to octrees the partitioning is done by planes parallel to the coordinate axes and each step of the partitioning subdivides space into eight octants as shown in figure 32 viewing transformations clipping and feedback chapter objectives after reading this chapter you ll be able to do the following view a three dimensional geometric model by transforming it to have any size orientation and perspective understand a variety of useful coordinate systems which ones are required by opengl and how to transform from one to the next transform surface normals clip your geometric model against arbitrary planes capture the geometric result of these transforms before displaying them previous chapters hinted at how to manipulate your geometry to fit into the viewing area on the screen but we ll give a complete treatment in this chapter this includes feedback the ability to send it back to the application as well as clipping the intersection of your geometry with planes either by opengl or by you typically you ll have many objects with independently specified geometric coordinates these need to be transformed moved scaled and oriented into the scene then the scene itself needs to be viewed from a particular location direction scaling and orientation this chapter contains the following major sections viewing provides an overview of how computer graphics simulates the three dimensional world on a two dimensional display user transformations characterize the various types of transformations that you can employ in shaders to manipulate vertex data opengl transformations are the transformations opengl implements transform feedback describes processing and storing vertex data using vertex transforming shaders to optimize rendering performance viewing if we display a typical geometric model coordinates directly onto the display device we probably won t see much the range of coordinates in the model e g to meters will not match the range of coordinates consumed by the display device e g to pixels and it is cumbersome to restrict ourselves to coordinates that would match in addition we want to view the model from different locations directions and perspectives how do we compensate for this fundamentally the display is a flat fixed two dimensional rectangle while our model contains extended three dimensional geometry this chapter will show how to project our model three dimensional coordinates onto the fixed two dimensional screen coordinates the key tools for projecting three dimensions down to two are a viewing model use of homogeneous coordinates application of linear transformations by matrix multiplication and setting up a viewportmapping these tools are each discussed in detail below chapter viewing transformations clipping and feedback viewing model for the time being it is important to keep thinking in terms of three dimensional coordinates while making many of the decisions that determine what is drawn on the screen it is too early to start thinking about which pixels need to be drawn instead try to visualize three dimensional space it is later after the viewing transformations are completed after the subjects of this chapter that pixels will enter the discussion camera model the common transformation process for producing the desired view is analogous to taking a photograph with a camera as shown in figure the steps with a camera or a computer might be the following figure steps in configuring and positioning the viewing frustum move your camera to the location you want to shoot from and point the camera the desired direction viewing transformation move the subject to be photographed into the desired location in the scene modeling transformation choose a camera lens or adjust the zoom projection transformation take the picture apply the transformations viewing stretch or shrink the resulting image to the desired picture size viewport transformation for graphics this also includes stretching or shrinking the depth depth range scaling this is not to be confused with step which selected how much of the scene to capture not how much to stretch the result notice that steps and can be considered doing the same thing but in opposite directions you can leave the camera where you found it and bring the subject in front of it or leave the subject where it is and move the camera toward the subject moving the camera to the left is the same as moving the subject to the right twisting the camera clockwise is the same as twisting the subject counterclockwise it is really up to you which movements you perform as part of step with the remainder belonging to step because of this these two steps are normally lumped together as the model view transform it will though always consist of some sequence of movements translations rotations and scalings the defining characteristic of this combination is in making a single unified space for all the objects assembled into one scene to view or eye space in opengl you are responsible for doing steps through above in your shaders that is you ll be required to hand opengl coordinates with the model view and projective transformations already done you are also responsible for telling opengl how to do the viewport transformation for step but the fixed rendering pipeline will do that transformation for you as described in opengl transformations on page figure summarizes the coordinate systems required by opengl for the full process so far we have discussed the second box user transforms but are showing the rest to set the context for the whole viewing stack finishing with how you specify your viewport and depth range to opengl the final coordinates handed to opengl for clipping and rasterization are normalized homogeneous coordinates that is the coordinates to be drawn will be in the range until opengl scales them to fit the viewport chapter viewing transformations clipping and feedback your starting coordinates you need these in order to translate and project opengl required input scaled by opengl to your viewport and depth range object units could be meters inches etc same units units normalized such that divide by w leaves visible points between to range of to for x and y and to for z x y are window coordinates z is depth coordinate x y units are in pixels with fractions z is in range of to or depth range rasterization figure coordinate systems required by opengl the coordinate systems are the boxes on the left the central boxes transform from one coordinate system to the next units are described to the right it will be useful to name additional coordinate systems lying within the view model and projection transforms these are no longer part of the opengl model but still highly useful and conventional when using shaders to assemble a scene or calculate lighting figure shows an expansion of the user transforms box from figure in particular most lighting calculations done in shaders will be done in eye space examples making full use of eye space are provided in chapter light and shadow viewing figure user coordinate systems unseen by opengl these coordinate systems while not used by opengl are still vital for lighting and other shader operations viewing frustum step in our camera analogy chose a lens or zoom amount this selects how narrow or wide of a rectangular cone through the scene the camera will capture only geometry falling within this cone will be in the final picture at the same time step will also produce the information needed in the homogeneous fourth coordinate w to later create the foreshortening effect of perspective chapter viewing transformations clipping and feedback figure a view frustum opengl will additionally exclude geometry that is too close or too far away that is those in front of a near plane or those behind a far plane there is no counterpart to this in the camera analogy other than cleaning foreign objects from inside your lens but is helpful in a variety of ways most importantly objects approaching the cone apex appear infinitely large which causes problems especially if they should reach the apex at the other end of this spectrum objects too far away to be drawn in the scene are best excluded for performance reasons and some depth precision reasons as well if depth must span too large a distance thus we have two additional planes intersecting the four planes of the rectangular viewing cone as shown in figure these six planes define a frustum shaped viewing volume frustum clipping any primitive falling outside the four planes forming the rectangular viewing cone will not get drawn culled as it would fall outside our rectangular display further anything in front of the near plane or behind the far plane will also be culled what about a primitive that spans both sides of one of these planes opengl will clip such primitives that is it will compute the intersection of their geometry with the plane and form new geometry for just the shape that falls within the frustum viewing because opengl has to perform this clipping to draw correctly the application must tell opengl where this frustum is this is part of step of the camera analogy where the shader must apply the transformations but opengl must know about it for clipping there are ways shaders can clip against additional user planes discussed later but the six frustum planes are an intrinsic part of opengl orthographic viewing model sometimes a perspective view is not desired and an orthographic view is used instead this type of projection is used by applications for architectural blueprints and computer aided design where it crucial to maintain the actual sizes of objects and the angles between them as they re projected this could be done simply by ignoring one of the x y or z coordinates letting the other two coordinates give two dimensional locations you would do that of course after orienting the objects and the scene with model view transformations as with the camera model but in the end you will still need to locate and scale the resulting model for display in normalized device coordinates the transformation for this is the last one given in the next section user transformations initial model coordinates stages that transform coordinates to opengl required form transform feedback coordinates opengl can map to a window clip coordinates rasterizer figure pipeline subset for user shader part of transforming coordinates chapter viewing transformations clipping and feedback the stages of the rendering pipeline that transform three dimensional coordinates for opengl viewing are shown in figure essentially they are the programmable stages appearing before rasterization because these stages are programmable you have a lot of flexibility in the initial form of your coordinates and in how you transform them however you are constrained to end with the form the subsequent fixed nonprogrammable stages need that is we ll need to make homogeneous coordinates that are ready for perspective division also referred to as clip coordinates what that means and how to do it are the subjects of the following sections each of the viewing model steps above was called out as a transformation they are all linear transformations that can be accomplished through matrix multiplication on homogeneous coordinates the upcoming matrix multiplication and homogenous coordinate sections give refreshers on these topics understanding them is the key to truly understanding how opengl transformations work in a shader transforming a vertex by a matrix looks like this version core uniform transform stays the same for many vertices primitive granularity in vertex per vertex data sent each time this shader is run void main transform vertex linear transformations are composable so just because our camera analogy needed four transformation steps does not mean we have to transform our data four times rather all those transformations can be composed into a single transformation if we want to transform our model first by transformation matrix a followed by transformation matrix b we will see we can do so with transformation matrix c where c ba because we are showing examples of matrix multiplication with the vertex on the right and the matrix on the left composing transforms show up in reverse order b is applied to the result of applying a to a vertex the details behind this are explained in the upcoming refresher so the good news is we can collapse any number of linear transformations into a single matrix multiply allowing the freedom to think in terms of whatever steps are most convenient user transformations matrix multiply refresher for our use matrices and matrix multiplication are nothing more than a convenient mechanism for expressing linear transformations which in turn are a useful way to do the coordinate manipulations needed for displaying models the vital matrix mechanism is explained here while interesting uses for it will come up in numerous places in subsequent discussions first a definition a matrix takes a four component vector to another four component vector through multiplication by the following rule a b c d x ax by cz dw i j k l z ix jy kz lw m n o p w now some observations mx ny oz pw each component of the new vector is a linear function of all the components of the old vector hence the need for values in the matrix the multiplication always takes the vector to this is characteristic of linear transformations and shows that if this was a matrix times a three component vector why translation moving can t be done with a matrix multiply we ll see how translating a three component vector becomes possible with a matrix and homogeneous coordinates in our viewing models we will want to take a vector through multiple transformations here expressed as matrix multiplications by matrices a and then b vi av vii bvi b av ba v and we ll want to do this efficiently by finding a matrix c such that vii cv where c ba chapter viewing transformations clipping and feedback being able to compose the b transform and the a transform into a single transform c is a benefit we get by sticking to linear transformations the following definition of matrix multiplication makes all of this work out b14 a14 c14 b34 a34 c34 b44 where a44 c44 cij bi4a4j that is matrix multiplication is noncommutative generally speaking when multiplying matrices and a and b ab ba and generally when multiplying matrix a and vector v av va so care is needed to multiply in the correct order matrix multiplication is fortunately associative c ba cb a cba that useful as accumulated matrix multiplies on a vector can be re associated c b av cba v this is a key result we will take advantage of to improve performance homogeneous coordinates the geometry we want to transform is innately three dimensional however we will gain two key advantages by moving from user transformations three component cartesian coordinates to four component homogeneous coordinates these are the ability to apply perspective and the ability to translate move the model using only a linear transform that is we will be able to get all the rotations translations scaling and projective transformations we need by doing matrix multiplication if we first move to a four coordinate system more accurately the projective transformation is a key step in creating perspective and it is the step we must perform in our shaders the final step is performed by the system when it eliminates this new fourth coordinate if you want to understand this and homogeneous coordinates more deeply read the next section if you just want to go on some faith and grab matrices that will get the job done you can skip to the next section advanced what are homogeneous coordinates three dimensional data can be scaled and rotated with linear transformations of three component vectors by multiplying by matrices unfortunately translating moving sliding over three dimensional cartesian coordinates cannot be done by multiplying with a matrix it requires an extra vector addition to move the point somewhere else this is a called an affine transformation which is not a linear transformation recall that any linear transformation maps to including that addition means the loss of the benefits of linear transformations like the ability to compose multiple transformations into a single transformation so we want to find a way to translate with a linear transformation fortunately by embedding our data in a four coordinate space affine transformations turn back into a simple linear transform meaning you can move your model laterally using only multiplication by a matrix for example to move data by in the y direction assuming a fourth vector coordinate of x x z z at the same time we acquire the extra component needed to do perspective chapter viewing transformations clipping and feedback a homogeneous coordinate has one extra component and does not change the point it represents when all its components are scaled by the same amount for example all these coordinates represent the same point in this way homogeneous coordinates act as directions instead of locations scaling a direction leaves it pointing in the same direction this is shown in figure standing at the homogeneous points and others along that line appear in the same place when projected onto the space they all become the point many two component homogeneous y coordinates for the number figure one dimensional homogeneous space shows how to embed the space into two dimensions at the location y to get homogeneous coordinates skewing is a linear transformation skewing figure can translate the embedded space as show in figure while preserving the location of in the space all linear transforms keep fixed user transformations figure translating by skewing the desire is to translate points in the space with a linear transform this is impossible within the space as the point needs to move something linear transformations cannot do however the skewing transformation is linear and accomplishes the goal of translating the space if the last component of an homogeneous coordinate is it implies a point at infinity the space has only two such points at infinity one in the positive direction and one in the negative direction however the space embedded in a four coordinate homogeneous space has a point at infinity for any direction you can point these points can model the perspective point where two parallel lines e g sides of a building or railroad tracks would appear to meet the perspective effects we care about though will become visible without needing to specifically think about this we will move to homogeneous coordinates by adding a fourth w component of and later go back to cartesian coordinates by dividing all components by the fourth component and dropping the fourth component divide by w drop w perspective transforms modify w components to values other than making w larger can make coordinates appear further away when it time to display geometry opengl will transform homogeneous coordinates chapter viewing transformations clipping and feedback back to the three dimensional cartesian coordinates by dividing their first three components by the last component this will make the objects farther away now having a larger w have smaller cartesian coordinates hence getting drawn on a smaller scale a w of implies x y coordinates at infinity the object got so close to the viewpoint that its perspective view got infinitely large this can lead to undefined results there is nothing fundamentally wrong with a negative w the following coordinates represent the same point but negative w can stir up trouble in some parts of the graphics pipeline especially if it ever gets interpolated toward a positive w as that can make it land on or very near the simplest way to avoid problems is to keep your w components positive linear transformations and matrices we start our task of mapping into device coordinates by adding a fourth component to our three dimensional cartesian coordinates with a value of to make homogeneous coordinates these coordinates are then ready to be multiplied by one or more matrices that rotate scale translate and apply perspective examples of how to use each of these transforms are given below the summary is that each of these transformations can be made through multiplication by a matrix and a series of such transformations can be composed into a single matrix once that can then be used on multiple vertices translation translating an object takes advantage of the fourth component we just added to our model coordinates and of the fourth column of a transformation matrix we want a matrix t to multiply all our object vertices v by to get translated vertices vi vi tv each component can be translated by a different amount by putting those amounts in the fourth column of t for example to translate by in the positive x direction and not at all in the y or z directions user transformations and multiplying by a vector v x y z gives x x y y z z this is demonstrated in figure y x figure translating an object in the x direction of course you ll want such matrix operations encapsulated there are numerous utilities available for this and one is included in the accompanying vmath h we already used it in chapter drawing with opengl to create a translation matrix using this utility call the following listing shows a use of this application c code include vmath h make a transformation matrix that translates coordinates by chapter viewing transformations clipping and feedback vmath translationmatrix vmath translate set this matrix into the current program translationmatrix after going through the next type of transformation we ll show a code example for combining transformations with this utility scaling grow or shrink an object as in figure by putting the desired scaling factor on the first three diagonal components of the matrix making a scaling matrix s which applied to all vertices v in an object would change its size x figure scaling an object to three times its size note that if the object is off center this also moves its center three times further from user transformations the following example makes geometry times larger x z note that nonisomorphic scaling is easily done as the scaling is per component but it would be rare to do so when setting up your view and model transforms if you want to stretch results vertically or horizontally do that at the end with the viewport transformation doing it too early would make shapes change when they rotate note that when scaling we didn t scale the w component as that would result in no net change to the point represented by the homogeneous coordinate since in the end all components are divided by w if the object being scaled is not centered at the simple matrix above will also move it further or closer to by the scaling amount usually it is easier to understand what happens when scaling if you first center the object around then scaling leaves it in the same place while changing its size if you want to change the size of an off center object without moving it first translate its center to then scale it and finally translate it back this is shown in figure chapter viewing transformations clipping and feedback y y x x figure scaling an object in place scale in place by moving to scaling and then moving it back this would use three matrices t s and t for translate to scale and translate back respectively when each vertex v of the object is multiplied by each of these matrices in turn the final effect is that the object would change size in place yielding a new set of vertices vi vi t s tv or vi t v which allows for pre multiplication of the three matrices into a single matrix m t vi mv m now does the complete job of scaling an off center object to create a scaling transformation with the included utility you can use user transformations the resulting matrix can be directly multiplied by another such transformation matrix to compose them into a single matrix that performs both transformations application c code include vmath h compose translation and scaling transforms vmath translatematrix vmath translate vmath scalematrix vmath scale vmath scaletranslatematrix scalematrix translatematrix any sequence of transformations can be combined into a single matrix this way rotation rotating an object follows a similar scheme we want a matrix r that when applied to all vertices v in an object will rotate it the following example shown in figure rotates degrees counterclockwise around the z axis figure shows how to rotate an object without moving its center instead of also revolving it around the z axis chapter viewing transformations clipping and feedback y x figure rotation rotating an object degrees in the xy plane around the z axis note if the object is off center it also revolves the object around the point y y x x figure rotating in place rotating an object in place by moving it to rotating and then moving it back user transformations cos sin cos x sin y cos sin x z z when rotating around the z axis above the vertices in the object keep their z values the same rotating in the xy plane to rotate instead around the x axis by an amount θ rx sin θ cos θ and around the y axis cos θ sin θ sin θ cos θ in all cases the rotation is in the direction of the first axis toward the second axis that is from the row with the cos sin pattern to the row with the sin cos pattern for the positive axes corresponding to these rows if the object being rotated is not centered at the matrices above will also rotate the whole object around changing its location again as with scaling it ll be easier to first center the object around so again translate it to transform it and then translate it back this could use three matrices t r and t to translate to rotate and translate back vi t r tv or vi t v which again allows for the pre multiplication into a single matrix chapter viewing transformations clipping and feedback to create a rotation transformation with the included utility you can use perspective projection this one is a bit tougher we now assume viewing and modeling transformations are completed with larger z values meaning objects are further away we will consider the following two cases symmetric centered frustum where the z axis is centered in the cone asymmetric frustum like seeing what through a window when you looking near it but not toward its middle for all the viewpoint is now at looking generally toward the positive z direction first however let consider an over simplified hypothetical perspective projection x x z z note the last matrix row replaces the w fourth coordinate with the z coordinate this will make objects with a larger z further away appear smaller when the division by w occurs creating a perspective effect however this particular method has some shortcomings for one all z values will end up at losing information about depth we also didn t have much control over the cone we are projecting and the rectangle we are projecting onto finally we didn t scale the result to the range expected by the viewport transform the remaining examples take all this into account so we consider now a fuller example for opengl using a symmetric centered frustum we refer back to our view frustum shown again with the size of the near plane labeled in figure user transformations figure frustum projection frustum with the near plane and half its width and height labeled we want to project points in the frustum onto the near plane directed along straight lines going toward any straight line emanating from keeps the ratio if z to x the same for all its points and similarly for the ratio of z to y thus the xproj yproj value of the projection on the near plane will keep the ratios of znear xproj and znear yproj we know z x z y there is an upcoming division by depth to eliminate homogeneous coordinates so solving for xproj while still in the homogeneous space simply gives xproj x znear similarly yproj y znear if we then include a divide by the size of the near plane to scale the near plane to the range of we end up with the requisite first two diagonal elements shown in the projection transformation matrix znear width znear height zfar znear this could also be computed from the angle of the viewing cone if so desired finally we consider the second perspective projection case the asymmetric frustum this is the fully general frustum when the near plane might not be centered on the z axis the z axis could even be completely chapter viewing transformations clipping and feedback outside it as mentioned earlier when looking at an interior wall next to a window your direction of view is the positive z axis which is not going through the window you see the window off to the side with an asymmetric perspective view of what outside the window in this case points on the near plane are already in the correct location but those further away need to be adjusted for the fact that the projection in the near plane is off center you can see this adjustment in the third column of the matrix which moves the points an amount based on how off center the near plane projection is scaled by how far away the points are because this column multiplies by z znear width left right width znear height top bottom height zfar znear all the above steps rotate scale translate project and possibly others will make matrices that can be multiplied together into a single matrix now with one multiplication by this new matrix we can simultaneously scale translate rotate and apply the perspective projection to create a perspective projection transformation with the included utility there are a couple of choices you can have full control as above using a frustum call or you can more casually and intuitively create one with the lookat call user transformations the resulting vectors still having four coordinates are the homogeneous coordinates expected by the opengl pipeline the final step in projecting the perspective view onto the screen is to divide the x y z coordinates in vi by the w coordinate in vi for every vertex however this is done internally by opengl it is not something you do in your shaders orthographic projection with an orthographic projection the viewing volume is a rectangular parallelpiped or more informally a box see figure unlike perspective projection the size of the viewing volume doesn t change from one end to the other so distance from the camera doesn t affect how large an object appears figure orthographic projection starts with straightforward projection of the parallelpiped onto the front plane x y and z will need to be scaled to fit into and respectively this will be done by dividing by the sizes of the width height and depth in the model this is done after all the translation scaling and rotation is done to look in the positive z direction to see the model to view with no perspective we will keep the w as it is accomplished by making the bottom row of the transformation matrix we will still scale z to lie within so z buffering can hide obscured objects but neither z nor w will have any effect on the screen location that leaves scaling x from the width of the model to and similarly for y for a symmetric volume chapter viewing transformations clipping and feedback positive z going down the middle of the parallelpiped this can be done with the following matrix width height zfar znear for the case of the positive z not going down the middle of the view but still looking parallel to the z axis to see the model the matrix is just slightly more complex we use the diagonal to scale and the fourth column to center right left right left right left top bottom top bottom top bottom near near to create an orthographic projection transformation with the included utility you can use the following transforming normals in addition to transforming vertices we will have need to transform surface normals that is vectors that point in the direction perpendicular to a surface at some point in perhaps one of the most confusing twists of terminology normals are often required to be normalized that is of length however the normal meaning perpendicular and the normal in normalize are completely unrelated and we will come upon needs for normalized normals when computing lighting typically when computing lighting a vertex will have a normal associated with it so the lighting calculation knows what direction the surface reflects light shaders doing these calculations appear in chapter light and shadow here though we will discuss the fundamantals of transforming them by taking them through rotations scaling and so on along with the vertices in a model user transformations normal vectors are typically only three component vectors not using homogeneous coordinates for one thing translating a surface does not change its normal so normals don t care about translation removing one of the reasons we used homogeneous coordinates since normals are mostly used for lighting which we complete in a pre perspective space we remove the other reason we use homogeneous coordinates projection perhaps counterintuitively normal vectors aren t transformed in the same way as vertices or position vectors are imagine a surface at an angle that gets stretched by a transformation stretching makes the angle of the surface shallower which changes the perpendicular direction in the opposite way than applying the same stretching to the normal would this would happen for example if you stretch a sphere to make an ellipse we need to come up with a different transformation matrix to transform normals than the one we used for vertices so how do we transform normals to start let m be the matrix that has all the rotations and scaling needed to transform your object from model coordinates to eye coordinates before transforming for perspective this would be the upper block in your transformation matrix before compounding translation or projection transformations into it then to transform normals use the following equation ni m that is take the transpose of the inverse of m and use that to transform your normals if all you did was rotation and isometric nonshape changing scaling you could transform directions with just m they d be scaled by a different amount but will no doubt have a normalize call in their future that will even that out opengl matrices while shaders know how to multiply matrices the api in the opengl core profile does not manipulate matrices beyond setting them possibly trans posed into uniform and per vertex data to be used by your shaders it is up to you to build up the matrices you want to use in your shader which you can do with the included helper routines as described in the previous section you will want to be multiplying matrices in your application before sending them to your shaders for a performance benefit for example let say you need matrices to do the following transformations chapter viewing transformations clipping and feedback move the camera to the right view translate and rotate move the model into view translate rotate and scale apply perspective projection that a total of six matrices you can use a vertex shader to do all this math as in example example multiplying multiple matrices in a vertex shader version core uniform viewt viewr modelt modelr models project in vertex void main project models modelr modelt viewr viewt vertex however that a lot of arithmetic to do for each vertex fortunately the intermediate results for many vertices will be the same each time to the extent consecutive transforms matrices are staying the same for a large number of vertices you ll want to instead pre compute their composition product in your application and send the single resulting matrix to your shader application c code include vmath h vmath viewt vmath rotate vmath viewr vmath translate vmath view viewr viewt vmath models vmath scale vmath modelr vmath rotate vmath modelt vmath translate vmath model models modelr modelt vmath project vmath frustum vmath modelviewproject project model view an intermediate situation might be to have a single view transformation and a single perspective projection but multiple model transformations you might do this if you reuse the same model to make many instances of an object in the same view user transformations version core uniform view model project in vertex void main view model project vertex in this situation the application would change the model matrix more frequently than the others this will be economical if enough vertices are drawn per change of the matrix model if only a few vertices are drawn per instance it will be faster to send the model matrix as a vertex attribute version core uniform view project in vertex in model a transform sent per vertex void main view model project vertex another alternative for creating multiple instances is to construct the model transformation within the vertex shader based on the built in variable this was described in detail in chapter drawing with opengl of course when you can draw a large number of vertices all with the same cumulative transformation you ll want to do only one multiply in your shader version core uniform modelviewproject in vertex void main modelviewproject vertex matrix rows and columns in opengl the notation used in this book corresponds to the broadly used traditional matrix notation we stay true to this notation regardless of how data is set into a matrix a column will always mean a vertical slice of a matrix when written in this traditional notation chapter viewing transformations clipping and feedback beyond notation matrices have semantics for setting and accessing parts of a matrix and these semantics are always column oriented in a shader using array syntax on a matrix yields a vector with values coming from a column of the matrix m columns rows v m v is initialized to the second column of m note neither the notation we use nor these column oriented semantics are to be confused with column major order and row major order which refer strictly to memory layout of the data behind a matrix the memory layout has nothing to do with our notation in this book and nothing to do with the language semantics of glsl you will probably not know whether internally a matrix is stored in column major or row major order caring about column major or row major memory order will only come up when you are in fact laying out the memory backing a glsl matrix yourself this is done when setting matrices in a uniform block as was shown in chapter shader fundamentals when discussing uniform blocks you use layout qualifiers and to control how glsl will load the matrix from this memory since opengl is not creating or interpreting your matrices you can treat them as you wish if you want to transform a vertex by matrix multiplication with the matrix on the right version core uniform m in vertex void main vertex m nontraditional order of multiplication then as expected x will be formed by the dot product of vertex and the first column of matrix m and so on for y z and w components transformed by the second third and fourth columns however we ll stick to the tradition of keeping the matrix on the left and the vertices on the right note glsl vectors automatically adapt to being either row vectors or column vectors depending on whether they are on the left side or right side of a matrix multiply respectively in this way they are different than a one column or one row matrix user transformations opengl transformations to tell opengl where you want the near and far planes use the gldepthrange commands viewport to tell opengl where to display the rectangular viewing cone use the underlying windowing system of your platform not opengl is responsible for opening a window on the screen however by default the viewport is set to the entire pixel rectangle of the window that opened you use glviewport to choose a smaller drawing region for example you can subdivide the window to create a split screen effect for multiple views in the same window multiple viewports you will sometimes want to render a scene through multiple viewports opengl has commands to support doing this and the geometry shading chapter viewing transformations clipping and feedback stage can select which viewport subsequent rendering will target more details and an example are given in multiple viewports and layered rendering on page advanced z precision one bizarre effect of these transformations is z fighting the hardware floating point numbers used to do the computation have limited precision hence depth coordinates that are mathematically distinct end up having the same or even reversed actual floating point z values this in turn causes incorrectly hidden objects in the depth buffer the effect varies per pixel and can cause disturbing flickering intersections of nearby objects precision of z is made even worse with perspective division which is applied to the depth coordinate along with all the other coordinates as the transformed depth coordinate moves farther away from the near clipping plane its location becomes increasingly less precise as shown in figure y z pre divide z values that will be distinct post divide x figure z precision an exaggerated showing of adjacent distinctly representable depths assuming an upcoming perspective division even without perspective division there is a finite granularity to floating point numbers but the divide makes it worse and nonlinear resulting in more severe problems at greater depths the bottom line is it is possible to ask for too much of too small a range of z values to avoid this take care to keep the far plane as close to the near plane as possible and don t compress the z values into a narrower range than necessary opengl transformations advanced user clipping opengl automatically clips geometry against the near and far planes as well as the viewport user clipping refers to adding additional clip planes at arbitrary orientations intersecting your geometry such that the display sees the geometry on one side of the plane but not on the other side you might use one for example to show a cut away of a complex object opengl user clipping is a joint effort between opengl and a special built in vertex shader array which you are responsible for setting this variable lets you control where vertices are in relation to a plane normal interpolation then assigns distances to the fragments between the vertices example shows a straight forward use of this built in variable example simple use of version core uniform plane a b c and d for ax by cz d in vertex w float declare use of clip plane void main evaluate plane equation dot vertex plane the convention is that a distance of means the vertex is on the plane a positive distance means the vertex is inside the keep it side of the clip plane and a negative distance means the point is outside the cull it side of the clip plane the clip distances will be linearly interpolated across the primitive and opengl will cull fragments whose interpolated distance is less than each element of the array is set up to represent one plane there are a limited number of clip planes likely around eight or more the built in is declared with no size you either need to redeclare it with a specific size or only access it with compile time constants all shaders in all stages that declare or use should make the array the same size the constant lets your shader know the maximum array size on the current opengl implementation this size needs to include all the clip planes that are enabled via the opengl api if the size does not include all enabled planes chapter viewing transformations clipping and feedback results are undefined to enable opengl clipping of the clip plane written to in example enable the following enumerant in your application glenable there are also other enumerates like these enumerants are organized sequentially so that is equal to i this allows programmatic selection of which and how many user clip planes to use your shaders should write to all the enabled planes or you ll end up with odd system clipping behavior the built in is also available in a fragment shader allowing nonclipped fragments to read their interpolated distances from each clip plane transform feedback transform feedback can be considered a stage of the opengl pipeline that sits after all of the vertex processing stages and directly before primitive assembly and rasterization transform feedback captures vertices as they are assembled into primitives points lines or triangles and allows some or all of their attributes to be recorded into buffer objects in fact the minimal opengl pipeline that produces useful work is a vertex shader with transform feedback enabled no fragment shader is necessary each time a vertex passes through primitive assembly those attributes that have been marked for capture are recorded into one or more buffer objects those buffer objects can then be read back by the application or their contents used in subsequent rendering passes by opengl transform feedback objects the state required to represent transform feedback is encapsulated into a transform feedback object this state includes the buffer objects that will be used for recording the captured vertex data counters indicating how full each buffer object is and state indicating whether transform feedback is currently active a transform feedback object is created by reserving a transform feedback object name and then binding it to the transform feedback object binding point on the current context to reserve transform feedback object names call to be more exact transform feedback is tightly integrated into the primitive assembly process as whole primitives are captured into buffer objects this is seen as buffers run out of space and partial primitives are discarded for this to occur some knowledge of the current primitive type is required in the transform feedback stage transform feedback the parameter n specifies how many transform feedback object names are to be reserved and ids specifies the address of an array where the reserved names will be placed if you want only one name you can set n to one and pass the address of a gluint variable in ids once you have reserved a name for a transform feedback object the object doesn t actually exist until the first time it is bound at which point it is created to bind a transform feedback object to the context call this binds the transform feedback object named id to the binding on the context indicated by target which in this case must be if this is the first time this name has been bound a new transform feedback object is created and initialized with the default state otherwise the existing state of that object again becomes current to determine whether a particular value is the name of a transform feedback object you can call glistransformfeedback whose prototype is as follows once a transform feedback object is bound all commands affecting transform feedback state affect that transform feedback object it not necessary to have a transform feedback object bound in order to use transform feedback functionality as there is a default transform feedback object the default transform feedback object assumes the id zero and so passing zero as the id parameter to glbindtransformfeedback returns chapter viewing transformations clipping and feedback the context to use the default transform feedback object unbinding any previously bound transform feedback object in the process however as more complex uses of transform feedback are introduced it becomes convenient to encapsulate the state of transform feedback into transform feedback objects therefore it good practice to create and bind a transform feedback object even if you intend to use only one once a transform feedback object is no longer needed it should be deleted by calling this function deletes the n transform feedback objects whose names are stored in the array whose address is passed in ids deletion of the object is deferred until it is no longer in use that is if the transform feedback object is active when gldeletetransformfeedbacks is called it is not deleted until transform feedback is ended transform feedback buffers transform feedback objects are primarily responsible for managing the state representing capture of vertices into buffer objects this state includes which buffers are bound to the transform feedback buffer binding points multiple buffers can be bound simultaneously for transform feedback and subsections of buffer objects can also be bound it is even possible to bind different subsections of the same buffer object to different transform feedback buffer binding points simultaneously to bind an entire buffer object to one of the transform feedback buffer binding points call transform feedback the target parameter should be set to and index should be set to the index of the transform feedback buffer binding point in the currently bound transform feedback object the name of the buffer to bind is passed in buffer the total number of binding points is an implementation dependent constant that can be discovered by querying the value of and index must be less than this value all opengl implementations must support at least transform feedback buffer binding points it also possible to bind a range of a buffer object to one of the transform feedback buffer binding points by calling again target should be index should be between zero and one less than the value of and buffer contains the name of the buffer object to bind the offset and size parameters define which section of the buffer object to bind this functionality can be used to bind different ranges of the same buffer object to different transform feedback buffer binding points care should be taken that the ranges do not overlap attempting to perform transform feedback into multiple overlapping sections of the same buffer object will result in undefined behavior possibly including data corruption or worse in addition to binding buffers or sections of buffers to the indexed binding points glbindbufferbase and glbindbufferrange also bind the buffer object to the generic buffer binding point indicated by target this is useful because it allows other commands that operate on buffer objects such as glbufferdata glbuffersubdata and glgetbuffersubdata to operate on the buffer object most recently bound to one of the indexed buffer binding points note though that neither glbindbufferbase nor glbindbufferrange create buffer objects unlike glbindbuffer and so you can t use a glbindbufferbase glbufferdata sequence to create and allocate space for a transform feedback buffer in order to allocate a transform feedback buffer code such as in that shown in example should be used chapter viewing transformations clipping and feedback example example initialization of a transform feedback buffer generate the name of a buffer object gluint buffer glgenbuffers buffer bind it to the binding to create it glbindbuffer buffer call glbufferdata to allocate of space glbufferdata target mb null no initial data usage now we can bind it to indexed buffer binding points glbindbufferrange target index buffer buffer name start of range first half of buffer glbindbufferrange target index buffer same buffer start half way second half notice how in example the newly reserved buffer object name is first bound to the target and then glbufferdata is called to allocate space the data parameter to glbufferdata is set to null to indicate that we wish to simply allocate space but do not wish to provide initial data for the buffer in this case the buffer contents will initially be undefined also is used as the usage parameter this provides a hint to the opengl implementation of the intended use for the buffer object the dynamic part of the token implies that the data in the buffer will be changed often and the copy part of the token indicates that we will be using opengl normally a gpu to produce data to be written into the buffer this together with the fact that the buffer is bound to the binding point should give the implementation enough information to intelligently allocate memory for the buffer object in an optimal manner for it to be used for transform feedback once the buffer has been bound to the generic target and space has been allocated for it sections of it are then bound to the indexed transform feedback buffer binding points by calling glbindbufferrange twice once to bind transform feedback the first half of the buffer to the first binding point and again to bind the second half of the buffer to the second binding point this demonstrates why the buffer needs to be created and allocated first before using it with glbindbufferrange glbindbufferrange takes an offset size parameters describing a range of the buffer object that must lie within the buffer object this cannot be determined if the object does not yet exist conﬁguring transform feedback varyings while the buffer bindings used for transform feedback are associated with a transform feedback object the configuration of which outputs of the vertex or geometry shader are to be recorded into those buffers is stored in the active program object to specify which varyings will be recorded during transform feedback call in this function program specifies the program object that will be used for transform feedback varyings contains an array of strings that represent the names of varying variables that are outputs of the fragment or geometry shader that are to be captured by transform feedback count is the number of strings in varyings buffermode is a token indicating how the captured varyings should be allocated to transform feedback buffers if buffermode is set to all of the varyings will be recorded one after another into the buffer object bound to the first transform feedback buffer binding point on the current transform feedback object if buffermode is each varying will be captured into its own buffer object chapter viewing transformations clipping and feedback an example of the use of gltransformfeedbackvaryings is shown in example below example example specification of transform feedback varyings create an array containing the names of varyings to record static const char const vars foo bar baz call gltransformfeedbackvaryings gltransformfeedbackvaryings prog sizeof vars sizeof vars varyings now the program object is set up to record varyings squashed together in the same buffer object alternatively we could call gltransformfeedbackvaryings prog sizeof vars sizeof vars varyings this sets up the varyings to be recorded into separate buffers now this is important link the program object even if it already been linked before gllinkprogram prog notice in example that there is a call to gllinkprogram directly after the call to gltransformfeedbackvaryings this is because the selection of varyings specified in the call to gltransformfeedbackvaryings does not take effect until the next time the program object is linked if the program has previously been linked and is then used without being re linked no errors will occur but nothing will be captured during transform feedback after the code in example has been executed whenever prog is in use while transform feedback is active the values written to foo bar and baz will be recorded into the transform feedback buffers bound to the current transform feedback object in the case where the buffermode parameter is set to the values of foo bar and baz will be tightly packed into the buffer bound to the first transform feedback buffer binding point as shown in figure calling gltransformfeedbackvaryings after a program object has already been linked and then not linking it again is a common error made even by experienced opengl programmers transform feedback vertex vertex figure transform feedback varyings packed in a single buffer however if buffermode is then each of foo bar and baz will be packed tightly into its own buffer object as shown in figure vertex vertex vertex vertex buffer vertex vertex vertex vertex buffer vertex vertex vertex vertex buffer figure transform feedback varyings packed in separate buffers chapter viewing transformations clipping and feedback in both cases the attributes will be tightly packed together the amount of space in the buffer object that each varying consumes is determined by its type in the vertex shader that is if foo is declared as a in the vertex shader it will consume exactly three floats in the buffer object in the case where buffermode is the value of bar will be written immediately after the value of foo in the case where buffermode is the values of foo will be tightly packed into one buffer with no gaps between them as will the values of bar and baz this seems rather rigid there are cases where you may wish to align the data written into the transform feedback buffer differently from default leaving unwritten gaps in the buffer there may also be cases where you would want to record more than one variable into one buffer but record other variables into another for example you may wish to record foo and bar into one buffer while recording baz into another in order to increase the flexibility of transform feedback varying setup and allow this kind of usage there are some special variable names reserved by opengl that signal to the transform feedback subsystem that you wish to leave gaps in the output buffer or to move between buffers these are and when any of the variants is encountered opengl will leave a gap for the number of components specified or in the transform feedback buffer these variable names can only be used when buffermode is an example of using this is shown in example example leaving gaps in a transform feedback buffer declare the transform feedback varying names static const char const vars foo bar baz set the varyings gltransformfeedbackvaryings prog sizeof vars sizeof vars varyings remember to link the program object gllinkprogram prog transform feedback when the other special variable name name is encountered opengl will start allocating varyings into the buffer bound to the next transform feedback buffer this allows multiple varyings to be recorded into a single buffer object additionally if is encountered when buffermode is or if two or more instances of are encountered in a row in it allows a whole binding point to be skipped and nothing recorded into the buffer bound there an example of is shown in example example assigning transform feedback outputs to different buffers declare the transform feedback varying names static const char const vars foo bar variables to record into buffer move to binding point baz variable to record into buffer set the varyings gltransformfeedbackvaryings prog sizeof vars sizeof vars varyings remember to link the program object gllinkprogram prog the special variables names and can be combined to allow very flexible vertex layouts to be created if it is necessary to skip over more than four components multiple instances of may be used back to back care should be taken with aggressive use of though because skipped components still contribute toward the count of the count of the number of components captured during transform feedback even though no data is actually written this may cause a reduction in performance or even a failure to link a program if there is a lot of unchanged static data in a buffer it may be preferable to separate the data into static and dynamic parts and leave the static data in its own buffer object allowing the dynamic data to be more tightly packed chapter viewing transformations clipping and feedback finally example shows an albeit rather contrived example of the combined use of and and figure shows how the data ends up laid out in the transform feedback buffers example assigning transform feedback outputs to different buffers declare the transform feedback varying names static const char const vars record foo a gap of float bar and then two floats foo bar move to binding point leave a gap of floats then record baz then leave another gap of floats baz move to binding point move directly to binding point without directing anything to binding point record iron and copper with a component gap between them iron copper set the varyings gltransformfeedbackvaryings prog sizeof vars sizeof vars varyings remember to link the program object gllinkprogram prog as you can see in example can come between varyings or at the start or end of the list of varyings to record into a single buffer putting a variant first in the list of varyings to capture will result in opengl leaving a gap at the front of the buffer before it records data and then a gap between each sequence of varyings also multiple variables can come back to back causing a buffer binding point to be entirely passed over and nothing recorded into that buffer the resulting output layout is shown in figure transform feedback vertex vertex buffer buffer buffer figure transform feedback varyings packed into multiple buffers starting and stopping transform feedback transform feedback can be started and stopped and even paused as might be expected starting transform feedback when it is not paused causes it to start recording into the bound transform feedback buffers from the begin ning however starting transform feedback when it is already paused causes it to continue recording from wherever it left off this is useful to allow mul tiple components of a scene to be recorded into transform feedback buffers with other components rendered in between that are not to be recorded to start transform feedback call glbegintransformfeedback chapter viewing transformations clipping and feedback the glbegintransformfeedback function starts transform feedback on the currently bound transform feedback object the primitivemode parameter must be or and must match the primitive type expected to arrive at primitive assembly note that it does not need to match the primitive mode used in subsequent draw commands if tessellation or a geometry shader is active because those stages might change the primitive type mid pipeline that will be covered in chapters and for the moment just set the primitivemode to match the primitive type you plan to draw with table shows the allowed combinations of primitivemode and draw command modes table drawing modes allowed during transform feedback transform feedback primitivemode allowed drawing types gl_lines gl_line_strip_adjacency once transform feedback is started it is considered to be active it may be paused by calling glpausetransformfeedback when transform feedback is paused it is still considered active but will not record any data into the transform feedback buffers there are also several restrictions about changing state related to transform feedback while transform feedback is active but paused the currently bound transform feedback object may not be changed it is not possible to bind different buffers to the binding points the current program object cannot be changed actually it is possible to change the current program object but an error will be generated by glresumetransformfeedback if the program object that was current when glbegintrans formfeedback was called is no longer current so be sure to put the original program object back before calling glresumetransformfeedback transform feedback glpausetransformfeedback will generate an error if transform feedback is not active or if it is already paused to restart transform feedback while it is paused glresumetransformfeedback must be used not glbegintransformfeedback likewise glresumetransformfeedback will generate an error if it is called when transform feedback is not active or if it is active but not paused when you ve completed rendering all of the primitives for transform feedback you change back to normal rendering mode by calling glendtransformfeedback transform feedback example particle system this section contains the description of a moderately complex use of transform feedback the application uses transform feedback in two ways to implement a particle system on a first pass transform feedback is used to capture geometry as it passes through the opengl pipeline the captured geometry is then used in a second pass along with another instance of transform feedback in order to implement a particle system that uses the vertex shader to perform collision detection between particles and the rendered geometry a schematic of the system is shown in figure chapter viewing transformations clipping and feedback object space vertices generates view and world space vertices double buffer position and velocity data view space geometry rendered as normal figure schematic of the particle system simulator in this application the particle system is simulated in world space in a first pass a vertex shader is used to transform object space geometry into both world space for later use in the particle system simulation and into eye space for rendering the world space results are captured into a buffer using transform feedback while the eye space geometry is passed through to the rasterizer the buffer containing the captured world space geometry is attached to a texture buffer object tbo so that it can be randomly accessed in the vertex shader that is used to implement collision detection in the second simulation pass using this mechanism any object that would normally be rendered can be captured so long as the vertex or geometry shader produces world space vertices in addition to eye space vertices this allows the particle system to interact with multiple objects potentially with each rendered using a different set of shaders perhaps even with tessellation enabled or other procedurally generated geometry the second pass is where the particle system simulation occurs particle position and velocity vectors are stored in a pair of buffers two buffers are used so that data can be double buffered as it not possible to update vertex data in place each vertex in the buffer represents a single particle in the system each instance of the vertex shader performs collision detection between the particle using its velocity to compute where it will move to during the time step and all of the geometry captured during the first pass it calculates new position and velocity vectors which are captured using transform feedback and written into a buffer object ready for the next step in the simulation care should be taken here tessellation can generate a very large amount of geometry all of which the simulated particles must be tested against which could severely affect performance and increase storage requirements for the intermediate geometry transform feedback example contains the source of the vertex shader used to transform the incoming geometry into both world and eye space and example shows how transform feedback is configured to capture the resulting world space geometry example vertex shader used in geometry pass of particle system simulator version core uniform uniform layout location in position layout location in normal out out void main void pos position pos normalize normal xyz pos example configuring the geometry pass of the particle system simulator static const char gltransformfeedbackvaryings gllinkprogram during the first geometry pass the code in exaples and will cause the world space geometry to be captured into a buffer object each triangle in the buffer is represented by three that are read three at a time during the second pass into the vertex shader and used to perform line segment against triangle intersection test a tbo is used to access the data in the intermediate buffer so that the three vertices can be read in a simple only triangles are used here it not possible to perform a meaningful physical collision detection between a line segment and another line segment or a point also individual trian gles are required for this to work if strips or fans are present in the input geometry it may be necessary to include a geometry shader in order to convert the connected triangles into independent triangles chapter viewing transformations clipping and feedback for loop the line segment is formed by taking the particle current position and using its velocity to calculate where it will be at the end of the time step this is performed for every captured triangle if a collision is found the point new position is reflected about the plane of the triangle to make it bounce off the geometry example contains the code of the vertex shader used to perform collision detection in the simulation pass example vertex shader used in simulation pass of particle system simulator version core uniform uniform uniform int layout location in position layout location in velocity out out uniform samplerbuffer uniform float bool intersect origin direction out point u v n w float r a b u v n cross u v origin a dot n b dot n direction r a b if r r return false point origin r direction float uu uv vv wu wv d uu dot u u uv dot u v vv dot v v w point transform feedback wu dot w u wv dot w v d uv uv uu vv float t uv wv vv wu d if return false t uv wu uu wv d if t t return false return true v n return v dot v n n void main void acceleration velocity acceleration position point int i for i i i texelfetch i xyz texelfetch i xyz texelfetch i xyz if intersect position xyz position xyz xyz point n normalize cross v1 v0 v0 point xyz point n n if y x position y velocity_out 9999 position chapter viewing transformations clipping and feedback the code to set up transform feedback to capture the updated particle position and velocity vectors is shown in example example configuring the simulation pass of the particle system simulator static const char varyings velocity_out gltransformfeedbackvaryings varyings gllinkprogram the inner rendering loop of the application is quite simple first the program object used for rendering the geometry is bound as is a transform feedback object representing the state required to capture the world space geometry then all of the solid objects in the scene are rendered causing the intermediate buffer to be filled with world space geometry next the program object used for updating particle positions is made current as is the transform feedback object used for capturing position and velocity data for the particle system finally the particles are rendered the code for this inner loop is shown in example example main rendering loop of the particle system simulator gluseprogram render_projection_matrix_loc glbindvertexarray glbindbufferbase glbegintransformfeedback object render glendtransformfeedback gluseprogram object getvertexcount if glbindvertexarray vao glbindbufferbase vbo transform feedback else glbindvertexarray vao glbindbufferbase vbo glbegintransformfeedback gldrawarrays min glendtransformfeedback glbindvertexarray the result of the program is shown in figure figure result of the particle system simulator chapter viewing transformations clipping and feedback chapter textures chapter objectives after reading this chapter you ll be able to do the following understand what texture mapping can add to your scene supply texture images in compressed and uncompressed formats control how a texture image is filtered as it is applied to a fragment create and manage texture images in texture objects supply texture coordinates describing what part of the texture image should be mapped onto objects in your scene perform complex texture operations using multiple textures in a single shader specify textures to be used for processing point sprites the goal of computer graphics generally speaking is to determine the colors that make up each part of an image while it is possible to calculate the color of a pixel using an advanced shading algorithm often the com plexity of such a shader is so great that it is not practical to implement such approaches instead we rely on textures large chunks of image data that can be used to paint the surfaces of objects to make them appear more realistic this chapter discusses various approaches and techniques to apply textures using shaders in your application this chapter has the following major sections texture mapping provides an overview the the process of texture mapping basic texture types provides an outline of the types of texture that are available in opengl creating and initializing textures explains how to create and set up a texture for use in your application proxy textures introduces the proxy texture targets which provide a mechanism to probe the capabilities of the opengl implementation specifying texture data provides a description of the formatting of texture data in opengl and how you get that data into your texture objects sampler objects shows how sampler objects can be used to control the way that opengl reads data from textures into your shaders using textures delves into the ways that you can make best use of textures in your shaders complex texture types describes some of the more advanced texture types that are available in opengl including array textures cube maps depth and buffer textures texture views describes how to share one texture data with one or more other textures and to interpret it in different ways compressed textures explores methods to use compressed texture data in your application in order to save memory and bandwidth which are both important performance considerations filtering outlines the various ways in which multiple texels may be combined in order to reduce artifacts and to improve the quality of your rendered images advanced texture lookup functions takes a closer look at some of the more advanced functions available in glsl that can be used to read data from textures with more control chapter textures point sprites describes a feature of opengl that provides texture coordinates automatically for geometry rendered as points allowing your application to very quickly render small bitmaps to the display rendering to texture maps explains how to render directly into a texture map by using framebuffer objects texture mapping in the physical world colors within your field of view can change rapidly odds are you re reading this book inside of a building look at the walls ceiling floors and objects in the room unless you ve furnished your home entirely at ikea it likely some surface in the room will have detail where the colors change rapidly across a small area capturing color changes with that amount of detail is both toilsome and data intensive effectively you need to specify a triangle for each region of linear color change it would be much simpler to be able to use a picture and glue it onto the surface like wallpaper enter texture mapping this technique allows you to look up values like colors from a shader in a special type of table while texture mapping is available in all of opengl shading stages we ll first discuss it in the context of processing fragments because that where it used most often often a texture map or just texture for short is an image captured by a camera or painted by an artist but there no requirement that be the case it possible that the image is procedurally generated see chapter procedural texturing or even rendered by opengl targeting a texture instead of the display device textures of this nature would be two dimensional but opengl supports many other types of textures as well one and three dimensional textures cube map textures and buffer textures array textures are also supported which are treated as a set of slices of similar dimension and format wrapped up in a single texture object all of these will be discussed in detail below textures are composed of texels which will often contain color values however there a lot of utility in merely considering a texture as a table of values that you can query in a shader and use for any purpose you desire in order to use texture mapping in your application you will need to do the following steps create a texture object and load texel data into it include texture coordinates with your vertices we applaud you if that not true except if you re currently operating a moving vehicle texture mapping associate a texture sampler with each texture map you intend to use in your shader retrieve the texel values through the texture sampler from your shader we ll discuss each of those steps in the following sections basic texture types opengl supports many types of texture object of varying dimensiona lities and layout each texture object represents a set of images that make up the complete texture each image is a or array of texels and many images may be stacked one on top of another to form what is known as a mipmap pyramid more information about mipmaps how they affect texturing and how to create them is covered in using and generating mipmaps on page furthermore textures may contain arrays of or slices such textures are known as array textures and each element of the array is known as a slice a cube map is a special case of an array texture that has a multiple of six slices a single cube map texture has exactly six faces whereas a cube map array represents an array of cube map textures always having an integer multiple of six faces textures may be used to represent multisampled surfaces by using the multisampled texture types for and array textures multisampling is a term that refers to an implementation of antialiasing where each texel or pixel is assigned multiple independent colors and those colors may be merged together later in the rendering process to produce the final output color a multisampled texture has several samples typically between two and eight for each texel textures are bound to the opengl context via texture units which are represented as binding points named through where i is one less than the number of texture units supported by the implementation many textures may be bound to the same context concurrently as the context supports many texture units once a texture has been bound to a context it may be accessed in shaders using sampler variables which were declared with dimensionality that matches the texture table gives a list of the available texture dimensionalities known as texture targets and the corresponding sampler type that must be used in shaders to access the texels in the texture chapter textures table texture targets and corresponding sampler types target sampler type dimensionality array array multisample multisample array cube samplercube cube map texture array samplercubearray cube map array rectangle samplerrect rectangle buffer samplerbuffer buffer in table a number of special texture targets are listed first the rectangle texture target is a special case of texture that represents a simple rectangle of texels it cannot have mipmaps and it cannot be used to represent a texture array also some of the texture wrapping modes are not supported for rectangle textures second the buffer texture which represents arbitrary arrays of texels like rectangle textures they do not have mipmaps and cannot be aggregated into arrays furthermore the storage i e memory for buffer textures is actually represented using a buffer object because of this the upper bound on the size of a buffer texture is much larger than a normal one dimensional texture buffer textures make it possible to access things like vertex data from any shader stage without needing to copy it into a texture image in the first few sections of this chapter we will cover basic texturing using single textures which will be sufficient to describe how to create initialize and access textures in shaders later in the chapter beginning in complex texture types on page we will discuss more advanced texture types such as volume textures buffer textures and texture arrays first we will continue our introduction to texturing using textures and then return to each of the special types in detail once the basics have been covered creating and initializing textures the first step in using textures in opengl is to reserve names for texture objects and bind them to the context texture units to create a texture as with many other objects in opengl names are reserved and then bound creating and initializing textures to their appropriate target to reserve names for texture objects call glgentextures specifying the number of names to reserve and the address of an array into which to deposit the names after texture object names have first been reserved they don t yet represent textures and so they don t have any dimensionality or type a texture object is created with the reserved name the first time it is bound to a texture target using glbindtexture the target used for this initial binding determines what type of texture is created from then on the texture may only be bound to that until it is destroyed or targets that are compatible with that type of texture chapter textures as already described the opengl context supports multiple texture units calling glbindtexture binds a texture object to the active texture unit with the dimensionality specified by target the active texture unit is inferred from a selector which may be changed by calling the glactivetexture function a single texture may be bound to multiple texture units simultaneously this causes the same texture data to be read through different samplers representing the texture units to which the texture is bound the maximum number of texture units supported by opengl can be determined by retrieving the value of the constant which is guaranteed to be at least as of opengl once a texture has been bound to a texture unit for the first time it is initialized with default and becomes a texture object until this point the reserved name is really just a placeholder and is not yet a texture object to determine whether a reserved name refers to a texture object you may call glistexture the default state of texture objects may be found in the state tables in the opengl specification creating and initializing textures once a texture object has reached the end of its useful life it should be deleted the function for deleting textures is gldeletetextures and it works similarly to glgentextures in that it takes a number of texture objects to delete and the address of an array containing the names of those textures any reference to the underlying storage associated with the textures is removed and that storage will eventually be released by opengl when it is no longer needed once a texture object has been deleted using gldeletetextures its name becomes unused again and may be returned from a subsequent call to glgentextures once we have created some texture objects we must specify storage and ultimately data for them each dimensionality of texture object has an associated storage function that defines the bounds of the texture these are and which define the storage for and textures respectively for array textures the next higher dimension is used to specify the size of the array for example is used to initialize storage for array textures and is used to initialize storage for array textures and cube map array textures array textures will be covered in more detail in array textures on page chapter textures void glenum target glsizei levels glenum internalformat glsizei width void glenum target glsizei levels glenum internalformat glsizei width glsizei height void glenum target glsizei levels glenum internalformat glsizei width glsizei height glsizei depth specify immutable texture storage for the texture object currently bound to target may be used to specify storage for textures and for this function target must be is used to specify storage for array textures when target is and for textures when target is for array textures width specifies the extent of the texture and height specifies the number of slices in the array for textures width and height specify the dimensions of the texture is used to specify storage for array textures when target is and for textures when target is for array textures width and height specify the dimensions of each slice and depth specifies the number of slices in the array for textures width height and depth specify the dimensions of the texel array the through are used to create immutable storage for textures the attributes of the storage for the texture include the amount of memory required to store all of the texels in all of the mipmap levels for the texture in the chosen internal format at the specified resolution once allocated with one of these functions the storage may not be redefined this is generally considered best practice in opengl as once defined the opengl implementation can make assumptions that the dimensions and format of the texture object will not change over its lifetime and thus can stop tracking certain aspects of the texture object note that it only the attributes of the storage that cannot change once a texture has been designated as immutable the contents of the texture may be changed using functions such as as explained in specifying texture data on page the functions and only allow the creation of storage for single sampled textures if a multisampled texture is being used you may call or to create storage for the textures creating and initializing textures void glenum target glsizei samples glenum internalformat glsizei width glsizei height glboolean fixedsamplelocations void glenum target glsizei samples glenum internalformat glsizei width glsizei height glsizei depth glboolean fixedsamplelocations specify immutable texture storage for the multisample texture object currently bound to target for target must be and it is used to specify storage multisample textures width and height specify the dimensions of the texture is used to specify storage for multisample array textures target must be for multisample array textures width and height specify the dimensions of each slice and depth specifies the number of slices in the array in both functions samples specifies the number of samples represented by the texture if fixedsamplelocations is then opengl will use the same sub texel position for the same sample in each texel of the texture if fixedsamplelocations is then opengl may choose a spatially varying location for a given sample in each texel although it is best practice to declare texture storage as immutable it may be desirable to allow texture objects to be redefined to be resized or have their format changed for example if immutable storage is not desired one of the mutable texture allocation functions may be used the mutable texture image specification commands include and or their multisample variants and chapter textures void glenum target glint level glint internalformat glsizei width glsizei height glsizei depth glint border glenum format glenum type const void data the functions and are used to specify mutable storage and to optionally provide initial image data for a single mipmap level of a or texture respec tively in addition and may be used to specify storage and image data for a single mipmap level of a or array texture the parameters width height and depth if present specify the width height and depth of the resulting texel array in texels for array texture specification height specifies the number of slices in a array texture and depth specifies the number of slices in a array texture internalformat specifies the format with which opengl should store the texels in the texture data specifies the location of the initial texel data in memory if a buffer is bound to the binding point texel data is read from that buffer object and data is interpreted as an offset into that buffer object from which to read the data if no buffer is bound to then data is interpreted as a direct pointer to the data in application memory unless it is null in which case the initial contents of the texture are undefined the format of the initial texel data is given by the combination of format and type opengl will convert the specified data from this format into the internal format specified by internalformat creating and initializing textures because initial data cannot be specified for multisample textures and functions such as cannot be used to update the contents of multisample textures the only way to place data into a multisample texture is to attach it to a framebuffer object and render into it rendering into textures using framebuffer objects is discussed in more detail in chapter color pixels and framebuffers texture formats the functions and and their corresponding multisample variants all take an internalformat parameter which determines the format that opengl will use to store the internal texture data they also take a format and type parameter indicating the format and type of the data supplied by the application internal formats the internal format of a texture is the format that opengl will use to internally store the texture data you give it your data will be converted if necessary into this format at image specification time there are a large number of internal formats that opengl can store image data in and each chapter textures comes with a size performance and quality tradeoff it is up to you the application writer to determine the appropriate format for your needs table lists all of the internal formats supported by opengl along with their bit sizes for each component table sized internal formats sized base r g b a shared internal format internal format bits bits bits bits bits gl_rgb4 s8 s8 s8 ui2 s16 s16 s16 creating and initializing textures table continued sized internal formats sized internal format base internal format r bits g bits b bits a bits shared bits f16 f16 f32 f32 f11 i8 i8 i8 ui8 ui8 i16 i16 i16 ui16 ui16 i32 i32 i32 ui32 ui32 chapter textures for each format listed in table the full format is made up of an identifier representing the base format one or more size indicators and an optional type the base format essentially determines which components of the texture are present formats starting with have only the red component present formats have both red and green formats contain red green and blue and finally contain red green blue and alpha the subsequent size indicator determines the number of bits that are used to store the texture data in many cases only a single size parameter is included in such cases all components present receive the same number of bits by default opengl stores textures in unsigned normalized format when data is stored in unsigned normalized format the values of the texels are stored in memory as an integer which when read into a shader is converted to floating point and divided by the maximum representable value for the corresponding size of integer this results in data in the range to i e normalized data being presented to the shader if the modifier is present as in for example then the data is signed normalized in this case the data in memory is treated as a signed integer and before it is returned to the shader it is converted to floating point and divided by the maximum representable signed integer value resulting in floating point values in the range to being returned to the shader type specifiers may be present in the internal format name these type specifiers are i ui and f indicating signed integer unsigned integer and floating point data respectively the signed and unsigned integer internal formats are designed to be used with signed or unsigned integer sampler types in your shader or for example the floating point internal formats are true floating point formats in that the data is stored in memory in a floating point representation and returned to the shader with the full precision supported by the opengl implemen tation in such cases the texels can represent floating point values outside the range to in some cases a different size specifier is used for some or each of the channels in these cases opengl will use a different number of bits for each of the channels for example textures are stored using a bit quantity per texel with bits allocated to each of the red green and blue channels but only bits allocated to the alpha channel this format of texture is useful for representing higher dynamic range textures with only a few levels of opacity or with the alpha channel used to store something other than traditional opacity the uses bits for each of red and green and bits for blue but stores each creating and initializing textures channel in a special reduced precision floating point format the bit components have no sign bit a bit exponent and a bit mantissa the format is special in that it is a shared exponent format each component is stored as an independent bit mantissa but shares a single bit exponent between all of the components this allows textures to be stored with a fairly high dynamic range but to only consume bits per texel the and formats are rgb textures in the srgb color space the former without alpha and the latter including an alpha channel the alpha channel in is represented separately because it is not part of the srgb color space and is not subject to the de gamma calculations affecting the other components external formats the external format is the format that you use to supply data through the opengl api and is represented by the format and type parameters to functions such as the format is made up of a part indicating which channels are present and an optional integer format specifier additionally there are a handful of packed integer formats that are used to represent prepacked texture data ideally there would be no conversion required to take your texture data and place it into the texture with the requested internal format the possible values for the format parameter are given in table which lists the external format identifier the components present their order and whether the data is comprised of integer values table external texture formats format components present red green blue red green gl_rgb red green blue red green blue alpha blue green red blue green red alpha red integer green integer chapter textures table continued external texture formats format components present blue integer red green integer red green blue integer red green blue alpha integer blue green red integer blue green red alpha integer again notice that the format specifiers listed in table indicate which components are present red green blue and alpha their order and an optional suffix if this suffix is present then the values passed to opengl are treated as unnormalized integer data and used verbatim if the internal format of the texture is a floating point format then the data is converted to floating point directly that is an integer value of becomes in floating point regardless of the incoming data type if you wish to receive integers in your shader then you should use an integer sampler type an integer internal format e g and an integer external format and type e g and the format parameter is used in conjunction with a type parameter to describe the texture data in memory type is normally one of or to indicate signed or unsigned bytes signed or unsigned shorts signed or unsigned integers or half precision or full precision floating point quantities may also be used to indicate double precision quantities these tokens correspond to the glbyte glubyte glshort glushort glint gluint glhalf glfloat and gldouble types respectively in addition to the tokens representing the native types several special tokens are used to specify packed or mixed type formats these are used when data is packed into larger native types with the boundaries between components not necessarily lining up nicely on native byte short or integer boundaries these type names are generally made up of a standard type specifier such as followed by a suffix indicating how the data is laid out in memory table shows a few examples of how components may be packed into native data types using packed format tokens creating and initializing textures table example component layouts for packed pixel formats format token component layout gl_unsigned_short_5_6_5_rev red green blue alpha gl_unsigned_int_10_10_10_2 alpha blue green red proxy textures in addition to the texture targets listed in table opengl supports what are known as proxy texture targets each standard texture has a corresponding proxy texture target table lists the standard texture targets and their corresponding proxy texture targets table texture targets and corresponding proxy targets texture proxy texture target target cube cube rectangle rectangle buffer n a all targets except for have a corresponding proxy texture target chapter textures proxy texture targets may be used to test the capabilities of the opengl implementation when certain limits are used in combination with each other for example consider an opengl implementation that reports a maximum texture size of texels which is the minimum requirement for opengl if one were to create a texture of texels with an internal format of which requires four bytes of storage per texel then the total storage requirement for such a texture would be at least a gigabyte more if mipmaps or other internal storage is required therefore such a request would fail on an opengl implementation with less than a gigabyte of available storage for textures by requesting such a texture allocation on a proxy texture target the implementation can tell you whether the request succeed on a normal target or whether it is destined to fail if an allocation of a texture on a proxy texture target fails the texture on the virtual proxy target will have a width and height of zero querying the dimensions of the proxy target will tell you whether the call was successful and whether such a request on a real target might succeed specifying texture data in this section we describe the method in which image data is loaded into texture objects two methods are covered first we show how to load images directly into the texture object either from data stored in arrays in your program or from buffer objects this illustrates the storage and data formats used for texture objects next we show how to use the vglloadimage function that is supplied as part of the sample code for this book and how it allows you to load images from files explicitly setting texture data in order to describe the process in which texture data is specified to opengl it possibly easiest to be able to see the image data directly in your program texture data is laid as you might expect it to be left to right top to bottom in example texture data is stored in a constant array declared in c just because an allocation appears to succeed on a proxy texture target it does not mean that it will definitely succeed on a real target it may fail for a variety of other reasons such as the total amount of other textures allocated or memory fragmentation for example however if it fails on a proxy texture target it will certainly fail on a real texture target there are several parameters supported by opengl that allow you to change the layout of image data in memory these are discussed later in this chapter but the defaults are sufficient for this example specifying texture data example direct specification of image data in c the following is an checkerboard pattern using data static const glubyte 0xff 0xff 0x00 0xff 0x00 0x00 0xff 0x00 0xff 0x00 0xff 0x00 0xff the following data represents a texture with red green blue and yellow texels represented as data static const glfloat red texel green texel blue texel yellow texel of course specifying texture data by hand directly in your code is not the most efficient way of creating textures for simple cases such as solid colors or basic checkerboard patterns it will suffice though you can load the data into a texture object using one of the or functions listed below chapter textures the data shown in example shows two simple textures directly coded into constant arrays the first specifies a simple region of texels of alternating full intensity 0xff and zero intensity 0x00 represented as single unsigned bytes the second array in example shows color data this time represented as floating point data with four channels the channels representing the amount of red green blue and in each texel example shows how to load this data into texture objects using example loading static data into texture objects first the black and white checkerboard texture bind the texture possibly creating it glbindtexture allocate storage for the texture data specify the data for the texture target first mipmap level x and y offset width and height format and type data alpha is normally used to represent opacity but in this case we have set the alpha channel to its maximum value to represent fully opaque texels specifying texture data next the color floating point data bind the next texture glbindtexture allocate storage specify the data target first mipmap level x and y offset width and height format and type data notice how in example we specify an internal format for the texture that somewhat matches our supplied texture data for the array of unsigned bytes we used the internal format which indicates a single channel bit format for the color data we used which is a four channel bit floating point format there is no requirement that we use an internal format that matches the data we ll supply there are well defined rules as to how opengl converts the data you supply into each internal format and these are explained in detail in the opengl specification using pixel unpack buffers the data parameter to may be interpreted in one of two ways the first is as a natural pointer to data stored in the application memory this is the use case shown in example data is interpreted this way if no buffer object is bound to the target the second interpretation of data which is used when there is a buffer bound to the target is as an offset into that buffer object this allows the application to stage data into a buffer object and transfer it from there into a texture object at a later time example loading data into a texture using a buffer object first bind a buffer to the binding glbindbuffer buf place our source data into the buffer glbufferdata sizeof bind the texture possibly creating it glbindtexture allocate storage for the texture data specify the data for the texture chapter textures target first mipmap level x and y offset width and height format type null data an offset into buffer in example we first place our source data into a buffer object bound to the binding point and then call as we did before however this time data is interpreted as an offset into the buffer object rather than a raw pointer because we left the buffer bound this causes opengl to take the data from the buffer object but not necessarily immediately the primary advantage of using a buffer object to stage texture data is that the transfer from the buffer object into the texture need not occur immediately so long as it occurs by the time the data is required by a shader this allows the transfer to occur in parallel with the application running if instead the data is located in application memory then the semantics of require that a copy of the data is made before the function returns preventing a parallel transfer the advantage of this method is that the application is free to modify the data it passed to the function as soon as the function returns copying data from the framebuffer it is possible to read part of the framebuffer into a texture object and then use it in subsequent rendering to do this use either the or functions whose prototypes are as follows specifying texture data when or is called it is essentially equivalent to calling glreadpixels and then immediately calling either or to re upload the image data into the texture void glenum target glint level glint xoffset glint x glint y glsizei width void glenum target glint level glint xoffset glint yoffset glint x glint y glsizei width glsizei height void glenum target glint level glint xoffset glint yoffset glint zoffset glint x glint y glsizei width glsizei height use image data from the framebuffer to replace all or part of a contiguous subregion of the current existing texture image in the texture object bound to target of the active texture unit x and y specify the x and y offset of the region in the framebuffer to copy width and height if present specify the width and height of the region to copy respectively xoffset yoffset and zoffset if present specify the origin of the destination region in the target texture if target is a array texture then yoffset is the layer of the texture into which the region of texels will be written otherwise it is the y coordinate of the origin of the destination region if target is a array cube map or cube map array texture then zoffset is the index of the layer of the texture containing the destination region otherwise it is the z coordinate of the destination region in a texture although it is possible to read from the framebuffer into a texture this involves making a copy of the image data and may also involve format conversions or other work being conducted by the graphics hardware in general it is more efficient to draw directly into the texture wherever possible this is covered in detail in chapter color pixels and framebuffers loading images from files the simple example of directly storing image data in arrays in your c code or from buffer objects isn t very practical if you have large images stored on disk in most applications you ll store your texture data in a chapter textures formatted image file a jpeg png gif or other type for image format opengl works either with raw pixels or with textures compressed with specific algorithms as such your application will need to decode the image file into memory that opengl can read to initialize its internal texture store to simplify that process for our examples we wrote a function vglloadimage which will read an image file and return the texels in memory along with other information you ll need to help opengl to decode the its pixel data width measured in pixels height measured in pixels opengl pixel format e g gl_rgb for rgb pixels a recommended internal format to use for the texture the number of mipmap levels present in the texture data type for each component in a pixel image data all of that data is stored in structure of type vglimagedata which is defined in loadimage h the definition of vglimagedata is shown in example below example definition of the vglimagedata structure enough mips for x which is the minimum required for opengl x and higher define each texture image data structure contains an array of of these mipmap structures the structure represents the mipmap data for all slices at that level struct vglimagemipdata glsizei width width of this mipmap level glsizei height height of this mipmap level glsizei depth depth pof mipmap level glsizeiptr mipstride distance between mip levels in memory glvoid data pointer to data this is the main image data structure it contains all the parameters needed to place texture data into a texture object using opengl currently dds files are supported by vglloadimage specifying texture data struct vglimagedata glenum target texture target cube map etc glenum internalformat recommended internal format glenum format format in memory glenum type type in memory gl_rgb etc glenum swizzle swizzle for rgba glsizei miplevels number of present mipmap levels glsizei slices number of slices for arrays glsizeiptr slicestride distance between slices of an array texture glsizeiptr totaldatasize total data allocated for texture vglimagemipdata mip actual mipmap data in order to create initialize manipulate and destroy images in memory we have defined two functions vglloadimage and vglunloadimage each takes a pointer to a vglimagedata structure vglloadimage fills it in and vglunloadimage releases any resources that were allocated by a previous call to vglloadimage the prototypes of vglloadimage and vglunloadimage are as follows to load an image file simply use code such as that shown in example in your application example simple image loading example vglimagedata image vglloadimage filename image use image data here vglunloadimage image the result of calling vglloadimage is that the texture data from the specified image file is loaded into memory and information about that image data is stored in the vglimagedata structure given to the function once the image data has been loaded from the file you may use it to establish the texels for your texture object to do this pass the data pointer chapter textures and texture dimensions to the appropriate texture image function if the texture is allocated as an immutable object using for example then the image data is specified using a texture subimage command such as the vglimagedata structure contains all of the parameters required to initialize the image example shows a simple but complete example of using the vglloadimage function to load an image from disk the function to allocate storage in texture object and to load the image data into the texture object example loading a texture using loadimage gluint loadtexture const char filename gluint texture glboolean generatemips vglimagedata image int level vglloadimage filename image if texture glgentextures texture glbindtexture image target texture switch image target case image target image miplevels image internalformat image mip width image mip height handle other texture targets here default break assume this is a texture for level level image miplevels level level image mip level width image mip level height image format image type image mip level data specifying texture data unload the image here as has consumed the data and we don t need it any more vglunloadimage image return texture as you can see this code could become quite complex depending on how generic your texture loading function might be and how many types of texture you might want to load to make things easier for you we have included the function vglloadtexture which internally uses vglloadimage to load an image file and then place its contents into a texture object for you the listing shows a simplified version of the vglloadtexture function which will take an image file and load it into a texture object for you it will handle any dimensional image array textures cube maps compressed textures and anything else that supported by the vglloadimage function the complete implementation of vglloadtexture is included in this book accompanying source code note it not possible to directly specify the image data for a multisampled texture the only way to place data into a multisampled texture is to attach it to a framebuffer object and render into it framebuffers and multisampling is explained in some detail in chapter color pixels and framebuffers chapter textures retrieving texture data once you have a texture containing data it is possible to read that data either back into your application memory or back into a buffer object the function for reading image data from a texture is glgetteximage whose prototype is as follows great care should be exercised when using this function the number of bytes written into image is determined by the dimensions of the texture currently bound to target and by format and type potentially a great deal of data could be returned and no bound checks are performed by opengl on the memory area you supply therefore incorrect usage of this function could lead to buffer overruns and bad things happening furthermore reading pixel data back from textures is generally not a high performance operation doing so should be a sparingly invoked operation and should certainly not be in a performance critical path of your application if you must read data back from textures we strongly recommend that you bind a buffer to the buffer target read the texels into that and subsequently map the buffer in order to transfer the pixel data into your application specifying texture data texture data layout so far our descriptions of the texture image specification commands have not addressed the physical layout of image data in memory in many cases image data is laid out left to right top to in memory with texels closely following each other however this is not always the case and so opengl provides several controls that allow you to describe how the data is laid out in your application these parameters are set using the glpixelstorei and glpixelstoref commands whose prototypes are as follows the unpack parameters set by glpixelstorei and glpixelstoref those beginning with specify how opengl will read data from client memory or the buffer bound to the binding in functions such as the packing parameters specify how opengl will write texture data into memory during functions such as glgetteximage since the corresponding parameters for packing and unpacking have the same meanings they re discussed together in the rest of this section and referred to without the or prefix for example refers to and if the parameter is the default the ordering of the bytes in memory is whatever is native for the opengl client otherwise the bytes are reversed the byte reversal it important to understand that textures don t really have a top and a bottom but rather they have an origin and a direction of increasing of texture coordinates what appears to be rendered at the top of a frame in window coordinates depends entirely on the texture coordinates used chapter textures applies to any size element but has a meaningful effect only for multibyte elements the effect of byte swapping may differ among opengl implementations if on an implementation glubyte has bits glushort has bits and gluint has bits then figure illustrates how bytes are swapped for different data types note that byte swapping has no effect on single byte data note as long as your opengl application doesn t share images with other machines you can ignore the issue of byte ordering if your application must render an opengl image that was created on a different machine and the two machines have different byte orders byte ordering can be swapped using however does not allow you to reorder elements e g to swap red and green byte byte short byte short byte short byte short byte integer byte integer byte integer byte integer byte integer byte integer byte integer byte integer byte figure byte swap effect on byte short and integer data the parameter applies only when drawing or reading bit images or bitmaps for which a single bit of data is saved or restored for each pixel if is the default the bits are taken from the bytes starting with the most significant bit otherwise they re taken in the opposite order for example if is and the byte in question is the bits in order are if is the order is sometimes you want to draw or read only a subrectangle of the entire rectangle of image data stored in memory if the rectangle in memory is larger than the subrectangle that being drawn or read you need to specify the actual length measured in pixels of the larger rectangle with if is zero which it is by default the row length is understood to be the same as the width that implied by the specifying texture data parameters to for example you also need to specify the number of rows and pixels to skip before starting to copy the data for the subrectangle these numbers are set using the parameters and as shown in figure by default both parameters are so you start at the lower left corner subimage image figure subimage a subimage identified by and parameters often a particular machine hardware is optimized for a particular byte alignment when moving pixel data to and from memory for example in a machine with bit words hardware can often retrieve data much faster if it initially aligned on a bit boundary which typically has an address that is a multiple of likewise bit architectures might work better when the data is aligned to byte boundaries on some machines however byte alignment makes no difference as an example suppose your machine works better with pixel data aligned to a byte boundary images are most efficiently saved by forcing the data for each row of the image to begin on a byte boundary if the image is pixels wide and each pixel consists of byte each of red green and blue information a row requires bytes of data maximum display efficiency can be achieved if the first row and each successive row begins on a byte boundary so there is byte of waste in the memory storage for each row if your data is stored in this way set the alignment parameter appropriately to in this case chapter textures if alignment is set to the next available byte is used if it a byte is skipped if necessary at the end of each row so that the first byte of the next row has an address that a multiple of in the case of bitmaps or bit images where a single bit is saved for each pixel the same byte alignment works although you have to count individual bits for example if you re saving a single bit per pixel if the row length is and if the alignment is then each row requires or bytes since is the smallest multiple of that is bigger than bytes of memory are used for each row if the alignment is then bytes are used for each row as is rounded up to the next byte note the default value for alignment is a common programming mistake is to assume that image data is tightly packed and byte aligned which assumes that alignment is set to the parameters and affect only the defining and querying of three dimensional textures and two dimensional texture arrays these pixel storage parameters allow the routines and glgetteximage to delimit and access any desired subvolume or subset of slices of an array texture if the three dimensional texture in memory is larger than the subvolume that is defined you need to specify the height of a single subimage with the parameter also if the subvolume does not start with the very first layer the parameter needs to be set is a pixel storage parameter that defines the height number of rows of a single layer of a three dimensional texture image as shown in figure if the value is zero a negative number is invalid then the number of rows in each two dimensional rectangle is the value of height which is the parameter passed to or this is commonplace because is zero by default otherwise the height of a single layer is the value figure pixel storage mode specifying texture data defines how many layers to bypass before accessing the first data of the subvolume if the value is a positive integer call the value n then the pointer in the texture image data is advanced that many layers n the size of one layer of texels the resulting subvolume starts at layer n and is several layers deep how many layers deep is determined by the depth parameter passed to or if the value is zero the default then accessing the texel data begins with the very first layer described in the texel array figure shows how the parameter can bypass several layers to get to where the subvolume is actually located in this example is and the subvolume begins at layer figure pixel storage mode sampler objects textures may be read by a shader by associating a sampler variable with a texture unit and using glsl built in functions to fetch texels from the texture image the way in which the texels are fetched depends on a number of parameters that are contained in another object called a sampler object sampler objects are bound to sampler units much as texture objects are bound to texture units for convenience a texture object may be considered to contain a built in sampler object of its own that will be used by default to read from it if no sampler object is bound to the corresponding sampler unit to create a sampler object as with most other object types in opengl we reserve a name for the new object and bind it to a binding point in this case one of the binding points the prototype of glgensamplers is as follows chapter textures glgensamplers will return a set of unused sampler object names once the names have been generated they are reserved for use as sampler objects and may be bound to the sampler binding points using the glbindsampler function whose prototype is shown below until a name has been bound to a sampler unit it is not yet considered a sampler object to determine if a given value is the name of an existing sampler object you can call glissampler whose prototype is as follows notice that there are a couple of subtle between the glbindsampler function and the glbindtexture function first there is no target parameter for samplers this is because the target is implied by these differences may seem to introduce inconsistency into the opengl api that is a fair observation however it is a byproduct of the way that opengl evolves the api takes small incremental steps each striking a balance among introduction of new functionality enable ment of innovation and maintenance of backwards compatibility it has been recognized that modern opengl implementations can support a huge number of textures and samplers and reserving tokens from through is simply not practical and is not forward looking thus the decision to sacrifice some consistency in order to achieve some forward compatibility was made sampler objects the function as sampler objects have no inherent dimensionality there is no reason to distinguish among multiple sampler object types secondly the unit parameter is present here and there is no selector for sampler objects that is there is no glactivesampler function furthermore in contrast to the parameter to glactivetexture which is a token between and where i is an arbitrarily large implementation defined maximum unit is a zero based unsigned integer allowing any number of sampler units to be supported by an opengl implementation without reserving a large number of opengl tokens sampler parameters each sampler object represents a number of parameter that controls the way texels will ultimately be read from a texture object the parameters of the sampler object are set using the glsamplerparameteri and glsamplerparameterf functions for integer and floating point parameters and glsamplerparameteriv and glsamplerparameterfv functions for vectors of integer and floating point parameters their prototypes are given below the glsamplerparameteri and similar functions set the parameters of a sampler object directly the sampler argument of the functions is the name of the sampler object that is being modified however as noted there is a default sampler object contained in each texture object that will be used to read from the texture when no sampler object is bound to the corresponding sampler unit to modify the parameters of this object similar gltexparameter functions are provided chapter textures for both the glsamplerparameter and gltexparameter functions there are a multitude of values that may be used for the pname parameters each controls a different aspect of sampling and for the gltexparameter functions there are some values for pname that are not related to sampling at all rather than introduce each and every legal value for pname here we introduce each in the following subsections as the topics to which they pertain are covered once you are done using a sampler object as with any other type of object in opengl it is good practice to clean up after yourself and delete any unused objects to delete sampler objects use the gldeletesamplers function using textures once you have created and initialized a texture object and have placed image data into it you may read from it using shaders in your application as already noted textures in shaders are represented as sampler variables of dimensioned sampler types each sampler variable is a combination of a set of image data represented by the texture object and a set of sampling using textures parameters that are represented by a sampler object or the texture own internal sampler object a texture is bound to a texture unit and a sampler object is bound to the corresponding sampler unit and together they are used to read data from the texture images this process is called sampling and is performed using the texture built in function in glsl or one of its many variants the usual way to read data from a texture in glsl is to use one of the built in functions glsl supports function overloading which is a term that should be familiar to c programmers among others function over loading is the process where a single function name can represent several different functions with different parameter types at compile time the compiler can determine which version of the function should be called based on the types of the parameter used to call it the basic overloaded variants of the texture lookup functions are given below all texture functions are listed in appendix c built in glsl variables and functions note a note on terminology in many of the glsl function prototypes you will see the term or other dimensional vectors this is a placeholder type that means a vector of any type it could stand for or likewise for example is a placeholder that may stand for or also if you see a parameter surrounded in square brackets i e and that means that the parameter is optional and may be omitted if desired the texture functions in glsl each take a sampler variable and a set of texture coordinates the return value from the functions is the result of sampling from the texture represented by the sampler chapter textures the sampler argument passed into the texture function can be an element of a sampler array or a parameter in a function in all cases the argument must be dynamically uniform that is the argument must be the result of an expression involving uniforms constants or variables otherwise known to have the same value for all the instances of the shader such as loop counters an example of using a texture function to read texels from a texture is given in example example simple texture lookup example fragment shader version core uniform tex in layout location out color void main void color texture tex in example a fragment shader that reads from a texture is given textures may be used from any shader stage but the effects of texturing are easiest to demonstrate in a fragment shader at the top of the shader a uniform sampler tex is declared the single input to the fragment shader is the texture coordinate which is declared as a and the output from the fragment shader is a single color output color the corresponding vertex shader is shown in example example simple texture lookup example vertex shader version core layout location in layout location in out void main void using textures in example the two inputs are the vertex position and the input texture coordinate which is passed directly to the shader outputs in this case these are the built in output and the user defined output that will be passed to the similarly named input in the fragment shader given in example texture coordinates texture coordinates are the coordinates within the texture at which to sample the image these are often supplied per vertex and then inter polated over the area of the resulting geometry to provide a per fragment coordinate this coordinate is used in the fragment shader to read from the texture and retrieve a color from the texture for the resulting fragment the texture coordinates in the example of examples and is supplied by the application passed to the vertex shader in interpolated by opengl and then passed to the fragment shader in before being used to read from the texture the application side code to set up a simple set of texture coordinates is shown in example example simple texturing example prog is the name of a linked program containing our example vertex and fragment shaders gluseprogram prog tex is the name of a texture object that has been initialized with some texture data glbindtexture tex simple quad with texture coordinates static const glfloat vertex positions texture coordinates create and initialize a buffer object gluint buf chapter textures glgenbuffers buf glbindbuffer buf glbufferdata sizeof setup vertex attributes gluint vao glgenvertexarrays vao glbindvertexarray vao glvertexattribpointer glvoid glenablevertexattribarray glvertexattribpointer glvoid sizeof float glenablevertexattribarray ready draw gldrawarrays in example the geometry for a simple quadrilateral is placed into a buffer object along with texture coordinates for each of its four vertices the position data is sent to vertex attribute and the texture coordinates are sent to vertex attribute in the example prog is the name of a program object that has previously had the shaders of examples and compiled and linked into it and tex is a texture object with texture data already loaded into it the result of rendering with this program is shown in figure figure output of the simple textured quad example using textures each of the texture lookup functions in glsl takes a set of coordinates from which to sample the texel a texture is considered to occupy a domain spanning from to along each axis remember you may use one two or even three dimensional textures it is the responsibility of the application to generate or supply texture coordinates for these functions to use as we have done in example normally these would be passed into your vertex shader in the form of a vertex input and then interpolated across the face of each polygon by opengl before being sent to the fragment shader in example the texture coordinates used range from to so all of the resulting interpolated coordinates lie within this range if texture coordinates passed to a texture lookup function end up outside the range to they must be modified to bring them back into this range there are several ways in which opengl will do this for you controlled by the and sampler parameters the and parameters control the way that texture coordinates outside the range to are handled by opengl for the s t and axes of the texture domain respectively the clamping mode in each dimension may be set to one of or the clamping modes work as follows if the mode is whenever a texture coordinate is outside the range to texels on the very edge of the texture are used to form the value returned to the shader when the mode is an attempt to read outside the texture will result in the constant border color for the texture being used to form the final value when the clamping mode is set to the texture is simply wrapped and considered to repeat infinitely in essence only the fractional part of the texture coordinate is used to lookup texels and the integer part is discarded the clamping mode is a special mode that allows a texture to be repeated in a mirrored fashion texture coordinates whose integer part is even have only their fractional part considered texture coordinates whose integer part is odd i e etc have their fractional part subtracted from in order to form the texture coordinates are traditionally referred to as t r and q to distinguish them from spatial coordinates x y z and w and color coordinates r g b and a one caveat is that in glsl r is already used for red so the four components of a texture coordinate are referred to as t p and q chapter textures final coordinate this mode can help to eliminate tiling artifacts from repeating textures figure shows each of the texture modes used to handle texture coordinates ranging from to all of these modes except for eventually take texels from somewhere in the texture data store in the case of the returned texels come from the texture virtual border which is a constant color by default this color is transparent black i e in each component of the texture however you may change this color by setting the value of the sampler parameter the snippet of example shows how to set the texture border color to red example setting the border color of a sampler gluint sampler this variable holds the name of our sampler gluint texture this variable holds the name of a texture const glfloat red opaque red set the for the sampler object glsamplerparameterfv sampler red or alternatively set the border color for a texture object this will be used when a texture is bound to a texture unit without a corresponding sampler object glbindtexture texture gltexparameterfv red figure effect of different texture wrapping modes top left top right bottom left and bottom right using textures arranging texture data suppose you have an external source of texture data say an image editing program or another component of your application perhaps written in another language or using another api over which you have no control it is possible that the texture data is stored using a component order other than red green blue alpha rgba for example abgr is fairly common i e rgba bytes stored in little endian order as is argb and even rgbx rgb data packed into a bit word with one byte left unused opengl is quite capable of consuming this data and making it appear as nicely formatted rgba data to your shader to do this we use texture swizzle which is a mechanism that allows you to rearrange the component order of texture data on the fly as it is read by the graphics hardware texture swizzle is a set of texture parameters one for each channel of the texture that can be set using the gltexparameteri function by passing one of the texture swizzle parameter names and the desired source for the data the swizzle texture parameters are and which specify the outgoing texture channels in the order red green blue and alpha respectively furthermore the token name is provided to allow all four channels to be configured using a single call to gltexparameteriv each one specifies what the source of data should be for the corresponding channel of the texture and may be set to one of the source selectors or these indicate the values of the red green blue or alpha channels of the incoming texture or the constant values one and zero respectively by default the swizzle settings are configured to pass the data directly through unmodified that is and are set to gl_green and respectively example shows how to configure a texture to read from abgr and rgbx data in the case of rgbx we specify that the constant value be returned for the missing alpha channel example texture swizzle example the name of a texture whose data is in abgr format gluint the name of a texture whose data is in rgbx format glyint chapter textures an array of tokens to set abgr swizzle in one function call static const glenum gl_red gl_green bind the abgr texture glbindtexture set all four swizzle parameters in one call to gltexparameteriv gltexparameteriv now bind the rgbx texture glbindtexture we re only setting the parameter here because the r g and b swizzles can be left as their default values gltexparameteri using multiple textures now that you have seen a simple application of texture to rendering you may have noticed some omissions from the sample above for example in example we did not set a value for the sampler in the fragment shader this is because we are only using a single texture in fact opengl can support many textures simultaneously a minimum of textures per shader stage are supported which when multiplied by the number of shader stages supported by opengl comes out to textures in fact opengl has texture units referred to by tokens named through whenever one of the texture functions such as glbindtexture is called it operates on the texture bound to the active texture unit which is implied by what is known as a selector by default the active texture selector is however it may be changed and will need to be if you want to use more than one texture the function to change the active texture selector is glactivetexture which was introduced above in order to use multiple textures in your shader you will need to declare multiple uniform sampler variables each will refer to a texture unit from the application side uniform samplers appear much like uniform integers they are enumerated using the normal technically they don t need to be associated with different texture units if two or more samplers refer to the same texture unit then they will both end up sampling from the same texture using textures glgetactiveuniform function and may have their values modified using the function the integer value assigned to a uniform sampler is the index of the texture unit to which it refers the steps to use multiple textures in a single shader or program are therefore as follows first we need to select the first texture unit using glactivetexture and bind a texture to one of its targets using glbindtexture we repeat this process for each texture unit then we set the values of the uniform sampler variables to the indices of the texture units that we wish to use by calling the function to illustrate this we will modify our example from the previous section to use two textures we will first change the vertex shader of example to produce two sets of texture coordinates the updated vertex shader is shown in example example simple multitexture example vertex shader version core layout location in layout location in out out uniform float time void main void const m cos time sin time sin time cos time m transpose m the new vertex shader performs simple animation by using a time uniform variable to construct a rotation matrix and uses that to rotate the incoming texture coordinates in opposite directions next we modify the original fragment shader from example to include two uniform sampler variables read a texel from each and sum them together this new shader is shown in example chapter textures example simple multitexture example fragment shader version core in in layout location out color uniform uniform void main void color texture texture in example we are using a different texture coordinate to sample from the two textures however it is perfectly reasonable to use the same set of texture coordinates for both textures in order to make this shader do something useful we need to set values for the two uniform samplers and and bind textures to the corresponding texture units we do this using the glactivetexture and glbindtexture functions as shown in example example simple multitexture example prog is the name of a linked program containing our example vertex and fragment shaders gluseprogram prog for the first texture we will use texture unit get the uniform location glint glgetuniformlocation prog set it to select texture unit glactivetexture bind a texture to it glbindtexture repeat the above process for texture unit glint glgetuniformlocation prog glactivetexture glbindtexture using textures the two source textures used in this example are shown in figure and the result of rendering with our updated fragment shader with two textures bound is shown in figure figure two textures used in the multitexture example figure output of the simple multitexture example complex texture types textures are often considered only as one or two dimensional images that may be read from however there are several types of textures including textures texture arrays and cube maps shadows depth stencil and buffer textures this section describes the types of texture and outlines their potential use cases chapter textures textures a texture can be thought of as a volume of texels arranged in a grid to create a texture generate a texture object name and bind it initially to the target once bound you may use or to create the storage for the texture object the texture has not only a width and a height but also a depth the maximum width and height of a texture is the same as that of a texture and may be found by retrieving the value of the maximum depth of a texture supported by your opengl implementation is found by retrieving the value of and this may be different than the maximum width and height of the texture textures are read in shaders using three dimensional texture coordinates otherwise they work very similarly to other textures types a typical use case for a texture is for volume rendering in fields such as medical imaging or fluid simulation in this type of application the content of the texture is usually a density map where each represents the density of a medium at that point a simple way to render a volume is to render planes cutting through the volume as a textured quadrilateral with a texture coordinate at each vertex the vertex shader in example shows how a set of two dimensional texture coordinates are transformed into three dimensional space using a transformation matrix these coordinates are then interpolated by opengl and used in the fragment shader of example example simple volume texture vertex shader version core position and texture coordinate from application layout location in layout location in in_tex_coord output texture coordinate after transformation out matrix to transform the texture coordinates into space uniform void main void a voxel is a term that refers to an element of a volume just as pixel refers to an element of a picture and texel refers to an element of a texture complex texture types multiply the texture coordinate by the transformation matrix to place it into space in_tex_coord stp pass position through unchanged example simple volume texture fragment shader version core incoming texture coordinate from vertex shader in final color layout location out color volume texture uniform tex void main void simply read from the texture at the texture coordinate and replicate the single channel across r g b and a color texture tex rrrr the result of rendering with the vertex and fragment shaders of examples and is shown in figure in this example the volume texture contains a density field of a cloud the example animates the cloud by moving a cutting plane through the volume and sampling the texture at each point on the plane figure output of the volume texture example chapter textures array textures for certain applications you may have a number of one or two dimensional textures that you might like to access simultaneously within the confines of a single draw call for instance suppose you re authoring a game that features multiple characters of basically the same geometry but each of which has its own costume or you might want to use multiple layers of texture for the character diffuse color a normal map a specular intensity map and a number of other attributes when using many textures like this you would need to bind all of the required textures before the draw command the calls to glbindtexture for each draw call could have performance implications for the application if the texture objects needed to be updated by opengl texture arrays allow you to combine a collection of one or two dimensional textures all of the same size and format into a texture of the next higher dimension e g an array of two dimensional textures becomes something like a three dimensional texture if you were to try to use a three dimensional texture to store a collection of two dimensional textures you would encounter a few inconveniences the indexing texture coordinate r in this case is normalized to the range to access the third texture in a stack of seven you would need to pass or thereabouts to access what you would probably like to access as texture arrays permit this type of texture selection additionally texture arrays allow suitable mipmap filtering within the texture accessed by the index in comparison a three dimensional texture would filter between the texture slices likely in a way that doesn t return the results you were hoping for compare the prototypes of the texture function for textures and for array textures the second function takes a sampler type and its texture coordinate p has an additional dimension this third component of p is the array index or slice cube map textures cube map textures are a special type of texture useful for environment mapping that takes a set of images and treats them as the faces of a cube the six faces of the cube are represented by six subtextures that must be complex texture types square and of the same size when you sample from a cube map the texture coordinate used is three dimensional and is treated as a direction from the origin this direction essentially points at the location on the surface of the cube from where to read the texture imagine you were standing in the middle of a square room with a laser pointer you could point the laser in any direction and hit part of the wall floor or ceiling of the room the spot where the pointer shines is the point from which you would sample the texture map cube maps are ideal for representing surrounding environments lighting and reflection effects and can also be used to wrap complex objects with textures allocating storage for cube map textures is achieved by binding a new texture name to the texture target and calling on the target this single call will allocate the storage for all six faces of the cube map however once allocated the cube map is represented by a set of six special targets which can be thought of as subtargets of the target these are and each face has its own complete set of mipmaps these special targets may be passed to the command in order to specify image data for the cube map faces example gives an example of how to create and initialize a cube map texture example initializing a cube map texture gluint tex texture to be created extern const glvoid data for the faces generate bind and initialize a texture object using the target glgentextures tex glbindtexture tex note that the tokens negative_y and have contiguous numeric values defined in that order thus it possible to index into the faces of the cube map by simply adding a face index to positive_x so long as the index is consistent with the defined ordering chapter textures now that storage is allocated for the texture object we can place the texture data into its texel array for int face face face glenum target face target face level x y offset size of face format type face data now optionally we could specify the data for the lower mipmap levels of each of the faces cube map textures may also be aggregated into arrays the texture target may be used to create and modify cube map array textures each cube in the cube map array consumes six contiguous slices of the underlying array texture thus an array with five cube map textures in it will have a total of slices the example shown in example is modified below in example to create a cube map array of five cubes in a single texture example initializing a cube map array texture gluint tex texture to be created extern const glvoid data for the faces generate bind and initialize a texture object using the target glgentextures tex glbindtexture tex now that storage is allocated for the texture object we can place the texture data into its texel array for int for int face face face glenum target face target face level offset cube index width height face count complex texture types gl_rgba format type face data cube map example sky boxes a common use for cube map texture is as sky boxs a sky box is an application of texturing where the entire scene is effectively wrapped in a large cube with the viewer placed in the center as the scene is rendered anything not covered by objects within the scene is displayed as the inside of the cube with an appropriate texture it appears as if the viewer is located in the environment represented by the cube map figure a shows a cube viewed from the outside illustrating that a sky box really is just a cube with a texture applied to it in figure b we have zoomed in until the sky box cuts the near plane and we can now see inside it finally in figure c we have placed the viewer at the very center of the cube making it appear as if we are in the environment represented by the cube map a b c figure a sky box shown as seen from the outside from close up and from the center the cube map images shown in this example were taken with permission from http humus name chapter textures to render the images shown in figure we simply render a unit cube centered at the origin and use the object space position as a texture coordinate from which to sample the cube map the vertex shader for this example is shown in example and the corresponding fragment shader is shown in example example simple skybox example vertex shader version core layout location in out uniform void main void tc_rotate example simple skybox example fragment shader version core in layout location out color uniform samplercube tex void main void color texture tex using cube maps for environment mapping now that we have created an environment into which we can place the components of our scene we can make the objects appear to be part of the environment this is known as environment mapping and is another common use for cube map textures here the cube map is employed as an environment map and is used to texture objects in the scene to implement environment mapping we must calculate the texture coordinate from which to sample the cube map by reflecting the incoming view vector around the surface normal at the point to be textured the vertex shader shown in example transforms the object space position into view space by multiplying it by a concatenated model view projection matrix it also rotates the surface normal into view space by multiplying it by a concatenated model view matrix complex texture types example cube map environment mapping example vertex shader version core incoming position and normal layout location in in_position layout location in outgoing surface normal and view space position out out model view projection and model view matrices uniform uniform void main void clip space position in_position view space normal and position mat_mv mat_mv in_position xyz once the view space normal and position of the surface point have been passed into the fragment shader we can use the glsl reflect function to reflect the fragment view space position around the surface normal at each point this effectively bounces the view vector off the surface and into the cube map we use this reflected vector as a texture coordinate to sample from the cube map using the resulting texel to color the surface the result of this is that the environment appears to be reflected in the object surface the fragment shader performing these operations is shown in example example cube map environment mapping example fragment shader version core incoming surface normal and view space position in in final fragment color layout location out color the cube map texture uniform samplercube tex void main void calculate the texture coordinate by reflecting the chapter textures view space position around the surface normal tc reflect normalize sample the texture and color the resulting fragment a golden color color 83 texture tex tc the fragment shader also slightly modifies the sampled texture value retrieved from the cube map in order to make it appear to be slightly golden in color the result of rendering with the vertex and fragment shaders of examples and is shown in figure below figure a golden environment mapped torus seamless cube map sampling a cube map is a collection of six independent faces possibly aggregated into arrays of cubes with an integer multiple of six faces in total when opengl samples from a cube map as a first step it uses the dominant component of the three dimensional texture coordinate to determine which of the six faces of the cube to sample from once this face has been determined it is effectively treated as a two dimensional texture and used to look up texel values by default at the edges of the texture normal texture coordinate wrapping modes are used at first thought this would seem logical and as the generated two dimensional texture coordinates always lie within a face we don t expect to see any issues with this complex texture types however if the texture filtering mode is linear toward the edges of the cube faces the adjoining faces texels are not considered when calculating the final filtered texel values this can cause a noticeable seam to appear in the filtered texture even worse if the texture coordinate wrapping mode is left at one of the repeating modes then texels from the opposite side of the face may be used causing quite incorrect results figure shows the result of sampling from a cube map texture across the join between two faces inset is a close up view of the seam that is visible between the adjacent faces of the cube map figure a visible seam in a cube map to avoid the visible seams between adjacent faces of a cube map we can enable seamless cube map filtering to do this call glenable with cap set to when seamless cube map filtering is enabled opengl will use texels from adjacent cube map faces to retrieve texels for use in filtering this will eliminate artifacts especially when there is an abrupt change in color from one face to another or when the cube map is a particularly low resolution figure shows the result of enabling seamless cube map filtering notice that the bright line of pixels has been eliminated chapter textures figure the effect of seamless cube map filtering shadow samplers a special type of sampler is provided in glsl called a shadow sampler a shadow sampler takes an additional component in the texture coordinate that is used as a reference against which to compare the fetched texel values when using a shadow sampler the value returned from the texture function is a floating point value between and indicating the fraction of fetched texel values that passed the comparison operator for texture accesses that sample only a single texel value using the filtering mode no mipmaps and one sample per texel the returned value will be either or depending on whether the texel passes the comparison or not if more than one texel would normally be used to construct the value returned to the shader such as when the filter mode is linear or if a multisample texture is used then the value may be anything between and depending on how many of those texels pass the comparison operator the shadow texturing functions are as follows complex texture types to enable the comparison function for a sampler call glsamplerparameteri or gltexparameteri if you are not using a sampler object with pname set to and param set to and to disable it set param to when the texture comparison mode is set to the comparison is carried out with the mode specified by the sampler this is set by calling glsamplerparameteri with pname set to and param set to one of the comparison functions gl_greater or these comparison functions have the same meanings as they do for depth testing a comprehensive example of using a shadow sampler is shown in shadow mapping on page of chapter light and shadow depth stencil textures instead of an image a texture can hold depth and stencil values one of each per texel using the texture format this is the typical way a framebuffer will store the rendered z component for depth and the stencil value as discussed in detail in chapter color pixels and framebuffers when texturing from a depth stencil texture by default a shader will read the depth however as of version a shader can also read the stencil value to do so the application must set to and the shader must use an integer sampler type chapter textures buffer textures buffer textures are a special class of texture that allow a buffer object to be accessed from a shader as if it were a large one dimensional texture buffer textures have certain restrictions and differences from normal one dimensional textures but otherwise appear similar to them in your code you create them as normal texture objects bind them to texture units control their with gltexparameteri however the storage for the texture data is actually owned and controlled by a buffer object hence the name buffer texture also buffer textures have no internal sampler and sampler objects have no effect on buffer textures the main differences between buffer textures and one dimensional textures are as follows one dimensional textures have sizes limited to the value of but buffer textures are limited to the value of which is often two gigabytes or more one dimensional textures support filtering mipmaps texture coordinate wrapping and other sampler parameters buffer textures do not texture coordinates for one dimensional textures are normalized floating point values but buffer textures use unnormalized integer texture coordinates whether you decide to use a buffer texture or a one dimensional texture for a particular application will depend on your needs in order to create a buffer texture you need to generate a name for your new texture using glgentextures bind it to the texture target and then associate a buffer object with the texture using the gltexbuffer function not all texture parameters are relevant for buffer textures and as no sampler is used with buffer textures sampler parameters are essentially ignored complex texture types the code shown in example shows an example of creating a buffer initializing its data store and then associating it with a buffer texture example creating and initializing a buffer texture buffer to be used as the data store gluint buf texture to be used as a buffer texture gluint tex data is located somewhere else in this program extern const glvoid data generate bind and initialize a buffer object using the binding assume we re going to use one megabyte of data here glgenbuffers buf glbindbuffer buf glbufferdata data now create the buffer texture and associate it with the buffer object glgentextures tex glbindtexture tex gltexbuffer buf to attach only a range of a buffer object to a buffer texture you may use the gltexbufferrange function whose prototype is as follows to access a buffer texture in a shader you must create a uniform samplerbuffer or one of its signed or unsigned integer variants isamplerbuffer or usamplerbuffer and use it with the texelfetch chapter textures to read individual samples from it the texelfetch function for buffer textures is defined as follows an example of the declaration of a buffer sampler and fetching from it using texelfetch is shown in example example texel lookups from a buffer texture version core uniform samplerbuffer buf in int layout location out color void main void color texelfetch buf texture views so far we have considered textures to be large buffers of data that have a specified format and consume a fixed amount of storage space the amount of space depends on the format and on other parameters such as the texture dimensions and whether it has mipmaps or not however conceptually the format and to some extent the dimensions can be separated from the size of the underlying storage requirements of a texture for example many texture internal formats will consume the same number of bits per texel and in some cases it is possible to interpret the texelfetch function may be used with regular textures as well as buffer textures when it is used to sample from a nonbuffer texture the texture sampler parameters are ignored and the texture coordinate is still interpreted as a nonnormalized integer value as it is with buffer textures we introduce this function here solely because its most common use is with buffer textures texture views textures with various different dimensionalities perhaps taking a single slice of an array texture and treating it as a single texture say opengl allows you to share a single data store between multiple textures each with its own format and dimensions first a texture is created and its data store initialized with one of the immutable data storage functions such as next we create a texture view of the parent texture in effect this increments a reference count to the underlying storage allocated for the first texture giving each view a reference to it to create a texture view call gltextureview whose prototype is as follows when creating views of existing textures the target for the new texture must be compatible with the target of the original texture the compatible targets are given in table table target compatibility for texture views original target compatible targets rectangle rectangle buffer none chapter textures table continued target compatibility for texture views original target compatible targets 1d_array 1d_array cube_map_array 2d_array cube_map_array 2d_multisample_array 2d_multisample 2d_multisample_array in addition to target compatibility the internal format of the new view must be of the same format class i e bits per texel of the original parent texture table lists the texture format classes and their compatible specific internal formats table internal format compatibility for texture views original target compatible targets bit gl_rgba32i bit gl_rgb32ui bit gl_rg32f gl_rgba16ui gl_rg32ui gl_rgba16i gl_rg32i gl_rgba16_snorm bit gl_rgb16_snorm gl_rgb16f gl_rgb16i bit gl_r11f_g11f_b10f gl_rgb10_a2ui gl_rg16ui gl_rgba8i gl_rg16i gl_r32i gl_rgba8 gl_srgb8_alpha8 bit gl_rgb8_snorm gl_rgb8i bit gl_rg8ui gl_rg8i gl_rg8 gl_r16_snorm bit gl_r8 texture views table continued internal format compatibility for texture views original target compatible targets gl_rgtc2_rg gl_compressed_rgba_bptc_unorm gl_compressed_rgb_bptc_unsigned_float given the format and target compatibility matrices above it is possible to reinterpret data in a texture in multiple ways simultaneously for example it is possible to create two views of an texture one as unsigned normalized returning floating point data to the shader and another as an unsigned integer texture which will return the underlying integer data to the shader example shows an example of how to achieve this example creating a texture view with a new format create two texture names one will be our parent one will be the view gluint tex glgentextures tex bind the first texture and initialize its data store here the store will be x 1024 texture with mipmaps and the format will be bits per component rgb unsigned normalized glbindtexture tex 1024 1024 now create a view of the texture this time using so as to receive the raw data from the texture gltextureview tex new texture view target for the new view tex original texture new format all mipmaps only one layer as a second example consider a case where you have a large array texture and wish to take a single slice of the array and use it as an independent texture to do this we can create a view of the target chapter textures even though the original texture is example shows an example of this example creating a texture view with a new target create two texture names one will be our parent one will be the view gluint tex glgentextures tex bind the first texture and initialize its data store we are going to create a array texture with a layer size of texels and layers glbindtexture tex now create a view of the texture extracting a single slice from the middle of the array gltextureview tex new texture view target for the new view tex original texture same format as original texture all mipmaps only one layer once a view of a texture has been created it can be used in any place that you can use a texture including image loads and stores or framebuffer attachments it is also possible to create views of views and views of those views etc with each view holding a reference to the original data store it is even legal to delete the original parent texture so long as at least one view of the data exists it will not be deleted other use cases for texture views include aliasing data of various formats for example bit casting floating point and integer data to enable atomic operations and opengl logic op to be performed on floating point data which would normally not be allowed aliasing a single data store as both srgb and linear data allows a single shader to simultaneously access the same data with and without srgb conversion applied a single array texture may effectively have different format data stored in its slices by creating multiple array views of the texture and rendering different outputs to different slices of the texture with some lateral thinking applied texture views become a very powerful way to access and manage texture data texture views compressed textures compression is a mechanism by which the amount of data required to store or transmit information is reduced because texture data can consume a very large amount of memory and consequently memory bandwidth opengl supports storing textures in compressed forms in order to reduce their size compression algorithms fall into two general categories lossless and lossy lossless compression algorithms will not discard any information and an exact copy of the original is retrievable after decompression however lossy compression sacrifices some of the original information during the process in order to make the remaining information more suited to the compression algorithm and reduce its size this will reduce quality some but normally provides much greater reduction in memory cost obviously for some content such as computer executables text documents and the like it is imperative that no information is lost you may be familiar with lossless compression in the form of zip type algorithms used to compress file archives for other content though some loss in quality is acceptable for example common audio and video compression algorithms such as mpeg are lossy they throw out some information in order to improve the compression ratio a trade off is made between the acceptable loss in quality and reduced file sizes without lossy compression players and streaming video would be almost impractical for the most part the loss in fidelity is not perceptible to most think when was the last time you noticed that the music you were listening to was compressed most texture compression schemes in use today are based on lossy algorithms designed to be easy to decompress even at the expense of additional complexity in the compression side of the algorithm there are two ways to get compressed texture data into opengl the first is to ask opengl to compress it for you in this case you supply uncompressed data but specify a compressed internal format the opengl implementation will take the uncompressed raw texture data and attempt to compress it because this is a real time process the compressor in the opengl implementation will often implement a rather naive algorithm in order to compress the data quickly resulting in a poor quality compressed texture the other way to bring compressed texture data into opengl is to compress it offline i e before your program runs and pass the lossless compressors such as flac are popular for archival of digital music these algorithms normally reach compression ratios of the order of to of the original file size however for day to day use lossy algorithms such as and can reach compression ratios of or less and provide satisfactory experience to most users chapter textures compressed data to opengl directly this way you can spend as much time as is necessary to achieve the desired quality level in the resulting texture without sacrificing run time performance under either mechanism the first step is to choose a compressed internal format there are a myriad of texture compression algorithms and formats and different hardware and implementations of opengl will support different sets of formats many of which are documented in extensions to determine which formats your opengl implementation supports you can examine the implementation list of extensions although the set of formats supported by a particular implementation of opengl may well contain several proprietary and possibly undocumented compression formats two format families are guaranteed to be supported by opengl these are rgtc red green texture compression and bptc block partitioned texture compression both formats are block based and store texels in units of texel blocks this means that they store the image in blocks of texels each independently compressed such blocks can be easily decompressed by hardware as they are brought from main memory into the graphics processor texture caches if you have chosen to ask the opengl implementation to compress your texture for you all you need to do is choose the appropriate compressed internal format and specify the texel data as normal opengl will take that data and compress it as its read however if you have texel data that has been processed offline and is already in its compressed form you need to call one of the compressed texture image specification functions to establish immutable storage for the texture using a compressed format you may use the or functions described earlier you may also create a mutable store for the texture using whose prototypes are shown below compressed textures when you specify compressed data the absolute size of the data is determined by the compression format therefore all of the compressed image data functions take a parameter that specifies this size in bytes it is your application responsibility to make sure that this size is correct and that the data you give to opengl is of a valid form for the compression format that you have chosen once storage for a texture object has been established it is also possible to update parts of that texture using the following functions chapter textures filtering texture maps may be linear square or rectangular or even but after being mapped to a polygon or surface and transformed into screen coordinates the individual texels of a texture rarely correspond directly to individual pixels of the final screen image depending on the transfor mations used and the texture mapping applied a single pixel on the screen can correspond to anything from a tiny portion of a single texel magnification to a large collection of texels minification as shown in figure in either case it unclear exactly which texel values should be used and how they should be averaged or interpolated consequently opengl allows you to specify any of several filtering options to determine these calculations the options provide different trade offs between speed and image quality also you can specify the filtering methods to be used for magnification and minification independently in some cases it isn t obvious whether magnification or minification is called for if the texture map needs to be stretched or shrunk in both the x and y directions then magnification or minification is needed if filtering figure effect of texture minification and magnification the texture map needs to be stretched in one direction and shrunk in the other opengl makes a choice between magnification and that in most cases gives the best result possible it best to try to avoid these situations by using texture coordinates that map without such distortion linear filtering linear filtering is a technique in which a coordinate is used to select adjacent samples from a discretely sampled signal and replace that signal with a linear approximation of the original consider the signal shown in figure figure resampling of a signal in one dimension in figure the signal represented by the solid line has been discretely sampled at the points shown by the large dots the original signal cannot be reconstructed by placing a straight line between each of the dots in some areas of the signal the linear reconstruction matches the original signal reasonably well however in other areas the reconstruction is not faithful to the original and sharp peaks that were present before resampling are lost when a texture is enlarged by different amounts in the horizontal and vertical axes this is referred to as anisotropic filtering this is exposed by some opengl implementations in the form of an extension however this is not part of core opengl chapter textures for image data the same technique can be applied so long as the sampling rate resolution of the texture is high enough relative to the sharp peaks in the image data details a linear reconstruction of the image will appear to have reasonably high quality the translation from a signal as shown in figure into a texture is easy to conceive when a texture is considered simply place the samples into a texture and reconstruct the original image from those samples as needed to do this opengl takes the texture coordinate that you pass it as a floating point number and finds the two samples that lie closest to it it uses the distance to each of those two points to create weights for each of the samples and then uses those weights to create a weighted average of them because linear resampling is opengl can apply this technique first in one dimension and then again in a second dimension in order to reconstruct images and even a third time for textures figure illustrates the process as applied to a image figure bilinear resampling a separable operation is one that can be deconstructed into two or more usually similar passes over the data in this case we can apply one pass per dimension of the image data filtering not only can linear filtering be used to smoothly transition from one sample to the adjacent ones in and textures it can also be used to blend texels sampled from adjacent mipmap levels in a texture this works in a very similar manner to that described above opengl calculates the mipmap level from which it needs to select samples and the result of this calculation will often be a floating point value with a fractional component this is used just as a fractional texture coordinate is used to filter spatially adjacent texels the two closest mipmaps are used to construct a pair of samples and the fractional part of the level of detail calculation is used to weight the two samples into an average all of these filtering options are controlled by the texture filter modes in opengl sampler objects as explained in sampler objects on page the sampler object represents a collection of parameters that control how texels are read from textures two of those parameters and control how opengl filters textures the first is used when the texture is magnified that is when the level of detail required is of a higher resolution than the highest resolution mipmip level by default level and represents cases where the mipmip calculation produces a level less than or equal to zero because under magnification only one mipmap level is used only two choices are available for these are and the first disables filtering and returns the nearest texel to the sample location the second enables linear filtering texture minification is where mipmapping takes effect and this is explained in some detail in the following sections advanced from a signaling theory perspective a texture needs to sample the original signal at at least twice the frequency of the highest frequency data present the original should be low pass filtered to some frequency then sampled at greater than twice that frequency this gives enough samples to exactly reconstruct the original image however linear filtering fails to do this reconstruction and can lead to aliasing also if the original filtering and sampling are not done aliasing and other artifacts can occur this is discussed in more detail in chapter procedural texturing and mipmapping as one technique for dealing with it is described below you can also do custom filtering using texture gathers to improve over the artifacts of linear filtering gathering texels is discussed later in this chapter chapter textures using and generating mipmaps textured objects can be viewed like any other objects in a scene at different distances from the viewpoint in a dynamic scene as a textured object moves farther from the viewpoint the ratio of pixels to texels in the texture becomes very low and the texture ends up being sampled at a very low rate this has the effect of producing artifacts in the rendered image due to undersampling of the texture data for example to render a brick wall you may use a large texture image say 1024 1024 texels when the wall is close to the viewer but if the wall is moved farther away from the viewer until it appears on the screen as a single pixel then the sampled texture may appear to change abruptly at certain transition points to reduce this effect we can pre filter the texture map and store the pre filtered images as successively lower and lower resolution versions of the full resolution image these are called mipmaps and are shown in figure the term mipmap was coined by lance williams when he introduced the idea in his paper pyramidal parametrics siggraph proceedings mip stands for the latin multum in parvo meaning many things in a small place mipmapping uses some clever methods to pack image data into memory when using mipmapping opengl automatically determines which resolution level of the texture map to use based on the size in pixels of the object being mapped with this approach the level of detail in the texture map is appropriate for the image that drawn on the screen as the image of the object gets smaller the size of the texture map decreases mipmapping requires some extra computation and texture storage area however when it not used textures that are mapped onto smaller objects might shimmer and flash as the objects move this description of opengl mipmapping avoids detailed discussion of the scale factor known as λ between texel size and polygon size this description also assumes default values for parameters related to mipmapping to see an explanation of λ and the effects of mipmapping parameters see calculating the mipmap level on page additional details on controlling λ from your application can be found in mipmap level of detail control on page filtering figure a pre filtered mipmap pyramid the parameter controls how texels are constructed when the mipmap level is greater than zero there are a total of six settings available for this parameter the first two are the same as for magnification and choosing one of these two modes disables mipmapping and causes opengl to only use the base level level of the texture the other four modes enable mipmapping and control how the mipmaps are used the four values are and notice how each mode is made up of two parts and the token names are structured as a b here a and b may both be either chapter textures nearest or linear the first part a controls how the texels from each of the mipmap levels is constructed and works the same way as the setting the second b controls how these samples are blended between the mipmap levels when it nearest only the closest mipmap level is used when it linear the two closest mipmaps are linearly interpolated to illustrate the effect of the parameter on a mipmapped texture figure shows how each affects a simple checker type pattern at different resolutions in a mipmap pyramid notice how with the intra mipmap filter specified as nearest as in and the checkerboard pattern becomes quite evident whereas when it is linear as in and it is less well defined and the texture appears blurred likewise when the inter mipmap filter mode is nearest as in nearest and the boundary between the mipmap levels is visible however when the inter mipmap filter is linear as in and that boundary is hidden by filtering figure effects of minification mipmap filters top left nearest top right bottom left and bottom right filtering to use mipmapping you must provide all sizes of your texture in powers of between the largest size and a map if you don t intend to use mipmapping to go all the way to a texture you can set the value of to the maximum level you have supplied and opengl will not consider any further levels in its evaluation of texture completeness if the highest resolution level of the texture is not square one dimension will reach one texel in size before the other in this case continue making new levels with that dimension sized to one texel until the level becomes texel in size for example if your highest resolution map is you must also provide maps of size and the smaller maps are typically filtered and down sampled versions of the largest map in which each texel in a smaller texture is a weighted average of the corresponding texels in the higher resolution texture since opengl doesn t require any particular method for calculating the lower resolution maps the differently sized textures could be totally unrelated in practice unrelated textures would make the transitions between mipmaps extremely noticeable as in figure figure illustration of mipmaps using unrelated colors the image in figure was generated by creating a texture and filling each of its mipmap levels with a different color the highest resolution level was filled with red then green blue yellow and so on chapter textures down the mipmap pyramid this texture was applied to a large plane extending into the distance the further the plane gets from the viewer the narrower it becomes in screen space and the more compressed the texture becomes opengl chooses successively higher mipmap levels lower resolution levels from the texture to further illustrate the effect the example sets the mipmap filtering mode to nearest and applies a bias to the calculated mipmap level to specify these textures allocate the texture using and then call once for each resolution of the texture map with different values for the level width height and image parameters starting with zero level identifies which texture in the series is specified with the previous example the highest resolution texture of size would be declared with level the texture with level and so on in addition for the mipmapped textures to take effect you need to choose one of the mipmapped minification filters as described earlier opengl provides a function to automatically generate all of the mipmaps for a texture under application control this function is called glgeneratemipmap and it is up to the opengl implementation to provide a mechanism to downsample the high resolution images to produce the lower resolution mipmaps this will often be implemented internally by using a shader or perhaps the texture filtering hardware the technique used will generally be designed for performance over quality and will vary from implementation to implementation if you want high quality well defined results it is best to generate and supply the mipmap images yourself however if you need to quickly generate a mipmap chain and are satisfied with whatever results you get you can rely on glgeneratemipmap for this purpose filtering calculating the mipmap level the computation of which mipmap level of a texture to use for a particular pixel depends on the scale factor between the texture image and the size of the polygon to be textured in pixels let call this scale factor ρ and also define a second value λ where λ ρ lodbias since texture images can be multidimensional it is important to clarify that ρ is the maximum scale factor of all dimensions lodbias is the level of detail bias for the sampler a constant value set by calling glsamplerparameteri with the pname parameter set to and is used to adjust λ by default lodbias which has no effect it best to start with this default value and adjust in small amounts if needed if λ then the texel is smaller than the pixel and so a magnification filter is used if λ then a minification filter is used if the minification filter selected uses mipmapping then λ indicates the mipmap level the minification to magnification switchover point is usually at λ but not always the choice of mipmapping filter may shift the switchover point for example if the texture image is texels and the polygon size is pixels then ρ not and therefore λ if the texture image is texels and the polygon size is pixels then ρ x scales by y by use the maximum value and therefore λ the equations for the calculation of λ and ρ are as follows λbase x y ρ x y λi x y λbase clamp biastexobj biasshader the calculation of mipmap level can be further controlled by a number of sampler parameters in particular the parameter may be used to bias λ once λ has been calculated it may be clamped into a user specified range which is given by the parameters and which are specified by passing those token values to glsamplerparameterf or to gltexparameterf if sampler objects are not in use the default values for and are and respectively allowing them to effectively pass through any value the values of and are represented by lodmin and lodmax in the following equation λ lodmax λi lodmax λi lodmin λi lodmax lodmin λi lodmin chapter textures the default parameters for filter and are and respectively notice that the default minification filter enables mipmapping this is important because in order to use mipmapping the texture must have a complete set of mipmap levels and they must have a con sistent set of resolutions as described in using and generating mipmaps on page otherwise the texture is considered in complete and will not return useful data to the shader textures allocated using the function are always com plete so you don t need to worry about that but these textures will still contain no data when they are newly created this is a common source of errors for new opengl programmers they forget to either change the filtering mode or fill in the mipmaps for newly created textures resulting in their texturing code not working mipmap level of detail control in addition to the parameters controlling lodmin lodmax and λbase during the calculation of λ further control over the selected level of the mipmap pyramid is provided through the and parameters which may be set using glsamplerparameteri specifies the lowest mipmap level i e highest resolution that will be sampled regardless of the value of λ whereas specifies the highest mipmap level i e lowest resolution that will be sampled this can be used to constrain sampling to a subset of the mipmap pyramid one potential use for is texture streaming when using texture streaming storage for the complete texture object is allocated using a function such as but the initial data is not loaded as the application runs and new objects come into view their texture data is loaded from lowest to highest resolution mipmap to ensure that something meaningful is displayed to the user even when the complete texture has not yet been loaded the value of base_level can be set to the highest resolution mipmap level that has been loaded so far that way as more and more texture data is loaded objects on the screen achieve higher and higher fidelity filtering advanced texture lookup functions in addition to simple texturing functions such as texture and texelfetch several more variants of the texture fetch functions are supported by the shading language these are covered in this subsection explicit level of detail normally when using mipmaps opengl will calculate the level of detail and the resulting mipmap levels from which to sample for you see calculating the mipmap level on page for more details on how opengl calculates mipmap levels however it is possible to override this calculation and specify the level of detail explicitly as an argument to the texture fetch function the texturelod function takes this lod parameter in place of the bias parameter that would normally be optionally supplied to the texture function like other texture functions supported by glsl texturelod has many overloaded prototypes for the various types and dimensionalities of the supported sampler types some key prototypes of texturelod are as follows a full list is in appendix c built in glsl variables and functions notice that because they don t support mipmaps samplerbuffer and samplerrect are missing from the supported sampler types for texturelod explicit gradient speciﬁcation it is also possible to override the level of detail calculation for mipmapping at an earlier part of the process rather than explicitly giving the level of detail parameter directly when the gradient texture functions are used chapter textures the partial derivative of the texture coordinates is given as a parameter some key prototypes are listed below a full list is in appendix c built in glsl variables and functions in the texturegrad functions the variable ρ as described in calculating the mipmap level on page is essentially passed in using dpdx and dpdy this can be useful when an analytic function for the derivative of a texture coordinate may be known or when a function that is not the derivative of the texture coordinate is required texture fetch with offsets some applications require a number of texels around a region of interest or may need to offset the texture coordinates slightly during sampling glsl includes functions for doing this that will likely be more efficient than physically offsetting the texture coordinates in the shader this functionality is exposed through an overloaded set of texture lookup functions called textureoffset with some example prototypes as follows a full list is in appendix c built in glsl variables and functions advanced texture lookup functions notice that for the textureoffset function the offset parameter is an integer value in fact this must be a constant expression and must be with a limited range this range is given by the built in glsl constants and projective texturing projective texturing is employed when a perspective transformation matrix has been used to transform texture coordinates the input to the transform is a set of homogeneous coordinates and the resulting output of this transform is a vector whose last component is unlikely to be the textureproj function can be used to divide through by this final component projecting the resulting texture coordinate into the coordinate space of the texture this is useful for techniques such as projecting decals onto flat surfaces e g the halo projected by a flashlight or in shadow some example prototypes are given below a full list is in appendix c built in glsl variables and functions an in depth example of shadow mapping is given in shadow mapping on page chapter textures texture queries in shaders the following two built in glsl functions don t actually read from the texture but return information about the texture or about how it will be processed the first function texturequerylod retrieves mipmap information calculated by the fixed function texture lookup hardware for each of these texturequerylod functions there is a corresponding query texturequerylevels that returns the number of mipmap levels present advanced texture lookup functions sometimes it may be necessary to know the dimensions of a texture from which you are about to sample for example you may need to scale an integer texture coordinate representing an absolute texel location into a floating point range suitable for sampling from the texture or to iterate over all the samples in a texture the texturesize function will return the dimensions of the texture at a specified level of detail its prototype is as follows a full list is in appendix c built in glsl variables and functions chapter textures gathering texels the texturegather function is a special function that allows your shader to read the four samples that would have been used to create a bilinearly filtered texel from a texture or cube map rectangle texture or array of these types typically used with single channel textures the optional comp component of the function allows you to select a channel other than the x or r component of the underlying data this function can provide significant performance advantages when you need to sample many times from a single channel of a texture because depending on the desired access pattern it is possible to use this function to cut the number of texture lookups by three quarters combining special functions in addition to all of the special texturing functions several more variants of these functions exist that combine features from multiple variants for example if you want to do projective texturing with an explicit level of detail or gradients each is described in explicit gradient specification on page then you can use the combined functions textureprojlod or textureprojgrad respectively the combined functions using a sampler are shown below variants of almost all of these functions exist for other dimensionalities and types of sampler and a full list is in appendix c built in glsl variables and functions advanced texture lookup functions textureprojlod tex p float lod textureprojgrad tex p dpdx dpdy textureprojoffset tex p offset float bias texturegradoffset tex p dpdx dpdy offset textureprojlodoffset tex p float lod offset textureprojgradoffset tex p dpdx dpdy offset advanced texture lookup functions may be combined to perform more than one special function in a single call textureprojlod performs projective texturing from the texture bound to the unit represented bytex as would be performed by textureproj but with explicit level of detail specified in lod as accepted by texturelod similarly textureprojgrad executes a projective texture lookup as performed by textureproj but with explicit gradients passed in dpdx and dpdy as would be accepted by texturegrad textureprojoffset performs a projective texture lookup with texel offsets applied to the post projected texture coordinates textureprojlodoffset and textureprojgradoffset further combine two special functions the first performs a projective texture fetch with explicit level of detail and texel offsets as accepted by textureoffset and the second performs a projective texture lookup with explicit gradients and texel offsets point sprites point sprites are essentially opengl points rendered using a fragment shader that takes the fragment coordinates within the point into account when running the coordinate within the point is available in the two dimensional vector this variable can be used in any number of ways two common uses are to use it as a texture coordinate this is the classic origin of the term point sprite or to use it to analytically compute color or coverage the following are a few examples of how to use the vector to produce interesting effects in the fragment shader chapter textures textured point sprites by using to lookup texels in a texture in the fragment shader simple point sprites can be generated each point sprite simply shows the texture as a square example is the vertex shader used in the example notice that we re writing to in the vertex shader this is to control the size of the point sprites they re scaled relative to their distance from the near plane here we ve used a simple linear mapping but more complex logarithmic mappings can be used example simple point sprite vertex shader uniform uniform layout location in position void main void pos position pos z pos w pos example shows the fragment shader used in this example not including the declaration of the texture and the output vector it a single line of real code we simply look up into the texture using as a texture coordinate example simple point sprite fragment shader uniform out color void main void color texture when we render points randomly placed in a two unit cube centered on the origin we get the result shown in figure point sprites figure result of the simple textured point sprite example analytic color and shape you are not limited to sourcing your point sprite data from a texture textures have a limited resolution but can be quite precise the shader shown in example demonstrates how you can analytically determine coverage in the fragment shader this shader centers around the origin and then calculates the squared distance of the fragment from the center of the point sprite if it greater than the square root of half the width of the sprite or the radius of a circle that just fits inside it then the fragment is rejected using the discard keyword otherwise we interpolate between two colors to produce the final output this produces a perfect circle note that the same vertex shown in example is used for this example as well example analytic shape fragment shader out color void main void const const temp float f dot temp temp if f discard color mix color2 smoothstep f chapter textures figure shows the output of this example figure analytically calculated point sprites by increasing the size of the point sprite and reducing the number of points in the scene it is possible to see the extremely smooth edges of the discs formed by the fragment shader as shown in figure figure smooth edges of circular point sprites point sprites controlling the appearance of points various controls exist to allow the appearance of points to be tuned by your application these parameters are set using glpointparameterf or glpointparameteri the two parameters that you can change with glpointparameteri or glpointparameterf are the origin for using the point fade threshold using the point sprite coordinate origin controls whether y increases from top down or bottom up in the fragment shader as points are rasterized by default the value of is meaning that it increases from top down note that this goes in the opposite direction to window coordinates which have their origin in the lower right by specifying for you can make y increase in the same direction as y which represents the fragment window coordinate the other parameter that can be changed controls how points and point sprites are antialiased when the size of a point falls below this threshold opengl has the option to stop performing true antialiasing and instead fade the point into the background using blending the default value of this parameter is which means that if a point whose size is less than is rasterized rather than only lighting a single sample within each fragment it may light all the fragments in that sample but end up having its alpha component chapter textures attenuated by the point fade factor which is computed as follows fade if threshold otherwise rendering to texture maps in addition to using framebuffer objects as described in framebuffer objects on page in chapter for offscreen rendering you can also use fbos to update texture maps you might do this to indicate changes in the texture for a surface such as damage to a wall in a game or to update values in a lookup table if you re doing gpgpu like computations in these cases you bind a level of a texture map as a framebuffer attachment after rendering the texture map can be detached from the framebuffer object and used for subsequent rendering note nothing prevents you from reading from a texture that is simultaneously bound as a framebuffer attachment for writing in this scenario called a framebuffer rendering loop the results are undefined for both operations that is the values returned from sampling the bound texture map as well as the values written into the texture level while bound are undefined and likely incorrect rendering to texture maps the glframebuffertexture family of routines attaches levels of a texture map as a framebuffer attachment glframebuffertexture attaches level of texture object texture assuming texture is not zero to attachment and each attach a specified texture image of a texture object as a rendering attachment to a framebuffer object target must be either or which is equivalent to attachment must be one of the framebuffer attachment points or in which case the internal format of the texture must be for texturetarget must be if texture is not zero for texturetarget must be gl_texture_cube_map_negative_x gl_texture_cube_map_negative_z and for texturetarget must be if texture is zero indicating that any texture bound to attachment is released no subsequent bind to attachment is made in this case texturetarget level and layer are ignored if texture is not zero it must be the name of an existing texture object created with glgentextures with texturetarget matching the texture type e g etc associated with the texture object or if texture is a cube map then texturetarget must be one of the cube map face targets otherwise a error is generated level represents the mipmap level of the associated texture image to be attached as a render target and for three dimensional textures or two dimensional texture arrays layer represents the layer of the texture to be used if texturetarget is or then level must be zero chapter textures example attaching a texture level as a framebuffer attachment fbotexture cpp glsizei texwidth texheight gluint framebuffer texture void init gluint renderbuffer create an empty texture glgentextures texture glbindtexture texture gl_rgba8 texwidth texheight gl_rgba gl_unsigned_byte null create a depth buffer for our framebuffer glgenrenderbuffers renderbuffer glbindrenderbuffer renderbuffer glrenderbufferstorage texwidth texheight attach the texture and depth buffer to the framebuffer glgenframebuffers framebuffer glbindframebuffer framebuffer texture glframebufferrenderbuffer gl_depth_attachment renderbuffer glenable void display render into the renderbuffer glbindframebuffer framebuffer glviewport texwidth texheight glclearcolor glclear generate mipmaps of our texture glgeneratemipmap bind to the window system framebuffer unbinding from the texture which we can use to texture other objects glbindframebuffer glviewport windowwidth windowheight glclearcolor glclear render using the texture glenable glutswapbuffers rendering to texture maps for three dimensional or one and two dimensional texture arrays you can also attach a single layer of the texture as a framebuffer attachment discarding rendered data advanced as a rule of thumb you should always clear the framebuffer before you begin rendering a frame modern gpus implement compression and other techniques to improve performance memory bandwidth requirements and so on when you clear a framebuffer the opengl implementation knows that it can discard any rendered data in the framebuffer and return it to a clean compressed state if possible however what happens if you re sure that you re about to render over the whole framebuffer it seems that clearing it would be a waste as you are about to draw all over the cleared area if you are certain that you are going to completely replace the contents of the framebuffer with new rendering you can discard it with a call to glinvalidateframebuffer or glinvalidatesubframebuffer their prototypes are as follows chapter textures discarding the content of a framebuffer can be far more efficient than clearing it depending on the opengl implementation furthermore this can eliminate some expensive data copies in systems with more than one gpu if rather than discarding the content of the attachments of a framebuffer object you wish to discard the content of a texture directly you can call glinvalidateteximage or glinvalidatetexsubimage the prototypes for glinvalidateteximage and glinvalidatetexsubimage are as follows rendering to texture maps chapter summary in this chapter we have given an overview of texturing in opengl applications of textures in computer graphics are wide ranging and surprisingly complex the best that can be done in a single chapter of a book is to scratch the surface and hopefully convey to the reader the depth and usefulness of textures entire books could be written on advanced uses of textures more information about textures can be found in subsequent chapters including examples of how to draw into textures use buffer textures and store nonimage data in textures texture redux to use a texture in your program create a texture by reserving a name for a texture using glgentextures how about binding its name to the appropriate binding point using glbindtexture specifying the dimensions and format of the texture using or the appropriate function for the specified texture target placing data into the texture using or the appropriate function for the specified texture target access the texture in your shader by declaring a uniform sampler in your shader to represent the texture associating the sampler with the desired texture unit using gluniform1i binding the texture object and optionally a sampler object to the correct texture unit reading from the texture in the shader using texture or one of other the built in texture functions to use a buffer object as a texture create a buffer texture by generating a texture name using glgentextures binding that name to the texture target create and initialize a buffer texture by generating a buffer name using glgenbuffers chapter textures binding the buffer to a target preferably the target defining the storage for the buffer object using glbufferdata attach the buffer object data store to the texture by binding the texture to the target and calling gltexbuffer with the name of the initialized buffer object texture best practices here are some tips to ensure that you allow opengl to use your textures most efficiently ensuring the best possible performance for your application some common pitfalls are enumerated here with some advice on how to avoid them immutable texture storage use immutable texture storage for textures wherever possible immutable storage for textures is created using the function or the appropriate one for your chosen texture target mutable storage is created by calling when a texture is marked as immutable the opengl implementation can make certain assumptions about the validity of a texture object for example the texture will always be complete mipmaps create and initialize the mipmap chain for textures unless you have a good reason not to allowing the graphics hardware to use a lower resolution mipmap when it needs to will not only improve the image quality of your program rendering but it will also make more efficient use of the caches in the graphics processor the texture cache is a small piece of memory that is used to store recently accessed texture data the smaller the textures your application uses the more of them will fit into the cache and the faster your application will run integer format textures don t forget to use an integer sampler usampler3d etc in your shader when your texture data is an unnormalized integer and you intend to use the integer values it contains directly in the shader a common mistake is to create a floating point sampler and use an integer internal format for the sampler such as in this case you may get undesired or even undefined results chapter summary this page intentionally left blank chapter light and shadow chapter objectives after reading this chapter you ll be able to do the following code a spectrum of fragment shaders to light surfaces with ambient diffuse and specular lighting from multiple light sources migrate lighting code between fragment and vertex shaders based on quality and performance trade offs use a single shader to apply a collection of lights to a variety of materials select from a variety of alternative lighting models have the objects in your scene cast shadows onto other objects in the real world we see things because they reflect light from a light source or because they are light sources themselves in computer graphics just as in real life we won t be able to see an object unless it is illuminated by or emits light we will explore how the opengl shading language can help us implement such models so that they can execute at interactive rates on programmable graphics hardware this chapter contains the following major sections classic lighting model shows lighting fundamentals first based on doing light computations in a fragment shader then in both the vertex and fragment shaders this section also shows how to handle multiple lights and materials in a single shader advanced lighting models introduces a sampling of advanced methods for lighting a scene including hemisphere lighting image based lighting and spherical harmonics these can be layered on top of the classic lighting model to create hybrid models shadow mapping shows a key technique for adding shadows to a scene lighting introduction the programmability of opengl shaders allows virtually limitless possibilities for lighting a scene old school fixed functionality lighting models were comparatively constraining lacking in some realism and in performance quality trade offs programmable shaders can provide far superior results especially in the area of realism nevertheless it is still important to start with an understanding of the classic lighting model that was embodied by old fixed functionality though we will be more flexible on which shader stages do which part this lighting model still provides the fundamentals on which most rasterization lighting techniques are based and is a springboard for grasping the more advanced techniques in that light we will first show a number of simple shaders that each perform some aspect of the classic lighting model with the goal being that you may pick and choose the techniques you want in your scene combine them and incorporate them into your shaders viewing transformations and other aspects of rendering are absent from these shaders so that we may focus just on lighting in the later examples in this chapter we explore a variety of more complex shaders that provide more flexible results but even with these more flexible shaders we are limited only by our imaginations keep exploring new lighting methods on your own chapter light and shadow classic lighting model the classic lighting model adds up a set of independently computed lighting components to get a total lighting effect for a particular spot on a material surface these components are ambient diffuse and specular each is described below and figure shows them visually figure elements of the classic lighting model ambient top left plus diffuse top right plus specular bottom light adding to an overall realistic effect ambient light is light not coming from any specific direction the classic lighting model considers it a constant throughout the scene forming a decent first approximation to the scattered light present in a scene computing it does not involve any analysis of the direction of light sources or the direction of the eye observing the scene it could either be accumulated as a base contribution per light source or be pre computed as a single global effect diffuse light is light scattered by the surface equally in all directions for a particular light source diffuse light is responsible for being able to see a surface lit by a light even if the surface is not oriented to reflect the light source directly toward your eye it doesn t matter which direction the eye is but it does matter which direction the light is it is brighter when the surface is more directly facing the light source simply because that orientation collects more light than an oblique orientation diffuse light classic lighting model computation depends on the direction of the surface normal and the direction of the light source but not the direction of the eye it also depends on the color of the surface specular highlighting is light reflected directly by the surface this highlighting refers to how much the surface material acts like a mirror a highly polished metal ball reflects a very sharp bright specular highlight while a duller polish reflects a larger dimmer specular highlight and a cloth ball would reflect virtually none at all the strength of this angle specific effect is referred to as shininess computing specular highlights requires knowing how close the surface orientation is to the needed direct reflection between the light source and the eye hence it requires knowing the surface normal the direction of the light source and the direction of the eye specular highlights might or might not incorporate the color of the surface as a first approximation it is more realistic to not involve any surface color making it purely reflective the underlying color will be present anyway from the diffuse term giving it the proper tinge fragment shaders for different light styles we ll next discuss how fragment shaders compute the ambient diffuse and speculative amounts for several types of light including directional lighting point lighting and spotlight lighting these will be complete with a vertex and fragment shader pair built up as we go from simplest to more complex the later shaders may seem long but if you start with the simplest and follow the incremental additions it will be easy to understand note the comments in each example highlight the change or difference from the previous step making it easy to look and identify the new concepts no lighting we ll start with the simplest lighting no lighting by this we don t mean everything will be black but rather that we ll just draw objects with color unmodulated by any lighting effects this is inexpensive occasionally useful and is the base we ll build on unless your object is a perfect mirror you ll need this color as the basis for upcoming lighting calculations all lighting calculations will somehow modulate this base color it is a simple matter to set a per vertex color in the vertex shader that will be interpolated and displayed by the fragment shader as shown in example chapter light and shadow example setting final color values with no lighting vertex shader vertex shader with no lighting version core uniform mvpmatrix model view projection transform in vertexcolor sent from the application includes alpha in vertexposition pre transformed position out color sent to the rasterizer for interpolation void main color vertexcolor mvpmatrix vertexposition fragment shader fragment shader with no lighting version core in color interpolated between vertices out fragcolor color result for this fragment void main fragcolor color in the cases of texture mapping or procedural texturing the base color will come from sending texture coordinates instead of a color using those coordinates to manifest the color in the fragment shader or if you set up material properties the color will come from an indexed material lookup either way we start with an unlit base color ambient light the ambient light doesn t change across primitives so we will pass it in from the application as a uniform variable it a good time to mention that light itself has color not just intensity the color of the light interacts with the color of the surface being lit this interaction of the surface color by the light color is modeled well by multiplication using to represent black and to represent full classic lighting model intensity enables multiplication to model expected interaction this is demonstrated for ambient light in example it is okay for light colors to go above though especially as we start adding up multiple sources of light we will start now using the min function to saturate the light at white this is important if the output color is the final value for display in a framebuffer however if it is an interme diate result skip the saturation step now and save it for application to a final color when that time comes example ambient lighting vertex shader vertex shader for ambient light version core uniform mvpmatrix in vertexcolor in vertexposition out color void main color vertexcolor mvpmatrix vertexposition fragment shader fragment shader for global ambient lighting version core uniform ambient sets lighting level same across many vertices in color out fragcolor void main scatteredlight ambient this is the only light modulate surface color with light but saturate at white fragcolor min color scatteredlight you probably have an alpha fourth component value in your color that you care about and don t want it modified by lighting so unless you re after specific transparency effects make sure your ambient color has as an alpha chapter light and shadow of or just include only the r g and b components in the computation for example the two lines of code in the fragment shader could read scatteredlight ambient this is the only light rgb min color rgb scatteredlight fragcolor rgb color a which passes the color alpha component straight through to the output fragcolor alpha component modifying only the r g and b components we will generally do this in the subsequent examples a keen observer might notice that scatteredlight could have been multiplied by color in the vertex shader instead of the fragment shader for this case the interpolated result would be the same since the vertex shader usually processes fewer vertices than the number of fragments processed by the fragment shader it would probably run faster too however for many lighting techniques the interpolated results will not be the same higher quality will be obtained by computing per fragment rather than per vertex it is up to you to make this performance vs quality trade off probably by experimenting with what is best for a particular situation we will first show the computation in the fragment shader and then discuss optimizations approximations that involve moving computations up into the vertex shader or even to the application feel free to put them whereever is best for your situation directional light if a light is far far away it can be approximated as having the same direction from every point on our surface we refer to such a light as directional similarly if a viewer is far far away the viewer eye can also be approxi mated as having the same direction from every point on our surface these assumptions simplify the math so the code to implement a directional light is simple and runs faster than the code for other types of lights this type of light source is useful for mimicking the effects of a light source like the sun we ll start with the ambient light computation from the previous example and add on the effects for diffuse scattering and specular highlighting we compute these effects for each fragment of the surface we are lighting again just like with ambient light the directional light will have its own color and we will modulate the surface color with this light color for the diffuse scattering the specular contribution will be computed separately to allow the specular highlights to be the color of the light source not modulated by the color of the surface the scattered and reflected amounts we need to compute vary with the cosine of the angles involved two vectors in the same direction form an angle of with a cosine of this indicates a completely direct classic lighting model reflection as the angle widens the cosine moves toward indicating less reflected light fortunately if our vectors are normalized having a length of these cosines are computed with a simple dot product as shown in example the surface normal will be interpolated between vertices though it could also come from a texture map or an analytic computation the far away light source assumption lets us pass in the light direction as the uniform variable lightdirection for a far away light and eye the specular highlights all peak for the same surface normal direction we compute this direction once in the application and pass it in through the uniform variable halfvector then cosines of this direction with the actual surface normal are used to start specular highlighting shininess for specular highlighting is measured with an exponent used to sharpen the angular fall off from a direct reflection squaring a number less than but near to makes it closer to higher exponents sharpen the effect even more that is leaving only angles near whose cosine is near with a final specular value near the other angles decay quickly to a specular value of hence we see the desired effect of a shiny spot on the surface overall higher exponents dim the amount of computed reflection so in practice you ll probably want to use either a brighter light color or an extra multiplication factor to compensate we pass such defining specular values as uniform variables because they are surface properties that are constant across the surface the only way either a diffuse reflection component or a specular reflection component can be present is if the angle between the light source direction and the surface normal is in the range a normal at means the surface itself is edge on to the light tip it a bit further and no light will hit it as soon as the angle grows beyond the cosine goes below we determine the angle by examining the variable diffuse this is set to the greater of and the cosine of the angle between the light source direction and the surface normal if this value ends up being the value that deter mines the amount of specular reflection is set to as well recall we assume that the direction vectors and surface normal vector are normalized so the dot product between them yields the cosine of the angle between them example directional light source lighting vertex shader vertex shader for a directional light computed in the fragment shader version core uniform mvpmatrix uniform normalmatrix to transform normals pre perspective chapter light and shadow in vertexcolor in vertexnormal we now need a surface normal in vertexposition out color out normal interpolate the normalized surface normal void main color vertexcolor transform the normal without perspective and normalize it normal normalize normalmatrix vertexnormal mvpmatrix vertexposition fragment shader fragment shader computing lighting for a directional light version core uniform ambient uniform lightcolor uniform lightdirection direction toward the light uniform halfvector surface orientation for shiniest spots uniform float shininess exponent for sharping highlights uniform float strength extra factor to adjust shininess in color in normal surface normal interpolated between vertices out fragcolor void main compute cosine of the directions using dot products to see how much light would be reflected float diffuse max dot normal lightdirection float specular max dot normal halfvector surfaces facing away from the light negative dot products won t be lit by the directional light if diffuse specular else specular pow specular shininess sharpen the highlight scatteredlight ambient lightcolor diffuse reflectedlight lightcolor specular strength classic lighting model don t modulate the underlying color with reflected light only with scattered light rgb min color rgb scatteredlight reflectedlight fragcolor rgb color a a couple more notes about this example first in this example we used a scalar strength to allow independent adjustment of the brightness of the specular reflection relative to the scattered light this could potentially be a separate light color allowing per channel red green or blue control as will be done with material properties a bit later in example second near the end of example it is easy for these lighting effects to add up to color components greater than again usually you ll want to keep the brightest final color to so we use the min function also note that we already took care to not get negative values as in this example we caught that case when we found the surface facing away from the light unable to reflect any of it however if negative values do come into play you ll want to use the clamp function to keep the color components in the range finally some interesting starting values would be a shininess of around for a pretty tight specular reflection with a strength of around to make it bright enough to stand out and with ambient colors around and lightcolor colors near that should make something interesting and visible for a material with color near as well and then you can fine tune the effect you want from there point lights point lights mimic lights that are near the scene or within the scene such as lamps or ceiling lights or street lights there are two main differences between point lights and directional lights first with a point light source the direction of the light is different for each point on the surface so cannot be represented by a uniform direction second light received at the surface is expected to decrease as the surface gets farther and farther from the light this fading of reflected light based on increasing distance is called attenuation reality and physics will state that light attenuates as the square of the distance however this attenuation normally fades too fast unless you are adding on light from all the scattering of surrounding objects and otherwise completely modeling everything physically happening with light in the classic model the ambient light helps fill in the gap from not doing a full modeling and attenuating linearly fills it in some more so we will show an attenuation model that includes coefficients for constant linear and quadratic functions of the distance chapter light and shadow the additional calculations needed for a point light over a directional light show up in the first few lines of the fragment shader in example the first step is to compute the light direction vector from the surface to the light position we then compute light distance by using the length function next we normalize the light direction vector so we can use it in a dot product to compute a proper cosine we then compute the attenuation factor and the direction of maximum highlights the remaining code is the same as for our directional light shader except that the diffuse and specular terms are multiplied by the attenuation factor example point light source lighting vertex shader vertex shader for a point light local source with computation done in the fragment shader version core uniform mvpmatrix uniform mvmatrix now need the transform minus perspective uniform normalmatrix in vertexcolor in vertexnormal in vertexposition out color out normal out position adding position so we know where we are void main color vertexcolor normal normalize normalmatrix vertexnormal position mvmatrix vertexposition pre perspective space mvpmatrix vertexposition includes perspective fragment shader fragment shader computing a point light local source lighting version core uniform ambient uniform lightcolor uniform lightposition location of the light eye space uniform float shininess uniform float strength uniform eyedirection uniform float constantattenuation attenuation coefficients classic lighting model uniform float linearattenuation uniform float quadraticattenuation in color in normal in position out fragcolor void main find the direction and distance of the light which changes fragment to fragment for a local light lightdirection lightposition position float lightdistance length lightdirection normalize the light direction vector so that a dot products give cosines lightdirection lightdirection lightdistance model how much light is available for this fragment float attenuation constantattenuation linearattenuation lightdistance quadraticattenuation lightdistance lightdistance the direction of maximum highlight also changes per fragment halfvector normalize lightdirection eyedirection float diffuse max dot normal lightdirection float specular max dot normal halfvector if diffuse specular else specular pow specular shininess strength scatteredlight ambient lightcolor diffuse attenuation reflectedlight lightcolor specular attenuation rgb min color rgb scatteredlight reflectedlight fragcolor rgb color a depending on what specific effects you are after you can leave out one or two of the constant linear or quadratic terms or you can attenuate the ambient term attenuating ambient light will depend on whether you have a global ambient color or per light ambient colors or both it would be the per light ambient colors for point lights that you d want to attenuate you could also put the constant attenuation in your ambient and leave it out of the attenuation expression chapter light and shadow spotlights in stage and cinema spotlights project a strong beam of light that illuminates a well defined area the illuminated area can be further shaped through the use of flaps or shutters on the sides of the light opengl includes light attributes that simulate a simple type of spotlight whereas point lights are modeled as sending light equally in all directions opengl models spotlights as restricted to producing a cone of light in a particular direction the direction to the spotlight is not the same as the focus direction of the cone from the spotlight unless you are looking from the middle of the spot well technically they d be opposite directions nothing a minus sign can t clear up once again our friend the cosine computed as a dot product will tell us to what extent these two directions are in alignment this is precisely what we need to know to deduce if we are inside or outside the cone of illumination a real spotlight has an angle whose cosine is very near so you might want to start with cosines around 99 to see an actual spot just as with specular highlighting we can sharpen or not the light falling within the cone by raising the cosine of the angle to higher powers this allows control over how much the light fades as it gets near the edge of the cutoff the vertex shader and the first and last parts of our spotlight fragment shader see example look the same as our point light shader shown earlier in example the differences occur in the middle of the shader we take the dot product of the spotlight focus direction with the light direction and compare it to a pre computed cosine cutoff value spotcoscutoff to determine whether the position on the surface is inside or outside the spotlight if it is outside the spotlight attenuation is set to otherwise this value is raised to a power specified by spotexponent the resulting spotlight attenuation factor is multiplied by the previously computed attenuation factor to give the overall attenuation factor the remaining lines of code are the same as they were for point lights example spotlight lighting vertex shader vertex shader for spotlight computed in the fragment shader version core uniform mvpmatrix uniform mvmatrix uniform normalmatrix classic lighting model in vertexcolor in vertexnormal in vertexposition out color out normal out position void main color vertexcolor normal normalize normalmatrix vertexnormal position mvmatrix vertexposition mvpmatrix vertexposition fragment shader fragment shader computing a spotlight effect version core uniform ambient uniform lightcolor uniform lightposition uniform float shininess uniform float strength uniform eyedirection uniform float constantattenuation uniform float linearattenuation uniform float quadraticattenuation uniform conedirection adding spotlight attributes uniform float spotcoscutoff how wide the spot is as a cosine uniform float spotexponent control light fall off in the spot in color in normal in position out fragcolor void main lightdirection lightposition position float lightdistance length lightdirection lightdirection lightdirection lightdistance float attenuation constantattenuation linearattenuation lightdistance quadraticattenuation lightdistance lightdistance chapter light and shadow how close are we to being in the spot float spotcos dot lightdirection conedirection attenuate more based on spot relative position if spotcos spotcoscutoff attenuation else attenuation pow spotcos spotexponent halfvector normalize lightdirection eyedirection float diffuse max dot normal lightdirection float specular max dot normal halfvector if diffuse specular else specular pow specular shininess strength scatteredlight ambient lightcolor diffuse attenuation reflectedlight lightcolor specular attenuation rgb min color rgb scatteredlight reflectedlight fragcolor rgb color a moving calculations to the vertex shader we ve been doing all these calculations per fragment for example position is interpolated and then the lightdistance is computed per fragment this gives pretty high quality lighting at the cost of doing an expensive square root computation hidden in the length built in function per fragment sometimes we can swap these steps perform the light distance calculation per vertex in the vertex shader and interpolate the result that is rather than interpolating all the terms in the calculation and calculating per fragment calculate per vertex and interpolate the result the fragment shader then gets the result as an input and directly uses it interpolating vectors between two normalized vectors vectors of length does not typically yield normalized vectors it easy to imagine two vectors pointing notably different directions the vector that the average of them comes out quite a bit shorter however when the two vectors are nearly the same the interpolated vectors between them all have length quite close to close enough in fact to finish doing decent lighting calculations in the fragment shader so there is a balance between having vertices far enough apart that you can improve performance by computing classic lighting model in the vertex shader but not too far apart that the lighting vectors surface normal light direction etc point in notably different directions example goes back to the point light code from example and moves some lighting calculations to the vertex shader example point light source lighting in the vertex shader vertex shader vertex shader pulling point light calculations up from the fragment shader version core uniform mvpmatrix uniform normalmatrix uniform lightposition consume in the vertex shader now uniform eyedirection uniform float constantattenuation uniform float linearattenuation uniform float quadraticattenuation in vertexcolor in vertexnormal in vertexposition out color out normal out position no longer need to interpolate this out lightdirection send the results instead out halfvector out float attenuation void main color vertexcolor normal normalize normalmatrix vertexnormal compute these in the vertex shader instead of the fragment shader lightdirection lightposition vertexposition float lightdistance length lightdirection lightdirection lightdirection lightdistance attenuation constantattenuation linearattenuation lightdistance quadraticattenuation lightdistance lightdistance halfvector normalize lightdirection eyedirection chapter light and shadow mvpmatrix vertexposition fragment shader fragment shader with point light calculations done in vertex shader version core uniform ambient uniform lightcolor uniform lightposition no longer need this uniform float shininess uniform float strength in color in normal in position no longer need this in lightdirection get these from vertex shader instead in halfvector in float attenuation out fragcolor void main lightdirection halfvector and attenuation are interpolated now from vertex shader calculations float diffuse max dot normal lightdirection float specular max dot normal halfvector if diffuse specular else specular pow specular shininess strength scatteredlight ambient lightcolor diffuse attenuation reflectedlight lightcolor specular attenuation rgb min color rgb scatteredlight reflectedlight fragcolor rgb color a there are no rules about where to do each calculation pick one or experiment to find what is best for your surfaces in the extreme the color can be completely computed in the vertex shader just at the vertex and then interpolated the fragment shader then has little to no lighting computation left to do this is the essence of gouraud shading while cheap from a computational perspective it leaves classic lighting model lighting artifacts that betray a surface tessellation to the viewer this is especially obvious for coarse tessellations and specular highlights when surface normals are interpolated and then consumed in the fragment shader we get variants of phong shading this is not to be confused with the phong reflection model which is essentially what this entire section on classic lighting has been describing multiple lights and materials typically a scene has many light sources and many surface materials normally you shade one material at a time but many lights will light that material we ll show a shading model where each invocation of the shader selects a material and then applies all of or a subset of the lights to light it multiple lights normally we need to light with multiple lights while we ve been writing example shaders for just one a scene might have a street light a flashlight and the moon for example with each surface fragment getting a share of light from all three you d likely model these three lights as a point light a spotlight and a directional light respectively and have a single shader invocation perform all three group a light characteristics into structure as shown in example and then create an array of them for the shader to process example structure for holding light properties structure for holding light properties struct lightproperties bool isenabled true to apply this light in this invocation bool islocal true for a point light or a spotlight false for a positional light bool isspot true if the light is a spotlight ambient light contribution to ambient light color color of light position location of light if is local is true otherwise the direction toward the light halfvector direction of highlights for directional light conedirection spotlight attributes float spotcoscutoff float spotexponent float constantattenuation local light attenuation coefficients float linearattenuation float quadraticattenuation other properties you may desire chapter light and shadow in this example we are using a couple of booleans islocal and isspot to select what kind of light is represented if you end up with lots of different light types to choose from this would be better done as an int going through a switch statement this structure also includes an ambient color contribution earlier we used a global ambient assumed to represent all ambient light but we can also have each light making its own contribution for directional lights it doesn t make any difference but for local lights it helps to have their ambient contribution attenuated you could also add separate diffuse and specular colors to get richer effects the first member isenabled can be used to selectively turn lights on and off if a light were truly off while rendering a whole scene it would be faster to not include it in the set of lights to begin with however sometimes we want one surface lit with a different subset of lights than another and so might be enabling and disabling a light at a faster rate depending on how frequently you enable disable it might be better as a separate array or even as a per vertex input all the pieces are put together in example we now need all the lighting forms together in a single shader so we can loop over different kinds of lights and do the right calculations for each one it is based on the shaders that did all lighting in the fragment shader but again performance quality trade offs can be made by moving some of it into the vertex shader example multiple mixed light sources vertex shader vertex shader for multiple lights stays the same with all lighting done in the fragment shader version core uniform mvpmatrix uniform mvmatrix uniform normalmatrix in vertexcolor in vertexnormal in vertexposition out color out normal out position void main color vertexcolor normal normalize normalmatrix vertexnormal classic lighting model position mvmatrix vertexposition mvpmatrix vertexposition fragment shader fragment shader for multiple lights version core struct lightproperties bool isenabled bool islocal bool isspot ambient color position halfvector conedirection float spotcoscutoff float spotexponent float constantattenuation float linearattenuation float quadraticattenuation the set of lights to apply per invocation of this shader const int maxlights uniform lightproperties lights maxlights uniform float shininess uniform float strength uniform eyedirection in color in normal in position out fragcolor void main scatteredlight or to a global ambient light reflectedlight loop over all the lights for int light light maxlights light if lights light isenabled continue halfvector lightdirection lights light position float attenuation for local lights compute per fragment direction halfvector and attenuation chapter light and shadow if lights light islocal lightdirection lightdirection position float lightdistance length lightdirection lightdirection lightdirection lightdistance attenuation lights light constantattenuation lights light linearattenuation lightdistance lights light quadraticattenuation lightdistance lightdistance if lights light isspot float spotcos dot lightdirection lights light conedirection if spotcos lights light spotcoscutoff attenuation else attenuation pow spotcos lights light spotexponent halfvector normalize lightdirection eyedirection else halfvector lights light halfvector float diffuse max dot normal lightdirection float specular max dot normal halfvector if diffuse specular else specular pow specular shininess strength accumulate all the lights effects scatteredlight lights light ambient attenuation lights light color diffuse attenuation reflectedlight lights light color specular attenuation rgb min color rgb scatteredlight reflectedlight fragcolor rgb color a material properties one material property we came across above was shininess we use shininess to control how sharply defined specular highlights are different materials have differently sized specular highlights and seeing this is key to your viewer recognizing a material once rendered on the screen we can also have material specific modulation of the color of ambient diffuse and specular lighting this is an easy new addition to our computations classic lighting model some metals and clothes display cool looking properties as having different underlying colors for scattered light and reflected light it your choice how many of these independent colors you mix together for the effect you want to create for example in the method below setting the material specular value to would make the model degenerate to the model used in the examples above materials can also have their own real or apparent light source for example something glowing will emit its own light this light could easily include colors not present in the any of the light sources so light won t be visible unless it is added on the light calculation we ve done so far it is natural to use a structure to store a material properties as shown in example example structure to hold material properties struct materialproperties emission light produced by the material ambient what part of ambient light is reflected diffuse what part of diffuse light is scattered specular what part of specular light is scattered float shininess exponent for sharpening specular reflection other properties you may desire these material properties and others you may wish to add are not specific to surface location so they can be passed into the shader as a uniform structure scenes have multiple materials with different properties if your application switches between materials frequently consider using the same fragment shader to shade several different materials without having to change shaders or update uniforms to do this make an array of materialproperties each element holding the description of a different material pass the material index into a vertex shader input which it will pass on to the fragment shader then the fragment shader will index into the material array and render properly for that material for example see example we ve modified snippets of the multilight shader to make a multilight selected material shader example code snippets for using an array of material properties fragment shader snippets of fragment shader selecting what material to shade with multiple lights version core chapter light and shadow struct materialproperties emission ambient diffuse specular float shininess a set of materials to select between per shader invocation const int nummaterials uniform materialproperties material nummaterials flat in int matindex input material index from vertex shader void main accumulate all the lights effects scatteredlight lights light ambient material matindex ambient attenuation lights light color material matindex diffuse diffuse attenuation reflectedlight lights light color material matindex specular specular attenuation rgb min material matindex emission color rgb scatteredlight reflectedlight fragcolor rgb color a two sided lighting you might want to render a surface differently if the eye is looking at the back of the surface than if looking at the front of the surface opengl shading language has a built in boolean variable allowing you to do so the variable is set for each fragment to true if the fragment is part of a front facing primitive the variable is set to false otherwise it is only available in fragment shaders if the backs have properties quite different than the fronts just make two sets of materialproperties as in example there are lots of ways classic lighting model to do this here we chose to double the array and use even indexes for the front and odd indexes for the back this is likely faster than having two separate arrays if the properties are extensive and mostly the same it might be more efficient to just expand materialproperties with the one or two differing properties example front and back material properties struct materialproperties emission ambient diffuse specular float shininess a set of materials to select between per shader invocation use even indexes for front facing surfaces and odd indexes for back facing const int nummaterials uniform materialproperties material nummaterials flat in int matindex input material index from vertex shader void main int mat if mat matindex else mat matindex accumulate all the lights effects scatteredlight lights light ambient material mat ambient attenuation lights light color material mat diffuse diffuse attenuation reflectedlight lights light color material mat specular specular attenuation rgb min material mat emission color rgb scatteredlight reflectedlight fragcolor rgb color a chapter light and shadow lighting coordinate systems to make any sense all the normal direction and position coordinates used in a lighting calculation must come from the same coordinate system if light position coordinates come after model view transforms but before perspective projection so should the surface coordinates that will be compared against them in this typical case both are in eye space that is the eye is at looking in the negative z direction this is a regular coordinate system not the component homogeneous space needed for perspective see the first block diagrams in chapter viewing transformations clipping and feedback to see where in the stack of transformations eye space resides this is why in the examples above we sent position separately with its own transform and the types involved are and rather than and generally we used eye space for all the directions and locations feeding light equations while alongside homogeneous coordinates were fed to the rasterizer opengl lighting calculations require knowing the eye direction in order to compute specular reflection terms for eye space the view direction is parallel to and in the direction of the z axis in the examples above we could have replaced the eyedirection with the vector knowing our coordinates were in eye space but for clarity and potential flexibility we used a variable this could be generalized a bit to allow a local viewer much like we had local lights rather than only directional lights with a local viewer specular highlights on multiple objects will tend toward the eye location rather than all being in the same parallel direction limitations of the classic lighting model the classic lighting model works pretty well at what it tries to do modeling the surface reflection properties modeling each light combin ing them together to modulate an underlying color and getting a pretty realistic approximation of what color is scattered and reflected yet there are some important things missing shadows are a big item we lit each surface as if it was the only surface present with no other objects blocking the path of the lights to the surface we will provide techniques for shadowing later in this chapter another big missing item is accurate ambient lighting if you look around a room you won t see a constant level of ambient lighting corners for example are darker than other areas as another example consider a bright red ball resting near other objects you ll probably see that the ambient light around the other objects has a reddish tint created by the classic lighting model red ball these nearby objects then reflect a redder ambient light than objects further from the ball we look at some techniques for addressing this in advanced lighting models on page other techniques for adding in this realism loosely referred to as global illumination are outside the scope of this book a glowing object or very bright object might also have both a halo around it as well as lens flare we used an emission value earlier to model a glowing object but that effect is limited to the actual geometric extent of the object whereas haloing and lens flare extend beyond the object in real life these effects are apparent not only when taking videos or photo graphs the lens and fluid in our eye also make them occur multiple techniques have been developed for rendering this effect a textured surface usually is not perfectly smooth the bumps on the surface must individually be affected by lighting or the surface ends up looking artificially flat bump mapping techniques for doing this are described in chapter procedural texturing advanced lighting models the classic lighting model lacks some realism to generate more realistic images we need to have more realistic models for illumination shadows and reflection than those we ve discussed so far in this section we explore how opengl shading language can help us implement some of these models much has been written on the topic of lighting in computer graphics we examine only a few methods now ideally you ll be inspired to try implementing some others on your own hemisphere lighting earlier we looked carefully at the classic lighting model however this model has a number of flaws and these flaws become more apparent as we strive for more realistic rendering effects one problem is that objects in a scene do not typically receive all their illumination from a small number of specific light sources interreflections between objects often have noticeable and important contributions to objects in the scene the chapter light and shadow traditional computer graphics illumination model attempts to account for this phenomena through an ambient light term however this ambient light term is usually applied equally across an object or an entire scene the result is a flat and unrealistic look for areas of the scene that are not affected by direct illumination another problem with the traditional illumination model is that light sources in real scenes are not point lights or even spotlights they are area lights consider the indirect light coming in from the window and illuminating the floor and the long fluorescent light bulbs behind a rectangular translucent panel for an even more common case consider the illumination outdoors on a cloudy day in this case the entire visible hemisphere is acting like an area light source in several presentations and tutorials chas boyd dan baker and philip taylor of microsoft described this situation as hemisphere lighting let look at how we might create an opengl shader to simulate this type of lighting environment the idea behind hemisphere lighting is that we model the illumination as two hemispheres the upper hemisphere represents the sky and the lower hemisphere represents the ground a location on an object with a surface normal that points straight up gets all of its illumination from the upper hemisphere and a location with a surface normal pointing straight down gets all of its illumination from the lower hemisphere see figure by picking appropriate colors for the two hemispheres we can make the sphere look as though locations with normals pointing up are illuminated and those with surface normals pointing down are in shadow to compute the illumination at any point on the surface we compute the linear interpolation for the illumination received at that point color a skycolor a groundcolor where a sin θ for θ a sin θ for θ with θ being the angle between the surface normal and the north pole direction advanced lighting models figure a sphere illuminated using the hemisphere lighting model in figure a point on the top of the sphere the black x receives illumination only from the upper hemisphere i e the sky color a point on the bottom of the sphere the white x receives illumination only from the lower hemisphere i e the ground color a point right on the equator would receive half of its illumination from the upper hemisphere and half from the lower hemisphere e g sky color and ground color but we can actually calculate a in another way that is simpler but roughly equivalent a cos θ this approach eliminates the need for a conditional furthermore we can easily compute the cosine of the angle between two unit vectors by taking the dot product of the two vectors this is an example of what jim blinn likes to call the ancient chinese art of chi ting in computer graphics if it looks good enough it is good enough it doesn t really matter whether your calculations are physically correct or a bit of a cheat the difference between the two functions is shown in figure the shape of the two chapter light and shadow curves is similar one is the mirror of the other but the area under the curves is the same this general equivalency is good enough for the effect we re after and the shader is simpler and will execute faster as well chi ting solution actual solution o figure analytic hemisphere lighting function compares the actual analytic function for hemisphere lighting to a similar but higher performance function for the hemisphere shader we need to pass in uniform variables for the sky color and the ground color we can also consider the north pole to be our light position if we pass this in as a uniform variable we can light the model from different directions example shows a vertex shader that implements hemisphere lighting as you can see the shader is quite simple the main purpose of the shader is to compute the diffuse color value and leave it in the user defined out variable color as with the chapter earlier examples results for this shader are shown in figure compare the hemisphere lighting d with a single directional light source a and b not only is the hemisphere shader simpler and more efficient it produces a much more realistic lighting effect too this lighting model can be used for tasks like model preview where it is important to examine all the details of a model it can also be used in conjunction with the traditional computer graphics illumination model point directional or spotlights can be added on top of the hemisphere lighting model to provide more illumination to impor tant parts of the scene and as always if you want to move some or all these computations to the fragment shader you may do so advanced lighting models figure lighting model comparison a comparison of some of the lighting models discussed in this chapter the model uses a base color of white rgb to emphasize areas of light and shadow a uses a directional light above and to the right of the model b uses a directional light directly above the model these two images illustrate the difficulties with the traditional lighting model detail is lost in areas of shadow d illustrates hemisphere lighting e illustrates spherical harmonic lighting using the old town square coefficients inc example vertex shader for hemisphere lighting version core uniform lightposition uniform skycolor uniform groundcolor uniform mvmatrix uniform mvpmatrix uniform normalmatrix in vertexposition in vertexnormal chapter light and shadow out color void main position mvmatrix vertexposition tnorm normalize normalmatrix vertexnormal lightvec normalize lightposition position float costheta dot tnorm lightvec float a costheta color mix groundcolor skycolor a mvpmatrix vertexposition one of the issues with this model is that it doesn t account for self occlusion regions that should really be in shadow because of the geometry of the model will appear too bright we will remedy this later image based lighting if we re trying to achieve realistic lighting in a computer graphics scene why not just use an environment map for the lighting this approach to illumination is called image based lighting it has been popularized in recent years by researcher paul debevec at the university of southern california churches and auditoriums may have dozens of light sources on the ceiling rooms with many windows also have complex lighting environments it is often easier and much more efficient to sample the lighting in such environments and store the results in one or more environment maps than it is to simulate numerous individual light sources the steps involved in image based lighting are as follows use a light probe e g a reflective sphere to capture e g photograph the illumination that occurs in a real world scene the captured omnidirectional high dynamic range image is called a light probe image use the light probe image to create a representation of the environment e g an environment map place the synthetic objects to be rendered inside the environment render the synthetic objects by using the representation of the environment created in step on his web site www debevec org debevec offers a number of useful things to developers for one he has made available a number of images that can be used as high quality environment maps to provide realistic advanced lighting models lighting in a scene these images are high dynamic range hdr images that represent each color component with a bit floating point value such images can represent a much greater range of intensity values than can bit per component images for another he makes available a tool called hdrshop that manipulates and transforms these environment maps through links to his various publications and tutorials he also provides step by step instructions on creating your own environment maps and using them to add realistic lighting effects to computer graphics scenes following debevec guidance we purchased a inch chrome steel ball from mcmaster carr supply company www mcmaster com we used this ball to capture a light probe image from the center of the square outside our office building in downtown fort collins colorado shown in figure we then used hdrshop to create a lat long environment map shown in figure and a cube map shown in figure the cube map and lat long map can be used to perform environment mapping that shader simulated a surface with an underlying base color and diffuse reflection characteristics that was covered by a transparent mirror like layer that reflected the environment flawlessly chapter light and shadow figure light probe image a light probe image of old town square fort collins colorado inc figure lat long map an equirectangular or lat long texture map of old town square fort collins colorado inc advanced lighting models figure cube map a cube map version of the old town square light probe image inc we can simulate other types of objects if we modify the environment maps before they are used a point on the surface that reflects light in a diffuse fashion reflects light from all the light sources that are in the hemisphere in the direction of the surface normal at that point we can t really afford to access the environment map a large number of times in our shader what we can do instead is similar to what we discussed for hemisphere lighting starting from our light probe image we can construct an environment map for diffuse lighting each texel in this environment map will contain the weighted average i e the convolution of other texels in the visible hemisphere as defined by the surface normal that would be used to access that texel in the environment chapter light and shadow again hdrshop has exactly what we need we can use hdrshop to create a lat long image from our original light probe image we can then use a command built into hdrshop that performs the necessary convolution this operation can be time consuming because at each texel in the image the contributions from half of the other texels in the image must be considered luckily we don t need a very large image for this purpose the effect is essentially the same as creating a very blurry image of the original light probe image since there is no high frequency content in the computed image a cube map with faces that are 64 or works just fine a single texture access into this diffuse environment map provides us with the value needed for our diffuse reflection calculation what about the specular contribution a surface that is very shiny will reflect the illumination from a light source just like a mirror a single point on the surface reflects a single point in the environment for surfaces that are rougher the highlight defocuses and spreads out in this case a single point on the surface reflects several points in the environment though not the whole visible hemisphere like a diffuse surface hdrshop lets us blur an environment map by providing a phong exponent a degree of shininess a value of convolves the environment map to simulate diffuse reflection and a value of or more convolves the environment map to simulate a somewhat shiny surface the shaders that implement these concepts end up being quite simple and quite fast in the vertex shader all that is needed is to compute the reflection direction at each vertex this value and the surface normal are sent to the fragment shader as out variables they are interpolated across each polygon and the interpolated values are used in the fragment shader to access the two environment maps in order to obtain the diffuse and the specular components the values obtained from the environment maps are combined with the object base color to arrive at the final color for the fragment the shaders are shown in example examples of images created with this technique are shown in figure advanced lighting models figure effects of diffuse and specular environment maps a variety of effects using the old town square diffuse and specular en vironment maps shown in figure left basecolor set to specularpercent is and diffusepercent is middle basecolor is set to specularpercent is set to and diffusepercent is set to right basecolor is set to specularpercent is set to and dif fusepercent is set to inc example shaders for image based lighting vertex shader vertex shader for image based lighting version core uniform mvmatrix uniform mvpmatrix uniform normalmatrix in vertexposition in vertexnormal out reflectdir out normal void main normal normalize normalmatrix vertexnormal pos mvmatrix vertexposition eyedir pos xyz reflectdir reflect eyedir normal mvpmatrix vertexposition fragment shader fragment shader for image based lighting version core chapter light and shadow uniform basecolor uniform float specularpercent uniform float diffusepercent uniform samplercube specularenvmap uniform samplercube diffuseenvmap in reflectdir in normal out fragcolor void main look up environment map values in cube maps diffusecolor texture diffuseenvmap normalize normal specularcolor texture specularenvmap normalize reflectdir add lighting to base color and mix mix basecolor diffusecolor basecolor diffusepercent color mix color specularcolor color specularpercent fragcolor color the environment maps that are used can reproduce the light from the whole scene of course objects with different specular reflection properties require different specular environment maps and producing these environment maps requires some manual effort and lengthy pre processing but the resulting quality and performance make image based lighting a great choice in many situations lighting with spherical harmonics in ravi ramamoorthi and pat hanrahan presented a method that uses spherical harmonics for computing the diffuse lighting term this method reproduces accurate diffuse reflection based on the content of a light probe image without accessing the light probe image at runtime the light probe image is pre processed to produce coefficients that are used in a mathematical representation of the image at runtime the mathematics behind this approach is beyond the scope of this book instead we lay the necessary groundwork for this shader by describing the underlying mathematics in an intuitive fashion the result is remarkably simple accurate and realistic and it can easily be codified in an opengl shader this technique has already been used successfully to provide real time advanced lighting models illumination for games and has applications in computer vision and other areas as well spherical harmonics provides a frequency space representation of an image over a sphere it is analogous to the fourier transform on the line or circle this representation of the image is continuous and rotationally invariant using this representation for a light probe image ramamoorthi and hanrahan showed that you could accurately reproduce the diffuse reflection from a surface with just nine spherical harmonic basis functions these nine spherical harmonics are obtained with constant linear and quadratic polynomials of the normalized surface normal intuitively we can see that it is plausible to accurately simulate the diffuse reflection with a small number of basis functions in frequency space since diffuse reflection varies slowly across a surface with just nine terms used the average error over all surface orientations is less than percent for any physical input lighting distribution with debevec light probe images the average error was shown to be less than percent and the maximum error for any pixel was less than percent each spherical harmonic basis function has a coefficient that depends on the light probe image being used the coefficients are different for each color channel so you can think of each coefficient as an rgb value a pre processing step is required to compute the nine rgb coefficients for the light probe image to be used ramamoorthi makes the code for this pre processing step available for free on his web site we used this program to compute the coefficients for all the light probe images in debevec light probe gallery as well as the old town square light probe image and summarized the results in table chapter light and shadow table spherical harmonic coefficients for light probe images 86 44 35 35 29 43 29 04 04 01 00 00 00 00 00 28 56 04 38 45 08 01 22 31 08 09 64 08 05 28 29 38 71 08 09 39 03 01 17 06 01 02 03 03 09 04 03 02 03 29 67 37 31 the formula for diffuse reflection using spherical harmonics is diffuse c5l20 l2m1yz l10z the constants result from the derivation of this formula and are shown in the vertex shader code in example the l coefficients are the nine basis function coefficients computed for a specific light probe advanced lighting models image in the pre processing phase the x y and z values are the coordinates of the normalized surface normal at the point that is to be shaded unlike low dynamic range images e g bits per color component that have an implicit minimum value of and an implicit maximum value of hdr images represented with a floating point value for each color component don t contain well defined minimum and maximum values the minimum and maximum values for two hdr images may be quite different from each other unless the same calibration or creation process was used to create both images it is even possible to have an hdr image that contains negative values for this reason the vertex shader contains an overall scaling factor to make the final effect look right the vertex shader that encodes the formula for the nine spherical harmonic basis functions is actually quite simple when the compiler gets hold of it it becomes simpler still an optimizing compiler typically reduces all the operations involving constants the resulting code is quite efficient because it contains a relatively small number of addition and multiplication operations that involve the components of the surface normal example shaders for spherical harmonics lighting vertex shader vertex shader for computing spherical harmonics version core uniform mvmatrix uniform mvpmatrix uniform normalmatrix uniform float scalefactor const float const float const float const float const float constants for old town square lighting const const const const const const const const const chapter light and shadow in vertexposition in vertexnormal out diffusecolor void main tnorm normalize normalmatrix vertexnormal diffusecolor tnorm x tnorm x tnorm y tnorm y tnorm z tnorm z l20 l2m2 tnorm x tnorm y l21 tnorm x tnorm z l2m1 tnorm y tnorm z l11 tnorm x l1m1 tnorm y l10 tnorm z diffusecolor scalefactor mvpmatrix vertexposition fragment shader fragment shader for lighting with spherical harmonics version core in diffusecolor out fragcolor void main fragcolor diffusecolor our fragment shader shown in example has very little work to do because the diffuse reflection typically changes slowly for scenes without large polygons we can reasonably compute it in the vertex shader and interpolate it during rasterization as with hemispherical lighting we can add procedurally defined point directional or spotlights on top of the spherical harmonics lighting to provide more illumination to important parts of the scene results of the spherical harmonics shader are shown in figure we could make the diffuse lighting from the spherical harmonics computation more subtle by blending it with the object base color advanced lighting models figure spherical harmonics lighting lighting using the coefficients from table from the left old town square grace cathedral galileo tomb campus sunset and st peter basilica inc the trade offs in using image based lighting versus procedurally defined lights are similar to the trade offs between using stored textures versus procedural textures image based lighting techniques can capture and re create complex lighting environments relatively easily it would be exceedingly difficult to simulate such an environment with a large number of procedural light sources on the other hand procedurally defined light sources do not use up texture memory and can easily be modified and animated shadow mapping recent advances in computer graphics have produced a plethora of techniques for rendering realistic lighting and shadows opengl can be used to implement almost any of them in this section we will cover one technique known as shadow mapping which uses a depth texture to determine whether a point is lit or not shadow mapping is a multipass technique that uses depth textures to provide a solution to rendering shadows a key pass is to view the scene from the shadow casting light source rather than from the final viewpoint by moving the viewpoint to the position of the light source you will notice that everything seen from that location is lit there are no shadows from the perspective of the light by rendering the scene depth from the point of view of the light into a depth buffer we can obtain a map of the shadowed and unshadowed points in the scene a shadow map those points visible to the light will be rendered and those points hidden from the light those in shadow will be culled away by the depth test the resulting depth buffer then contains the distance from the light to the closest point to the light for each pixel it contains nothing for anything in shadow chapter light and shadow the condensed two pass description is as follows render the scene from the point of view of the light source it doesn t matter what the scene looks like you only want the depth values create a shadow map by attaching a depth texture to a framebuffer object and rendering depth directly into it render the scene from the point of view of the viewer project the surface coordinates into the light reference frame and compare their depths to the depth recorded into the light depth texture fragments that are further from the light than the recorded depth value were not visible to the light and hence in shadow the following sections provide a more detailed discussion along with sample code illustrating each of the steps creating a shadow map the first step is to create a texture map of depth values as seen from the light point of view you create this by rendering the scene with the viewpoint located at the light position before we can render depth into a depth texture we need to create the depth texture and attach it to a framebuffer object example shows how to do this this code is included in the initialization sequence for the application example creating a framebuffer object with a depth attachment create a depth texture glgentextures glbindtexture allocate storage for the texture data gl_float null set the default filtering modes gltexparameteri gltexparameteri set up depth comparison mode gltexparameteri gltexparameteri gl_texture_compare_func set up wrapping modes gltexparameteri gltexparameteri glbindtexture gl_texture_2d create fbo to render depth into glgenframebuffers glbindframebuffer shadow mapping attach the depth texture to it glframebuffertexture disable color rendering as there are no color attachments gldrawbuffer in example 15 the depth texture is created and allocated using the internal format this creates a texture that is capable of being used as the depth buffer for rendering and as a texture that can be used later for reading from notice also how we set the texture comparison mode this allows us to leverage shadow textures a feature of opengl that allows the comparison between a reference value and a value stored in the texture to be performed by the texture hardware rather than explicitly in the shader in the example has previously been defined to be the desired size for the shadow map this should generally be at least as big as the default framebuffer your opengl window otherwise aliasing and sampling artifacts could be present in the resulting images however making the depth texture unnecessarily large will waste lots of memory and bandwidth and adversely affect the performance of your program the next step is to render the scene from the point of view of the light to do this we create a view transformation matrix for the light source using the provided lookat function we also need to set the light projection matrix as world and eye coordinates for the light viewpoint we can multiply these matrices together to provide a single view projection matrix in this simple example we can also bake the scene model matrix into the same matrix providing a model view projection matrix to the light shader the code to perform these steps is shown in example example setting up the matrices for shadow map generation time varying light position sinf t 0f 141592f 0f 0f cosf t 0f 141592f 0f 0f matrices for rendering the scene rotate t 0f y matrices used when rendering from the light position lookat 0f y frustum 0f 0f 0f 0f 0f now we render from the light position into the depth buffer select the appropriate program gluseprogram chapter light and shadow gluniformmatrix4fv render_light_uniforms mvpmatrix light_projection_matrix in example we set the light position using a function of time t and point it towards the origin this will cause the shadows to move around is set to the maximum depth over which the light will influence and represents the far plan of the light frustum the near plane is set to 0f but ideally the ratio of far plane to near plane distance should be as small as possible i e the near plane should be as far as possible from the light and the far plane should be as close as possible to the light to maximize the precision of the depth buffer the shaders used to generate the depth buffer from the light position are trivial the vertex shader simply transforms the incoming position by the provided model view projection matrix the fragment shader writes a constant into a dummy output and is only present because opengl requires it the vertex and fragment shaders used to render depth from the light point of view are shown in example 17 example 17 simple shader for shadow map generation vertex shader vertex shader for shadow map generation version core uniform mvpmatrix layout location in position void main void mvpmatrix position fragment shader fragment shader for shadow map generation version core layout location out color void main void color the results of rasterization are undefined in opengl if no fragment shader is present it is legal to have no fragment shader when rasterization is turned off but here we do want to rasterize so that we can generate depth values for the scene shadow mapping at this point we are ready to render the scene into the depth texture we created earlier we need to bind the framebuffer object with the depth texture attachment and set the viewport to the depth texture size then we clear the depth buffer which is actually our depth texture now and draw the scene example contains the code to do this example 18 rendering the scene from the light point of view bind the depth only fbo and set the viewport to the size of the depth texture glbindframebuffer glviewport depth_texture_size clear glcleardepth 0f glclear enable polygon offset to resolve depth fighting isuses glenable glpolygonoffset 0f 0f draw from the light point of view drawscene true gldisable notice that we re using polygon offset here this pushes the generated depth values away from the viewer the light in this case by a small amount in this application we want the depth test to be conservative insofar as when there is doubt about whether a point is in shadow or not we want to light it if we did not do this we would end up with depth fighting in the rendered image due to precision issues with the floating point depth buffer figure shows the resulting depth map of our scene as seen from the light position chapter light and shadow figure 10 depth rendering depths are rendered from the light position within rendered objects closer points have smaller depths and show up darker using a shadow map now that we have the depth for the scene rendered from the light point of view we can render the scene with our regular shaders and use the resulting depth texture to produce shadows as part of our lighting calculations this is where the meat of the algorithm is first we need to set up the matrices for rendering the scene from the viewer position the matrices we ll need are the model matrix view matrix which transforms vertices for classic lighting and the projection matrix which transforms coordinates to projective space for rasterization also we ll need a shadow matrix this matrix transforms world coordinates into the light projective space and simultaneously applies a scale and bias to the resulting depth values the transformation to the light eye space is performed by transforming the world space vertex coordinates through the light view matrix followed by the light projection matrix which we calculated earlier the scale and bias matrix maps depth values in projection space which lie between and into the range to shadow mapping the code to set all these matrices up is given in example example matrix calculations for shadow map rendering scene_model_matrix rotate t 0f y translate 0f 0f 0f scene_projection_matrix frustum 0f 0f aspect aspect 0f 0f 0f 0f 0f 0f 0f 0f 0f 0f 0f light_projection_matrix the vertex shader used for the final render transforms the incoming vertex coordinates through all of these matrices and provides world coordinates eye coordinates and shadow coordinates to the fragment shader which will perform the actual lighting calculations this vertex shader is given in example example vertex shader for rendering from shadow maps version core uniform uniform uniform uniform layout location in position layout location in normal out normal vertex void main void model_matrix position projection_matrix vertex xyz vertex eye_pos xyz vertex world_pos chapter light and shadow vertex normal view_matrix model_matrix normal clip_pos finally the fragment shader performs lighting calculations for the scene if the point is considered to be illuminated by the light the light contribution is included in the final lighting calculation otherwise only ambient light is applied the shader given in example performs these calculations example 21 fragment shader for rendering from shadow maps version core uniform uniform uniform uniform uniform uniform float layout location out color in normal fragment void main void n fragment normal l normalize fragment r reflect l n e normalize fragment float ndotl dot n l float edotr dot e r float diffuse max ndotl float specular max pow edotr float f textureproj depth_texture fragment color f diffuse specular shadow mapping don t worry about the complexity of the lighting calculations in this shader the important part of the algorithm is the use of the sampler type and the textureproj function the sampler is a special type of texture that when sampled will return either if the sampled texture satisfies the comparison test for the texture and if it does not the texture comparison mode for the depth texture was set earlier in example 15 by calling gltexparameteri with the parameter name and parameter value when the depth comparison mode for the texture is configured like this the texel values will be compared against the reference value that is supplied in the third component of fragment which is the z component of the scaled and biased projective space coordinate of the fragment as viewed from the light the depth comparison function is set to which causes the test to pass if the reference value is less than or equal to the value in the texture when multiple texels are sampled e g when the texture mode is linear the result of reading from the texture is the average of all the and 0s for the samples making up the final texel that is near the edge of a shadow the returned value might be or and so on rather than just or we scale the lighting calculations by this result to take light visibility into account during shading the textureproj function is a projective texturing function it divides the incoming texture coordinate in this case fragment by its own last component fragment w to transform it into normalized device coordinates which is exactly what the perspective transformation performed by opengl before rasterization does the result of rendering our scene with this shader is shown in figure chapter light and shadow figure final rendering of shadow map that wraps up shadow mapping there are many other techniques including enhancements to shadow mapping and we encourage you to explore on your own shadow mapping this page intentionally left blank chapter procedural texturing chapter objectives after reading this chapter you ll be able to do the following texture a surface without using texture look ups instead texture a surface using a shader that computes the texture procedurally antialias a procedurally generated texture light a surface using a bump map use noise to modulate shapes and textures to get quite realistic surfaces and shapes generate your own noise texture map for storing multiple octaves of portable noise generally this chapter will cover using computation in shaders to supply quality versions of what might normally come from large texture maps complex geometry or expensive multisampling however accessing textures won t be forbidden we ll still occasionally use them as side tables to drive the calculations performed in the shaders this chapter contains the following major sections procedural texturing shows several techniques for using computation to create patterns rather than accessing images stored in memory bump mapping presents a key method to give the appearance of a bumpy surface without having to construct geometry to represent it antialiasing procedural textures explains how to compute amount of color for each pixel such that aliasing does not occur especially for edges and patterns created procedurally noise will explain what noise is and how to use it to improve realism procedural texturing the fact that we have a full featured high level programming language to express the processing at each fragment means that we can algorithmically compute a pattern on an object surface we can use this freedom to create a wide variety of rendering effects that wouldn t be possible otherwise we can also algorithmically compute the content of a volume from which a surface is cut away as in a wood object made from a tree this can lead to a result superior to texture mapping the surface in previous chapters we discussed shaders that achieve their primary effect by reading values from texture memory this chapter focuses on shaders that do interesting things primarily by means of an algorithm defined by the shader the results from such a shader are synthesized according to the algorithm rather than being based primarily on precomputed values such as a digitized painting or photograph this type of shader is sometimes called a procedural texture shader and the process of applying such a shader is called procedural texturing or procedural shading often the texture coordinate or the object coordinate position at each point on the object is the only piece of information needed to shade the object with a shader that is entirely procedural in principle procedural texture shaders can accomplish many of the same tasks as shaders that access stored textures in practice there are times when it is more convenient or feasible to use a procedural texture shader and times when it is more convenient or feasible to use a stored texture chapter procedural texturing shader when deciding whether to write a procedural texture shader or one that uses stored textures keep in mind some of the main advantages of procedural texture shaders textures generated procedurally have very low memory requirements compared with stored textures the only primary representation of the texture is in the algorithm defined by the code in the procedural texture shader this representation is extremely compact compared with the size of stored textures typically it is a couple of orders of magnitude smaller e g a few kilobytes for the code in a procedural shader versus a few hundred kilobytes or more for a high quality texture this means procedural texture shaders require far less memory on the graphics accelerator procedural texture shaders have an even greater advantage when the desire is to have a solid texture applied to an object a few kilobytes versus tens of megabytes or more for a stored texture textures generated by procedural texture shaders have no fixed area or resolution they can be applied to objects of any scale with precise results because they are defined algorithmically rather than with sampled data as in the case of stored textures there are no decisions to be made about how to map a image onto a surface patch that is larger or smaller than the texture and there are no seams or unwanted replication as your viewpoint gets closer and closer to a surface rendered with a procedural texture shader you won t see reduced detail or sampling artifacts like you might with a shader that uses a stored texture procedural texture shaders can be written to parameterize key aspects of the algorithm these parameters can easily be changed allowing a single shader to produce an interesting variety of effects very little can be done to alter the shape of the pattern in a stored texture after it has been created when a volume is computed by a procedural texture rather than a surface surface cutaways of that volume can be far more realistic than any method of pasting a texture onto the surface and while a texture could be used getting high resolution with a texture can take a prohibitive amount of memory some of the disadvantages of using procedural shaders rather than stored textures are as follows procedural texture shaders require the algorithm to be encoded in a program not everyone has the technical skills needed to write such a program whereas it is fairly straightforward to create a or texture with limited technical skills procedural texturing performing the algorithm embodied by a procedural texture shader at each location on an object can take longer than accessing a stored texture procedural texture shaders can have serious aliasing artifacts that can be difficult to overcome today graphics hardware has built in capabilities for antialiasing stored textures e g filtering methods and mipmaps because of differences in arithmetic precision and differences in implementations of built in functions such as noise procedural texture shaders could produce somewhat different results on different platforms the ultimate choice of whether to use a procedural shader or a stored texture shader should be made pragmatically things that would be artwork in the real world paintings billboards anything with writing etc are good candidates for rendering with stored textures objects that are extremely important to the final look of the image character faces costumes important props can also be rendered with stored textures because this presents the easiest route for an artist to be involved things that are relatively unimportant to the final image and yet cover a lot of area are good candidates for rendering with a procedural shader walls floors ground often a hybrid approach is the right answer a golf ball might be rendered with a base color a hand painted texture map that contains scuff marks a texture map containing a logo and a procedurally generated dimple pattern stored textures can also control or constrain procedural effects if our golf ball needs grass stains on certain parts of its surface and it is important to achieve and reproduce just the right look an artist could paint a grayscale map that would direct the shader to locations where grass smudges should be applied on the surface for instance black portions of the grayscale map and where they should not be applied white portions of the grayscale map the shader can read this control texture and use it to blend between a grass smudged representation of the surface and a pristine surface all that said let turn our attention to a few examples of shaders that are entirely procedural regular patterns for our first example we construct a shader that renders stripes on an object a variety of man made objects can be rendered with such a shader children toys wallpaper wrapping paper flags fabrics and so on chapter procedural texturing the object in figure is a partial torus rendered with a stripe shader the stripe shader and the application in which it is shown were both developed in by lightwork design a company that develops soft ware to provide photorealistic views of objects created with commercial cad cam packages the application developed by lightwork design contains a graphical user interface that allows the user to interactively modify the shader parameters the various shaders that are available are accessible on the upper right portion of the user interface and the modifiable parameters for the current shader are accessible in the lower right portion of the user interface in this case you can see that the parameters for the stripe shader include the stripe color blue the background color orange the stripe scale how many stripes there will be and the stripe width the ratio of stripe to background in this case it is to make blue and orange stripes of equal width figure procedurally striped torus close up of a partial torus rendered with the stripe shader described in regular patterns courtesy of lightwork design for our stripe shader to work properly the application needs to send down only the geometry vertex values and the texture coordinate at each vertex the key to drawing the stripe color or the background color is the t texture coordinate at each fragment the texture coordinate is not used at all the application must also supply values that the vertex shader uses to perform a lighting computation and the aforementioned stripe color procedural texturing background color scale and stripe width must be passed to the fragment shader so that our procedural stripe computation can be performed at each fragment stripes vertex shader the vertex shader for our stripe effect is shown in example example vertex shader for drawing stripes version core uniform lightposition uniform lightcolor uniform eyeposition uniform specular uniform ambient uniform float kd uniform mvmatrix uniform mvpmatrix uniform normalmatrix in mcvertex in mcnormal in out diffusecolor out specularcolor out float texcoord void main ecposition mvmatrix mcvertex tnorm normalize normalmatrix mcnormal lightvec normalize lightposition ecposition viewvec normalize eyeposition ecposition hvec normalize viewvec lightvec float spec clamp dot hvec tnorm spec pow spec diffusecolor lightcolor kd dot lightvec tnorm diffusecolor clamp ambient diffusecolor specularcolor clamp lightcolor specular spec texcoord t mvpmatrix mcvertex there are some nice features to this particular shader nothing in it really makes it specific to drawing stripes it provides a good example of how we might do the lighting calculation in a general way that would be compa tible with a variety of fragment shaders chapter procedural texturing as we mentioned the values for doing the lighting computation lightposition lightcolor eyeposition specular ambient and kd are all passed in by the application as uniform variables the purpose of this shader is to compute diffusecolor and specularcolor two out variables that will be interpolated across each primitive and made available to the fragment shader at each fragment location these values are computed in the typical way a small optimization is that ambient is added to the value computed for the diffuse reflection so that we send one less value to the fragment shader as an out variable the incoming texture coordinate is passed down to the fragment shader as the out variable texcoord and the vertex position is transformed in the usual way stripes fragment shader the fragment shader contains the algorithm for drawing procedural stripes it is shown in example example fragment shader for drawing stripes version core uniform stripecolor uniform backcolor uniform float width uniform float fuzz uniform float scale in diffusecolor in specularcolor in float texcoord out fragcolor void main float scaledt fract texcoord scale float clamp scaledt fuzz float clamp scaledt width fuzz finalcolor mix backcolor stripecolor finalcolor finalcolor diffusecolor specularcolor fragcolor finalcolor the application provides one other uniform variable called fuzz this value controls the smooth transitions i e antialiasing between stripe color and background color with a scale value of 10 a reasonable procedural texturing value for fuzz is it can be adjusted as the object changes size to prevent excessive blurriness at high magnification levels or aliasing at low magnification levels it shouldn t really be set to a value higher than maximum blurriness of stripe edges the first step in this shader is to multiply the incoming t texture coordinate by the stripe scale factor and take the fractional part this computation gives the position of the fragment within the stripe pattern the larger the value of scale the more stripes we have as a result of this calculation the resulting value for the local variable scaledt is in the range from we d like to have nicely antialiased transitions between the stripe colors one way to do this would be to use smoothstep in the transition from stripecolor to backcolor and use it again in the transition from backcolor to stripecolor but this shader uses the fact that these transitions are symmetric to combine the two transitions into one so to get our desired transition we use scaledt to compute two other values and these two values tell us where we are in relation to the two transitions between backcolor and stripecolor for if scaledt fuzz is greater than that indicates that this point is not in the transition zone so we clamp the value to if scaledt is less than fuzz scaledt fuzz specifies the fragment relative distance into the transition zone for one side of the stripe we compute a similar value for the other edge of the stripe by subtracting width from scaledt dividing by fuzz clamping the result and storing it in these values represent the amount of fuzz blurriness to be applied at one edge of the stripe is and is the relative distance into the transition zone at the other edge of the stripe is and is the relative distance into the transition zone our next line of code produces a value that can be used to do a proper linear blend between backcolor and stripecolor but we d actually like to perform a transition that is smoother than a linear blend the next line of code performs a hermite interpolation in the same way as the smoothstep function the final value for frac1 performs the blend between backcolor and stripecolor the result of this effort is a smoothly fuzzed boundary in the transition region between the stripe colors without this fuzzing effect we would have aliasing abrupt transitions between the stripe colors that would flash and pop as the object is moved on the screen the fuzzing of the transition region eliminates those artifacts a close up view of the fuzzed boundary is shown in figure more information about antialiasing procedural shaders can be found in antialiasing procedural textures on page chapter procedural texturing now all that remains to be done is to apply the diffuse and specular lighting effects computed by the vertex shader and supply an alpha value of to produce our final fragment color by modifying the five basic parameters of our fragment shader we can create a fairly interesting number of variations of our stripe pattern using the same shader figure stripes close up extreme close up view of one of the stripes that shows the effect of the fuzz calculation from the stripe shader courtesy of lightwork design brick as a second example of a regular pattern we will look at a shader that draws brick with a slightly different method of lighting than the stripes example again the vertex shader here is somewhat generic and could be used with multiple different fragment shaders to see the effect they will produce see figure our brick example will also clearly display aliasing which we will come back and visit in the upcoming antialiasing section there is a close up of this aliasing in the left picture in figure 19 procedural texturing figure brick patterns a flat polygon a sphere and a torus rendered with the brick shaders bricks vertex shader let dive right in with the vertex shader shown in example it has little to do with drawing brick but does compute how the brick will be lit if you wish read through it and if you ve internalized the beginning of chapter as well as the first example given above it should all start to make sense the brick pattern will come from the fragment shader and we ll explain that next example vertex shader for drawing bricks version core in mcvertex in mcnormal uniform mvmatrix uniform mvpmatrix uniform normalmatrix uniform lightposition const float specularcontribution const float diffusecontribution specularcontribution out float lightintensity out mcposition void main ecposition mvmatrix mcvertex tnorm normalize normalmatrix mcnormal lightvec normalize lightposition ecposition reflectvec reflect lightvec tnorm viewvec normalize ecposition float diffuse max dot lightvec tnorm chapter procedural texturing float spec if diffuse spec max dot reflectvec viewvec spec pow spec lightintensity diffusecontribution diffuse specularcontribution spec mcposition mcvertex xy mvpmatrix mcvertex bricks fragment shader the fragment shader contains the core algorithm to make the brick pattern it is provided in example and we will point out the key computations that make it work example fragment shader for drawing bricks version core uniform brickcolor mortarcolor uniform bricksize uniform brickpct in mcposition in float lightintensity out fragcolor void main color position usebrick position mcposition bricksize if fract position y position x position fract position usebrick step position brickpct color mix mortarcolor brickcolor usebrick x usebrick y color lightintensity fragcolor color procedural texturing the colors to make the brick and mortar are selected by the application and sent in as brickcolor and mortarcolor the size of the brick pattern uses two independent components for width and height and is also sent by the application in bricksize finally the application selects what percentage of the pattern will be brick in brickpct with the remaining being mortar the sizes are in the same units as the position coming from the vertex shader mcposition which in turn was passed into the vertex shader from the application the input mcposition is effectively our texture coordinate the key to knowing where we are in the brick pattern is looking at the fractional part of dividing mcposition by the brick size each time the pattern completes we are at a whole number of repetitions of the brick hence the fractional part goes to as we move through one iteration of the brick the fractional part approaches these computations are done with math so we get both dimensions answered at the same time because alternating rows of brick are offset we conditionally add to the x dimension for alternating counts of the repeat pattern in the y dimension this is cryptically done as fract position y for which you might have other ways of expressing once we know where we are in the brick pattern we could use a bunch of if tests to select the right color or we could use math in this example we chose math the range of position is and we need brickpct to be in the same range the step function says the first argument is an edge the left of which should return and the right of which should so for a particular dimension step position brickpct will return if we are in the brick and if in the mortar we want to draw mortar if either dimension says to draw mortar well with these and results multiplying them answers that question without using any if tests finally the mix function is used to pick one of the colors no actual mixing occurs because the ratio of mixing is either going to be or it simply selects the first or second argument additional reasons for using step and mix in this way will become clear when we antialias toy ball programmability is the key to procedurally defining all sorts of texture patterns this next shader takes things a bit further by shading a sphere with a procedurally defined star pattern and a procedurally defined stripe this shader was inspired by the ball in one of pixar early short chapter procedural texturing animations luxo jr this shader is quite specialized it shades any surface as long as it a sphere the reason is that the fragment shader exploits the following property of the sphere the surface normal for any point on the surface points in the same direction as the vector from the center of the sphere to that point on the surface this property is used to analytically compute the surface normal used in the shading calculations within the fragment shader this is actually a reasonable approximation for convex hulls that aren t too far from being spherical the key to this shader is that the star pattern is defined by the coefficients for five half spaces that define the star shape these coefficients were chosen to make the star pattern an appropriate size for the ball points on the sphere are classified as in or out relative to each half space locations in the very center of the star pattern are in with respect to all five half spaces locations in the points of the star are in with respect to four of the five half spaces all other locations are in with respect to three or fewer half spaces fragments that are in the stripe pattern are simpler to compute after we have classified each location on the surface as star stripe or other we can color each fragment appropriately the color computations are applied in an order that ensures a reasonable result even if the ball is viewed from far away a surface normal is calculated analytically i e exactly within the fragment shader a lighting computation that includes a specular highlight calculation is also applied at every fragment application setup the application needs only to provide vertex positions for this shader to work properly both colors and normals are computed algorithmically in the fragment shader the only catch is that for this shader to work properly the vertices must define a sphere the sphere can be of arbitrary size because the fragment shader performs all the necessary computations based on the known geometry of a sphere a number of parameters to this shader are specified with uniform variables the values that produce the images shown in the remainder of this section are summarized in example example values for uniform variables used by the toy ball shader halfspace halfspace halfspace halfspace halfspace stripewidth procedural texturing inoroutinit fwidth 005 starcolor stripecolor basecolor ballcenter lightdir 57735 57735 57735 hvector 32506 32506 88808 specularcolor specularexponent ka kd ks vertex shader the fragment shader is the workhorse for this shader duo so the vertex shader needs only to compute the ball center position in eye coordinates the eye coordinate position of the vertex and the clip space position at each vertex the application could provide the ball center position in eye coordinates but our vertex shader doesn t have much to do and doing it this way means the application doesn t have to keep track of the model view matrix this value could easily be computed in the fragment shader but the fragment shader will likely have a little better performance if we leave the computation in the vertex shader and pass the result as a flat interpolated out variable see example example vertex shader for drawing a toy ball version core uniform mcballcenter uniform mvmatrix uniform mvpmatrix uniform normalmatrix in mcvertex out ocposition out ecposition flat out ecballcenter void main void ocposition mcvertex xyz ecposition mvmatrix mcvertex ecballcenter mvmatrix mcballcenter mvpmatrix mcvertex chapter procedural texturing fragment shader the toy ball fragment shader is a little bit longer than some of the previous examples so we build it up a few lines of code at a time and illustrate some intermediate results the definitions for the local variables that are used in the toy ball fragment shader are as follows normal analytically computed normal pshade point in shader space surfcolor computed color of the surface float intensity computed light intensity distance computed distance values float inorout counter for classifying star pattern the first thing we do is turn the surface location that we re shading into a point on a sphere with a radius of we can do this with the normalize function pshade xyz normalize ocposition xyz pshade w we don t want to include the w coordinate in the computation so we use the component selector xyz to select the first three components of ocposition this normalized vector is stored in the first three components of pshade with this computation pshade represents a point on the sphere with radius so all three components of pshade are in the range the w coordinate isn t really pertinent to our computations at this point but to make subsequent calculations work properly we initialize it to a value of we are always going to be shading spheres with this fragment shader so we analytically calculate the surface normal of the sphere normal normalize ecposition xyz ecballcenter xyz next we perform our half space computations we initialize a counter inorout to a value of we increment the counter each time the surface location is in with respect to a half space because five half spaces are defined the final counter value will be in the range values of or signify that the fragment is within the star pattern values of or less signify that the fragment is outside the star pattern inorout inoroutinit initialize inorout to we have defined the half spaces as an array of five values done our in or out computations and stored the results in an array of five float procedural texturing values but we can take a little better advantage of the parallel nature of the underlying graphics hardware if we do things a bit differently you ll see how in a minute first we compute the distance between pshade and the first four half spaces by using the built in dot product function distance dot p halfspace distance dot p halfspace distance dot p halfspace distance dot p halfspace the results of these half space distance calculations are visualized in a d of figure surface locations that are in with respect to the half space are shaded in gray and points that are out are shaded in black you may have been wondering why our counter was defined as a float instead of an int we re going to use the counter value as the basis for a smoothly antialiased transition between the color of the star pattern and the color of the rest of the ball surface to this end we use the smoothstep function to set the distance to if the computed distance is less than fwidth to if the computed distance is greater than fwidth and to a smoothly interpolated value between and if the computed distance is between those two values by defining distance as a we can perform the smooth step computation on four values in parallel the built in function smoothstep implies a divide operation and because fwidth is a float only one divide operation is necessary this makes it all very efficient distance smoothstep fwidth fwidth distance now we can quickly add the values in distance by performing a dot product between distance and a containing for all components inorout dot distance because we initialized inorout to we add the result of the dot product to the previous value of inorout this variable now contains a value in the range and we have one more half space distance to compute we compute the distance to the fifth half space and we do the computation to determine whether we re in or out of the stripe around the ball we call the smoothstep function to do the same operation on these two values as was performed on the previous four half space distances we update the inorout counter by adding the result from the distance computation with the final half space the distance computation with respect to the fifth half space is illustrated in e of figure chapter procedural texturing figure visualizing the results of the half space distance calculations courtesy of amd distance x dot pshade halfspace distance y stripewidth abs pshade z distance xy smoothstep fwidth fwidth distance xy inorout distance x in this case we re performing a smooth step operation only on the x and y components the value for inorout is now in the range this intermediate result is illustrated in figure a by clamping the value of inorout to the range we obtain the result shown in figure b inorout clamp inorout at this point we can compute the surface color for the fragment we use the computed value of inorout to perform a linear blend between yellow and red to define the star pattern if we were to stop here the result would look like ball a in figure if we take the results of this calculation and do a linear blend with the color of the stripe we get the result shown for ball b figure because we used smoothstep the values of inorout and distance y provide a nicely antialiased edge at the border between colors surfcolor mix basecolor starcolor inorout surfcolor mix surfcolor stripecolor distance y the result at this stage is flat and unrealistic performing a lighting calculation will fix this the first step is to analytically compute the normal for this fragment which we can do because we know the eye coordinate position of the center of the ball it provided in the in variable ecballcenter and we know the eye coordinate position of the fragment it passed in the in variable ecposition calculate analytic normal of a sphere normal normalize ecposition xyz ecballcenter xyz procedural texturing figure intermediate results from the toy ball shader in a the procedurally defined star pattern is displayed in b the stripe is added in c diffuse lighting is applied in d the analytically defined nor mal is used to apply a specular highlight courtesy of ati research inc the diffuse part of the lighting equation is computed with these three lines of code per fragment diffuse lighting intensity ka ambient intensity kd clamp dot lightdir xyz normal surfcolor intensity the result of diffuse only lighting is shown as ball c in figure the final step is to add a specular contribution with these three lines of code per fragment specular lighting intensity clamp dot hvector xyz normal intensity ks pow intensity specularexponent surfcolor rgb specularcolor rgb intensity chapter procedural texturing figure intermediate results from in or out computation surface points that are in with respect to all five half planes are shown in white and points that are in with respect to four half planes are shown in gray a the value of inorout is clamped to the range to produce the result shown in b courtesy of amd notice in ball d in figure that the specular highlight is perfect because the surface normal at each fragment is computed exactly there is no misshapen specular highlight caused by tessellation facets like we re used to seeing the resulting value is written to fragcolor and sent on for final processing before ultimately being written into the framebuffer fragcolor surfcolor voila your very own toy ball created completely out of thin air the complete listing of the toy ball fragment shader is shown in example example fragment shader for drawing a toy ball version core uniform halfspace half spaces used to define star pattern uniform float stripewidth uniform float inoroutinit uniform float fwidth 005 uniform starcolor uniform stripecolor uniform basecolor uniform lightdir light direction should be normalized uniform hvector reflection vector for infinite light procedural texturing uniform specularcolor uniform float specularexponent uniform float ka uniform float kd uniform float ks in ecposition surface position in eye coordinates in ocposition surface position in object coordinates flat in ecballcenter ball center in eye coordinates out fragcolor void main normal analytically computed normal pshade point in shader space surfcolor computed color of the surface float intensity computed light intensity distance computed distance values float inorout counter for classifying star pattern pshade xyz normalize ocposition xyz pshade w inorout inoroutinit initialize inorout to distance dot pshade halfspace distance dot pshade halfspace distance dot pshade halfspace distance dot pshade halfspace float fwidth fwidth pshade distance smoothstep fwidth fwidth distance inorout dot distance distance x dot pshade halfspace distance y stripewidth abs pshade z distance xy smoothstep fwidth fwidth distance xy inorout distance x inorout clamp inorout surfcolor mix basecolor starcolor inorout surfcolor mix surfcolor stripecolor distance y calculate analytic normal of a sphere normal normalize ecposition xyz ecballcenter xyz per fragment diffuse lighting intensity ka ambient intensity kd clamp dot lightdir xyz normal chapter procedural texturing surfcolor intensity per fragment specular lighting intensity clamp dot hvector xyz normal intensity ks pow intensity specularexponent surfcolor rgb specularcolor rgb intensity fragcolor surfcolor lattice here a little bit of a gimmick in this example we show how not to draw the object procedurally in this example we look at how the discard command can be used in a fragment shader to achieve some interesting effects the discard command causes fragments to be discarded rather than used to update the framebuffer we use this to draw geometry with holes the vertex shader is the exact same vertex shader used for stripes regular patterns the fragment shader is shown in example example fragment shader for procedurally discarding part of an object in diffusecolor in specularcolor in texcoord out fragcolor uniform scale uniform threshold uniform surfacecolor void main float ss fract texcoord scale float tt fract texcoord t scale t if ss threshold tt threshold t discard finalcolor surfacecolor diffusecolor specularcolor fragcolor finalcolor the part of the object to be discarded is determined by the values of the and t texture coordinates a scale factor is applied to adjust the frequency procedural texturing of the lattice the fractional part of this scaled texture coordinate value is computed to provide a number in the range these values are compared with the threshold values that have been provided if both values exceed the threshold the fragment is discarded otherwise we do a simple lighting calculation and render the fragment in figure the threshold values were both set to 13 this means that more than three quarters of the fragments were being discarded figure the lattice shader applied to the cow model inc procedural shading summary a master magician can make it look like something is created out of thin air with procedural textures you as a shader writer can express algorithms that turn flat gray surfaces into colorful patterned bumpy or reflective ones the trick is to come up with an algorithm that expresses the texture you envision by coding this algorithm into a shader you too can create something out of thin air in this section we only scratched the surface of what possible we created a stripe shader but grids and checkerboards and polka dots are no more difficult we created a toy ball with a star but we could have created a beach ball with snowflakes shaders can be written to procedurally include chapter procedural texturing or exclude geometry or to add bumps or grooves additional procedural texturing effects are illustrated in this rest of this chapter in particular noise shows how an irregular function noise can achieve a wide range of procedural texturing effects procedural textures are mathematically precise are easy to parameterize and don t require large amounts of texture memory bandwidth or filtering the end goal of a fragment shader is to produce a color value and possibly a depth value that will be written into the framebuffer because the opengl shading language is a procedural programming language the only limit to this computation is your imagination bump mapping we have already seen procedural shader examples that modified color brick and stripes and opacity lattice another class of interesting effects can be applied to a surface with a technique called bump mapping bump mapping involves modulating the surface normal before lighting is applied we can perform the modulation algorithmically to apply a regular pattern we can add noise to the components of a normal or we can look up a perturbation value in a texture map bump mapping has proved to be an effective way of increasing the apparent realism of an object without increasing the geometric complexity it can be used to simulate surface detail or surface irregularities the technique does not truly alter the shape of the surface being shaded it merely tricks the lighting calculations therefore the bumping does not show up on the silhouette edges of an object imagine modeling a planet as a sphere and shading it with a bump map so that it appears to have mountains that are quite large relative to the diameter of the planet because nothing has been done to change the underlying geometry which is perfectly round the silhouette of the sphere always appears perfectly round even if the mountains bumps should stick out of the silhouette edge in real life you would expect the mountains on the silhouette edges to prevent the silhouette from looking perfectly round also bump to bump interactions of lighting and occlusion aren t necessarily correct for these reasons it is a good idea to use bump mapping to apply only small effects to a surface at least relative to the size of the surface or to surfaces that won t be viewed near edge on wrinkles on an orange embossed logos and pitted bricks are all good examples of things that can be successfully bump mapped bump mapping adds apparent geometric complexity during fragment processing so once again the key to the process is our fragment shader this implies that the lighting operation must be performed by our bump mapping fragment shader instead of by the vertex shader where it is often handled again this points out one of the advantages of the programmability that is available through the opengl shading language we are free to perform whatever operations are necessary in either the vertex shader or the fragment shader we don t need to be bound to the fixed functionality ideas of where things like lighting are performed the key to bump mapping is that we need a valid surface normal at each fragment location and we also need a light source vector and a viewing direction vector if we have access to all these values in the fragment shader we can procedurally perturb the normal prior to the light source calculation to produce the appearance of bumps in this case we really are attempting to produce bumps or small spherical nodules on the surface being rendered the light source computation is typically performed with dot products for the result to have meaning all the components of the light source calculation must be defined in the same coordinate space so if we used the vertex shader to perform lighting we would typically define light source positions or directions in eye coordinates and would transform incoming normals and vertex values into this space to do the calculation however the eye coordinate system isn t necessarily the best choice for doing lighting in the fragment shader we could normalize the direction to the light and the surface normal after transforming them to eye space and then pass them to the fragment shader as out variables however the light direction vector would need to be renormalized after interpolation to get accurate results moreover whatever method we use to compute the perturbation normal it would need to be transformed into eye space and added to the surface normal that vector would also need to be normalized without renormalization the lighting artifacts would be quite noticeable performing these operations at every fragment might be reasonably costly in terms of performance there is a better way let us look at another coordinate space called the surface local coordinate space this coordinate system adapts over a rendered object surface assuming that each point is at and that the unperturbed surface normal at each point is this is a highly convenient coordinate system in which to do our bump mapping calculations but to do our lighting computation we need to make sure that our light direction viewing direction and the computed perturbed normal are all defined in the same coordinate system if our perturbed normal is defined in surface local coordinates that means we need to transform our light direction and viewing direction into surface local space as well how is that accomplished chapter procedural texturing what we need is a transformation matrix that transforms each incoming vertex into surface local coordinates i e incoming vertex x y z is transformed to we need to construct this transformation matrix at each vertex then at each vertex we use the surface local transformation matrix to transform both the light direction and the viewing direction in this way the surface local coordinates of the light direction and the viewing direction are computed at each vertex and interpolated across the primitive at each fragment we can use these values to perform our lighting calculation with the perturbed normal that we calculate but we still haven t answered the real question how do we create the transformation matrix that transforms from object coordinates to surface local coordinates an infinite number of transforms will transform a particular vertex to to transform incoming vertex values we need a way that gives consistent results as we interpolate between them the solution is to require the application to send down one more attribute value for each vertex a surface tangent vector furthermore we require the application to send us tangents that are consistently defined across the surface of the object by definition this tangent vector is in the plane of the surface being rendered and perpendicular to the incoming surface normal if defined consistently across the object it serves to orient consistently the coordinate system that we derive if we perform a cross product between the tangent vector and the surface normal we get a third vector that is perpendicular to the other two this third vector is called the binormal and it something that we can compute in our vertex shader together these three vectors form an orthonormal basis of a new coordinate system which is what we need to define the transformation from object coordinates into surface local coordinates because this particular surface local coordinate system is defined with a tangent vector as one of the basis vectors this coordinate system is sometimes referred to as tangent spaces the transformation from object space to surface local space is as follows sx tx ty tz ox sy bx by bz oy we transform the object space vector ox oy oz into surface local space by multiplying it by a matrix that contains the tangent vector tx ty tz in the first row the binormal vector bx by bz in the second row and the surface normal nx ny nz in the third row we can use this process to transform both the light direction vector and the viewing direction vector into surface local coordinates the transformed vectors are interpolated bump mapping across the primitive and the interpolated vectors are used in the frag ment shader to compute the reflection with the procedurally perturbed normal application setup for our procedural bump map shader to work properly the application must send a vertex position a surface normal and a tangent vector in the plane of the surface being rendered the application passes the tangent vector as a generic vertex attribute and binds the index of the generic attribute to be used to the vertex shader variable tangent by calling glbindattriblocation the application is also responsible for providing values for the uniform variables lightposition surfacecolor bumpdensity bumpsize and specularfactor you must be careful to orient the tangent vectors consistently between vertices otherwise the transformation into surface local coordinates will be inconsistent and the lighting computation will yield unpredictable results to be consistent vertices near each other need to have tangent vectors that point in nearly the same direction flat surfaces would have the same tangent direction everywhere consistent tangents can be computed algorithmically for mathematically defined surfaces consistent tangents for polygonal objects can be computed with neighboring vertices and by application of a consistent orientation with respect to the object texture coordinate system the problem with inconsistently defined normals is illustrated in figure this diagram shows two triangles one with consistently defined tangents and one with inconsistently defined tangents the gray arrowheads indicate the tangent and binormal vectors the surface normal is pointing straight out of the page the white arrowheads indicate the direction toward the light source in this case a directional light source is illustrated when we transform vertex to surface local coordinates we get the same initial result in both cases when we transform vertex we get a large difference because the tangent vectors are very different between the two vertices if tangents were defined consistently this situation would not occur unless the surface had a high degree of curvature across this polygon and if that were the case we would really want to tessellate the geometry further to prevent this from happening the result is that in case our light direction vector is smoothly interpolated from the first vertex to the second and all the interpolated chapter procedural texturing case consistent tangents case inconsistent tangents y t x y t x l2 case surface local space for vertex case surface local space for vertex y l b b t2 y 2x t x x l 2y 2y l1 case surface local space for vertex case surface local space for vertex case small interpolation between light vectors case large interpolation between light vectors figure inconsistently defined tangents leading to large lighting errors vectors are roughly the same length if we normalize this light vector at each vertex the interpolated vectors are very close to unit length as well but in case the interpolation causes vectors of wildly different lengths to be generated some of them near zero this causes severe artifacts in the lighting calculation bump mapping remember opengl does not need to send down a binormal vertex attribute only a normal vector and a tangent vector so we don t compute the binormal in the application rather we have the vertex shader compute it automatically simple computation is typically faster than memory access or transfer vertex shader the vertex shader for our procedural bump map shader is shown in example this shader is responsible for computing the surface local direction to the light and the surface local direction to the eye to do this it accepts the incoming vertex position surface normal and tangent vector computes the binormal and transforms the eye space light direction and viewing direction using the created surface local transformation matrix the texture coordinates are also passed on to the fragment shader because they are used to determine the position of our procedural bumps example vertex shader for doing procedural bump mapping version core uniform lightposition uniform mvmatrix uniform mvpmatrix uniform normalmatrix in mcvertex in mcnormal in mctangent in out lightdir out eyedir out texcoord void main eyedir mvmatrix mcvertex texcoord st n normalize normalmatrix mcnormal t normalize normalmatrix mctangent b cross n t v v x dot lightposition t v y dot lightposition b v z dot lightposition n lightdir normalize v v x dot eyedir t chapter procedural texturing v y dot eyedir b v z dot eyedir n eyedir normalize v mvpmatrix mcvertex fragment shader the fragment shader for doing procedural bump mapping is shown in example 10 a couple of the characteristics of the bump pattern are parameterized by being declared as uniform variables namely bumpdensity how many bumps per unit area and bumpsize how wide each bump will be two of the general characteristics of the overall surface are also defined as uniform variables surfacecolor base color of the surface and specularfactor specular reflectance property the bumps that we compute are round because the texture coordinate is used to determine the positioning of the bumps the first thing we do is multiply the incoming texture coordinate by the density value this controls whether we see more or fewer bumps on the surface using the resulting grid we compute a bump located in the center of each grid square the components of the perturbation vector p are computed as the distance from the center of the bump in the x direction and the distance from the center of the bump in the y direction we only perturb the normal in the x and y directions the z value for our perturbation normal is always we compute a pseudodistance d by squaring the components of p and summing them the real distance could be computed at the cost of doing another square root but it not really necessary if we consider bumpsize to be a relative value rather than an absolute value to perform a proper reflection calculation later on we really need to normalize the perturbation normal this normal must be a unit vector so that we can perform dot products and get accurate cosine values for use in the lighting computation we generally normalize a vector by multiplying each component of the normal by y2 because of our computation for d we ve already computed part of what we need i e y2 furthermore because we re not perturbing z at all we know that will always be to minimize the computation we just finish computing our normalization factor at this point in the shader by computing d bump mapping next we compare d to bumpsize to see if we re in a bump or not if we re not we set our perturbation vector to and our normalization factor to the lighting computation happens in the next few lines we compute our normalized perturbation vector by multiplying through with the normalization factor f the diffuse and specular reflection values are computed in the usual way except that the interpolated surface local coordinate light and view direction vectors are used we get decent results without normalizing these two vectors as long as we don t have large differences in their interpolated values between vertices example 10 fragment shader for procedural bump mapping version core uniform surfacecolor 7 18 uniform float bumpdensity uniform float bumpsize 15 uniform float specularfactor in lightdir in eyedir in texcoord out fragcolor void main litcolor c bumpdensity texcoord st p fract c float d f d dot p p f inversesqrt d if d bumpsize p f normdelta p x p y f litcolor surfacecolor rgb max dot normdelta lightdir reflectdir reflect lightdir normdelta float spec max dot eyedir reflectdir spec pow spec spec specularfactor litcolor min litcolor spec fragcolor litcolor surfacecolor a chapter procedural texturing the results from the procedural bump map shader are shown applied to two objects a simple box and a torus in figure the texture coordinates are used as the basis for positioning the bumps and because the texture coordinates go from to four times around the diameter of the torus the bumps look much closer together on that object figure simple box and torus with procedural bump mapping inc normal maps it is easy to modify our shader so that it obtains the normal perturbation values from a texture rather than generating them procedurally a texture that contains normal perturbation values for the purpose of bump mapping is called a bump map or a normal map an example of a normal map and the results applied to our simple box object are shown in figure 10 individual components for the normals can be in the range to be encoded into an rgb texture with bits per component they must be mapped into the range the normal map appears chalk blue because the default perturbation vector of is encoded in the normal map as 5 the normal map could be stored in a floating point texture today graphics hardware supports textures both with bit floating point values per color component and 32 bit floating point values per color component if you use a floating point texture format for storing normals your image quality tends to increase for instance reducing banding effects in specular highlights of course textures that are bits per component require twice as much bump mapping texture memory as bit per component textures and performance might be reduced figure 10 normal mapping a normal map left and the rendered result on a simple box and a sphere inc the vertex program is identical to the one described in bump mapping the fragment shader is almost the same except that instead of computing the perturbed normal procedurally the fragment shader obtains it from a normal map stored in texture memory antialiasing procedural textures jaggies popping sparkling stair steps strobing and marching ants they re all names used to describe the anathema of computer graphics aliasing anyone who has used a computer has seen it for still images it not always that noticeable or objectionable but as soon as you put an object in motion the movement of the jagged edges catches your eye and distracts you from the early days of computer graphics the fight to eliminate these nasty artifacts has been called antialiasing this section introduces the main reasons aliasing occurs techniques to avoid it and the facilities within the opengl shading language to help with antialiasing armed with this knowledge you should be well on your way to fighting the jaggies in your own shaders sources of aliasing aliasing can be generally explained by sampling theory while specific forms of aliasing can be explained more concretely by specific situations we will tie together both approaches and this will become clearer as the forms are discussed most generally from a sampling theory perspective a graphics image is made from point samples of the scene if patterns in the chapter procedural texturing scene vary at a high spatial frequency with respect to the samples the samples can t accurately reproduce the scene they are hit and miss on interesting features a periodic pattern needs to be sampled at at least twice the frequency of the pattern itself otherwise the image will break down when it has a pattern changing faster than every two samples causing moiré patterns in a static image and sparkling in a moving image the edge of an object is an interesting case as it forms a step function as it is crossed this is effectively a square wave which includes super high frequencies it an infinite sum of ever increasing frequencies so it is impossible to correctly sample an edge with point samples without undersampling this is discussed further as we go and should become more clear the human eye is extremely good at noticing edges this is how we comprehend shape and form and how we recognize letters and words our eye is naturally good at it and we spend our whole lives practicing it so naturally it is something we do very very well a computer display is limited in its capability to present an image the display is made up of a finite number of discrete elements pixels at a given time each pixel can produce only one color this makes it impossible for a computer display to accurately represent detail that is smaller than one pixel in screen space such as an edge especially when each pixel is only representing a point sample for the pixel center when you combine these two things the human eye ability to discern edges and the computer graphics display limitations in replicating them you have a problem and this problem is known as aliasing in a nutshell aliasing occurs when we try to reproduce a signal with an insufficient sampling frequency less than two times the highest frequency present in the image with a computer graphics display we ll always have a fixed number of samples pixels with which to reconstruct our image and this will always be insufficient to provide adequate sampling for edges so we will always have aliasing unless we use the pixels to represent something other than point samples in the end we can eliminate aliasing by reducing the spatial frequency in the image to half the spatial frequency of the pixels exchanging aliasing for some other problem that is less objectionable like loss of detail blurriness or noise and sometimes also lowering the render time performance the problem is illustrated in figure in this diagram we show the results of trying to draw a gray object the intended shape is shown in figure a the computer graphics display limits us to a discrete sampling grid if we choose only one location within each grid square usually the center and determine the color to be used by sampling the desired image at that point we see some apparent artifacts this is called antialiasing procedural textures point sampling and is illustrated in figure b the result is ugly aliasing artifacts for edges that don t line up naturally with the sampling grid see figure 11 c it actually depends on your display device technology whether pixels are more like overlapping circles crt or collections of smaller red green and blue sub pixels lcd but the artifacts are obvious in all cases figure 11 aliasing artifacts caused by point sampling the gray region represents the shape of the object to be rendered a the computer graphics display presents us with a limited sampling grid b the result of choosing to draw or not draw gray at each pixel results in jaggies or aliasing artifacts c aliasing takes on other forms as well if you are developing a sequence of images for an animation and you don t properly time sample objects that are in motion you might notice temporal aliasings this is caused by objects that are moving too rapidly for the time sampling frequency being used objects may appear to stutter as they move or blink on and off the classic example of temporal aliasing comes from the movies a vehicle car truck or covered wagon in motion is going forward but the spokes of its wheels appear to be rotating backwards this effect is caused when the time sampling rate movie frames per second is too low relative to the motion of the wheel spokes in reality the wheel may be rotating two and three quarter revolutions per frame but on film it looks like it rotating one quarter revolution backward each frame to render images that look truly realistic rather than computer generated we need to develop techniques for overcoming the inherent limitations of the graphics display both spatially and temporally avoiding aliasing one way to achieve good results without aliasing is to avoid situations in which aliasing occurs chapter procedural texturing for instance if you know that a particular object will always be a certain size in the final rendered image you can design a shader that looks good while rendering that object at that size this is the assumption behind some of the shaders presented previously in this book the smoothstep mix and clamp functions are handy functions to use to avoid sharp transitions and to make a procedural texture look good at a particular scale aliasing is often a problem when you are rendering an object at different sizes mipmap textures address this very issue and you can do something similar with shaders if you know that a particular object must appear at different sizes in the final rendering you can design a shader for each different size each of these shaders would provide an appropriate level of detail and avoid aliasing for an object of that size for this to work the application must determine the approximate size of the final rendered object before it is drawn and then install the appropriate shader in addition if a continuous zoom in or out is applied to a single object some popping will occur when the level of detail changes you can avoid aliasing in some situations by using a texture instead of computing something procedurally this lets you take advantage of the filtering support that is built into the texture mapping however linear filtering between adjacent texels is only a solution to aliasing when the resolution of the texels is similar to the resolution of the pixels otherwise you can still end up undersampling a texture and still get aliasing proper use of mipmaps will help keep you in antialiasing territory of course there are other issues with using stored textures as opposed to doing things procedurally as discussed earlier in this chapter increasing resolution the effects of aliasing can be reduced through a brute force method called supersampling that performs sampling at several locations within a pixel and averages the result of those samples this is exactly the approach supported in today graphics hardware with the multisample buffer this method of antialiasing replaces a single point sampling operation with a several point sampling operation so it doesn t actually eliminate aliasing but it can reduce aliasing to the point that it is no longer objectionable you may be able to ignore the issue of aliasing if your shaders will always be used in conjunction with a multisample buffer but this approach does use up hardware resources graphics board memory for storing the multisample buffer and even with hardware acceleration it still may be slower than performing the antialiasing as part of the procedural texture generation algorithm and because this approach antialiasing procedural textures doesn t eliminate aliasing your result is still apt to exhibit signs of aliasing albeit at a higher frequency less visibly than before supersampling is illustrated in figure each of the pixels is rendered by sampling at four locations rather than at one the average of the four samples is used as the value for the pixel this averaging provides a better result but it is not sufficient to eliminate aliasing because high frequency components can still be misrepresented figure 12 supersampling supersampling with four samples per pixel yields a better result but aliasing artifacts are still present the shape of the object to be rendered is shown in a sampling occurs at four locations within each pixel as shown in b the results are averaged to produce the final pixel value as shown in c some samples that are almost half covered were sampled with just one supersample point instead of two and one pixel contains image data that was missed entirely even with supersampling supersampling can also be implemented within a fragment shader the code that is used to produce the fragment color can be constructed as a function and this function can be called several times from within the main function of the fragment shader to sample the function at several discrete locations the returned values can be averaged to create the final value for the fragment results are improved if the sample positions are varied stochastically rather than spaced on a regular grid supersampling within a fragment shader has the obvious downside of requiring n times as much processing per fragment where n is the number of samples computed at each fragment there will be times when aliasing is unavoidable and supersampling is infeasible if you want to perform procedural texturing and you want a single shader that is useful at a variety of scales there little choice but to take steps to counteract aliasing in your shaders chapter procedural texturing antialiasing high frequencies aliasing does not occur until we attempt to represent a continuous image with discrete samples this conversion occurs during rasterization there are only two choices either don t have high frequency detail in the image to render or somehow deal with undersampling of high frequency detail since the former is almost never desirable due to viewing with a variety of scales we focus on the latter therefore our attempts to mitigate its effects will always occur in the fragment shader they will still include both tools of removing high frequencies or sampling at higher rates but both are done after rasterization where we can compare the frequencies of detail present in the image with the frequency of the pixels the opengl shading language has several functions for this purpose that are available only to fragment shaders to help explain the motivation for some of the language facilities for filter estimation we develop a worst case scenario alternating black and white stripes drawn on a sphere developing a fragment shader that performs antialiasing enables us to further illustrate the aliasing problem and the methods for reducing aliasing artifacts bert freudenberg developed the first version of the glsl shaders discussed in this section generating stripes the antialiasing fragment shader determines whether each fragment is to be drawn as white or black to create lines on the surface of an object the first step is to determine the method to be used for drawing lines we use a single parameter as the basis for our stripe pattern for illustration let assume that the parameter is the coordinate of the object texture coordinate we have the vertex shader pass this value to us as a floating point out variable named v eventually giving us a method for creating vertical stripes on a sphere figure 13 a shows the result of using the texture coordinate directly as the intensity grayscale value on the surface of the sphere the viewing position is slightly above the sphere so we are looking down at the north pole the texture coordinate starts off at black and increases to white as it goes around the sphere the edge where black meets white can be seen at the pole and it runs down the back side of the sphere the front side of the sphere looks mostly gray but increases from left to right we create a sawtooth wave by multiplying the texture coordinate by and taking the fractional part see figure 13 b this causes the intensity value to start at rise quickly to and then drop back down to this sequence is repeated times the glsl shader code to implement this is float sawtooth fract v antialiasing procedural textures figure 13 using the texture coordinate to create stripes on a sphere in a the texture coordinate is used directly as the intensity gray value in b a modulus function creates a sawtooth function in c the abso lute value function turns the sawtooth function into a triangle function courtesy of bert freudenberg university of magdeburg this isn t quite the stripe pattern we re after to get closer we employ the absolute value function see figure 13 c by multiplying the value of sawtooth by and subtracting we get a function that varies in the range taking the absolute value of this function results in a function that goes from down to and then back to i e a triangle wave the line of code to do this is float triangle abs sawtooth a stripe pattern is starting to appear but it either too blurry or our glasses need adjustment we make the stripes pure black and white by using the step function when we compare our triangle variable to 5 this function returns whenever triangle is less than or equal to 5 and whenever triangle is greater than 5 this could be written as float square step 5 triangle this effectively produces a square wave and the result is illustrated in figure a we can modify the relative size of the alternating stripes by adjusting the threshold value provided in the step function analytic pre ﬁltering in figure a we see that the stripes are now distinct but aliasing has reared its ugly head the step function returns values that are either or with nothing in between so the jagged edges in the transitions between white and black are easy to spot they will not go away if we increase the resolution of the image they ll just be smaller the problem is caused by the fact that the step function introduced an immediate transition from chapter procedural texturing figure 14 antialiasing the stripe pattern we can see that the square wave produced by the step function produces aliasing artifacts a the smoothstep function with a fixed width filter produces too much blurring near the equator but not enough at the pole b an adaptive approach provides reasonable antialiasing in both regions c courtesy of bert freudenberg university of magdeburg white to black or an edge which includes frequencies marching up toward infinity there is no way to sample this transition at a high enough frequency to eliminate the aliasing artifacts to get good results we need to take steps within our shader to remove such high frequencies a variety of antialiasing techniques rely on eliminating overly high frequencies before sampling this is called low pass filtering because low frequencies are passed through unmodified whereas high frequencies are eliminated the visual effect of low pass filtering is that the resulting image is blurred to eliminate the high frequencies from the stripe pattern we use the smoothstep function we know that this function produces a smooth transition between white and black it requires that we specify two edges and a smooth transition occurs between those two edges figure 14 b illustrates the result from the following line of code float square smoothstep triangle adaptive analytic pre ﬁltering analytic pre filtering produces acceptable results in some regions of the sphere but not in others the size of the smoothing filter is defined in parameter space but the parameter does not vary at a constant rate in screen space in this case the texture coordinate varies quite rapidly in screen space near the poles and less rapidly at the equator our fixed width filter produces blurring across several pixels at the equator and very little effect at the poles what we need is a way to determine the size of the antialiasing procedural textures smoothing filter adaptively so that transition can be appropriate at all scales in screen space this requires a measurement of how rapidly the function we re interested in is changing at a particular position in screen space fortunately glsl provides a built in function that can give us the rate of change derivative of any parameter in screen space the function dfdx gives the rate of change in screen coordinates in the x direction and dfdy gives the rate of change in the y direction because these functions deal with screen space they are available only in a fragment shader these two functions can provide the information needed to compute a gradient vector for the position of interest given a function f x y the gradient of f at the position x y is defined as the vector f x y in english the gradient vector comprises the partial derivative of function f with respect to x i e the measure of how rapidly f is changing in the x direction and the partial derivative of the function f with respect to y i e the measure of how rapidly f is changing in the y direction the important properties of the gradient vector are that it points in the direction of the maximum rate of increase of the function f x y the gradient direction and that the magnitude of this vector equals the maximum rate of increase of f x y in the gradient direction these properties are useful for image processing too as we see later the built in functions dfdx and dfdy give us exactly what we need to define the gradient vector for functions used in fragment shaders the magnitude of the gradient vector for the function f x y is commonly called the gradient of the function f x y it is defined as ig f x y i f x f y in practice it is not always necessary to perform the possibly costly square root operation the gradient can be approximated with absolute values ig f x y i if x y f x y i if x y f x y i this is exactly what is returned by the built in function fwidth the sum of the absolute values is an upper bound on the width of the sampling filter needed to eliminate aliasing if it is too large the resulting image looks somewhat blurrier than it should but this is usually acceptable chapter procedural texturing the two methods of computing the gradient are compared in figure 15 as you can see there is little visible difference because the value of the gradient was quite small for the function being evaluated on this object the values were scaled so that they would be visible figure 15 visualizing the gradient in a the magnitude of the gradient vector is used as the intensity gray value in b the gradient is approximated with absolute values actual gradient values are scaled for visualization courtesy of bert freudenberg university of magdeburg to compute the actual gradient for the in variable v within a fragment shader we use float width length dfdx v dfdy v to approximate it we use the potentially higher performance calculation float width fwidth v we then use the filter width within our call to smoothstep as follows float edge dp frequency float square smoothstep 5 edge 5 edge triangle if we put this all together in a fragment shader we get example 11 example 11 fragment shader for adaptive analytic antialiasing version core uniform float frequency stripe frequency uniform uniform antialiasing procedural textures in float v generic varying in float lightintensity out fragcolor void main float sawtooth fract v frequency float triangle abs sawtooth float dp length dfdx v dfdy v float edge dp frequency float square smoothstep 5 edge 5 edge triangle color mix square fragcolor color fragcolor rgb lightintensity if we scale the frequency of our texture we must also increase the filter width accordingly after the value of the function is computed it is replicated across the red green and blue components of a and used as the color of the fragment the results of this adaptive antialiasing approach are shown in figure 14 c the results are much more consistent across the surface of the sphere a simple lighting computation is added and the resulting shader is applied to the teapot in figure figure effect of adaptive analytical antialiasing on striped teapots on the left the teapot is drawn with no antialiasing on the right the adaptive antialiasing shader is used a small portion of the striped surface is magnified percent to make it easier to see the difference this approach to antialiasing works well until the filter width gets larger than the frequency this is the situation that occurs at the north pole of the sphere the stripes very close to the pole are much thinner than one pixel so no step function will produce the correct gray value here in such regions you need to switch to integration or frequency clamping both of which are discussed in subsequent sections chapter procedural texturing analytic integration the weighted average of a function over a specified interval is called a convolution the values that do the weighting are called the convolution kernel or the convolution filter in some cases we can reduce or eliminate aliasing by determining the convolution of a function ahead of time and then sampling the convolved function rather than the original function the convolution can be performed over a fixed interval in a computation that is equivalent to convolving the input function with a box filter a box filter is far from ideal but it is simple and easy to compute and often good enough this method corresponds to the notion of antialiasing by area sampling it is different from point sampling or super sampling in that we attempt to calculate the area of the object being rendered relative to the sampling region referring to figure 12 if we used an area sampling technique we would get more accurate values for each of the pixels and we wouldn t miss that pixel that just had a sliver of coverage in advanced renderman creating cgi for motion pictures apodaca and gritz explain how to perform analytic antialiasing of a periodic step function sometimes called a pulse train darwyn peachey described how to apply this method to his procedural brick renderman shader in texturing and modeling a procedural approach and dave baldwin published a glsl version of this shader in the original paper on the opengl shading language we use this technique to analytically antialias the procedural brick shader we introduced at the beginning of this chapter in the subsection regular patterns on page this example uses the step function to produce the periodic brick pattern the function that creates the brick pattern in the horizontal direction is illustrated in figure 17 from to brickpct x the brick width fraction the function is at the value of brickpct x there is an edge with infinite slope as the function drops to 0 at the value the function jumps back up to 0 and the process is repeated for the next brick the key to antialiasing this function is to compute its integral or accumulated value we have to consider the possibility that in areas of high complexity the filter width that is computed by fwidth will cover several of these pulses by sampling the integral rather than the function itself we get a properly weighted average and avoid the high frequencies caused by point sampling that would produce aliasing artifacts so what is the integral of this function it is illustrated in figure 18 from 0 to brickpct x the function value is so the integral increases with a slope of from brickpct x to 0 the function has a value of 0 so the antialiasing procedural textures integral stays constant in this region at the function jumps back to 0 so the integral increases until the function reaches brickpct x at this point the integral changes to a slope of 0 again and this pattern of ramps and plateaus continues figure 17 periodic step function the periodic step function or pulse train that defines the horizontal com ponent of the procedural brick texture figure 18 periodic step function pulse train and its integral chapter procedural texturing we perform antialiasing by determining the value of the integral over the area of the filter and we do that by evaluating the integral at the edges of the filter and subtracting the two values the integral for this function consists of two parts the sum of the area for all the pulses that have been fully completed before the edge we are considering and the area of the partially completed pulse for the edge we are considering for our procedural brick shader we use the variable position x as the basis for generating the pulse function in the horizontal direction so the number of fully completed pulses is just floor position x because the height of each pulse is 0 the area of each fully completed pulse is just brickpct x multiplying floor position x by brickpct x gives the area for all the fully completed pulses the edge that we re considering may be in the part of the function that is equal to 0 or it may be in the part of the function that is equal to we can find out by computing fract position x 0 brickpct x if the result of this subtraction is less than 0 we were in the part of the function that returns 0 so nothing more needs to be done but if the value is greater than 0 we are partway into a region of the function that is equal to because the height of the pulse is the area of this partial pulse is fract position x 0 brickpct x therefore the second part of our integral is the expression max fract position x 0 brickpct x 0 0 we use this integral for both the horizontal and vertical components of our procedural brick pattern because the application knows the brick width and height fractions brickpct x and brickpct y it can easily compute 0 brickpct x and 0 brickpct y and provide them to our fragment shader as well this keeps us from unnecessarily computing these values several times for every fragment that is rendered we call these values the mortar percentage because we evaluate this expression twice with different arguments we define it as a macro or a function for convenience define integral x p notp floor x p max fract x notp 0 0 the parameter p indicates the value that is part of the pulse i e when the function is 0 and notp indicates the value that is not part of the pulse i e when the function is 0 using this macro we can write the code to compute the value of the integral over the width of the filter as follows fw usebrick fw fwidth position usebrick integral position fw brickpct mortarpct integral position brickpct mortarpct fw antialiasing procedural textures the result is divided by the area of the filter a box filter is assumed in this case to obtain the average value for the function in the selected interval antialiased brick fragment shader now we can put all this to work to build better bricks we replace the simple point sampling technique with analytic integration the resulting shader is shown in example 12 the difference between the aliased and antialiased brick shaders is shown in figure 19 figure 19 brick shader with and without antialiasing on the left the results of the brick shader without antialiasing on the right results of antialiasing by analytic integration inc example 12 source code for an antialiased brick fragment shader version core uniform brickcolor mortarcolor uniform bricksize uniform brickpct uniform mortarpct in mcposition in float lightintensity out fragcolor define integral x p notp floor x p max fract x notp 0 0 void main position fw usebrick color determine position within the brick pattern chapter procedural texturing position mcposition bricksize adjust every other row by an offset of half a brick if fract position y 0 5 0 5 position x 0 5 calculate filter size fw fwidth position perform filtering by integrating the pulse made by the brick pattern over the filter width and height usebrick integral position fw brickpct mortarpct integral position brickpct mortarpct fw determine final color color mix mortarcolor brickcolor usebrick x usebrick y color lightintensity fragcolor color 0 frequency clamping certain functions do not have an analytic solution or they are just too difficult to solve if this is the case you might try a technique called frequency clampings in this technique the average value of the function replaces the actual value of the function when the filter width is too large this is convenient for functions such as sine and noise whose average is known antialiased checkerboard fragment shader the checkerboard pattern is the standard measure of the quality of an antialiasing technique see figure 20 larry gritz wrote a checkerboard renderman shader that performs antialiasing by frequency sampling and dave baldwin translated this shader to glsl example 13 shows a fragment shader that produces a procedurally generated antialiased checkerboard pattern the vertex shader transforms the vertex position and passes along the texture coordinate nothing more the application provides values for the two colors of the checkerboard pattern the average of these two colors the application can compute this and provide it through a uniform variable rather than having the fragment shader compute it for every fragment and the frequency of the checkerboard pattern the fragment shader computes the appropriate size of the filter and uses it to perform smooth interpolation between adjoining checkerboard squares if the filter is too wide i e the in variable is changing too quickly for proper filtering the average color is substituted even though this fragment shader uses a conditional statement care is taken to avoid aliasing in the transition zone between the if clause and the else clause antialiasing procedural textures figure 20 checkerboard pattern rendered with the antialiased checkerboard shader on the left the filter width is set to 0 so aliasing occurs on the right the filter width is computed using the fwidth function a smooth interpolation is performed between the computed color and the average color example 13 source code for an antialiased checkerboard fragment shader version core uniform uniform uniform avgcolor uniform float frequency in texcoord fragcolor void main color chapter procedural texturing determine the width of the projection of one pixel into t space fw fwidth texcoord determine the amount of fuzziness fuzz fw frequency 0 float fuzzmax max fuzz fuzz t determine the position in the checkerboard pattern checkpos fract texcoord frequency if fuzzmax 0 5 if the filter width is small enough compute the pattern color p smoothstep 0 5 fuzz 0 5 checkpos 0 smoothstep 0 0 fuzz checkpos color mix p x p y 0 p x 0 p y fade in the average color when we get close to the limit color mix color avgcolor smoothstep 0 0 5 fuzzmax else otherwise use only the average color color avgcolor fragcolor color 0 procedural antialiasing summary with increased freedom comes increased responsibility the opengl shading language permits the computation of procedural textures without restriction it is quite easy to write a shader that exhibits unsightly aliasing artifacts using a conditional or a step function is all it takes and it can be difficult to eliminate these artifacts after describing the aliasing problem in general terms this chapter explored several options for antialiasing procedural textures facilities in the language such as the built in functions for smooth interpolation smoothstep for determining derivatives in screen space dfdx dfdy and for estimating filter width fwidth can assist in the fight against jaggies moiré patterns and sparkling points these functions were fundamental components of shaders that were presented to perform antialiasing by prefiltering adaptive prefiltering integration and frequency clamping antialiasing procedural textures noise in computer graphics it easy to make things look good by definition geometry is drawn and rendered precisely however when realism is a goal perfection isn t always such a good thing real world objects have dents and dings and scuffs they show wear and tear computer graphics artists have to work hard to make a perfectly defined bowling pin look like it has been used and abused for 20 years in a bowling alley or to make a spaceship that seems a little worse for wear after many years of galactic travel this was the problem that ken perlin was trying to solve when he worked for a company called magi in the early magi was working with disney on the original feature film tron that was the most ambitious film in its use of computer graphics until that time perlin recognized the imperfection of the perfectly rendered objects in that film and he resolved to do something about it with techniques still highly useful today in a seminal paper published in perlin described a renderer that he had written that used a technique he called noise his definition of noise was a little different from the common definition of noise normally when we refer to noise we re referring to something like a random changing pattern of pixels on an old television with no signal also called snow or a grainy image taken with a digital camera in low light induced by thermal noise however an always changing randomness like this isn t that useful for computer graphics for computer graphics we need a function that is repeatable so that an object can be drawn from different view angles we also need the ability to draw the object the same way frame after frame in an animation normal random number functions do not depend on any input location so an object rendered with such a function would look different each time it was drawn the visual artifacts caused by this type of rendering would look horrible as the object was moved around the screen what is needed is a function that produces the same output value for a given input location every time and yet gives the appearance of randomness that is for a typical surface we want random variation across space but not across time unless that is also desired this function also needs to be continuous at all levels of detail fast to compute and have some other important properties discussed shortly perlin was the first to come up with a usable function perlin noise for that purpose since then a variety of similar noise functions have been defined and used in combinations to produce interesting rendering effects such as rendering natural phenomena clouds fire smoke wind effects etc rendering natural materials marble granite wood mountains etc chapter procedural texturing rendering man made materials stucco asphalt cement etc adding imperfections to perfect models rust dirt smudges dents etc adding imperfections to perfect patterns wiggles bumps color variations etc adding imperfections to time periods time between blinks amount of change between successive frames etc adding imperfections to motion wobbles jitters bumps etc actually the list is endless today most rendering libraries include support for perlin noise or something nearly equivalent it is a staple of realistic rendering and it been heavily used in the generation of computer graphics images for the movie industry for his groundbreaking work in this area perlin was presented with an academy award for technical achievement in because noise is such an important technique it is included as a built in function in the opengl shading language however not all glsl platforms implement it or implement it in exactly the same way so if you need maximum portability you ll want to use methods you have complete control over giving complete portability we ll focus on such a portable method in this section once you have a source of noise there are several ways to make use of it within a fragment shader after laying the groundwork for a portable noise we take a look at several shader examples that employ noise to achieve a variety of interesting effects deﬁnition of noise the purpose of this section is to provide a definition and enough of an intuitive feel that you can grasp the noise based opengl shaders presented in this section and then use glsl to create additional noise based effects as ken perlin describes it you can think of noise as seasoning for graphics it often helps to add a little noise a perfect model looks a little less perfect and therefore a little more realistic if some subtle noise effects are applied the ideal noise function has the following important qualities that make it the valuable tool we need for creating a variety of interesting effects needed for successful use in modeling rendering or animation it does not show any obvious regular or repeated patterns it is a continuous function and its derivative is also continuous that is there are no sudden steps or sharp bends only smooth variation noise and zooming in to smaller and smaller scales still shows only smooth variation it is a function that is repeatable across time i e it generates the same value each time it is presented with the same input it has a well defined range of output values usually the range is or 0 it is a function whose small scale form is roughly independent of large scale position there is an underlying frequency to variation or statistical character that is the same everywhere it is a function that is isotropic its statistical character is the same in all directions it can be defined for or even more dimensions it is fast to compute for any given input in practice all this adds up to a noise function that quickly and smoothly perturbs or adds an apparent element of randomness to an initial regular periodic pattern for example taking a normal square grid and moving each intersection a bit in a some psuedo random direction a variety of functions can do this but each makes various trade offs in quality and performance so they meet the preceding criteria with varying degrees of success we can construct a simple noise function called value noise by peachey by first assigning a pseudorandom number in the range to each integer value along the x axis as shown in figure 21 and then smoothly figure 21 a discrete noise function chapter procedural texturing interpolating between these points as shown in figure 22 the function is repeatable in that for a given input value it always returns the same output value figure 22 a continuous noise function a key choice to be made in this type of noise function is the method used to interpolate between successive points linear interpolation is not good enough because it is not continuous making the resulting noise pattern show obvious artifacts a cubic interpolation method is usually used to produce smooth looking results by varying the frequency and the amplitude you can get a variety of noise functions see figure as you can see the features in these functions get smaller and closer together as the frequency increases and the amplitude decreases when two frequencies are related by a ratio of it called an octave figure illustrates five octaves of the noise function these images of noise don t look all that useful but by themselves they can provide some interesting characteristics to shaders if we add the functions at different frequencies see figure 24 we start to see something that looks even more interesting the result is a function that contains features of various sizes the larger bumps from the lower frequency functions provide the overall shape whereas the smaller bumps from the higher frequency functions provide detail and interest at a smaller scale the function that results from summing the noise of consecutive octaves each at half the amplitude of the previous octave was called f noise by perlin but the terms fractional brownian motion or fbm are used more commonly today if you sum octaves of noise in a procedural shader at some point you will begin to add frequencies that cause aliasing artifacts when the frequency noise frequency 5 amplitude 5 0 frequency 5 amplitude 5 0 5 frequency 5 amplitude 5 0 frequency 5 32 amplitude 5 0 frequency 5 64 amplitude 5 0 figure 23 varying the frequency and the amplitude of the noise function of noise is greater than twice the frequency of sampling e g pixel spacing you really do start getting random sample values that will cause the flickering forms of aliasing hence algorithms for antialiasing noise functions typically stop adding detail higher frequency noise before this occurs this is another useful feature of the noise function it can be faded to the average sample value at the point at which aliasing artifacts would begin to occur the noise function defined by perlin perlin noise is sometimes called gradient noise it is defined as a function whose value is 0 at each integer input value and its shape is created by defining a pseudorandom gradient vector for the function at each of these points the characteristics of this noise function make it a somewhat better choice in general for the effects we re after it is used for the implementation of the noise function in chapter procedural texturing figure 24 summing noise functions shows the result of summing noise functions of different amplitude and frequency renderman and it is also intended to be used for implementations of the noise function built into glsl lots of other noise functions have been defined and there are many ways to vary the basic ideas the examples of perlin noise shown previously have a frequency multiplier of but it can be useful to use a frequency noise multiplier such as 21 that is not an integer value this frequency multiplier is called the lacunarity of the function the word comes from the latin word lacuna which means gap using a value larger than allows us to build up more variety more quickly e g by summing fewer octaves to achieve the same apparent visual complexity similarly it is not necessary to divide the amplitude of each successive octave by summed noise functions are the basis for the terrain and features found in the planet building software package mojoworld from pandromeda in texturing and modeling a procedural approach ken musgrave defines a fractal as a geometrically complex object the complexity of which arises through the repetition of a given form over a range of scales the relationship between the change in frequency and the change in amplitude determines the fractal dimension of the resulting function if we use a noise function as the basis for generating a terrain model we can take steps to make it behave differently at different locations for instance natural terrain has plains rolling hills foothills and mountains varying the fractal dimension based on location can create a similar appearance such a function is called a multifractal you can achieve interesting effects by using different noise functions for different situations or by combining noise functions of different types it not that easy to visualize in advance the results of calculations that depend on noise values so varied experience will be a key ally as you try to achieve the effect you re after noise armed with a basic idea of what the noise function looks like in one dimension we can take a look at two dimensional noise figure contains images of perlin noise at various frequencies mapped into the range 0 and displayed as a grayscale image each successive image is twice the frequency of the previous one in each image the contrast has been enhanced to make the peaks brighter and the valleys darker in actual use each subsequent image has an average that is half the previous one and an amplitude that is half the previous one if we were to print images of the actual values the images would be much grayer and it would be harder to see what noise really looks like as in the case adding the different frequency functions provides more interesting results figure chapter procedural texturing figure basic noise at frequencies and 32 contrast enhanced figure summed noise at and octaves contrast enhanced the first image in figure is exactly the same as the first image in figure the second image in figure is the sum of the first image in figure plus half of the second image in figure shifted so that its average intensity value is 0 this causes intensity to be increased in some areas and decreased in others the third image in figure 26 adds the third octave of noise to the first two and the fourth image in figure 26 adds the fourth octave the fourth picture is starting to look a little bit like clouds in the sky higher dimensions of noise and noise functions are obvious extensions of the and functions it a little hard to generate pictures of noise but the images in figure can be thought of as slices out of a noise function neighboring slices have continuity between them often a higher dimension of noise is used to control the time aspect of the next lower dimension noise function for instance noise can add some wiggle to otherwise straight lines in a drawing if you have a noise function one dimension can control the wiggle and the second dimension can animate the effect i e make the wiggles move in successive frames similarly a noise function can create a cloud pattern whereas a noise function can generate the cloud pattern noise and animate it in a realistic way with a noise function you can create a object like a planet and use the fourth dimension to watch it evolve in fits and starts using noise in the opengl shading language you include noise in a shader in the following three ways use glsl built in noise functions write your own noise function in glsl use a texture map to store a previously computed noise function with today graphics systems options and give the best portability and option typically gives the best performance here we will focus on techniques based on option option is not done to the exclusion of options and as the previously computed noise function comes from them the difference is really whether the function is computed on the fly for arbitrary inputs options and or precomputed and stored away for a predetermined set of inputs option typically as a texture map noise textures the programmability offered by glsl lets us use values stored in texture memory in new and unique ways we can precompute a noise function and save it in a or texture map we can then access this texture map or texture maps from within a shader because textures can contain up to four components we can use a single texture map to store four octaves of noise or four completely separate noise functions example 14 shows a c function that generates a noise texture this function creates an rgba texture with the first octave of noise stored in the red texture component the second octave stored in the green texture component the third octave stored in the blue component and the fourth octave stored in the alpha component each octave has twice the frequency and half the amplitude as the previous one this function assumes the existence of a function that can generate noise values in the range if you want you can start with perlin c implementation john kessenich made some changes to that code adding a setnoisefrequency function to produce noise values that wrap smoothly from one edge of the array to the other this means we can use the texture with the wrapping mode set to and we won t see any discontinuities in the function when it wraps the revised version of the code is in the glsldemo program from chapter procedural texturing example 14 c function to generate a noise texture int gluint 0 glubyte void void int f i j k inc int startfrequency int numoctaves double ni double inci incj inck int frequency startfrequency glubyte ptr double amp 0 5 if glubyte malloc null fprintf stderr error could not allocate noise texture n exit for f 0 inc 0 f numoctaves f frequency inc amp 0 5 setnoisefrequency frequency ptr ni 0 ni ni 0 inci 0 frequency for i 0 i i ni 0 inci incj 0 frequency for j 0 j j ni incj inck 0 frequency for k 0 k k ni inck ptr ptr inc glubyte ni 0 amp 0 noise this function computes noise values for four octaves of noise and stores them in a rgba texture of size this code also assumes that each component of the texture is stored as an bit integer value the first octave has a frequency of and an amplitude of 0 5 in the innermost part of the loop we call the function to generate a noise value based on the current value of ni the function returns a value in the range so by adding we end up with a noise value in the range 0 multiplying by our amplitude value of 0 5 gives a value in the range 0 finally we multiply by to give us an integer value in the range 0 that can be stored in the red component of a texture when accessed from within a shader the value is a floating point value in the range 0 0 5 the amplitude value is cut in half and the frequency is doubled in each pass through the loop the result is that integer values in the range 0 64 are stored in the green component of the noise texture integer values in the range 0 32 are stored in the blue component of the noise texture and integer values in the range 0 are stored in the alpha component of the texture we generated the images in figure by looking at each of these channels independently after scaling the values by a constant value that allowed them to span the maximum intensity range i e integer values in the range 0 255 or floating point values in the range 0 after the values for the noise texture are computed the texture can be provided to the graphics hardware with the code in example 15 first we pick a texture unit and bind to it the texture we ve created we set up its wrapping parameters so that the texture wraps in all three dimensions this way we always get a valid result for our noise function no matter what input values are used we still have to be somewhat careful to avoid using the texture in a way that makes obvious repeating patterns the next two lines set the texture filtering modes to linear because the default is mipmap linear and we re not using mipmap textures here we are controlling the scaling factors from within our noise shaders so a single texture is sufficient though we won t go into it more deeply here using a mipmapped texture will improve quality when using a broad range of level of detail when zoomed in to avoid seeing blockiness in the noise you ll need a base texel frequency two times greater than the highest frequency noise when zoomed out you ll need a properly filtered mipmap to avoid seeing aliasing when the pixel frequency approaches or surpasses the noise frequency when all the parameters are set up we can download the noise texture to the hardware by using the function chapter procedural texturing example 15 a function for activating the noise texture void glgentextures glactivetexture glbindtexture noise3dtexname gltexparameterf gl_texture_wrap_s gltexparameterf gl_texture_wrap_t gltexparameterf gltexparameterf gl_texture_mag_filter gltexparameterf gl_texture_min_filter glteximage3d 0 gl_rgba noise3dtexsize 0 gl_rgba gl_unsigned_byte this is an excellent approach if the period of repeatability can be avoided in the final rendering one way to avoid it is to make sure that no texture value is accessed more than once when the target object is rendered for instance if a texture is being used and the position on the object is used as the input to the noise function the repeatability won t be visible if the entire object fits within the texture trade offs as previously mentioned three methods can be used to generate noise values in a shader how do you know which is the best choice for your application a lot depends on the underlying implementation but generally speaking if we assume a hardware computation of noise that does not use texturing the points favoring usage of glsl built in noise function are the following it doesn t consume any texture memory a 128 128 texture map stored as rgba with bits per component uses of texture memory it doesn t use a texture unit it is a continuous function rather than a discrete one so it does not look pixelated no matter what the scaling is the repeatability of the function should be undetectable especially for 2d and noise but it depends on the hardware implementation shaders written with the built in noise function don t depend on the application to set up appropriate textures noise the advantages of using a texture map to implement the noise function are as follows because the noise function is computed by the application the appli cation has total control of this function and can ensure matching behavior on every hardware platform you can store four noise values i e one each for the r g b and a values of the texture at each texture location this lets you precompute four octaves of noise for instance and retrieve all four values with a single texture access accessing a texture map may be faster than calling the built in noise function user defined functions can implement noise functions that provide a different appearance from that of the built in noise functions a user defined function can also provide matching behavior on every platform whereas the built in noise functions cannot at least not until all graphics hardware developers support the noise function in exactly the same way but hardware developers will optimize the built in noise function perhaps accelerating it with special hardware so it is apt to be faster than user defined noise functions in the long run using the built in noise function or user defined noise functions will be the way to go for most applications this will result in noise that doesn t show a repetitive pattern has greater numerical precision and doesn t use up any texture resources applications that want full control over the noise function and can live within the constraints of a fixed size noise function can be successful using textures for their noise with current generation hardware noise textures may also provide better performance and require fewer instructions in the shader a simple noise shader now we put all these ideas into some shaders that do some interesting rendering for us the first shader we look at uses noise in a simple way to produce a cloud effect application setup very little needs to be passed to the noise shaders discussed in this section or in turbulence and granite the vertex position must be passed in as always and the surface normal is needed for performing lighting computations colors and scale factors are parameterized as uniform variables for the various shaders chapter procedural texturing vertex shader the code shown in example is the vertex shader that we use for the four noise fragment shaders that follow it is fairly simple because it really only needs to accomplish three things as in all vertex shaders our vertex shader transforms the incoming vertex value and stores it in the built in special variable using the incoming normal and the uniform variable lightpos the vertex shader computes the light intensity from a single white light source and applies a scale factor of 5 to increase the amount of illumination the vertex shader scales the incoming vertex value and stores it in the out variable mcposition this value is available to us in our fragment shader as the modeling coordinate position of the object at every fragment it is an ideal value to use as the input for our texture lookup no matter how the object is drawn fragments always produce the same position values or very close to them therefore the noise value obtained for each point on the surface is also the same or very close to it the application can set a uniform variable called scale to optimally scale the object in relationship to the size of the noise texture example cloud vertex shader version core uniform mvmatrix uniform mvpmatrix uniform normalmatrix uniform lightpos uniform float scale in mcvertex in mcnormal out float lightintensity out mcposition void main ecposition mvmatrix mcvertex mcposition mcvertex scale tnorm normalize normalmatrix mcnormal lightintensity dot normalize lightpos ecposition tnorm lightintensity 5 mvpmatrix mcvertex noise fragment shader after we ve computed a noise texture and used opengl calls to download it to the graphics card we can use a fairly simple fragment shader together with the vertex shader described in the previous section to make an interesting cloudy sky effect see example 17 this shader results in something that looks like the sky on a mostly cloudy day you can experiment with the color values to get a result that is visually pleasing this fragment shader receives as input the two in variables lightintensity and mcposition that were computed by the vertex shader shown in the previous section these values were computed at each vertex by the vertex shader and then interpolated across the primitive by the rasterization hardware here in our fragment shader we have access to the interpolated value of each of these variables at every fragment the first line of code in the shader performs a texture lookup on our noise texture to produce a four component result we compute the value of intensity by summing the four components of our noise texture this value is then scaled by 5 and used to perform a linear blend between two colors white and sky blue the four channels in our noise texture have mean values of 0 0 0 and 0 an additional 0 term is added to account for the average values of all the octaves at higher frequencies you can think of this as fading to the average values of all the higher frequency octaves that aren t being included in the calculation as described earlier in definition of noise scaling the sum by 5 stretches the resulting value to use up more of the range from 0 the computed color is then scaled by lightintensity value to simulate a diffuse surface lit by a single light source the result is assigned to the out variable fragcolor with an alpha value of 0 to produce the color value that is used by the remainder of the opengl pipeline an object rendered with this shader is shown in figure notice that the texture on the teapot looks a lot like the final image in figure 26 example 17 fragment shader for cloudy sky effect version core uniform noise uniform skycolor 0 0 0 0 0 uniform cloudcolor 0 0 0 in float lightintensity in mcposition out fragcolor chapter procedural texturing void main noisevec texture noise mcposition float intensity noisevec 0 noisevec noisevec noisevec 0 5 color mix skycolor cloudcolor intensity lightintensity fragcolor color 0 figure 27 teapots rendered with noise shaders clockwise from upper left a cloud shader that sums four octaves of noise and uses a blue to white color gradient to code the result a sun surface shader that uses the absolute value function to introduce discon tinuities turbulence a granite shader that uses a single high frequency noise value to modulate between white and black a marble shader that uses noise to modulate a sine function to produce alternating veins of color inc turbulence we can obtain some additional interesting effects by taking the absolute value of the noise function this technique introduces a discontinuity of the derivative because the function folds on itself when it reaches 0 when this folding is done to noise functions at several frequencies and the results are summed the result is cusps or creases in the texture at various scales perlin started referring to this type of noise as turbulence because it is reminiscent of turbulent flow it shows up in a variety of places in nature noise so this type of noise can be used to simulate various things like flames or lava the two dimensional appearance of this type of noise is shown in figure 28 figure 28 absolute value noise or turbulence sun surface shader we can achieve an effect that looks like a pit of hot molten lava or the surface of the sun by using the same vertex shader as the cloud shader and a slightly different fragment shader the main difference is that we scale each noise value and shift it over so that it is centered at 0 then we take its absolute value after summing the values we scale the result again to occupy nearly the full range of 0 we clamp this value and use it to mix between yellow and red to get the result shown in figure 27 see example 18 this technique can be extended to change the results over time using another dimension of noise for time resulting in animation of the effect chapter procedural texturing example 18 sun surface fragment shader version core in float lightintensity in mcposition uniform noise uniform 0 0 7 0 0 uniform 0 0 0 0 uniform float noisescale out fragcolor void main noisevec texture noise mcposition noisescale float intensity abs noisevec 0 0 abs noisevec 0 abs noisevec 0 abs noisevec 0 intensity clamp intensity 6 0 0 0 0 color mix color1 color2 intensity lightintensity fragcolor color 0 marble yet another variation on the noise function is to use it as part of a perio dic function such as sine by adding noise to the input value for the sine function we get a noisy oscillating function we use this to create a look similar to the alternating color veins of some types of marble example 19 shows the fragment shader to do it again we use the same vertex shader results of this shader are also shown in figure 27 example 19 fragment shader for marble version core uniform noise uniform marblecolor uniform veincolor in float lightintensity in mcposition out fragcolor noise void main noisevec texture noise mcposition float intensity abs noisevec 0 0 abs noisevec 0 abs noisevec 0 abs noisevec 0 float sineval sin mcposition y 6 0 intensity 12 0 0 5 0 5 color mix veincolor marblecolor sineval lightintensity fragcolor color 0 granite with noise it also easy just to try to make stuff up in this example we want to simulate a grayish rocky material with small black specks to generate a relatively high frequency noise texture we use only the fourth component the highest frequency one we scale it by an arbitrary amount to provide an appropriate intensity level and then use this value for each of the red green and blue components the shader in example 20 generates an appearance similar to granite as shown in figure 27 example 20 granite fragment shader version core uniform noise uniform float noisescale in float lightintensity in mcposition out fragcolor void main noisevec texture noise noisescale mcposition float intensity min 0 noisevec 18 0 color intensity lightintensity fragcolor color 0 wood we can do a fair approximation of wood with this approach as well in advanced renderman anthony a apodaca and larry gritz describe a chapter procedural texturing model for simulating the appearance of wood we can adapt their approach to create wood shaders in glsl following are the basic ideas behind the wood fragment shader shown in example 21 wood is composed of light and dark areas alternating in concentric cylinders surrounding a central axis noise is added to warp the cylinders to create a more natural looking pattern the center of the tree is taken to be the y axis throughout the wood a high frequency grain pattern gives the appearance of wood that has been sawed exposing the open grain nature of the wood the wood shader uses the same vertex shader as the other noise based shaders discussed in this section application setup the wood shaders don t require too much from the application the application is expected to pass in a vertex position and a normal per vertex using the usual opengl entry points in addition the vertex shader takes a light position and a scale factor that are passed in as uniform variables the fragment shader takes a number of uniform variables that parameterize the appearance of the wood the uniform variables needed for the wood shaders are initialized as follows lightpos 0 0 0 0 0 scale 0 lightwood 0 6 0 0 darkwood 0 0 0 07 ringfreq 0 lightgrains 0 darkgrains 0 0 grainthreshold 0 5 noisescale 0 5 0 0 noisiness 0 grainscale 27 0 fragment shader example 21 shows the fragment shader for procedurally generated wood noise example 21 fragment shader for wood version core uniform noise uniform lightwood uniform darkwood uniform float ringfreq uniform float lightgrains uniform float darkgrains uniform float grainthreshold uniform noisescale uniform float noisiness uniform float grainscale in float lightintensity in mcposition out fragcolor void main noisevec texture noise mcposition noisescale noisiness location mcposition noisevec float dist sqrt location x location x location z location z dist ringfreq float r fract dist noisevec 0 noisevec noisevec 0 if r 0 r 0 r color mix lightwood darkwood r r fract mcposition x mcposition z grainscale 0 5 noisevec r if r grainthreshold color lightwood lightgrains noisevec else color lightwood darkgrains noisevec color lightintensity fragcolor color 0 as you can see we ve parameterized quite a bit of this shader through the use of uniform variables to make it easy to manipulate through the application user interface as in many procedural shaders the object position is the basis for computing the procedural texture in this case the object position is multiplied by noisescale a that allows us to scale the noise independently in the x y and z directions and the computed value is used as the index into our noise texture the noise values obtained from the texture are scaled by the value noisiness which allows us to increase or decrease the contribution of the noise chapter procedural texturing our tree is assumed to be a series of concentric rings of alternating light wood and dark wood to give some interest to our grain pattern we add the noise vector to our object position this has the effect of adding our low frequency first octave noise to the x coordinate of the position and the third octave noise to the z coordinate the y coordinate won t be used the result is rings that are still relatively circular but have some variation in width and distance from the center of the tree to compute where we are in relation to the center of the tree we square the x and z components and take the square root of the result this gives us the distance from the center of the tree the distance is multiplied by ringfreq a scale factor that gives the wood pattern more rings or fewer rings following this we attempt to create a function that goes from 0 up to 0 and then back down to 0 we add three octaves of noise to the distance value to give more interest to the wood grain pattern we could compute different noise values here but the ones we ve already obtained will do just fine taking the fractional part of the resulting value gives us a function in the range 0 0 0 multiplying this value by 0 gives us a function in the range 0 0 0 and finally by subtracting 0 from values that are greater than 0 we get our desired function that varies from 0 to 0 and back to 0 we use this triangle function to compute the basic color for the fragment using the built in mix function the mix function linearly blends lightwood and darkwood according to our computed value r at this point we would have a pretty nice result for our wood function but we attempt to make it a little better by adding a subtle effect to simulate the look of open grain wood that has been sawed you may not be able to see this effect on the object shown in figure 29 our desire is to produce streaks that are roughly parallel to the y axis so we add the x and z coordinates multiply by the grainscale factor another uniform variable that we can adjust to change the frequency of this effect add 0 5 and take the fractional part of the result again this gives us a function that varies from 0 0 0 but for the default values for grainscale 27 0 and ringfreq 0 this function for r goes from 0 to 0 much more often than our previous function for r we could just make our grains go linearly from light to dark but we try something a little more subtle we multiply the value of r by our third octave noise value to produce a value that increases nonlinearly finally we compare our value of r to the grainthreshold value the default is 0 5 if the value of r is less than grainthreshold we modify our current noise figure 29 a bust of beethoven rendered with the wood shader inc color by adding to it a value we computed by multiplying the lightwood color the lightgrains color and our modified noise value conversely if the value of r is greater than grainthreshold we modify our current color by subtracting from it a value we computed by multiplying the darkwood color the darkgrains color and our modified noise value by default the value of lightgrains is 0 and the value of darkgrains is 0 so we don t actually see any change if r is greater than grainthreshold you can play around with this effect and see if it really does help the appearance it seemed to me that it added to the effect of the wood texture for the default settings i ve chosen but there probably is a way to achieve a better effect more simply with our final color computed all that remains is to multiply the color by the interpolated diffuse lighting factor and add an alpha value of 0 to produce our final fragment value the results of our shader are applied to a bust of beethoven in figure 29 chapter 8 procedural texturing noise summary this section introduced noise an incredibly useful function for adding irregularity to procedural shaders after a brief description of the mathematical definition of this function we used it as the basis for shaders that simulated clouds turbulent flow marble granite and wood there is a noise function available as a built in function in some implementations of glsl portable noise functions can be created with user defined shader functions or textures however it is implemented noise can increase the apparent realism of an image or an animation by adding imperfections complexity and an element of apparent randomness further information the book texturing and modeling a procedural approach third edition by david s ebert et al is entirely devoted to creating images procedurally this book contains a wealth of information and inspires a ton of ideas for the creation and use of procedural models and textures it contains several significant discussions of noise including a description by perlin of his original noise function darwyn peachey also provides a taxonomy of noise functions called making noises the application of different noise functions and combinations of noise functions are discussed by ken musgrave in his section on building procedural planets the shaders written in the renderman shading language are often procedural in nature and the renderman companion by steve upstill and advanced renderman creating cgi for motion pictures by anthony a apodaca and larry gritz contain some notable examples bump mapping was invented by jim blinn and described in his siggraph paper simulation of wrinkled surfaces a very good overview of bump mapping techniques can be found in a paper titled a practical and robust bump mapping technique for today gpus by mark kilgard a photoshop plug in for creating a normal map from an image is available at nvidia developer web site http developer nvidia com most signal processing and image processing books contain a discussion of the concepts of sampling reconstruction and aliasing books by glassner wolberg and gonzalez and woods can be consulted for additional information on these topics technical memos by alvy ray smith address the issues of aliasing in computer graphics directly the book advanced renderman creating cgi for motion pictures by anthony a apodaca and larry gritz contains a chapter that describes further information shader antialiasing in terms of the renderman shading language and much of the discussion is germane to the opengl shading language as well darwyn peachey has a similar discussion in texturing modeling a procedural approach third edition by david ebert et al bert freudenberg developed a glsl shader to do adaptive antialiasing and presented this work at the siggraph in san antonio texas this subject is also covered in his ph d thesis real time stroke based halftoning ken perlin has a tutorial and history of the noise function as well as a reference implementation in the java programming language at his web site a lot of other interesting things are available on ken home page at nyu http mrl nyu edu p erlin his paper an image synthesizer appeared in the siggraph proceedings and his improvements to the original algorithm were published in the paper improving noise as part of siggraph he also described a clever method for combining two small textures to get a large perlin like noise function in the article implementing improved perlin noise in the book gpu gems chapter 8 procedural texturing chapter tessellation shaders chapter objectives after reading this chapter you ll be able to do the following understand the differences between tessellation shaders and vertex shaders identify the phases of processing that occur when using tessellation shaders recognize the various tessellation domains and know which one best matches the type of geometry you need to generate initialize data and draw using the patch geometric primitive this chapter introduces opengl tessellation shader stages it has the following major sections tessellation shaders provides an overview of how tessellation shaders work in opengl tessellation patches introduces tessellation rendering primitive the patch tessellation control shaders explains the operation and purpose of the first tessellation shading tessellation evaluation shaders describes the second tessellation stage and how it operates a tessellation example the teapot shows an example of rendering a teapot using tessellation shaders and bézier patches additional tessellation techniques discusses some additional techniques that are enabled by tessellation shading tessellation shaders up to this point only vertex shaders have been available for us to manipulate geometric primitives while there are numerous graphics techniques you can do using vertex shaders they do have their limitations one limitation is that they can t create additional geometry during their execution they really only update the data associated with the current vertex they are processing and they can t even access the data of other vertices in the primitives to address those issues the opengl pipeline contains several other shader stages that address those limitations in this chapter we introduce tessellation shaders which for example can generate a mesh of triangles using a new geometric primitive type called a patch tessellation shading adds two shading stages to the opengl pipeline to generate a mesh of geometric primitives as compared to having to specify all of the lines or triangles to form your model as you do with vertex shading with tessellation you begin by specifying a patch which is just an ordered list of vertices when a patch is rendered the tessellation control shaders executes first operating on your patch vertices and specifying how much geometry should be generated from your patch tessellation control shaders are optional and we ll see what required if you don t use one after the tessellation control shader completes the second shader the tessellation evaluation shaders positions the vertices of the generated mesh chapter tessellation shaders using tessellation coordinatess and sends them to the rasterizer or for more processing by a geometry shader which we describe in chapter 10 geometry shaders as we describe opengl process of tessellation we ll start at the beginning with describing patches in tessellation patches on page then move to describe the tessellation control shader operation detail in tessellation control shaders on page opengl passes the output of the tessellation control shader to the primitive generator which generates the mesh of geometric primitives and tessellation coordinates that the tessellation evaluation shader stage uses finally the tessellation evaluation shader positions each of the vertices in the final mesh a process described in tessellation evaluation shaders on page we conclude the chapter with a few examples including a demonstration of displacement mapping which combines texture mapping for vertices which is discussed in chapter 6 textures with tessellation shaders tessellation patches the tessellation process doesn t operate on opengl classic geometric primitives points lines and triangles but uses a new primitive added in opengl version 0 called a patch patches are processed by all of active shading stages in the pipeline by comparison other primitive types are only processed by vertex fragment and geometry shaders and bypass the tessellation stage in fact if any tessellation shaders are active passing any other type of geometry will generate a error conversely you ll get a error if you try to render a patch without any tessellation shaders specifically a tessellation evaluation shader we ll see that tessellation control shaders are optional bound patches are nothing more than a list of vertices that you pass into opengl which preserves their order during processing when rendering with tessellation and patches you use opengl rendering commands like gldrawarrays and specify the total number of vertices to be read from the bound vertex buffer objects and processed for that draw call when you re rendering with the other opengl primitives opengl implicitly knows how many vertices to use based on the primitive type you specified in your draw call like using three vertices to make a triangle however when you use a patch opengl needs to be told how many vertices from your vertex array to use to make one patch which you specify using glpatchparameteri patches processed by the same draw call will all be the same size tessellation patches to specify a patch use the input type into any opengl drawing command example demonstrates issuing two patches each with four vertices example specifying tessellation patches glfloat vertices 0 0 0 0 25 0 25 0 25 0 0 25 0 25 0 25 0 0 25 0 75 0 25 0 25 0 25 glbindvertexarray vao glbindbuffer vbo glbufferdata sizeof vertices vertices glvertexattribpointer vpos gl_float gl_false 0 0 glpatchparameteri gldrawarrays 0 8 the vertices of each patch are first processed by the currently bound vertex shader and then used to initialize the array which is implicitly declared in the tessellation control shader the number of elements in is the same as the patch size specified by glpatchparameteri inside of a tessellation control shader the variable provides the number of elements in as does querying length tessellation control shaders once your application issues a patch the tessellation control shader will be called if one is bound and is responsible for completing the following actions generate the tessellation output patch vertices that are passed to the tessellation evaluation shader as well as update any per vertex or per patch attribute values as necessary chapter tessellation shaders specify the tessellation level factors that control the operation of the primitive generator these are special tessellation control shader variables called and and are implicitly declared in your tessellation control shader we ll discuss each of these actions in turn generating output patch vertices tessellation control shaders use the vertices specified by the application which we ll call input patch vertexs to generate a new set of vertices the output patch vertices which are stored in the array of the tessella tion control shader at this point you might be asking what going on why not just pass in the original set of vertices from the application and skip all this work tessellation control shaders can modify the values passed from the application but can also create or remove vertices from the input patch vertices when producing the output patch vertices you might use this func tionality when working with sprites or when minimizing the amount of data sent from the application to opengl which may increase performance you already know how to set the number of input patch vertices using glpatchparameteri you specify the number of output patch vertices using a layout construct in your tessellation control shader as demonstrated below which sets the number of output patch vertices to layout vertices 16 out the value set by the vertices parameter in the layout directive does two things it sets the size of the output patch vertices and specifies how many times the tessellation control shader will execute once for each output patch vertex in order to determine which output vertex is being processed the tessellation control shader can use the variable its value is most often used as an index into the array while a tessellation control shader is executing it has access to all patch vertex data both input and output this can lead to issues where a shader invocation might need data values from a shader invocation that hasn t happened yet tessellation control shaders can use the glsl barrier function which causes all of the control shaders for an input patch to execute and wait until all of them have reached that point thus guaranteeing that all of the data values you might set will be computed a common idiom of tessellation control shaders is just passing the input patch vertices out of the shader example demonstrates this for an output patch with four vertices tessellation control shaders example passing through tessellation control shader patch vertices version core layout vertices out void main and then set tessellation levels tessellation control shader variables the array is actually an array of structures with each element defined as in float float and for each value that you need downstream e g in the tessellation evaluation shader you ll need to assign values similar to what we did with the field the array has the same fields but is a different size specified by which as we saw was set in the tessellation control shader out layout qualifier additionally the following scalar values described in table are provided for determining which primitive and output vertex invocation is being shaded table tessellation control shader input variables variable declaration description invocation index for the output vertex of the current tessellation control shader primitive index for current input patch number of vertices in the input patch which is the dimension of number of vertices in the output patch which is the dimension of chapter tessellation shaders if you have additional per vertex attribute values either for input or output these need to be declared as either in or out arrays in your tessellation control shader the size of an input array needs to be sized to the input patch size or can be declared unsized and opengl will appropriately allocate space for all its values similarly per vertex output attributes which you will be able to access in the tessellation evaluation shader need to be sized to the number of vertices in the output patch or can be declared unsized as well controlling tessellation the other function of a tessellation control shader is to specify how much to tessellate the output patch while we haven t discussed tessellation evaluation shaders in detail yet they control the type of output patch for rendering and consequently the domain where tessellation occurs opengl supports three tessellation domains a quadrilateral a triangle and a collection of isolines the amount of tessellation is controlled by specifying two sets of values the inner and outer tessellation levels the outer tessellation levels control how the perimeter of the domain is subdivided and is stored in an implicitly declared four element array named similarly the inner tessellation levels specify how the interior of the domain is subdivided and stored in a two element array named all tessellation level factors are floating point values and we ll see the effect that fractional values have on tessellations in a bit one final point is that while the dimensions of the implicitly declared tessellation level factors arrays are fixed the number of values used from those arrays depends on the type of tessellation domain understanding how the inner and outer tessellation levels operate is key to getting tessellation to do what you want each of the tessellation level factors specifies how many segments to subdivide a region as well as how many tessellation coordinates and geometric primitives to generate how that subdivision is done varies by domain type we ll discuss each type of domain in turn as they operate differently quad tessellation using the quadrilaterial domain may be the most intuitive so we ll begin with it it useful when your input patches are rectangular in shape as you might have when using two dimensional spline surfaces like bézier surfaces the quad domain subdivides the unit square using all of the tessellation control shaders inner and outer tessellation levels for instance if we were to set the tessellation level factors to the following values opengl would tessellate the quad domain as illustrated in figure 0 0 0 0 figure quad tessellation a tessellation of a quad domain using the tessellation levels from exam ple example tessellation levels for quad domain tessellation illustrated in figure 0 0 0 0 5 0 0 0 0 chapter tessellation shaders notice that the outer tessellation levels values correspond to the number of segments for each edge around the perimeter while the inner tessellation levels specify how many regions are in the horizontal and vertical directions in the interior of the domain also shown in figure is a possible triangularization of the domain shown using the dashed lines likewise the solid circles represent the tessellation coordinates each of which will be provided as input into the tessellation evaluation shader in the case of the quad domain the tessellation coordinates will have two coordinates u v which will both be in the range 0 and each tessellation coordinate will be passed into an invocation of an tessellation evaluation shader isoline tessellation similar to the quad domain the isoline domain also generates u v pairs as tessellation coordinates for the tessellation evaluation shader isolines however use only two of the outer tessellation levels to determine the amount of subdivsion and none of the inner tessellation levels this is illustrated in figure for the tessellation level factors shown in example example tesslation levels for an isoline domain tessellation shown in figure 0 6 8 you ll notice that there a dashed line along the v edge that because isolines don t include a tessellated isoline along that edge and if you place two isoline patches together i e two patches share an edge there isn t overlap of the edges triangle tessellation finally let discuss tessellation using a triangle domain as compared to either the quad or isolines domains coordinates related to the three vertices of a triangle aren t very conveniently represented by a u v pair instead triangular domains use barycentric coordinates to specify their tessellation coordinates barycentric coordinates are represented by a triplet of numbers a b c each of which lies in the range 0 and which triangularization of the domain is implementation dependent tessellation control shaders have the property that a b c think of a b or c as weights for each individual triangle vertex 0 0 0 0 figure isoline tessellation a tessellation of an isolines domain using the tessellations levels from ex ample as with any of the other domains the generated tessellation coordinates are a function of the tessellation level factors and in particular the first three outer tessellation levels and only inner tessellation level zero the tessellation of a triangular domain with tessellation level factors set as in example 5 is shown in figure example 5 tesslation levels for a triangular domain tessellation shown in figure 0 6 1 5 8 0 5 chapter tessellation shaders 0 1 0 0 0 1 1 1 0 0 figure triangle tessellation a tessellation of a triangular domain using the tessellation levels from example 5 as with the other domains the outer tessellation levels control the subdivision of the perimeter of the triangle and the inner tessellation level controls how the interior is partitioned as compared to the rectangular domains where the interior is partitioned in a set of rectangles forming a grid the interior of the triangular domain is partitioned into a set of concentric triangles that form the regions specifically let t represent the inner tessellation level if t is an even value then the center of the triangular domain barycentric coordinate 1 1 1 is located and then t 1 concentric triangles are generated between the center point and the perimeter conversely if t is an odd value then t 1 concentric triangles are out to the perimeter however the center point in barycentric coordinates will not be a tessellation coordinate these two scenarios are shown in figure bypassing the tessellation control shader as we mentioned often your tessellation control shader will just be a pass through shader copying data from input to output in such a case you can actually bypass using a tessellation control shader and set the tessellation control shaders odd inner tessellation levels create a small triangle in the center of the triangular tessellation domain even inner tessellation levels create a single tessellation coordinate in the center of the triangular tessellation domain figure even and odd tessellation examples of how even and odd inner tessellation levels affect triangular tessellation tessellation level factors using the opengl api as compared to using a shader the glpatchparameterfv function can be used to set the inner and outer tessellation levels tessellation evaluation shaders the final phase in opengl tessellation pipeline is the tessellation evaluation shader execution the bound tessellation evaluation shader is executed one for each tessellation coordinate that the primitive generator chapter tessellation shaders emits and is responsible for determining the position of the vertex derived from the tessellation coordinate as we ll see tessellation evaluation shaders look similar to vertex shaders in transforming vertices into screen positions unless the tessellation evaluation shader data is going to be further processed by a geometry shader the first step in configuring a tessellation evaluation shader is to configure the primitive generator which is done using a layout directive similar to what we did in the tessellation control shader its parameters specify the tessellation domain and subsequently the type of primitives generated face orientation for solid primitives used for face culling and how the tessellation levels should be applied during primitive generation specifying the primitive generation domain we ll now describe the parameters that you will use to set up the tessellation evaluation shader out layout directive first we ll talk about specifying the tessellation domain as you ve seen there are three types of domains used for generating tessellation coordinates which are described in table table evaluation shader primitive types primitive type description domain coordinates quads a rectangular domain over the unit square triangles a triangular shaped domain using barycentric coordinates isolines a collection of lines across the unit square a u v pair with u v values ranging from 0 to 1 a b c with a b and c values ranging from 0 to 1 and where a b c 1 a u v pair with u values ranging from 0 to 1 and v values ranging from 0 to almost 1 specifying the face winding for generated primitives as with any filled primitive in opengl the order the vertices are issued determines the facedness of the primitive since we don t issue the vertices directly in this case but rather have the primitive generator do it on our behalf we need to tell it the face winding of our primitives in the layout directive specify cw for clockwise vertex winding or ccw for counterclock wise vertex winding tessellation evaluation shaders specifying the spacing of tessellation coordinates additionally we can control how fractional values for the outer tessellation levels are used in determining the tessellation coordinate generation for the perimeter edges inner tessellation levels are affected by these options table describes the three spacing options available where max represents an opengl implementation maximum accepted value for a tessellation level table options for controlling tessellation level effects option description tessellation level is clamped to 1 max and is then rounded up to the next largest integer value the value is clamped to max and then rounded up to the next largest even integer value n the edge is then divided into n 2 equal length parts and two other parts one at either end which may be shorter than the other lengths the value is clamped to 1 max 1 and then rounded up to the next largest odd integer value n the edge is then divided into n 2 equal length parts and two other parts one at either end which may be shorter than the other lengths additional tessellation evaluation shader layout options finally should you want to output points as compared to isolines or filled regions you can supply the option which will render a single point for each vertex processed by the tessellation evaluation shader the order of options within the layout directive is not important as an example the following layout directive will request primitives generated on a triangular domain using equal spacing counterclockwise oriented triangles but only rendering points as compared to connected primitives layout triangles ccw points out specifying a vertex position the vertices output from the tessellation control shader i e the values in array are made available in the evaluation chapter tessellation shaders shader in the variable which when combined with tessellation coordinates can be used to generate the output vertex position tessellation coordinates are provided to the shader in the variable in example 6 we use a combination of equal spaced quads to render a simple patch in this case the tessellation coordinates are used to color the surface and illustrates how to compute the vertex position example 6 a sample tessellation evaluation shader version core layout quads ccw in out color void main float u x float omu 1 u one minus u float v y float omv 1 v one minus v color omu omv 0 u omv 1 u v 2 omu v tessellation evaluation shader variables similar to tessellation control shaders tessellation evaluation shaders have a array that is actually an array of structures with each element defined as shown in example 7 example 7 parameters for tessellation evaluation shaders in float float tessellation evaluation shaders additionally the following scalar values described in table are provided for determining which primitive and for computing the position of the output vertex table tessellation control shader input variables variable declaration description primitive index for current input patch number of vertices in the input patch which is the dimension of outer tessellation level values 2 inner tessellation level values coordinates in patch domain space of the vertex being shaded in the evaluation shader the output vertex data is stored in an interface block defined as follows out float float a tessellation example the teapot all of that theory could use a concrete demonstration in this section we ll render the famous utah teapot using bézier patches a bézier patch named after french engineer pierre bézier defines a parametric surface evaluated over the unit square using control points arranged in a grid for our example we ll use 16 control points arranged in a 4 grid as such we make the following observations to help us set up our tessellation bézier patches are defined over the unit square which indicates we should use the quads domain type that we ll specify in our layout directive in the tessellation evaluation shader each patch has 16 control points so our should be set to 16 using glpatchparameteri the 16 control points also define the number of input patch vertices which tells us our maximum index into the array in our tessellation control shader finally since the tessellation control shader doesn t add or remove any vertices to the patch the number of output patch vertices will also be 16 which specifies the value we ll use in our layout directive in the tessellation control shader chapter tessellation shaders processing patch input vertices given the information from our patches we can easily construct the tessellation control shader for our application which is shown in example 8 example 8 tessellation control shader for teapot example version core layout vertices 16 out void main 0 4 1 4 0 4 1 4 2 4 4 using the tessellation level factors from example 8 figure 5 shows the patches of the teapot shrunk slightly to expose each individual patch this is a very simple example of a tessellation control shader in fact it a great example of a pass through shader where mostly the inputs are copied to the output the shader also sets the inner and outer tessellation levels to constant values which could also be done in the application using a call to glpatchparameterfv however we include the example here for completeness evaluating tessellation coordinates for the teapot bézier patches use a bit of mathematics to determine the final vertex position from the input control points the equation mapping a tessellation coordinate to a vertex position for our 4 4 patch is 3 3 p u v b i u b j v vij i 0 j 0 a tessellation example the teapot figure 5 the tessellated patches of the teapot with p being the final vertex position vij the input control point at index i j in our input patch both of which are in glsl and b which are two scaling functions while it might not seem like it we can map easily the formula to a tessellation evaluation shader as show in example in the following shader the b function will be a glsl function we ll define in a moment we also specify our quads domain spacing options and polygon face orientation in the layout directive example the main routine of the teapot tessellation evaluation shader version core layout quads ccw out uniform mv model view matrix uniform p projection matrix void main chapter tessellation shaders p 0 0 float u x float v y for int j 0 j 4 j for int i 0 i 4 i p b i u b j v 4 j i p mv p our b function is one of the bernstein polynomials which is an entire family of mathematical functions each one returns a scalar value we re using a small select set of functions which we index using the first parameter and evaluate the function value at one component of our tessellation coordinate here the mathematical definition of our functions b i u 3 ui 1 u 3 i where the 3 is a particular instance of a mathematical construct called a binomial coefficient 2 we ll spare you the gory details and just say we re lucky that it evaluates to either 1 or 3 in our cases and which we ll hard code into a lookup table bc in the function definition and that we ll index using i as such we can rewrite b i u as b i u bci ui 1 u 3 i this also translates easily into glsl shown in example 10 example 10 definition of b i u for the teapot tessellation evaluation shader float b int i float u binomial coefficient lookup table const bc 1 3 3 1 return bc i pow u i pow 1 0 u 3 i 2 binomial coefficients in generally defined using the formula n n where n is the n 1 n a tessellation example the teapot while that conversation involved more mathematics than most of the other techniques we ve described in the book it is representative of what you will encounter when working with tessellated surfaces while discussion of the mathematics of surfaces is outside of this text copious resources are available that describe the required techniques additional tessellation techniques in this final section we briefly describe a few additional techniques you can employ while using tessellation shaders view dependent tessellation most of the examples in this chapter have set the tessellation level factors to constant values either in the shader or through uniform variables one key feature of tessellation is being able to compute tessellation levels dynamically in the tessellation control shader and in particular basing the amount of tessellation on view dependent parameters for example you might implement a level of detail scheme based on the distance of the patch from the eye location in the scene in example 11 we use the average of all the input patch vertices to specify a single representative point for the patch and derive all the tessellation level factors from the distance of that point to the eye point example 11 computing tessellation levels based on view dependent parameters uniform eyeposition void main center 0 0 for int i 0 i length i center i center length float d distance center eyeposition 1 0 const float lodscale 2 5 distance scaling variable float tesslod mix 0 0 d lodscale chapter tessellation shaders for int i 0 i 4 i i tesslod tesslod clamp 0 5 tesslod 0 0 0 tesslod 1 tesslod example 11 is a very rudimentary method for computing a patch level of detail in particular each perimeter edge is tessellated the same amount regardless of its distance from the eye this doesn t take full advantage of tessellation possibilities based on view information which is usually employed as a geometry optimization technique i e reducing the object geometric complexity the farther from the eye that object is assuming a perspective projection is used another failing of this approach is that if you have multiple patches that share edges it likely that the shared edges may be assigned different levels of tessellation depending on the objects orientation with respect to the eye position and that might lead to cracking along the shared edges cracking is an important issue with tessellation and we address another concern in shared tessellated edges and cracking on page to address guaranteeing that shared edges are tessellated the same we need to find a method that returns the same tessellation factor for those edges however as compared to example 11 which doesn t need to know anything about the logical ordering of input patch vertices any algorithm that needs to know which vertices are incident on a perimeter edge is data dependent this is because a patch is a logical ordering only the application knows how it ordered the input patch vertices for example 12 we introduce the following array of structures that contain our edge information for our tessellation control shader struct edgecenters edgecenter 4 the application would need to populate this array using the world space positions of the centers of each edge in that example we ll assume we re working with the quads domain which is why there are four points in each edgecenters structure the number of points would need to be modified for the other domains the number of edgecenters structures in the array is the number of patches that will be issued in the draw call additional tessellation techniques process we would modify the tessellation control shader to implement the following example 12 specifying tessellation level factors using perimeter edge centers struct edgecenters edgecenter 4 uniform vec3 eyeposition uniform edgecenters patch void main for int i 0 i 4 i float d distance patch edgecenter i eyeposition 1 0 const float lodscale 2 5 distance scaling variable float tesslod mix 0 0 d lodscale gl_tesslevelouter i tesslod tesslod clamp 0 5 tesslod 0 0 0 tesslod 1 tesslod gl_invocationid gl_position gl_invocationid gl_position shared tessellated edges and cracking often a geometric model that uses tessellation will have patches with shared edges tessellation in opengl guarantees that the geometry generated for the primitives within a patch won t have any cracks between them but it can t make the same claim for patches that share edges that something the application needs to address and clearly the starting point is that shared edges need to be tessellated the same amounts however there s a secondary issue that can creep in precision in mathematical computations done by a computer for all but trivial tessellation applications the points along a perimeter edge will be positioned using multiple tessellation control shader output patch vertices which are combined with the tessellation coordinates in the tessellation evaluation shader in order to truly prevent cracking along edges between similarly tessellated adjacent patches the chapter tessellation shaders order of accumulation of mathematical operations in the tessellation evaluation shader must also match depending upon how the tessellation evaluation shader generates the mesh s vertices final positions you may need to reorder the processing of vertices in the tessellation evaluation shader a common approach to this problem is to recognize the output patch vertices that contribute to a vertex incident to a perimeter edge and sort those vertices in a predictable manner say in terms of increasing magnitude along the edge another technique to avoid cracking is applying the precise qualifier to shader computations where points might be in reversed order between two shader invocations this is illustrated in figure 9 6 end points same location value end points same location value figure 9 6 tessellation cracking when walking the interior edge in opposite directions the computed sub division points need to result in the same value or the edge may crack as explained in the precise qualifier on page in chapter 2 this computation can result in different values if the expression is the same and the input values are the same but some of them are swapped due to the opposite direction of edge walking qualifying the results of such computations as precise will prevent this 