business intelligence and analytics bi a has emerged as an important area of study for both practitioners and researchers reflecting the magnitude and impact of data related problems to be solved in contemporary business organizations this introduction to the mis quarterly special issue on business intelligence research first provides a framework that identifies the evolution applications and emerging research areas of bi a bi a bi a and bi a are defined and described in terms of their key characteristics and capabilities current research in bi a is analyzed and challenges and opportunities associated with bi a research and education are identified we also report a bibliometric study of critical bi a publications researchers and research topics based on more than a decade of related academic and industry publications finally the six articles that comprise this special issue are introduced and characterized in terms of the proposed bi a research framework keywords business intelligence and analytics big data analytics web introduction i percent of companies with revenues exceeding million were found to use some form of business analytics a report business intelligence by and the mckinsey analytics global institute manyika bi a et al pre and the field of big data analytics dicted that have by the become united states alone will increasingly face a short im in both the academic and the business communities over the age of to people with deep analytical skills past two decades industry studies have highlighted this as well as a shortfall of million data sawy managers with significant development for example based on a survey of the know how to analyze big data to make effective decisions over information technology it professionals from countries and industries the ibm tech trends report hal varían chief economist at google and emeritus profes identified business analytics as one of the four major sor at the university of california berkeley commented on technology trends in the in a survey of the state of the emerging opportunities for it professionals and students business analytics by bloomberg businessweek in data analysis as follows mis quarterly vol no pp december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et ai introduction business intelligence research so what getting ubiquitous and cheap data and what is complementary to data analysis so my recommendation is to take lots of courses about how to manipulate and analyze data databases machine learning econometrics statistics visualization and so on the opportunities associated with data and analysis in dif ferent organizations have helped generate significant interest in bi a which is often referred to as the techniques tech nologies systems practices methodologies and applications that analyze critical business data to help an enterprise better understand its business and market and make timely business decisions in addition to the underlying data processing and analytical technologies bi a includes business centric practices and methodologies that can be applied to various high impact applications such as e commerce market intelli gence e government healthcare and security this introduction to the mis quarterly special issue on business intelligence research provides an overview of this exciting and high impact field highlighting its many chal lenges and opportunities figure shows the key sections of this paper including bi a evolution applications and emerging analytics research opportunities we then report on a bibliometric study of critical bi a publications researchers and research topics based on more than a decade of related bi a academic and industry publications educa tion and program development opportunities in bi a are presented followed by a summary of the six articles that appear in this special issue using our research framework the final section concludes the paper bi a evolution key characteristics and capabilities the term intelligence has been used by researchers in artificial intelligence since the business intelligence became a popular term in the business and it communities only in the in the late business analytics was introduced to represent the key analytical component in bi davenport more recently big data and big data analytics have been used to describe the data sets and ana lytical techniques in applications that are so large from terabytes to exabytes and complex from sensor to social media data that they require advanced and unique data storage management analysis and visualization technol ogies in this article we use business intelligence and ana lytics bi a as a unified term and treat big data analytics as a related field that offers new directions for bi a research bi a as a data centric approach bi a has its roots in the long standing database management field it relies heavily on various data collection extraction and analysis technologies chaudhuri et al turban et al watson and wixom the bi a technologies and applications currently adopted in industry can be considered as bi a where data are mostly structured collected by companies through various legacy systems and often stored in commer cial relational database management systems rdbms the analytical techniques commonly used in these systems popularized in the are grounded mainly in statistical methods developed in the and data mining techniques developed in the data management and warehousing is considered the foun dation of bi a design of data marts and tools for extraction transformation and load etl are essential for converting and integrating enterprise specific data database query online analytical processing olap and reporting tools based on intuitive but simple graphics are used to explore important data characteristics business performance management bpm using scorecards and dashboards help analyze and visualize a variety of performance metrics in addition to these well established business reporting func tions statistical analysis and data mining techniques are adopted for association analysis data segmentation and clustering classification and regression analysis anomaly detection and predictive modeling in various business appli cations most of these data processing and analytical tech nologies have already been incorporated into the leading com mercial bi platforms offered by major it vendors including microsoft ibm oracle and sap sallam et al among the capabilities considered essential for bi plat forms according to the gartner report by sallam et al the following eight are considered bi a reporting dashboards ad hoc query search based bi olap interactive visualization scorecards predictive modeling and data mining a few bi a areas are still under active devel opment based on the gartner bi hype cycle analysis for emerging bi technologies which include data mining work benchs column based dbms in memory dbms and real hal varían answers your questions february http time decision tools bitterer academic curricula in www freakonomics com hal varian answers your questions information systems is and computer science cs often mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al introduction business intelligence research include well structured courses such as database management web analytics tools such as google analytics can provide a systems data mining and multivariate statistics trail of the user online activities and reveal the user browsing and purchasing patterns web site design product placement optimization customer transaction analysis market bi a structure analysis and product recommendations can be accomplished through web analytics the many web since the early the internet and the web began to offer applications developed after have also created an abun unique data collection and analytical research and develop dance of user generated content from various online social ment opportunities the http based web systems media such as forums online groups web blogs social net characterized by web search engines such as google working and sites social multimedia sites for photos and videos yahoo and e commerce businesses such as amazon and and even virtual worlds and social games o reilly in ebay allow organizations to present their businesses online addition to capturing celebrity chatter references to everyday and interact with their customers directly in addition events to and socio political sentiments expressed in these porting their traditional rdbms based product information media web applications can efficiently gather a large and business contents online detailed and ip specific user volume of timely feedback and opinions from a diverse search and interaction logs that are collected seamlessly customer population for different types of businesses through cookies and server logs have become a new gold mine for understanding customers needs and identifying many new marketing researchers believe that social media business opportunities web intelligence web analytics analytics and presents a unique opportunity for businesses to treat the user generated content collected through web based the market as a conversation between businesses and social and crowd sourcing systems doan et al customers instead of the traditional business to customer o reilly have ushered in a new and exciting era one way of marketing lusch et al unlike bi a bi a research in the centered on text and web technologies that are already integrated into commercial analytics for unstructured web contents enterprise it systems future bi a systems will require the integration of mature and scalable techniques in text an immense amount of company industry product and mining e g information extraction topic identification customer information can be gathered from the web and opinion mining question answering web mining social organized and visualized through various text and web mining network analysis and spatial temporal analysis with existing techniques by analyzing customer clickstream data logs dbms based bi a systems mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al introduction business intelligence research except for basic query and search capabilities no advanced text analytics for unstructured content are currently con sidered in the capabilities of the gartner bi platforms several however are listed in the gartner bi hype cycle including information semantic services natural language question answering and content text analytics bitterer new is and cs courses in text mining and web mining have emerged to address needed technical training bi a whereas web based bi a has attracted active research from academia and industry a new research opportunity in bi a is emerging as reported prominently in an october article in the economist the number of mobile phones and tablets about million units surpassed the number of laptops and pcs about million units for the first time in although the number of pcs in use surpassed billion in the same article projected that the number of mobile connected devices would reach billion in mobile devices such as the ipad iphone and other smart phones and their complete ecosystems of downloadable applicationss from travel advisories to multi player games are transforming different facets of society from education to healthcare and from entertainment to governments other sensor based internet enabled devices equipped with rfid barcodes and radio tags the internet of things are opening up exciting new steams of innovative applications the ability of such mobile and internet enabled devices to support highly mobile location aware person centered and context relevant operations and transactions will continue to offer unique research challenges and opportunities throughout the mobile interface visualization and hcl human computer interaction design are also promising research areas although the coming of the web mobile and sensor based era seems certain the underlying mobile analytics and location and context aware techniques for collecting processing analyzing and visualizing such large scale and fluid mobile and sensor data are still unknown no integrated commercial bi a systems are foreseen for the near future most of the academic research on mobile bi is still in an embryonic stage although not included in the current bi platform core capabilities mobile bi has been included in the gartner bi hype cycle analysis as one of the new technologies that has the potential to disrupt the bi market significantly bitterer the uncertainty asso ciated with bi a presents another unique research direction for the is community table summarizes the key characteristics of bi a and in relation to the gartner bi platforms core capa bilities and hype cycle the decade of the promises to be an exciting one for high impact bi a research and development for both indus try and academia the business community and industry have already taken important steps to adopt bi a for their needs the is community faces unique challenges and opportunities in making scientific and societal impacts that are relevant and long lasting chen la is research and education pro grams need to carefully evaluate future directions curricula and action plans from bi a to bi a applications from big data to big impact several global business and it trends have helped shape past and present bi a research directions international travel high speed network connections global supply chain and outsourcing have created a tremendous opportunity for it advancement as predicted by thomas freeman in his seminal book the world is flat in addition to ultra fast global it connections the development and deployment of business related data standards electronic data interchange edi formats and business databases and information systems have greatly facilitated business data creation and utilization the development of the internet in the and the subsequent large scale adoption of the world wide web since the have increased business data generation and collection speeds exponentially recently the big data era has quietly descended on many communities from govern ments and e commerce to health organizations with an overwhelming amount of web based mobile and sensor generated data arriving at a terabyte and even exabyte scale the economist new science discovery and insights can be obtained from the highly detailed contex tualized and rich contents of relevance to any business or organization in addition to being data driven bi a is highly applied and can leverage opportunities presented by the abundant data and domain specific analytics needed in many critical and high impact application areas several of these promising and high impact bi a applications are presented below with a discussion of the data and analytics characteristics potential impacts and selected illustrative examples or studies e commerce and market intelligence e government and politics science and technology smart health and mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al lntroduction business intelligence research gartner bl platforms core key characteristics capabilities gartner hype cycle bi a dbms based structured content ad hoc query search based bl column based dbms rdbms data warehousing reporting dashboards scorecards ln memory dbms etl olap olap real time decision dashboards scorecards interactive visualization data mining workbenches data mining statistical analysis predictive modeling data mining bi a web based unstructured content information semantic information retrieval and extraction services opinion mining natural language question question answering answering web analytics and web content text analytics intelligence social media analytics social network analysis spatial temporal analysis bi a mobile and sensor based content mobile bl location aware analysis person centered analysis context relevant analysis mobile visualization hcl well being and security and public safety by carefully analyzing the application and data characteristics researchers and practitioners can then adopt or develop the appropriate analytical techniques to derive the intended impact in addi tion to technical system implementation significant business or domain knowledge as well as effective communication skills are needed for the successful completion of such bi a projects is departments thus face unique opportunities and challenges in developing integrated bi a research and education programs for the new generation of data analytics sawy and business relevant students and professionals chen e commerce and market intelligence the excitement surrounding bi a and big data has arguably been generated primarily from the web and e commerce communities significant market transformation has been accomplished by leading e commerce vendors such amazon and ebay through their innovative and highly scalable e commerce platforms and product recommender systems major internet firms such as google amazon and facebook continue to lead the development of web analytics cloud computing and social media platforms the emergence of customer generated web content on various forums newsgroups social media platforms and crowd sourcing systems offers another opportunity for researchers and prac titioners to listen to the voice of the market from a vast number of business constituents that includes customers em ployees investors and the media doan et al o rielly unlike traditional transaction records collected from various legacy systems of the the data that e commerce systems collect from the web are less structured and often contain rich customer opinion and behavioral information for social media analytics of customer opinions text analysis and sentiment analysis techniques are frequently adopted pang and lee various analytical techniques have also been developed for product recommender systems such as association rule mining database segmentation and clustering anomaly detection and graph mining adomavicius and tuzhilin long tail marketing accomplished by reaching the millions of niche markets at the shallow end of the product bitstream has become possible via highly targeted searches and personalized recommendations anderson the netfix prize for the best collaborative filtering algorithm to predict user movie ratings helped gener ate significant academic and industry interest in recommender systems development and resulted in awarding the grand prize of million to the bellkor pragmatic chaos team which prize http www netflixprize eom community viewtopic php id accessed july mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al introduction business intelligence research surpassed netflix own algorithm for predicting ratings by astrophysics and oceanography to genomics and environ percent however the publicity associated with the mental research to facilitate information sharing and data competition also raised major unintended customer privacy analytics the national science foundation nsf recently concerns mandated that every project is required to provide a data management plan cyber infrastructure in particular has much bi a related e commerce research and become development critical for supporting such data sharing initiatives information is appearing in academic is and cs papers as well as in popular it magazines the nsf program solicitation is an obvious example of the u s government funding agency concerted efforts to promote big data analytics the program e government and politics aims to advance the core scientific and technological the advent of web has generated much excitement means for of managing analyzing visualizing and ex reinventing governments the u s house senate tracting and useful information from large diverse dis presidential elections provided the first signs of success tributed for and heterogeneous data sets so as to accel online campaigning and political participation dubbed erate the progress of scientific discovery and innova politics politicians use the highly participatory and tion lead to new fields of inquiry that would not multimedia web platforms for successful policy discussions otherwise be possible encourage the development of campaign advertising voter mobilization event announce new data analytic tools and algorithms facilitate ments and online donations as government and political scalable accessible and sustainable data infrastruc processes become more transparent participatory online ture and increase understanding of human and social multimedia rich there is a great opportunity for adopting processes and interactions and promote economic bi a research in e government and politics applications growth and improved health and quality of life selected opinion mining social network analysis and social media analytics techniques can be used to support several online s t disciplines have already begun their journey political participation e democracy political blogs toward and big data analytics for example in biology the nsf forums analysis e government service delivery and funded process iplant is using cyberinfrastructure to transparency and accountability chen chen support et al a community of researchers educators and students for e government applications semantic information working in plant sciences iplant is intended to foster a new directory and ontological development as exemplified generation below of biologists equipped to harness rapidly ex can also be developed to better serve their target citizens panding computational techniques and growing data sets to address the grand challenges of plant biology the iplant data despite the significant transformational potential for bi a set is in diverse and includes canonical or reference data e government research there has been less academic experimental research data simulation and model data observational than for example e commerce related bi a research data e and other derived data it also offers various open government research often involves researchers from political source data processing and analytics tools science and public policy for example karpf ana lyzed the growth of the political blogosphere in the in united astronomy the sloan digital sky survey sdss shows states and found significant innovation of existing political how computational methods and big data can support and institutions in adopting blogging platforms into their facilitate web sense making and decision making at both the offerings in his research blogspace mapping with macroscopic com and the microscopic level in a rapidly growing posite rankings helped reveal the partisan makeup of and the globalized research field the sdss is one of the most american political blogsphere yang and callan ambitious and influential surveys in the history of astronomy demonstrated the value for ontology development for govern ment services through their development of the ontocop system which works interactively with a user to organize core and techniques and technologies for advancing big data science summarize online public comments from citizens engineering bigdata program solicitation nsf http www nsf gov pubs nsfl htm accessed august science and technology collaborative http www iplantcollaborative org about accessed august many areas of science and technology s t are reaping sloan the digital sky survey mapping the universe http www sdss org benefits of high throughput sensors and instruments accessed from august mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al introduction business intelligence research over its eight years of operation it has obtained deep multi color images covering more than a quarter of the sky and created three dimensional maps containing more than galaxies and over quasars continuing to gather data at a rate of gigabytes per night sdss has amassed more than terabytes of data the international large hadron collider lhc effort for high energy physics is another example of big data producing about petabytes of data in a year brumfiel smart health and wellbeing much like the big data opportunities facing the e commerce and s t communities the health community is facing a tsunami of health and healthcare related content generated from numerous patient care points of contact sophisticated medical instruments and web based health communities two main sources of health big data are genomics driven big data genotyping gene expression sequencing data and payer provider big data electronic health records insurance records pharmacy prescription patient feedback and responses miller the expected raw sequencing data from each person is approximately four terabytes from the payer provider side a data matrix might have hundreds of thousands of patients with many records and parameters demographics medications outcomes collected over a long period of time extracting knowledge from health big data poses significant research and practical challenges especially considering the hipaa health insurance portability and accountability act and irb institutional review board requirements for building a privacy preserving and trust worthy health infrastructure and conducting ethical health related research gelfand health big data ana lytics in general lags behind e commerce bi a applications because it has rarely taken advantage of scalable analytical methods or computational platforms miller over the past decade electronic health records ehr have been widely adopted in hospitals and clinics worldwide significant clinical knowledge and a deeper understanding of patient disease patterns can be gleanded from such collections hanauer et al hanauer et al lin et al hanauer et al for example used large scale longi tudinal ehr to research associations in medical diagnoses and consider temporal relations between events to better elucidate patterns of disease progression lin et al used symptom disease treatment sdt association rule mining on a comprehensive ehr of approximately million records from a major hospital based on selected international classification of diseases icd codes they were able to identify clinically relevant and accurate sdt associations from patient records in seven distinct diseases ranging from cancers to chronic and infectious diseases in addition to ehr health social media sites such as daily strength and patientslikeme provide unique research oppor tunities in healthcare decision support and patient empower ment miller especially for chronic diseases such as diabetes parkinson alzheimer and cancer association rule mining and clustering health social media monitoring and analysis health text analytics health ontologies patient network analysis and adverse drug side effect analysis are promising areas of research in health related bi a due to the importance of hipaa regulations privacy preserving health data mining is also gaining attention gelfand partially funded by the national institutes of health nih the nsf bigdata program solicitation includes common interests in big data across nsf and nih clinical decision making patient centered therapy and knowledge bases for health disease genome and environment are some of the areas in which bi a techniques can contribute chen lb wactlar et al another recent major nsf initiative related to health big data analytics is the nsf smart health and wellbeing shb program which seeks to address fundamental technical and scientific issues that would support a much needed transformation of healthcare from reactive and hospital centered to preventive proactive evidence based person centered and focused on wellbeing rather than disease control the shb research topics include sensor technology networking information and machine learning technology modeling cognitive processes system and process modeling and social and economic issues wactlar et al most of which are relevant to healthcare bi a security and public safety since the tragic events of september security research has gained much attention especially given the increasing dependency of business and our global society on digital enablement researchers in computational science information systems social sciences engineering medicine and many other fields have been called upon to help enhance our ability to fight violence terrorism cyber crimes and other cyber security concerns critical mission areas have been identified where information technology can contribute as suggested in the u s office of homeland security report national strategy for homeland security released in including intelligence and warning border and transportation smart health and wellbeing sbh program solicitation nsf http www nsf gov pubs nsfl nsfl htm accessed august mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al lntroduction business intelligence research security domestic counter terrorism protecting critical infra structure including cyberspace defending against catastro phic terrorism and emergency preparedness and response facing the critical missions of international security and various data and technical challenges the need to develop the science of security informatics was recognized with its main objective being the development of advanced information technologies systems algorithms and databases for security related applications through an integrated techno logical organizational and policy based approach chen p bi a has much to contribute to the emerging field of security informatics security issues are a major concern for most organizations according to the research firm international data corpora tion large companies are expected to spend billion in computer security in and small and medium size companies will spend more on security than on other it purchases over the next three years perlroth and rusli in academia several security related disciplines such as computer security computational criminology and terrorism informatics are also flourishing brantingham chen et al intelligence security and public safety agencies are gathering large amounts of data from multiple sources from criminal records of terrorism incidents and from cyber security threats to multilingual open source intelligence companies of dif ferent sizes are facing the daunting task of defending against cybersecurity threats and protecting their intellectual assets and infrastructure processing and analyzing security related data however is increasingly difficult a significant chal lenge in security it research is the information stovepipe and overload resulting from diverse data sources multiple data formats and large data volumes current research on tech nologies for cybersecurity counter terrorism and crime fighting applications lacks a consistent framework for addressing these data challenges selected bi a technol ogies such as criminal association rule mining and clustering criminal network analysis spatial temporal analysis and visualization multilingual text analytics sentiment and affect analysis and cyber attacks analysis and attribution should be considered for security informatics research the university of arizona coplink and dark web research programs offer significant examples of crime data mining and terrorism informatics within the is community chen the coplink information sharing and crime data mining system initially developed with funding from nsf and the department of justice is currently in use by more than police agencies in the united states and by nato countries and was acquired by ibm in the dark web research funded by nsf and the department of defense dod has generated one of the largest known academic terrorism research databases about terabytes of terrorist web sites and social media content and generated advanced multilingual social media analytics techniques recognizing the challenges presented by the volume and complexity of defense related big data the u s defense advanced research project agency darpa within dod initiated the xdata program in to help develop com putational techniques and software tools for processing and analyzing the vast amount of mission oriented information for defense activities xdata aims to address the need for scalable algorithms for processing and visualization of imperfect and incomplete data the program engages applied mathematics computer science and data visualization com munities to develop big data analytics and usability solutions for warfighters bi a researchers could contribute signifi cantly in this area table summarizes these promising bi a applications data characteristics analytics techniques and potential impacts bi a research framework foundational technologies and emerging research in analytics the opportunities with the abovementioned emerging and high impact applications have generated a great deal of excitement within both the bi a industry and the research community whereas industry focuses on scalable and inte grated systems and implementations for applications in dif ferent organizations the academic community needs to continue to advance the key technologies in analytics emerging analytics research opportunities can be classified into five critical technical areas big data analytics text analytics web analytics network analytics and mobile analytics all of which can contribute to to bi a and the classification of these five topic areas is intended darpa calls for advances in big data to help the warfighter march http www darpa mil newsevents releases aspx accessed august mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al lntroductiori business intelligence research e commerce and e government and science smart health and security and market intelligence politics technology wellbeing public safety applications recommender ubiquitous s t innovation human and plant crime analysis systems government services hypothesis testing genomics computational social media equal access and knowledge healthcare criminology monitoring and public services discovery decision support terrorism analysis citizen engagement patient community informatics crowd sourcing and participation analysis open source systems political campaign intelligence social and virtual and e polling cyber security games data search and user government informa s t instruments genomics and criminal records logs tion and services and system sequence data crime maps customer transac rules and regula generated data electronic health criminal networks tion records tions sensor and records ehr news and web customer citizen feedback and network content health and patient contents generated content comments social media terrorism incident databases viruses cyber attacks and botnets characteristics characteristics characteristics characteristics characteristics structured web fragmented informa high throughput disparate but highly personal identity based user tion sources and instrument based linked content information incom generated content legacy systems rich data collection fine person specific plete and deceptive rich network informa textual content grained multiple content hipaa irb content rich group tion unstructured unstructured informal modality and large and ethics issues and network infor informal customer citizen conversations scale records s t mation multilingual opinions specific data formats content analytics association rule information integra s t based genomics and criminal mining tion domain specific sequence analysis association rule database segmen content and text mathematical and and visualization mining and tation and analytics analytical models ehr association clustering clustering government informa mining and criminal network anomaly detection tion semantic ser clustering analysis graph mining vices and ontologies health social spatial temporal social network social media moni media monitoring analysis and analysis toring and analysis and analysis visualization text and web social network health text multilingual text analytics analysis analytics analytics sentiment and sentiment and affect health ontologies sentiment and affect analysis analysis patient network affect analysis analysis cyber attacks adverse drug analysis and side effect attribution analysis privacy preserving data mining impacts long tail marketing transforming govern s t advances improved healthcare improved public targeted and person ments empowering scientific impact quality improved safety and security alized recommenda citizens improving long term care tion increased sale transparency partici patient empower and customer pation and equality ment satisfaction mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al lntroduction business intelligence research big data analytics text analytics web analytics network analytics mobile analytics foundational rdbms information information bibliometric web services technologies data warehousing retrieval retrieval analysis smartphone etl document computational citation network platforms olap representation linguistics coauthorship bpm query processing search engines network data mining relevance feedback web crawling social network clustering user models web site ranking theories regression search engines search log analysis network metrics classification enterprise search recommender and topology association systems systems mathematical analysis web services network models anomaly detection mashups network neural networks visualization genetic algorithms multivariate statistical analysis optimization heuristic search emerging statistical machine statistical nlp cloud services link mining mobile web research learning information cloud computing community services sequential and extraction social search and detection mobile pervasive temporal mining topic models mining dynamic network apps spatial mining question answering reputation systems modeling mobile sensing mining high speed systems social media agent based apps data streams and opinion mining analytics modeling mobile social sensor data sentiment affect web visualization social influence innovation process mining analysis web based and information mobile social privacy preserving web stylometric auctions diffusion models networking data mining analysis internet ergms mobile visualiza network mining multilingual monetization virtual communities tion hci web mining analysis social marketing criminal dark personalization and column based text visualization web privacy networks behavioral dbms multimedia ir security social political modeling in memory dbms mobile ir analysis gamification parallel dbms hadoop trust and reputation mobile advertising cloud computing mapreduce and marketing hadoop mapreduce to highlight the key characteristics of each area however a few of these areas may leverage similar underlying tech nologies in each analytics area we present the foundational technologies that are mature and well developed and suggest selected emerging research areas see table big data analytics data analytics refers to the bi a technologies that are grounded mostly in data mining and statistical analysis as mentioned previously most of these techniques rely on the mature commercial technologies of relational dbms data warehousing etl olap and bpm chaudhuri et al since the late various data mining algorithms have been developed by researchers from the artificial intelligence algorithm and database communities in the ieee international conference on data mining icdm the most influential data mining algorithms were identified based on expert nominations citation counts and a community survey in ranked order they are k means svm support vector machine apriori em expectation maximi zation pagerank adaboost knn k nearest neighbors naïve bayes and cart wu et al these algorithms cover classification clustering regression association analy sis and network analysis most of these popular data mining algorithms have been incorporated in commercial and open source data mining systems witten et al other mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al introduction business intelligence research advances such as neural networks for classification prediction and clustering and genetic algorithms for optimization and machine learning have all contributed to the success of data mining in different applications two other data analytics approaches commonly taught in business school are also critical for bi a grounded in statistical theories and models multivariate statistical analysis covers analytical techniques such as regression factor analy sis clustering and discriminant analysis that have been used successfully in various business applications developed in the management science community optimization techniques and heuristic search are also suitable for selected bi a prob lems such as database feature selection and web crawling spidering most of these techniques can be found in business school curricula due to the success achieved collectively by the data mining and statistical analysis community data analytics continues to be an active area of research statistical machine learning often based on well grounded mathematical models and powerful algorithms techniques such as bayesian networks hidden markov models support vector machine reinforce ment learning and ensemble models have been applied to data text and web analytics applications other new data analytics techniques explore and leverage unique data charac teristics from sequential temporal mining and spatial mining to data mining for high speed data streams and sensor data increased privacy concerns in various e commerce e government and healthcare applications have caused privacy preserving data mining to become an emerging area of research many of these methods are data driven relying on various anonymization techniques while others are process driven defining how data can be accessed and used gelfand over the past decade process mining has also emerged as a new research field that focuses on the analysis of processes using event data process mining has become possible due to the availability of event logs in various industries e g healthcare supply chains and new process discovery and conformance checking techniques van der aalst furthermore network data and web content have helped generate exciting research in network analytics and web analytics which are presented below in addition to active academic research on data analytics industry research and development has also generated much excitement especially with respect to big data analytics for semi structured content unlike the structured data that can be handled repeatedly through a rdbms semi structured data may call for ad hoc and one time extraction parsing processing indexing and analytics in a scalable and dis tributed mapreduce or hadoop environment mapreduce has been hailed as a revolutionary new platform for large scale massively parallel data access patterson inspired in part by mapreduce hadoop provides a java based software framework for distributed processing of data intensive transformation and analytics the top three com mercial database suppliers oracle ibm and microsoft have all adopted hadoop some within a cloud infrastructure the open source apache hadoop has also gained significant traction for business analytics including chukwa for data collection hbase for distributed data storage hive for data summarization and ad hoc querying and mahout for data mining henschen in their perspective paper stone braker et al compared mapreduce with the parallel dbms the commercial parallel dbms showed clear advan tages in efficient query processing and high level query language and interface whereas mapreduce excelled in etl and analytics for read only semi structured data sets new hadoop and mapreduce based systems have become another viable option for big data analytics in addition to the commercial systems developed for rdbms column based dbms in memory dbms and parallel dbms chaudhuri et al text analytics a significant portion of the unstructured content collected by an organization is in textual format from e mail commu nication and corporate documents to web pages and social media content text analytics has its academic roots in information retrieval and computational linguistics in infor mation retrieval document representation and query pro cessing are the foundations for developing the vector space model boolean retrieval model and probabilistic retrieval model which in turn became the basis for the modern digital libraries search engines and enterprise search systems saltón in computational linguistics statistical natural language processing nlp techniques for lexical acquisition word sense disambiguation part of speech tagging post and probabilistic context free grammars have also become important for representing text manning and schütze in addition to document and query representations user models and relevance feedback are also important in enhancing search performance since the early search engines have evolved into mature commercial systems consisting of fast distributed crawling efficient inverted indexing inlink based page ranking and search logs analytics many of these founda tional text processing and indexing techniques have been deployed in text based enterprise search and document management systems in bi a mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al l introduction business intelligence research leveraging the power of big data for training and statistical nlp for building language models text analytics techniques have been actively pursued in several emerging areas including information extraction topic models question answering q a and opinion mining information extraction is an area of research that aims to automatically extract specific kinds of structured information from documents as a building block of information extraction ner named entity recognition also known as entity extraction is a process that identifies atomic elements in text and classifies them into predefined categories e g names places dates ner techniques have been successfully developed for news analysis and biomedical applications topic models are algo rithms for discovering the main themes that pervade a large and otherwise unstructured collection of documents new topic modeling algorithms such as lda latent dirichlet allocation and other probabilistic models have attracted recent research blei question answering q a sys tems rely on techniques from nlp information retrieval and human computer interaction primarily designed to answer factual questions i e who what when and where kinds of questions q a systems involve different techniques for question analysis source retrieval answer extraction and answer presentation maybury the recent successes of ibm watson and apple siri have highlighted q a research and commercialization opportunities many pro mising q a system application areas have been identified including education health and defense opinion mining refers to the computational techniques for extracting classi fying understanding and assessing the opinions expressed in various online news sources social media comments and other user generated contents sentiment analysis is often used in opinion mining to identify sentiment affect subjec tivity and other emotional states in online text web and social media content have created abundant and exciting opportunities for understanding the opinions of the general public and consumers regarding social events political move ments company strategies marketing campaigns and product preferences pang and lee in addition to the above research directions text analytics also offers significant research opportunities and challenges in several more focused areas including web stylometric analysis for authorship attribution multilingual analysis for web documents and large scale text visualization multi media information retrieval and mobile information retrieval are two other related areas that require support of text analytics techniques in addition to the core multimedia and mobile technologies similar to big data analytics text analytics using mapreduce hadoop and cloud services will continue to foster active research directions in both academia and industry web analytics over the past decade web analytics has emerged as an active field of research within bi a building on the data mining and statistical analysis foundations of data analytics and on the information retrieval and nlp models in text analytics web analytics offers unique analytical challenges and opportunities http html based hyperlinked web sites and associated web search engines and directory systems for locating web content have helped develop unique internet based technologies for web site crawling spidering web page updating web site ranking and search log analysis web log analysis based on customer transactions has subsequently turned into active research in recommender systems how ever web analytics has become even more exciting with the maturity and popularity of web services and web systems in the mid o reilly based on xml and internet protocols http smtp web services offer a new way of reusing and integrating third party or legacy systems new types of web services and their associated apis application programming interface allow developers to easily integrate diverse content from different web enabled system for example rest representational state transfer for invoking remote services rs s really simple syndication for news pushing json javascript object notation for lightweight data interchange and ajax asynchronous javascript xml for data interchange and dynamic display such lightweight programming models support data syndication and notification and mashups of multimedia content e g flickr youtube google maps from different web sources a process somewhat similar to etl extraction transformation and load in bi a most of the e commerce vendors have provided mature apis for accessing their product and customer content schonfeld for example through amazon web services devel opers can access product catalog customer reviews site ranking historical pricing and the amazon elastic compute cloud for computing capacity similarly google web apis support ajax search map api gdata api for calendar gmail etc google translate and google app engine for cloud computing resources web services and apis continue to provide an exciting stream of new data sources for bi a research a major emerging component in web analytics research is the development of cloud computing platforms and services which include applications system software and hardware delivered as services over the internet based on service oriented architecture soa server virtualization and utility computing cloud computing can be offered as software as a mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al introduction business intelligence research service saas infrastructure as a service iaas or platform as a service paas only a few leading it vendors are cur rently positioned to support high end high throughput bi a applications using cloud computing for example amazon elastic compute cloud enables users to rent virtual computers on which to run their own computer applications its simple storage service provides online storage web service google app engine provides a platform for devel oping and hosting java or python based web applications google bigtable is used for backend data storage microsoft windows azure platform provides cloud services such as sql azure and sharepoint and allows net framework applications to run on the platform the industry led web and cloud services offer unique data collection processing and analytics challenges for bi a researchers in academia current web analytics related research encom passes social search and mining reputation systems social media analytics and web visualization in addition web based auctions internet monetization social marketing and web privacy security are some of the promising research directions related to web analytics many of these emerging research areas may rely on advances in social network analy sis text analytics and even economics modeling research network analytics network analytics is a nascent research area that has evolved from the earlier citation based bibliometric analysis to include new computational models for online community and social network analysis grounded in bibliometric analysis citation networks and coauthorship networks have long been adopted to examine scientific impact and knowledge diffusion the h index is a good example of a citation metric that aims to measure the productivity and impact of the published work of a scientist or scholar hirsch since the early network science has begun to advance rapidly with contri butions from sociologists mathematicians and computer scientists various social network theories network metrics topology and mathematical models have been developed that help understand network properties and relationships e g centrality betweenness cliques paths ties structural holes structural balance random network small world network scale free network barabási watts recent network analytics research has focused on areas such as link mining and community detection in link mining one seeks to discover or predict links between nodes of a network within a network nodes may represent customers end users products and or services and the links between nodes may represent social relationships collaboration e mail exchanges or product adoptions one can conduct link mining using only the topology information liben nowell and kleinberg techniques such as common neighbors jaccard coefficient adamic adar measure and katz measure are popular for predicting missing or future links the link mining accuracy can be further improved when the node and link attributes are considered community detection is also an active research area of relevance to bi a fortunato by representing networks as graphs one can apply graph partitioning algorithms to find a minimal cut to obtain dense subgraphs representing user communities many techniques have been developed to help study the dynamic nature of social networks for example agent based models sometimes referred to as multi agent systems have been used to study disease contact networks and criminal or terrorist networks national research council such models simulate the actions and interactions of autonomous agents of either individual or collective entities such as organizations or groups with the intent of assessing their effects on the system as a whole social influence and infor mation diffusion models are also viable techniques for studying evolving networks some research is particularly relevant to opinion and information dynamics of a society such dynamics hold many qualitative similarities to disease infections bettencourt et al another network analytics technique that has drawn attention in recent years is the use of exponential random graph models frank and strauss robins et al ergms are a family of statistical models for analyzing data about social and other networks to support statistical inference on the processes influencing the formation of network structure ergms consider the set of all possible alternative networks weighted on their similarity to an observed network in addition to studying traditional friendship or disease networks ergms are promising for understanding the underlying network propertities that cause the formation and evolution of customer citizen or patient networks for bi a most of the abovementioned network analytics techniques are not part of the existing commercial bi a platforms signifi cant open source development efforts are underway from the social network analysis community tools such as ucinet borgatti et al and pajek batagelj and mrvar have been developed and are widely used for large scale network analysis and visualization new network analytics tools such as ergm have also been made available to the academic community hunter et al online virtual communities criminal and terrorist networks social and political networks and trust and reputation networks are some of the promising new applications for network analytics mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et aljintroduction business intelligence research mobile analytics as an effective channel for reaching many users and as a means of increasing the productivity and efficiency of an organization workforce mobile computing is viewed by respondents of the recent ibm technology trends survey ibm as the second most in demand area for software development mobile bi was also considered by the gartner bi hype cycle analysis as one of the new technologies that have the potential to drastically disrupt the bi market bitterer according to emarketer the market for mobile ads is expected to explode soaring from an estimated billion in to billion in snider mobile computing offers a means for it professional growth as more and more organizations build applications with its large and growing global install base android has been ranked as the top mobile platform since this open source platform based on java and xml offers a much shorter learning curve and this contributes to its popularity with it professionals percent of the ibm survey respondents planned to use android as their mobile develop ment platform while percent planned to use ios and percent planned to use windows the ability to collect fine grained location specific context aware highly personalized content through these smart devices has opened new possi bilities for advanced and innovative bi a opportunities in addition to the hardware and content advantages the unique apps ecosystem developed through the volunteer community of mobile app developers offers a new avenue for bi a research the apple app store alone offers more than apps in almost any conceivable category as of august the number of android apps also reached in august many different revenue models have begun to emerge for mobile apps from paid or free but ad supported apps to mobile gamification which incentivizes participants e g users or employees by giving rewards for contributions snider for mobile bi companies are considering enterprise apps industry specific apps e commerce apps and social apps in ranked order according to the ibm survey the lightweight programming models of the current web services e g html xml css ajax flash and the maturing mobile development platforms such as android and ios have contributed to the rapid development of mobile web services e g mobile ajax mobile flash in various mobile pervasive applications from disaster manage ment to healthcare support new mobile analytics research is emerging in different areas e g mobile sensing apps that are location aware and activity sensitive mobile social innova tion for m health and m learning mobile social networking and crowd sourcing mobile visualization hci and personali zation and behavioral modeling for mobile apps in addition social behavioral and economic models for gamification mobile advertising and social marketing are under way and may contribute to the development of future bi a systems mapping the bi a knowledge landscape a bibliometric study of academic and industry publications in an effort to better understand the current state of bi a related research and identify future sources of knowledge we conducted a bibliometric study analyzing relevant literature major bi a scholars disciplines and publications and key research topics a collection transformation and analytics process was followed in the study much like a typical bi a process adopted in other applications to discern research trends in bi a related literature from the past decade was collected relevant it publications were identified from several large scale and reputable digital libraries web of science thomson reuters covering more than of the highest impact journals in sciences engineering and humanities business source complete ebsco covering peer reviewed business journals as well as non journal content such as industry trade magazines ieee xplore institute of electrical and elec tronics engineers providing access to the ieee digital library sciencedirect elsevier covering over journals from the scientific technical and medical literature and engineering village elsevier used to retrieve selected acm conference papers because the acm digital library interface does not support automated downloading these sources contain high quality bibliometric metadata including journal name and date author name and institution and article title and abstract to ensure data consistency and relevance across our collec tion we retrieved only those publications that contained the iphone learn about apps from the app store http www apple com iphone built in apps app store html accessed august keywords business intelligence business analytics or big data within their title abstract or subject indexing when applicable the choice of these three keywords was intended android statistics http www appbrain com stats number of to focus our search and analysis on publications of direct rele android apps accessed august vance to our interest however this search procedure may mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al introduction business intelligence research also omit articles that use other bi a relevant terms e g overall the largest source of academic business intelligence data warehousing data mining but not the three specific publications was academic conferences the conference on keywords in the title or abstract this kind of limitation is business intelligence and financial engineering and common in bibliometric studies the collected data was conference on electronic commerce and business intelli exported as xml records and parsed into a relational data gence are specialized academic conferences devoted to base sql server for analysis the number of records business intelligence one is conference ranks in the top initially retrieved totaled papers after removing dupli list hawaii international conference on systems sciences cates the number of unique records totaled hicss with publications ieee holds the majority of conferences on the list through various outlets several are figure shows the statistics and growth trends of publications related to emerging technical areas such as data mining relating to the three search keywords overall business intel internet computing and cloud computing the ieee inter ligence had the largest coverage and the longest history this national conference on data mining icdm is highly is consistent with the evolution of bi a as the term regarded bi and ranks acm has two publications in the top appeared first in the early in our collection business list communications of the acm and the acm sigkdd analytics and big data began to appear in the literature in international conference on knowledge discovery and data but only gained much attention after about the mining both are well known in cs again the data mining business intelligence related publications numbered community has contributed significantly to bi a other whereas business analytics and big data publications each technical conferences in cs are also contributing to bi a in numbered only and respectively while the overall areas such as computational intelligence web intelligence publication trend for business intelligence remains stable evolutionary computation and natural language processing business analytics and big data publications have seen a faster all of which are critical for developing future data text and growth pattern in recent years web analytics techniques discussed in our research frame knowledge of the most popular publications as well as pro lific authors is beneficial for understanding an emerging major is conferences icis international conference on information conferences research discipline and industry table magazines summarizes with the bi a top publications journals may tions systems have are not and also covered wits published workshop in significant the five on major information bi a digital research libraries technologies however to which and their systems we collec have the top academic bi a authors are identified in table access and thus are not included in this analysis mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al introduction business intelligence research top top academic publication publications industry publication publications conf on business intelligence and financial engineering computerworld hawaii international conf on systems sciences information today conf on electronic commerce and business intelligence informationweek international technology conf on computer web workshops intelligence and intelligent weekly agent ieee international conf on data mining microsoft data mining ieee international conf on e technology e commerce and g infoworld e service ieee intelligent systems cio ieee cloud computing km world decision support varbusiness systems crn formerly ieee congress on evolutionary computation stores magazine journal of business ethics forbes communications of the acm crm magazine european journal of marketing network world ieee acm international financial symposium on cluster cloud and executive grid computing international journal of technology management healthcare management financial a a acm and sigkdd data international chain mining conf on store knowledge discovery age international symposium on natural language processing strategic finance ieee internet computing traffic world international engineering conf data on computational strategy intelligence and software ieee software cfp work journals table are summa somew cation volume publications although mo decision support several systems others m f business school ment journals commu als in areas such authors as business are clos et management other jay majo f n business intelligence wingyan related chun the aforementioned connection source from industry watson tend to be th specific bi focus we also e g report com today at popular and inform metric table however authors there based ar such as microsoft publications data min a crm magazine propensity that to c related topics of data mini customer relation headers managem are welco tionally been and topics results of or inte for mis quarterly vol this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al introduction business intelligence research academic publication publications decision support systems communications of the ais journal of management information systems management science information systems research journal of the association for information systems informs journal on computing management information systems quarterly rank name affiliation discipline region total pagerank hsinchun chen university of arizona u s is north america shenghong li zhejiang university china math asia yong shi university of nebraska u s cs north america kin keung lai city university of hong kong china is asia barbara h wixom university of virginia u s is north america hugh j watson university of georgia u s is north america elizabeth chang curtin university australia is australia sheila wright de montfort university u k marketing europe matteo golfarelli university of bologna italy cs europe farookh hussain university of technology sydney australia cs australia michaelchau hong kong university china is asia josef schiefer vienna university of technology austria cs europe craig s fleisher college of costal georgia u s management north america lingling zhang towson university u s communication north america olivera marjanovic university of sydney australia is australia xiaofeng zhang changsha university of science and is asia technology china stefano rizzi university of bologna italy cs europe jay f nunamaker university of arizona u s is north america wingyan chung santa clara university u s is north america zahir urabu brunei university u k management europe analysis reveals broad and even contribution related trigrams of authors such from as customer relation man north america asia europe and australia enterprise reflecting resource the planning these keyword diversity and international interest in ranked the field based of on bi a their frequency and the top displayed using the tagcloud visualization mor the last set of analyses investigated keywords the content are of highlighted bi a with larger fonts publications from mallet figure mccallum for example a competitive advantage b java based open source nlp text analytics warehousing tool was and used decision to support emerged as extract the top bigrams two word phrases topics in for the each bi a year literature a other bi a relate few bi grams were combined to form as more customer meaningful relation bi management data mining mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et ai introduction business intelligence research intelligence enterprise resource planning and knowledge management were also highly ranked overall the topics extracted were highly relevant to bi a especially for its managerial and application values although most of the detailed technical terms as described in the previous research framework sections were not present this could be attri buted to the tendency of authors to use broad terminologies in article titles and abstracts bi a education and program development bi a provides opportunities not only for the research com munity but also for education and program development in july columbia university and new york city announced plans to invest over million dollars in a new center for data science which is expected to generate thousands of jobs and millions of dollars in tax revenues from startup companies over the next years associated press bi a is data science in business job postings seeking data scientists and business analytics specialists abound these days there is a clear shortage of professionals with the deep knowledge required to manage the three v of big data volume velocity and variety russom there is also an increasing demand for individuals with the deep knowledge needed to manage the three perspectives of business decision making descriptive predictive and pre scriptive analytics in this section we describe bi a educa tion in business schools present the challenges facing is departments and discuss bi a program development oppor tunities we also provide some suggestions for the is disci pline in addressing these challenges chiang et al education challenges bi a focuses on understanding interpretation strategizing and taking action to further organizational interests several academic disciplines have contributed to bi a including is cs statistics management and marketing as shown in our bibliometric study is programs in particular are uniquely positioned to train a new generation of scholars and students due to their emphasis on key data management and infor mation technologies business oriented statistical analysis and management science techniques and broad business disci pline exposure e g finance accounting marketing and economics since its inception approximately years ago is as an academic discipline has primarily focused on business needs in an era when the major challenges involved the management of internal business and transaction data in the age of big data these problems remain but the emphasis in industry has shifted to data analysis and rapid business decision making based on huge volumes of information such time critical decision making largely takes place outside of the is function i e in business units such as marketing finance and logistics can is programs serve the needs of these business decision makers can we provide courses in data mining text mining opinion mining social media network analytics web mining and predictive analytics that are required for marketing and finance majors we should also ask ourselves about the skill sets needed by students should we recruit students with strong math and statistical skills for example we contend that a new vision for is or at least for some is programs should address these questions bi a presents a unique opportunity for is units in business schools to position themselves as a viable option for edu mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al introduction business intelligence research eating professionals with the necessary depth and academic rigor to tackle the increased complexity of bi a issues is programs housed within business schools have access to a variety of business courses as well as courses intended to improve communication and presentation skills it is also common for business schools to house management science and statistics faculty in the same is unit bi a knowledge and skills bi a education should be interdisciplinary and cover critical analytical and it skills business and domain knowledge and communication skills required in a complex data centric business environment analytical and it skills include a variety of evolving topics they are drawn from disciplines such as statistics and computer science for managing and analyzing both structured data and unstructured text coverage of these topics ranges from bi a to bi a the academic programs intended to produce bi a professionals should consider these analytical and it skills as suggested in table of our research framework to provide useful insights and decision making support the bi a professionals must be capable of understanding the business issues and framing the appropriate analytical solu tions the necessary business knowledge for bi a profes sionals ranges from general familiarity with the areas of accounting finance management marketing logistics and operation management to the domain knowledge required in specific bi a applications some of which are discussed earlier and summarized in table the importance of an organization wide culture for informed fact based decision making for business analytics is empha sized by davenport to support such a culture bi a professionals need to know not only how to turn raw data and information through analytics into meaningful and action able knowledge for an organization but also how to properly interact with and communicate this knowledge to the business and domain experts of the organization program development bi a provides a unique opportunity for is units in business schools to develop new courses certificate programs and degree programs charged with preparing the next generation of analytical thinkers there are many options for delivering bi a education because of the depth of knowledge required graduate programs are the obvious choice viable program development options in delivering bi a education include creating a master of science ms degree in bi a creating a bi a concentration in existing ms is programs offering a graduate bi a certificate program the first option requires the effort of developing a new program a few universities have embarked on this endeavor a nonexhaustive list includes north carolina state univer sity saint joseph university northwestern university the university of denver stevens institute of technology and fordham university new york university will launch its new program in may new ms degree programs can be designed explicitly to attract analytically strong students with undergraduate degrees in areas such as mathematics science and computer science and to prepare these students for careers not only in the is or it groups in industry but also in functional areas such as research and development marketing media logistics and finance the second option leverages existing ms is programs with a bi a concentration that would supplement the already existing curriculum in it data management and business and communication courses with additional analytics coverage this option has been adopted by a number of schools including the is departments at carnegie mellon university and the university of arizona this option provides bi a knowledge and skills for students who will primarily find careers in is groups in industry for working it professionals who wish to expand into bi a a part time ms or certificate program the third option offer practical and valid alternatives these certificate programs can be delivered online or on site and need to provide the skills that will complement the current it or business experi ence of it professionals and or provide technical and analy tical skills to business professionals in non it areas online programs that are currently available include northwestern university ms in predictive analytics and stanford univer sity graduate certificate on mining big data in addition is programs can help design a bi a concentration in mba programs to help train a new generation of data and analytics sawy managers a key to success for a bi a program is to integrate the concept of learning by doing in the bi a curriculum via hands on projects internships and industry guided practicum big data analytics requires trial and error and experimen tation strong relationships and partnerships between aca demic programs and industry partners are critical to foster the experiential learning aspect of the bi a curriculum mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et al introduction business intelligence research papers in this special issue the idea for this special issue began in may when detmar straub the editor in chief of mis quarterly soli cited suggestions for special issues from the editorial board members we submitted the special issue proposal on busi ness intelligence research in august with the call for papers approved and distributed at the annual interna tional conference on information systems icis in decem ber of that year submissions to this special issue needed to relate to mis quarterly mission with strong managerial organizational and societal relevance and impact in addition to the design science approach hevner et al march storey rigorous and relevant bi related research using management science modeling optimization information economics and organizational and behavioral methodologies case studies surveys was also welcomed a total of manuscripts was received by october in the following months six of the manuscripts went through three or four review rounds and were then accepted for this issue the six papers address various aspects of the bi a research framework presented in this introduction paper see table all six papers are based on bi a with three also based on bi a the first three papers by chau and xu park et al and lau et al focus on bi a with applications on e commerce and market intelligence using text web and net work analytics in the next two papers both hu et al and abbasi et al work in the category of bi a with a focus on security but hu et al use network analytics whereas abbasi et al emphasize security analysis and data analytics finally sahoo et al also work in bi a with direct appli cation to e commerce and market intelligence using web and data analytics in business intelligence in blogs understanding consumer interactions and communities michael chau and jennifer xu recognized the potential gold mine of blog content for business intelligence and developed a framework for gathering business intelligence by automatically collecting and analyzing blog content and bloggers interaction net works a system developed using this framework was applied to two case studies which revealed novel patterns in blogger interactions and communities sung hyuk park soon young huh wonseok oh and sang pil han in their paper a social network based inference model for validating customer profile data argue that busi ness intelligence systems are of limited value when they deal with inaccurate and unreliable data the authors proposed a social network driven inference framework to determine the accuracy and reliability of self reported customer profiles the framework utilized the individuals social circles and communication patterns within their circles to construct the specific inference and validation model a combination of methods was used including query processing statistical inference social network analysis and user profiling the authors analyzed over million actual mobile call trans actions and their proposed social network based inference model consistently outperformed the alternative approaches in web environmental scanning and adaptive decision support for business mergers and acquisitions raymond lau stephen liao k f wong and dickson chiù analyzed company mergers and acquisitions m a online environ mental scanning with web provides top executives with opportunities to tap into collective web intelligence to develop better insights about the socio cultural and political economic factors that cross border m as face grounded on porter five forces model this research designed a due diligence scorecard model that leverages collective web intelligence to enhance m a decision making the authors also developed an adaptive business intelligence bi system which they applied to chinese companies cross border m a activities in their paper network based modeling and analysis of systemic risk in banking systems daning hu j leon zhao zhimin hua and michael wong analyzed systemic risk in banking systems by treating banks as a network linked with financial relationships leading to a network approach to risk management narm the authors used narm to analyze systemic risk attributed to each individual bank via simulation based on real world data from the federal deposit insurance corporation narm offered a new means by which con tagious bank failures could be predicted and capital injection priorities at the individual bank level could be determined in the wake of a financial crisis a simulation study showed that under significant market shocks the interbank payment links became more influential than the correlated bank portfolio links in determining an individual bank survival ahmed abbasi conan albrecht anthony vance and james hansen in their paper metafraud a meta learning frame work for detecting financial fraud employed a design science approach to develop metafraud a meta learning framework for enhanced financial fraud detection a series of experiments was conducted on thousands of legitimate and fraudulent firms to demonstrate the effectiveness of the frame work over existing benchmark methods the research results have implications for compliance officers investors audit firms and regulators the paper by nachiketa sahoo param vir singh and tridas mukhopadhyay a hidden markov model for collaborative filtering reports on the analysis of making personalized recommendations when user preferences are changing the mis quarterly vol no december this content downloaded from on mon jul utc all use subject to http about jstor org terms chen et a i introduction business intelligence research authors and titles evolutions applications data analytics research impacts chau and xu business bi a on market intelligence user generated text and network increased sales intelligence in blogs under social media on consumers and content extracted analytics and customer standing consumer inter network communities from blogs community detection satisfaction actions and communities analytics network visualization park et al a social bi a market intelligence self reported user network analytics personalized network based inference on social in predicting cus profiles and mobile anomaly detection recommendation model for validating network tomers profiles call records predictive analytics and increased customer profile data analysis and customer statistical satisfaction analysis lau et al web bi a and market intelligence business information text and web analytics strategic decision environmental scanning and on on environmental extracted from sentiment and affect making in adaptive decision support scorecards scanning internet and analysis mergers and for business mergers and and web proprietary financial relation mining acquisitions acquisitions analytics information hu et al network based bi a on systemic risk u s banking infor network and data monitoring and modeling and analysis of statistical analysis and mation extracted from analytics mitigating of systemic risk in banking analysis management in fdic and federal descriptive and contagious bank systems banking systems reserve wire predictive modeling failures network discrete event simulation abbasi et al metafraud a bi a on fraud detection financial ratios and data analytics financial fraud meta learning framework data mining organizational and classification detection for detecting financial and meta industrial level context generalization fraud learning features adaptive learning sahoo et al a hidden bi a on recommender sys blog reading data data and web analytics personalized markov model for col statistical terns with changing netflix prize data set statistical dynamic model recommendations laborative filtering analysis user preferences and last fm data collaborative filtering authors proposed a hidden markov model based on while collabo bi a is still maturing we find ourselves poised at rative filtering to predict user preferences and make the the brink most of bi a with all the attendant uncertainty that appropriate personalized recommendations for the new predicted and potentially revolutionary technologies bring preference the authors employed real world data sets and simulations to show that when user preferences are changing this mis quarterly special issue on business intelligence there is an advantage to using the proposed algorithm research over is intended to serve in part as a platform and existing ones conversation guide for examining how the is discipline can better serve the needs of business decision makers in light of maturing and emerging bi a technologies ubiquitous big summary and conclusions data and the predicted shortages of data sawy managers and of business professionals with deep analytical skills how can academic is programs continue to meet the needs of their through bi a initiatives businesses and organizations traditional students while also reaching the working it from all sectors began to gain critical insights from professional the in need of new analytical skills a new vision structured data collected through various enterprise for systems is may be needed to address this and other questions and analyzed by commercial relational database management systems over the past several years web intelligence by highlighting web several applications such as e commerce analytics web and the ability to mine unstructured market user intelligence e government healthcare and security generated contents have ushered in a new and exciting and era by of mapping important facets of the current bi a bi a research leading to unprecedented intelligence knowledge on landscape we hope to contribute to future sources consumer opinion customer needs and recognizing of knowledge new and to augment current discussions on the business opportunities now in this era of big data importance even of relevant academic research chen et al introduction business intelligence research finally the six papers chosen for this special issue are them selves a microcosm of the current state of bi a research these best of the best research papers showcase how high quality academic research can address real world problems and contribute solutions that are relevant and long lasting exactly the challenge that our discipline continues to face low cost range sensors are an attractive alternative to expensive laser scanners in application areas such as indoor mapping surveillance robotics and forensics a recent development in consumer grade range sensing technology is microsoft kinect sensor kinect was primarily designed for natural interaction in a computer game environment however the characteristics of the data captured by sensors kinect have attracted the attention of researchers from other fields including mapping and modeling a demonstration of the potential of kinect for modeling of indoor environments can be seen in the work of henry et al the kinect sensor captures depth and color images simultaneously at a frame rate of up to fps the integration of depth and color data results in a colored point cloud that contains about points in every frame by registering the consecutive depth images one can obtain an increased point density but also create a complete point cloud of an indoor environment possibly in real time to realize the full potential of the sensor for mapping applications an analysis of the systematic and random errors of the data is necessary the correction of systematic errors is a prerequisite for the alignment of the depth and color data and relies on the identification of the mathematical model of depth measurement and the calibration parameters involved the characterization of random errors is important and useful in further processing of the depth data for example in weighting the point pairs or planes in the registration algorithm since kinect is a recent development it was released in november little information about the geometric quality of its data is available geometric investigation and calibration of similar range sensors such as the swissranger and pmd has been the topic of several previous works however these sensors are based on the time of flight measurement principle and are fundamentally different from the kinect which is a triangulation sensor in this paper our primary focus is on the depth data the objective of the paper is to provide an insight into the geometric quality of the kinect depth data through calibration and an analysis of the accuracy and density of the points we present a mathematical model for obtaining object coordinates from the raw image measurements and discuss the calibration parameters involved in the model further a theoretical random error model is derived and verified by an experiment the paper proceeds with a description of the depth measurement principle the mathematical model and the calibration parameters in section in section the error sources are discussed and a theoretical error model is presented in section the models are verified through a number of experiments and the results are discussed the paper concludes with some remarks in section depth measurement by triangulation the kinect sensor consists of an infrared laser emitter an infrared camera and an rgb camera the inventors describe the measurement of depth as a triangulation process the laser source emits a single beam which is split into multiple beams by a diffraction grating to create a constant pattern of speckles projected onto the scene this pattern is captured by the infrared camera and is correlated against a reference pattern the reference pattern is obtained by capturing a plane at a known distance from the sensor and is stored in the memory of the sensor when a speckle is projected on an object whose distance to the sensor is smaller or larger than that of the reference plane the position of the speckle in the infrared image will be shifted in the direction of the baseline between the laser projector and the perspective center of the infrared camera these shifts are measured for all speckles by a simple image correlation procedure which yields a disparity image for each pixel the distance to the sensor can then be retrieved from the corresponding disparity as described in the next section figure illustrates the depth measurement from the speckle pattern sensors figure a infrared image of the pattern of speckles projected on a sample scene b the resulting depth image mathematical model figure illustrates the relation between the distance of an object point k to the sensor relative to a reference plane and the measured disparity d to express the coordinates of the object points we consider a depth coordinate system with its origin at the perspective center of the infrared camera the z axis is orthogonal to the image plane towards the object the x axis perpendicular to the z axis in the direction of the baseline b between the infrared camera center and the laser projector and the y axis orthogonal to x and z making a right handed coordinate system figure relation between relative depth and measured disparity assume that an object is on the reference plane at a distance z o to the sensor and a speckle on the object is captured on the image plane of the infrared camera if the object is shifted closer to or further away from the sensor the location of the speckle on the image plane will be displaced in the x a b o k d c ir camera l laser projector b reference plane object plane z o z k f d z x sensors direction this is measured in image space as disparity d corresponding to a point k in the object space from the similarity of triangles we have d b zz o z o k and d f d k where z k z denotes the distance depth of the point k in object space b is the base length f is the focal length of the infrared camera d is the displacement of the point k in object space and d is the observed disparity in image space substituting d from equation into equation and expressing z k in terms of the other variables yields z k z o z o fb d equation is the basic mathematical model for the derivation of depth from the observed disparity provided that the constant parameters z o f and b can be determined by calibration the z coordinate of a point together with f defines the imaging scale for that point the planimetric object coordinates of each point can then be calculated from its image coordinates and the scale x z f xxx δ f yyy k o where x k k k k o y k z k δ and y k are the image coordinates of the point x o and y o are the coordinates of the principal point and δx and δy are corrections for lens distortion for which several models with different coefficients exist see for instance note that here we assume that the image coordinate system is parallel with the base line and thus with the depth coordinate system calibration as mentioned above the calibration parameters involved in the mathematical model for the calculation of coordinates from the raw image measurements include focal length f principal point offsets x o y o lens distortion coefficients in δx δy base length b distance of the reference pattern z o in addition we may consider a misalignment angle between the x axis of the image coordinate system and the base line however this does not affect the calculation of the object coordinates if we define the depth coordinate system to be parallel with the image coordinate system instead of the base line we may therefore ignore this misalignment angle sensors from the calibration parameters listed above the first three can be determined by a standard calibration of the infrared camera in practice however the calibration parameters of the infrared camera do not directly correspond to the disparity images because the size of the disparity images computed by the internal processor of kinect is pixels which is smaller than the actual size of the infrared sensor pixels due to the bandwidth limitation of the usb connection the images of the infrared sensor are also streamed in a reduced size of pixels corresponding to the disparity images that is the images are resized and cropped therefore a convenient approach to the calibration is to estimate the calibration parameters from the reduced infrared images instead of the actual sensor provided that a pixel to pixel correspondence exists between the reduced infrared images and the disparity images by examining the images we observed a shift of pixels in the x direction between the disparity and infrared images supposedly implying the application of a pixel wide correlation window for calculating disparities once this shift is corrected for the calibration parameters estimated from the reduced infrared images can be applied to the measurements in the disparity images the determination of the base length and the reference distance is more complicated for the following reason in practice the raw disparity measurements are normalized and quantized between and and streamed as bit integers therefore in equation d should be replaced with md n with d the normalized disparity and m n the parameters of a supposedly linear normalization in fact denormalization including these in equation and inverting it yields z k fb n m fb zd o equation expresses a linear relation between the inverse depth of a point and its corresponding normalized disparity by observing the normalized disparity for a number of object points or planes at known distances to the sensor the coefficients of this linear relation can be estimated in a least squares fashion however the inclusion of the normalization parameters does not allow determining b and z o separately the calibration parameters mentioned above completely define the relation between the image measurements x y d and object coordinates x y z of each point once estimated they enable the generation of a point cloud from each disparity image note that these parameters do not describe the internal geometry of the infrared camera as they are estimated from the resized and cropped images adding color to the point cloud the integration of the depth and color data requires the orientation of the rgb camera relative to the depth coordinate system since we defined the depth coordinate system at the perspective center of the infrared camera we can estimate the orientation parameters by a stereo calibration of the two cameras the parameters to be estimated include three rotations between the camera coordinate system of the rgb camera and that of the infrared camera and the position of the perspective center of the rgb camera in the coordinate system of the infrared camera in addition the interior orientation parameters of the rgb camera i e the focal length principal point offsets and the lens distortion parameters must be estimated sensors in practice the images of the rgb camera are also streamed in a reduced size therefore it is more convenient to perform a stereo calibration of the reduced images instead of the physical cameras the resulting parameters describe the relation between the coordinates of each point and its corresponding pixel coordinates in the reduced rgb image once these parameters are estimated we can add color to the point cloud by projecting each point to the rgb image and interpolating the color depth accuracy and resolution accuracy and point density are two important measures for evaluating the quality of a point cloud in the following sections factors influencing the accuracy and density of kinect data are discussed and a theoretical random error model is presented error sources error and imperfection in the kinect data may originate from three main sources the sensor measurement setup properties of the object surface the sensor errors for a properly functioning device mainly refer to inadequate calibration and inaccurate measurement of disparities inadequate calibration and or error in the estimation of the calibration parameters lead to systematic error in the object coordinates of individual points such systematic errors can be eliminated by a proper calibration as described in the previous section inaccurate measurement of disparities within the correlation algorithm and particularly the quantization of the disparities also influence the accuracy of individual points errors caused by the measurement setup are mainly related to the lighting condition and the imaging geometry the lighting condition influences the correlation and measurement of disparities in strong light the laser speckles appear in low contrast in the infrared image which can lead to outliers or gap in the resulting point cloud the imaging geometry includes the distance to the object and the orientation of the object surface relative to the sensor the operating range of the sensor is between m to m according to the specifications and as we will see in the following section the random error of depth measurement increases with increasing distance to the sensor also depending on the imaging geometry parts of the scene may be occluded or shadowed in figure the right side of the box is occluded as it cannot be seen by the infrared camera though it may have been illuminated by the laser pattern the left side of the box is shadowed because it is not illuminated by the laser but is captured in the infrared image both the occluded areas and shadows appear as gaps in the point cloud the properties of the object surface also impact the measurement of points as it can be seen in figure smooth and shiny surfaces that appear overexposed in the infrared image the lower part of the box impede the measurement of disparities and result in a gap in the point cloud sensors theoretical random error model assuming that in equation the calibration parameters are determined accurately and that d is a random variable with a normal distribution we can propagate the variance of the disparity measurement to obtain the variance of the depth measurement as follows σ z z d σ d after simplification this yields the following expression for the standard deviation of depth σ m σ with σ d z fb z d and σ z respectively the standard deviation of the measured normalized disparity and the standard deviation of the calculated depth equation basically expresses that the random error of depth measurement is proportional to the square distance from the sensor to the object since depth is involved in the calculation of the planimetric coordinates see equation we expect the error in x and y to be also a second order function of depth by propagating the errors in equation and assuming that the random error of image coordinates x y can be ignored we obtain the random error of x and y σ bf mx z σ σ y σ d depth resolution and point density the resolution of the infrared camera or more precisely the pixel size of the disparity image determines the point spacing of the depth data on the xy plane perpendicular to camera axis since each depth image contains a constant pixels the point density will decrease with increasing distance of the object surface from the sensor considering the point density as the number of points per unit area while the number of points remains constant the area is proportional to the square distance from the sensor thus the point density on the xy plane is inversely proportional to squared distance from the sensor the depth resolution refers to the minimum depth difference that can be measured and is determined by the number of bits per pixel used to store the disparity measurements the kinect disparity measurements are stored as bit integers where one bit is reserved to mark the pixels for which no disparity is measured so called no data thus a disparity image contains levels of disparity since depth is inversely proportional to disparity the resolution of depth is also inversely related to the levels of disparity let z d denote depth as a function of normalized disparity d then depth resolution is simply the depth difference corresponding to two successive levels of disparity i e δ z x d my bf z d z d z d and as we learned the differential yields z m fb z sensors thus the depth resolution is also a quadratic function of depth and decreases with increasing distance from the sensor experiments and results experiments were carried out to first determine the calibration parameters of the sensor and then investigate the systematic and random errors in the depth data the following sections describe the tests and discuss the results calibration results a standard camera calibration was performed using the reduced images of both the infrared camera and the rgb camera to estimate the calibration parameters in the photomodeler software a total of eight images of a target pattern were taken by both cameras from different angles to avoid the disturbance of the laser speckles in the infrared images the aperture of the laser emitter was covered by a piece of opaque tape to model the lens distortion we used the well known model of brown with three radial distortion parameters and two decentering parameters the calibration was first performed with all lens distortion parameters as unknowns then those parameters whose standard deviation was large compared to the estimated parameter value were removed from the estimation model and the remaining parameters were estimated again as a result parameter was excluded from the parameter sets of both cameras and was excluded from the parameter set of the rgb camera table summarizes the results of the calibration procedure the overall calibration accuracy as the rms of point marking residuals in image space was pixels for the ir images and pixels for the rgb images in the parameters of the rgb images notice the very large principal point offset y o of mm corresponding to pixels see also figure b this value is close to pixels which is the offset we would expect if a reduced image was obtained by resizing a full resolution image to one half and cropping it at the top the infrared images however do not have large principal point offsets meaning that they were cropped at the center the reason for this apparently inconsistent cropping and the large y o in the rgb images is not known to the authors table calibration parameters estimated for the infrared and rgb images calibration parameter ir images rgb images focal length f mm 006 mm principal point offset x o mm mm y o mm mm frame dimension w mm mm h mm mm pixel size p x μm μm p y μm μm radial lens distortion k2 decentring lens distortion sensors figure lens distortions of a ir camera and b rgb camera the principal points are marked by x and the image centers by figure shows the combined effect of radial and decentering distortions for both the ir and the rgb images notice the larger effect of decentering distortions in the ir image as compared to the rgb image the magnitude of radial distortions however is larger in the rgb image particularly in the upper left corner where radial distortions reach pixels mm this can be verified by examining the radial distortion curves in figure which show that radial distortions of the rgb camera are generally larger than those of the ir camera in practice radial distortions in the rgb images lead to misalignments between the color and depth data in the point cloud a distortion of mm in the image space corresponds to a misalignment of cm at the maximum range of the sensor m figure radial distortion curves for the ir and rgb images for the stereo calibration images of a checkerboard pattern were taken simultaneously by the two cameras and the relative orientation parameters were estimated in a bundle adjustment table lists the resulting parameters as it can be seen the rotations are quite small and the relative position a b sensors parameters indicate that the center of the rgb camera is approximately on the base line between the ir camera and the laser emitter table exterior orientation parameters of the rgb camera relative to the ir camera rotation parameters degree position parameters mm r x r y r z t x t y t z to determine the parameters involved in the disparity depth relation equation depth values were measured at eight different distances to the sensor using a measuring tape the inverse of the measured distances were then plotted against the corresponding normalized disparities observed by the sensor see figure as it can be seen the relation is linear as we expected from the mathematical model in equation the parameters of the disparity depth relation were obtained by a simple least squares line regression these were found to be as the slope and as the intercept of the line using these parameters we can now calculate depth values from the observed normalized disparities figure linear relation of normalized disparity with inverse depth comparison of kinect point cloud with the point cloud of a high end laser scanner to investigate the systematic errors in kinect data a comparison was made with a point cloud obtained by a high end laser scanner the kinect point cloud was obtained from the disparity image using equations and and the calibration parameters from the previous step the laser scanner point cloud was obtained of the same scene by a calibrated faro ls laser scanner the nominal range accuracy of the laser scanner is mm for a highly reflective target at a distance of m to the scanner the average point spacing of the laser scanner point cloud on a surface perpendicular to the range direction and also the optical axis of the infrared camera of kinect was mm it was therefore assumed that the laser scanner point cloud is sufficiently accurate and dense to serve as reference for the accuracy evaluation of the kinect point cloud in the absence of any systematic errors the mean of discrepancies between the two point clouds is expected to be close to zero to enable this analysis first an accurate registration of the two point clouds is necessary the registration accuracy is important because any registration error may be misinterpreted as error in the sensors kinect point cloud to achieve the best accuracy two registration methods were tested the first method consisted of a manual rough alignment followed by a fine registration using the iterative closest point icp algorithm to make icp more efficient a variant suggested by pulli was followed in which randomly selected correspondences closest points with a rejection rate of were used in the second method the two roughly aligned point clouds were segmented into planar surfaces and corresponding segments were manually selected then a robust plane fitting using ransac was applied to obtain plane parameters and the inlying points the registration was then performed by minimizing the distances from the points in one point cloud to their corresponding planes in the other point cloud in both registrations the estimated transformation parameters consisted of a rotation and a translation to reveal a possible scale difference between the point clouds a third registration was performed using the plane based method augmented with a scale parameter table summarizes the registration residuals pertaining to the three methods it is clear that the methods perform similarly all yielding very comparable residuals furthermore the scale parameter obtained from the third registration was found to be the largest effect of such a scale on the furthest point of the point cloud is cm which is not larger than the random error and depth resolution of the data as will be shown later thus we can conclude that there is no scale difference between the kinect point cloud and the laser scanner point cloud table registration results of the three methods transformation parameters residuals r x r y r z deg t x t y t z cm min cm mean cm med cm std cm max cm point point distances icp point plane distances without scale point plane distances with scale for the comparison between the two point clouds the result of the icp registration method was used a total of points were randomly selected from the kinect point cloud and for each point the nearest neighbor was found in the laser scanner point cloud these closest point pairs were the basis for evaluating the accuracy of the kinect point cloud however it was considered that the point pairs might contain incorrect correspondences because the two sensors had slightly different viewing angles and therefore areas that could not be seen by one sensor might be captured by the other and vice versa figure shows the two point clouds and the closest point pairs figure shows the histograms of discrepancies between the point pairs in x y and z table lists the statistics related to these discrepancies the mean and median discrepancies are close to zero which is an indication that there are no systematic shifts between the two point clouds for comparison the last three rows of table show the discrepancies between the laser scanner point cloud and an uncalibrated kinect point cloud measured on the same number of sampled point pairs the discrepancies are clearly larger when the uncalibrated point cloud is used indicating the effect of calibration sensors figure comparison of kinect point cloud cyan with the point cloud obtained by faro laser scanner white the larger points are samples randomly selected from the kinect data blue and their closest point in the laser scanner data red the thumbnail on the lower right is a color image of the setup figure shows the distribution of the point pair distances in the x z plane in general points that are located further away from the sensor particularly those at the sides of the point cloud show larger discrepancies this is what we expected based on the theoretical random error model overall the comparison of the two point clouds shows that about of the point pairs are less than cm apart figure histograms of discrepancies between the closest point pairs in x y and z direction sensors table statistics of discrepancies between the closest point pairs the last three rows show for comparison the same statistics obtained for an uncalibrated kinect point cloud figure distribution of point pair distances in the x z plane mean median standard interquartile percentage in percentage in percentage in cm cm deviation cm range cm cm cm cm cm cm cm dx dy dz dx dy dz plane fitting test to verify the relation between the random error and the distance to the sensor a plane fitting test was carried out the planar surface of a door was measured at various distances from m to m the operation range of the sensor with m intervals in each resulting point cloud a same part of the door was selected and a plane was fitted to the selected points the ransac plane fitting method was used to avoid the influence of outliers figure shows the measurement setup figure the planar surface of a door measured at different distances to the sensor the boxes show the plane fitting area m m m m m sensors since in all measurements the selected planar surface was approximately perpendicular to the optical axis of the sensor the residuals of the plane fitting procedure can be seen as a representation of the depth random error to evaluate this random error at different distances an equal number of samples samples were randomly selected from each plane and the standard deviation of the residuals was calculated over the selected samples figure shows the calculated standard deviations plotted against the distance from the plane to the sensor the black squares it can be seen that the errors increase quadratically from a few millimeters at m distance to about cm at the maximum range of the sensor although the plane fitting residuals can be seen as an indication of random error of depth measurement they are also influenced by the depth resolution at each plane having determined the calibration parameters we can now evaluate equations and to obtain the theoretical random error and resolution of individual depth measurements at different distances from the sensor in figure the red curve shows the theoretical random error obtained from equation with m fb from the depth calibration result figure and assuming a disparity measurement error σ d of pixel the blue curve is a plot of depth resolution obtained by evaluating equation the disparity error of pixel seems a fair estimate since the theoretical random error curve is consistent with the observed standard deviations considering that the low depth resolution has a minor effect on the estimate of the standard deviation of plane fitting residuals when a large number of samples are used figure standard deviation of plane fitting residuals at different distances of the plane to the sensor the curves show the theoretical random error red and depth resolution blue although depth resolution does not have a large influence on the standard deviation of plane fitting residuals its effect on the level of individual points should not be understated this effect is more pronounced at larger distances from the sensor such that at the maximum range of meters the point spacing in the depth direction is centimeters the combined effect of the random error and low depth resolution at large distances results in a planar surface perpendicular to the sensor appearing as several layers of points in the data when seen from side view figure shows the point clouds of the door plane at three distances projected on the y z plane z being the depth direction see section for the definition of the coordinate system while at m the point spacing in the depth direction is sensors quite small about mm at m and m the points are clearly distributed in several layers at intervals corresponding to the depth resolution which is about cm for the plane at m distance and close to cm at m figure point cloud of a planar surface at meter a meter b and meter c distance from the sensor projected on the y z plane colors represent distance to the best fit plane in centimeters a b c conclusions the paper presented a theoretical and experimental analysis of the geometric quality of depth data acquired by the kinect sensor the geometric quality measures represent the depth accuracy and resolution for individual points indoor mapping applications are often based on the extraction of objects instead of an irregular set of points in order to describe the quality of extracted objects some basic error propagation would be needed while fitting geometric object models to the data can reduce the influence of random errors and low depth resolution the effect of systematic errors can only be eliminated through a proper calibration procedure from the results of calibration and error analysis the following main conclusions can be drawn to eliminate distortions in the point cloud and misalignments between the colour and depth data an accurate stereo calibration of the ir camera and the rgb camera is necessary the random error of depth measurements increases quadratically with increasing distance from the sensor and reaches cm at the maximum range of meters the depth resolution also decreases quadratically with increasing distance from the sensor the point spacing in the depth direction along the optical axis of the sensor is as large as cm at the maximum range of meters in general for mapping applications the data should be acquired within m distance to the sensor at larger distances the quality of the data is degraded by the noise and low resolution of the depth measurements a brain computer interface bci is a hardware and software communications system that permits cerebral activity alone to control computers or external devices the immediate goal of bci research is to provide communications capabilities to severely disabled people who are totally paralyzed or locked in by neurological neuromuscular disorders such as amyotrophic lateral sclerosis brain stem stroke or spinal cord injury here we review the state of the art of bcis looking at the different steps that form a standard bci signal acquisition preprocessing or signal enhancement feature extraction classification and the control interface we discuss their advantages drawbacks and latest advances and we survey the numerous technologies reported in the scientific literature to design each step of a bci first the review examines the neuroimaging modalities used in the signal acquisition step each of which monitors a different functional brain activity such as electrical magnetic or metabolic activity second the review discusses different electrophysiological control signals that determine user intentions which can be detected in brain activity third the review includes some techniques used in the signal enhancement step to deal with the artifacts in the control signals and improve the performance fourth the review studies some mathematic algorithms used in the feature extraction and classification steps which translate the information in the control signals into commands that operate a computer or other device finally the review provides an overview of various bci applications that control a range of devices keywords brain computer interface bci electroencephalography eeg rehabilitation artifact neuroimaging brain machine interface collaborative sensor system open access sensors introduction a brain computer interface bci also referred to as a brain machine interface bmi is a hardware and software communications system that enables humans to interact with their surroundings without the involvement of peripheral nerves and muscles by using control signals generated from electroencephalographic activity bci creates a new non muscular channel for relaying a person intentions to external devices such as computers speech synthesizers assistive appliances and neural prostheses that is particularly attractive for individuals with severe motor disabilities such an interface would improve their quality of life and would at the same time reduce the cost of intensive care a bci is an artificial intelligence system that can recognize a certain set of patterns in brain signals following five consecutive stages signal acquisition preprocessing or signal enhancement feature extraction classification and the control interface the signal acquisition stage captures the brain signals and may also perform noise reduction and artifact processing the preprocessing stage prepares the signals in a suitable form for further processing the feature extraction stage identifies discriminative information in the brain signals that have been recorded once measured the signal is mapped onto a vector containing effective and discriminant features from the observed signals the extraction of this interesting information is a very challenging task brain signals are mixed with other signals coming from a finite set of brain activities that overlap in both time and space moreover the signal is not usually stationary and may also be distorted by artifacts such as electromyography emg or electrooculography eog the feature vector must also be of a low dimension in order to reduce feature extraction stage complexity but without relevant information loss the classification stage classifies the signals taking the feature vectors into account the choice of good discriminative features is therefore essential to achieve effective pattern recognition in order to decipher the user intentions finally the control interface stage translates the classified signals into meaningful commands for any connected device such as a wheelchair or a computer bci technology has traditionally been unattractive for serious scientific investigation the idea of successfully deciphering thoughts or intentions by means of brain activity has often been rejected in the past as very strange and remote hence investigation in the field of brain activity has usually been limited to the analysis of neurological disorders in the clinic or to the exploration of brain functions in the laboratory the bci design was considered too complex because of the limited resolution and reliability of information that was detectable in the brain and its high variability furthermore bci systems require real time signal processing and up until recently the requisite technology either did not exist or was extremely expensive however this context has undergone radical change over the last two decades bci research which was confined to only three groups years ago and only six to eight groups years ago is now a flourishing field with more than active research groups all over the world studying the topic the number of articles published regarding neural interface technology has increased exponentially over the past decade successful studies on brain signal phenomena have lent further weight to these advances the development of more and more inexpensive computer hardware and software has allowed more sophisticated online analysis likewise the chances of using bcis as auxiliary technology that might serve severely disabled people has increased social acceptance in this field and the need to accelerate its progress interest in this technology is now found outside of the laboratory or sensors the clinic small specialized companies such as emotiv or neurosky have already developed some initial applications oriented towards the general public nevertheless despite these advances most bci based applications are still limited to the laboratory broader applicability of bcis requires greater ease of use which in turn means reducing time spent on preparation training and calibration bci research is a relatively young multidisciplinary field integrating researchers from neuroscience physiology psychology engineering computer science rehabilitation and other technical and health care disciplines as a result in spite of some notable advances a common language has yet to emerge and existing bci technologies vary which makes their comparison difficult and in consequence slows down the research the community of bci researchers has therefore stressed the need to establish a general framework for bci design mason et al for example proposed a new functional model for bci systems and taxonomy design this review of the state of the art of bci systems is arranged as follows section discusses existing neuroimaging approaches to bcis and section describes the most commonly found control signals in bci systems section briefly explains certain types of bcis sections and respectively cover the different signal processing methods used for feature extraction artifact reduction and feature classification section provides an overview of bci applications and finally the conclusions are drawn in section neuroimaging approaches in bcis bcis use brain signals to gather information on user intentions to that effect bcis rely on a recording stage that measures brain activity and translates the information into tractable electrical signals two types of brain activities may be monitored i electrophysiological and ii hemodynamic electrophysiological activity is generated by electro chemical transmitters exchanging information between the neurons the neurons generate ionic currents which flow within and across neuronal assemblies the large variety of current pathways can be simplified as a dipole conducting current from a source to a sink through the dendritic trunk these intracellular currents are known as primary currents conservation of electric charges means that the primary currents are enclosed by extracellular current flows which are known as secondary currents electrophysiological activity is measured by electroencephalography electrocorticography magnetoencephalography and electrical signal acquisition in single neurons the hemodynamic response is a process in which the blood releases glucose to active neurons at a greater rate than in the area of inactive neurons the glucose and oxygen delivered through the blood stream results in a surplus of oxyhemoglobin in the veins of the active area and in a distinguishable change of the local ratio of oxyhemoglobin to deoxyhemoglobin these changes can be quantified by neuroimaging methods such as functional magnetic resonance and near infrared spectroscopy these kinds of methods are categorized as indirect because they measure the hemodynamic response which in contrast to electrophysiological activity is not directly related to neuronal activity most current bcis obtain the relevant information from the brain activity through electroencephalography electroencephalography is by far the most widely used neuroimaging modality owing to its high temporal resolution relative low cost high portability and few risks to the users bcis based on electroencephalography consist of a set of sensors that acquire electroencephalography signals sensors from different brain areas however the quality of electroencephalography signals is affected by scalp skull and many other layers as well as background noise noise is key to electroencephalography and to other neuroimaging methods insofar as it reduces the snr and therefore the ability to extract meaningful information from the recorded signals non invasive approaches have successfully been used by severely and partially paralyzed patients to reacquire basic forms of communication and to control neuroprostheses and wheelchairs despite the outstanding utility of non invasive approaches in bci applications motor recovery has been limited because of the need for brain signals with a higher resolution invasive recording methods such as electrocorticography or intracortical neuron recording were introduced in an effort to improve the quality of brain signals monitored by bcis most researchers agree that movement restoration through prostheses with multiples degrees of freedom can only be achieved through invasive approaches it is unlikely that the power of non invasive modalities will be enhanced in the near future accordingly it would appear that invasive modalities are indispensable for accurate neuroprostheses control nevertheless this issue is not yet entirely clear and some opinions disagree with this conjecture contrary to established opinion wolpaw suggested that performance in multidimensional control may be independent of the recording method further refinements of recording and analysis techniques will probably increase the performance of both invasive and non invasive modalities however the latest studies in neuroprostheses control appear to indicate that invasive modalities have inherent advantages in neuroprosthesis control applications invasive modalities need to implant microelectrode arrays inside the skull that involves significant health risks which restricts their use to experimental settings two invasive modalities can be found in bci research electrocorticography which places electrodes on the surface of the cortex either outside the dura mater epidural electrocorticography or under the dura mater subdural electrocorticography and intracortical neuron recording which implants electrodes inside the cortex several issues had to be addressed before they become suitable for long term applications first tissue acceptance of the microelectrode has to be addressed for which reason proposals exist for electrodes with neurotropic mediums that promote neuronal growth to improve biocompatibility perhaps the future of nanotechnologies that might develop nano detectors to be implanted inertly in the brain may provide a definite solution to the problems of long term invasive applications second a link between the microelectrode and external hardware that uses wireless technology is needed to reduce the risks of infection wireless transmission of neuronal signals has already been tested in animals and third continuous stress caused by plugging and unplugging the recording system may lead to tissue damage or system failure each neuroimaging modality is explained below firstly electrophysiological methods such as electroencephalography electrocorticography magnetoencephalography and electrical signal acquisition in single neurons will be discussed secondly metabolic methods such as functional magnetic resonance and near infrared spectroscopy will be described finally functional imaging modalities are listed in table along with information related to activity measured temporal and spatial resolutions safety and portability sensors table summary of neuroimaging methods neuroimaging method measured activity measurement indirect direct resolution temporal spatial resolution risk portability eeg electrical direct mm non invasive portable meg magnetic direct mm non invasive non portable ecog electrical direct mm invasive portable intracortical neuron recording mm lfp electrical direct mm mua invasive portable mm sua fmri metabolic indirect mm non invasive non portable nirs metabolic indirect mm non invasive portable electroencephalography eeg eeg measures electric brain activity caused by the flow of electric currents during synaptic excitations of the dendrites in the neurons and is extremely sensitive to the effects of secondary currents eeg signals are easily recorded in a non invasive manner through electrodes placed on the scalp for which that reason it is by far the most widespread recording modality however it provides very poor quality signals as the signals have to cross the scalp skull and many other layers this means that eeg signals in the electrodes are weak hard to acquire and of poor quality this technique is moreover severely affected by background noise generated either inside the brain or externally over the scalp the eeg recording system consists of electrodes amplifiers a d converter and a recording device the electrodes acquire the signal from the scalp the amplifiers process the analog signal to enlarge the amplitude of the eeg signals so that the a d converter can digitalize the signal in a more accurate way finally the recording device which may be a personal computer or similar stores and displays the data the eeg signal is measured as the potential difference over time between signal or active electrode and reference electrode an extra third electrode known as the ground electrode is used to measure the differential voltage between the active and the reference points the minimal configuration for eeg measurement therefore consists of one active one reference and one ground electrode multi channel configurations can comprise up to or active electrodes these electrodes are usually made of silver chloride agcl electrode scalp contact impedance should be between kω and kω to record an accurate signal the electrode tissue interface is not only resistive but also capacitive and it therefore behaves as a low pass filter the impedance depends on several factors such as the interface layer electrode surface area and temperature eeg gel creates a conductive path between the skin and each electrode that reduces the impedance use of the gel is cumbersome however as continued maintenance is required to assure a relatively good quality signal electrodes that do not need to use of gels called dry electrodes have been made with other materials such as titanium and stainless steel these kinds of electrodes may be dry active electrodes which have preamplification circuits for dealing with very high electrode skin interfacial sensors impedances or dry passive electrodes which have no active circuits but are linked to eeg recording systems with ultra high input impedance the amplitude of electrical bio signals is in the order of microvolts consequently the signal is very sensitive to electronic noise external sources such power lines may generate background noise and thermal shot flicker and burst noises are generated by internal sources design considerations should be addressed to reduce the effects of the noise such as electromagnetic interference shielding or reduction for common mode signal amongst others eeg comprises a set of signals which may be classified according to their frequency well known frequency ranges have been defined according to distribution over the scalp or biological significance these frequency bands are referred to as delta δ theta θ alpha α beta β and gamma γ from low to high respectively relevant characteristics of these bands are detailed below the delta band lies below hz and the amplitude of delta signals detected in babies decreases as they age delta rhythms are usually only observed in adults in deep sleep state and are unusual in adults in an awake state a large amount of delta activity in awake adults is abnormal and is related to neurological diseases due to low frequency it is easy to confuse delta waves with artifact signals which are caused by the large muscles of the neck or jaw theta waves lie within the to hz range in a normal awake adult only a small amount of theta frequencies can be recorded a larger amount of theta frequencies can be seen in young children older children and adults in drowsy meditative or sleep states like delta waves a large amount of theta activity in awake adults is related to neurological disease theta band has been associated with meditative concentration and a wide range of cognitive processes such as mental calculation maze task demands or conscious awareness alpha rhythms are found over the occipital region in the brain these waves lie within the to hz range their amplitude increases when the eyes close and the body relaxes and they attenuate when the eyes open and mental effort is made these rhythms primarily reflect visual processing in the occipital brain region and may also be related to the memory brain function there is also evidence that alpha activity may be associated with mental effort increasing mental effort causes a suppression of alpha activity particularly from the frontal areas consequently these rhythms might be useful signals to measure mental effort mu rhythms may be found in the same range as alpha rhythms although there are important physiological differences between both in contrast to alpha rhythms mu rhythms are strongly connected to motor activities and in some cases appear to correlate with beta rhythms beta rhythms within the to hz range are recorded in the frontal and central regions of the brain and are associated with motor activities beta rhythms are desynchronized during real movement or motor imagery beta waves are characterized by their symmetrical distribution when there is no motor activity however in case of active movement the beta waves attenuate and their symmetrical distribution changes gamma rhythms belong to the frequency range from to hz the presence of gamma waves in the brain activity of a healthy adult is related to certain motor functions or perceptions among others some experiments have revealed a relationship in normal humans between motor activities and gamma waves during maximal muscle contraction this gamma band coherence is replaced by a beta band coherence during weak contractions suggesting a correlation between gamma or beta sensors cortical oscillatory activity and force also several studies have provided evidence for the role of gamma activity in the perception of both visual and auditory stimuli gamma rhythms are less commonly used in eeg based bci systems because artifacts such as electromyography emg or electrooculography eog are likely to affect them nevertheless this range is attracting growing attention in bci research because compared to traditional beta and alpha signals gamma activity may increase the information transfer rate and offer higher spatial specifity as explained above eeg is recorded by electrodes the electrodes placed over the scalp are commonly based on the international system which has been standardized by the american electroencephalographic society the system uses two reference points in the head to define the electrode location one of these reference points is the nasion located at the top of the nose at the same level as the eyes the other reference point is the inion which is found in the bony lump at the base of the skull the transverse and median planes divide the skull from these two points the electrode locations are determined by marking these planes at intervals of and figure the letters in each location corresponds to specific brain regions in such a way that a represents the ear lobe c the central region p g the nasopharyngeal p the parietal f the frontal f p the frontal polar and o the occipital area meg is a non invasive imaging technique that registers the brain magnetic activity by means of magnetic induction meg measures the intracellular currents flowing through dendrites which produce magnetic fields that are measurable outside of the head the neurophysiological processes that produce meg signals are identical to those that produce eeg signals nevertheless while eeg is extremely sensitive to secondary current sources meg is more sensitive to those of primary agnetic fields are detected by superconducting quantum interferences devices which are extremely sensitive to magnetic disturbances produced by neural activity the electronic equipment that measures magnetic brain activity is cooled to almost degrees celsius to facilitate sensor superconductivity meg requires effective shielding from electromagnetic interferences the electronic equipment is installed inside a magnetically shielded room which attenuates the effects of magnetic fields from external sources meg provides signals with higher spatiotemporal resolution than eeg which reduces the training time needed to control a bci and speeds up reliable communications meg has also been successfully used to localize active regions inside the brain in spite of these advantageous features meg is not often used in bci design because meg technology is too bulky and expensive to become an acquisition modality suitable for everyday use in lal et al presented the first online meg based bci although further studies have followed meg based bcis as compared to eeg based bcis are still at an early stage electrocorticography ecog ecog is a technique that measures electrical activity in the cerebral cortex by means of electrodes placed directly on the surface of the brain compared to eeg ecog provides higher temporal and spatial resolution as well as higher amplitudes and a lower vulnerability to artifacts such as blinks and eye movement however ecog is an invasive recording modality which requires a craniotomy to implant an electrode grid entailing significant health hazards for that reason the first studies on ecog were with animals early studies involving animals evaluated the long term stability of the signals from the brain that ecog could acquire the results showed that subdural electrodes could provide stable signals over several months nevertheless the long term stability of the signals acquired by ecog is currently unclear more recent experiments with monkeys have shown that ecog can perform at a high level for months without any drift in accuracy or recalibration the hand positions and arm joint angles could be successfully decoded during asynchronous movements these studies have also developed minimally invasive protocols to implant the ecog probes in humans ecog has been used for the analysis of alpha and beta waves or gamma waves produced during voluntary motor action with regard to the use of ecog in bcis systems levine et al designed a bci which classified motor actions on the basis of the identification of the event related potentials erp using ecog leuthardt et al showed for the first time that an ecog based bci could provide information to control a one dimensional cursor as this information is more precise and more quickly acquired than by eeg based bcis some years later schalk et al presented a more advanced ecog based bci which allowed the user to control a two dimensional cursor the results of all these studies might make it more feasible for people with severe motor disabilities to use ecog based bcis for their communication and control needs sensors intracortical neuron recording intracortical neuron recording is a neuroimaging technique that measures electrical activity inside the gray matter of the brain it is an invasive recording modality that needs to implant microelectrode arrays inside the cortex to capture spike signals and local field potentials from neurons three signals can be obtained by intracortical neuron recording single unit activity sua multi unit activity mua and local field potentials lfps sua is obtained by high pass filtering hz of the signal of a single neuron mua is obtained in the same way but the signals may come from multiple neurons lfps are extracted by low pass filtering hz of the neuron activity in the vicinity of an electrode tip lfps are analog signals whereas sua and mua measure the spiking activity of single neurons and can be reduced to discrete events in time intracortical neuron recording provides much higher spatial and temporal resolution than eeg recording hence the intracortical signals may be easier to use than eeg signals however signal quality may be affected by the reaction of cerebral tissue to the implanted recording microelectrode and by changes in the sensitivity of the microelectrode which may be progressively damaged over the course of days and years the user can naturally adapt to these slow changes in the relative sensitivity of the microelectrode without the need for specific retraining nevertheless periodic recalibrations of electrode sensitivity may be necessary the first attempts in the intracortical neuron recording field were made in animals multielectrode arrays have been used to record neural activity from the motor cortex in monkeys or rats during learned movements these initial studies have shown that intracortical neuron recordings can indicate the nature of a movement and its direction these studies do not reveal whether the same patterns will be present when the real movements are not made in that regard taylor and schwartz experimented with rhesus macaques which made real and virtual arm movements in a computer the results suggested that the same patterns persisted the most recent studies with monkeys investigated the control of prosthetic devices for direct real time interaction with the physical environment with regard to the application of intracortical neuron recording in bci systems microelectrode arrays such as the utah intracortical electrode array uiea have been reported as a suitable means of providing simultaneous and proportional control of a large number of external devices also kennedy et al employed cortical control signals to design a bci that allowed users to control cursor movement and flexion of a cyber digit finger on a virtual hand functional magnetic resonance imaging fmri fmri is a non invasive neuroimaging technique which detects changes in local cerebral blood volume cerebral blood flow and oxygenation levels during neural activation by means of electromagnetic fields fmri is generally performed using mri scanners which apply electromagnetic fields of strength in the order of or the main advantage of the use of fmri is high space resolution for that reason fmri have been applied for localizing active regions inside the brain however fmri has a low temporal resolution of about or seconds additionally the hemodynamic response introduces a physiological delay from to seconds fmri appears unsuitable for rapid communication in bci systems and is highly susceptible to head motion artifacts sensors in bci systems fmri is typically used to measure the blood oxygen level dependent bold during neuronal activation although the bold signal is not directly related to neuronal activity a correspondence between both does exist the use of fmri in bci technology is relatively recent before the emergence of real time fmri brain activity recording by fmri has traditionally taken a long time the data acquired by fmri techniques were processed offline and the results only became available after several hours or even days fmri based bcis have been made possible thanks to the development of real time fmri the information transfer rate in fmri based bcis is between and bits min non clinical fmri applications are not expected because fmri requires overly bulky and expensive hardware near infrared spectroscopy nirs nirs is an optical spectroscopy method that employs infrared light to characterize noninvasively acquired fluctuations in cerebral metabolism during neural activity infrared light penetrates the skull to a depth of approximately cm below its surface where the intensity of the attenuated light allows alterations in oxyhemoglobin and deoxyhemoglobin concentrations to be measured due to shallow light penetration in the brain this optical neuroimaging technique is limited to the outer cortical layer in a similar way to fmri one of the major limitations of nirs is the nature of the hemodynamic response because vascular changes occur a certain number of seconds after its associated neural activity the spatial resolution of nirs is quite low in the order of cm nevertheless nirs offers low cost high portability and an acceptable temporal resolution in the order of milliseconds a nirs system consists of a light source a driving electronic device a light detector signal processing devices and a recording device the light source is an infrared emitting diode ired placed in direct contact with the scalp the driving electronic device is an electronic circuit that controls the ired in order to modulate the light the light detector is a photodiode placed right next to the light source the signal processing devices are amplifiers and filters that process the electrical signal and reduce the noise due to ambient light the recording device is a personal computer or any other device that digitalizes stores and displays the electrical signal ensuring good coupling light from the optical sources and detectors to and from the subject head is not a trivial issue head motions or hair obstruction can worsen performance and signal quality good quality signals and noise reduction especially background noise induced by head motions are important requirements in real time bci systems hair obstruction can be overcome by combing the hair out of the photons path by means of hair gel and hair clips noise can be reduced partially by bandpass filtering moving averaging and wiener filtering these classes of algorithms usually fail to remove abrupt spike like noise produced by head motion head motion artifacts can be minimized by ensuring rigid optode positioning solutions have been introduced that are based on helmets thermoplastic molded to the contours of each subject head spring loaded fibers attached to semi rigid plastic forms and fibers embedded in neoprene rubber forms background noise effects can also be attenuated by exploiting the strong statistical association between oxygenated and deoxygenated hemoglobin dynamics sensors although nirs is relatively new measurement modality nirs promises to be a potent neuroimaging modality for future applicability to bcis nirs provides now a low information transfer rate of about bits min but it would be increased in the future this neuroimaging modality might be a good alternative to eeg as neither conductive gel nor corrosive electrodes are required nevertheless communication speeds in nirs based bcis are limited due to the inherent delays of the hemodynamic response some studies have already demonstrated the feasibility of mental task detection through nirs derived optical responses control signal types in bcis the purpose of a bci is to interpret user intentions by means of monitoring cerebral activity brain signals involve numerous simultaneous phenomena related to cognitive tasks most of them are still incomprehensible and their origins are unknown however the physiological phenomena of some brain signals have been decoded in such way that people may learn to modulate them at will to enable the bci systems to interpret their intentions these signals are regarded as possible control signals in bcis numerous studies have described a vast group of brain signals that might serve as control signals in bci systems nevertheless only those control signals employed in current bci systems will be discussed below visual evoked potentials slow cortical potentials evoked potentials and sensorimotor rhythms all the signal controls are listed in table along with some of their main features table summary of control signals signal physiological phenomena number of choices training transfer information rate vep brain signal modulations in the visual cortex high no bits min scp slow voltages shift in the brain signals low or very difficult yes bits min positive peaks due to infrequent stimulus high no bits min sensorimotor rhythms modulations in sensorimotor rhythms synchronized to motor activities low yes bits min visual evoked potentials veps veps are brain activity modulations that occur in the visual cortex after receiving a visual stimulus these modulations are relatively easy to detect since the amplitude of veps increases enormously as the stimulus is moved closer to the central visual field veps may be classified according to three different criteria i by the morphology of the optical stimuli ii by the frequency of visual stimulation and iii by field stimulation according to the first criterion veps may be caused by using flash stimulation or using graphic patterns such as checkerboard lattice gate and random dot map according to the frequency veps can also be classified as transient veps tveps and as steady state veps ssveps tveps occur when the sensors frequency of visual stimulation is below hz while ssveps occur in reaction to stimuli of a higher frequency lastly according to the third criterion veps can be divided into whole field veps half field veps and part field veps depending on the area of on screen stimulus for instance if only half of the screen displays graphics the other half will not display any visual stimulation and the person will look at the centre of the screen which will induce a half field vep tveps can be elicited by any change in the visual field those used most frequently are tveps are i flash tveps that are caused by flashing lights ii pattern onset offset tveps that are caused by letting a pattern appear abruptly on a diffuse background and iii pattern reversal tveps that are caused by reversing the phase of a pattern i e a checkerboard lattice that changes the checks from black to white and from white to black abruptly the evoked responses vary with the stimulus presented flash tveps present a series of negative and positive peaks the most prominently peaks are negative and positive peaks at around ms and ms respectively pattern onset offset tveps have three main peaks positive ms negative ms and positive ms pattern reversal tveps usually present one negative peak at ms one positive peak at ms and one negative peak at ms ssveps are elicited by the same visual stimulus in this case the stimulus changes at a frequency higher than hz if the stimulus is a flash ssvep shows a sinusoidal like waveform the fundamental frequency of which is the same as the blinking frequency of the stimulus if the stimulus is a pattern the ssvep occurs at the reversal rate and at their harmonics in contrast to tvep constituent discrete frequency components of ssveps remain closely constant in amplitude and phase over long periods of time ssveps are less susceptible than tveps to artifacts produced by blinks and eye movements and to electromyographic noise contamination indeed tveps not are typically used for bci ssvep based bcis allow users to select a target by means of an eye gaze the user visually fixes attention on a target and the bci identifies the target through ssvep features analysis considering a bci as a communications channel ssvep based bcis can be classified into three categories depending on the specific stimulus sequence modulation in use time modulated vep t vep bcis frequency modulated vep f vep bcis and pseudorandom code modulated vep c vep bcis veps that react to different stimulus sequences should be orthogonal or near orthogonal to each other in some domain to ensure reliable identification of the target in a t vep bci the flash sequences of different targets are orthogonal in time that is the flash sequences for different targets are either strictly non overlapping or stochastic in an f vep bci each target is flashed at a unique frequency generating a periodic sequence of evoked responses with the same fundamental frequency as its harmonics in a c vep bci pseudo random sequences are used the duration of on and off states of each target flash is determined by a pseudorandom sequence signal modulations can optimize the information transfer rate indeed code modulation provides the highest communication speed table summarizes the features of each modulation sensors table features of vep modulations t vep f vep and c vep vep modulation features t vep relatively low information transfer rate bits min synchronous signal is necessary no user training required f vep high information transfer rate bits min simple system configuration no user training required more suitable for application with few options c vep very high information transfer rate bits min synchronous signal is necessary user training required more suitable for application with many options the typical vep based bci application displays flashing stimuli such as digits or letters on a screen to induce ssveps while the user stares at one of the symbols the user can move their gaze to the flashing digits or letters in order to communicate with the computer the advantage of this type of control signal is that very little training is required however it presents the drawback that the user has to watch the screen and keep his eyes fixed on one point this type of control signal can only be used for exogenous bcis see section therefore veps are not suitable for patients in advanced stages of amyotrophic lateral sclerosis als or with uncontrollable eye or neck movements some independent ssvep based bcis that are controlled by the attention of the user have been introduced to overcome this drawback ssvep are usually elicited through light emitting diodes leds cathode ray tube crt monitors or liquid crystal display lcd leds outperform lcd or crt stimulators but they need more complex hardware lcd and crt monitors make the target presentation easier than led stimulators because both systems can easily be connected to a pc however led stimulators may be preferable for a multiple target bci because the refresh rate of an lcd or crt monitor can limit the number of targets led stimulators offer more versatility because the flickering frequency and phase of each led can be controlled independently by a programmable logic device the stimulation decision can be made on the basis of the number of choices that the bci offers lcd screens are optimal for low complexity bci less than choices because they induce less eye tiredness than crt screens for medium complexity bci choices lcd or crt screens are optimal for high complexity bci more than commands led are preferred slow cortical potentials scps scps are slow voltage shifts in the eeg that last a second to several seconds scps belong to the part of the eeg signals below hz scps are associated with changes in the level of cortical activity negative scps correlate with increased neuronal activity whereas positive scps coincide with decreased activity in individual cells these brain signals can be self regulated by both healthy users and paralyzed patients to control external devices by means of a bci scp shifts can be used to move a cursor and select the targets presented on a computer screen sensors people can be trained to generate voluntary scp changes using a thought translation device the thought translation device is a tool used for self regulation scp training which shows visual auditory marks so that the user can learn to shift the scp the thought translation device typically comprises a cursor on a screen in such a way that the vertical position of the cursor constantly reflects the amplitude of scp shifts although most thought translation devices show continuous feedback it is possible to train scp self modulation in the absence of continuous feedback success in scp self regulation training depends on numerous factors such as the patient psychological and physical state motivation social context or the trainer patient relationship it is known that the learning capability of the user drastically affects scp modulation training self regulation training is therefore strongly recommended for patients at the early stage of a progressive disease furthermore initial scp modulation skills have an effect on future performance following training therefore the value of scps as a suitable control signal for each patient can only be determined on the basis of initial trials other factors such as sleep quality pain and mood also have an influence on self regulation performance their effects are not identical for all patients and further investigation is certainly needed to establish general rules on this matter self regulation of scps has been tested extensively with patients suffering from als typical accuracy rates achieved for scp classification are acceptable and vary between and per cent but the rates of information provided by scp based bci are relatively low besides longer training is required to use scp based bci and it is likely that users will need continuous practice for several months evoked potentials evoked potentials are positive peaks in the eeg due to infrequent auditory visual or somatosensory stimuli these endogenic responses are elicited about ms after attending to an oddball stimulus among several frequent stimuli some studies have proven that the less probable the stimulus the larger the amplitude of the response peak the use of based bcis does not require training however the performance may be reduced because the user gets used to the infrequent stimulus and consequently amplitude is decreased a typical application of a bci based on visual evoked potentials comprises a matrix of letters numbers or other symbols or commands the rows or columns of this matrix are flashed at random while the eeg is monitored the user gazes at the desired symbol and counts how many times the row or column containing the desired choice flashes is elicited only when the desired row or column flashes thus the bci uses this effect to determine the target symbol due to the low signal to noise ratio in eeg signals the detection of target symbols from a single trial is very difficult the rows or columns must be flashed several times for each choice the epochs corresponding to each row or column are averaged over the trials in order to improve their accuracy however these repetitions decrease the number of choices per minute e g with repetitions only two characters are spelled per minute although most of the applications based on evoked potentials employ visual stimuli auditory stimuli have been used for people with visual impairment based bcis provide a very low rate of information transmission because the classifier based on an average is too simple and the accuracy of potential detection is too low sensors consequently too many trials are required to select a single symbol in the matrix accuracy of based bcis can be improved while using a more complicated classifier than a simple average to ensure that the number of repetitions remain unaffected other studies have proven that the detection accuracy of visual evoked potentials also depends on the properties of the visual matrix such as the dimensions or colors of the symbols performance decreases when matrices with smaller symbols are used and it is enhanced when a green and blue chromatic flicker matrix is used rather than a gray and black one information transmission rates provided by based bci can be also improved by considering the bci as a noisy transmission system bci can therefore benefit from the use of error correcting codes however optimizing the code solely according to the maximal minimum hamming distance implies an increase in target frequency of target stimuli which might violate physiological constraints leading to difficulties in classifying the individual erps due to overlap and refractory effects further overlap and refractory effects are generally the main error source in these kinds of bcis some recent novel approaches have tried to reduce them by superimposing the targets on a checkerboard or by using alternative stimulus type methods based on motion the response is not markedly affected by whether or not the subject gazes directly at the target in contrast to the vep response which is larger when the target is foveated this distinction is important for clinical applications because eye movements are often impaired or lost in the target population nevertheless the performance of a based bci is substantially improved when subjects gaze at the desired item therefore the performance of the visual based bcis depends not only on the evoked potential but also on the vep response that in turn strongly depends on eye gaze direction sensorimotor rhythms mu and beta rhythms sensorimotor rhythms comprise mu and beta rhythms which are oscillations in the brain activity localized in the mu band hz also known as the rolandic band and beta band hz respectively both rhythms are associated in such a way that some beta rhythms are harmonic mu rhythms although some beta rhythms may also be independent the amplitude of the sensorimotor rhythms varies when cerebral activity is related to any motor task although actual movement is not required to modulate the amplitude of sensorimotor rhythms similar modulation patterns in the motor rhythms are produced as a result of mental rehearsal of a motor act without any overt motor output sensorimotor rhythms have been used to control bcis because people can learn to generate these modulations voluntarily in the sensorimotor rhythms sensorimotor rhythms can endure two kinds of amplitude modulations known as event related desynchronization erd and event related synchronization ers that are generated sensory stimulation motor behavior and mental imagery erd involves an amplitude suppression of the rhythm and ers implies amplitude enhancement figure left panel shows the temporal behavior of erd and ers during a voluntary movement experiment which involves brisk finger lifting the mu band erd starts before movement on set reaches the maximal erd shortly after movement onset and recovers its original level within a few seconds in contrast the beta rhythm shows a short erd during the movement initiation of movement followed by ers that reaches the sensors maximum after movement execution this ers occurs while the mu rhythm is still attenuated figure also shows the gamma oscillation hz which is another rhythm related to motor tasks as well gamma rhythms reveal an ers shortly before movement onset finally the right panel of figure illustrates that simultaneous erd and ers are possible at different scalp locations figure left panel superimposed band power time courses computed for three different frequency bands hz hz and hz from eeg trials recorded from electrode position during right index finger lifting eeg data triggered with respect to movement offset vertical line at t right panel examples of ongoing eeg recorded during right finger movement adapted from sensorimotor rhythms are related to motor imagery without any actual movement this makes it possible to use sensorimotor rhythms for the design of endogenous bcis which are more useful than exogenous bcis nevertheless self control of sensorimotor rhythms is not easy and most people have difficulties with motor imagery people tend to imagine visual images of related real movements which is not sufficiently useful for a bci system because the patterns of these sensorimotor rhythms differ from actual motor imagery user training should emphasize kinesthetic experiences instead of visual representations of actions motor imagery training is traditionally based on visual or auditory feedback this kind of training asks the users to perform a certain motor imagery task and then the sensorimotor rhythms are extracted and classified by comparing them with a reference finally visual or auditory feedback is provided to the participant according to the success of the result this kind of training has been widely used although usually its effectiveness was not very high hwang et al presented more effective motor imagery training based on a system that displayed real time cortical activity as feedback which allowed the users to watch their own cortical activity through a real time monitoring system sensorimotor rhythms have been investigated extensively in bci research well known bci systems such as wadsworth berlin or graz bcis employ sensorimotor rhythms as control signals the bcis based on sensorimotor rhythms can operate in either synchronous or asynchronous mode the latest advances in the field of bcis based on sensorimotor rhythms have shown that it is possible to predict human voluntary movements before they occur based on the modulations in sensorimotor rhythms furthermore this prediction could be provided without the user making any movements at all the bcis can be categorized into i exogenous or endogenous and ii synchronous cue paced or asynchronous self paced types of bci are listed in tables and along with information related to brain signals that can be modulated to convey information as well as advantages and disadvantages also bcis can be classified into dependent and independent this distinction will not be detailed in this review because it is very similar to exogenous and endogenous distinction advantages and disadvantages in both taxonomies are analogous table main differences between exogenous and endogenous bci approach brain signals advantages disadvantages exogenous bci minimal training control signal set up easily and quickly high bit rate bits min only one eeg channel required permanent attention to external stimuli may cause tiredness in some users endogenous bci ssvep very time consuming training months or weeks not all users are able to obtain control multichannel eeg recordings required for good performance lower bit rate bits min table main differences between synchronous and asynchronous bcis approach advantages disadvantages synchronous bci independent of any stimulation scps sensorimotor rhythms can be operated at free will useful for users with sensory organs affected suitable for cursor control applications simpler design and performance evaluation the user can avoid generating artifacts since they can perform blinks and other eye movements when brain signals are not analyzed does not offer a more natural mode of interaction asynchronous bci no requirement to wait for external cues offers a more natural mode of interaction much more complicate design more difficult evaluation according to the nature of the signals used as input bci systems can be classified as either exogenous or endogenous exogenous bci uses the neuron activity elicited in the brain by an external stimulus such as veps or auditory evoked potentials exogenous systems do not require extensive training since their control signals ssveps and can be easily and quickly set up besides the signal controls can be realized with only one eeg channel and can achieve a high information transfer rate of up to bits min on the other hand endogenous bci is based on self regulation of brain rhythms and potentials without external stimuli through neurofeedback training the users learn to generate specific brain patterns which may be decoded by the bci such as modulations in the sensorimotor rhythms or the scps the advantage of an endogenous bci is that the user can operate the bci at free will and move a cursor to any point in a sensors two dimensional space while an exogenous bci may constrain the user to the choices presented also endogenous bci are especially useful for users with advanced stages of als or whose sensory organs are affected table summarizes the differences between exogenous and endogenous bcis according to the input data processing modality bci systems can be classified as synchronous or asynchronous synchronous bcis analyze brain signals during predefined time windows any brain signal outside the predefined window is ignored therefore the user is only allowed to send commands during specific periods determined by the bci system for example the standard graz bci represents a synchronous bci system the advantage of a synchronous bci system is that the onset of mental activity is known in advance and associated with a specific cue moreover the patients may also perform blinks and other eye movements which would generate artifacts if the bci did not analyze the brain signals to avoid their misleading effects this simplifies the design and evaluation of synchronous bci asynchronous bcis continuously analyze brain signals no matter when the user acts they offer a more natural mode of human machine interaction than synchronous bci however asynchronous bcis are more computation demanding and complex table summarizes the differences between synchronous and asynchronous bcis features extraction and selection different thinking activities result in different patterns of brain signals bci is seen as a pattern recognition system that classifies each pattern into a class according to its features bci extracts some features from brain signals that reflect similarities to a certain class as well as differences from the rest of the classes the features are measured or derived from the properties of the signals which contain the discriminative information needed to distinguish their different types the design of a suitable set of features is a challenging issue the information of interest in brain signals is hidden in a highly noisy environment and brain signals comprise a large number of simultaneous sources a signal that may be of interest could be overlapped in time and space by multiple signals from different brain tasks for that reason in many cases it is not enough to use simple methods such as a band pass filter to extract the desired band power brain signals can be measured through multiples channels not all information provided by the measured channels is generally relevant for understanding the underlying phenomena of interest dimension reduction techniques such as principal component analysis or independent component analysis can be applied to reduce the dimension of the original data removing the irrelevant and redundant information computational costs are thereby reduced brain signals are inherently non stationary time information about when a certain feature occurs should be obtained some approaches divide the signals into short segments and the parameters can be estimated from each segment however the segment length affects the accuracy of estimated features fft performs very poorly with short data segments wavelet transform or adaptive autoregressive components are preferred to reveal the non stationary time variations of brain signals also a novel technique called stationary subspace analysis ssa has recently been introduced to deal with the non stationarity of eeg signals ssa decomposes multivariate time series into stationary and non stationary components sensors multiples features can be extracted from several channels and from several time segments before being concatenated into a single feature vector one of the major difficulties in bci design is choosing relevant features from the vast number of possible features high dimensional feature vectors are not desirable due to the curse of dimensionality in training classification algorithms see next section the feature selection may be attempted examining all possible subsets of the features however the number of possibilities grows exponentially making an exhaustive search impractical for even a moderate number of features some more efficient optimization algorithms can be applied with the aim of minimizing the number of features while maximizing the classification performance this section discusses methods to obtain the relevant characteristics of brain signals as well as feature selection methods firstly dimensional reduction methods such as principal component analysis or independent component analysis are explained secondly time and or frequency methods such as matched filtering or wavelet transform and parametric modeling such as autoregressive component are also surveyed thirdly an explanation is given of the common spatial pattern algorithm this method designs a preprocessing spatial filter by means of spatial covariance from input data and signal whitening that enhances the difference between classes before the feature extraction stage and finally feature selection methods such as genetic algorithms or sequential selection are included all these methods including feature extraction and feature selection methods are listed respectively in tables and along with information on their properties and bci applications table summary of feature extraction methods method properties applications dimension reduction linear transformation set of possibly correlated observations is transformed into a set of uncorrelated variables pca optimal representation of data in terms of minimal mean square error no guarantees always a good classification valuable noise and dimension reduction method pca requires that artifacts are uncorrelated with the eeg signal ica splits a set of mixed signals into its sources mutual statistical independence of underlying sources is assumed powerful and robust tool for artifact removal artifacts are required to be independent from the eeg signal may corrupt the power spectrum space csp spatial filter designed for class problems multiclass extensions exist good result for synchronous bcis less effective for asynchronous bcis its performance is affected by the spatial resolution some electrode locations offer more discriminative information for some specific brain activities than others improved versions of csp wcsp cssp csssp sensors table cont time frequency spectrum model ar high frequency resolution for short time segments not suitable for non stationary signals adaptive version of ar mvaar mf detects a specific pattern on the basis of its matches with predetermined known signals or templates suitable for detection of waveforms with consistent temporal characteristics cwt provides both frequency and temporal information suitable for non stationary signals dwt provides both frequency and temporal information suitable for non stationary signals reduces the redundancy and complexity of cwt table summary of feature extraction methods method properties applications features selection ga high resource consumption possible premature convergence sfs sbs suboptimal methods sffs sbfs modified versions of sfs sbs methods based on plus l take away r algorithm partially overcome the deficiencies of sfs sbs principal component analysis pca pca is a statistical features extraction method that uses a linear transformation to convert a set of observations possibly correlated into a set of uncorrelated variables called principal components linear transformation generates a set of components from the input data sorted according to their variance in such a way that the first principal component has the highest possible variance this variance allows pca to separate the brain signal into different components pca projects the input data on a k dimension eigenspace of k eigenvectors which are calculated from the covariance matrix of the training data p p p p n p i is i th d dimension training sample and n is the number of samples the covariance matrix is computed as where is the mean vector of the training samples p i the covariance matrix is a real and symmetric matrix therefore has d different eigenvectors and eigenvalues by means of the eigenvalues it is possible to know which eigenvectors represent the most significant information contained in the dataset the eigenvectors with the highest sensors eigenvalue represent the principal components of the training dataset p pca selects that k with k d eigenvectors having the largest eigenvalues these selected eigenvectors serve to build a projection matrix a that will be used to extract the feature vector from the test data q the k eigenvectors are sorted into columns in matrix a such that the first column of a corresponds to the largest eigenvalue finally pca computes the feature vector v from the data in matrix a by projecting the test data q onto the new subspace such that where represents the mean vector of training samples p i pca is also a procedure to reduce the dimension of the feature since the number of columns is less than the number of eigenvectors the dimension of the output projected data is less than the dimension of the input data this decrease in dimensionality can reduce the complexity of the subsequent classifying step in a bci system pca does not always guarantee a good classification since the best discriminating components may not figure among the largest principal components pca reduces data dimension by seeking a new optimal representation of data in terms of minimal mean square error between the representation and the original data it will not guarantee that the discriminative features are optimal for classification despite this shortcoming it has been proven that pca is a reliable noise reduction method with regard to the applications of pca in bci systems pca has been used to identify the artifactual components in a reasonably successful way in eeg signals and to reconstruct the signals without the artifactual components nevertheless the artifacts must not be correlated with the eeg signal for pca to function in this way pca has also been employed in order to reduce feature space dimensionality independent component analysis ica ica is a statistical procedure that splits a set of mixed signals into its sources with no previous information on the nature of the signal the only assumption involved in ica is that the unknown underlying sources are mutually independent in statistical terms ica assumes that the observed eeg signal is a mixture of several independent source signals coming from multiple cognitive activities or artifacts ica therefore expresses the resulting eeg signal x t in relation to their sources t as where f is any unknown mixer function and n t is an additive random noisy vector the dimension of the input vector t depends on the number of sources the dimension of output vector x t is equal to the number of measured data channels the number of sources is usually assumed to be less than or equal to the number of channels although more generalized ica methods are possible the whole ica problem consists in the calculation of the unmixing function by inverting f and obtaining an estimation of t by mapping x t to the source space to solve the problem ica can fall into two different models on the basis of f which may be either a linear or nonlinear function the nonlinear assumption is suitable in those cases where the linear model might be too simple to describe the observed data x t however the nonlinear problem is usually too complex and generally intractable due to its high number of indeterminations the assumption of a linear mixing function sensors simplifies equation it is possible to rewrite it as a matrix multiplication where a is the mixing matrix the equation gives the mathematical expression of the linear ica model although the approximation given by equation can be considered too simple it works reasonably well in brain signal processing applications furthermore it is possible to remove the noise term n t from equation by assuming that the observed data is noiseless or that the noise is too weak for consideration finally t and a are obtained from x t by means of certain algorithms such as infomax or further modification of the infomax ica has traditionally been used as a preprocessing tool before the feature extraction step in order to remove ocular artifacts in bci systems although ica has been proven to be a powerful and robust tool for artifact removal in signal analysis some studies have indicated that artifact suppression may also corrupt the power spectrum of the underlying neural activity in addition ica requires that the artifacts are independent in relation to the eeg signal it is also possible to find authors that have employed ica as a classifier ica can be modified to classify eeg signals by fitting the generative ica model to each task and employing bayes rule to create the classifier autoregressive components ar ar spectral estimation is a method for modeling signals ar models the eeg signal as the output random signal of a linear time invariant filter where the input is white noise with a mean of zero and a certain variance of the aim of the ar procedure is to obtain the filter coefficients since it is assumed that different thinking activities will produce different filter coefficients the filter coefficients will be used as the features of the signal ar assumes that the transfer function of the filter will only contain poles in the denominator the number of poles in the denominator corresponds to the order of the autoregressive model the assumption of an all pole filter makes the filter coefficients computation easier because it is only necessary to solve linear equations mathematically the ar model of order p describes the eeg signal y t as where a i is the i th filter coefficient and n t is the noise there are several methods that compute the filter coefficients such as the yule walker burg covariance and forward backward algorithms the resulting coefficients can be used to estimate the power spectrum of the eeg signal y ω such that where a k are the estimated filter coefficients and p is the ar model order in other words the number of poles in the ar model the determination of an appropriate order p for a given input signal is a trade off issue if the order is too low to model the input signal the result will not faithfully represent the signal sensors because the spectrum is too smooth in contrast if the order is too high the spectrum may exhibit spurious peaks ar spectral estimation is preferred to fourier transform because of its superior resolution for short time segments nevertheless ar performs poorly when the signal is not stationary due to the non stationary nature of eeg signals a multivariate adaptive ar mvaar model has been proposed to design more effective on line bci systems jiang et al applied mvaar for the classification of motor imagery showing that mvaar is a valuable adaptive method for feature extraction the computation algorithm was very similar to the original ar model in a bci with m channels the vector of m eeg values at each point in time k was represented as as in the ar case the mvaar model was expressed as where was the vector of white noise values were the adaptive coefficients and p was the model order the recursive least squares algorithm a special variant of the kalman filter were used to update coefficients at every point k matched filtering mf mf is a feature extraction method that attempts to detect a specific pattern on the basis of its matches with predetermined known signals or templates the intention of the user is revealed by means of the correlation between the unknown eeg signals and the set of templates each template represents an intention of the user a higher correlation would imply better matching between the template and the user intention each matched filter can simply be modeled as a sum of the harmonically related sinusoidal components cos where n is the template sample number f is the sampling frequency f f is the fundamental frequency of the rhythm template n is the number of harmonics and a t and are the amplitude and phase of the individual harmonics respectively the model parameters a t and can be obtained from the fft spectrum mf has been proven especially effective for the detection of waveforms with consistent temporal characteristics krusienski et al used mf for the identification of user intentions through μ rhythms and brunner et al also used it for ssvep feature extraction wavelet transform wt wt is a mathematical tool widely used for extracting information from many different kinds of data such as audio or image data among others wt is particularly suitable when signals are not stationary because it provides a flexible way of representing the time frequency of a signal sensors wavelets are functions of varying frequency and limited duration that allow simultaneous study of the signal in both the time and the frequency domain in contrast to other modalities of signal analysis such as fourier transform ft ft provides only an analysis of the signal activity in the frequency domain ft gives information about the frequency content but it is not accompanied by information on when those frequencies occur short term fourier transform stft was proposed to overcome this shortcoming of the fourier analysis the stft divides the signal into successive time windows and applies the ft in each epoch of the signal in time in this approach the design of window length is a trade off because smaller windows lead to higher temporal resolution but also to lower frequency resolution at the same time the wt overcomes this drawback by decomposing the signal in both the time and the frequency domain at multiple resolutions by using a modulated window that is shifted along the signal at various scales continuous wavelet transform cwt is defined as the convolution of the signal with the wavelet function is the wavelet coefficient that corresponds to the frequency associated with the scale and the time τ of the wavelet function and the symbol expresses the complex conjugation the wavelet function is a dilated and shifted version of a mother wavelet a mother wavelet can take multiples shapes but it always satisfies the next condition the cwt defined in the equation is actually a kind of template matching similar to a matched filter in which the cross variance between the signal and a predefined waveform is calculated the advantage of the cwt over classic template matching methods arises from the special properties of the wavelet template the wavelets are suitable for transient signal analysis in which the spectral properties of the signal vary over time wt is a powerful tool for the decomposition of transient brain signals into their constituent parts based on a combination of criteria such as frequency and temporal position signals of identical frequency ranges can be distinguished by means of the temporal position likewise it is possible to separate temporally overlapping processes thanks to the different frequency content the cwt introduces a lot of redundancy and complexity since it involves the analysis of a signal at a very high number of frequencies using multiple dilations and shifting of the mother wavelet discrete wavelet transform dwt was introduced to reduce this redundancy and complexity the dwt translates and dilates the mother wavelet in certain discrete values only farina et al showed a pattern recognition approach for the classification of single trial movement related cortical potentials where the feature space is built from coefficients of a discrete wavelet transformation although dwt is less redundant and less complex than cwt cwt is still employed to extract sensors features from and scp because it can clarify subtle information that dwt is unable to extract the use of wt requires the selection of a mother wavelet many different mother wavelets can be found in bci applications and the selection of any one depends on what types of features need to be extracted from the signal the mexican hat wavelet is well localized in the time domain and is employed for the localization of erp components in time the morlet wavelet is well localized in the frequency domain and has been used for the analysis of gamma activity the bi scale wavelet has been employed successfully for designing an asynchronous bci based on detection of imaginary movement in the hz frequency range also the daubechies wavelet a very well known mother wavelet has been used for the classification of scps common spatial pattern csp csp is a feature extraction method that projects multichannel eeg signals into a subspace where the differences between classes are highlighted and the similarities are minimized it aims to make the subsequent classification much more effective by designing a spatial filter that transforms the input data into output data with an optimal variance for the subsequent discrimination csp has been designed for the analysis of multichannel data belonging to class problems nevertheless some extensions for multiclass bcis have also been proposed csp calculates the normalized spatial covariance c from the input data e which represents the raw data of a single trial by means of where e is an matrix in which t is the number of channels i e recording electrodes and n the number of samples per channel the apostrophe denotes the transpose operator while trace x is the sum of the diagonal elements of x assuming csp is used to classify two classes e g left and right motor imagery csp calculates the spatial covariances and for each of the two classes by averaging the covariances over the successive training trials of each class over time the composite spatial covariance is computed as since is real and symmetric it can be factored as where is the matrix of eigenvectors and is the diagonal matrix of eigenvalues by means of the whitening transform the variances are equalized in the space spanned by and all eigenvalues of are equal to one if and are transformed as then and will share common eigenvectors if then and where is the identity matrix as a result of the sum of two corresponding eigenvalues being always sensors one the eigenvectors with the largest eigenvalues for correspond to the smallest eigenvalue for and vice versa this property is very useful for subsequent classification because the variance of the signal is maximized for one class while minimized for the other class finally the feature vector z is obtained from the trial e as where is the spatial filter matrix built by the csp procedure csp increases the accuracy of synchronous bcis where it is allowed to send signals only during certain predefined time periods however csp does not offer the same improvement in asynchronous bcis this is mainly due to the nonstationary properties of eeg signals also the performance of csp is affected by the spatial resolution and it has been proven that some electrode locations offer more discriminative information for some specific brain activities than others for these reasons several methods improving the original csp method have been proposed to increase the performance wavelet common spatial pattern wcsp common spatio spectral pattern cssp and common sparse spectral spatial pattern csssp genetic algorithm ga ga is an optimization procedure to establish whether a certain set of features is the most efficient ga has been used in very diverse fields to solve optimization problems in bci research ga has been used as an automatic method to extract an optimal set of relevant features the baseline of the algorithm is a population of candidate solutions called individuals creatures or phenotypes which are encoded by strings named chromosomes or the genotype of the genome these strings are coded either by binary information or no binary information the standard steps of the ga can be explained briefly as follows figure ga begins with an initial population which is randomly generated unless the algorithm has previous of the final solution in the case of having initial information the initial population may be directed towards areas where optimal solutions are more likely to reduce the number of iterations the fitness of every individual population is evaluated according to their fitness some representatives of the population may be discarded to vacate space for newly generated individuals other individuals may be selected as parents in order to breed new individuals also some individuals may be stochastically selected to keep diversity in the population preventing premature convergence after the selection step the individuals are crossed with each other in the crossover step mating is performed among the selected parents to generate one or more offspring to keep a fixed population size the number of offspring is usually the same as the number of discarded individuals the parents genes are split into pieces and then combined to form new offspring following the crossover step mutations are introduced to alter the population in order to avoid converging towards a local suboptimum solution before exploring the entire search space as a result of the mutation it is possible to discover areas that cannot be explored by crossover finally the fitness of the new population is evaluated when an acceptable solution is reached or the maximum number of generations has been produced the algorithm is terminated otherwise another iteration of the algorithm is produced sensors figure genetic algorithm start initial population selection evaluation crossover optimum population mutation or maximum number of iterations yes end sequential selection sequential selection is an optimization approach that aims at finding the optimal subset of features by adding or removing features sequentially there are two algorithms that perform sequential selection sequential forward selection and sequential backward selection sequential forward selection sfs is a bottom up algorithm firstly the best individual feature is found as the first feature in the subset next for each subsequent step the algorithm chooses the feature from the remaining set which in combination with the previously selected features yields the best subset of features finally the algorithm finishes when the required number of features is reached the shortcoming of this algorithm is that the superfluous features are not removed once other features are added sequential backward selection sbs in contrast to sfs is a top down process the process starts with the entire set of features and removes step by step features in such a way that the error is as low as possible this algorithm is also suboptimal because it discards some features that may be helpful after discarding other features sfs has been used with success in the field of bcis another refined method is introduced to partially overcome the aforementioned deficiencies this method known as plus l take away r method l r adds l features and remove r features that is not working well with other selected features sequential forward floating search sffs or sequential backward floating search sbfs are based on the plus l take away r method sffs starts with a null feature set and for each step the r best features are included in the current feature set in other words r steps of sfs are performed next the algorithm verifies the possibility that some feature may be excluded then l worst features are eliminated from the set in other words l steps of sbs sffs increases and decreases the number of features until the desired number of features is reached sbfs works analogously but starting with the full feature set and performing the search until the desired dimension is reached using sbs and sfs steps in bci research sffs has been used to reduce the dimensionality of the feature space to an appropriate size for the available training data no sensors artifacts in bcis artifacts are undesirable signals that contaminate brain activity and are mostly of non cerebral origin since the shape of neurological phenomenon is affected artifacts may reduce the performance of bci based systems artifacts may be classified into two major categories physiological artifacts and non physiological or technical artifacts physiological artifacts are usually due to muscular ocular and heart activity known as electromyography emg electrooculography eog and electrocardiography ecg artifacts respectively emg artifacts which imply typically large disturbances in brain signals come from electrical activity caused by muscle contractions which occur when patients are talking chewing or swallowing eog artifacts are produced by blinking and other eye movements blinking makes generally high amplitude patterns over brain signals in contrast to eye movements which produce low frequency patterns these electrical patterns are due to the potential difference between the cornea and the retina as their respective charges are positive and negative for that reason the electric field around the eye changes when this dipole moves eog artifacts mostly affect the frontal area because they are approximately attenuated according to the square of the distance finally ecg artifacts which reflect heart activity introduce a rhythmic signal into brain activity technical artifacts are mainly attributed to power line noises or changes in electrode impedances which can usually be avoided by proper filtering or shielding therefore the bci community focuses principally on physiological artifacts given that their reduction during brain activity acquisition is a much more challenging issue than non physiological artifact handling several ways of handling physiological artifacts can be found in the literature artifacts may be avoided rejected or removed from recordings of brain signals artifact avoidance involves asking patients to avoid blinking or moving their body during the experiments this approach to artifact handling is very simple because it does not require any computation as brain signals are not assumed to have artifacts however this assumption is not always feasible given that some artifacts involuntary heart beats eye and bodily twitches are not easily avoidable during data recording especially in cases of strong neurological disorders artifact rejection approaches suggest discarding the epochs contaminated by the artifacts manual artifact rejection is an option to remove artifacts in brain signals and an expert could identify and eliminate all artifact contaminated epochs the main disadvantage in using manual rejection is that it requires intensive human labor so this approach is not suitable for on line bci systems nevertheless this task can be performed automatically by emg and eog artifact detection if emg and eog signals are monitored the brain signal samples may be removed whenever ocular or muscular activity of the arms is detected automatic rejection is an effective way of artifact handling but it may fail when eog amplitudes are too small besides rejection methodology means that the user loses device control when artifact contaminated signals are discarded instead of rejecting samples the artifact removal approach attempts to identify and remove artifacts while keeping the neurological phenomenon intact common methods for removing artifacts in eeg are linear filtering linear combination and regression bss and pca some of which were discussed in section instead of avoided rejected or removed artifacts from recordings of brain signals some systems acquire and process artifacts to offer a communication path that either disabled or healthy people can sensors use in many tasks and in different environments this kind of system is not considered a bci because communication is not independent of peripheral nerves and muscles emg computer interface human computer interface hci emg based human computer interface emg based human machine interface emg based human robot interface muscle computer interface muci man machine interface mmi and biocontroller interface are different terms used to name communication interfaces in the scientific literature that can employ artifact signals among others these systems usually have greater reliability than bcis but they cannot be used by severely disabled people with strong constraints in voluntary movements classification algorithms the aim of the classification step in a bci system is recognition of a user intentions on the basis of a feature vector that characterizes the brain activity provided by the feature step either regression or classification algorithms can be used to achieve this goal but using classification algorithms is currently the most popular approach regression algorithms employ the features extracted from eeg signals as independent variables to predict user intentions in contrast classification algorithms use the features extracted as independent variables to define boundaries between the different targets in feature space mcfarland et al illustrated the differences between the two alternatives for a two target case both the regression approach and the classification approach require the parameters of a single function to be determined in a four target case assuming that the targets are distributed linearly the regression approach still requires only a single function in contrast the classification approach requires the determination of three functions one for each of the three boundaries between the four targets therefore the classification approach might be more useful for two target applications and the regression approach may be preferable for greater numbers of targets when these targets can be ordered along one or more dimensions moreover the regression approach is better for continuous feedback e g applications which involve continuous control of cursor movement figure illustrates the differences between classification and regression approaches figure classification and regression approaches to bci control of two targets adapted from the regression algorithms employ the features extracted from eeg signals as independent variables to predict user intentions in contrast the classification approach uses the features extracted as independent variables to define boundaries between the different targets in feature space classification regression sensors classification algorithms can be developed via either offline online or both kinds of sessions the offline session involves the examination of data sets such as bci competitions data sets which are collected from an adaptive or closed loop system the statistics of the data may be estimated from observations across entire sessions and long term computations may be performed the results can be reviewed by the analyst with the aim of fine tuning the algorithms offline data analysis is valuable but it does not address real time issues in contrast online sessions provide a means of bci system evaluation in a real world environment the data are processed in a causal manner and the algorithms are tested in an environment in which the users change over the time as a result of e g changes in motivation or fatigue although some researchers test new algorithms with only offline data both offline simulation and online experiments are necessary for effective algorithm design in closed loop systems in other words offline simulation and cross validation can be valuable methods to develop and test new algorithms but only online analysis can yield solid evidence of bci system performance classification algorithms have traditionally been calibrated by users through supervised learning using a labeled data set it is assumed that the classifier is able to detect the patterns of the brain signal recorded in online sessions with feedback however this assumption results in a reduction in the performance of bci systems because the brain signals are inherently non stationary in this regard shenoy et al described two main sources of non stationarity on the one hand the patterns observed in the experimental samples during calibration sessions may be different from those recorded during the online session on the other hand progressive mental training of the users or even changes in concentration attentiveness or motivation may affect the brain signals therefore adaptive algorithms are essential for improving bci accuracy adaptation to non stationary signals is particularly necessary in asynchronous and non invasive bcis apart from the fact that supervised learning is not optimal for non stationary signals classification large data sets and thus long initial calibration sessions are usually required to achieve acceptable accuracy semi supervised learning has been suggested to reduce training time and to update the classifier in the online session on a continuous basis in semi supervised learning the classifier is initially trained using a small labeled data set after which the classifier is updated with on line test data in a realistic bci scenario the signal associated with the subject intentions is not usually known and the labels are not available either unsupervised learning or reinforcement learning can be applied for bci adaptation when the labeled data set is not available unsupervised methods attempt to find hidden structures in unlabeled data in order to classify them some unsupervised methods rely on techniques for co adaptive learning of user and machine or covariate shift adaptation reinforcement learning methods are based on the fact that distinguishing eeg potentials are elicited when a subject is aware of an erroneous decision these potentials are used as learning signals to prevent that error from being repeated in the future the adaptation generally results in enhanced performance nevertheless it is worth highlighting that inherent risks exist in an adaptive bci a bci that learns too fast may confuse the user because training will take place in a changing environment in addition adaptive procedures can hide some relevant signal features accordingly there is a tradeoff between highly sensitive adaptation and feature extraction sensors classifiers also have to face two main problems related to the pattern recognition task the curse of dimensionality and the bias variance tradeoff the curse of dimensionality means that the number of training data needed to offer good results increases exponentially with the dimensionality of the feature vector unfortunately the available training sets are usually small in bci research because training process takes a long time and is a tiring process for users the bias variance tradeoff represents the natural trend of the classifiers towards a high bias with low variance and vice versa stable classifiers are characterized by high bias with low variance while unstable classifiers show high variance with low bias to achieve the lowest classification error bias and variance should be low simultaneously a set of stabilization techniques such as the combination of classifiers or regularization can be used to reduce the variance the design of the classification step involves the choice of one or several classification algorithms from many alternatives several classification algorithms have been proposed such as k nearest neighbor classifiers linear classifiers support vector machines and neural networks among others the general trend prefers simple algorithms to complex alternatives simple algorithms have an inherent advantage because their adaptation to the features of the brain signal is inherently simpler and more effective than for more complex algorithms nevertheless simple algorithms whenever outperformed in online and offline evaluations should be replaced by more complex alternatives table summary of classification methods approach properties applications generative model assigns the observed feature vector to the labeled class to which it has bayesian the highest probability of belonging analysis produces nonlinear decision boundaries not very popular in the bci systems linear simple classifier with acceptable accuracy lda low computation requirements fails in the presence of outliers or strong noise regularization required usually two class extended multiclass version exits improved lda versions blda flda svm linear and non linear gaussian modalities binary or multiclass method maximizes the distance between the nearest training samples and the hyperplanes fails in the presence of outliers or strong noise regularization required speedy classifier non linear uses metric distances between the test feature and their neighbors k nnc multiclass efficient with low dimensional feature vectors very sensitive to the dimensionality of the feature vectors ann very flexible classifier multiclass multiple architectures pnn fuzzy artmap ann firnn pegnc sensors finally certain inherent dangers of classification algorithm usage should be pointed out although classification algorithms have clearly helped to characterize task relevant brain states several pitfalls may occur when these algorithms are used by non experts bias and variance of the estimated error of the algorithms and their overfitting are the main source of difficulties if a classifier is overfitted then it will only be able to classify the training data or similar data overfitting can be avoided by restricting the complexity of the classification procedure classification error is estimated by means of cross validation once a classification algorithm is trained the algorithm is validated on a validation data set which should be independent of the training data set this procedure is usually repeated several times using different partitions of the sample data the resulting validation errors are averaged across multiple rounds this approach presents some inherent dangers that must be prevented because some elements of the partition may not be independent of each other or may not be identically distributed among other reasons next this section presents the properties of a set of classifiers in order to make it easier to choose an appropriate classifier for a given type of bci all classifier methods are listed in table along with their main properties k nearest neighbor classifier k nnc k nearest neighbor classifiers k nnc are based on the principle that the features corresponding to the different classes will usually form separate clusters in the feature space while the close neighbors belong to the same class this classifier takes k metric distances into account between the test samples features and those of the nearest classes in order to classify a test feature vector the metric distances are a measure of the similarities between the features of the test vector and the features of each class the advantage of taking k neighbors into account in the classification is that error probability in the decision is decreased some training samples may be affected by noise and artifacts which may seriously influence the classification results if a decision involving several neighbors is made then it is less likely that an error will occur because the probability of several simultaneous erroneous datum is much lower rather than only the closest sample if several k closest classes are considered then a voting scheme is required to decide between competing choices since there are no reasons to assume that the distributions of those neighbors are homogenous it is clear to see that the k nnc has to assign different ranks to the nearest neighbors according to their distances from the test example therefore k nnc needs to define a weighting function which varies with the distance in such a way that the output value decreases as the distance between the test feature vector and the neighbor increases the function defined by equation meets this requirement where denotes the distance of the i th nearest neighbor from a test example that is corresponds to the nearest neighbor and to the furthest the decision rule of k nnc assigns the unknown examples to the class with the greatest sum of weights among its k nearest neighbors k nnc is not very common in bci research because this classifier is very sensitive to the dimensionality of the feature vector nevertheless k nnc has been proven to be efficient with sensors low dimension feature vectors also k nnc has been tested in a multiclass environment and applied to cursor movements on a vertical axis when classifying scps linear discriminant analysis lda lda is a very simple classifier that provides acceptable accuracy without high computation requirements lda is very common in the bci community and is a good choice for designing online bci systems with a rapid response but limited computational resources lda provides relatively acceptable accuracy and has been used successfully in numerous bci systems such as speller multiclass or synchronous bcis nevertheless it can lead to completely erroneous classifications in the presence of outliers or strong noise lda is usually applied to classify patterns into two classes although it is possible to extend the method to multiples classes for a two class problem lda assumes that the two classes are linearly separable according to this assumption lda defines a linear discrimination function which represents a hyperplane in the feature space in order to distinguish the classes the class to which the feature vector belongs will depend on the side of the plane where the vector is found figure in the case of an n class problem n several hyperplanes are used the decision plane can be represented mathematically as where w is known as the weight vector x is the input feature vector and w is a threshold the input feature vector is assigned to one class or the other on the basis of the sign of figure linear classifier and margins the decision boundary is the thick line adapted from there are many methods to compute w for example w may be calculated as μ μ where μ i is the estimated mean of class i and is the estimated common covariance matrix the average of the two class empirical covariance matrices the estimators of the covariance matrix and of the mean are calculated as μ μ sensors μ where is a matrix containing n feature vectors figure eigenvalue spectrum of a given covariance matrix bold line and eigenvalue spectra of covariance matrices estimated from a finite number of samples n note that accuracy increases as the number of trials increase adapted from real n n n r o t c e v n e ig e n index of eigenvalue the estimation of the covariance defined in equation is unbiased and has good properties under usual conditions nevertheless it may become imprecise in some cases where the dimensionality of the features is too high compared to the number of available trials the estimated covariance matrix is different from the true covariance matrix because the large eigenvalues of the original covariance matrix are over estimated and the small eigenvalues are under estimated figure it leads to a systematic error which degrades lda performance for this reason a new procedure has been proposed to estimate the covariance improving the standard estimator defined in the equation the new standard estimator of the covariance matrix is given by the γ value is referred to as a shrinkage parameter and is tunable between and ν is defined as with d being the dimensionality of the features space the selection of a shrinkage parameter implies a trade off and is estimated on the basis of the input data some improved algorithms have been introduced based on lda such as fisher lda flda and bayesian lda blda in the first example performance was improved by projecting the data to a lower dimensional space in order to achieve larger intervals between the projected classes and simultaneously to reduce the variability of the data in each class however flda does not work well sensors when the number of features becomes too large in relation to the number of training examples this is known as the small sample size problem the second modification can be seen as an extension of flda blda solves the small sample size problem by introducing a statistical method known as regularization the regularization is estimated through bayesian analysis of training data and is used to prevent overfitting of high dimensional and possibly noisy datasets overfitting means the classifier has lost generality and is therefore undesirable in a classifier if a classifier is overfitted then it is only able to classify the training data or similar data in comparison to flda the blda algorithm provides higher classification accuracy and bitrates especially in those cases where the number of features is large additionally blda requires only slightly more computation time which is a crucial requirement in real bci systems support vector machine svm svm is a classifier that in a similar way to lda classifiers constructs a hyperplane or set of hyperplanes in order to separate the feature vectors into several classes however in contrast to lda svm selects the hyperplanes that maximize the margins that is the distance between the nearest training samples and the hyperplanes the basis of svm is to map data into a high dimensional space and find a separating hyperplane with the maximal margin according to cover theorem on the separability of patterns cover theorem states that a complex classification problem cast in a high dimensional nonlinear space is more likely to be linearly separable than in a low dimensional nonlinear space also as for linear analysis classifier an svm uses regularization in order to prevent the classifier from accommodating possibly noisy datasets svm has been used to classify feature vectors for binary and multiclass problems it has also been successfully used in a large number of synchronous bcis such a classifier is regarded as a linear classifier since it uses one or several hyperplanes nevertheless it is also possible to create a svm with non linear decision boundary by means of a kernel function k x y non linear svm leads to a more flexible decision boundary in the data space which may increase classification accuracy the kernel that is usually used in the bci field is the gaussian or radial basis function rbf exp the gaussian svm has been applied in bcis to classify evoked potentials svm has been widely used in bci because it is a simple classifier that performs well and is robust with regard to the curse of dimensionality which means a large training set is not required for good results even with very high dimensional feature vectors these advantages come at the expense of execution speed nevertheless svm is speedy enough for real time bcis bayesian statistical classifier bayesian statistical classifier is a classifier which aims to assign with the highest probability an observed feature vector x from its class y the bayes rule is used to obtain the a posteriori probability that a feature vector has of belonging to a given class assuming for example two classes l sensors and r corresponding to imaginary left and right movements of the hand the a posteriori probabilities of each class are computed using the bayes rule as typically it is assumed that the a priori probabilities are equal p y p l p r since it is supposed the user has no predilection for any movement in order to calculate the probabilities it is usually supposed that a gaussian statistical distribution applies to the features for each class although it may also be assumed that the distribution is a weighted mixture of gaussian distributions where w i is the weight of each gaussian prototype and m is the number of prototypes two ways are feasible to estimate the gaussian prototypes mixture the first is to divide the feature space in several equally sized regions and calculate the mean and variance of the gaussian prototypes in each area from training data the set of gaussian prototypes is equally weighted and the weights w i are equal to the second uses a gaussian mixture models gmm the different weights w i and the mean variance and covariance matrices that define each gaussian prototype are calculated by the expectation maximization em algorithm em algorithm is an iterative procedure which guarantees the maximum likelihood or maximum a posteriori map estimates of the parameters in the statistical model lui et al made gmm adaptive to significant changes in the statistical distribution of the data during long term use in these improvements the initial mean variance and covariance of each class is updated over time using a specific number of recent trials bayesian statistical classifiers are not very popular in the bci community nevertheless they have been used for classifying motor imagery or visual evoked potentials artificial neural network ann anns are non linear classifiers that have been used in many applications in a wide variety of disciplines such as computer science physics and neuroscience the idea of anns is inspired in how the brain processes the information the purpose is to mimic brain activity that immediately solves certain problems which a conventional computer program processes poorly for example anns are widely used in pattern recognition because they are capable of learning from training data the ability to learn from examples is one of most important properties of anns once trained the anns are capable of recognizing a set of training data related patterns anns are therefore associated with bci applications since pattern recognition is performed to ascertain user intentions an ann comprises a set of nodes and connections that are modified during the training process the ann is fed on a set of training examples and the output is observed if the output is incorrect then the internal weights are modified by the training algorithm to minimize the difference between desired and actual output this training continues until the network reaches a steady state where no further sensors significant improvement is achieved in this state not only should the ann produce correct outputs for all examples of the training set but also for inputs that were not encountered during training from a mathematical point of view anns define a mapping from an input space to an output space that can be described as a vector valued function where both x and y may be of any dimensionality the mapping function f is a combination of mappings which are individually performed by single nodes or neurons each neuron processes the information non linearly and the resulting mapping is therefore non linear this property is important especially in those cases where the physical mechanism that generates the input signal is non lineal one of the most well known ann structures is the multilayer perceptron mlp introduced by rumelhart and mcclelland in mlps are very flexible classifiers that can classify any number of classes and adapt to numerous kinds of problems in the field of bcis mlp have been applied to classify two three and five different tasks and to design synchronous and asynchronous bcis moreover mlp has been used for preprocessing eeg signals before the feature extraction step rather than the classification step in order to improve the separability of eeg features besides mlp different types of ann architecture have been used in the design of bci systems such as probabilistic neural networks pnn fuzzy artmap neural networks finite impulse response neural networks firnn or probability estimating guarded neural classifiers pegnc bci applications bcis offer their users new communication and control channels without any intervention of peripheral nerves and muscles hence many researchers focus on building bci applications in the hope that this technology could be helpful for those with severe motor disabilities various bci applications have very recently been developed thanks to significant advances in the field of eeg based bci eeg signals are used by most bci applications because they offer an acceptable signal quality that combines low cost and easy to use equipment thanks to bci applications it is hoped that the quality of life of severely disabled people can be improved likewise the attention given by caregivers will be less intensive reducing its costs and making the life of relatives less onerous moreover bci applications potentially represent a powerful tool for revealing hidden information in the user brain that cannot be expressed the main target populations for bci applications fall into three classes the first group includes complete locked in state clis patients who have lost all motor control because they may be at a terminal stage of als or suffer severe cerebral palsy the second group comprises locked in state lis patients who are almost completely paralyzed but with residual voluntary movement such as eye movement eye blinks or twitches with the lip the third group of potential bci users includes abled bodied people and those with substantial neuromuscular control particularly speech and or hand control bci have little to offer to the third group because they can send the same information much more quickly and easily via other interfaces rather than a bci despite this bcis are increasingly used by healthy people in neuromarketing and video games as a tool to reveal affective information of sensors the users which cannot be so easily reported through conventional interfaces likewise bci can be used for some people that suffer from neurological disorders such as schizophrenia or depression the level of impairment of the potential target population is related to the performance of a bci system kübler et al reported a strong correlation between physical impairment and bci performance clis patients were unable to control a bci voluntary brain regulation for communication was only possible in lis patients however considering only lis patients this relationship between physical impairment and bci performance disappeared figure shows the relationship between bci application areas and bci information transfer rates and user capabilities figure relationship between bci application areas bci information transfer rates and user capabilities horizontal axis information transfer rate that would make the application controllable vertical axis the degree of capability e i t i healthy users and non severely disabled l i b a p people entertainment a c lis patients neuroprosthesis r e u locomotion environmental communication control clis patients information transfer rate bci it is currently unclear whether bci technology will ever outperform other established technologies that include eye or muscle based devices currently the latter devices tend to be easier to use and offer better benefit cost ratios for example the detection of eye movement is quicker easier and more accurate than the detection of erp modulations a spelling rate of words per minute can be obtained with unimpaired eye movement by means of an eyetracker in that regard hybrid bci systems have been proposed to improve performance they are the combination of two different kinds of bcis or the combination a bci with other existing assistive technology unless the performance of bci systems improves considerably bci as assistive technology may only be especially attractive for severely disabled people when other technologies are unsuitable at present lis patients and those likely to develop clis constitute the principal candidates for bci despite the low information transfer rates provided by bci the high grade of disability among lis patients force them to use a bci rather than more reliable conventional interfaces such as muscle or eye gaze based system eye gaze control constraints in some lis patients are an important issue because they are obliged to use bcis that does not depend on eye gaze control also eye gaze control constraints make some bci applications more difficult such as steering a wheelchair nowadays there are a vast number of very different bci applications such as word processors adapted web browsers brain control of a wheelchair or neuroprostheses and games among others however most applications have solely been designed for training or demonstration purposes despite the most recent significant advances in bci technology there are still many challenges to employing bci control for real world tasks i the information transfer rate provided by bcis is too low for sensors natural interactive conversation even for experienced subjects and well tuned bci systems ii the high error rate further complicates the interaction iii bci systems cannot be used autonomously by disabled people because bci systems require assistants to apply electrodes or signal receiving devices before the disabled person can communicate iv a bci user may be able to turn the bci system off by means of brain activity as input but usually cannot turn it back on again which is termed the midas touch problem and v handling bci applications demands a high cognitive load that can usually be achieved by users in quiet laboratory environment but not in the real world nevertheless despite all these challenging difficulties the first steps on the path to long term independent home use of bcis have already been taken before describing the practical usage of bci applications it is worth considering the distinction between bcis and their applications as a tool that executes a specific function particular bci specifications correspond to the way it performs that function these specifications can therefore be applied to wide variety of applications even though the function remains unchanged the important thing in bci evaluation is its performance when executing its specific function in contrast applications are described in terms of the tools they employ and the purposes they serve therefore bci evaluation focuses on how well it performs its purpose in other words the term bci refers to the system that records analyses and translates the input into commands and the term application denotes the environment in which the bci estimated output commands are applied consequently the evaluation procedures for bci systems and their applications differ in each case the following sub sections briefly describe bci applications classified into five main areas communication motor restoration environmental control locomotion and entertainment communication bci applications for communication deal with severe communication disabilities resulting from neurological diseases this kind of application probably represents the most pressing research in the field of bci because communication activity is essential for humans applications for communication purposes outline an operation that typically displays a virtual keyboard on screen where the user selects a letter from the alphabet by means of a bci the distinguishing element in each approach is usually the bci and the type of control signal voluntary control of scps may be used for letter selection with extensive training completely paralyzed patients are able to produce positive and negative changes in their scp to drive the vertical movement of a cursor based on this kind of control signal birbaumer et al developed a spelling device with an on screen display which used a cursor to select letters of the alphabet trials involving two patients at advanced stages of als showed that they achieved a rate of about characters per minute when writing text messages other types of control signals such as detection of eye blinks which normally represent an artifact in eeg signals or classification of three mental tasks are also used to select the blocks or characters in a virtual keyboard both approaches are nearly the same apart from the control signal in both cases the virtual keyboard consisted of a total of symbols english letters plus the space to separate words organized in a three row by nine column matrix likewise both applications were based on the same protocol of writing a single letter which required three steps firstly the whole keyboard was divided into three sensors blocks each with nine letters each then the user could select a set of nine letters by producing a single two or three eye blinks or imagining one of three available tasks depending on the case after the first selection the set of nine letters was distributed into three subsets each with three letters and once again the user again selected one of them finally at the third level the user chose a single letter amongst the three remaining symbols the correct spelling rate of each speller was one character per minute using blinks and characters per minute for three mental tasks obermaier et al also designed a letter spelling based on standard graz bci which also included a virtual keyboard the letter selection protocol is very similar to the approaches discussed above except that the entire alphabet consisted of letters and was divided into two halves at each step in this case the user chooses either subset of letters by eeg modulation through mental hand and leg motor imagery the spelling rate achieved by three healthy users varied between and letters per minute this is a lower rate than in previous cases nevertheless it appears easy to increase the number of letters spelled per minute just by expanding the number of classes to more than two event related brain potentials are also very popular in bci letter spelling applications based bcis have been proven sufficiently suitable for als patients in the early and middle stages of the disease besides this kind of bci is very handy because the response occurs spontaneously and consequently does not require substantial training furthermore recent progress with based spellers have allowed the development of commercial applications available to the general public one of the best known spellers was designed by farwell and donchin in in this speller the letters of the alphabet together with several other symbols and commands are displayed on screen in a matrix figure with randomly flashing rows and columns then the user focuses attention on the screen and concentrates successively on the characters to be written while the eeg response is monitored two are elicited for each looked for element on the matrix when the desired row or column flashed thereby allowing the system to identify the desired symbol the results of the farwell donchin speller trials involving healthy people yielded an acceptable spelling rate of about characters per minute figure original speller matrix of symbols displayed on a screen computer which serves as the keyboard or prosthetic device adapted from the farwell donchin speller provides a relatively high rate and accuracy but its precision can be improved by reducing perceptual errors in the farwell donchin paradigm perceptual error happens when a response is elicited due to flashing rows or columns adjacent to the target symbol an issue which is its major source of error hence a new letter distribution was presented to sensors overcome this problem figure the idea is to have several regions flashing instead of using rows and columns the characters are placed into a two level distribution at the first level the characters are distributed into seven groups each with seven characters which are also flashing randomly the group containing the target character is found by detection at the second level the characters in the detected group are repositioned and the level one procedure is repeated and so on until the target character is finally selected figure the proposed region based paradigm for the improved speller a the first level of intensification where each group contains up to seven characters and b one region is expanded at the second level adapted from a b townsend et al presented a newly enhanced bci based on a checkerboard paradigm instead of the standard row column paradigm introduced by farwell and donchin in this new approach the standard matrix containing the targets was superimposed on a checkerboard trials with advanced als patients and healthy people showed a significantly higher mean accuracy for the checkerboard paradigm than for the row column paradigm ahi et al also recently improved the farwell donchin speller by introducing a dictionary to decrease the number of misclassifications in the spelling the dictionary was used for checking the candidate word proposed by the classifier of responses in case of misspelling the dictionary gave a certain number of suggestions from which the system could select additionally in order to reduce the probability of misspelling due to perceptual errors the usual letter position in the matrix was changed according to the analysis of word similarities in the constructed dictionary all previous spellers are based on the recording of visual event related brain potentials however there is no sense in using visual stimuli in cases of severely paralyzed patients with impaired vision or poor control over eye movements in these cases auditory stimulation is used in order to make spellers suitable for this group of patients other important applications of communication related bcis are internet browsers adapted to users with severe disabilities because over the last decade the internet has become a very important part of daily life in this area descartes is one of the first eeg controlled web browsers which can be operated by scps its browser interface is based on arranging the links alphabetically in a dichotomous decision tree where the user selects or rejects each item producing positive or negative scp shifts descartes presents the shortcoming that only a limited number of web pages can be o q t r u p s a c f d g b e h j m k n i l x v y w z sensors browsed because the user receives a number of predefined links arranged in a tree at the start of the web surfing besides graphical links cannot be chosen since the textual label is used to identify the link a more advanced prototype called nessi overcomes these shortcomings thanks to a better user interface colored frames are placed around links or selectable items on the web page instead of arranging the links in a tree more recently evoked potentials are also used to enhance browser functionality mugler et al built an internet browser with control where the options are all presented as icons in an matrix jinghai et al developed a browser based on veps one of the advantages of erps is that they occur quickly and can lead to relatively high web surfing speeds motor restoration spinal cord injury sci or other neurological diseases with associated loss of sensory and motor functions dramatically decrease the patient quality of life and create life long dependency on home care services motor restoration may alleviate their psychological and social suffering restoring movement such as grasping is feasible in quadriplegic patients through neuroprostheses guided by functional electrical stimulation fes fes compensates for the loss of voluntary functions by eliciting artificial muscle contractions electrical currents generate artificial action potential by depolarizing intact peripheral motor nerves that innervate the targeted muscle and cause a muscle contraction see for a review eeg based bci can be used to generate a control signal for the operation of fes because eeg signals are unaffected by electrical activation of upper extremity muscles thanks to their merging of bci and fes pfurtscheller et al developed an application where a tetraplegic patient suffering from a traumatic spinal cord injury was able to control paralyzed hands to grasp a cylinder in that application the patient generated beta oscillations in the eeg by foot movement imagery then the bci analyzed and classified the beta burst and the output signal was used to control the fes device that activated the extremity also fes has been used for rehabilitation training after a stroke hu et al developed a combined fes robot system which was continuously driven by the user residual electromyography on the affected side for wrist joint training after a stroke in order to involve the user own neuromuscular effort during the training fes has been proven to be an effective way to restore movement nevertheless fes requires the use of residual movements which are not possible in severely injured patients for this reason some groups have started to explore approaches that couple neuroprostheses and bci without fes intervention pfurtscheller et al demonstrated that a tetraplegic patient whose residual upper limb muscle activity was restricted to the left biceps due to an upper spinal cord injury could effectively control a hand orthosis using changes in rolandic oscillations which were produced by motor imagery a lengthy training period was required to use this application however the patient was finally able to open and close the hand orthosis almost without any errors some years later the same group validated the coupling of eeg based bcis and an implanted neuroprosthesis giving further evidence that bci is a feasible option for the control of a neuroprostheses in this study bci classified distinctive eeg patterns that involved power decreases in certain specific frequency bands these patterns were generated by the user from mental imagery of his paralyzed left hand in motion more recently erps are also used to provide motor restoration muller et al presented a novel neuroprosthetic device for the restoration of the grasp function for people spinal cord injuries this sensors neuroprosthetic device consisted of a dual axis electrical hand prosthesis controlled by bci based on four class ssveps hence it is possible to select only four movements according to the four leds flickering in different frequencies the user gaze shifted between the different leds in order to select a movement one light on the finger index flickering at hz and another light on the pinky finger flickering at hz served to turn the hand in supination or pronation the two remaining lights on the wrist flickering at hz and hz represented the orders to open and close each hand within the field of bci application in motor restoration bci systems have been also applied for movement reconstruction in patients with severe post stroke motor disability bci training is hypothesized to provide feedback to sensorimotor cortex and by doing so movement is restored as cerebral pathways reorganize to link up motor commands with motor movements buch et al developed a bci system that used meg activity evoked by patient intent to move a completely paralyzed hand in order to control grasping motions of a mechanical orthosis attached to the affected hand thanks to the hand prosthesis attached to the paralyzed hand and using visual feedback the patient could learn to open a hand by increasing smr over the injured hemisphere and to close the hand by decreasing it meg provides a much larger and more localized smr response which means that even a digit finger may be controlled meg based bci is too expensive for widespread applications for that reason broetz et al proposed a combination of meg and eeg based bcis initially the meg based bci was used to boost rehabilitation training success later the user continued rehabilitation training with an eeg based bci a more affordable technology than meg finally the patient practiced physiotherapy training the results of this study suggest that the combination of bci training with goal directed active physical therapy improves the motor abilities of chronic stroke patients in similar experiments caria et al reaffirmed the success of a combination of bci training and physiotherapy this study encourages further research on the role of bcis in brain plasticity and post stroke recovery environmental control one of the main goals of bci based applications is to achieve maximum independence for the patient despite any motor disability people who suffer severe motor disabilities are often homebound and for this reason environmental control applications focus on the control of domestic devices such as tv lights or ambient temperatures apart from improving the quality of life of severely disabled people assistive devices mean that the tasks of the caregiver are less intensive costs are reduced and the life of relatives is less onerous cincotti et al presented a pilot study dealing with the integration of bci technology into the domestic environment in this study fourteen patients with severe motor disabilities due to progressive neurodegenerative disorders tested a device that provided environmental control through an interface designed to support different levels of motor capacities for each user typical peripherals such as keyboard mouse or joystick were offered to allow the device control through upper limb residual motor abilities head trackers and microphones for voice recognition were also available in cases of people with impaired limbs but intact neck muscles and comprehensive speech lastly in cases of totally disabled people the system could be controlled by voluntary modulations of sensorimotor rhythms recorded by the eeg based bci thereby the application offered the patient sensors different access modalities that matched their gradual loss of motor abilities due to progressive neurodegenerative diseases as output devices the system allowed the use of a basic group of domestic appliances such as lights tv and stereo sets a motorized bed an acoustic alarm a front door opener and a telephone as well as wireless cameras to monitor the surrounding environment invasive techniques have also been proposed in environmental control applications hochberg et al implanted braingate sensors in the primary motor cortex of a tetraplegic patient to control a cursor the initial trials yielded promising results where the patient could handle e mail applications or operate devices such as a television by imagining limb motions even while conversing locomotion bci applications that allow disabled people to control a means of transportation represent an important field in their use thanks to these applications people suffering from paraplegia or with other physical impairments can autonomously drive a wheelchair making them more autonomous and improving their life quality portability is a necessity for these kinds of applications hence the use of a bci based on eeg recording is enforced eeg signals are typically very noisy and are highly variable which means a relatively long time between commands that will be of high uncertainty therefore the main challenge is to achieve sufficient accuracy in driving as well as reaching real time control in spite of the ultra low information transfer rates provided by bci for this reason some studies on bcis proposed invasive techniques to record eeg signals because they achieved more spatial resolution and reduced noise serruya et al experimented with monkeys implanting an electrode array in the motor cortex these initial experiments showed that the monkeys were able to move a computer cursor to any position opening up new human applications however the risks related to invasive bcis lead research towards building non invasive applications for human use some pilot experiments concerned with locomotion illustrate the feasibility of using eeg signals for continuous control of a mobile robot in an indoor environment with several rooms corridors and doorways the results of these experiments opened the possibility for physically disabled people to use a portable eeg based bci for controlling wheelchairs to the best of our knowledge in tanaka et al presented the first application of wheelchair control using only eeg in this study the surrounding floor was divided into squares between which the user decided to move by imagining left or right limb movements driven by user decisions the wheelchair therefore moved from one square to another tests with six healthy subjects were quite encouraging and demonstrated the viability of wheelchairs control solely through the use of eeg signals in wheelchair control by bci based systems the usual problems are the infrequent control signal and the low information transfer rate and accuracy provided by a bci in that respect some improvements have been presented over the past few years synchronous based bcis have been introduced in order to assure better accuracy likewise to overcome the usual low bit rate in bcis the systems have been endowed with certain autonomy decreasing the number of interactions required rebsamen et al designed a simplified wheelchair control by constraining the movements to guidepaths defined by the patient or a helper these guidepaths were attached to a specific point in the environment and stored by the system the user selected the destination through a based bci and the wheelchair automatically followed the path the user only had to decide when the wheelchair sensors would stop for path guidance the system steering the wheelchair had to be kept informed of its localization uninterruptedly to that effect the wheelchair relied on an odometer and a bar code scanner to read bar code patterns previously placed on the floor along the paths some years later the system was improved to ensure safer control two faster bcis based on and the μ β rhythm were employed allowing the user to stop the wheelchair more quickly both applications were tested with healthy people the main disadvantage found in the preceding approaches is that the control assistance has little flexibility and is not capable of dealing with unknown and populated scenarios iturrate et al overcame this shortcoming by making the system create a dynamic reconstruction of the surrounding scenario other studies suggested that help should only be available in those cases where the user experienced more difficulties driving the wheelchair e g in a narrow corridor three levels of assistance may be possible in the shared control collision avoidance obstacle avoidance and orientation recovery which are only activated as required by the user before executing the user steering commands the share control evaluates the situation from the data provided by a set of laser scanners scanners inspect the environment and detect potential obstacles or walls entertainment entertainment orientated bci applications have typically had a lower priority in this field until now research into bci technology has usually focused on assistive applications such as spelling devices wheelchair control or neuroprostheses rather than applications with entertainment purposes however interest in entertainment applications has arisen over the recent years due to the significant advances in this technology in fact improvements in its performance have opened the way to extending bci use to non disabled people bcis create a new interaction modality which may turn video games into even more challenging and attractive experiences additionally bci may provide a way of accessing knowledge on the user experiences thereby improving games through information from brain activity bcis can report when the gamer is bored anxious or frustrated with the aim of using this knowledge for designing future games figure pacman game the gamer has to move through the maze to reach the exit in the right wall the shortest path is marked with gray track marks but the gamer can decide to run the rest of maze to receive additional credits adapted from nose direction of next move discrimination line indication of competing laterality intentions s sensors entertain p pacman pon il llustration w with the aim f filled with r n nose points g game was d r recording te external d designed a ssver t c could be mo o on the one s ssvers a s ssver th b because it re l lalor et al ti ight rope w to o phase rev im mplemented u useful tool f all previ c context ho e emotiv s spirit moun electrode a an applicati a applications nment orient ng and simi figure m of giving red and gre to the direc developed chniques fo evoked pot simple fligh this simulat oved to the hand the c and on the e results of equired littl prese walker in b versing che d the min for attention figure ous exampl wever ther has already ntain demo es the so ca ion program is made ted bci ap ilar games shows a scre the gamer en color fro ction in wh in order t or complex c tentials have ht simulator tor was ver left or the r control com other hand f the trials w e or no train ented the balance th eckerboard ndgame ba n training b a emotiv a les involved re are some y developed o game am alled epoc mming inter much sim pplications so that they eenshot of a enough tim om bottom hich the use o illustrate control task e been also controlled ry modest a right only t mmand righ the select with able bo ning since t mindbalan he applicati patterns b ased on ecause the p v epoc neu d experimen e companie d a numero mong others c neurohead rface api mpler the have adap y may be pla a pacman ga me to perfor up as the p r intends to that it is ks used for ga by a bci ba and only tw two metho ht or left w tion was ide odied partic the system c nce game w ion was al by means o events potent uroheadset ntal games t es preparing us set of b furtherm dset figure thanks to company pted very w ayed throug ame the pa rm a contro player inte o take the p possible to ame implem ased on ste wo control a ods were tes was detecte entified tak cipants show capitalized where six h lso based o of other ki it was sug tials are als b neu that have on g commerci bci based g more the co e a whi o this api neurosky well known gh motor im acman mak ol command ention to tu pacman in o successfu mentations eady state v actions were sted over th d accordin king into ac wed that the on the natu healthy user on ssvep inds of er ggested that o attention m urosky min b nly been fo ial bci gam games such ompany sell ich can be b the develop also m n video gam magery kes one step d the pacm urn rises an another stu ully apply middendor visual evok e possible he airplane c ng to the str ccount the f e last one w ural occurrin rs were ask generated rps finke t this game markers ndwave und useful mes for fut h as cortex ls a low co bought acco pment of th markets the mes such a by way o each man head nd the yellow udy a pinba non invasiv rf et al ked respons the positio control trial rength of th frequency o was preferre ng response ked to keep in respons et al e would be in a researc ture market x arcade an ost bci wit ompanied b he bci base e mindwav as of is w all ve se on ls he of ed es a se a ch ts nd th by ed ve sensors neuroheadset figure b with software applications that can respond to user brainwaves or mental states likewise it provides a set of software tools for developers also large software companies such as microsoft have shown interest in bci research exploring the development of pilot novel applications that use bcis other bci applications bci systems have also been used in a broad variety of applications beyond the traditional areas of communication motor restoration environmental control locomotion and entertainment the ability of bci feedback to induce cortical plasticity may be the basis for medical applications users can acquire selective control over certain brain areas by means of neurofeedback with the aim of inducing behavioral changes in the brain neurofeedback provided by a bci system may improve cognitive performance speech skills affection and pain management and has been used in the treatment of mental disorders such as epilepsy attention deficit schizophrenia depression alcohol dependence or paedophilia on the other hand brain signal recordings can be used in an assessment of brain functions to evaluate their status in health and disease the opportunity to examine brain signals can also be commercially exploited neuromarketing is a relatively young field of research that applies neuroscientific methods to marketing research to date few neuromarketing studies have been conducted although some evidence has been found to suggest that neuroimaging could have a role in several areas of marketing neuromarketing may provide a more efficient trade off between costs and benefits product concepts could be tested by means of neuromarketing removing those that are not promising at the start of the manufacturing process this would lead to a more efficient distribution of sources because only the more promising products would be developed in addition neuromarketing may be a source of more accurate information on the underlying preferences of the users rather than data from standard market research studies neuroimaging may reveal hidden information on consumers true preferences that cannot be explicitly expressed the brain response to advertisements could be measured and the effectiveness of advertising campaigns could therefore be quantified despite it being an emerging field several companies such as neurofocus neuroconsult neuro insight or emsense among others currently offer neuromarketing services it is also attracting increasing attention among researchers the field has raised some ethical issues concerning this technology in as much as it may be able to manipulate the brain and consumer behavior conclusions this article has reviewed the state of the art of bci systems discussing fundamental aspects of bci system design the most significant goals that have driven bci research over the last years have been presented it has been noted that many breakthroughs were achieved in bci research different neuroimaging approaches have been successfully applied in bci i eeg which provides acceptable quality signals with high portability and is by far the most usual modality in bci ii fmri and meg which are proven and effective methods for localizing active regions inside the brain iii nirs sensors which is a very promising neuroimaging method in bci and iv invasive modalities which have been presented as valuable methods to provide the high quality signals required in some multidimensional control applications e g neuroprostheses control a wide variety of signal features and classification algorithms have been tested in the bci design although bci research is relatively young many advances have been achieved in a little over two decades because many of these methods are based on previous signal processing and pattern recognition research many studies have demonstrated the valuable accuracy of bcis and provided acceptable information bit rate despite the inherent major difficulties in brain signal processing accordingly user training time has been significantly reduced which has led to more widespread bci applications in the daily life of disabled people such as word processing browsers email wheelchair control simple environmental control or neuroprostheses among others in spite of the recent important advances in the bci field some issues still need to be solved first the relative advantages and disadvantages of the different signal acquisition methods are still unclear their clarification will require further human and animal studies second invasive methods need further investigation to deal with tissue damage risk of infection and long term stability concerns electrodes that contain neurotropic mediums that promote neuronal growth and wireless transmission of neuronal signals recorded have already been proposed third the electrophysiological and metabolic signals that are best able to encode user intent should be better identified and characterized the majority of bci studies have treated time frequency and spatial dimensions of brain signals independently these signal dimension interdependencies may lead to significant improvement in bci performance fourth information bit rate provided by current bcis is low for effective human machine interaction in some applications exogenous based bci may provide much higher throughput fifth the unsupervised adaptation is a key challenge for bci deployment outside the lab some moderately successful adaptive classification algorithms have already been proposed and finally most bci applications are at the research stage and they are not ready to be introduced into people homes for continuous use in their daily life in addition to their low information transfer rates and variable reliability most current bci systems are uncomfortable because the electrodes need to be moistened the software may require initiation and the electrode contacts need continuous correction an easy to use based bci with remote monitoring using a high speed internet connection has already been proposed to reduce dependence on technical experts the latest advances in bci research suggest that innovative developments may be forthcoming in the near future these achievements and the potential for new bci applications have obviously given a significant boost to bci research involving multidisciplinary scientists e g neuroscientists engineers mathematicians and clinical rehabilitation specialists among others interest in the bci field is expected to increase and bci design and development will in all probability continue to bring benefits to the daily lives of disabled people furthermore recent commercial interest within certain companies suggests that bci systems may find useful applications in the general population and not just for people living with severe disabilities in the near future bci systems may therefore become a new mode of human machine interaction with levels of everyday use that are similar to other current interfaces sensors acknowledgements this work was partially supported by the regional research project plan of the junta de castilla y león spain under project sensors lebedev m a nicolelis m a l brain machine interfaces past present and future trends neurosci kennedy p r bakay r a e moore m m adams k goldwaithe j direct control of a computer from the human central nervous system ieee trans rehabil eng bossetti c a carmena j m nicolelis m a l wolf p d transmission latencies in a telemetry linked brain machine interface ieee trans biomed eng teplan m fundamentals of eeg measurement meas sci rev sinclair c m gasper m c blum a s basic electronics in clinical neurophysiology in the clinical neurophysiology primer blum a s rutkove s b eds humana press inc totowa nj usa pp usakli a b improvement of eeg signal acquisition an electrical aspect for state of the art of front end comput intell neurosci fonseca c cunha j p s martins r e ferreira v m de sa j p m barbosa m a da silva a m a novel dry active electrode for eeg recording ieee trans biomed eng taheri b a knight r t smith r l a dry electrode for eeg recording electroencephalogr clin neurophysiol gargiulo g calvo r a bifulco p cesarelli m jin c mohamed a van schaik a a new eeg recording system for passive dry electrodes clin neurophysiol leach w m jr fundamentals of low noise analog circuit design proc ieee kübler a kotchoubey b kaiser j wolpaw j r birbaumer n brain computer communication unlocking the locked in american psychological association washington dc usa anand b k chhina g s singh b some aspects of electroencephalographic studies in yogis electroencephalogr clin neurophysiol aftanas l i golocheikine s a human anterior and frontal midline theta and lower alpha reflect emotionally positive state and internalized attention high resolution eeg investigation of meditation neurosci lett fernández t harmony t rodríguez m bernal j silva j reyes a marosi e eeg activation patterns during the performance of tasks involving different components of mental calculation electroencephalogr clin neurophysiol caplan j b madsen j r raghavachari s kahana m j distinct patterns of brain oscillations underlie two basic parameters of human maze learning j neurophysiol klimesch w doppelmayr m yonelinas a kroll n e a lazzara m röhm d gruber w theta synchronization during episodic retrieval neural correlates of conscious awareness cogn brain res jaime a p the functional significance of mu rhythms translating seeing and hearing into doing brain res rev black a h the operant conditioning of central nervous system electrical activity in psychology of learning and motivation gordon h b ed academic press new york ny usa volume pp sensors klimesch w eeg alpha rhythms and memory processes int j psychophysiol 34 venables l fairclough s the influence of performance feedback on goal setting and mental effort regulation motiv emotion pfurtscheller g brunner c schlögl a lopes da silva f h mu rhythm de synchronization and eeg single trial classification of different motor imagery tasks neuroimage pfurtscheller g neuper c motor imagery and direct brain computer communication proc ieee lee k h williams l m breakspear m gordon e synchronous gamma activity a review and contribution to an integrative neuroscience model of schizophrenia brain res rev brown p salenius s rothwell j c hari r cortical correlate of the piper rhythm in humans j neurophysiol mima t simpkins n oluwatimilehin t hallett m force level modulates human cortical oscillatory activities neurosci lett lutzenberger w pulvermüller f elbert t birbaumer n visual stimulation alters local hz responses in humans an eeg study neurosci lett müller m m keil a gruber t elbert t processing of affective pictures modulates right hemispheric gamma band eeg activity clin neurophysiol müller m m bosch j elbert t kreiter a sosa m v sosa p v rockstroh b visually induced gamma band responses in human electroencephalographic activity a link to animal studies exp brain res zhang l he w he c wang p improving mental task classification by adding high frequency band information j med syst 34 darvas f scherer r ojemann j g rao r p miller k j sorensen l b high gamma mapping using eeg neuroimage miller k j leuthardt e c schalk g rao r p n anderson n r moran d w miller j w ojemann j g spectral changes in cortical surface potentials during motor movement j neurosci jasper h h the ten twenty electrode system of the international federation electroencephalogr clin neurophysiol waldert s pistohl t braun c ball t aertsen a mehring c a review on directional information in neural signals for brain machine interfaces j phisiol paris salmelin r hámáaláinen m kajola m hari r functional segregation of movement related rhythmic activity in the human brain neuroimage babiloni c pizzella v gratta c d ferretti a romani g l fundamentals of electroencefalography magnetoencefalography and functional magnetic resonance imaging in brain machine interfaces for space applications luca r dario i leopold s eds academic press new york ny usa volume pp mellinger j schalk g braun c preissl h rosenstiel w birbaumer n kübler a an meg based brain computer interface bci neuroimage sensors wang w sudre g p xu y kass r e collinger j l degenhart a d bagic a i weber d j decoding and cortical source localization for intended movement direction with meg j neurophysiol lal t n schröder m hill n j preissl h hinterberger t mellinger j bogdan m rosenstiel w hofmann t birbaumer n schölkopf b a brain computer interface with online feedback based on magnetoencephalography in proceedings of the international conference on machine learning icml bonn germany august pp kauhanen l nykopp t lehtonen j jylanki p heikkonen j rantanen p alaranta h sams m eeg and meg brain computer interface for tetraplegic patients ieee trans neural syst rehabil eng georgopoulos a langheim f leuthold a merkle a magnetoencephalographic signals predict movement trajectory in space exp brain res mellinger j schalk g braun c preissl h rosenstiel w birbaumer n kübler a an meg based brain computer interface bci neuroimage jinyin z sudre g xin l wei w weber d j bagic a clustering linear discriminant analysis for meg based brain computer interfaces ieee trans neural syst rehabil eng sabra n i wahed m a the use of meg based brain computer interface for classification of wrist movements in four different directions in proceedings of the national radio science conference nrsc cairo egypt april pp ball t kern m mutschler i aertsen a schulze bonhage a signal quality of simultaneously recorded invasive and non invasive eeg neuroimage loeb g e walker a e uematsu s konigsmark b w histological reaction to various conductive and dielectric films chronically implanted in the subdural space j biomed mater res bullara l a agnew w f yuen t g h jacques s pudenz r h evaluation of electrode array material for neural prostheses neurosurgery yuen t g h agnew w f bullara l a tissue response to potential neuroprosthetic materials implanted subdurally biomaterials margalit e weiland j d clatterbuck r e fujii g y maia m tameesh m torres g d anna s a desai s piyathaisere d v et al visual and electrical evoked response recorded from subdural electrodes implanted above the visual cortex in normal dogs under two methods of anesthesia j neurosci methods chao z c nagasaka y fujii n long term asynchronous decoding of arm motion using electrocorticographic signals in monkeys front neuroeng doi fneng matsuo t kawasaki k osada t sawahata h suzuki t shibata m miyakawa n nakahara k iijima a sato n et al intrasulcal electrocorticography in macaque monkeys with minimally invasive neurosurgical protocols front syst neurosci doi fnsys crone n e miglioretti d l gordon b sieracki j m wilson m t uematsu s lesser r p functional mapping of human sensorimotor cortex with electrocorticographic spectral analysis i alpha and beta event related desynchronization brain sensors crone n e miglioretti d l gordon b lesser r p functional mapping of human sensorimotor cortex with electrocorticographic spectral analysis ii event related synchronization in the gamma band brain miller k j dennijs m shenoy p miller j w rao r p n ojemann j g real time functional brain mapping using electrocorticography neuroimage levine s p huggins j e bement s l kushwaha r k schuh l a passaro e a rohde m m ross d a identification of electrocorticogram patterns as the basis for a direct brain interface j clin neurophysiol leuthardt e c schalk g wolpaw j r ojemann j g moran d w a brain computer interface using electrocorticographic signals in humans j neural eng doi 2560 schalk g schalk g kubánek j miller k j anderson n r leuthardt e c ojemann j g limbrick d moran d gerhardt l a et al decoding two dimensional movement trajectories using electrocorticographic signals in humans j neural eng doi 2560 polikov v s tresco p a reichert w m response of brain tissue to chronically implanted neural electrodes j neurosci methods maynard e m nordhausen c t normann r a the utah intracortical electrode array a recording structure for potential brain computer interfaces electroencephalogr clin neurophysiol lauer r t peckham p h kilgore k l heetderks w j applications of cortical signals to neuroprosthetic control a critical review ieee trans rehabil eng georgopoulos a schwartz a kettner r neuronal population coding of movement direction science schwartz a b motor cortical activity during drawing movements population representation during sinusoid tracing j neurophysiol wessberg j stambaugh c r kralik j d beck p d laubach m chapin j k kim j biggs s j srinivasan m a nicolelis m a l real time prediction of hand trajectory by ensembles of cortical neurons in primates nature taylor d m tillery s i h schwartz a b direct cortical control of neuroprosthetic devices science jackson a mavoori j fetz e e correlations between the same motor cortex cells and arm muscles during a trained task free behavior and natural sleep in the macaque monkey j neurophysiol velliste m perel s spalding m c whitford a s schwartz a b cortical control of a prosthetic arm for self feeding nature 1101 vargas irwin c e shakhnarovich g yadollahpour p mislow j m k black m j donoghue j p decoding complete reach and grasp actions from local primary motor cortex populations j neurosci carpaneto j umiltà m a fogassi l murata a gallese v micera s raos v decoding the activity of grasping neurons recorded from the ventral premotor area of the macaque monkey neuroscience sensors kennedy p r kirby m t moore m m king b mallory a computer control using human intracortical local field potentials ieee trans neural syst rehabil eng decharms r c christoff k glover g h pauly j m whitfield s gabrieli j d e learned regulation of spatially localized brain activation using real time fmri neuroimage weiskopf n mathiak k bock s w scharnowski f veit r grodd w goebel r birbaumer n principles of a brain computer interface bci based on real time functional magnetic resonance imaging fmri ieee trans biomed eng lee j ryu j jolesz f a cho z h yoo s s brain machine interface via real time fmri preliminary study on thought controlled robotic arm neurosci lett logothetis n k pauls j augath m trinath t oeltermann a neurophysiological investigation of the basis of the fmri signal nature weiskopf n sitaram r josephs o veit r scharnowski f goebel r birbaumer n deichmann r mathiak k real time functional magnetic resonance imaging methods and applications magn reson imaging christopher decharms r applications of real time fmri nat rev neurosci moench t hollmann m grzeschik r mueller c luetzkendorf r baecke s luchtmann m wagegg d bernarding j real time classification of activated brain areas for fmri based human brain interfaces society of photo optical instrumentation engineers belligham wa usa ward b d mazaheri y information transfer rate in fmri experiments measured using mutual information theory j neurosci methods coyle s m ward t e markham c m brain computer interface using a simplified functional near infrared spectroscopy system j neural eng doi 2560 taga g homae f watanabe h effects of source detector distance of near infrared spectroscopy on the measurement of the cortical hemodynamic response in infants neuroimage kennan r p horovitz s g maki a yamashita y koizumi h gore j c simultaneous recording of event related auditory oddball response using transcranial near infrared optical topography and surface eeg neuroimage izzetoglu m devaraj a bunce s onaral b motion artifact cancellation in nir spectroscopy using wiener filtering ieee trans biomed eng strangman g boas d a sutton j p non invasive neuroimaging using near infrared light biol psychiatry cui x bray s reiss a l functional near infrared spectroscopy nirs signal improvement based on negative correlation between oxygenated and deoxygenated hemoglobin dynamics neuroimage shirley c ward t markham c mcdarby g on the suitability of near infrared nir systems for next generation brain computer interfaces physiol meas power s d kushki a chau t towards a system paced near infrared spectroscopy brain computer interface differentiating prefrontal activity due to mental arithmetic and mental singing from the no control state j neural eng sensors villringer a planck j hock c schleinkofer l dirnagl u near infrared spectroscopy nirs a new tool to study hemodynamic changes during activation of brain function in human adults neurosci lett sitaram r zhang h guan c thulasidas m hoshi y ishikawa a shimizu k birbaumer n temporal classification of multichannel near infrared spectroscopy signals of motor imagery for developing a brain computer interface neuroimage 34 1427 regan d human brain electrophysiology evoked potentials and evoked magnetic fields in science and medicine elsevier new york ny usa yijun w ruiping w xiaorong g bo h shangkai g a practical vep based brain computer interface ieee trans neural syst rehabil eng jinghai y derong j jianfeng h design and application of brain computer interface web browser based on vep in proceedings of the international conference on future biomedical information engineering fbie sanya china december pp xiaorong g dingfeng x ming c shangkai g a bci based environmental controller for the motion disabled ieee trans neural syst rehabil eng odom j v bach m barber c brigell m marmor m f tormene a p holder g e visual evoked potentials standard doc ophthalmol zhu d bieger j garcia molina g aarts r m a survey of stimulation methods used in ssvep based bcis comput intell neurosci doi 702357 galloway n human brain electrophysiology evoked potentials and evoked magnetic fields in science and medicine br j ophthalmol perlstein w m cole m a larson m kelly k seignourel p keil a steady state visual evoked potentials reveal frontally mediated working memory activity in humans neurosci lett gray m kemp a h silberstein r b nathan p j cortical neurophysiology of anticipatory anxiety an investigation utilizing steady state probe topography sspt neuroimage guangyu b xiaorong g yijun w bo h shangkai g vep based brain computer interfaces time frequency and code modulations research frontier ieee comput intell mag lee p hsieh j wu c shyu k wu y brain computer interface using flash onset and offset visual evoked potentials clin neurophysiol allison b z mcfarland d j schalk g zheng s d jackson m m wolpaw j r towards an independent brain computer interface using steady state visual evoked potentials clin neurophysiol dan z alexander m xiaorong g bo h andreas k e shangkai g an independent brain computer interface using covert non spatial visual selective attention j neural eng yijun w xiaorong g bo h chuan j shangkai g brain computer interfaces based on visual evoked potentials ieee eng med biol mag wu z lai y xia y wu d yao d stimulator selection in ssvep based bci med eng phys sensors birbaumer n elbert t canavan a g rockstroh b slow potentials of the cerebral cortex and behavior physiol rev hinterberger t schmidt s neumann n mellinger j blankertz b curio g birbaumer n brain computer communication and slow cortical potentials ieee trans biomed eng kaiser j self initiation of eeg based communication in paralyzed patients clin neurophysiol neumann n birbaumer n predictors of successful self control during brain computer communication j neurol neurosurg psychiatry kübler a kotchoubey b hinterberger t ghanayim n perelmouter j schauer m fritsch c taub e birbaumer n the thought translation device a neurophysiological approach to communication in total motor paralysis exp brain res birbaumer n hinterberger t kubler a neumann n the thought translation device ttd neurobehavioral mechanisms and clinical outcome ieee trans neural syst rehabil eng iversen i h ghanayim n kübler a neumann n birbaumer n kaiser j a brain computer interface tool to assess cognitive functions in completely paralyzed patients with amyotrophic lateral sclerosis clin neurophysiol 2223 farwell l a donchin e talking off the top of your head toward a mental prosthesis utilizing event related brain potentials electroencephalogr clin neurophysiol donchin e smith d b d the contingent negative variation and the late positive wave of the average evoked potential electroencephalogr clin neurophysiol polich j ellerson p c cohen j stimulus intensity modality and probability int j psychophysiol ravden d polich j on measurement stability habituation intra trial block variation and ultradian rhythms biol psychol tanaka k matsunaga k wang h o electroencephalogram based control of an electric wheelchair ieee trans robot mugler e m ruf c a halder s bensch m kubler a design and implementation of a based brain computer interface for controlling an internet browser ieee trans neural syst rehabil eng furdea a halder s krusienski d j bross d nijboer f birbaumer n kübler a an auditory oddball spelling system for brain computer interfaces psychophysiology rivet b souloumiac a attina v gibert g xdawn algorithm to enhance evoked potentials application to brain computer interface ieee trans biomed eng rakotomamonjy a guigue v bci competition iii dataset ii ensemble of svms for bci speller ieee trans biomed eng salvaris m sepulveda f visual modifications on the speller bci paradigm j neural eng sensors takano k komatsu t hata n nakajima y kansaku k visual stimuli for the brain computer interface a comparison of white gray and green blue flicker matrices clin neurophysiol ikegami s takano k saeki n kansaku k operation of a based brain computer interface by individuals with cervical spinal cord injury clin neurophysiol hill j farquhar j martens s bießmann f schölkopf b effects of stimulus type and of error correcting code design on bci speller performance in proceedings of the twenty second annual conference on neural information processing systems nips vancouver canada december martens s m m hill n j farquhar j schölkopf b overlap and refractory effects in a brain computer interface speller based on the visual event related potential j neural eng townsend g lapallo b k boulay c b krusienski d j frye g e hauser c k schwartz n e vaughan t m wolpaw j r sellers e w a novel based brain computer interface stimulus presentation paradigm moving beyond rows and columns clin neurophysiol brunner p joshi s briskin s wolpaw j r bischof h schalk g does the speller depend on eye gaze j neural eng pfurtscheller g lopes da silva f h event related eeg meg synchronization and desynchronization basic principles clin neurophysiol jeannerod m mental imagery in the motor context neuropsychologia 1432 pfurtscheller g neuper c flotzinger d pregenzer m eeg based discrimination between imagination of right and left hand movement electroencephalogr clin neurophysiol blankertz b sannelli c halder s hammer e m kübler a müller k r curio g dickhaus t neurophysiological predictor of smr based bci performance neuroimage 1309 neuper c scherer r reiner m pfurtscheller g imagery of motor actions differential effects of kinesthetic and visual motor mode of imagery in single trial eeg cogn brain res nijboer f furdea a gunst i mellinger j mcfarland d j birbaumer n kübler a an auditory brain computer interface bci j neurosci methods hwang h kwon k im c neurofeedback based motor imagery training for brain computer interface bci j neurosci methods wolpaw j r mcfarland d j vaughan t m brain computer interface research at the wadsworth center ieee trans rehabil eng blankertz b losch f krauledat m dornhege g curio g muller k r the berlin brain computer interface accurate performance from first session in bci naïve subjects ieee trans biomed eng pfurtscheller g neuper c muller g r obermaier b krausz g schlogl a scherer r graimann b keinrath c skliris d et al graz bci state of the art and clinical applications ieee trans neural sys rehabil eng sensors bai o rathi v lin p huang d battapady h fei d schneider l houdayer e chen x hallett m prediction of human voluntary movement before it occurs clin neurophysiol kleber b birbaumer n direct brain communication neuroelectric and metabolic approaches at tübingen cogn process krusienski d j schalk g mcfarland d j wolpaw j r a mu rhythm matched filter for continuous control of a brain computer interface ieee trans biomed eng tsui c gan j asynchronous bci control of a robot simulator with supervised online training in intelligent data engineering and automated learning ideal yin h tino p corchado e byrne w yao x eds springer berlin germany volume pp ghafar r hussain a samad s a tahir n m comparison of fft and ar techniques for scalp eeg analysis in proceedings of the kuala lumpur international conference on biomedical engineering kuala lumpur malaysia june abu osman n a ibrahim f wan abas w a b abdul rahman h s ting h n magjarevic r eds springer berlin heidelberg berlin germany volume pp von bünau p meinecke f c király f c müller k r finding stationary subspaces in multivariate time series phys rev lett lin c j hsieh m h classification of mental task from eeg data using neural networks based on particle swarm optimization neurocomputing 1121 mcfarland d j anderson c w muller k r schlogl a krusienski d j bci meeting workshop on bci signal processing feature extraction and translation ieee trans neural syst rehabil eng lins o g picton t w berg p scherg m ocular artifacts in recording eegs and event related potentials ii source dipoles and source components brain topogr boye a t kristiansen u q billinger m do nascimento o f farina d identification of movement related cortical potentials with optimized spatial filtering and principal component analysis biomed signal process control te won l lewicki m s girolami m sejnowski t j blind source separation of more sources than mixtures using overcomplete representations ieee signal process lett kun l sankar r arbel y donchin e single trial independent component analysis for bci system in proceedings of the annual international conference of the ieee engineering in medicine and biology society embcs minneapolis mn usa september pp castellanos n p makarov v a recovering eeg brain signals artifact suppression with wavelet enhanced independent component analysis j neurosci methods bell a sejnowski t an information maximization approach to blind separation and blind deconvolution neural comput sensors lee t girolami m sejnowski t j independent component analysis using an extended infomax algorithm for mixed subgaussian and supergaussian sources neural comput gao j yang y lin p wang p zheng c automatic removal of eye movement and blink artifacts from eeg signals brain topogr flexer a bauer h pripfl j dorffner g using ica for removal of ocular artifacts in eeg recorded from blind subjects neural netw jung t p makeig s westerfield m townsend j courchesne e sejnowski t j removal of eye activity artifacts from visual event related potentials in normal and clinical subjects clin neurophysiol wallstrom g l kass r e miller a cohn j f fox n a automatic correction of ocular artifacts in the eeg a comparison of regression based and component based methods int j psychophysiol chiappa s barber d eeg classification using generative independent component analysis neurocomputing kay s m marple s l jr spectrum analysis a modern perspective proc ieee 1419 krusienski d j mcfarland d j wolpaw j r an evaluation of autoregressive spectral estimation model order for brain computer interface applications in proceedings of the annual international conference of the ieee engineering in medicine and biology society embs new york ny usa september pp florian g pfurtscheller g dynamic spectral analysis of event related eeg data electroencephalogr clin neurophysiol jiang w guizhi x lei w huiyuan z feature extraction of brain computer interface based on improved multivariate adaptive autoregressive models in proceedings of the international conference on biomedical engineering and informatics bmei yantai china october pp brunner c allison b z krusienski d j kaiser v müller putz g r pfurtscheller g neuper c improved signal processing approaches in an offline simulation of a hybrid brain computer interface j neurosci methods ghanbari a a kousarrizi m r n teshnehlab m aliyari m wavelet and hilbert transform based brain computer interface in proceedings of the international conference on advances in computational tools for engineering applications actea beirut lebanon july pp samar v j bopardikar a rao r swartz k wavelet analysis of neuroelectric waveforms a conceptual tutorial brain lang demiralp t yordanova j kolev v ademoglu a devrim m samar v j time frequency analysis of single sweep event related potentials by means of fast wavelet transform brain lang hubbard b b the world according to wavelets the story of a mathematical technique in the making a k peters natick ma usa sensors farina d do nascimento o f lucas m f doncarli c optimization of wavelets for classification of movement related cortical potentials generated by variation of force related parameters j neurosci methods bostanov v bci competition data sets ib and iib feature extraction from event related brain potentials with the continuous wavelet transform and the t value scalogram ieee trans biomed eng 1061 senkowski d herrmann c s effects of task difficulty on evoked gamma activity and erps in a visual discrimination task clin neurophysiol mason s g birch g e a brain controlled switch for asynchronous control applications ieee trans biomed eng hinterberger t kübler a kaiser j neumann n birbaumer n a brain computer interface bci for the locked in comparison of different eeg classifications for the thought translation device clin neurophysiol ramoser h muller gerking j pfurtscheller g optimal spatial filtering of single trial eeg during imagined hand movement ieee trans rehabil eng grosse wentrup m buss m multiclass common spatial patterns and information theoretic feature extraction ieee trans biomed eng mousavi e a maller j j fitzgerald p b lithgow b j wavelet common spatial pattern in asynchronous offline brain computer interfaces biomed signal process control lemm s blankertz b curio g muller k r spatio spectral filters for improving the classification of single trial eeg ieee trans biomed eng dornhege g blankertz b krauledat m losch f curio g muller k r combined optimization of spatial and temporal filters for improving brain computer interfacing ieee trans biomed eng dal seno b matteucci m mainardi l a genetic algorithm for automatic feature extraction in detection in proceedings of the ieee international joint conference on neural networks ijcnn hong kong china june pp fatourechi m birch g e ward r k application of a hybrid wavelet feature selection method in the design of a self paced brain interface system j neuroeng rehabil ciaccio e dunn s akay m biosignal pattern recognition and interpretation systems methods for feature extraction and selection ieee eng med biol mag dias n kamrunnahar m mendes p schiff s correia j feature selection on movement imagery discrimination and attention detection med biol eng comput lakany h conway b a understanding intention of movement from electroencephalograms expert syst pudil p novovičová j kittler j floating search methods in feature selection pattern recogn lett townsend g graimann b pfurtscheller g a comparison of common spatial patterns with complex band power features in a four class bci experiment ieee trans biomed eng sensors hasan b a s gan j q unsupervised adaptive gmm for bci in proceedings of the international ieee embs conference on neural engineering ner antalya turkey may pp neuper c scherer r wriessnegger s pfurtscheller g motor imagery and action observation modulation of sensorimotor brain rhythms during mental control of a brain computer interface clin neurophysiol fatourechi m bashashati a ward r k birch g e emg and eog artifacts in brain computer interface systems a survey clin neurophysiol croft r j barry r j removal of ocular artifact from the eeg a review neurophysiol clin vigário r n extraction of ocular artefacts from eeg using independent component analysis electroencephalogr clin neurophysiol del r millan j mourino j franze m cincotti f varsta m heikkonen j babiloni f a local neural classifier for the recognition of eeg patterns associated to mental tasks ieee trans neural netw changmok c micera s carpaneto j jung k development and quantitative performance evaluation of a noninvasive emg computer interface ieee trans biomed eng nojd n hannula m hyttinen j electrode position optimization for facial emg measurements for human computer interface in proceedings of the international conference on pervasive computing technologies for healthcare pcth dublin ireland february pp inhyuk m myoungjoon l museong m a novel emg based human computer interface for persons with disability in proceedings of the proceedings of the ieee international conference on mechatronics icm tunis tunisia june pp gomez gil j san jose gonzalez i nicolas alonso l f alonso garcia s steering a tractor by means of an emg based human machine interface sensors nan b okamoto m tsuji t a hybrid motion classification approach for emg based human robot interfaces using bayesian and neural networks ieee trans robot zhang x chen x lantz v yang j h wang k q exploration on the feasibility of building muscle computer interfaces using neck and shoulder motions in proceedings of the annual international conference of the ieee engineering in medicine and biology society embc minneapolis mn usa september pp de la rosa r alonso a carrera a durán r fernández p man machine interface system for neuromuscular training and evaluation based on emg and mmg signals eckhouse r h maulucci r a a multimedia system for augmented sensory assessment and treatment of motor disabilities telemat inf lotte f congedo m lécuyer a lamarche f arnaldi b a review of classification algorithms for eeg based brain computer interfaces j neural eng mcfarland d j wolpaw j r sensorimotor rhythm based brain computer interface bci feature selection by regression improves performance ieee trans neural syst rehabil eng sensors blankertz b muller k r krusienski d j schalk g wolpaw j r schlogl a pfurtscheller g millan j r schroder m birbaumer n the bci competition iii validating alternative approaches to actual bci problems ieee trans neural syst rehabil eng mcfarland d j krusienski d j wolpaw j r brain computer interface signal processing at the wadsworth center mu and sensorimotor beta rhythms in progress in brain research christa n wolfgang k eds elsevier new york ny usa volume pp daly j j wolpaw j r brain computer interfaces in neurological rehabilitation lancet neurol shenoy p krauledat m blankertz b rao r p n müller k r towards adaptive classification for bci j neural eng millan j r mourino j asynchronous bci and local neural classifiers an overview of the adaptive brain interface project ieee trans neural syst rehabil eng galán f nuttin m lew e ferrez p w vanacker g philips j millán j d r a brain actuated wheelchair asynchronous and non invasive brain computer interfaces for continuous control of robots clin neurophysiol 2169 li y guan c li h chin z a self training semi supervised svm algorithm and its application in an eeg based brain computer interface speller system pattern recogn lett vidaurre c sannelli c müller k r blankertz b machine learning based coadaptive calibration for brain computer interfaces neural comput shijian l cuntai g haihong z unsupervised brain computer interface based on intersubject information and online adaptation ieee trans neural syst rehabil eng yan l kambara h koike y sugiyama m application of covariate shift adaptation techniques in brain computer interfaces ieee trans biomed eng ferrez p w del r millan j error related eeg potentials generated during simulated brain computer interaction ieee trans biomed eng millan j r on the need for on line learning in brain computer interfaces in proceedings of the ieee international joint conference on neural networks ijcnn budapest hungary july pp jain a k duin r p w jianchang m statistical pattern recognition a review ieee trans pattern anal lemm s blankertz b dickhaus t müller k r introduction to machine learning for brain imaging neuroimage denoeux t a k nearest neighbor classification rule based on dempster shafer theory ieee trans syst man cyb dudani s a the distance weighted k nearest neighbor rule ieee trans syst man cyb smc borisoff j f mason s g bashashati a birch g e brain computer interface design for asynchronous control applications improvements to the lf asd asynchronous brain switch ieee trans biomed eng 992 sensors schlögl a lee f bischof h pfurtscheller g characterization of four class motor imagery eeg data for the bci competition j neural eng kayikcioglu t aydemir o a polynomial fitting and k nn based approach for improving classification of motor imagery bci data pattern recogn lett garrett d peterson d a anderson c w thaut m h comparison of linear nonlinear and feature selection methods for eeg signal classification ieee trans neural syst rehabil eng scherer r muller g r neuper c graimann b pfurtscheller g an asynchronously controlled eeg based virtual keyboard improvement of the spelling rate ieee trans biomed eng 984 muller k r anderson c w birch g e linear and nonlinear methods for brain computer interfaces ieee trans neural syst rehabil eng blankertz b lemm s treder m haufe s müller k r single trial analysis and classification of erp components a tutorial neuroimage vidaurre c krämer n blankertz b schlögl a time domain parameters as a feature for eeg based brain computer interfaces neural netw hoffmann u vesin j m ebrahimi t diserens k an efficient based brain computer interface for disabled subjects j neurosci methods burges c j c a tutorial on support vector machines for pattern recognition data min knowl discov xiang l dezhong y wu d chaoyi l combining spatial filters for the classification of single trial eeg in a finger movement task ieee trans biomed eng cover t m geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition ieee trans electron comput ec garcia g n ebrahimi t vesin j m support vector eeg classification in the fourier and time frequency correlation domains in proceedings of the first international ieee embs conference on neural engineering ner capri island italy march pp 240 blankertz b curio g müller k classifying single trial eeg towards brain computer interfacing in proceedings of the advances in neural information processing systems anps vancouver bc canada december pp haihong z cuntai g chuanchu w asynchronous based brain computer interfaces a computational approach with statistical models ieee trans biomed eng kaper m meinicke p grossekathoefer u lingner t ritter h bci competition data set iib support vector machines for the speller paradigm ieee trans biomed eng 1076 meinicke p kaper m hoppe f heumann m ritter h improving transfer rates in brain computer interfacing a case study in proceedings of the advances in neural inf proc systems nips vancouver bc canada december pp thulasidas m cuntai g jiankang w robust classification of eeg signal for brain computer interface ieee trans neural sys rehabil eng sensors ruiting y gray d a ng b w mingyi h comparative analysis of signal processing in brain computer interface in proceedings of the ieee conference on industrial electronics and applications iciea xi an china may pp liu g huang g meng j zhang d zhu x improved gmm with parameter initialization for unsupervised adaptation of brain computer interface int j numer meth biomed eng lemm s schafer c curio g bci competition data set iii probabilistic modeling of sensorimotor mu rhythms for classification of imaginary hand movements ieee trans biomed eng 1080 pires g castelo branco m nunes u visual based bci to steer a wheelchair a bayesian approach in proceedings of the annual international conference of the ieee engineering in medicine and biology society embs vancouver canada august pp palaniappan r brain computer interface design using band powers extracted during mental tasks in proceedings of the international ieee embs conference on neural engineering ner arlington va usa march pp nakayaman k inagaki k a brain computer interface based on neural network with efficient pre processing in proceedings of the international symposium on intelligent signal processing and communications ispacs yonago japan december pp haselsteiner e pfurtscheller g using time dependent neural networks for eeg classification ieee trans rehabil eng coyle d mcginnity t m prasad g improving the separability of multiple eeg features for a bci by neural time series prediction preprocessing biomed signal process control hazrati m k erfanian a an online eeg based brain computer interface for controlling hand grasp using an adaptive probabilistic neural network med eng phys ting w guo zheng y bang hua y hong s eeg feature extraction based on wavelet packet decomposition for brain computer interface measurement palaniappan r paramesran r nishida s saiwaki n a new brain computer interface design using fuzzy artmap ieee trans neural sys rehabil eng felzer t freisieben b analyzing eeg signals using the probability estimating guarded neural classifier ieee trans neural syst rehabil eng kübler a birbaumer n brain computer interfaces and communication in paralysis extinction of goal directed thinking in completely paralysed patients clin neurophysiol 2666 betke m gips j fleming p the camera mouse visual tracking of body features to provide computer access for people with severe disabilities ieee trans neural sys rehabil eng miller l a stubblefield k a lipschutz r d lock b a kuiken t a improved myoelectric prosthesis control using targeted reinnervation surgery a case series ieee trans neural sys rehabil eng sensors treder m s blankertz b c overt attention and visual speller design in an erp based brain computer interface behav brain funct pfurtscheller g allison b z bauernfeind g n brunner c solis escalante t scherer r zander t o mueller putz g neuper c birbaumer n the hybrid bci front neurosci doi 9081 liu y zhou z hu d gaze independent brain computer speller with covert visual search tasks clin neurophysiol 1136 treder m s schmidt n m blankertz b gaze independent brain computer interfaces based on covert attention and feature attention j neural eng moore m m real world applications for brain computer interface technology ieee trans neural syst rehabil eng birbaumer n ghanayim n hinterberger t iversen i kotchoubey b kubler a perelmouter j taub e flor h a spelling device for the paralysed nature chambayil b singla r jha r virtual keyboard bci using eye blinks in eeg in proceedings of the ieee international conference on wireless and mobile computing networking and communications wimob niagara falls on canada october pp obermaier b muller g r pfurtscheller g virtual keyboard controlled by spontaneous eeg activity ieee trans neural sys rehabil eng silvoni s volpato c cavinato m marchetti m priftis k merico a tonin p koutsikos k beverina f piccione f based brain computer interface communication evaluation and follow up in amyotrophic lateral sclerosis front neurosci intendix available online http www intendix com accessed on july fazel rezai r abhari k a region based speller for brain computer interface can j elect comput e 34 ahi s t kambara h koike y a dictionary driven speller with a modified interface ieee trans neural syst rehabil eng halder s rea m andreoni r nijboer f hammer e m kleih s c birbaumer n kübler a an auditory oddball brain computer interface for binary choices clin neurophysiol klobassa d s vaughan t m brunner p schwartz n e wolpaw j r neuper c sellers e w toward a high throughput auditory based brain computer interface clin neurophysiol kübler a furdea a halder s hammer e m nijboer f kotchoubey b a brain computer interface controlled auditory event related potential spelling system for locked in patients ann n y acad sci karim a a hinterberger t richter j mellinger j neumann n flor h kübler a birbaumer n neural internet web surfing with brain potentials for the completely paralyzed neurorehabil neural repair bensch m karim a a mellinger j hinterberger t tangermann m bogdan m rosenstiel w birbaumer n nessi an eeg controlled web browser for severely paralyzed patients comput intell neurosci sensors braz g p russold m davis g m functional electrical stimulation control of standing and stepping after spinal cord injury a review of technical characteristics neuromodulation technol neural interface lauer r t peckham p h kilgore k l eeg based control of a hand grasp neuroprosthesis neuroreport pfurtscheller g müller g r pfurtscheller j gerner h j rupp r thought control of functional electrical stimulation to restore hand grasp in a patient with tetraplegia neurosci lett hu x l tong k y li r chen m xue j j ho s k chen p n post stroke wrist rehabilitation assisted with an intention driven functional electrical stimulation fes robot system in proceedings of the ieee international conference on rehabilitation robotics icorr zurich switzerland july pp pfurtscheller g guger c müller g krausz g neuper c brain oscillations control hand orthosis in a tetraplegic neurosci lett müller putz g r scherer r pfurtscheller g rupp r eeg based neuroprosthesis control a step towards clinical practice neurosci lett buch e weber c cohen l g braun c dimyan m a ard t mellinger j caria a soekadar s fourkas a birbaumer n think to move a neuromagnetic brain computer interface bci system for chronic stroke stroke braun c schweizer r elbert t birbaumer n taub e differential activation in somatosensory cortex for different discrimination tasks j neurosci broetz d braun c weber c soekadar s r caria a birbaumer n combination of brain computer interface training and goal directed physical therapy in chronic stroke a case report neurorehabil neural repair caria a weber c brötz d ramos a ticini l f gharabaghi a braun c birbaumer n chronic stroke recovery after combined bci training and physiotherapy a case report psychophysiology hochberg l r serruya m d friehs g m mukand j a saleh m caplan a h branner a chen d penn r d donoghue j p neuronal ensemble control of prosthetic devices by a human with tetraplegia nature serruya m d hatsopoulos n g paninski l fellows m r donoghue j p brain machine interface instant neural control of a movement signal nature millan j r renkens f mourino j gerstner w noninvasive brain actuated control of a mobile robot by human eeg ieee trans biomed eng 1033 rebsamen b burdet e cuntai g chee leong t qiang z ang m laugier c controlling a wheelchair using a bci with low information transfer rate in proceedings of the ieee international conference on rehabilitation robotics icorr noordwijk the netherlands june pp 1008 rebsamen b cuntai g haihong z chuanchu w cheeleong t ang m h burdet e a brain controlled wheelchair to navigate in familiar environments ieee trans neural syst rehabil eng sensors iturrate i antelis j m kubler a minguez j a noninvasive brain actuated wheelchair based on a neurophysiological protocol and automated navigation ieee trans robot philips j del r millan j vanacker g lew e galan f ferrez p w van brussel h nuttin m adaptive shared control of a brain actuated simulated wheelchair in proceedings of the ieee international conference on rehabilitation robotics icorr noordwijk the netherlands june pp 414 vanacker g del r millán j lew e context based filtering for assisted brain actuated wheelchair driving comput intell neurosci fun of gaming measuring the human experience of media enjoyment available online http fuga aalto fi accessed on july roman k benjamin b gabriel c klaus robert m the berlin brain computer interface bbci towards a new communication channel for online control in gaming applications multimed tools appl tangermann m krauledat m grzeska k sagebaum m blankertz b vidaurre c müller k playing pinball with non invasive bci proc nips 298 middendorf m mcmillan g calhoun g jones k s brain computer interfaces based on the steady state visual evoked response ieee trans rehabil eng lalor e c kelly s p finucane c steady state vep based brain computer interface control in an immersive gaming environment j appl signal proc finke a lenhardt a ritter h the mindgame a based brain computer interface game neural netw microsoft research computational user experiences brain computer interfaces available online http research microsoft com en us um redmond groups cue bci accessed on july angelakis e stathopoulou s frymiare j green d lubar j kounios j eeg neurofeedback a brief overview and an example of peak alpha frequency training for cognitive enhancement in the elderly clin neuropsychol hanslmayr s sauseng p doppelmayr m schabus m klimesch w increasing individual upper alpha power by neurofeedback improves cognitive performance in human subjects appl psychophysiol biofeedback rota g sitaram r veit r erb m weiskopf n dogil g birbaumer n self regulation of regional cortical activity using real time fmri the right inferior frontal gyrus and linguistic processing hum brain mapp sitaram r lee s ruiz s rana m veit r birbaumer n real time support vector classification and feedback of multiple emotional brain states neuroimage decharms r c maeda f glover g h ludlow d pauly j m soneji d gabrieli j d e mackey s c control over brain activation and pain learned by using real time functional mri proc natl acad sci usa 18631 walker j e kozlowski g p neurofeedback treatment of epilepsy child adolesc psychiatr clin n am sensors 308 sterman m egner t foundation and practice of neurofeedback for the treatment of epilepsy appl psychophysiol biofeedback strehl u leins u goth g klinger c hinterberger t birbaumer n self regulation of slow cortical potentials a new treatment for children with attention deficit hyperactivity disorder pediatrics schneider f rockstroh b heimann h lutzenberger w mattes r elbert t birbaumer n bartels m self regulation of slow cortical potentials in psychiatric patients schizophrenia appl psychophysiol biofeedback schneider f heimann h mattes r lutzenberger w birbaumer n self regulation of slow cortical potentials in psychiatric patients depression appl psychophysiol biofeedback 312 schneider f elbert t heimann h welker a stetter f mattes r birbaumer n mann k self regulation of slow cortical potentials in psychiatric patients alcohol dependency appl psychophysiol biofeedback renaud p joyal c stoleru s goyette m weiskopf n birbaumer n real time functional magnetic imaging brain computer interface and virtual reality promising tools for the treatment of pedophilia in progress in brain research green a m chapman c e kalaska j f lepore f eds elsevier new york ny usa volume pp georgopoulos a p karageorgiou e leuthold a c lewis s m lynch j k alonso a a aslam z carpenter a f georgopoulos a hemmy l s et al synchronous neural interactions assessed by magnetoencephalography a functional biomarker for brain disorders j neural eng doi 2560 cook i a warren c pajot s k schairer d leuchter a f regional brain activation with advertising images j neurosci psychol econ ambler t ioannides a rose s brands on the brain neuro images of advertising bus strateg rev vecchiato g de vico fallani f astolfi l toppi j cincotti f mattia d salinari s babiloni f the issue of multiple univariate comparisons in the context of neuroelectric brain mapping an application in a neuromarketing experiment j neurosci methods ambler t braeutigam s stins j rose s swithenby s salience and choice neural correlates of shopping decisions psychol market ariely d berns g s neuromarketing the hope and hype of neuroimaging in business nat rev neurosci 292 neuromarketing neuroscientific consumer testing neurofocus available online http www neurofocus com accessed on december neuroconsult available online http www neuroconsult at accessed on october neuro insight available online http www neuro insight com accessed on october emsense quantitative biosensory metrics available online http www emsense com accessed on october right now we work with online information using two main tools search and links we type keywords into a search engine and find a set of documents related to them we look at the documents in that set possibly navigating to other linked documents this is a powerful way of interacting with our online archive but something is missing imagine searching and exploring documents based on the themes that run through them we might zoom in and zoom out to find specific or broader themes we might look at how those themes changed through time or how they are connected to each other rather than finding documents through keyword search alone we might first find the theme that we are interested in and then examine the documents related to that theme for example consider using themes to explore the complete history of the new york times at a broad level some of the themes might correspond to the sections of the newspaper foreign policy national affairs sports we could zoom in on a theme of interest such as foreign policy to reveal various aspects of it chinese foreign policy the conflict in the middle east the u s relationship with russia we could then navigate through time to reveal how these specific themes have changed tracking for example the changes in the conflict in the middle east over the last years and in all of this exploration we would be pointed to the original articles relevant to the themes the thematic structure would be a new kind of window through which to explore and digest the collection but we do not interact with electronic archives in this way while more and more texts are available online we simply do not have the human power to read and study them to provide the kind of browsing experience described above to this end machine learning researchers have developed probabilistic topic modeling a suite of algorithms that aim to discover and annotate large archives of documents with thematic information topic modeling algorithms are statistical methods that analyze the words of the original texts to discover the themes that run through them how those themes are connected to each other and how they change over time see for example figure for topics found by analyzing the yale law journal topic modeling algorithms do not require any prior annotations or labeling of the documents the topics emerge from the analysis of the original texts topic modeling enables us to organize and summarize electronic archives at a scale that would be impossible by human annotation latent dirichlet allocation we first describe the basic ideas behind latent dirichlet allocation lda which is the simplest topic model the intuition behind lda is that documents exhibit multiple topics for example consider the article in figure this article entitled seeking life bare genetic necessities is about using data analysis to determine the number of genes an organism needs to survive in an evolutionary sense by hand we have highlighted different words that are used in the article words about data analysis such as computer and prediction are highlighted in blue words about evolutionary biology such as life and organism are highlighted in pink words about genetics such as sequenced and genes are highlighted in yellow if we took the time to highlight every word in the article you would see that this article blends genetics data analysis and evolutionary biology in different proportions we exclude words such as and but or if which contain little topical content furthermore knowing that this article blends those topics would help you situate it in a collection of scientific articles lda is a statistical model of document collections that tries to capture this intuition it is most easily described by its generative process the imaginary random process by which the model assumes the documents arose the interpretation of lda as a probabilistic model is fleshed out later we formally define a topic to be a distribution over a fixed vocabulary for example the genetics topic has words about genetics with high probability and the evolutionary biology topic has words about evolutionary biology with high probability we assume that these topics are specified before any data has been generated a now for each document in the collection we generate the words in a two stage process randomly choose a distribution over topics for each word in the document randomly choose a topic from the distribution over topics in step randomly choose a word from the corresponding distribution over the vocabulary this statistical model reflects the intuition that documents exhibit multiple topics each document exhibits the topics in different proportion step each word in each document is drawn from one of the topics step where the selected topic is chosen from the per document distribution over topics step b in the example article the distribution over topics would place probability on genetics data analysis and evolutionary biology and each word is drawn from one of those three topics notice that the next article in the collection might be about data analysis and neuroscience its distribution over topics would place probability on those two topics this is the distinguishing characteristic of latent dirichlet allocation all the documents in the collection share the same set of topics but each document exhibits those topics in different proportion as we described in the introduction the goal of topic modeling is to automatically discover the topics from a collection of documents the documents themselves are observed while the topic structure the topics per document topic distributions and the per document per word topic assignments is hidden structure the central computational problem for topic modeling is to use the observed documents to infer the hidden topic structure this can be thought of as reversing the generative process what is the hidden structure that likely generated the observed collection figure illustrates example inference using the same example document from figure here we took articles from science magazine and used a topic modeling algorithm to infer the hidden topic structure the algorithm assumed that there were topics we then computed the inferred topic distribution for the example article figure left the distribution over topics that best describes its particular collection of words notice that this topic distribution though it can use any of the topics has only activated a handful of them further we can examine the most probable terms from each of the most probable topics figure right on examination we see that these terms are recognizable as terms about genetics survival and data analysis the topics that are combined in the example article we emphasize that the algorithms have no information about these subjects and the articles are not labeled with topics or keywords the interpretable topic distributions arise by computing the hidden structure that likely generated the observed collection of documents c for example figure illustrates topics discovered from yale law journal here the number of topics was set to be topics about subjects like genetics and data analysis are replaced by topics about discrimination and contract law the utility of topic models stems from the property that the inferred hidden structure resembles the thematic structure of the collection this interpretable hidden structure annotates each document in the collection a task that is painstaking to perform by hand and these annotations can be used to aid tasks like information retrieval classification and corpus exploration d in this way topic modeling provides an algorithmic solution to managing organizing and annotating large archives of texts lda and probabilistic models lda and other topic models are part of the larger field of probabilistic modeling in generative probabilistic modeling we treat our data as arising from a generative process that includes hidden variables this generative process defines a joint probability distribution over both the observed and hidden random variables we perform data analysis by using that joint distribution to compute the conditional distribution of the hidden variables given the observed variables this conditional distribution is also called the posterior distribution lda falls precisely into this framework the observed variables are the words of the documents the hidden variables are the topic structure and the generative process is as described here the computational problem of inferring the hidden topic structure from the documents is the problem of computing the posterior distribution the conditional distribution of the hidden variables given the documents we can describe lda more formally with the following notation the topics are k where each βk is a distribution over the vocabulary the distributions over words at left in figure the topic proportions for the dth document are θd where θd k is the topic proportion for topic k in document d the cartoon histogram in figure the topic assignments for the dth document are zd where zd n is the topic assignment for the nth word in document d the colored coin in figure finally the observed words for document d are wd where wd n is the nth word in document d which is an element from the fixed vocabulary notice that this distribution specifies a number of dependencies for example the topic assignment zd n depends on the per document topic proportions θd as another example the observed word wd n depends on the topic assignment zd n and all of the topics k operationally that term is defined by looking up as to which topic zd n refers to and looking up the probability of the word wd n within that topic these dependencies define lda they are encoded in the statistical assumptions behind the generative process in the particular mathematical form of the joint distribution and in a third way in the probabilistic graphical model for lda probabilistic graphical models provide a graphical language for describing families of probability distributions e the graphical model for lda is in figure these three representations are equivalent ways of describing the probabilistic assumptions behind lda in the next section we describe the inference algorithms for lda however we first pause to describe the short history of these ideas lda was developed to fix an issue with a previously developed probabilistic model probabilistic latent semantic analysis plsi that model was itself a probabilistic version of the seminal work on latent semantic analysis which revealed the utility of the singular value decomposition of the document term matrix from this matrix factorization perspective lda can also be seen as a type of principal component analysis for discrete data posterior computation for lda we now turn to the computational problem computing the conditional distribution of the topic structure given the observed documents as we mentioned this is called the posterior using our notation the posterior is the numerator is the joint distribution of all the random variables which can be easily computed for any setting of the hidden variables the denominator is the marginal probability of the observations which is the probability of seeing the observed corpus under any topic model in theory it can be computed by summing the joint distribution over every possible instantiation of the hidden topic structure that number of possible topic structures however is exponentially large this sum is intractable to compute f as for many modern probabilistic models of interest and for much of modern bayesian statistics we cannot compute the posterior because of the denominator which is known as the evidence a central research goal of modern probabilistic modeling is to develop efficient methods for approximating it topic modeling algorithms like the algorithms used to create figures and are often adaptations of general purpose methods for approximating the posterior distribution topic modeling algorithms form an approximation of equation by adapting an alternative distribution over the latent topic structure to be close to the true posterior topic modeling algorithms generally fall into two categories sampling based algorithms and variational algorithms sampling based algorithms attempt to collect samples from the posterior to approximate it with an empirical distribution the most commonly used sampling algorithm for topic modeling is gibbs sampling where we construct a markov chain a sequence of random variables each dependent on the previous whose limiting distribution is the posterior the markov chain is defined on the hidden topic variables for a particular corpus and the algorithm is to run the chain for a long time collect samples from the limiting distribution and then approximate the distribution with the collected samples often just one sample is collected as an approximation of the topic structure with maximal probability see steyvers and for a good description of gibbs sampling for lda and see http cran r project org package lda for a fast open source implementation variational methods are a deterministic alternative to sampling based algorithms rather than approximating the posterior with samples variational methods posit a parameterized family of distributions over the hidden structure and then find the member of that family that is closest to the posterior g thus the inference problem is transformed to an optimization problem variational methods open the door for innovations in optimization to have practical impact in probabilistic modeling see blei et al for a coordinate ascent variational inference algorithm for lda see hoffman et al for a much faster online algorithm and open source software that easily handles millions of documents and can accommodate streaming collections of text loosely speaking both types of algorithms perform a search over the topic structure a collection of documents the observed random variables in the model are held fixed and serve as a guide toward where to search which approach is better depends on the particular topic model being used we have so far focused on lda but see below for other topic models and is a source of academic debate for a good discussion of the merits and drawbacks of both see asuncion et al research in topic modeling the simple lda model provides a powerful tool for discovering and exploiting the hidden thematic structure in large archives of text however one of the main advantages of formulating lda as a probabilistic model is that it can easily be used as a module in more complicated models for more complicated goals since its introduction lda has been extended and adapted in many ways relaxing the assumptions of lda lda is defined by the statistical assumptions it makes about the corpus one active area of topic modeling research is how to relax and extend these assumptions to uncover more sophisticated structure in the texts one assumption that lda makes is the bag of words assumption that the order of the words in the document does not matter to see this note that the joint distribution of equation remains invariant to permutation of the words of the documents while this assumption is unrealistic it is reasonable if our only goal is to uncover the course semantic structure of the texts h for more sophisticated goals such as language generation it is patently not appropriate there have been a number of extensions to lda that model words nonexchangeably for example developed a topic model that relaxes the bag of words assumption by assuming that the topics generate words conditional on the previous word griffiths et al developed a topic model that switches between lda and a standard hmm these models expand the parameter space significantly but show improved language modeling performance another assumption is that the order of documents does not matter again this can be seen by noticing that equation remains invariant to permutations of the ordering of documents in the collection this assumption may be unrealistic when analyzing long running collections that span years or centuries in such collections we may want to assume that the topics change over time one approach to this problem is the dynamic topic a model that respects the ordering of the documents and gives a richer posterior topical structure than lda figure shows a topic that results from analyzing all of science magazine under the dynamic topic model rather than a single distribution over words a topic is now a sequence of distributions over words we can find an underlying theme of the collection and track how it has changed over time one direction for topic modeling is to develop evaluation methods that match how the algorithms are used how can we compare topic models based on how interpretable they are a third assumption about lda is that the number of topics is assumed known and fixed the bayesian non parametric topic provides an elegant solution the number of topics is determined by the collection during posterior inference and furthermore new documents can exhibit previously unseen topics bayesian nonparametric topic models have been extended to hierarchies of topics which find a tree of topics moving from more general to more concrete whose particular structure is inferred from the data there are still other extensions of lda that relax various assumptions made by the model the correlated topic and pachinko allocation allow the occurrence of topics to exhibit correlation for example a document about geology is more likely to also be about chemistry than it is to be about sports the spherical topic allows words to be unlikely in a topic for example wrench will be particularly unlikely in a topic about cats sparse topic models enforce further structure in the topic distributions and bursty topic models provide a more realistic model of word counts incorporating metadata in many text analysis settings the documents contain additional information such as author title geographic location links and others that we might want to account for when fitting a topic model there has been a flurry of research on adapting topic models to include metadata the author topic is an early success story for this kind of research the topic proportions are attached to authors papers with multiple authors are assumed to attach each word to an author drawn from a topic drawn from his or her topic proportions the author topic model allows for inferences about authors as well as documents rosen zvi et al show examples of author similarity based on their topic proportions such computations are not possible with lda many document collections are linked for example scientific papers are linked by citation or web pages are linked by hyperlink and several topic models have been developed to account for those links when estimating the topics the relational topic model of chang and assumes that each document is modeled as in lda and that the links between documents depend on the distance between their topic proportions this is both a new topic model and a new network model unlike traditional statistical models of networks the relational topic model takes into account node attributes here the words of the documents in modeling the links other work that incorporates metadata into topic models includes models of linguistic structure models that account for distances between corpora and models of named entities general purpose methods for incorporating metadata into topic models include dirichlet multinomial regression and supervised topic models other kinds of data in lda the topics are distributions over words and this discrete distribution generates observations words in documents one advantage of lda is that these choices for the topic parameter and data generating distribution can be adapted to other kinds of observations with only small changes to the corresponding inference algorithms as a class of models lda can be thought of as a mixed membership model of grouped data rather than associating each group of observations document with one component topic each group exhibits multiple components in different proportions lda like models have been adapted to many kinds of data including survey data user preferences audio and music computer code network logs and social networks we describe two areas where mixed membership models have been particularly successful in population genetics the same probabilistic model was independently invented to find ancestral populations for example originating from africa europe the middle east among others in the genetic ancestry of a sample of individuals the idea is that each individual genotype descends from one or more of the ancestral populations using a model much like lda biologists can both characterize the genetic patterns in those populations the topics and identify how each individual expresses them the topic proportions this model is powerful because the genetic patterns in ancestral populations can be hypothesized even when pure samples from them are not available lda has been widely used and adapted in computer vision where the inference algorithms are applied to natural images in the service of image retrieval classification and organization computer vision researchers have made a direct analogy from images to documents in document analysis we assume that documents exhibit multiple topics and the collection of documents exhibits the same set of topics in image analysis we assume that each image exhibits a combination of visual patterns and that the same visual patterns recur throughout a collection of images in a preprocessing step the images are analyzed to form collections of visual words topic modeling for computer vision has been used to classify images connect images and captions build image hierarchies and other applications research in topic modeling the simple lda model provides a powerful tool for discovering and exploiting the hidden thematic structure in large archives of text however one of the main advantages of formulating lda as a probabilistic model is that it can easily be used as a module in more complicated models for more complicated goals since its introduction lda has been extended and adapted in many ways relaxing the assumptions of lda lda is defined by the statistical assumptions it makes about the corpus one active area of topic modeling research is how to relax and extend these assumptions to uncover more sophisticated structure in the texts one assumption that lda makes is the bag of words assumption that the order of the words in the document does not matter to see this note that the joint distribution of equation remains invariant to permutation of the words of the documents while this assumption is unrealistic it is reasonable if our only goal is to uncover the course semantic structure of the texts h for more sophisticated goals such as language generation it is patently not appropriate there have been a number of extensions to lda that model words nonexchangeably for example developed a topic model that relaxes the bag of words assumption by assuming that the topics generate words conditional on the previous word griffiths et al developed a topic model that switches between lda and a standard hmm these models expand the parameter space significantly but show improved language modeling performance another assumption is that the order of documents does not matter again this can be seen by noticing that equation remains invariant to permutations of the ordering of documents in the collection this assumption may be unrealistic when analyzing long running collections that span years or centuries in such collections we may want to assume that the topics change over time one approach to this problem is the dynamic topic a model that respects the ordering of the documents and gives a richer posterior topical structure than lda figure shows a topic that results from analyzing all of science magazine under the dynamic topic model rather than a single distribution over words a topic is now a sequence of distributions over words we can find an underlying theme of the collection and track how it has changed over time one direction for topic modeling is to develop evaluation methods that match how the algorithms are used how can we compare topic models based on how interpretable they are a third assumption about lda is that the number of topics is assumed known and fixed the bayesian non parametric topic provides an elegant solution the number of topics is determined by the collection during posterior inference and furthermore new documents can exhibit previously unseen topics bayesian nonparametric topic models have been extended to hierarchies of topics which find a tree of topics moving from more general to more concrete whose particular structure is inferred from the data there are still other extensions of lda that relax various assumptions made by the model the correlated topic and pachinko allocation allow the occurrence of topics to exhibit correlation for example a document about geology is more likely to also be about chemistry than it is to be about sports the spherical topic allows words to be unlikely in a topic for example wrench will be particularly unlikely in a topic about cats sparse topic models enforce further structure in the topic distributions and bursty topic models provide a more realistic model of word counts incorporating metadata in many text analysis settings the documents contain additional information such as author title geographic location links and others that we might want to account for when fitting a topic model there has been a flurry of research on adapting topic models to include metadata the author topic is an early success story for this kind of research the topic proportions are attached to authors papers with multiple authors are assumed to attach each word to an author drawn from a topic drawn from his or her topic proportions the author topic model allows for inferences about authors as well as documents rosen zvi et al show examples of author similarity based on their topic proportions such computations are not possible with lda many document collections are linked for example scientific papers are linked by citation or web pages are linked by hyperlink and several topic models have been developed to account for those links when estimating the topics the relational topic model of chang and assumes that each document is modeled as in lda and that the links between documents depend on the distance between their topic proportions this is both a new topic model and a new network model unlike traditional statistical models of networks the relational topic model takes into account node attributes here the words of the documents in modeling the links other work that incorporates metadata into topic models includes models of linguistic structure models that account for distances between corpora and models of named entities general purpose methods for incorporating metadata into topic models include dirichlet multinomial regression and supervised topic models other kinds of data in lda the topics are distributions over words and this discrete distribution generates observations words in documents one advantage of lda is that these choices for the topic parameter and data generating distribution can be adapted to other kinds of observations with only small changes to the corresponding inference algorithms as a class of models lda can be thought of as a mixed membership model of grouped data rather than associating each group of observations document with one component topic each group exhibits multiple components in different proportions lda like models have been adapted to many kinds of data including survey data user preferences audio and music computer code network logs and social networks we describe two areas where mixed membership models have been particularly successful in population genetics the same probabilistic model was independently invented to find ancestral populations for example originating from africa europe the middle east among others in the genetic ancestry of a sample of individuals the idea is that each individual genotype descends from one or more of the ancestral populations using a model much like lda biologists can both characterize the genetic patterns in those populations the topics and identify how each individual expresses them the topic proportions this model is powerful because the genetic patterns in ancestral populations can be hypothesized even when pure samples from them are not available lda has been widely used and adapted in computer vision where the inference algorithms are applied to natural images in the service of image retrieval classification and organization computer vision researchers have made a direct analogy from images to documents in document analysis we assume that documents exhibit multiple topics and the collection of documents exhibits the same set of topics in image analysis we assume that each image exhibits a combination of visual patterns and that the same visual patterns recur throughout a collection of images in a preprocessing step the images are analyzed to form collections of visual words topic modeling for computer vision has been used to classify images connect images and captions build image hierarchies and other applications future directions topic modeling is an emerging field in machine learning and there are many exciting new directions for research evaluation and model checking there is a disconnect between how topic models are evaluated and why we expect topic models to be useful typically topic models are evaluated in the following way first hold out a subset of your corpus as the test set then fit a variety of topic models to the rest of the corpus and approximate a measure of model fit for example probability for each trained model on the test set finally choose the model that achieves the best held out performance but topic models are often used to organize summarize and help users explore large corpora and there is no technical reason to suppose that held out accuracy corresponds to better organization or easier interpretation one open direction for topic modeling is to develop evaluation methods that match how the algorithms are used how can we compare topic models based on how interpretable they are this is the model checking problem when confronted with a new corpus and a new task which topic model should i use how can i decide which of the many modeling assumptions are important for my goals how should i move between the many kinds of topic models that have been developed these questions have been given some attention by statisticians but they have been scrutinized less for the scale of problems that machine learning tackles new computational answers to these questions would be a significant contribution to topic modeling visualization and user interfaces another promising future direction for topic modeling is to develop new methods of interacting with and visualizing topics and corpora topic models provide new exploratory structure in large collections how can we best exploit that structure to aid in discovery and exploration one problem is how to display the topics typically we display topics by listing the most frequent words of each see figure but new ways of labeling the topics by either choosing different words or displaying the chosen words differently may be more effective a further problem is how to best display a document with a topic model at the document level topic models provide potentially useful information about the structure of the document combined with effective topic labels this structure could help readers identify the most interesting parts of the document moreover the hidden topic proportions implicitly connect each document to the other documents by considering a distance measure between topic proportions how can we best display these connections what is an effective interface to the whole corpus and its inferred topic structure these are user interface questions and they are essential to topic modeling topic modeling algorithms show much promise for uncovering meaningful thematic structure in large collections of documents but making this structure useful requires careful attention to information visualization and the corresponding user interfaces topic models for data discovery topic models have been developed with information engineering applications in mind as a statistical model however topic models should be able to tell us something or help us form a hypothesis about the data what can we learn about the language and other data based on the topic model posterior some work in this area has appeared in political science bibliometrics and psychology this kind of research adapts topic models to measure an external variable of interest a difficult task for unsupervised learning that must be carefully validated in general this problem is best addressed by teaming computer scientists with other scholars to use topic models to help explore visualize and draw hypotheses from their data in addition to scientific applications such as genetics and neuroscience one can imagine topic models coming to the service of history sociology linguistics political science legal studies comparative literature and other fields where texts are a primary object of study by working with scholars in diverse fields we can begin to develop a new interdisciplinary computational methodology for working with and drawing conclusions from archives of texts back to top summary we have surveyed probabilistic topic models a suite of algorithms that provide a statistical solution to the problem of managing large archives of documents with recent scientific advances in support of unsupervised machine learning flexible components for modeling scalable algorithms for posterior inference and increased access to massive datasets topic models promise to be an important component for summarizing and understanding our growing digitized archive of information a few useful things to know about machine learning automatically learn programs from data this is often a very attractive alternative to manually constructing them and in the key insights last decade the use of machine learning has spread rapidly throughout computer science and beyond machine learning is used in web search spam filters machine learning algorithms can figure out how to perform important tasks by generalizing from examples this is often feasible and cost effective where recommender systems ad placement credit scoring manual programming is not as more data becomes available more ambitious fraud detection stock trading drug design and many problems can be tackled other applications a recent report from the mckinsey global institute asserts that machine learning a k a machine learning is widely used in computer science and other fields however developing successful data mining or predictive analytics will be the driver machine learning applications requires a substantial amount of black art that is of the next big wave of innovation several fine difficult to find in textbooks textbooks are available to interested practitioners and researchers for example and witten et al however much of the folk knowledge that this article summarizes key lessons that machine learning researchers and practitioners have learned these include pitfalls to avoid important issues to focus on and answers to common questions communications of the acm october vol no is needed to successfully develop machine learning applications is not readily available in them as a result many machine learning projects take much longer than necessary or wind up producing less than ideal results yet much of this folk knowledge is fairly easy to communicate this is the purpose of this article many different types of machine learning exist but for illustration purposes i will focus on the most mature and widely used one clas sification nevertheless the issues i will discuss apply across all of ma chine learning a classifier is a sys tem that inputs typically a vector of discrete and or continuous fea ture values and outputs a single dis crete value the class for example a spam filter classifies email mes sages into spam or not spam and its input may be a boolean vec tor x x x j x d where x j if the jth word in the dictionary appears in the email and x j otherwise a learner inputs a training set of ex amples x i y i where x i x i x i d is an observed input and y i is the corresponding output and outputs a classifier the test of the learner is whether this classifier produces the correct output y t for future examples x t for example whether the spam filter correctly classifies previously unseen email messages as spam or not spam learning representation evaluation optimization suppose you have an application that you think machine learning might be good for the first problem facing you is the bewildering variety of learning al gorithms available which one to use there are literally thousands available and hundreds more are published each year the key to not getting lost in this huge space is to realize that it consists of combinations of just three compo nents the components are representation a classifier must be represented in some formal lan guage that the computer can handle conversely choosing a representa tion for a learner is tantamount to choosing the set of classifiers that it can possibly learn this set is called the hypothesis space of the learner if a classifier is not in the hypothesis space it cannot be learned a related question that i address later is how to represent the input in other words what features to use evaluation an evaluation func tion also called objective function october vol no communications of the acm or scoring function is needed to dis tinguish good classifiers from bad ones the evaluation function used internally by the algorithm may dif fer from the external one that we want the classifier to optimize for ease of optimization and due to the issues i will discuss optimization finally we need a method to search among the clas sifiers in the language for the high est scoring one the choice of op timization technique is key to the efficiency of the learner and also helps determine the classifier pro duced if the evaluation function has more than one optimum it is com mon for new learners to start out using off the shelf optimizers which are lat er replaced by custom designed ones the accompanying table shows common examples of each of these three components for example k nearest neighbor classifies a test ex ample by finding the k most similar training examples and predicting the majority class among them hyper plane based methods form a linear review articles combination of the features per class and predict the class with the high est valued combination decision trees test one feature at each internal node with one branch for each fea ture value and have class predictions at the leaves algorithm above shows a bare bones decision tree learner for boolean domains using information gain and greedy search infogain x j y is the mutual informa tion between feature x j and the class y makenode x c c returns a node that tests feature x and has c as the child for x and c as the child for x of course not all combinations of one component from each column of the table make equal sense for exam ple discrete representations naturally go with combinatorial optimization and continuous ones with continu ous optimization nevertheless many learners have both discrete and con tinuous components and in fact the communications of the acm october vol no table the three components of learning algorithms representation evaluation optimization instances accuracy error rate combinatorial optimization k nearest neighbor precision and recall greedy search support vector machines squared error beam search hyperplanes likelihood branch and bound naive bayes posterior probability continuous optimization logistic regression information gain unconstrained decision trees k l divergence gradient descent sets of rules cost utility conjugate gradient propositional rules margin quasi newton methods logic programs constrained neural networks linear programming graphical models quadratic programming bayesian networks conditional random fields algorithm decision tree induction learndt trainset if all examples in trainset have the same class y then return makeleaf y if no feature x j has infogain x j y then y most frequent class in trainset return makeleaf y x argmax x j infogain x j y ts examples in trainset with x ts examples in trainset with x return makenode x learndt ts learndt ts day may not be far when every single possible combination has appeared in some learner most textbooks are organized by representation and it is easy to over look the fact that the other compo nents are equally important there is no simple recipe for choosing each component but i will touch on some of the key issues here as we will see some choices in a machine learning project may be even more important than the choice of learner it generalization that counts the fundamental goal of machine learning is to generalize beyond the examples in the training set this is because no matter how much data we have it is very unlikely that we will see those exact examples again at test time notice that if there are words in the dictionary the spam fil ter described above has pos sible different inputs doing well on the training set is easy just memorize the examples the most common mistake among machine learning be ginners is to test on the training data and have the illusion of success if the chosen classifier is then tested on new data it is often no better than ran dom guessing so if you hire someone to build a classifier be sure to keep some of the data to yourself and test the classifier they give you on it con versely if you have been hired to build a classifier set some of the data aside from the beginning and only use it to test your chosen classifier at the very end followed by learning your final classifier on the whole data contamination of your classifier by test data can occur in insidious ways for example if you use test data to tune parameters and do a lot of tun ing machine learning algorithms have lots of knobs and success of ten comes from twiddling them a lot so this is a real concern of course holding out data reduces the amount available for training this can be mit igated by doing cross validation ran domly dividing your training data into say subsets holding out each one while training on the rest testing each learned classifier on the examples it did not see and averaging the results to see how well the particular param eter setting does in the early days of machine learn ing the need to keep training and test data separate was not widely appreci ated this was partly because if the learner has a very limited representa tion for example hyperplanes the difference between training and test error may not be large but with very flexible classifiers for example deci sion trees or even with linear classifi ers with a lot of features strict separa tion is mandatory notice that generalization being the goal has an interesting conse quence for machine learning unlike in most other optimization problems we do not have access to the function we want to optimize we have to use training error as a surrogate for test error and this is fraught with dan ger how to deal with it is addressed later on the positive side since the objective function is only a proxy for the true goal we may not need to fully optimize it in fact a local optimum returned by simple greedy search may be better than the global optimum data alone is not enough generalization being the goal has an other major consequence data alone is not enough no matter how much of it you have consider learning a boolean function of say vari ables from a million examples there are examples whose classes you do not know how do you figure out what those classes are in the ab sence of further information there is just no way to do this that beats flip ping a coin this observation was first made in somewhat different form by the philosopher david hume over years ago but even today many mis takes in machine learning stem from failing to appreciate it every learner must embody some knowledge or as sumptions beyond the data it is given in order to generalize beyond it this notion was formalized by wolpert in his famous no free lunch theorems according to which no learner can beat random guessing over all pos sible functions to be learned this seems like rather depressing news how then can we ever hope to learn anything luckily the functions we want to learn in the real world are not drawn uniformly from the set of all mathematically possible functions in fact very general assumptions like smoothness similar examples hav ing similar classes limited depen dences or limited complexity are often enough to do very well and this is a large part of why machine learn ing has been so successful like de duction induction what learners do is a knowledge lever it turns a small amount of input knowledge into a large amount of output knowledge induction is a vastly more powerful lever than deduction requiring much less input knowledge to produce use ful results but it still needs more than zero input knowledge to work and as with any lever the more we put in the more we can get out a corollary of this is that one of the key criteria for choosing a representa tion is which kinds of knowledge are easily expressed in it for example if we have a lot of knowledge about what makes examples similar in our do review articles main instance based methods may one that is accurate on both it be a good choice if we have knowl has overfit edge about probabilistic dependen everyone in machine learning cies graphical models are a good fit knows about overfitting but it comes and if we have knowledge about what in many forms that are not immedi kinds of preconditions are required by ately obvious one way to understand each class if then rules may overfitting is by decomposing gener be the best option the most useful alization error into bias and variance learners in this regard are those that bias is a learner tendency to con do not just have assumptions hard sistently learn the same wrong thing wired into them but allow us to state variance is the tendency to learn ran them explicitly vary them widely and dom things irrespective of the real sig incorporate them automatically into nal figure illustrates this by an anal the learning for example using first ogy with throwing darts at a board a order or linear learner has high bias because in retrospect the need for knowl when the frontier between two classes edge in learning should not be sur is not a hyperplane the learner is un prising machine learning is not able to induce it decision trees do not magic it cannot get something from have this problem because they can nothing what it does is get more represent any boolean function but from less programming like all en on the other hand they can suffer from gineering is a lot of work we have to high variance decision trees learned build everything from scratch learn on different training sets generated by ing is more like farming which lets the same phenomenon are often very nature do most of the work farmers different when in fact they should be combine seeds with nutrients to grow crops learners combine knowledge with data to grow programs figure bias and variance in dart throwing overfitting has many faces what if the knowledge and data we have are not sufficient to completely low variance y c a r u c c a t e t e t high variance determine the correct classifier then we run the risk of just hallucinating a classifier or parts of it that is not high bias grounded in reality and is simply en coding random quirks in the data this problem is called overfitting and is the bugbear of machine learning when your learner outputs a classi low bias fier that is accurate on the train ing data but only accurate on test data when in fact it could have output figure naïve bayes can outperform a state of the art rule learner even when the true classifier is a set of rules bayes 10000 number of examples october vol no communications of the acm review articles the same similar reasoning applies to the choice of optimization meth od beam search has lower bias than greedy search but higher variance be cause it tries more hypotheses thus contrary to intuition a more powerful learner is not necessarily better than a less powerful one figure illustrates this a even though the true classifier is a set of rules with up to examples na ive bayes is more accurate than a rule learner this happens despite naive bayes false assumption that the frontier is linear situations like this are common in machine learn ing strong false assumptions can be better than weak true ones because a learner with the latter needs more data to avoid overfitting cross validation can help to com bat overfitting for example by using it to choose the best size of decision tree to learn but it is no panacea since if we use it to make too many parameter choices it can itself start to overfit besides cross validation there are many methods to combat overfit ting the most popular one is adding a regularization term to the evaluation function this can for example pe nalize classifiers with more structure thereby favoring smaller ones with less room to overfit another option is to perform a statistical significance test like chi square before adding new structure to decide whether the dis tribution of the class really is differ ent with and without this structure these techniques are particularly use ful when data is very scarce neverthe less you should be skeptical of claims that a particular technique solves the overfitting problem it is easy to avoid overfitting variance by falling into the opposite error of underfitting bias simultaneously avoiding both requires learning a perfect classifier and short of knowing it in advance there is no single technique that will always do best no free lunch a common misconception about overfitting is that it is caused by noise a training examples consist of boolean fea tures and a boolean class computed from them according to a set of if then rules the curves are the average of runs with different randomly generated sets of rules error bars are two standard deviations see domingos and for details communications of the acm october vol no like training examples labeled with the wrong class this can indeed ag gravate overfitting by making the learner draw a capricious frontier to keep those examples on what it thinks is the right side but severe overfitting can occur even in the absence of noise for instance suppose we learn a bool ean classifier that is just the disjunc tion of the examples labeled true in the training set in other words the classifier is a boolean formula in disjunctive normal form where each term is the conjunction of the feature values of one specific training exam ple this classifier gets all the training examples right and every positive test example wrong regardless of whether the training data is noisy or not the problem of multiple is closely related to overfitting standard statistical tests assume that only one hypothesis is being tested but mod ern learners can easily test millions before they are done as a result what looks significant may in fact not be for example a mutual fund that beats the market years in a row looks very impressive until you realize that if there are funds and each has a chance of beating the market on any given year it is quite likely that one will succeed all times just by luck this problem can be combatted by correcting the significance tests to take the number of hypotheses into account but this can also lead to un derfitting a better approach is to con trol the fraction of falsely accepted non null hypotheses known as the false discovery rate intuition fails in high dimensions after overfitting the biggest problem in machine learning is the curse of dimensionality this expression was coined by bellman in to refer to the fact that many algorithms that work fine in low dimensions become intractable when the input is high dimensional but in machine learn ing it refers to much more general izing correctly becomes exponentially harder as the dimensionality number of features of the examples grows be cause a fixed size training set covers a dwindling fraction of the input space even with a moderate dimension of and a huge training set of a trillion examples the latter covers only a frac tion of about of the input space this is what makes machine learning both necessary and hard more seriously the similarity based reasoning that machine learn ing algorithms depend on explicitly or implicitly breaks down in high di mensions consider a nearest neigh bor classifier with hamming distance as the similarity measure and sup pose the class is just x x if there are no other features this is an easy problem but if there are irrelevant features x x the noise from them completely swamps the signal in x and x and nearest neighbor effec tively makes random predictions even more disturbing is that near est neighbor still has a problem even if all features are relevant this is because in high dimensions all examples look alike suppose for instance that examples are laid out on a regular grid and consider a test example x t if the grid is d dimen sional x t nearest examples are all at the same distance from it so as the dimensionality increases more and more examples become nearest neighbors of x t until the choice of nearest neighbor and therefore of class is effectively random this is only one instance of a more general problem with high dimen sions our intuitions which come from a three dimensional world of ten do not apply in high dimensional ones in high dimensions most of the mass of a multivariate gaussian dis tribution is not near the mean but in an increasingly distant shell around it and most of the volume of a high dimensional orange is in the skin not the pulp if a constant number of ex amples is distributed uniformly in a high dimensional hypercube beyond some dimensionality most examples are closer to a face of the hypercube than to their nearest neighbor and if we approximate a hypersphere by in scribing it in a hypercube in high di mensions almost all the volume of the hypercube is outside the hypersphere this is bad news for machine learning where shapes of one type are often ap proximated by shapes of another building a classifier in two or three dimensions is easy we can find a rea sonable frontier between examples of different classes just by visual in spection it has even been said that if people could see in high dimensions machine learning would not be neces sary but in high dimensions it is dif ficult to understand what is happen ing this in turn makes it difficult to design a good classifier naively one might think that gathering more fea tures never hurts since at worst they provide no new information about the class but in fact their benefits may be outweighed by the curse of dimen sionality fortunately there is an effect that partly counteracts the curse which might be called the blessing of non uniformity in most applications examples are not spread uniformly throughout the instance space but are concentrated on or near a lower dimensional manifold for example k nearest neighbor works quite well for handwritten digit recognition even though images of digits have one dimension per pixel because the space of digit images is much smaller than the space of all possible images learners can implicitly take advan tage of this lower effective dimension or algorithms for explicitly reducing the dimensionality can be used for example theoretical guarantees are not what they seem machine learning papers are full of theoretical guarantees the most com mon type is a bound on the number of examples needed to ensure good gen eralization what should you make of these guarantees first of all it is re markable that they are even possible induction is traditionally contrasted with deduction in deduction you can guarantee that the conclusions are correct in induction all bets are off or such was the conventional wisdom for many centuries one of the major developments of recent decades has been the realization that in fact we can have guarantees on the results of in duction particularly if we are willing to settle for probabilistic guarantees the basic argument is remarkably simple let say a classifier is bad if its true error rate is greater than ε then the probability that a bad clas sifier is consistent with n random in dependent training examples is less than ε n let b be the number of review articles bad classifiers in the learner hypoth esis space h the probability that at least one of them is consistent is less than b ε n by the union bound as one of the major developments of recent decades has been the realization that we can have guarantees on the results of induction suming the learner always returns a consistent classifier the probability that this classifier is bad is then less than h ε n where we have used the fact that b h so if we want this probability to be less than δ it suffices to make n ln δ h ln ε ε ln h ln δ unfortunately guarantees of this type have to be taken with a large grain of salt this is because the bounds ob particularly if we are willing to settle for probabilistic guarantees tained in this way are usually extreme ly loose the wonderful feature of the bound above is that the required num ber of examples only grows logarith mically with h and δ unfortunate ly most interesting hypothesis spaces are doubly exponential in the number of features d which still leaves us needing a number of examples expo nential in d for example consider the space of boolean functions of d boolean variables if there are e pos sible different examples there are possible different functions so since there are possible examples the total number of functions is and even for hypothesis spaces that are merely exponential the bound is still very loose because the union bound is very pessimistic for exam ple if there are boolean features and the hypothesis space is decision trees with up to levels to guarantee δ ε in the bound above we need half a million examples but in prac tice a small fraction of this suffices for accurate learning further we have to be careful about what a bound like this means for instance it does not say that if your learner returned a hypothesis consistent with a particular training set then this hypothesis probably generalizes well what it says is that given a large enough training set with high probability your learner will ei ther return a hypothesis that general izes well or be unable to find a consis tent hypothesis the bound also says nothing about how to select a good hypothesis space it only tells us that if the hypothesis space contains the true classifier then the probability that the learner outputs a bad classi fier decreases with training set size october vol no communications of the acm review articles if we shrink the hypothesis space the bound improves but the chances that it contains the true classifier shrink also there are bounds for the case where the true classifier is not in the hypothesis space but similar consid erations apply to them another common type of theoreti cal guarantee is asymptotic given in finite data the learner is guaranteed to output the correct classifier this is reassuring but it would be rash to choose one learner over another be cause of its asymptotic guarantees in practice we are seldom in the asymp totic regime also known as asymp topia and because of the bias vari ance trade off i discussed earlier if learner a is better than learner b given infinite data b is often better than a given finite data the main role of theoretical guar antees in machine learning is not as a criterion for practical decisions but as a source of understanding and driving force for algorithm design in this capacity they are quite useful in deed the close interplay of theory and practice is one of the main reasons machine learning has made so much progress over the years but caveat emptor learning is a complex phe nomenon and just because a learner has a theoretical justification and works in practice does not mean the former is the reason for the latter feature engineering is the key at the end of the day some machine learning projects succeed and some fail what makes the difference eas ily the most important factor is the features used learning is easy if you have many independent features that each correlate well with the class on the other hand if the class is a very complex function of the features you may not be able to learn it often the raw data is not in a form that is ame nable to learning but you can con struct features from it that are this is typically where most of the effort in a machine learning project goes it is often also one of the most interesting parts where intuition creativity and black art are as important as the technical stuff first timers are often surprised by how little time in a machine learning project is spent actually doing ma communications of the acm october vol no a dumb algorithm with lots and lots of data beats a clever one with modest amounts of it chine learning but it makes sense if you consider how time consuming it is to gather data integrate it clean it and preprocess it and how much trial and error can go into feature design also machine learning is not a one shot process of building a dataset and running a learner but rather an itera tive process of running the learner analyzing the results modifying the data and or the learner and repeat ing learning is often the quickest part of this but that is because we have already mastered it pretty well feature engineering is more diffi cult because it is domain specific while learners can be largely general purpose however there is no sharp frontier between the two and this is another reason the most useful learn ers are those that facilitate incorpo rating knowledge of course one of the holy grails of machine learning is to automate more and more of the feature engi neering process one way this is often done today is by automatically gener ating large numbers of candidate fea tures and selecting the best by say their information gain with respect to the class but bear in mind that features that look irrelevant in isola tion may be relevant in combination for example if the class is an xor of k input features each of them by it self carries no information about the class if you want to annoy machine learners bring up xor on the other hand running a learner with a very large number of features to find out which ones are useful in combination may be too time consuming or cause overfitting so there is ultimately no replacement for the smarts you put into feature engineering more data beats a cleverer algorithm suppose you have constructed the best set of features you can but the classifiers you receive are still not ac curate enough what can you do now there are two main choices design a better learning algorithm or gather more data more examples and pos sibly more raw features subject to the curse of dimensionality machine learning researchers are mainly con cerned with the former but pragmati cally the quickest path to success is often to just get more data as a rule of thumb a dumb algorithm with lots and lots of data beats a clever one with modest amounts of it after all ma chine learning is all about letting data do the heavy lifting this does bring up another prob lem however scalability in most of computer science the two main lim ited resources are time and memory in machine learning there is a third one training data which one is the bottleneck has changed from decade to decade in the it tended to be data today it is often time enor mous mountains of data are avail able but there is not enough time to process it so it goes unused this leads to a paradox even though in principle more data means that more complex classifiers can be learned in practice simpler classifiers wind up being used because complex ones take too long to learn part of the an swer is to come up with fast ways to learn complex classifiers and indeed there has been remarkable progress in this direction for example hulten and part of the reason using cleverer algorithms has a smaller payoff than you might expect is that to a first ap proximation they all do the same this is surprising when you consider representations as different as say sets of rules and neural networks but in fact propositional rules are readily encoded as neural networks and sim ilar relationships hold between other representations all learners essen tially work by grouping nearby exam ples into the same class the key dif ference is in the meaning of nearby with nonuniformly distributed data learners can produce widely different frontiers while still making the same predictions in the regions that matter those with a substantial number of training examples and therefore also where most test examples are likely to appear this also helps explain why powerful learners can be unstable but still accurate figure illustrates this in the effect is much stronger in high dimensions as a rule it pays to try the simplest learners first for example naïve bayes before logistic regression k nearest neighbor before support vector ma chines more sophisticated learn review articles ers are seductive but they are usually cycles in research papers learners harder to use because they have more are typically compared on measures knobs you need to turn to get good re of accuracy and computational cost sults and because their internals are but human effort saved and insight more opaque gained although harder to measure learners can be divided into two are often more important this favors major types those whose representa learners that produce human under tion has a fixed size like linear classi standable output for example rule fiers and those whose representation sets and the organizations that make can grow with the data like decision the most of machine learning are trees the latter are sometimes called those that have in place an infrastruc nonparametric learners but this is ture that makes experimenting with somewhat unfortunate since they many different learners data sources usually wind up learning many more and learning problems easy and effi parameters than parametric ones cient and where there is a close col fixed size learners can only take ad laboration between machine learning vantage of so much data notice how experts and application domain ones the accuracy of naive bayes asymptotes at around in figure variable learn many models not just one size learners can in principle learn any in the early days of machine learn function given sufficient data but in ing everyone had a favorite learner practice they may not because of limi together with some a priori reasons tations of the algorithm for example to believe in its superiority most ef greedy search falls into local optima fort went into trying many variations or computational cost also because of it and selecting the best one then of the curse of dimensionality no ex systematic empirical comparisons isting amount of data may be enough showed that the best learner varies for these reasons clever algorithms from application to application and those that make the most of the data systems containing many different and computing resources available learners started to appear effort now often pay off in the end provided you went into trying many variations of are willing to put in the effort there many learners and still selecting just is no sharp frontier between design the best one but then researchers ing learners and learning classifiers noticed that if instead of selecting rather any given piece of knowledge the best variation found we combine could be encoded in the learner or many variations the results are bet learned from data so machine learn ter often much better and at little ing projects often wind up having a extra effort for the user significant component of learner de creating such model ensembles is sign and practitioners need to have now standard in the simplest tech some expertise in it nique called bagging we simply gen in the end the biggest bottleneck erate random variations of the train is not data or cpu cycles but human ing set by resampling learn a classifier on each and combine the results by figure very different frontiers can yield similar predictions and are training examples of two classes voting this works because it greatly reduces variance while only slightly increasing bias in boosting training examples have weights and these are n bayes varied so that each new classifier fo cuses on the examples the previous knn svm ones tended to get wrong in stacking the outputs of individual classifiers become the inputs of a higher level learner that figures out how best to combine them many other techniques exist and d tree the trend is toward larger and larger ensembles in the netflix prize teams from all over the world competed to build the best video recommender october vol no communications of the acm review articles system http netflixprize com as continues to improve by adding clas the competition progressed teams sifiers even after the training error has found they obtained the best results reached zero another counterexam by combining their learners with oth ple is support vector machines which er teams and merged into larger and larger teams the winner and runner up were both stacked ensembles of over learners and combining the two ensembles further improved the results doubtless we will see even larger ones in the future model ensembles should not be just because a function can be represented does not mean it can be learned can effectively have an infinite num ber of parameters without overfitting conversely the function sign sin ax can discriminate an arbitrarily large arbitrarily labeled set of points on the x axis even though it has only one pa rameter thus contrary to intuition there is no necessary connection be confused with bayesian model av tween the number of parameters of a eraging bma the theoretically model and its tendency to overfit optimal approach to learning in a more sophisticated view instead bma predictions on new examples equates complexity with the size of are made by averaging the individual the hypothesis space on the basis that predictions of all classifiers in the smaller spaces allow hypotheses to be hypothesis space weighted by how represented by shorter codes bounds well the classifiers explain the train like the one in the section on theoreti ing data and how much we believe cal guarantees might then be viewed in them a priori despite their su as implying that shorter hypotheses perficial similarities ensembles and generalize better this can be further bma are very different ensembles refined by assigning shorter codes to change the hypothesis space for ex the hypotheses in the space we have ample from single decision trees to some a priori preference for but linear combinations of them and viewing this as proof of a trade off can take a wide variety of forms bma between accuracy and simplicity is assigns weights to the hypotheses in circular reasoning we made the hy the original space according to a fixed potheses we prefer simpler by design formula bma weights are extremely and if they are accurate it is because different from those produced by our preferences are accurate not be say bagging or boosting the latter cause the hypotheses are simple in are fairly even while the former are the representation we chose extremely skewed to the point where a further complication arises from the single highest weight classifier the fact that few learners search their usually dominates making bma ef hypothesis space exhaustively a fectively equivalent to just selecting learner with a larger hypothesis space it a practical consequence of this is that tries fewer hypotheses from it that while model ensembles are a key is less likely to overfit than one that part of the machine learning toolkit tries more hypotheses from a smaller bma is seldom worth the trouble space as points out the size of the hypothesis space is only a rough simplicity does not guide to what really matters for relat imply accuracy ing training and test error the proce occam razor famously states that dure by which a hypothesis is chosen entities should not be multiplied be surveys the main argu yond necessity in machine learning ments and evidence on the issue of this is often taken to mean that given occam razor in machine learning two classifiers with the same training the conclusion is that simpler hy error the simpler of the two will likely potheses should be preferred because have the lowest test error purported simplicity is a virtue in its own right proofs of this claim appear regularly not because of a hypothetical connec in the literature but in fact there are tion with accuracy this is probably many counterexamples to it and the what occam meant in the first place no free lunch theorems imply it can not be true representable does not we saw one counterexample previ imply learnable ously model ensembles the gener essentially all representations used in alization error of a boosted ensemble variable size learners have associated communications of the acm october vol no theorems of the form every function can be represented or approximated arbitrarily closely using this repre sentation reassured by this fans of the representation often proceed to ignore all others however just be cause a function can be represented does not mean it can be learned for example standard decision tree learn ers cannot learn trees with more leaves than there are training examples in continuous spaces representing even simple functions using a fixed set of primitives often requires an infinite number of components further if the hypothesis space has many local optima of the evaluation function as is often the case the learner may not find the true function even if it is rep resentable given finite data time and memory standard learners can learn only a tiny subset of all possible func tions and these subsets are different for learners with different represen tations therefore the key question is not can it be represented to which the answer is often trivial but can it be learned and it pays to try different learners and possibly combine them some representations are exponen tially more compact than others for some functions as a result they may also require exponentially less data to learn those functions many learners work by forming linear combinations of simple basis functions for exam ple support vector machines form combinations of kernels centered at some of the training examples the support vectors representing parity of n bits in this way requires basis functions but using a representation with more layers that is more steps between input and output parity can be encoded in a linear size classifier finding methods to learn these deeper representations is one of the major re search frontiers in machine learning correlation does not imply causation the point that correlation does not imply causation is made so often that it is perhaps not worth belaboring but even though learners of the kind we have been discussing can only learn correlations their results are often treated as representing causal relations isn t this wrong if so then why do people do it review articles more often than not the goal of learning predictive models is to the internet has led to the creation of a digital society where almost everything is connected and is acces sible from anywhere however despite their widespread adop tion traditional ip networks are complex and very hard to manage it is both difficult to configure the network according to predefined policies and to reconfigure it to respond to faults load and changes to make matters even more difficult current networks are also vertically integrated the control and data planes are bundled together software defined network ing sdn is an emerging paradigm that promises to change this state of affairs by breaking vertical integration separating the network control logic from the underlying routers and switches promoting logical centralization of network control and introducing the ability to program the network the separation of concerns introduced between the definition of network policies their implementation in switching hardware and the forwarding of traffic is key to the desired flexibility by breaking the network control problem into tractable pieces sdn makes it easier to create and introduce new abstractions in networking simplifying network management and facilitating network evolution in this paper we present a comprehensive survey on sdn we start by introducing the motivation for sdn explain its main concepts and how it differs from traditional networking its roots and the standardization activities regard ing this novel paradigm next we present the key building blocks of an sdn infrastructure using a bottom up layered approach we provide an in depth analysis of the hardware infrastructure southbound and northbound application prog ramming interfaces apis network virtualization layers network operating systems sdn controllers network prog ramming languages and network applications we also look at cross layer problems such as debugging and troubleshooting in an effort to anticipate the future evolution of this new pa radigm we discuss the main ongoing research efforts and challenges of sdn in particular we address the design of switches and control platformsvwith a focus on aspects such as resiliency scalability performance security and dependabilityvas well as new opportunities for carrier trans port networks and cloud providers last but not least we ana lyze the position of sdn as a key enabler of a software defined environment keywords carrier grade networks dependability flow based networking network hypervisor network operating sys manuscript received june revised october accepted november date of current version december tems noss network virtualization openflow programmable d kreutz and f m v ramos are with the department of informatics of faculty of networks programming languages scalability software sciences university of lisbon lisbon portugal e mail kreutz ieee org fvramos fc ul pt defined environments software defined networking sdn p e verı ssimo is with the interdisciplinary centre for security reliability and trust snt university of luxembourg l walferdange luxembourg e mail paulo verissimo uni lu c e rothenberg is with the school of electrical and computer i introduction engineering feec university of campinas campinas brazil e mail chesteve dca fee unicamp br s azodolmolky is with the gesellschaft für wissenschaftliche datenverarbeitung the distributed control and transport network protocols mbh running inside the routers and switches are the key tech göttingen gwdg göttigen germany e mail siamak azodolmolky gwdg de s uhlig is with queen mary university of london london u k e mail steve eecs qmul ac uk nologies that allow information in the form of digital packets to travel around the world despite their wide digital object identifier jproc 2371999 spread adoption traditional ip networks are complex and see http www ieee org publications rights index html ó ieee personal use is permitted but republication redistribution for requires more information ieee permission proceedings of the ieee vol no january kreutz et al software defined networking a comprehensive survey hard to manage to express the desired high level net work policies network operators need to configure each individual network device separately using low level and often vendor specific commands in addition to the config uration complexity network environments have to endure the dynamics of faults and adapt to load changes automa tic reconfiguration and response mechanisms are virtually nonexistent in current ip networks enforcing the required policies in such a dynamic environment is therefore highly challenging to make it even more complicated current networks are also vertically integrated the control plane that de cides how to handle network traffic and the data plane that forwards traffic according to the decisions made by the control plane are bundled inside the networking de vices reducing flexibility and hindering innovation and evolution of the networking infrastructure the transition from to started more than a decade ago and still largely incomplete bears witness to this challenge while in fact represented merely a protocol update due to the inertia of current ip networks a new routing protocol can take five to ten years to be fully designed evaluated and deployed likewise a clean slate approach to change the internet architecture e g replacing ip is regarded as a daunting taskvsimply not feasible in practice ultimately this situation has inflated the capital and ope rational expenses of running an ip network software defined networking sdn is an emerging networking paradigm that gives hope to change the limitations of current network infrastructures first it breaks the vertical integration by separating the network control logic the control plane from the underlying rout ers and switches that forward the traffic the data plane second with the separation of the control and data planes network switches become simple forwarding devices and the control logic is implemented in a logically centralized controller or network operating simplifying po licy enforcement and network re configuration and evol ution a simplified view of this architecture is shown in fig it is important to emphasize that a logically cen tralized programmatic model does not postulate a physi cally centralized system in fact the need to guarantee adequate levels of performance scalability and reliability would preclude such a solution instead production level sdn network designs resort to physically distributed con trol planes the separation of the control plane and the data plane can be realized by means of a well defined programming interface between the switches and the sdn controller the controller exercises direct control over the state in the data plane elements via this well defined application prog ramming interface api as depicted in fig the most notable example of such an api is openflow an openflow switch has one or more tables of packet vol no january proceedings of the ieee handling rules flow table each rule matches a subset of the traffic and performs certain actions dropping for warding modifying etc on the traffic depending on the rules installed by a controller application an openflow switch canvinstructed by the controllervbehave like a router switch firewall or perform other roles e g load balancer traffic shaper and in general those of a middlebox an important consequence of the sdn principles is the separation of concerns introduced between the definition of network policies their implementation in switching hardware and the forwarding of traffic this separation is key to the desired flexibility breaking the network control problem into tractable pieces and making it easier to create and introduce new abstractions in networking sim plifying network management and facilitating network evolution and innovation although sdn and openflow started as academic experiments they gained significant traction in the industry over the past few years most vendors of com mercial switches now include support of the openflow api in their equipment the sdn momentum was strong enough to make google facebook yahoo microsoft verizon and deutsche telekom fund open networking foundation onf with the main goal of promotion and adoption of sdn through open standards develop ment as the initial concerns with sdn scalability were addressed vin particular the myth that logical cen tralization implied a physically centralized controller an issue we will return to later onvsdn ideas have matured and evolved from an academic exercise to a commercial success google for example has deployed an sdn to interconnect its data centers across the globe this pro duction network has been in deployment for three years helping the company to improve operational efficiency will use these two terms interchangeably and significantly reduce costs vmware network fig simplified view of an sdn architecture kreutz et al software defined networking a comprehensive survey virtualization platform nsx is another example nsx is a commercial solution that delivers a fully func tional network in software provisioned independent of the underlying networking devices entirely based around sdn principles as a final example the world largest it companies from carriers and equipment manufacturers to cloud providers and financial services companies have recently joined sdn consortia such as the onf and the opendaylight initiative another indication of the importance of sdn from an industrial perspective a few recent papers have surveyed specific architectu ral aspects of sdn an overview of openflow and a short literature review can be found in and these openflow oriented surveys present a relatively simplified three layer stack composed of high level net work services controllers and the controller switch inter face in jarraya et al go a step further by proposing a taxonomy for sdn however similarly to the previous works the survey is limited in terms of scope and it does not provide an in depth treatment of fundamental aspects of sdn in essence existing surveys lack a thorough dis cussion of the essential building blocks of an sdn such as the network operating systems noss programming lan fig condensed overview of this survey on sdn proceedings of the ieee vol no january guages and interfaces they also fall short on the analysis of cross layer issues such as scalability security and de pendability a more complete overview of ongoing re search efforts challenges and related standardization activities is also missing in this paper we present to the best of our knowledge the most comprehensive literature survey on sdn to date we organize this survey as depicted in fig we start in the next two sections by explaining the context introduc ing the motivation for sdn and explaining the main concepts of this new paradigm and how it differs from traditional networking our aim in the early part of the survey is also to explain that sdn is not as novel as a technological advance indeed its existence is rooted at the intersection of a series of old ideas technology driv ers and current and future needs the concepts underly ing sdnvthe separation of the control and data planes the flow abstraction upon which forwarding decisions are made the logical centralization of network control and the ability to program the networkvare not novel by themselves however the integration of already tested concepts with recent trends in networkingvnamely the availability of merchant switch silicon and the huge interest in feasible forms of network virtualizationvare leading to this paradigm shift in networking as a result of the high industry interest and the potential to change the status quo of networking from multiple perspectives a number of standardization efforts around sdn are ongo ing as we also discuss in section iii section iv is the core of this survey presenting an extensive and comprehensive analysis of the building blocks of an sdn infrastructure using a bottom up layered approach the option for a layered approach is grounded on the fact that sdn allows thinking of networking along two fundamental concepts which are common in other disciplines of computer science separation of concerns leveraging the concept of abstraction and recursion our layered bottom up approach divides the networking prob lem into eight parts hardware infrastructure south bound interfaces network virtualization hypervisor layer between the forwarding devices and the noss noss sdn controllers and control platforms northbound interfaces to offer a common programming abstraction to the upper layers mainly the network appli cations virtualization using slicing techniques provid ed by special purpose libraries or programming languages and compilers network programming languages and finally network applications in addition we also look at cross layer problems such as debugging and troubleshoot ing mechanisms the discussion in section v on ongoing research efforts challenges future work and opportuni ties concludes this paper ii status quo in networking computer networks can be divided in three planes of func tionality the data control and management planes see fig the data plane corresponds to the networking de vices which are responsible for efficiently forwarding data the control plane represents the protocols used to populate the forwarding tables of the data plane elements the management plane includes the software services such as simple network management protocol snmp fig layered view of networking functionality kreutz et al software defined networking a comprehensive survey based tools used to remotely monitor and configure the control functionality network policy is defined in the man agement plane the control plane enforces the policy and the data plane executes it by forwarding data accordingly in traditional ip networks the control and data planes are tightly coupled embedded in the same networking devices and the whole structure is highly decentralized this was considered important for the design of the inter net in the early days it seemed the best way to guarantee network resilience which was a crucial design goal in fact this approach has been quite effective in terms of network performance with a rapid increase of line rate and port densities however the outcome is a very complex and relatively static architecture as has been often reported in the net working literature e g and it is also the fundamental reason why traditional networks are rigid and complex to manage and control these two character istics are largely responsible for a vertically integrated in dustry where innovation is difficult network misconfigurations and related errors are ex tremely common in today networks for instance more than configuration errors have been observed in border gateway protocol bgp routers from a single misconfigured device very undesired network behavior may result including among others packet losses for warding loops setting up of unintended paths or service contract violations indeed while rare a single miscon figured router is able to compromise the correct operation of the whole internet for hours to support network management a small number of vendors offer proprietary solutions of specialized hard ware operating systems and control programs network applications network operators have to acquire and maintain different management solutions and the corre sponding specialized teams the capital and operational cost of building and maintaining a networking infrastruc ture is significant with long return on investment cycles which hamper innovation and addition of new features and services for instance access control load balancing energy efficiency traffic engineering to alleviate the lack of in path functionalities within the network a myriad of specialized components and middleboxes such as fire walls intrusion detection systems and deep packet inspec tion engines proliferate in current networks a recent survey of enterprise networks shows that the number of middleboxes is already on par with the number of routers in current networks despite helping in path func tionalities the net effect of middleboxes has increased complexity of network design and its operation iii what is software defined networking the term sdn was originally coined to represent the ideas and work around openflow at stanford university vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey proceedings of the ieee vol no january stanford ca usa as originally defined sdn refers and information technology being already an ubiquitous to a network architecture where the forwarding state in the feature of many computer architectures and systems data plane is managed by a remotely controlled plane de ideally the forwarding abstraction should allow any coupled from the former the networking industry has on forwarding behavior desired by the network application many occasions shifted from this original view of sdn by the control program while hiding details of the under referring to anything that involves software as being sdn lying hardware openflow is one realization of such ab we therefore attempt in this section to provide a much straction which can be seen as the equivalent to a device less ambiguous definition of sdn driver in an operating system we define an sdn as a network architecture with four the distribution abstraction should shield sdn appli pillars cations from the vagaries of distributed state making the the control and data planes are decoupled con distributed control problem a logically centralized one its trol functionality is removed from network devices realization requires a common distribution layer which in that will become simple packet forwarding sdn resides in the nos this layer has two essential elements functions first it is responsible for installing the control forwarding decisions are flow based instead of commands on the forwarding devices second it collects destination based a flow is broadly defined by a status information about the forwarding layer network set of packet field values acting as a match filter devices and links to offer a global network view to net criterion and a set of actions instructions in the work applications sdn openflow context a flow is a sequence of the last abstraction is specification which should al packets between a source and a destination all low a network application to express the desired network packets of a flow receive identical service policies behavior without being responsible for implementing that at the forwarding devices the flow behavior itself this can be achieved through virtualization abstraction allows unifying the behavior of differ solutions as well as network programming languages ent types of network devices including routers these approaches map the abstract configurations that the switches firewalls and middleboxes flow applications express based on a simplified abstract model programming enables unprecedented flexibility of the network into a physical configuration for the global limited only to the capabilities of the implemen network view exposed by the sdn controller fig de ted flow tables picts the sdn architecture concepts and building blocks control logic is moved to an external entity the as previously mentioned the strong coupling between so called sdn controller or nos the nos is a control and data planes has made it difficult to add new software platform that runs on commodity server functionality to traditional networks a fact illustrated in technology and provides the essential resources fig the coupling of the control and data planes and its and abstractions to facilitate the programming of physical embedding in the network elements makes the forwarding devices based on a logically central development and deployment of new networking features ized abstract network view its purpose is there fore similar to that of a traditional operating system the network is programmable through software applications running on top of the nos that in teracts with the underlying data plane devices this is a fundamental characteristic of sdn con sidered as its main value proposition note that the logical centralization of the control logic in particular offers several additional benefits first it is simpler and less error prone to modify network policies through high level languages and software components compared with low level device specific configurations second a control program can automatically react to spurious changes of the network state and thus maintain the high level policies intact third the centralization of the control logic in a controller with global knowledge of the network state simplifies the development of more so phisticated networking functions services and applications following the sdn concept introduced in an sdn can be defined by three fundamental abstractions for warding distribution and specification in fact abstrac tions are essential tools of research in computer science fig sdn architecture and its fundamental abstractions kreutz et al software defined networking a comprehensive survey lancing and routing applications can be combined sequentially with load balancing decisions having precedence over routing policies a terminology to identify the different elements of an sdn as un equivocally as possible we now present the essential terminology used throughout this work forwarding devices fd these are hardware or software based data plane devices that perform a set of elementary operations the forwarding devices have well defined instruction sets e g flow rules used to take ac tions on the incoming packets e g forward to specific ports drop forward to the controller rewrite some header these instructions are defined by southbound interfaces e g openflow forces protocol oblivious forwarding pof and are installed in the forwarding devices by the sdn controllers implementing the southbound protocols data plane dp forwarding devices are intercon fig traditional networking versus sdn with sdn management nected through wireless radio channels or wired cables becomes simpler and middleboxes services can be delivered as the network infrastructure comprises the interconnected sdn controller applications forwarding devices which represent the data plane southbound interface si the instruction set of the e g routing algorithms very difficult since it would forwarding devices is defined by the southbound api imply a modification of the control plane of all network which is part of the southbound interface furthermore devicesvthrough the installation of new firmware and in the si also defines the communication protocol between some cases hardware upgrades hence the new network forwarding devices and control plane elements this pro ing features are commonly introduced via expensive spe tocol formalizes the way the control and data plane ele cialized and hard to configure equipment also known as ments interact middleboxes such as load balancers intrusion detection systems idss and firewalls among others these mid control plane cp forwarding devices are prog dleboxes need to be placed strategically in the network rammed by control plane elements through well defined making it even harder to later change the network topo si embodiments the control plane can therefore be seen logy configuration and functionality as the network brain all control logic rests in the appli in contrast sdn decouples the control plane from the cations and controllers which form the control plane network devices and becomes an external entity the nos or sdn controller this approach has several advantages northbound interface ni the nos can offer an api it becomes easier to program these applications to application developers this api represents a north since the abstractions provided by the control plat bound interface i e a common interface for developing form and or the network programming languages applications typically a northbound interface abstracts can be shared the low level instruction sets used by southbound inter all applications can take advantage of the same faces to program forwarding devices network information the global network view leading arguably to more consistent and effective management plane mp the management plane is policy decisions while reusing control plane soft the set of applications that leverage the functions offered ware modules by the ni to implement network control and operation these applications can take actions i e reconfig logic this includes applications such as routing fire ure forwarding devices from any part of the net walls load balancers monitoring and so forth essen work there is therefore no need to devise a precise tially a management application defines the policies strategy about the location of the new functionality which are ultimately translated to southbound specific the integration of different applications becomes instructions that program the behavior of the forwarding more straightforward for instance load ba devices vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey b alternative and broadening definitions since its inception in the original open flow centered sdn term has seen its scope broadened beyond architectures with a cleanly decoupled control plane interface the definition of sdn will likely continue to broaden driven by the industry business oriented views on sdnvirrespective of the decoupling of the control plane in this survey we focus on the original canonical sdn definition based on the aforementioned key pillars and the concept of layered abstractions however for the sake of completeness and clarity we acknowledge alternative sdn definitions as follows control plane broker sdn a networking approach that retains existing distributed control planes but offers new apis that allow applications to interact bidirection ally with the network an sdn controllervoften called orchestration platformvacts as a broker between the ap plications and the network elements this approach effectively presents control plane data to the application and allows a certain degree of network programmability by means of plug ins between the orchestrator function and network protocols this api driven approach corresponds to a hybrid model of sdn since it enables the broker to manipulate and directly interact with the control planes of devices such as routers and switches examples of this view on sdn include recent standardization efforts at the in ternet engineering task force ietf see section iii c and the design philosophy behind the opendaylight pro ject that goes beyond the openflow split control mode overlay sdn this is a networking approach where the software or hardware based network edge is dyna mically programmed to manage tunnels between hyper visors and or network switches introducing an overlay network in this hybrid networking approach the distrib uted control plane providing the underlay remains un touched the centralized control plane provides a logical overlay that utilizes the underlay as a transport network this flavor of sdn follows a proactive model to install the overlay tunnels the overlay tunnels usually terminate inside virtual switches within hypervisors or in physical devices acting as gateways to the existing network this approach is very popular in recent data center network virtualization and are based on a variety of tunneling technologies e g stateless transport tunneling 34 virtualized layer networks vxlan network vir tualization using generic routing encapsulation nvgre locator id separation protocol lisp and generic network virtualization encapsulation geneve recently other attempts to define sdn in a layered ap proach have appeared from a practical perspective and trying to keep backward compatibility with existing network management approaches one initiative in the irtf software defined networking research group sdnrg proceedings of the ieee vol no january proposes a management plane at the same level of the control plane i e it classifies solutions in two categories control logic with control plane southbound interfaces and management logic with management plane southbound interfaces in other words the management plane can be seen as a control platform that accommodates traditional network management services and protocols such as snmp bgp path computation element communication protocol pcep and network configuration protocol netconf in addition to the broadening definitions above the term sdn is often used to define extensible network man agement planes e g openstack whitebox bare metal switches with open operating systems e g cumulus linux open source data planes e g xorplus quagga specialized programmable hardware devices e g netfpga virtualized software based appli ances e g open platform for network functions virtualiza tion opnfv in spite of lacking a decoupled control and data plane or common interface along its api hybrid sdn models are further discussed in section v g c standardization activities the standardization landscape in sdn and sdn related issues is already wide and is expected to keep evolving over time while some of the activities are being carried out in standard development organizations sdos other related efforts are ongoing at industrial or community consortia e g opendaylight openstack opnfv delivering results often considered candidates for de facto standards these results often come in the form of open source implementations that have become the common strategy toward accelerating sdn and related cloud and networking technologies the reason for this fragmentation is due to sdn concepts spanning different areas of it and networking both from a network segmentation point of view from access to core and from a technology perspective from optical to wireless table presents a summary of the main sdos and organizations contributing to the standardization of sdn as well as the main outcomes produced to date the onf was conceived as a member driven organi zation to promote the adoption of sdn through the devel opment of the openflow protocol as an open standard to communicate control decisions to data plane devices the onf is structured in several working groups wgs some wgs are focused on either defining extensions to the openflow protocol in general such as the extensibility wg or tailored to specific technological areas examples of the latter include the optical transport ot wg the wireless and mobile w m wg and the northbound in terfaces nbi wg other wgs center their activity in providing new protocol capabilities to enhance the pro tocol itself such as the architecture wg or the forwarding abstractions fa wg similar to how network programmability ideas have been considered by several ietf working groups wgs in the past the present sdn trend is also influencing a table openflow standardization activities kreutz et al software defined networking a comprehensive survey number of activities a related body that focuses on re search aspects for the evolution of the internet irtf has created the sdnrg this group investigates sdn from vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey proceedings of the ieee vol no january various perspectives with the goal of identifying the ap ing innovation inside the network by allowing program proaches that can be defined deployed and used in the mability and altogether changing the network operational near term as well as identifying future research challenges model through automation and a real shift to software in the international telecommunications union tele based platforms communication sector itu t some study groups sgs finally the mobile networking industry genera have already started to develop recommendations for tion partnership project consortium is studying the sdn and a joint coordination activity on sdn jca management of virtualized networks an effort aligned sdn has been established to coordinate the sdn stan with the etsi nfv architecture and as such likely to dardization work leverage from sdn the broadband forum bbf is working on sdn topics through the service innovation market requirements d history of sdn simr wg the objective of the bbf is to release recom albeit a fairly recent concept sdn leverages on net mendations for supporting sdn in multiservice broadband working ideas with a longer history in particular it networks including hybrid environments where only builds on work made on programmable networks such as some of the network equipment is sdn enabled active networks programmable atm networks the metro ethernet forum mef is approaching sdn and on proposals for control and data plane separa with the aim of defining service orchestration with apis tion such as the network control point ncp and for existing networks routing control platform bcp at the ieee the lan man standards committee in order to present a historical perspective we sum has recently started some activities to standardize sdn marize in table different instances of sdn related work capabilities on access networks based on ieee infra prior to sdn splitting it into five categories along with structure through the project for both wired the categories we defined the second and third columns of and wireless technologies to embrace new control the table mention past initiatives pre sdn i e before the interfaces openflow based initiatives that sprung into the sdn con the optical internetworking forum oif carrier wg cept and recent developments that led to the definition released a set of requirements for transport sdn the ini of sdn tial activities have as main goal to describe the features and data plane programmability has a long history active functionalities needed to support the deployment of sdn networks represent one of the early attempts on capabilities in carrier transport networks the open data building new network architectures based on this concept center alliance odca is an organization working on the main idea behind active networks is for each node to unifying data center in the migration to cloud computing have the capability to perform computations on or modify environments through interoperable solutions through the content of packets to this end active networks pro the documentation of usage models specifically one for pose two distinct approaches programmable switches and sdn the odca is defining new requirements for cloud capsules the former does not imply changes in the existing deployment the alliance for telecommunication industry packet or cell format it assumes that switching devices solutions atis created a focus group for analyzing ope support the downloading of programs with specific in rational issues and opportunities associated with the prog structions on how to process packets the second approach rammable capabilities of network infrastructure on the other hand suggests that packets should be replaced at the european telecommunication standards insti by tiny programs which are encapsulated in transmission tute etsi efforts are being devoted to network function frames and executed at each node along their path virtualization nfv through a newly defined industry forces openflow and pof represent specification group isg nfv and sdn concepts are recent approaches for designing and deploying program considered complementary sharing the goal of accelerat mable data plane devices in a manner different from table summarized overview of the history of programmable networks kreutz et al software defined networking a comprehensive survey active networks these new proposals rely essentially on junos extremexos and sr os de modifying forwarding devices to support flow tables spite being more specialized noss targeting network de which can be dynamically configured by remote entities vices such as high performance core routers these noss through simple operations such as adding removing or abstract the underlying hardware to the network operator updating flow rules i e entries on the flow tables making it easier to control the network infrastructure as the earliest initiatives on separating data and control well as simplifying the development and deployment of signaling date back to the and the ncp new protocols and management applications is probably the first attempt to separate control and data finally initiatives that can be seen as technology pull plane signaling ncps were introduced by at t to im drivers are also worth recalling back in the a prove the management and control of its telephone net movement toward open signaling began to happen work this change promoted a faster pace of innovation of the main motivation was to promote the wider adoption of the network and provided new means for improving its the ideas proposed by projects such as ncp and efficiency by taking advantage of the global view of the tempest the open signaling movement worked to network provided by ncps similarly other initiatives such ward separating the control and data signaling by propos as tempest forces rcp and pce ing open and programmable interfaces curiously a rather proposed the separation of the control and data planes similar movement can be observed with the recent advent for improved management in atm ethernet bgp of openflow and sdn with the lead of the onf this and multiprotocol label switching mpls networks type of movement is crucial to promote open technologies respectively into the market hopefully leading equipment manufactur more recently initiatives such as sane ethane ers to support open standards and thus fostering interop openflow nox and pof proposed erability competition and innovation the decoupling of the control and data planes for ethernet for a more extensive intellectual history of program networks interestingly these recent solutions do not re mable networks and sdn we direct the reader to the quire significant modifications on the forwarding devices recent paper by feamster et al making them attractive not only for the networking re search community but even more to the networking in dustry openflow based devices for instance can easily coexist with traditional ethernet devices enabling a iv software defined networks bottom up progressive adoption i e not requiring a disruptive an sdn architecture can be depicted as a composition of change to existing networks different layers as shown in fig b each layer has its network virtualization has gained a new traction with own specific functions while some of them are always the advent of sdn nevertheless network virtualization present in an sdn deployment such as the southbound also has its roots back in the the tempest project api noss northbound api and network applications is one of the first initiatives to introduce network others may be present only in particular deployments such virtualization by introducing the concept of switchlets in as hypervisor or language based virtualization atm networks the core idea was to allow multiple fig presents a trifold perspective of sdns the sdn switchlets on top of a single atm switch enabling multiple layers are represented in fig b as explained above independent atm networks to share the same physical fig a and c depicts a plane oriented view and a sys resources similarly mbone was one of the early tem design perspective respectively initiatives that targeted the creation of virtual network to the following sections introduce each layer following pologies on top of legacy networks or overlay networks a bottom up approach for each layer the core properties this work was followed by several other projects such as and concepts are explained based on the different tech planet lab geni and vini flowvisor nologies and solutions additionally debugging and trou is also worth mentioning as one of the first recent bleshooting techniques and tools are discussed initiatives to promote a hypervisor like virtualization ar chitecture for network infrastructures resembling the a layer i infrastructure hypervisor model common for compute and storage more an sdn infrastructure similarly to a traditional net recently koponen et al proposed a network virtualization work is composed of a set of networking equipment platform nvp for multitenant data centers using switches routers and middlebox appliances the main sdn as a base technology difference resides in the fact that those traditional physical the concept of a nos was reborn with the introduction devices are now simple forwarding elements without em of openflow based noss such as nox onix and bedded control or software to take autonomous decisions onos indeed noss have been in existence for the network intelligence is removed from the data plane decades one of the most widely known and deployed is devices to a logically centralized control system i e the the cisco ios which was originally conceived back nos and applications as shown in fig c more impor in the early other noss worth mentioning are tantly these new networks are built conceptually on top vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey fig software defined networks in a planes b layers and c system design architecture of open and standard interfaces e g openflow a crucial approach for ensuring configuration and communication compatibility and interoperability among different data and control plane devices in other words these open in terfaces enable controller entities to dynamically program heterogeneous forwarding devices something difficult in traditional networks due to the large variety of proprietary and closed interfaces and the distributed nature of the control plane in an sdn openflow architecture there are two main elements the controllers and the forwarding devices as shown in fig a data plane device is a hardware or software element specialized in packet forwarding while a controller is a software stack the network brain run ning on a commodity hardware platform an openflow enabled forwarding device is based on a pipeline of flow tables where each entry of a flow table has three parts a matching rule actions to be executed on matching packets and counters that keep statistics of matching packets this high level and simplified model derived from fig openflow enabled sdn devices proceedings of the ieee vol no january openflow is currently the most widespread design of sdn data plane devices nevertheless other specifications of sdn enabled forwarding devices are being pursued including pof and the negotiable datapath models ndms from the onf forwarding abstractions working group fawg inside an openflow device a path through a sequence of flow tables defines how packets should be handled when a new packet arrives the lookup process starts in the first table and ends either with a match in one of the tables of the pipeline or with a miss when no rule is found for that packet a flow rule can be defined by combining different matching fields as illustrated in fig if there is no default rule the packet will be discarded however the common case is to install a default rule which tells the switch to send the packet to the controller or to the normal non openflow pipeline of the switch the priority of the rules follows the natural sequence number of the tables and the row order in a flow table possible actions include forward the packet to outgoing port kreutz et al software defined networking a comprehensive survey table different match fields statistics and capabilities have been added on each openflow protocol revision the number of required req and optional opt capabilities has grown considerably encapsulate it and forward it to the controller drop it optimized tcam memory that supports from up send it to the normal processing pipeline and send it to to flow table entries this is a clear sign the next flow table or to special tables such as group or that the size of the flow tables is growing at a pace aiming metering tables introduced in the latest openflow protocol to meet the needs of future sdn deployments networking as detailed in table each version of the openflow hardware manufacturers have produced various kinds of specification introduced new match fields including openflow enabled devices as is shown in table these ethernet mpls tcp udp etc however only devices range from equipment for small businesses e g a subset of those matching fields are mandatory to be gbe switches to high class data center equipment e g compliant to a given protocol version similarly many ac high density switch chassis with up to connectiv tions and port types are optional features flow match ity for edge to core applications with tens of terabits per rules can be based on almost arbitrary combinations of bits second of switching capacity of the different packet headers using bit masks for each software switches are emerging as one of the most field adding new matching fields has been eased with the promising solutions for data centers and virtualized net extensibility capabilities introduced in openflow work infrastructures examples of software version through an openflow extensible match based openflow switch implementations include switch oxm based on type length value tlv structures to light ofsoftswitch13 open vswitch improve the overall protocol extensibility with openflow openflow reference pantou and version tlv structures have been also added to ports xorplus recent reports show that the number of vir tables and queues in replacement of the hard coded tual access ports is already larger than physical access ports counterparts of earlier protocol versions on data centers network virtualization has been one of the drivers behind this trend software switches such as overview of available openflow devices several open vswitch have been used for moving network func openflow enabled forwarding devices are available on tions to the edge with the core performing traditional ip the market both as commercial and open source products forwarding thus enabling network virtualization see table there are many off the shelf ready to de an interesting observation is the number of small ploy openflow switches and routers among other ap startup enterprises devoted to sdn such as big switch pliances most of the switches available on the market have cyan plexxi and noviflow this seems to imply relatively small ternary content addressable memory that sdn is springing a more competitive and open net tcams with up to entries nonetheless this is working market one of its original goals other effects of changing at a fast pace some of the latest devices released this openness triggered by sdn include the emergence of in the market go far beyond that figure gigabit ethernet so called bare metal switches or whitebox switches gbe switches for common business purposes are already where software and hardware are sold separately and the end supporting up to layer layer or user is free to load an operating system of its choice exact match flows enterprise class switches are being delivered with more than b layer ii southbound interfaces layer flow entries other switching devices southbound interfaces or southbound apis are the using high performance chips e g ezchip np provide connecting bridges between control and forwarding vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey table openflow enabled hardware and software devices proceedings of the ieee vol no january elements thus being the crucial instrument for clearly collected by the controller third packet in messages are separating control and data plane functionality however sent by forwarding devices to the controller when they do these apis are still tightly tied to the forwarding elements not known what to do with a new incoming flow or of the underlying physical or virtual infrastructure because there is an explicit send to controller action in typically a new switch can take two years to be ready the matched entry of the flow table these information for commercialization if built from scratch with upgrade channels are the essential means to provide flow level cycles that can take up to nine months the software de information to the nos velopment for a new product can take from six months to albeit the most visible openflow is not the only one year the initial investment is high and risky as a available southbound interface for sdn there are other central component of its design the southbound apis api proposals such as forces open vswitch represent one of the major barriers for the introduction and database ovsdb pof opflex acceptance of any new networking technology in this light openstate revised open flow library rofl the emergence of sdn southbound api proposals such as hardware abstraction layer hal and openflow is seen as welcome by many in the industry programmable abstraction of data path pad these standards promote interoperability allowing the de forces proposes a more flexible approach to traditional ployment of vendor agnostic network devices this has al network management without changing the current archi ready been demonstrated by the interoperability between tecture of the network i e without the need of a logically openflow enabled equipments from different vendors centralized external controller the control and data as of this writing openflow is the most widely ac planes are separated but can potentially be kept in the cepted and deployed open southbound standard for sdn same network element however the control part of the it provides a common specification to implement open network element can be upgraded on the fly with third flow enabled forwarding devices and for the commu party firmware nication channel between data and control plane devices ovsdb is another type of southbound api de e g switches and controllers the openflow protocol signed to provide advanced management capabilities for provides three information sources for noss first event open vswitches beyond openflow capabilities to confi based messages are sent by forwarding devices to the gure the behavior of flows in a forwarding device an open controller when a link or port change is triggered second vswitch offers other networking functions for instance it flow statistics are generated by the forwarding devices and allows the control elements to create multiple virtual kreutz et al software defined networking a comprehensive survey switch instances set quality of service qos policies on hal is not exactly a southbound api but is interfaces attach interfaces to the switches configure closely related differently from the aforementioned ap tunnel interfaces on openflow data paths manage queues proaches hal is rather a translator that enables a south and collect statistics therefore the ovsdb is a comple bound api such as openflow to control heterogeneous mentary protocol to openflow for open vswitch hardware devices it thus sits between the southbound api one of the first direct competitors of openflow is pof and the hardware device recent research experiments one of the main goals of pof is to enhance the with hal have demonstrated the viability of sdn control current sdn forwarding plane with openflow switches in access networks such as gigabit ethernet passive optical have to understand the protocol headers to extract the networks gepons and cable networks docsiss required bits to be matched with the flow tables entries a similar effort to hal is pad a proposal that this parsing represents a significant burden for data plane goes a bit further by also working as a southbound api by devices in particular if we consider that openflow itself more importantly pad allows a more generic prog version already contains more than header fields ramming of forwarding devices by enabling the control of besides this inherent complexity backward compatibility data path behavior using generic byte operations defining issues may arise every time new header fields are included protocol headers and providing function definitions in or removed from the protocol to achieve its goal pof proposes a generic flow instruction set fis that makes c layer iii network hypervisors the forwarding plane protocol oblivious a forwarding virtualization is already a consolidated technology in element does not need to know by itself anything about modern computers the fast developments of the past de the packet format in advance forwarding devices are seen cade have made virtualization of computing platforms as white boxes with only processing and forwarding mainstream based on recent reports the number of vir capabilities in pof packet parsing is a controller task that tual servers has already exceeded the number of physical results in a sequence of generic keys and table lookup servers instructions that are installed in the forwarding elements hypervisors enable distinct virtual machines to share the behavior of data plane devices is therefore com the same hardware resources in a cloud infrastructure as pletely under the control of the sdn controller similar to a service iaas each user can have its own virtual re a central processing unit cpu in a computer system a sources from computing to storage this enabled new pof switch is application and protocol agnostic revenue and business models where users allocate re a recent southbound interface proposal is opflex sources on demand from a shared physical infrastructure contrary to openflow and similar to forces one at a relatively low cost at the same time providers of the ideas behind opflex is to distribute part of the make better use of the capacity of their installed physical complexity of managing the network back to the forward infrastructures creating new revenue streams without ing devices with the aim of improving scalability similar significantly increasing their capital expenditure and to openflow policies are logically centralized and operational expenditure opex costs one of the abstracted from the underlying implementation the dif interesting features of virtualization technologies today is ferences between openflow and opflex are a clear illus the fact that virtual machines can be easily migrated from tration of one of the important questions to be answered one physical server to another and can be created and or when devising a southbound interface where to place each destroyed on demand enabling the provisioning of elastic piece of the overall functionality services with flexible and easy management unfor in contrast to opflex and pof openstate and tunately virtualization has been only partially realized in rofl do not propose a new set of instructions for practice despite the great advances in virtualizing com programming data plane devices openstate proposes ex puting and storage elements the network is still mostly tended finite machines stateful programming abstrac statically configured in a box by box manner tions as an extension superset of the openflow match the main network requirements can be captured along action abstraction finite state machines allow the imple two dimensions network topology and address space mentation of several stateful tasks inside forwarding de different workloads require different network topologies vices i e without augmenting the complexity or overhead and services such as flat or services or even more of the control plane for instance all tasks involving only complex services for advanced functionality cur local state such as media access control mac learning rently it is very difficult for a single physical topology to operations port knocking or stateful edge firewalls can be support the diverse demands of applications and services performed directly on the forwarding devices without any similarly address space is hard to change in current net extra control plane communication and processing delay works today virtualized workloads have to operate in the rofl on the other hand proposes an abstraction layer same address of the physical infrastructure therefore it is that hides the details of the different openflow versions hard to keep the original network configuration for a te thus providing a clean api for software developers nant virtual machines cannot migrate to arbitrary loca simplifying application development tions and the addressing scheme is fixed and hard to vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey proceedings of the ieee vol no january change for example cannot be used by the virtual topology address and control function virtualization all machines vms of a tenant if the underlying physical these properties are necessary in multitenant environ forwarding devices support only ments where virtual networks need to be managed and to provide complete virtualization the network should migrated according to the computing and storage virtual provide similar properties to the computing layer the resources virtual network topologies have to be mapped network infrastructure should be able to support arbitrary onto the underlying forwarding devices with virtual network topologies and addressing schemes each tenant addresses allowing tenants to completely manage their should have the ability to configure both the computing address space without depending on the underlying net nodes and the network simultaneously host migration work elements addressing schemes should automatically trigger the migration of the corre autoslice is another sdn based virtualization sponding virtual network ports one might think that long proposal differently from flowvisor it focuses on the standing virtualization primitives such as vlans virtua automation of the deployment and operation of virtual lized domain nat virtualized ip address space and sdn vsdn topologies with minimal mediation or arbi mpls virtualized path are enough to provide full and tration by the substrate network operator additionally automated network virtualization however these tech autoslice targets also scalability aspects of network hyper nologies are anchored on a box by box basis configuration visors by optimizing resource utilization and by mitigating i e there is no single unifying abstraction that can be the flow table limitations through a precise monitoring of leveraged to configure or reconfigure the network in a the flow traffic statistics similarly to autoslice autov global manner as a consequence current network provi flow also enables multidomain network virtualiza sioning can take months while computing provisioning tion however instead of having a single third party to takes only minutes control the mapping of vsdn topologies as is the case of there is hope that this situation will change with sdn autoslice autovflow uses a multiproxy architecture that and the availability of new tunneling techniques e g allows network owners to implement flow space virtuali vxlan and nvgre for instance solutions zation in an autonomous way by exchanging information such as flowvisor flown nvp among the different domains openvirtex ibm sdn ve flown is based on a slightly different radiovisor autovflow extensible datapath concept whereas flowvisor can be compared to a full vir daemon xdpd optical transport network tualization technology flown is analogous to a container virtualization and version agnostic openflow slicing based virtualization i e a lightweight virtualization mechanisms have been recently proposed eval approach flown was also primarily conceived to address uated and deployed in real scenarios for on demand pro multitenancy in the context of cloud platforms it is de visioning of virtual networks signed to be scalable and allows a unique shared controller platform to be used for managing multiple domains in a slicing the network flowvisor is one of the early cloud environment each tenant has full control over its technologies to virtualize an sdn its basic idea is to allow virtual networks and is free to deploy any network ab multiple logical networks share the same openflow net straction and application on top of the controller platform working infrastructure for this purpose it provides an the compositional sdn hypervisor was designed abstraction layer that makes it easier to slice a data plane with a different set of goals its main objective is to allow based on off the shelf openflow enabled switches allow the cooperative sequential or parallel execution of appli ing multiple and diverse networks to coexist five slicing cations developed with different programming languages dimensions are considered in flowvisor bandwidth topo or conceived for diverse control platforms it thus offers logy traffic device cpu and forwarding tables moreover interoperability and portability in addition to the typical each network slice supports a controller i e multiple functions of network hypervisors controllers can coexist on top of the same physical network infrastructure each controller is allowed to act only on its commercial multitenant network hypervisors none of own network slice in general terms a slice is defined as a the aforementioned approaches is designed to address all particular set of flows on the data plane from a system challenges of multitenant data centers for instance te design perspective flowvisor is a transparent proxy that nants want to be able to migrate their enterprise solutions intercepts openflow messages between switches and con to cloud providers without the need to modify the network trollers it partitions the link bandwidth and flow tables of configuration of their home network existing networking each switch each slice receives a minimum data rate and technologies and migration strategies have mostly failed to each guest controller gets its own virtual flow table in the meet both tenant and service provider requirements a switches multitenant environment should be anchored in a network similarly to flowvisor openvirtex acts as hypervisor capable of abstracting the underlaying forward a proxy between the nos and the forwarding devices ing devices and physical network topology from the te however its main goal is to provide virtual sdns through nants moreover each tenant should have access to control kreutz et al software defined networking a comprehensive survey abstractions and manage its own virtual networks inde memory and provide security protection mechanisms pendently and isolated from other tenants these functionalities and resources are key enablers for with the market demand for network virtualization increased productivity making the life of system and ap and the recent research on sdn showing promise as an plication developers easier their widespread use has sig enabling technology different commercial virtualization nificantly contributed to the evolution of various platforms based on sdn concepts have started to appear ecosystems e g programming languages and the devel vmware has proposed a network virtualization platform opment of a myriad of applications nvp that provides the necessary abstractions to in contrast networks have so far been managed and allow the creation of independent virtual networks for configured using lower level device specific instruction large scale multitenant environments nvp is a complete sets and mostly closed proprietary noss e g cisco ios network virtualization solution that allows the creation of and juniper junos moreover the idea of operating sys virtual networks each with independent service model tems abstracting device specific characteristics and provid topologies and addressing architectures over the same ing in a transparent way common functionalities is still physical network with nvp tenants do not need to know mostly absent in networks for instance today designers of anything about the underlying network topology config routing protocols need to deal with complicated distrib uration or other specific aspects of the forwarding devices uted algorithms when solving networking problems net nvp network hypervisor translates the tenants config work practitioners have therefore been solving the same urations and requirements into low level instruction sets problems over and over again to be installed on the forwarding devices for this purpose sdn is promised to facilitate network management and the platform uses a cluster of sdn controllers to mani ease the burden of solving networking problems by means pulate the forwarding tables of the open vswitches in the of the logically centralized control offered by a nos host hypervisor forwarding decisions are therefore made as with traditional operating systems the crucial value of a exclusively on the network edge after the decision is nos is to provide abstractions essential services and made the packet is tunneled over the physical network to common apis to developers generic functionality as net the receiving host hypervisor the physical network sees work state and network topology information device dis nothing but ordinary ip packets covery and distribution of network configuration can be ibm has also recently proposed sdn ve provided as services of the nos with noss to define another commercial and enterprise class network virtualiza network policies a developer no longer needs to care about tion platform sdn ve uses opendaylight as one of the the low level details of data distribution among routing building blocks of the so called software defined environ elements for instance such systems can arguably create a ments sdes a trend further discussed in section v this new environment capable of fostering innovation at a solution also offers a complete implementation framework faster pace by reducing the inherent complexity of creating for network virtualization like nvp it uses a host based new network protocols and network applications overlay approach achieving advanced network abstraction a nos or a controller is a critical element in an sdn that enables application level network services in large scale architecture as it is the key supporting piece for the control multitenant environments interestingly sdn ve is logic applications to generate the network configuration capable of supporting in one single instantiation up to based on the policies defined by the network operator virtual networks and virtual machines similar to a traditional operating system the control plat to summarize currently there are already a few net form abstracts the lower level details of connecting and work hypervisor proposals leveraging the advances of interacting with forwarding devices i e of materializing sdn there are however still several issues to be ad the network policies dressed these include among others the improvement of virtual to physical mapping techniques the defini architecture and design axes there is a very diverse tion of the level of detail that should be exposed at the set of controllers and control platforms with different de logical level and the support for nested virtualization sign and architectural choices exist we anticipate however this ecosystem to expand in the ing controllers can be categorized based on many aspects near future since network virtualization will most likely from an architectural point of view one of the most rele play a key role in future virtualized environments simi vant is if they are centralized or distributed this is one of larly to the expansion we have been witnessing in virtual the key design axes of sdn control platforms so we start ized computing by discussing this aspect next d layer iv network operating systems controllers centralized versus distributed a centralized control traditional operating systems provide abstractions ler is a single entity that manages all forwarding devices of e g high level programming apis for accessing lower the network naturally it represents a single point of fail level devices manage the concurrent access to the under ure and may have scaling limitations a single controller lying resources e g hard drive network adapter cpu may not be enough to manage a network with a large vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey proceedings of the ieee vol no january number of data plane elements centralized controllers consistency semantics which means that data updates on such as nox mt maestro beacon and distinct nodes will eventually be updated on all controller floodlight have been designed as highly concurrent nodes this implies that there is a period of time in which systems to achieve the throughput required by enterprise distinct nodes may read different values old value or new class networks and data centers these controllers are value for the same property strong consistency on the based on multithreaded designs to explore the parallelism other hand ensures that all controller nodes will read the of multicore computer architectures as an example most updated property value after a write operation de beacon can deal with more than million flows per sec spite its impact on system performance strong consistency ond by using large size computing nodes of cloud providers offers a simpler interface to application developers to such as amazon other centralized controllers such date only onix onos and smartlight provide this data as trema ryu nos meridian and consistency model programmableflow target specific environ another common property of distributed controllers is ments such as data centers cloud infrastructures and fault tolerance when one node fails another neighbor carrier grade networks furthermore controllers such as node should take over the duties and devices of the failed rosemary offer specific functionality and guarantees node so far despite some controllers tolerating crash namely security and isolation of applications by using a failures they do not tolerate arbitrary failures which container based architecture called micro nos it achieves means that any node with an abnormal behavior will not be its primary goal of isolating applications and preventing replaced by a potentially well behaved one the propagation of failures throughout the sdn stack a single controller may be enough to manage a small contrary to a centralized design a distributed nos can network however it represents a single point of failure be scaled up to meet the requirements of potentially any similarly independent controllers can be spread across the environment from small to large scale networks a dis network each of them managing a network segment re tributed controller can be a centralized cluster of nodes or ducing the impact of a single controller failure yet if the a physically distributed set of elements while the first control plane availability is critical a cluster of controllers can alternative can offer high throughput for very dense data be used to achieve a higher degree of availability and or for centers the latter can be more resilient to different kinds supporting more devices ultimately a distributed controller of logical and physical failures a cloud provider that spans can improve the control plane resilience and scalability and multiple data centers interconnected by a wide area net reduce the impact of problems caused by network partition work may require a hybrid approach with clusters of con for instance sdn resiliency as a whole is an open challenge trollers inside each data center and distributed controller that will be further discussed in section v c nodes in the different sites onix hyperflow hp van sdn dissecting sdn controller platforms to provide a bet onos disco yanc pane ter architectural overview and understanding the design of smart light and fleet are examples of distri a nos table summarizes some of the most relevant buted controllers most distributed controllers offer weak architectural and design properties of sdn controllers and table architecture and design elements of control platforms kreutz et al software defined networking a comprehensive survey fig sdn control platforms elements services and interfaces control platforms we have focused on the elements ser core controller functions the base network service vices and interfaces of a selection of production level functions are what we consider the essential functionality well documented controllers and control platforms each all controllers should provide as an analogy these func line in the table represents a component we consider tions are like base services of operating systems such as important in a modular and scalable control platform we program execution input output i o operations control observe a highly diversified environment with different communications protection and so on these services are properties and components being used by distinct control used by other operating system level services and user platforms this is not surprising given an environment applications in a similar way functions such as topology with many competitors willing to be at the forefront of statistics notifications and device management together sdn development note also that not all components are with shortest path forwarding and security mechanisms available on all platforms for instance east westbound are essential network control functionalities that network apis are not required in centralized controllers such as applications may use in building its logic for instance the beacon in fact some platforms have very specific niche notification manager should be able to receive process markets such as telecom companies and cloud providers and forward events e g alarm notifications security so the requirements will be different alarms state changes security mechanisms are based on the analysis of the different sdn controllers another example as they are critical components to pro proposed to date both those presented in table and vide basic isolation and security enforcement between others such as nox meridian forces services and applications for instance rules generated by and fortnox we extract several common elements high priority services should not be overwritten with rules and provide a first attempt to clearly and systematically created by applications with a lower priority dissect an sdn control platform in fig there are at least three relatively well defined layers in southbound on the lower level of control platforms most of the existing control platforms the application the southbound apis can be seen as a layer of device driv orchestration and services the core controller func ers they provide a common interface for the upper layers tions and the elements for southbound communica while allowing a control platform to use different south tions the connection at the upper level layers is based on bound apis e g openflow ovsdb and forces and northbound interfaces such as rest apis and prog protocol plug ins to manage existing or new physical or ramming languages such as fml frenetic and virtual devices e g snmp bgp and netconf this is netcore on the lower level part of a control essential both for backward compatibility and heteroge platform southbound apis and protocol plug ins interface neity i e to allow multiple protocols and device the forwarding elements the core of a controller platform management connectors therefore on the data plane a can be characterized as a combination of its base network mix of physical devices virtual devices e g open vswitch service functions and the various interfaces vrouter and a variety of device vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey fig distributed controllers east westbound apis interfaces e g openflow ovsdb of config netconf and snmp can coexist most controllers support only openflow as a south bound api still a few of them such as opendaylight onix and hp van sdn controller offer a wider range of southbound apis and or protocol plug ins onix supports both the openflow and ovsdb protocols the hp van sdn controller has other southbound connectors such as and agents opendaylight goes a step beyond by providing a service layer abstraction sla that allows several southbound apis and protocols to coexist in the control platform for instance its original architecture was designed to support at least seven different protocols and plug ins openflow ovsdb netconf pcep snmp bgp and lisp flow mapping hence openday light is one of the few control platforms being conceived to support a broader integration of technologies in a single control platform eastbound and westbound east westbound apis as illustrated in fig are a special case of interfaces required by distributed controllers currently each controller imple ments its own east westbound api the functions of these interfaces include import export data between controllers algorithms for data consistency models and monitoring notification capabilities e g check if a controller is up or notify a take over on a set of forwarding devices similarly to southbound and northbound interfaces east westbound apis are essential components of distrib uted controllers to identify and provide common compa tibility and interoperability between different controllers it is necessary to have standard east westbound interfaces for instance sdni defines common requirements to coordinate flow setup and exchange reachability informa tion across multiple domains in essence such protocols can be used in an orchestrated and interoperable way to create more scalable and dependable distributed control platforms interoperability can be leveraged to increase the diversity of the control platform element indeed diversity proceedings of the ieee vol no january increases the system robustness by reducing the probabil ity of common faults such as software faults other proposals that try to define interfaces between controllers include onix data import export functions forces ce ce interface forces intra ne cold standby mechanisms for high availability and distrib uted data stores an east westbound api requires advanced data distribution mechanisms such as the advanced message queuing protocol amqp used by disco techniques for distributed concurrent and consistent policy composition transactional databases and dhts as used in onix or advanced algorithms for strong consistency and fault tolerance in a multidomain setup east westbound apis may re quire also more specific communication protocols between sdn domain controllers some of the essential func tions of such protocols are to coordinate flow setup origi nated by applications exchange reachability information to facilitate inter sdn routing reachability update to keep the network state consistent among others another important issue regarding east westbound in terfaces is heterogeneity for instance besides communi cating with peer sdn controllers controllers may also need to communicate with subordinate controllers in a hierarchy of controllers and non sdn controllers as is the case of closed flow to be interoperable east westbound interfaces thus need to accommodate dif ferent controller interfaces with their specific set of ser vices and the diverse characteristics of the underlying infrastructure including the diversity of technology the geographic span and scale of the network and the distinc tion between wan and lanvpotentially across adminis trative boundaries in those cases different information has to be exchanged between controllers including adja cency and capability discovery topology information to the extent of the agreed contracts between administrative domains billing information among many others last an sdn compass methodology suggests a finer distinction between eastbound and westbound horizontal interfaces referring to westbound interfaces as sdn to sdn protocols and controller apis while eastbound interfaces would be used to refer to standard protocols used to communicate with legacy network con trol planes e g pcep and gmpls northbound current controllers offer a quite broad variety of northbound apis such as ad hoc apis restful apis multilevel programming interfaces file systems among other more specialized apis such as nvp nbapi and sdmn api section iv e is devoted to a more detailed discussion on the evolving layer of northbound apis a second kind of northbound interfaces are those stemming out of sdn programming languages such as frenetic nettle netcore procera pyretic netkat and other query based languages section iv g gives a more detailed overview of the several existing program ming languages for sdn wrapping up remarks and platforms comparison table shows a summary of some of the existing con trollers with their respective architectures and character table controllers classification kreutz et al software defined networking a comprehensive survey istics as can be observed most controllers are centralized and multithreaded curiously the northbound api is very diverse in particular five controllers onix floodlight mul meridian and sdn unified controller pay a bit more attention to this interface as a statement of its im portance consistency models and fault tolerance are only present in onix hyperflow hp van sdn onos and smartlight last when it comes to the openflow stan dard as southbound api only ryu supports its three major versions and to conclude it is important to emphasize that the control platform is one of the critical points for the success of sdn one of the main issues that needs to be addressed in this respect is interoperability this is rather interesting as it was the very first problem that south bound apis such as openflow tried to solve for instance while wifi and long term evolution lte net works need specialized control platforms such as mobileflow or softran data center networks have different requirements that can be met with plat forms such as onix or opendaylight for this reason in environments where diversity of networking vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey 34 proceedings of the ieee vol no january infrastructures is a reality coordination and cooperation plane fault tolerance and a variety of basic building blocks between different controllers is crucial standardized apis to ease software module and application development for multicontroller and multidomain deployments are sfnet is another example of a northbound inter therefore seen as an important step to achieve this goal face it is a high level api that translates application re quirements into lower level service requests however e layer v northbound interfaces sfnet has a limited scope targeting queries to request the the northbound and southbound interfaces are two key congestion state of the network and services such as band abstractions of the sdn ecosystem the southbound inter width reservation and multicast face has already a widely accepted proposal openflow other proposals use different approaches to allow ap but a common northbound interface is still an open issue plications to interact with controllers the yanc control at this moment it may still be a bit too early to define a platform explores this idea by proposing a general standard northbound interface as use cases are still being control platform based on linux and abstractions such as worked out anyway it is to be expected a common the virtual file system vfs this approach simplifies the or ade facto northbound interface to arise as sdn evolves development of sdn applications as programmers are able an abstraction that would allow network applications not to use a traditional concept files to communicate with to depend on specific implementations is important to lower level devices and subsystems explore the full potential of sdn eventually it is unlikely that a single northbound in the northbound interface is mostly a software eco terface emerges as the winner as the requirements for system not a hardware one as is the case of the south different network applications are quite different apis for bound apis security applications are likely to be different from those in these ecosystems the implementation is commonly for routing or financial applications one possible path of the forefront driver while standards emerge later and are evolution for northbound apis are vertically oriented pro essentially driven by wide adoption nevertheless an posals before any type of standardization occurs a chal initial and minimal standard for northbound interfaces can lenge the onf has started to undertake in the nbi wg in still play an important role for the future of sdn discussions parallel to open source sdn developments the onf about this issue have already begun 244 and a architectural work includes the possibility of north common consensus is that northbound apis are indeed bound apis providing resources to enable dynamic and important but that it is indeed too early to define a single granular control of the network resources from customer standard right now the experience from the development applications eventually across different business and orga of different controllers will certainly be the basis for nizational boundaries coming up with a common application level interface there are also other kinds of apis such as those pro open and standard northbound interfaces are crucial to vided by the pane controller designed to be suit promote application portability and interoperability among able for the concept of participatory networking pane the different control platforms a northbound api can be allows network administrators to define module specific compared to the posix standard in operating sys quotas and access control policies on network resources tems representing an abstraction that guarantees program the controller provides an api that allows end host appli ming language and controller independence nosix cations to dynamically and autonomously request network is one of the first examples of an effort in this direction it resources for example audio e g voip and video ap tries to define portable low level e g flow model appli plications can easily be modified to use the pane api to cation interfaces making southbound apis such as open reserve bandwidth for certain quality guarantees during flow look like device drivers however nosix is not the communication session pane includes a compiler and exactly a general purpose northbound interface but rather verification engine to ensure that bandwidth requests do a higher level abstraction for southbound interfaces in not exceed the limits set by the administrator and to avoid deed it could be part of the common abstraction layer in a starvation i e other applications will not be impaired by control platform as the one described in section iv d new resource requests existing controllers such as floodlight trema nox onix and opendaylight propose and define their own f layer vi language based virtualization northbound apis 247 however each of them has two essential characteristics of virtualization solutions its own specific definitions programming languages such are the capability of expressing modularity and of allowing as frenetic nettle netcore procera different levels of abstractions while still guaranteeing de pyretic and netkat also abstract the sired properties such as protection for instance virtua inner details of the controller functions and data plane lization techniques can allow different views of a single behavior from the application developers moreover as we physical infrastructure as an example one virtual big explain in section iv g programming languages can pro switch could represent a combination of several underly vide a wide range of powerful abstractions and mechan ing forwarding devices this intrinsically simplifies the isms such as application composition transparent data task of application developers as they do not need to think kreutz et al software defined networking a comprehensive survey about the sequence of switches where forwarding rules ensures that properties such as isolation are enforced have to be installed but rather see the network as a simple among slices i e no packets of slice a can traverse to big switch such kind of abstraction significantly simpli slice b unless explicitly allowed fies the development and deployment of complex network other solutions such as libnetvirt try to integ applications such as advanced security related services rate heterogeneous technologies for creating static net pyretic is an interesting example of a program work slices libnetvirt is a library designed to provide a ming language that offers this type of high level abstrac flexible way to create and manage virtual networks in dif tion of network topology it incorporates this concept of ferent computing environments its main idea is similar to abstraction by introducing network objects these objects the openstack quantum project while quantum is consist of an abstract network topology and the sets of designed for openstack cloud environments libnetvirt policies applied to it network objects simultaneously hide is a more general purpose library which can be used in information and offer the required services different environments additionally it goes one step be another form of language based virtualization is static yond openstack quantum by enabling qos capabilities in slicing this a scheme where the network is sliced by a virtual networks the libnetvirt library has two compiler based on application layer definitions the out layers a generic network interface and technology specific put of the compiler is a monolithic control program that device drivers e g vpn mpls openflow on top of has already slicing definitions and configuration com the layers are the network applications and virtual network mands for the network in such a case there is no need for descriptions the openflow driver uses a nox controller a hypervisor to dynamically manage the network slices to manage the underlying infrastructure using openflow static slicing can be valuable for deployments with specific rule based flow tables to create isolated virtual networks requirements in particular those where higher perfor by supporting different technologies it can be used as a mance and simple isolation guarantees are preferable to bridging component in heterogeneous networks dynamic slicing table summarizes the hypervisor and nonhypervisor one example of static slicing approach is the splendid based virtualization technologies as can be observed only isolation in this solution the network slices are libnetvirt supports heterogeneous technologies not re made of three components topology consisting of stricting its application to openflow enabled networks switches ports and links mapping of slice level flowvisor autoslice and openvirtex allow multiple con switches ports and links on the network infrastructure trollers one per network slice flown provides a container and predicates on packets where each port of the slice based approach where multiple applications from different edge switches has an associated predicate the topology is a users can coexist on a single controller flowvisor allows simple graph of the sliced nodes ports and links mapping qos provisioning guarantees by using vlan pcp bits for will translate the abstract topology elements into the priority queues sdn ve and nvp also provide their own corresponding physical ones the predicates are used to provisioning methods for guaranteeing qos indicate whether a packet is permitted to enter a specific slice different applications can be associated to each slice g layer vii programming languages the compiler takes the combination of slices topology programming languages have been proliferating for mapping and predicates and respective programs to gen decades both academia and industry have evolved from erate a global configuration for the entire network it also low level hardware specific machine languages such as table virtualization solutions vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey proceedings of the ieee vol no january assembly for architectures to high level and powerful another interesting feature that programming lan programming languages such as java and python the ad guage abstractions provide is the capability of creating and vancements toward more portable and reusable code writing programs for virtual network topologies have driven a significant shift on the computer industry this concept is similar to object oriented prog ramming where objects abstract both data and specific similarly programmability in networks is starting to functions for application developers making it easier to move from low level machine languages such as openflow focus on solving a particular problem without worrying assembly to high level programming languages about data structures and their management for instance assembly like machine lan in an sdn context instead of generating and installing guages such as openflow and pof essen rules in each forwarding device one can think of creating tially mimic the behavior of forwarding devices forcing simplified virtual network topologies that represent the developers to spend too much time on low level details entire network or a subset of it for example the appli rather than on the problem solve raw openflow programs cation developer should be able to abstract the network as have to deal with hardware behavior details such as over an atomic big switch rather than a combination of sev lapping rules the priority ordering of rules and data plane eral underlying physical devices the programming inconsistencies that arise from in flight packets whose flow languages or runtime systems should be responsible for rules are under installation the use of generating and installing the lower level instructions these low level languages makes it difficult to reuse soft required at each forwarding device to enforce the user ware to create modular and extensive code and leads to a policy across the network with such kind of abstractions more error prone development process developing a routing application becomes a straightfor abstractions provided by high level programming ward process similarly a single physical switch could be languages can significantly help address many of the chal represented as a set of virtual switches each of them lenges of these lower level instruction sets belonging to a different virtual network these two exam in sdns high level programming languages ples of abstract network topologies would be much harder can be designed and used to to implement with low level instruction sets in contrast create higher level abstractions for simplifying the a programming language or runtime system can more task of programming forwarding devices easily provide abstractions for virtual network topologies enable more productive and problem focused en as has already been demonstrated by languages such as vironments for network software programmers pyretic speeding up development and innovation promote software modularization and code reus high level sdn programming languages high level ability in the network control plane programming languages can be powerful tools as a mean foster the development of network virtualization for implementing and providing abstractions for different several challenges can be better addressed by program important properties and functions of sdn such as ming languages in sdns for instance in pure openflow network wide structures distributed updates modular based sdns it is hard to ensure that multiple tasks of a composition virtualization and formal verification single application e g routing monitoring access con low level instruction sets suffer from several pro trol do not interfere with each other for example rules blems to address some of these challenges higher level generated for one task should not override the function programming languages have been proposed with diverse ality of another task another example is goals such as when multiple applications run on a single controller avoiding low level and device specific configura typically each applica tions and dependencies spread across the network tion generates rules based on its own needs and policies as happens in traditional network configuration without further knowledge about the rules generated by approaches other applications as a consequence conflicting rules can providing abstractions that allow different man be generated and installed in forwarding devices which agement tasks to be accomplished through easy to can create problems for network operation programming understand and maintain network policies languages and runtime systems can help to solve these decoupling of multiple tasks e g routing access problems that would be otherwise hard to prevent control traffic engineering important software design techniques such as code implementing higher level programming interfaces modularity and reusability are very hard to achieve using to avoid low level instruction sets low level programming models applications thus solving forwarding rules problems e g conflicting built are monolithic and consist of building blocks that or incomplete rules that can prevent a switch event cannot be reused in other applications the end result is to be triggered in an automated way a very time consuming and error prone development addressing different race condition issues which process are inherent to distributed systems enhancing conflict resolution techniques on envi ronments with distributed decision makers providing native fault tolerance capabilities on data plane path setup reducing the latency in the processing of new flows easing the creation of stateful applications e g stateful firewall programming languages can also provide specialized abstractions to cope with other management require ments such as monitoring for instance the runtime system of a programming language can do all the laundry work of installing rules polling the counters receiving the responses combining the re sults as needed and composing monitoring queries in conjunction with other policies consequently application developers can take advantage of the simplicity and power of higher level query instructions to easily implement mo nitoring modules or applications another aspect of paramount importance is the portability of the programming language necessary so that developers do not need to re implement applications for different control platforms the portability of a prog ramming language can be considered as a significant added value to the control plane ecosystem mechanisms such as decoupled back ends could be key architectural ingredi ents to enable platform portability similarly to the java virtual machine a portable northbound interface will table programming languages kreutz et al software defined networking a comprehensive survey easily allow applications to run on different controllers without requiring any modification as an example the pyretic language requires only a standard socket interface and a simple openflow client on the target controller platform several programming languages have been proposed for sdns as summarized in table the great majority propose abstractions for openflow enabled networks the predominant programming paradigm is the declarative one with a single exception pyretic which is an impera tive language most declarative languages are functional but there are instances of the logic and reactive types the purposevi e the specific problems they intend to solvev and the expressiveness power vary from language to lan guage while the end goal is almost always the same to provide higher level abstractions to facilitate the develop ment of network control logic programming languages such as fml nettle and procera are functional and reactive poli cies and applications written in these languages are based on reactive actions triggered by events e g a new host connected to the network or the current network load such languages allow users to declaratively express differ ent network configuration rules such as access control lists acls virtual lans vlans and many others rules are essentially expressed as allow or deny policies which are applied to the forwarding elements to ensure the desired network behavior vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey proceedings of the ieee vol no january other sdn programming languages such as frenetic existing systems to achieve this goal merlin generates hierarchical flow tables hfts netcore specific code for each type of component taking a policy and pyretic were designed with the simul definition as input merlin compiler determines forward taneous goal of efficiently expressing packet forwarding ing paths transformation placement and bandwidth policies and dealing with overlapping rules of different allocation the compiled outputs are sets of component applications offering advanced operators for parallel and specific low level instructions to be installed in the devices sequential composition of software modules to avoid merlin policy language also allows operators to delegate overlapping conflicts frenetic disambiguates rules with the control of a subnetwork to tenants while ensuring iso overlapping patterns by assigning different integer prior lation this delegated control is expressed by means of ities while hft uses hierarchical policies with enhanced policies that can be further refined by each tenant owner conflict resolution operators allowing them to customize policies for their particular see every packet abstractions and race free semantics needs also represent interesting features provided by program other recent initiatives e g systems programming ming languages such as frenetic the former en languages target problems such as detecting ano sures that all control packets will be available for analysis malies to improve the security of network protocols e g sooner or later while the latter provides the mechanisms open flow and optimizing horizontal scalability for for suppressing unimportant packets as an example achieving high throughput in applications running on packets that arise from a network race condition such as multicore architectures nevertheless there is still due to a concurrent flow rule installation on switches can scope for further investigation and development on prog be simply discarded by the runtime system ramming languages for instance one recent research has advanced operators for parallel and sequential com revealed that current policy compilers generate unneces position help bind through internal workflow operators sary redundant rule updates most of which modify only the key characteristics of programming languages such as the priority field pyretic parallel composition makes it possible to most of the value of sdn will come from the network operate multiple policies on the same set of packets while managements applications built on top of the infrastruc sequential composition facilitates the definition of a se ture advances in high level programming languages are a quential workflow of policies to be processed on a set of fundamental component to the success of a prolific sdn packets sequential policy processing allows multiple application development ecosystem to this end efforts modules e g access control and routing to operate in a are undergoing to shape forthcoming standard interfaces cooperative way by using sequential composition complex cf and toward the realization of integrated devel applications can be built out of a combination of different opment environments e g netide with the goal modules in a similar way as pipes can be used to build of fostering the development of a myriad of sdn applica sophisticated unix applications tions we discuss these next further advanced features are provided by other sdn programming languages fattire is an example of a h layer viii network applications declarative language that heavily relies on regular expres network applications can be seen as the network sions to allow programmers to describe network paths with brains they implement the control logic that will be fault tolerance requirements for instance each flow can translated into commands to be installed in the data plane have its own alternative paths for dealing with failure of dictating the behavior of the forwarding devices take a the primary paths interestingly this feature is provided in simple application as routing as an example the logic of a very programmer friendly way with the application this application is to define the path through which packets programmer having only to use regular expressions with will flow from point a to point b to achieve this goal a special characters such as an asterisk in the particular routing application has to based on the topology input case of fattire an asterisk will produce the same behavior decide on the path to use and instruct the controller to as a traditional regular expression but translated into install the respective forwarding rules in all forwarding alternative traversing paths devices on the chosen path from a to b programming languages such as flowlog and sdns can be deployed on any traditional network en flog bring different features such as model check vironment from home and enterprise networks to data ing dynamic verification and stateful middleboxes for centers and internet exchange points such variety of en instance using a programming language such as flog it is vironments has led to a wide array of network applications possible to build a stateful firewall application with only existing network applications perform traditional func five lines of code tionality such as routing load balancing and security po merlin is one of the first examples of unified licy enforcement but also explore novel approaches such framework for controlling different network components as reducing power consumption other examples include such as forwarding devices middleboxes and end hosts fail over and reliability functionalities to the data plane an important advantage is backward compatibility with end to end qos enforcement network virtualization kreutz et al software defined networking a comprehensive survey mobility management in wireless networks among many leveraged to optimize the energy consumption of the net others the variety of network applications combined with work by using specialized optimization algorithms real use case deployments is expected to be one of the and diversified configuration options it is possible to meet major forces on fostering a broad adoption of sdn the infrastructure goals of latency performance and fault despite the wide variety of use cases most sdn appli tolerance for instance while reducing power consump cations can be grouped in one of five categories traffic tion with the use of simple techniques such as shutting engineering mobility and wireless measurement and mo down links and devices intelligently in response to traffic nitoring security and dependability and data center net load dynamics data center operators can save up to of working tables and summarize several applications the network energy in normal traffic conditions categorized as such stating their main purpose controller one of the important goals of data center networks is to where it was implemented evaluated and southbound avoid or mitigate the effect of network bottlenecks on the api used operation of the computing services offered linear bisec tion bandwidth is a technique that can be adopted for traffic engineering several traffic engineering appli traffic patterns that stress the network by exploring path cations have been proposed including elastictree diversity in a data center topology such technique has hedera openflow based server load balancing been proposed in an sdn setting allowing the maximiza plug n serve and aster x in packet tion of aggregated network utilization with minimal sched bloom filter sim ple 292 qnox qos uling overhead sdn can also be used to provide a framework qos for sdn alto fully automated system for controlling the configuration of viaggre sdn procel flowqos and routers this can be particularly useful in scenarios that middlepipes in addition to these recent proposals apply virtual aggregation this technique allows net include optimization of rules placement the use of work operators to reduce the data replicated on routing mac as a universal label for efficient routing in data cen tables which is one of the causes of routing tables growth ters among other techniques for flow management a specialized routing application can calculate fault tolerance topology update and traffic characteriza divide and configure the routing tables of the different rout tion the main goal of most applications is to engineer ing devices through a southbound api such as openflow traffic with the aim of minimizing power consumption traffic optimization is another interesting application maximizing aggregate network utilization providing opti for large scale service providers where dynamic scale out mized load balancing and other generic traffic optimiza is required for instance the dynamic and scalable provi tion techniques sioning of vpns in cloud infrastructures using protocolols load balancing was one of the first applications envi such as alto can be simplified through an sdn sioned for sdn openflow different algorithms and tech based approach recent work has also shown that niques have been proposed for this purpose optimizing rules placement can increase network effi one particular concern is the scalability of these ciency solutions such as procel designed for solutions a technique to allow this type of applications to cellular core networks are capable of reducing the signal scale is to use wildcard based rules to perform proactive ing traffic up to which represents a significant load balancing wildcards can be utilized for aggre achievement gating client requests based on the ranges of ip prefixes other applications that perform routing and traffic en for instance allowing the distribution and directing of gineering include application aware networking for video large groups of client requests without requiring controller and data streaming 345 and improved qos by intervention for every new flow in tandem operation in employing multiple packet schedulers and other reactive mode may still be used when traffic bursts are techniques 289 as traffic engineer detected the controller application needs to monitor the ing is a crucial issue in all kinds of networks upcoming network traffic and use some sort of threshold in the flow methods techniques and innovations can be expected in counters to redistribute clients among the servers when the context of sdns bottlenecks are likely to happen sdn load balancing also simplifies the placement of mobility and wireless the current distributed control network services in the network every time a new plane of wireless networks is suboptimal for managing the server is installed the load balancing service can take the limited spectrum allocating radio resources implement appropriate actions to seamlessly distribute the traffic ing handover mechanisms managing interference and among the available servers taking into consideration both performing efficient load balancing between cells sdn the network load and the available computing capacity of based approaches represent an opportunity for making it the respective servers this simplifies network manage easier to deploy and manage different types of wireless ment and provides more flexibility to network operators networks such as wlans and cellular networks existing southbound interfaces can be used for actively traditionally hard to monitoring the data plane load this information can be implement but desired features are indeed becoming a vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey table network applications proceedings of the ieee vol no january reality with the sdn based wireless networks these in clude seamless mobility through efficient handovers load balancing creation of on demand virtual access points vaps down link scheduling e g an openflow switch can do a rate shaping or time division dynamic spectrum usage enhanced intercell interference coordination device to device offloading i e decide when and how lte transmissions should be offloaded to users adopt ing the device to device paradigm per client and or base station resource block allocations i e time and frequency slots in lte orthogonal frequency division multiple access ofdma networks which are known as resource blocks control and assign transmission and power parameters in devices or in a group basis e g algorithms to optimize the transmission and power parameters of wlan devices define and assign transmission power values to each resource block at each base station in lte ofdma networks sim plified administration easy manage ment of heterogeneous network technologies interoperability between different networks shared wireless infrastructures seamless sub scriber mobility and cellular networks qos and ac cess control policies made feasible and easier 348 and easy deployment of new applications one of the first steps toward realizing these features in wireless networks is to provide programmable and flexible stack layers for wireless networks one of the first examples is openradio which proposes a soft table network applications kreutz et al software defined networking a comprehensive survey ware abstraction layer for decoupling the wireless protocol definition from the hardware allowing shared mac layers across different protocols using commodity multicore plat forms openradio can be seen as the openflow for wire less networks similarly softran proposes to rethink the radio access layer of current lte infrastruc tures its main goal is to allow operators to improve and optimize algorithms for better handovers fine grained control of transmit powers resource block allocation among other management tasks light virtual access points lvaps is another interest ing way of improving the management capabilities of wire less networks as proposed by the odin framework differently from openradio it works with existing wireless hardware and does not impose any change on ieee standards an lvap is implemented as a unique basic service set identification associated with a specific client which means that there is a one to one mapping between lvaps and clients this per client access point ap abstraction simplifies the handling of client associations authentication handovers and unified slicing of both wired and wireless portions of the network odin achieves control logic isolation between slices since lvaps are the primitive type upon which applications make control decisions and applications do not have visibility of lvaps from outside their slice this empowers infrastruc ture operators to provide services through odin applica tions such as a mobility manager client based load balancer channel selection algorithm and wireless trou bleshooting application within different network slices for vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey proceedings of the ieee vol no january instance when a user moves from one ap to another the other initiatives of this second class propose a stronger network mobility management application can automati decoupling between basic primitives e g matching and cally and proactively act and move the client lvap from counting and heavier traffic analysis functions such as the one ap to the other in this way a wireless client will not detection of anomaly conditions attacks a stronger even notice that it started to use a different ap because separation favors portability and flexibility for instance a there is no perceptive handoff delay as it would be the case functionality to detect abnormal flows should not be con in traditional wireless networks strained by the basic primitives or the specific hardware very dense heterogeneous wireless networks have also implementation put in another way developers should be been a target for sdn these densenets have limitations empowered with streaming abstractions and higher level due to constraints such as radio access network bottle programming capabilities necks control overhead and high operational costs in that vein some data and control plane abstractions a dynamic two tier sdn controller hierarchy can be have been specifically designed for measurement purposes adapted to address some of these constraints local opensketch is a special purpose southbound api controllers can be used to take fast and fine grained deci designed to provide flexibility for network measurements sions while regional or global controllers can have a for instance by allowing multiple measurement tasks to broader coarser grained scope i e that take slower but execute concurrently without impairing accuracy the in more global decisions in such a way designing a single ternal design of an opensketch switch can be thought of as integrated architecture that encompasses lte macro pico a pipeline with three stages hashing classification and femto and wifi cells while challenging seems feasible counting input packets first pass through a hashing func tion then they are classified according to a matching measurement and monitoring measurement and mo rule finally the match rule identifies a counting index nitoring solutions can be divided into two classes first which is used to calculate the counter location in the applications that provide new functionality for other net counting stage while a tcam with few entries is enough working services and second proposals that target to im for the classification stage the flexible counters are stored prove features of openflow based sdns such as to reduce in sram this makes the opensketch operation efficient control plane overload due to the collection of statistics fast matching and cost effective cheaper srams to store an example of the first class of applications is improving counters the visibility of broadband performance an other monitoring frameworks such as opensample sdn based broadband home connection can simplify the and payless propose different mechanisms for addition of new functions in measurement systems such as delivering real time low latency and flexible monitoring bismark allowing the system to react to changing capabilities to sdn without impairing the load and perfor conditions in the home network as an example a home mance of the control plane the proposed solutions take gateway can perform reactive traffic shaping considering advantage of sampling technologies like sflow to the current measurement results of the home network monitor high speed networks and flexible collections of the second class of solutions typically involve different loosely coupled plug and play components to provide ab kinds of sampling and estimation techniques to be applied stract network views yielding high performance and effi in order to reduce the burden of the control plane with cient network monitoring approaches respect to the collection of data plane statistics different techniques have been applied to achieve this goal such as security and dependability an already diverse set of stochastic and deterministic packet sampling techniques security and dependability proposals is emerging in the traffic matrix estimation fine grained moni context of sdns most take advantage of sdn for improv toring of wildcard rules two stage bloom filters ing services required to secure systems and networks such to represent monitoring rules and provide high measure as policy enforcement e g access control firewalling ment accuracy without incurring in extra memory or con middleboxes as middlepipes trol plane traffic overhead and special monitoring dos attacks detection and mitigation functions extensions to openflow in forwarding devices random host mutation i e randomly and frequently to reduce traffic and processing load on the control plane mutate the ip addresses of end hosts to break the attackers point to point traffic matrix estimation in partic assumption about static ips which is the common case ular can help in network design and operational tasks monitoring of cloud infrastructures for fine grained such as load balancing anomaly detection capacity plan security inspections i e automatically analyze and detour ning and network provisioning with information on the suspected traffic to be further inspected by specialized set of active flows in the network routing information network security appliances such as deep packet inspec e g from the routing application flow paths and flow tion systems traffic anomaly detection counters in the switches it is possible to construct a traffic fine grained flow based network access control matrix using diverse aggregation levels for sources and fine grained policy enforcement for personal mobile destinations applications and so on kreutz et al software defined networking a comprehensive survey others address openflow based view of sdn security issues and challenges can be found in networks issues such as flow rule prioritization security section v f services composition protection against traffic overload and protection against malicious administrators data center networking from small enterprises to large scale cloud providers most of the existing it systems there are essentially two approaches one involves and services are strongly dependent on highly scalable and using sdns to improve network security and another for efficient data centers yet these infrastructures still pose improving the security of the sdn itself the focus has significant challenges regarding computing storage and been thus far in the latter networking concerning the latter data centers should be designed and deployed in such a way as to offer high and using sdn to improve the security of current networks flexible cross section bandwidth and low latency qos probably the first instance of sdn was an application for based on the application requirements high levels of resi security policies enforcement an sdn allows the lience intelligent resource utilization to reduce energy enforcement to be done on the first entry point to the consumption and improve overall efficiency agility in network e g the ethernet switch to which the user is provisioning network resources for example by means of connected to alternatively in a hybrid environment network virtualization and orchestration with computing security policy enforcement can be made on a wider net and storage and so forth not surprisingly work perimeter through programmable devices without many of these issues remain open due to the complexity the need to migrate the entire infrastructure to openflow and inflexibility of traditional network architectures with either application malicious actions are the emergence of sdn is expected to change the cur blocked before entering the critical regions of the network rent state of affairs early research efforts have indeed sdn has been successfully applied for other purposes showed that data center networking can significantly be namely for the detection and reaction against distributed nefit from sdn in solving different problems such as live denial of service ddos flooding attacks and active network migration improved network management security openflow forwarding devices make it eminent failure avoidance rapid easier to collect a variety of information from the network deployment from development to production networks in a timely manner which is very handy for algorithms troubleshooting optimization of net specialized in detecting ddos flooding attacks work utilization dynamic and the capabilities offered by sdns in increasing the ability elastic provisioning of middleboxes as a service and to collect statistics data from the network and of allowing minimization of flow setup latency and reduction of con applications to actively program the forwarding devices are troller operating costs sdn can also offer network powerful for proactive and smart security policy enforcement ing primitives for cloud applications solutions to predict techniques such as active security this novel security network transfers of applications mechanisms methodology proposes a novel feedback loop to improve the for fast reaction to operation problems network aware vm control of defense mechanisms of a networked infrastruc placement qos support real time ture and is centered around five core capabilities protect network monitoring and problem detection sense adjust collect and counter in this perspective active security policy enforcement services and mechan security provides a centralized programming interface that isms and enable programmatic adaptation of simplifies the integration of mechanisms for detecting transport protocols attacks by collecting data from different sources to sdn can help infrastructure providers to expose more identify attacks converging to a consistent configuration networking primitives to their customers by allowing vir for the security appliances and enforcing counter tual network isolation custom addressing and the place measures to block or minimize the effect of attacks ment of middleboxes and virtual desktop cloud applications to fully explore the potential of virtual net improving the security of sdn itself there are already works in clouds an essential feature is virtual network some research efforts in identifying the critical security migration similarly to traditional virtual machine migra threats of sdns and in augmenting its security and depen tion a virtual network may need to be migrated when its dability early approaches try to apply virtual machines move from one place to another integ simple techniques such as classifying applications and rating live migration of virtual machines and virtual net using rule prioritization to ensure that rules generated by works is one of the forefront challenges to achieve security applications will not be overwritten by lower this goal it is necessary to dynamically reconfigure all priority applications other proposals try to go a step affected networking devices physical or virtual this was further by providing a framework for developing security shown to be possible with sdn platforms such as nvp related applications in sdns 259 however there is still another potential application of sdn in data centers is a long way to go in the development of secure and in detecting abnormal behaviors in network operation dependable sdn infrastructures an in deep over by using different behavioral models and collecting vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey proceedings of the ieee vol no january the necessary information from elements involved in the debugging and troubleshooting in networking is at a operation of a data center infrastructure operators appli very primitive stage in traditional networks engineers cations it is possible to continuously build signatures for and developers have to use tools such as ping applications by passively capturing control traffic then traceroute tcpdump nmap netflow and the signature history can be used to identify differences in snmp statistics for debugging and troubleshooting behavior every time a difference is detected operators debugging a complex network with such primitive tools can reactively or proactively take corrective measures this is very hard even when one considers frameworks such as can help to isolate abnormal components and avoid further xtrace netreplay and netcheck which damage to the infrastructure improve debugging capabilities in networks it is still diffi cult to troubleshoot networking infrastructures for in toward sdn app stores as can be observed in stance these frameworks require a huge effort in terms of tables and most sdn applications rely on nox and network instrumentation the additional complexity intro openflow nox was the first controller available for duced by different types of devices technologies and general use making it a natural choice for most use cases vendor specific components and features makes matters so far as indicated by the sheer number of security related worse as a consequence these solutions may find it hard to applications security is probably one of the killer appli be widely implemented and deployed in current networks cations for sdns curiously while most use cases rely on sdn offers some hope in this respect the hardware openflow new solutions such as softran are considering agnostic software based control capabilities and the use of different apis as is the case of the femto api open standards for control communication can potentially this diversity of applications and apis will most probably make debugging and troubleshooting easier the flexibility keep growing in sdn and programmability introduced by sdn is indeed opening there are other kinds of network applications that do new avenues for developing better tools to debug trouble not easily fit in our taxonomy such as avior oess shoot verify and test networks and sdn app store avior and oess early debugging tools for openflow enabled networks are graphical interfaces and sets of software tools that such as ndb ofrewind and netsight make it easier to configure and manage controllers e g make it easier to discover the source of network problems floodlight and openflow enabled switches respectively such as faulty device firmware inconsistent or non by leveraging their graphical functions it is possible to existing flow rules lack of reachability program openflow enabled devices without coding in a and faulty routing similarly to the particular programming language well known gdb software debugger ndb provides basic the sdn app store owned by hp is debugging actions such as breakpoint watch backtrace probably the first sdn application market store custo single step and continue these primitives help applica mers using hp openflow controller have access to the tion developers to debug networks in a similar way to online sdn app store and are able to select applications to traditional software by using ndb postcards i e a be dynamically downloaded and installed in the controller unique packet identifier composed of a truncated copy of the idea is similar to the android market or the apple the packet header the matching flow entry the switch store making it easier for developers to provide new and the output port for instance a programmer is able to applications and for customers to obtain them quickly identify and isolate a buggy openflow switch with hardware or software problems if the switch is presenting i cross layer issues abnormal behavior such as corrupting parts of the packet in this section we look at cross layer problems such as header by analyzing the problematic flow sequences with debugging and troubleshooting testing verification simula a debugging tool one can find in a matter of few seconds tion and emulation a summary of the existing tools for where the packets of a flow are being corrupted and take dealing with these cross layer issues can be found on table the necessary actions to solve the problem the ofrewind tool works differently the idea is debugging and troubleshooting debugging and trou to record and replay network events in particular control bleshooting have been important subjects in computing messages these usually account for less than of the infrastructures parallel and distributed systems embed data traffic and are responsible for of the bugs ded systems and desktop applications the this tool allows operators to perform fine grained two predominant strategies applied to debug and trouble tracing of network behavior being able to decide which shoot are runtime debugging e g gdb like tools and subsets of the network will be recorded and afterwards post mortem analysis e g tracing replay and visualiza select specific parts of the traces to be replayed these tion despite the constant evolution and the emergence of replays provide valuable information to find the root cause new techniques to improve debugging and troubleshoot of the network misbehavior likewise netrevert ing there are still several open avenues and research also records the state of openflow networks however questions the primary goal is not to reproduce network behavior but table debugging verification and simulation kreutz et al software defined networking a comprehensive survey vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey proceedings of the ieee vol no january rather to provide rollback recovery in case of failures allow the development and execution of a rich set of tests which is a common approach used in distributed systems on openflow enabled devices its ultimate goal is to mea for eliminating transient errors in nodes sure the processing capacity and bottlenecks of control despite the availability of these debugging and verifi applications and forwarding devices with this tool users cation tools it is still difficult to answer questions such as are able to observe and evaluate forwarding table consis what is happening to my packets that are flowing from tency flow setup latency flow space granularity packet point a to point b what path do they follow what header modification types and traffic monitoring capabilities modifications do they undergo on the way to answer e g counters some of these questions one could recur to the history of flowchecker often and veriflow the packets a packet history corresponds to the paths it are three examples of tools to verify correctness properties uses to traverse the network and the header modifications violations on the system while the former two are based in each hop of the path netsight is a platform whose on offline analysis the latter is capable of online checking primary goal is to allow applications that use the history of of network invariants verification constraints include the packets to be built in order to find out problems in a security and reachability issues configuration updates on network this platform is composed of three essential ele the network loops black holes etc ments netsight with its dedicated servers that receive other formal modeling techniques such as alloy can and process the postcards for building the packet history be applied to sdns to identify unexpected behavior the netsigh switchassist which can be used in for instance a protocol specification can be weak when it switches to reduce the processing burden on the dedicated under specifies some aspects of the protocol or due to a servers and the netsight hostassist to generate and very specific sequence of events in such situations model process postcards on end hosts e g in the hypervisor on a checking techniques such as alloy can help to find and virtualized infrastructure correct unexpected behaviors netwatch netshark and nprof tools such as flowguard are specifically de are three examples of tools built over netsight the signed to detect and resolve security policy violations in first one is a live network invariant monitor for instance openflow enabled networks flowguard is able to an alarm can be triggered every time a packet violates any examine on the fly network policy updates check indirect invariant e g no loops the second one netshark security violations e g openflow set field actions enables users to define and execute filters on the entire modification and perform stateful monitoring the frame history of packets with this tool a network operator can work uses five resolution strategies for real time security view a complete list of properties of packets at each hop policy violation resolution flow rejecting dependency such as input port output port and packet header values breaking update rejecting flow removing and packet finally nprof can be used to profile sets of network links blocking these resolutions are applied over diverse to provide data for analyzing traffic patterns and routing update situations in openflow enabled networks decisions that might be contributing to link load more recently tools such as vericon have been designed to verify the correctness of sdn applications in a testing and verification verification and testing tools large range of network topologies and by analyzing a broad can complement debugging and troubleshooting recent range of sequences of network events in particular research 385 has shown that vericon confirms or not the correct execution of the verification techniques can be applied to detect and avoid sdn program problems in sdn such as forwarding loops and black one of the challenges in testing and verification is to holes verification can be done at different layers at the verify forwarding tables in very large networks to find controllers network applications or network devices routing errors which can cause traffic losses and security additionally there are different network propertiesv breaches as quickly as possible in large scale networks it mostly topology specificvthat can be formally verified is not possible to assume that the network snapshot at any provided a network model is available examples of such point is consistent due to the frequent changes in routing properties are connectivity loop freedom and access con state therefore solutions such as hsa anteater trol a number of tools have also been proposed to netplumber veri flow and assertion evaluate the performance of openflow controllers by languages are not suited for this kind of environ emulating the load of large scale networks e g cbench ment another important issue is related on how fast the ofcbenchmark pktblaster similarly verification process is done especially in modern data benchmarking tools for openflow switches are also centers that have very tight timing requirements libra available e g oflops and flops turbo represents one of the first attempts to address these tools such as nice generate sets of diverse particular challenges of large scale networks this tool streams of packets to test as many events as possible ex provides the means for capturing stable and consistent posing corner cases such as race conditions similarly snapshots of large scale network deployments while also oflops provides a set of features and functions that applying long prefix matching techniques to increase the kreutz et al software defined networking a comprehensive survey scalability of the system by using mapreduce computa v ongoing research efforts tions libra is capable of verifying the correctness of a and challenges network with up to nodes within one minute anteater is a tool that analyzes the data plane state of network devices by encoding switch configurations as boolean satisfiability problem sat instances allowing to use a sat solver to analyze the network state the tool is capable of verifying violations of invariants such as loop free forwarding connectivity and consistency these invariants usually indicate a bug in the network i e their detection the research developments we have surveyed so far seek to overcome the challenges of realizing the vision and fulfilling the promises of sdn while section iv provided a per spective structured across the layers of the sdn stack this section highlights research efforts we consider of particular importance for unleashing the full potential of sdn and that therefore deserves a specific coverage in this survey helps to increase the reliability of the network data plane a switch designs simulation and emulation simulation and emulation currently available openflow switches are very di software is of particular importance for fast prototyping and verse and exhibit notable differences in terms of feature testing without the need for expensive physical devices set e g flow table size optional actions performance mininet is the first system that provides a quick and e g fast versus slow path control channel latency easy way to prototype and evaluate sdn protocols and throughput interpretation and adherence to the protocol applications one of the key properties of mininet is its use specification e g barrier command and architecture of software based openflow switches in virtualized con e g hardware versus software designs tainers providing the exact same semantics of hardware based openflow switches this means that controllers or heterogeneous implementations implementation applications developed and tested in the emulated envi choices have a fundamental impact on the behavior accu ronment can be in theory deployed in an openflow racy and performance of switches ranging from differ enabled network without any modification users can easily ences in flow counter behavior to a number of other emulate an openflow network with hundreds of nodes and performance metrics one approach to accommodate dozens of switches by using a single personal computer such heterogeneity is through nosix a portable api that mininet hifi is an evolution of mininet that en separates the application expectations from the switch hances the container based lightweight virtualization heterogeneity to do so nosix provides a pipeline with mechanisms to enforce performance isolation re of multiple virtual flow tables and switch drivers virtual source provisioning and accurate monitoring for perfor flow tables are intended to meet the expectations of appli mance fidelity one of the main goals of mininet hifi is to cations and are ultimately translated by the drivers into improve the reproducibility of networking research actual switch flow tables toward taming the complexity of mininet ce and sdn cloud dc are exten multiple openflow protocol versions with different sets of sions to mininet for enabling large scale simulations mini required and optional capabilities a roadblock for sdn net ce combines groups of mininet instances into one practitioners tinynbi has been proposed as a simple cluster of simulator instances to model global scale net api providing a unifying set of core abstractions of five works sdn cloud dc enhances mininet and pox to openflow protocol versions from to ongoing emulate an sdn based intra dc network by implementing efforts to introduce a new hal for non openflow capable new software modules such as data center topology discovery devices include the development of open source and network traffic generation recent emulation platform artifacts like revised openflow library rofl and the proposals that enable large scale experiments following a extensible datapath daemon xdpd a framework for distributed approach include max inet dot creating new openflow data path implementations based and cityflow the latter is a project with the main goal on a diverse set of hardware and software platforms a of building an emulated control plane for a city of one million related open source effort to develop a common library to inhabitants such initiatives are a starting point to provide implement openflow and protocol endpoints experimental insights for large scale sdn deployments switch agents and controllers is libfluid winner of the capability of simulating openflow devices has also the openflow driver competition organized by the onf been added to the popular ns simulator another within the onf the forwarding abstraction working simulator is fs sdn which extends the fs simulation engine group fawg is pursuing another solution to the hetero by incorporating a controller and switching compo geneity problem through table type patterns ttps nents with openflow support its main goal is to provide a a ttp is a standards based and negotiated switch level more realistic and scalable simulation platform as com behavioral abstraction it consists of the relationships pared to mininet finally sts is a simulator de between tables forming a graph structure the types of signed to allow developers to specify and apply a variety of tables in the graph a set of the parameterized table pro test cases while allowing them to interactively examine perties for each table in the graph the legal flow mod the state of the network and table mod commands for each flow table and the vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey proceedings of the ieee vol no january metadata mask that can be passed between each table pair agent inside the switch our current understanding in the graph indicates that an effective way forward is a native design of sdn switches consistent with the evolution of the south flow table capacity flow matching rules are stored bound api standardization activities in flow tables inside network devices one practical chal lenge is to provide switches with large and efficient flow evolving switch designs and hardware enhancements tables to store the rules tcams are a common choice as in any software hardware innovation cycle a number of to hold flow tables while flexible and efficient in terms of advancements are to be expected from the hardware per matching capabilities tcams are costly and usually small spective to improve sdn capabilities and performance from to entries some tcam chips today new sdn switch designs are appearing in a myriad of integrate million bit configured as entries ã hardware combinations to efficiently work together with bit per entry into a single chip working at mhz tcams such as static random access memory sram i e capable of million lookups per second dynamic random access memory dram reduced however these chips are expensive and have a high power latency dram graphics processing unit gpu field consumption representing a major power drain in a programmable gate array fpga network processors switching device these are some of the reasons why cpus among other specialized network processors currently available openflow devices have tcams with these early works suggest the need for roughly entries where the actual capacity in terms of additional efforts into new hardware architectures for openflow table size has a nontrivial relationship to the future sdn switching devices for instance some pro type of flow entries being used 427 openflow posals target technologies such as gpus that have demon version introduced multiple tables thereby adding strated gigabits per second gb with flow tables of extra flexibility and scalability indeed openflow up to million exact match entries and up to implied state explosion due to its flat table model wildcard entries alternatives to tcam based de however supporting multiple tables in hardware is signs include new hardware architectures and com challenging and limitedvyet another motivation for the ponents as well as new and more scalable forwarding ongoing onf fawg work on ttps planes such as the one proposed by the rain man firm some efforts focus on compression techniques to re ware other design solutions such as parallel lookup duce the number of flow entries in tcams models can also be applied to sdn to reduce costs in the espresso heuristic can be used to compress wild switching and routing devices recent proposals on cache cards of openflow based interdomain routing tables re like openflow switch arrangements shed some light ducing the forwarding information base fib by and on overcoming the practical limitations of flow table sizes consequently saving up to flow table entries with clever switching designs additionally counters re shadow macs propose label switching for solving present another practical challenge in sdn hardware im two problems consistent updates and rule space exhaus plementations many counters already exist and they tion by using opaque values similar to mpls labels to could lead to significant control plane monitoring over encode fine grained paths as labels a major benefit of head software defined counters sdcs have fixed size labels is relying on exact math lookups which been proposed to provide both scalability and flexibility can be easily and cost effectively implemented by simple application aware sdn architectures are being pro hardware tables instead of requiring rules to be encoded in posed to generalize the standard openflow forwarding expensive tcam tables abstractions by including stateful actions to allow process ing information from layers to to this end performance today the throughput of commercial application flow tables are proposed as data plane applica openflow switches varies from to flow mod tion modules that require only local state i e do not per second with most devices achieving a throughput depend on a global view of the network those tiny appli lower than flow mod per second this cation modules run inside the forwarding devices and can is clearly a limiting factor that will be addressed in the be installed on demand alleviating the overhead on the switch design processvsupport of openflow in existing control plane and augmenting the efficiency of certain product lines has been more a retrofitting activity than a tasks which can be kept in the data plane similarly other clean feature planning and implementation activity de initiatives propose solutions based on preinstalled state ployment experiences have pointed to a series of machines flow level state transition fast allows challenges stemming from the limited embedded cpu controllers to proactively program state transitions in for power of current commercial openflow switches one warding devices allowing switches to run dynamic actions approach to handle the problem consists of adding more that require only local information powerful cpus into the switches as proposed in other approaches toward evolving switch designs in others have proposed to rethink the distribution of control clude caching in buckets cab a reactive wildcard actions between external controllers and the openflow caching proposal that uses a geometric representation of kreutz et al software defined networking a comprehensive survey the rule set which is divided into small logical structures overcoming some of the limitations of openflow e g buckets through this technique cab is able to expressiveness support of user defined protocols memory solve the rule dependency problem and achieve efficient efficiency through generic flow instruction sets open usage of control plane resources namely bandwidth con source prototypes are available as well as evaluation troller processing load and flow setup latency results showing the line speed capabilities using a network new programmable ethernet switch chips such as processing unit npu based proof of concept imple xpliant ethernet are emerging into this new market mentation in this line we already mentioned openstate of programmable networks its main aim is enabling new another initiative that aims to augment the capabi protocol support and the addition of new features through lity and flexibility of forwarding devices by taking advan software updates increasing flexibility one example of tage of extended finite state machines xfsms such flexibility is the support of geneve a recent openstate proposes an abstractionvas a super set effort toward generic network virtualization encapsulation of openflow primitivesvto enable stateful handling of protocols and openflow the throughput of the first openflow rules inside forwarding devices family of xpliant ethernet chip varies from gb to in the same way as ttps allow controllers to compile terabits per second tb supporting up to ports of the right set of low lever instructions known to be sup gbe or gbe for instance ported by the switches a new breed of switch referred to as microchip companies like intel are already shipping programmable protocol independent packet processor processors with flexible sdn capabilities to the market suggests an evolution path for openflow recent advances in general purpose cpu technol based on a high level compiler this proposal would allow ogy include a data plane development kit dpdk the functionality of programmable switches i e pipeline that allows high level programming of how data packets header parsing field matching to be not only specified by will be processed directly within network interface cards the controller but also changed in the field in this model prototype implementations of intel dpdk accelerated programmers are able to decide how the forwarding plane switch shows the potential to deliver high performance processes packets without caring about implementation sdn software switches this trend is likely to conti details it is then the compiler that transforms the impera nue since high speed and specialized hardware is needed tive program into a control flow graph that can be mapped to boost sdn performance and scalability for large real to different target switches world networks hardware programmable technologies such as fpga are widely used to reduce time and costs b controller platforms of hardware based feature implementations netfpga for in the sdn model the controller platform is a critical instance has been a pioneering technology used to imple pillar of the architecture and as such efforts are being ment openflow switches providing a commod devoted to turn sdn controllers into high performance ity cost effective prototyping solution another line of scalable distributed modular and highly available work on sdn data planes proposes to augment switches programmer friendly software distributed controller plat with fpga to remotely define the queue management forms in particular have to address a variety of challenges and scheduling behavior of packet switches finally deserving special consideration are the latency between recent developments have shown that state of the art forwarding devices and controller instances fault toler system on chip soc platforms such as the xilinx zynq ance load balancing consistency and synchronization board can be used to implement openflow devices among other issues operators should yielding gb throughput for flow supporting also be able to observe and understand how the combina dynamic updates tion of different functions and modules can impact their network native sdn switch designs most of the sdn switch as the sdn community learns from the development re design efforts so far follow an evolutionary approach to and operational experiences with openflow controllers retrofit openflow specific programmable features into e g beacon further advancements are expected in existing hardware layouts following common wisdom on terms of raw performance of controller implementations switch router designs and consolidated technologies e g including the exploitation of hierarchical designs and sram tcam fpga one departure from this approach optimized buffer sizing one approach to increase is the ongoing work on forwarding meta morphosis the performance of controllers is the iris io engine a reconfigurable match table model inspired from risc enabling significant increases in the flow setup rate of like pipeline architecture applied to switching chips this sdn controllers another way of reducing the control work illustrates the feasibility of realizing a minimal set of plane overhead is by keeping a compressed copy of the action primitives for flexible header processing in hard flow tables in the controller memory ware at almost no additional cost or power also in line with the core sdn goals of highly flexible and program modularity and flexibility a series of ongoing re mable hardware based data planes pof aims at search efforts target the modular and flexible composition vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey proceedings of the ieee vol no january of controllers raon proposes a recursive abstrac by automatically and transparently resolving conflicts in tion of openflow controllers where each controller sees other words statesman allows a safe composition of unco the controllers below as openflow switches open re ordinated or conflicting application actions search issues include the definition of suitable interfaces another recent approach to simplify network manage between the different layers in such a hierarchy of con ment is the idea of compositional sdn hypervisors trollers other open issues to be further investigated in this its main feature is allowing applications written in differ context are the east westbound apis and their use in ent languages or on different platforms to work together enabling suitable hierarchical designs to achieve scalabil in processing the same traffic the key integration compo ity modularity and security for instance each level nent is a set of simple prioritized lists of openflow rules of a hierarchy of controllers can offer different abstractions which can be generated by different programming lan and scopes for either intradata and interdata center rout guages or applications ing thus increasing scalability and modularity similarly from a security perspective each hierarchical level may high availability in production sdn controllers be a part of a different trust domain therefore east need to sustain healthy operation under the pressure of westbound interfaces between the different layers of con different objectives from the applications they host many trollers should be capable of enforcing both intradomain advances are called for in order to deal with potential risk and interdomain security policies vectors of controller based solutions certainly another important observation is that currently the many solutions will leverage on results from the distrib lack of modularity in most sdn controllers forces devel uted systems and security communities made over the last opers to re implement basic network services from scratch decade for instance recent efforts propose consistent in each new application fault tolerant data stores for building reliable distributed as in software engineering in general lack of modu controllers larity results in controller implementations that are another possible approach toward building low laten hard to build maintain and extendvand ultimately be cy highly available sdn controllers is to exploit controller come resistant to further innovations resembling tradi locality classical models of distributed sys tional hardware defined networks as surveyed in tems such as local and congest can be ex section iv g sdn programming abstractions e g pyretic plored to solve this problem those models can be used to introduce modularity in sdn applications and develop coordination protocols that enable each controller simplify their development altogether further research to take independent actions over events that take place in efforts e g corybantic try to achieve modularity in its local neighborhood sdn control programs other contributions toward another core challenge relates to the fundamental achieving modular controllers can be expected from other tradeoffs between the consistency model of state distribu areas of computer science e g principles from operating tion in distributed sdn controllers the consistency re system and best practices of modern cloud scale quirements of control applications and performance software applications to ease development the application should ideally not be aware of the vagaries of distributed state this im interoperability and application portability similarly to plies a strong consistency model which can be achieved forwarding device vendor agnosticism that stems from with distributed data stores as proposed recently standard southbound interfaces it is important to foster however keeping all control data in a consistent distri interoperability between controllers early initiatives to buted data store is unfeasible due to the inherent perfor ward more interoperable control platforms include porta mance penalties therefore hybrid solutions are likely to ble programming languages such as pyretic and east coexist requiring application developers to be aware of the westbound interfaces among controllers such as sdni tradeoffs and penalties of using or not a strong consis forces ce ce interface and forces tency model a tenet of the distributed onix controller intra ne mechanisms however these efforts are yet high availability can also be achieved through far from fully realizing controller interoperability and ap improved southbound apis and controller placement heu plication portability ristics and formal models these aim to maxi in contrast to pyretic pane maple mize resilience and scalability by allowing forwarding and corybantic which are restricted to traffic engi devices to connect to multiple controllers in a cost effec neering applications and or impose network state conflict tive and efficient way early efforts in this direction resolution at the application level making application de have already shown that forwarding devices connecting to sign and testing more complicated statesman pro two or three controllers can typically achieve high availa poses a framework to enable a variety of loosely coupled bility up to five nines and robustness in terms of control network applications to coexist on the same control plane plane connectivity it has also been shown without compromising network safety and performance that the number of required controllers is more dependent this framework makes application development simpler on the topology than on network size another kreutz et al software defined networking a comprehensive survey finding worth mentioning is the fact that for most common comes a challenge when considering critical control plane topologies and network sizes fewer than ten controllers functions such as those related to link failure detection or seem to be enough fast reaction decisions the resilience of an openflow network depends on fault tolerance in the data plane as in delegation of control to increase operational effi traditional networks but also on the high availability of ciency sdn controllers can delegate control functions to the logically centralized control plane functions hence report state and attribute value changes threshold crossing the resilience of sdn is challenging due to the multiple alerts hardware failures and so forth these notifications possible failures of the different pieces of the architecture typically follow a publish subscribe model i e controllers as noted in there is a lack of sufficient research and applications subscribe on demand to the particular and experience in building and operating fault tolerant class of notifications they are interested in in addition sdns google may be one of the few examples that these subsystems may provide resilience and trustworthi have proven that sdn can be resilient at scale a number ness properties of related efforts have some reasons for delegating control to the data plane started to tackle the concerns around control plane split include architectures the distributed controller architectures sur low latency response to a variety of network events veyed in section iv d are examples of approaches toward the amount of traffic that must be processed in the resilient sdn controller platforms with different tradeoffs data plane in particular in large scale networks in terms of consistency durability and scalability such as data centers on a detailed discussion on whether the cap theorem low level functions such as those byte or bit applies to networks panda et al argue that the oriented required by repetitive synchronous di tradeoffs in building consistent available and partition gital hierarchy sdh multiplex section tolerant distributed databases i e cap theorem are ap overhead plicable to sdn the cap theorem demonstrates that it is functions well understood and standardized such impossible for data store systems to simultaneously as encryption bip ais insertion mac achieve strong consistency availability and partition learning and codec control message ccm tolerance while availability and partition tolerance pro exchanges blems are similar in both distributed databases and net controller failure tolerance i e essential network works the problem of consistency in sdn relates to the functions should be able to keep a basic network consistent application of policies operation even when controllers are down considering an openflow network when a switch basic low level functions usually available in data detects a link failure port down event a notification plane silicon such as protection switching state is sent to the controller which then takes the required machines ccm counters and timers actions reroute computation precomputed backup path all those functions that do not add any value when lookup and installs updated flow entries in the required moved from the data to the control plane switches to redirect the affected traffic such reactive strong candidates for execution in the forwarding strategies imply high restoration time due to the necessary devices instead of being implemented in the control interaction with the controller and additional load on the platforms thus include oam icmp processing mac control channel one experimental work on openflow for learning neighbor discovery defect recognition and in carrier grade networks investigated the restoration process tegration this would not only reduce the overhead and measured a restoration times in the order of ms traffic and computing of the control plane but also im the delay introduced by the controller may in some prove network efficiency by keeping basic networking cases be prohibitive functions in the data plane in order to meet carrier grade requirements e g ms of recovery time protection schemes are required c resilience to mitigate the effects of a separate control plane suitable achieving resilient communication is a top purpose of protection mechanisms e g installation of preestablished networking as such sdns are expected to yield the same backup paths in the forwarding devices can be imple levels of availability as legacy and any new alternative mented by means of openflow group table entries using technology split control architectures as sdn are com fast fail over actions an openflow fault management monly questioned about their actual capability of approach similar to mpls global path protection being resilient to faults that may compromise the control could also be a viable solution provided that openflow to data plane communications and thus result in brain switches are extended with end to end path monitoring less networks indeed the malfunctioning of particular capabilities similarly to those specified by bidirectional sdn elements should not result in the loss of availability forwarding detection bfd such protection the relocation of sdn control plane functionality from schemes are a critical design choice for larger scale net inside the boxes to remote logically centralized loci be works and may also require considerable additional flow vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey proceedings of the ieee vol no january space by using primary and secondary path pairs prog as a result several efforts have been devoted to tackle rammed as openflow fast fail over group table entries a the sdn scaling concerns including devoflow path restoration time of ms has been reported sdcs difane onix hyperflow using bfd sessions to quickly detect link failures kandoo maestro nox mt and maple on a related line of data plane resilience slickflow still related to scalability the notion of elasticity in leverages the idea of using packet header space to sdn controllers is also being pursued 363 carry alternative path information to implement resilient elastic approaches include dynamically changing the source routing in openflow networks under the presence number of controllers and their locations under different of failures along a primary path packets can be rerouted to conditions alternative paths by the switches themselves without in most of the research efforts addressing scaling limi volving the controller another recent proposal that uses tations of sdn can be classified in three categories data in packet information is inflex an sdn based plane control plane and hybrid while targeting the data architecture for cross layer network resilience which pro plane proposals such as devoflow and sdcs vides on demand path fail over by having endpoints tag actually reduce the overhead of the control plane by dele packets with virtual routing plane information that can be gating some work to the forwarding devices for instance used by egress routers to reroute by changing tags upon instead of requesting a decision from the controller for failure detection every flow switches can selectively identify the flows e g similarly to slickflow osp proposes a protec elephant flows that may need higher level decisions from tion approach for data plane resilience it is based on the control plane applications another example is to in protecting individual segments of a path avoiding the in troduce more powerful general purpose cpus in the for tervention of the controller upon failure the recovery warding devices to enable sdcs a general purpose cpu time depends on the failure detection time i e a few tens and sdcs offer new possibilities for reducing the control of milliseconds in the proposed scenarios in the same plane overhead by allowing software based implementa direction other proposals are starting to appear for ena tions of functions for data aggregation and compression bling fast fail over mechanisms for link protection and for instance restoration in openflow based networks maestro nox mt kandoo beacon language based solutions to the data plane fault and maple are examples of the effort on tolerance problem have also been proposed in this designing and deploying high performance controllers work the authors propose a language that compiles regular i e trying to increase the performance of the control expressions into openflow rules to express what network plane these controllers mainly explore well known tech paths packets may take and what degree of link level fault niques from networking computer architectures and tolerance is required such abstractions around fault to high performance computing such as buffering pipelin lerance allow developers to build fault recovery capabilities ing and parallelism to increase the throughput of the into applications without huge coding efforts control platform the hybrid category is composed of solutions that try to split the control logic functions between specialized data d scalability plane devices and controllers in this category difane scalability has been one of the major concerns of sdns proposes authoritative intermediate switches to from the outset this is a problem that needs to be keep all traffic in the data plane targeting a more scalable addressed in any systemve g in traditional networksv and efficient control plane authoritative switches are re and is obviously also a matter of much discussion in the sponsible for installing rules on the remaining switches context of sdn most of the scalability concerns in while the controller is still responsible for generating all sdns are related to the decoupling of the control and data the rules required by the logic of applications by dividing planes of particular relevance are reactive network con the controller work with these special switches the overall figurations where the first packet of a new flow is sent by system scales better the first forwarding element to the controller the addi table provides a nonexhaustive list of proposals tional control plane traffic increases network load and addressing scalability issues of sdn we characterize these makes the control plane a potential bottleneck addition issues by application domain control or data plane their ally as the flow tables of switches are configured in real purpose the throughput in terms of number of flows per time by an outside entity there is also the extra latency second when the results of the experiments are reported introduced by the flow setup process in large scale net and the strategies used as can be observed the vast ma works controllers will need to be able to process millions jority are control plane solutions that try to increase scala of flows per second without compromising the bility by using distributed and multicore architectures quality of its service therefore these overheads on the some figures are relatively impressive with some solu control plane and on flow setup latency are arguably two tions achieving up to million flows however we of the major scaling concerns in sdn should caution the reader that current evaluations kreutz et al software defined networking a comprehensive survey table summary and characterization of scalability proposals for sdns consider only simple applications and count basically the e performance evaluation number of packet in and packet out messages to as introduced in section iv a there are already several measure throughput the actual performance of control openflow implementations from hardware and software lers will be affected by other factors such as the number vendors being deployed in different types of networks and complexity of the applications running on the from small enterprise to large scale data centers there controller and security mechanisms implemented for fore a growing number of experiments over sdn enabled example a routing algorithm consumes more computing networks is expected in the near future this will naturally resources and needs more time to execute than a simple create new challenges as questions regarding sdn perfor learning switch application also current evaluations are mance and scalability have not yet been properly inves done using plain tcp connections the performance is tigated understanding the performance and limitation of very likely to change when basic security mechanisms are the sdn concept is a requirement for its implementation put in place such as tls or more advanced mechanisms in production networks there are very few performance to avoid eavesdropping man in the middle and dos evaluation studies of openflow and sdn architecture attacks on the control plane although simulation studies and experimentation are another important issue concerning scalability is data among the most widely used performance evaluation tech distribution among controller replicas in distributed archi niques analytical modeling has its own benefits as well a tectures distributed control platforms rely on data distri closed form description of a networking architecture paves bution mechanisms to achieve their goals for instance the way for network designers to have a quick and controllers such as onix hyperflow and onos need approximate estimate of the performance of their design mechanisms to keep a consistent state in the distributed without the need to spend considerable time for simula control platform recently experimental evaluations have tion studies or expensive experimental setup shown that high performance distributed and fault toler some work has investigated ways to improve the per ant data stores can be used to tackle such challenges formance of switching capabilities in sdn these mainly nevertheless further work is necessary to properly under consist of observing the performance of openflow stand state distribution tradeoffs enabled networks regarding different aspects such as vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey proceedings of the ieee vol no january lookup performance hardware acceleration device performance and better decide which one is best the influence of types of rules and packet sizes per suited for the target network infrastructure formance bottlenecks of current openflow implementa surprisingly despite being designed to evaluate the tions how reactive settings impact the performance performance of controllers cbench is currently a single on data center networks and the impact of configu threaded tool therefore multiple instances have to be ration on openflow switches started to utilize multiple cpus it also only establishes design choices can have a significant impact on the one controller connection for all emulated switches un lookup performance of openflow switching in linux fortunately this means little can be derived from the operating system using standard commodity network results in terms of controller performance and behavior or interface cards just by using commodity network estimation of different bounds at the moment for in hardware the packet switching throughput can be im stance aggregated statistics are gathered for all switches proved by up to when compared to one based on soft but not for each individual switch as a result it is not openflow switching similarly hardware accel possible to identify whether all responses of the controller eration based on network processors can also be applied are for a single switch or whether the capacity of the to perform openflow switching in such cases early re controller is actually shared among the switches flexible ports indicate that performance in terms of packet delay openflow controller benchmarks are available though can be improved by when compared to conventional ofcbenchmark is one of the recent developments designs it creates a set of message generating virtual switches by utilizing intel dpdk library it has been which can be configured independently from each other to shown that it is possible to provide flexible traffic steering emulate a specific scenario and to maintain their own capability at the hypervisor level e g kvm without the statistics performance limitations imposed by traditional hardware another interesting question to pose when evaluating switching techniques such as sr iov this is the performance of sdn architectures is what is the re particularly relevant since most of the current enterprise quired number of controllers for a given network topology deployments of sdn are in virtualized data center infra and where to place the controllers by analyz structures as in vmware nvp solution ing the performance of controllers in different network current openflow switch implementations can lead to topologies it is possible to conclude that one controller is performance bottlenecks with respect to the cpu load often enough to keep the latency at a reasonable rate yet modifications on the protocol specification can moreover as observed in the same experiments in the help reduce the occurrence of these bottlenecks further general case adding k controllers to the network can re investigations provide measurements regarding the per duce the latency by a factor of k however there are cases formance of the openflow switch for different types of such as large scale networks and wans where more con rules and packet sizes trollers should be deployed to achieve high reliability and in data centers a reactive setting of flow rules can lead low control plane latency to an unacceptable performance when only eight switches recent studies also show that the sdn control plane are handled by one openflow controller this cannot be fully physically centralized due to responsive means that large scale sdn deployments should probably ness reliability and scalability metrics not rely on a purely reactive modus operandi but rather therefore distributed controllers are the natural choice on a combination of proactive and reactive flow setup for creating a logically centralized control plane while be to foster the evaluation of different performance as ing capable of coping with the demands of large scale pects of openflow devices frameworks such as oflops networks however distributed controllers bring addition oflops turbo cbench and ofc al challenges such as the consistency of the global network benchmark have been proposed they provide a set view which can significantly affect the performance of the of tools to analyze the performance of openflow switches network if not carefully engineered taking two applica and controllers cbench is a benchmark tool tions as examples one that ignores inconsistencies and developed to evaluate the performance of openflow con another that takes inconsistency into consideration it is trollers by taking advantage of the cbench it is possible possible to observe that optimality is significantly affected to identify performance improvements for openflow con when inconsistencies are not considered and that the trollers based on different environment and system con robustness of an application is increased when the con figurations such as the number of forwarding devices troller is aware of the network state distribution network topology overall network workload type of most of these initiatives toward identifying the limita equipments forwarding complexity and overhead of the tions and bottlenecks of sdn architectures can take a lot of applications being executed on top of controllers time and effort to produce consistent outputs due to the therefore such tools can help system designers make practical development and experimentation requirements better decisions regarding the performance of devices and as mentioned before analytic models can quickly pro the network while also allowing end users to measure the vide performance indicators and potential scalability kreutz et al software defined networking a comprehensive survey bottlenecks for an openflow switch controller system be fore detailed data are available while simulation can pro vide detailed insight into a certain configuration the analytical model greatly simplifies a conceptual deploy ment decision for instance a network calculus based model can be used to evaluate the performance of an sdn switch and the interaction of sdn switches and controllers the proposed sdn switch model captured the closed form of the packet delay and buffer length inside the sdn switch according to the parameters of a cumulative arrival process using recent measurements the authors have reproduced the packet processing delay of two va riants of openflow switches and computed the buffer re quirements of an openflow controller analytic models based on queuing theory for the forwarding speed and blocking probability of current openflow switches can also be used to estimate the performance of the network f security and dependability cyber attacks against financial institutions energy fa cilities government units and research institutions are becoming one of the top concerns of governments and agencies around the globe different inci dents such as stuxnet have already shown the per sistence of threat vectors put another way these attacks are capable of damaging a nation wide infrastruc ture which represent a significant and concerning issue as expected one of the most common means of executing those attacks is through the network either the internet or the local area network it can be used as a simple transport infrastructure for the attack or as a potentialized weapon to amplify the impact of the attack for instance high capacity networks can be used to launch large scale at tacks even though the attacker has only a low capacity network connection at his premises due to the danger of cyber attacks and the current landscape of digital threats security and dependability are top priorities in sdn while research and experimen tation on sdns is being conducted by some commercial players e g google yahoo rackspace microsoft commercial adoption is still in its early stage industry experts believe that security and dependability are issues that need to be addressed and further investigated in sdn additionally from the dependability perspective avail ability of internet routers is today a major concern with the widespread of clouds and their strong expectations about the network it is therefore crucial to achieve high levels of availability on sdn control platforms if they are to become the main pillars of networked applica tions different threat vectors have already been identified in sdn architectures as well as several security issues and weaknesses in openflow based networks while some threat vectors are common to existing networks others are more specific to sdn such as vol no january proceedings of the ieee fig main threat vectors of sdn architectures attacks on control plane communication and logically cen tralized controllers it is worth mentioning that most threats vectors are independent of the technology or the protocol e g openflow pof and forces because they represent threats on conceptual and architectural layers of sdn itself as shown in fig and table there are at least seven identified threats vector in sdn architectures the first threat vector consists of forged or faked traffic flows in the data plane which can be used to attack forwarding devices and controllers the second allows an attacker to exploit vulnerabilities of forwarding devices and conse quently wreak havoc with the network threat vectors three four and five are the most dangerous ones since they can compromise the network operation attacks on the control plane controllers and applications can easily grant an attacker the control of the network for instance a faulty or malicious controller or application could be used to reprogram the entire network for data theft pur poses e g in a data center the sixth threat vector is linked to attacks on and vulnerabilities in administrative stations a compromised critical computer directly con nected to the control network will empower the attacker table sdn specific versus nonspecific threats kreutz et al software defined networking a comprehensive survey table attacks to openflow networks with resources to launch more easily an attack to the con troller for instance last threat vector number seven re presents the lack of trusted resources for forensics and remediation which can compromise investigations e g forensics analysis and preclude fast and secure recovery modes for bringing the network back into a safe operation condition as can be observed in table threat vectors to are specific to sdn as they stem from the separation of the control and data planes and the consequent introduction of a new entity in these networksvthe logically centralized controller the other vectors were already present in tra ditional networks however the impact of these threats could be larger than todayvor at least it may be expressed differentlyvand as a consequence it may need to be dealt with differently openflow networks are subject to a variety of security and dependability problems such as spoofing tam pering repudiation information disclosure denial of service elevation of privileges and the assumption that all applications are benign and will not affect sdn operation the lack of isolation protection access control and stronger security recommendations are some of the reasons for these vulnerabilities we will ex plore these next openflow security assessment there is already a number of identified security issues in openflow enabled networks starting from a stride methodology it is possible to identify different attacks to openflow enabled networks table summarizes these attacks based on for instance information disclosure can be achieved through side channel attacks targeting the flow rule setup process when reactive flow setup is in place obtaining information about network operation is rela tively easy an attacker that measures the delay experi enced by the first packet of a flow and the subsequent can easily infer that the target network is a reactive sdn and proceed with a specialized attack this attackvknown as fingerprinting vmay be the first step to launch a dos attack intended to exhaust the resources of the network for example if the sdn is proactive guessing its forward ing rule policies is harder but still feasible inter proceedings of the ieee vol no january estingly all reported threats and attacks affect all versions to of the openflow specification it is also worth emphasizing that some attacks such as spoofing are not specific to sdn however these attacks can have a larger impact in sdns for instance by spoofing the ad dress of the network controller the attacker using a fake controller could take over the control of the entire net work a smart attack could persist for only a few seconds i e just the time needed to install special rules on all forwarding devices for its malicious purposes e g traffic cloning such attack could be very hard to detect taking counter falsification as another example an attacker can try to guess installed flow rules and subse quently forge packets to artificially increase the counter such attack would be specially critical for billing and load balancing systems for instance a customer could be charged for more traffic than she in fact used while a load balancing algorithm may take nonoptimal decisions due to forged counters flow networks include the lack of strong security re commendations for developers the lack of tls and access control support on most switch and controller implemen tations the belief that tcp is enough because links are physically secure the fact that many switches have listener mode activated by default allowing the establishment of malicious tcp connections for in stance or that flow table verification capabilities are harder to implement when tls is not in use in addition the high denial of service risk posed to cen tralized controllers is worth mentioning as well as the vulnerabilities in the controllers themselves bugs and vulnerabilities in applications targeted flooding attacks insecure northbound inter faces that can lead to security breaches and the risk of resource depletion attacks for instance it has been shown that an attacker can easily compromise control plane communications through dos attacks and launch a resource depletion attack on control platforms by exploiting a single application such as a learning switch 512 another point of concern is the fact that current con trollers such as floodlight opendaylight pox and beacon have several security and resiliency issues common application development problems bugs such as the sudden exit of an application or the continuous kreutz et al software defined networking a comprehensive survey table countermeasures for security threats in openflow networks rate limiting packet dropping shorter timeouts and flow aggregations are techniques that can be applied on controllers and forwarding devices to mitigate different types of attacks such as denial of service and information disclosure for instance reduced timeouts can be used to mitigate the effect of an attack exploring the reactive ope ration mode of the network to make the controller install rules that divert traffic to a malicious machine with re duced timeouts the attacker would be forced to constantly generate a number of forged packets to avoid timeout expiration making the attack more likely to be detected rate limiting and packet dropping can be applied to avoid dos attacks on the control plane or stop ongoing attacks directly on the data plane by installing specific rules on the devices where the attacks is being originated forensics and remediation encompass mechanisms such as secure logging event correlation and consistent reporting if anything wrong happens with the network operators should be able to safely figure out the root cause of the problem and put the network to work on a secure operation mode as fast as possible additionally tech niques to tolerate faults and intrusions such as state ma allocation of memory space are enough to crash existing chine replication proactive reactive recovery controllers on the security perspective a simple mali and diversity can be added to control platforms for cious action such as changing the value of a data structure increasing the robustness and security properties by auto in memory can also directly affect the operation and relia matically masking and removing faults put differently bility of current controllers these examples are illustra sdn controllers should be able to resist against different tive that from a security and dependability perspective types of events e g power outages network disruption there is still a long way to go communication failures network partitioning and attacks e g ddos resource exhaustion one of the countermeasures for openflow based sdns several most traditional ways of achieving high availability is countermeasures can be put in place to mitigate the secu through replication yet proactive reactive recovery and rity threats in sdns table summarizes a number of diversity are two examples of crucial techniques that add countermeasures that can be applied to different elements value to the system for resisting against different kinds of of an sdn openflow enabled network some of these attacks and failures e g those exploring common vul measures namely rate limiting event filtering packet nerabilities or caused by software aging problems dropping shorter timeouts and flow aggregation are al other countermeasures to address different threats and ready recommended in the most recent versions of the issues of sdn include enhancing the security and depen openflow specification version and later how dability of controllers protection and isolation of applica ever most of them are not yet supported or implemented tions trust management in sdn deployments between controllers and forwarding devices integ traditional techniques such as access control attack rity checks of controllers and applications forensics detection mechanisms event filtering e g controller de and remediation verification frameworks cides which asynchronous messages he is not going to 519 and resilient control planes accept firewalls and intrusion detection systems can be 506 521 protection and isolation mechanisms used to mitigate the impact of or to avoid attacks they can should be part of any controller applications should be be implemented in different devices such as controllers isolated from each other and from the controller forwarding devices middleboxes and so forth middle different techniques such as security domains e g boxes can be a good option for enforcing security policies kernel security and user level and data access protection in an enterprise because they are in general more robust mechanisms should be put in place in order to avoid and special purpose high performance devices such a security threats from network applications strategy also reduces the potential overhead cause by im implementing trust between controllers and forward plementing these countermeasures directly on controllers ing is another requirement for ensuring that malicious or forwarding devices however middleboxes can add elements cannot harm the network without being extra complexity to the network management i e in detected an attacker can try to spoof the ip address of crease the opex at the cost of better performance the controller and make switches connect to its own vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey proceedings of the ieee vol no january controller this is currently the case since most controllers effect on the sales division of vendors that are required to and switches only establish insecure tcp connections adapt accordingly complementary integrity checks on controller and appli pioneering sdn operational deployments have been cation software can help to ensure that safe code is being mainly greenfield scenarios and or tightly controlled sin bootstrapped which eliminates harmful software from be gle administrative domains initial rollout strategies are ing started once the system restarts besides integrity mainly based on virtual switch overlay models or open checks other things such as highly specialized malware flow only network wide controls however a broader detection systems should be developed for sdn third adoption of sdn beyond data center silosvand between party network applications should always be scanned for themselvesvrequires considering the interaction and bad code and vulnerabilities because a malicious applica integration with legacy control planes providing traditional tion represents a significant security threat to the network switching routing and operation administration and it is worth mentioning that there are also other ap management oam functions certainly rip and replace proaches for mitigating security threats in sdn such as is not a viable strategy for the broad adoption of new net declarative languages to eliminate network protocol vul working technologies nerabilities this kind of descriptive languages can hybrid networking in sdn should allow deploying specify semantic constraints structural constraints and openflow for a subset of all flows only enable openflow safe access properties of openflow messages then a on a subset of devices and or ports only and provide op compiler can use these inputs to find programmers imple tions to interact with existing oam protocols legacy de mentation mistakes on message operations in other words vices and neighboring domains as in any technology such languages can help find and eliminate implementation transition period where forklift upgrades may not be a vulnerabilities of southbound specifications choice for many migration paths are critical for adoption proposals providing basic security properties such as hybrid networking in sdn spans several levels the authentication and access control are starting migration working group of the onf is tackling the sce to appear c bas is a certificate based authentica nario where hybrid switch architectures and hybrid tion authorization and accounting aaa architecture for openflow and non openflow devices coexist hybrid improving the security control on sdn experimental faci switches can be configured to behave as a legacy switch or lities solutions in the spirit of c bas can be made highly as an openflow switch and in some cases as both simul secure and dependable through hybrid system architec taneously this can be achieved for example by partition tures which combine different technologies and tech ing the set of ports of a switch where one subset is devoted niques from distributed systems security and fault and to openflow controlled networks and the other subset to intrusion tolerance 526 legacy networks for these subsets to be active at the same time each one having its own data plane multitable sup g migration and hybrid deployments port at the forwarding engine e g via tcam partition the promises by sdn to deliver easier design opera ing is required besides port based partitioning it is also tion and management of computer networks are endan possible to rely on vlan based prior to entering the gered by challenges regarding incremental deployability openflow pipeline or flow based partitioning using robustness and scalability a prime sdn adoption chal openflow matching and the local and or normal ac lenge relates to organizational barriers that may arise due tions to redirect packets to the legacy pipeline or the to the first and second order effects of sdn automation switch local networking stack and its management stack capabilities and layer domain blurring some level of flow based partitioning is the most flexible option as it human resistance is to be expected and may affect the allows each packet entering a switch to be classified by an decision and deployment processes of sdn especially by openflow flow description and treated by the appropriate those that may regard the control refactorization of sdn as data plane openflow or legacy a risk to the current chain of control and command or there are diverse controllers such as opendaylight even to their job security this complex social challenge is hp van sdn and opencontrail that similar and potentially larger to known issues between have been designed to integrate current non sdn technol the transport and ip network divisions of service providers ogies e g snmp pcep bgp and netconf with sdn or the system administrator storage networking and sec interfaces such as openflow and ovsdb nonetheless urity teams of enterprise organizations such a challenge is controllers such as closedflow have been recently observable on today virtualized data centers through the proposed with the aim of introducing sdn like program shift in role and decision power between the networking ming capabilities in traditional network infrastructures and server people similarly the development and opera making the integration of legacy and sdn enabled tions devops movement has caused a shift in the locus of networks a reality without side effects in terms of prog influence not only on the network architecture but also on rammability and global network control closedflow is purchasing and this is an effect that sdn may exacerbate designed to control legacy ethernet devices e g cisco these changes in role and power causes a second order switches with a minimum ios of se in a similar kreutz et al software defined networking a comprehensive survey way as openflow controller allows administrators to con common centralized configuration interface to build vir trol openflow enabled devices more importantly clo tual networks using vlans an abstraction of the physical sedflow does not impose any change on the forwarding network topology is taken into account by a centralized devices it only takes advantage of the existing hardware controller that applies a path finder mechanism in order and firmware capabilities to mimic an sdn control over to calculate network paths and program the openflow the network i e allow dynamic and flexible programma switches via rest interfaces and legacy devices using bility in the data plane the next step could be the integ netconf ration of controllers like closedflow and openflow based more recently frameworks such as escape and controllers promoting interoperability among controllers its extensions have been proposed to provide multilayer and a smooth transition from legacy infrastructures to service orchestration in multidomains such frameworks sdn enabled infrastructure with nearly all the capabilities combine different tools and technologies such as click of a clean slate sdn enabled infrastructure pox opendaylight and netconf furthermore controllers may have to be separated into in other words those frameworks integrate different sdn distinct peer domains for different reasons such as scala solutions with traditional ones therefore they might be bility technology controllers from different vendors con useful tools on the process of integrating or migrating le trollers with different service functionality and diversity gacy networking infrastructure to sdn of administrative domains controllers from differ other hybrid solutions starting to emerge include open ent domains or with distinct purposes are also required source hybrid ip sdn oshi oshi combines to be backward compatible either by retrofitting or ex quagga for open shortest path first routing and sdn tending existing multidomain protocols e g bgp or by capable switching devices e g open vswitch on linux to proposing new sdn to sdn protocols also known as provide backward compatibility for supporting incremen east westbound apis tal sdn deployments i e enabling interoperability with some efforts have been already devoted to the chal non of forwarding devices in carrier grade networks lenges of migration and hybrid sdns routeflow while full sdn deployments are straightforward only implements an ip level control plane on top of an open in some green field deployments such as data center net flow network allowing the underlying devices to act as ip works or by means of an overlay model approach hybrid routers under different possible arrangements the sdn approaches represent a very likely deployment model cardigan project has deployed routeflow at that can be pursued by different means including the a live internet exchange now for over a year legacyflow following extends the openflow based controlled network to topology based hybrid sdn based on a topological embrace non openflow nodes there are also some other separation of the nodes controlled by traditional early use cases on integrating complex legacy system such and sdn paradigms the network is partitioned as docsis gigabit ethernet passive optical network in different zones and each node belongs to only and dwdm reconfigurable optical add drop multiplexer one zone roadm the common grounds of these service based hybrid sdn conventional networks pieces of work are considering hybrid as the coexistence and sdn provide different services where overlap of traditional environments of closed vendor routers and ping nodes controlling a different portion of the switches with new openflow enabled devices targeting fib or generalized flow table of each node ex the interconnection of both control and data planes of le amples include network wide services like for gacy and new network elements and taking a controller warding that can be based on legacy distributed centric approach drawing the hybrid line outside of any control while sdn provides edge to edge services device itself but into the controller application space such as enforcement of traffic engineering and ac panopticon defines an architecture and method cess policies or services requiring full traffic visi ology to consistently implement sdn inside enterprise bility e g monitoring legacy networks through network orchestration under class based hybrid sdn based on the partition strict budget constraints the proposed architecture in of traffic in classes some controlled by sdn and cludes policy configurations troubleshooting and main the remaining by legacy protocols while each tenance tasks establishing transitional networks sdn and paradigm controls a disjoint set of node for legacy in structures called solitary confinement trees warding entries each paradigm is responsible for scts where vlan ids are efficiently used by orches all network services for the assigned traffic tration algorithms to build paths in order to steer traffic classes through sdn switches defying the partial sdn imple integrated hybrid sdn a model where sdn is mentation concept they confirm that this could be a long responsible for all the network services and uses term operational strategy solution for enterprise networks traditional protocols e g bgp as an interface to hybnet presents a network management frame node fibs for example it can control forwarding work for hybrid openflow legacy networks it provides a paths by injecting carefully selected routes into a vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey proceedings of the ieee vol no january routing system or adjusting protocol settings e g carrier networks are using the sdn paradigm as the igp weights past efforts on rcps and the technology means for solving a number of long standing ongoing efforts within odl can be considered problems some of these efforts include new architectures examples of this hybrid model for a smooth migration from the current mobile core in in general benefits of hybrid approaches include ena frastructure to sdn and techno economic models bling flexibility e g easy match on packet fields for mid for virtualization of these networks carrier dleboxing and sdn specific features e g declarative grade openflow virtualization schemes 553 in management interface while partially keeping the in cluding virtualized broadband access infrastructures herited characteristics of conventional networking such as techniques that are allowing the offer of network as a robustness scalability technology maturity and low de service programmable gepon and dwdm ployment costs on the negative side the drawbacks of roadm large scale interautonomous sys hybridization include the need for ensuring profitable tems ass sdn enabled deployments flexible con interactions between the networking paradigms sdn and trol of network resources including offering mpls traditional while dealing with the heterogeneity that services using an sdn approach and the investiga largely depends on the model tion of novel network architectures from proposals to se initial tradeoff analyses suggest that the combi parate the network edge from the core with nation of centralized and distributed paradigms may pro the latter forming the fabric that transports packets as vide mutual benefits however future work is required to defined by an intelligent edge to software defined inter devise techniques and interaction mechanisms that maxi net exchange points 561 mize such benefits while limiting the added complexity of use case analysis of management functions re the paradigm coexistence quired by carrier networks have identified a set of require ments and existing limitations in the sdn protocol toolbox h meeting carrier grade and cloud requirements for instance it has been pinpointed that of config a number of carrier grade infrastructure providers needs a few extensions in order to meet the carrier require e g ntt at t verizon deutsche telekom are at the ments such as physical resource discovery logical link core of the sdn community with the ultimate goal of configuration logical switch instantiation and device and solving their long standing networking problems in the link oam configuration similarly openflow exten telecom world ntt can be considered one of the forefront sions have also been proposed to realize packet optical runners in terms of investing in the adoption and deploy integration with sdn in order to support sdn con ment of sdn in all kinds of network infrastructures from cepts in large scale wide area networks different exten backbone data center to edge customers in sions and mechanisms are required both technology ntt launched an sdn based on demand elastic provi specific e g mpls bfd and technology agnostic such sioning platform of network resources e g bandwidth as resiliency mechanisms for surviving link failures for hd video broadcasters similarly as a global failures of controller or forwarding elements solutions for cloud provider with data centers spread across the globe integrating residential customer services in different forms the same company launched a similar service for its i e support also current technologies new energy cloud customers who are now capable of taking advantage efficient networking approaches qos properties for packet of dynamic networking provisioning intradata and inter classification metering coloring policing shaping and data centers at t is another telecom company that scheduling and multilayer aspects outlining different is investing heavily in new services such as user defined stages of packet optical integration network clouds that take advantage of recent develop sdn technology also brings new possibilities for cloud ments in nfv and sdn as we mentioned before providers by taking advantage of the logically centralized sdn and nfv are complementary technologies that can be control of network resources it is possible to applicable to different types of networks from local net simplify and optimize network management of data cen works and data centers to transport networks ters and achieve efficient intradata center networking recently several research initiatives have worked toward including fast recovery mechanisms for the data and con combining sdn and nfv through intel dpdk a set of trol planes adaptive traffic engineering libraries and drivers that facilitates the development of with minimal modifications to dc networks simpli network intensive applications and allows the implemen fied fault tolerant routing performance isolation tation of fine grained network functions early work and easy and efficient resource migration e g of toward service chaining has been proposed by combining vms and virtual networks improved interdata sdn and nfv technologies 547 and studies center communication including the ability to fully utilize around the forces applicability to sdn enhanced the expensive high bandwidth links without impairing nfv have also come to light these are some of the quality of service higher levels of reliability early examples of the opportunities sdns seem to bring to with novel fault management mechanisms etc telecom and cloud providers and cost reduction by replacing kreutz et al software defined networking a comprehensive survey table carrier grade and cloud provider expectations and challenges complex expensive hardware by simple and cheaper for workload changes recent advances makes on demand warding devices provisioning of resources possible at nearly all infrastruc table summarizes some of the carrier grade net tural layers the fully automated provisioning and orches work and cloud infrastructure providers requirements in tration of it infrastructures as been recently named this table we show the current challenges and what is to software defined environments sdes by be expected with sdn as we saw before some of the ibm this is a novel approach that is expected to have expectations are already becoming a reality but many are significant potential in simplifying it management opti still open issues what seems to be clear is that sdn re mizing the use of the infrastructure reduce costs and presents an opportunity for telecom and cloud providers reduce the time to market of new ideas and products in in providing flexibility cost effectiveness and easier man an sde workloads can be easily and automatically assigned agement of their networks to the appropriate it resources based on application characteristics security and service level policies and the i sdn the missing piece toward best available resources to deliver continuous dynamic software defined environments optimization and reconfiguration to address infrastructure the convergence of different technologies is enabling issues in a rapid and responsive manner table sum the emergence of fully programmable it infrastructures it marizes the traditional approaches and some of the key is already possible to dynamically and automatically con features being enabled by sdes figure or reconfigure the entire it stack from the network in an sde the workloads are managed independently infrastructure up to the applications to better respond to of the systems and underlying infrastructure i e are not vol no january proceedings of the ieee kreutz et al software defined networking a comprehensive survey table sde pushing it to the next frontier tied to a specific technology or vendor anoth er characteristic of this new approach is to offer a prog rammatic access to the environment as a whole selecting the best available resources based on the current status of the infrastructure and enforcing the policies defined in this sense it shares much of the philosophy of sdn interestingly one of the missing key pieces of an sde was until now sdn the four essential building blocks of an sde 578 are sdns software defined storage sds software defined compute sdc software defined management sdm in the last decade the advances in virtualization of compute and storage together with the availability of so phisticated cloud orchestration tools have enabled sds sdc and sdm these architectural components have been widely used by cloud providers and for building it infrastructures in different enterprise environments however the lack of programmable network control has so far hindered the realization of a complete sde sdn is seen as the technology that may fill this gap as attested by the emergence of cloud scale network virtualization plat forms based on this new paradigm the ibm smartcloud orchestrator is one of the first examples of an sde it integrates compute storage management and networking in a structured way fig overview of an it infrastructure based on an sde proceedings of the ieee vol no january fig gives a simplified overview of an sde by taking the approach developed by ibm as its basis the main idea of an sde based infrastructure is that the business needs that define the workloads trigger the reconfiguration of the global it infrastructure compute storage network this is an important step toward a more customizable it in frastructure that focuses on the business requirements rather than on the limitations of the infrastructure itself vi conclusion traditional networks are complex and hard to manage one of the reasons is that the control and data planes are vertically integrated and vendor specific another con curring reason is that typical networking devices are also tightly tied to line products and versions in other words each line of product may have its own particular config uration and management interfaces implying long cycles for producing product updates e g new firmware or upgrades e g new versions of the devices all this has given rise to vendor lock in problems for network infra structure owners as well as posing severe restrictions to change and innovation sdn created an opportunity for solving these long standing problems some of the key ideas of sdn are the introduction of dynamic programmability in forwarding devices through open southbound interfaces the decoupl ing of the control and data plane and the global view of the kreutz et al software defined networking a comprehensive survey network by logical centralization of the network brain sdn has successfully managed to pave the way toward while data plane elements became dumb but highly effi a next generation networking spawning an innovative re cient and programmable packet forwarding devices the search and development environment promoting advances control plane elements are now represented by a single in several areas switch and controller platform design entity the controller or nos applications implementing evolution of scalability and performance of devices and the network logic run on top of the controller and are architectures promotion of security and dependability much easier to develop and deploy when compared to we will continue to witness extensive activity around traditional networks given the global view consistency of sdn in the near future emerging topics requiring further policies is straightforward to enforce sdn represents a research are for example the migration path to sdn ex major paradigm shift in the development and evolution of tending sdn toward carrier transport networks realization networks introducing a new pace of innovation in net of the network as a service cloud computing paradigm or working infrastructure sdes as such we would like to receive feedback from the in spite of recent and interesting attempts to survey networking sdn community as this novel paradigm evolves this new chapter in the history of networks the to make this a living document that gets updated and literature was still lacking to the best of our knowledge improved based on the community feedback we have set up a single extensive and comprehensive overview of the a github repository https github com sdn survey latex building blocks concepts and challenges of sdns try wiki for this purpose and we invite our readers to join us in ing to address this gap this paper used a layered ap this communal effort additionally new releases of the proach to methodically dissect the state of the art in survey will be available at http arxiv org abs h terms of concepts ideas and components of sdn cover ing a broad range of existing solutions as well as future directions ambient intelligence ami is a new paradigm in information technology aimed at empowering people capa bilities by means of digital environments that are sensitive adaptive and responsive to human needs habits gestures and emotions this futuristic vision of daily environment will enable innovative human machine interactions characterized by per vasive unobtrusive and anticipatory communications such innovative interaction paradigms make ami technology a suitable candidate for developing various real life solutions including in the healthcare domain this survey will discuss the emergence of ami techniques in the healthcare domain in order to provide the research community with the necessary background we will examine the infrastructure and technol ogy required for achieving the vision of ami such as smart environments and wearable medical devices we will summa rize the state of the art artificial intelligence ai methodolo gies used for developing ami system in the healthcare domain including various learning techniques for learning from user interaction reasoning techniques for reasoning about users goals and intensions and planning techniques for planning activities and interactions we will also discuss how ami technology might support people affected by various physical or mental disabilities or chronic disease finally we will point to some of the successful case studies in the area and we will look at the current and future challenges to draw upon the possible future research paths keywords ambient intelligence ami healthcare sensor networks smart environments i introduction a what is ambient intelligence imagine a day when a small tricoder device moni tors your health status in a continuous manner diagnoses any possible health conditions has a conversation with you to persuade you to change your lifestyle for maintaining better health and communicates with your doctor if needed the device might even be embedded into your regular clothing fibers in the form of very tiny sensors and it might communicate with other devices around you including the variety of sensors embedded into your home to monitor your lifestyle for example you might be alarmed about the lack of a healthy diet based on the items present in your fridge and based on what you are eating outside regularly this might seem like science fiction for now but many respecters in the field of ambient intel ligence ami expect such scenarios to be part of our daily life in not so far future the ami paradigm represents the future vision of in telligent computing where environments support the manuscript received october accepted april date of publication august date of current version november g acampora is with the school of science and technology nottingham trent people inhabiting them in this new computing paradigm the conventional input and output media no university nottingham u k e mail giovanni acampora ntu ac uk d j cook is with the department of electrical and computer engineering washington state university pullman wa usa e mail cook eecs wsu edu longer exist rather the sensors and processors will be integrated into everyday objects working together in p rashidi is with the biomedical engineering department university of florida gainesville fl usa e mail parisa rashidi ufl edu a v vasilakos is with the department of computer and telecommunications harmony in order to support the inhabitants by relying on various artificial intelligence ai techniques ami engineering university of western macedonia kozani greece e mail vasilako ath forthnet gr digital object identifier jproc multifunction handheld device used for sensing and data analysis in start trek series proceedings of the ieee vol no december 9219 ó ieee acampora et al a survey on ambient intelligence in healthcare promises the successful interpretation of the wealth of contextual information obtained from such embedded sensors and will adapt the environment to the user needs in a transparent and anticipatory manner an ami system is particularly identified by several characteristics context aware it exploits the contextual and situational information personalized it is personalized and tailored to the needs of each individual anticipatory it can anticipate the needs of an individual without the conscious mediation of the individual adaptive it adapts to the changing needs of individuals ubiquity it is embedded and is integrated into our everyday environments transparency it recedes into the background of our daily life in an unobtrusive way besides characteristics such as transparency and ubi quity an important characteristic of ami is the intelligence aspect by drawing from advances in ai ami systems can be even more sensitive responsive adaptive and ubiquitous while ami draws from the field of ai it is not synonymous with ai in addition to the ai subareas such as reasoning activity recognition decision making and spatio temporal logic an ami system has to rely upon advances in variety of other fields some example areas include sensor networks to facilitate the data collection robotics to build actuators and assistive robots and human computer interaction to build more natural interfaces we have already embarked on the path of achieving such a vision today we are surrounded by various computing devices such as personal computers smartphones global positioning systems gpss tablets various sensors such as radio frequency identification rfid tags infrared motion sensors as well as biometric identification sensors the widespread presence of such devices and sensors and accompanying services such as location service has already sparked the realization of ami in addition recent compu tational and electronics advancements have made it possible for researchers to work on ambitious concepts such as smart homes and to bring us one step closer to the full realization of ami in our daily environments b healthcare challenges these days the majority of industrialized nations are facing significant complications regarding the quality and cost of various healthcare and wellbeing services these difficulties will exacerbate even more due to an increasing aging population which translates into a multitude of chronic diseases and tremendous demand for various health care services as a result the cost of the healthcare sector might not be sustainable and therefore industrialized countries need to find and plan policies and strategies to use the limited economical resources more efficiently and effectively this need for sustainable healthcare systems vol no december proceedings of the ieee translates into a range of challenges in science and technology which if solved ultimately could benefit our global society and economy in particular the exploitation of information and communication technology for implement ing autonomous and proactive healthcare services will be extremely beneficial in the past decades consumer driven healthcare in conjunction with web based platforms and electronic health records have led to an array of improved healthcare solutions in recent years we also have witnessed the emergence of many smartphone apps that are becoming readily available for physiological status monitoring however despite being an important step toward personal ized medicine these solutions often suffer from scalability security and privacy issues furthermore such solutions are only able to provide a snapshot of physiological conditions rather than a continuous view of the overall health over the course of many years with recent advances in sensor networks research we are already embarking on the path of revolutionary low cost healthcare monitoring systems embedded within the home and living environments in particular ami systems have the potential to enhance the healthcare domain dramatically for example ami technology can be used to monitor the health status of older adults or people with chronic diseases and it can provide assistive care for individuals with physical or mental limitations it can be used for developing persuasive services to motivate people to lead a healthier lifestyle it also can be used in rehabilitation settings or in general in enhancing the wellbeing of individuals ultimately it can support the healthcare professionals in terms of providing innovative communica tion and monitoring tools these systems will provide health monitoring in a transparent and unobtrusive way fig depicts how ami systems might be used as cohesive services integrated into different environments and devices in this survey we will explore how different scientific and technological methodologies can be used for support ing the development of ami based solutions for health care and we will review a multitude of applications that support such healthcare solutions fig interconnected world of ami health services acampora et al a survey on ambient intelligence in healthcare ii supporting infrastructure and technology this section will introduce and describe the supporting infrastructure and technologies used in ami systems in the context of healthcare domain in particular we will ex plain body area networks bans and dense mesh sensor networks in smart homes and we point to some recent trends in sensor technology such as epidermal electronics and microelectromechanical system mems sensors among others a body area networks bans the widespread use of wireless networks and the constant miniaturization of electrical devices has empow ered the development of bans in a ban various sensors are attached on clothing or on the body or even implanted under the skin this new communication approach offers numerous new practical and innovative applications for improving human health and the quality of life by continuously monitoring health features such as heartbeat body temperature physical activity blood pressure electrocardiogram ecg electroencephalogra phy eeg and electromyography emg bans provide a technological infrastructure for remotely streaming sen sored data to a medical doctor site for a real time diag nosis to a medical database for record keeping or to a corresponding technological equipment that proactively and autonomously can issue an emergency alert or intel fig a three tier architecture of ban communication system proceedings of the ieee vol no december ligently manage this information for taking suitable actions and improving the quality of human life there are several benefits of using wireless bans in healthcare applications mainly communication efficiency and cost effectiveness indeed physiological signals ob tained by body sensors can be effectively processed to obtain reliable and accurate physiological estimations at the same time the ultralow power consumption provision of such sensors makes their batteries long lasting more over with the increasing demand of body sensors in the consumer electronics market more sensors will be mass produced at a relatively low cost especially for medical purposes another important benefit of ban is their scala bility and integration with other network infrastructure bans may interface with wireless sensor networks wsns rfid bluetooth bluetooth low energy ble previously called wibree video surveillance systems wireless personal area network wpan wireless local area networks wlans the internet and cellular networks all of these important benefits are opening and expanding new marketing opportunities for advanced consumer electronics in the field of ubiquitous computing for healthcare applications fig better depicts bans communication architec ture in terms of three different layers tier intra ban tier inter ban and tier beyond ban communica tions these architectural layers cover multiple aspects of communication that range from low level to high level design issues and facilitate the creation of a acampora et al a survey on ambient intelligence in healthcare component based efficient ban system for a wide range means of telecommunications the design of beyond ban of applications communication is application specific and should adapt to the term intra ban communications refers to radio the requirements of user specific services for example if communications of about m around the human body any abnormalities are found based on the up to date body which can be further subcategorized as communica signal transmitted to the database an alarm can be notified tions between body sensors and communications be to the patient or the doctor through e mail or short mes tween body sensors and a portable personal server ps sage service sms in fact it might be possible for the device i e a pda as shown in fig due to the direct doctor to remotely diagnose a problem by relying on both relationship with body sensors and bans the design of video communications with the patient and the patient intra ban communications is very critical furthermore physiological data information stored in the database or the intrinsically battery operated and low bit rate features retrieved by a ban worn by the patient of existing body sensor devices make it a challenging issue to design an energy efficient media access control mac b dense mesh sensor networks for smart protocol with quality of service qos provisioning living environments the inter ban communications enables the com besides ban sensors can be embedded into our envi munications between the body sensors and one or more ronments resulting in intelligent and proactive living access points aps the aps can be deployed as part of the environments capable of supporting and enhancing daily infrastructure or be strategically placed in a dynamic life especially in case of elderly or individuals suffering environment for handling emergency situations similarly from mental or motor deficiencies in particular wireless the functionality of a tier network as shown in fig is mesh sensor networks wmsns could be used for design used to interconnect bans with various networks that are ing unobtrusive interconnected adaptable dynamic and easy to access in daily life such as the internet and cellular intelligent environments where processors and sensors are networks we divide the paradigms of inter ban com embedded in everyday objects clothes household devices munications into two categories infrastructure based furniture and so on the sensors embedded into architecture and ad hoc based architecture while the daily environments are usually called ambient sensors infrastructure based architecture provides larger bandwidth as opposed to body sensors the ambient sensors will with centralized control and flexibility the ad hoc based collect various type of data to deduce the activities of in architecture facilitates fast deployment when encountering a habitants and to anticipate their needs in order to maxi dynamic environment such as medical emergency care mize their comfort and quality of life wmsns are response or at a disaster site e g aid n system based on mesh networking topology a type of networking most ban applications use infrastructure based inter where each node must not only capture and disseminate its ban communications that assumes an environment with own data but also serve as a relay for other nodes in other limited space e g a waiting room in a hospital a home an words each sensor must collaborate to propagate the data office etc compared to its ad hoc networks counterpart in the network the main benefits of wmsns is their infrastructure based networks offer the advantage of cen capability to be dynamically self organized and self tralized management and security control due to this configured with the network automatically establishing centralized structure the ap also works as the database and maintaining mesh connectivity among sensors server in some applications such as smart or wmsns do not require centralized aps to mediate the carenet wireless communication and they are particularly suitable tier is intended for streaming body sensor data to to be used in complex and dynamic environments such as metropolitan areas sensor data are moved from inter ban the living spaces network to beyond ban network by using a gateway the general architecture of wmsns described in device for instance a pda could be employed to create a is composed of three distinct wireless network elements wireless link between these two networks transfer body network gateways information between geographical networks and conse access points quently enhance the application and coverage range of mobile and stationary nodes healthcare systems by enabling authorized healthcare per these elements are usually referred to as mesh nodes sonnel e g doctor or nurse to remotely access a patient mns in wmsns each node acts not only as a client but medical information through cellular network or the also as a router unlike wifi hotspots which need a direct internet a database is also an important component of the connection to the internet mesh networks pass a data beyond ban tier in the scenario of healthcare this request until a network connection is found the architec database maintains the user profile and medical history ture of wmsns can be classified into three classes according to a user service priority and or doctor avail infrastructure backbone wmns client wmsns and ability the doctor may access the user information as hybrid wmsns in infrastructure wmsns mesh routers needed at the same time automated notifications can be form an infrastructure for clients in client wmsns all issued to his her relatives based on these data via various client nodes constitute the actual network to perform vol no december proceedings of the ieee acampora et al a survey on ambient intelligence in healthcare routing and configuration functionalities hybrid networks processing and communication technology in general a are a combination of the former two as a result mesh wireless sensor is characterized by its small size and its clients can perform mesh functions with other ones as well capability of sensing environmental in the case of ambient as access the network sensors or physiological information in the case of body the innovative wmsns networking platform allows sensors smart environments to offer new solutions that provide high reliability and power efficiency wmsns also enable ambient sensor architecture ambient sensors typi high adaptability and scalability since low profile mesh cally consist of transducers for measuring the quantity of modules can be easily embedded and integrated with interest e g room temperature and transceivers for existing sensing devices throughout a building to form communicating the collected information seamless networks in general wmsns enable intelligent different approaches can be taken for designing the environments to be characterized by the following transducer hardware the most common and scalable ap faster retrofitting one of the main reasons of in proach is based on development of transducer boards that creasing costs and time delays in retrofitting office can be attached to the main processor board by the means space is caused by the labor intensive movement of of an expansion bus a typical transducer board can provide utility wiring to conform to the new wall organi light temperature microphone sounder tone detector zation by means of wmsns systems designers two axis accelerometer and two axis magnetometer de can relocate sensors quickly and conveniently vices alternatives include economical versions that pro without intrusive disruptive and costly rewiring vide a reduced set of transducers or more expensive efforts versions that boast gps for instance special boards are simplified maintenance low maintenance costs are also available that carry no transducers but provide input a key concern in designing a sensor network the output i o connectors that custom developers can use to self configuring and self healing capabilities of connect their own devices the alternative design approach wmsns combined with its low power usage yield puts transducers directly on the microcontroller board an effective solution to the maintenance issue transducers are soldered or can be mounted if needed but reduced life cycle costs wmsns continuously lead the available options are very limited and generality and to economic benefits because they are easy to expandability is affected however the onboard transducers maintain move or replace resulting in a distrib design can cut production costs and provides more ro uted system with life cycle costs that are signifi bustness than standalone transducer boards which may de cantly less than traditional wired installations tach from the microcontroller board in harsh environments seamless upgrades transitions with the conver through a transceiver circuitry a sensor device com gence and coordination between principal stan municates the sensed information to nearby units using a dard communication corporations such as zigbee physical layer based on radio frequency rf communica alliance and the ashrae bacnet committee tion over the physical layer different protocols have been the transition to a wireless solution is not an implemented for allowing sensors to communicate among all or nothing proposition in this way wmsns themselves the higher number of supported proto can be phased in easilyvone room area floor or cols makes it easier for a ban to be integrated with other building at a time applications bluetooth is a popular wireless protocol for flexibility free from wiring problems systems short range communications but bans need protocols designers can install a wmsn by placing wireless that support low energy consumption and the self controllers virtually anywhere this approach organizing feature seen in ad hoc networks even though results in easily reconfigurable systems to create bluetooth has a very good communications mechanism adaptable workspaces or less intrusively retrofit over a short range it is not a very feasible solution for the existing network infrastructures while saving bans to overcome these problems most of the ban ap time and reducing costs plications use the zigbee protocol a key component of the some examples of wmsns for intelligent living zigbee protocol is its ability to support mesh networks environments have been provided by the siemens apo zigbee is used today for communications among sensors gee project and by the homemesh project both in a network some of the advantages of using zigbee are projects highlight that starting from wmsns features it it incurs low energy consumption for communications will be possible to design living spaces particularly suitable between the nodes it has a low duty cycle that enables it for supporting the capabilities of elderly or individuals to provide longer battery life its communications pri with disabilities in order to enhance their quality of life mitives enable low latency communications and it supports b security in addition it has all the basic c sensor technology features required for communications between the sensors both bans and wmsns can be viewed as a collection in wireless nodes zigbee also enables broadbased of interconnected wireless sensors based on a particular deployment of sensor networks in a cost effective manner proceedings of the ieee vol no december some of the most widely used ambient sensors are summarized in table bans hardware and devices a body sensor node mainly consists of two parts the physiological signal sensor and the radio platform to which multiple body sensors can be connected in order to create a complex communication network the general functionality of body sensors is to collect analog signals that correspond to human physio logical activities or body actions the analog signal is later digitized by an analog to digital converter a d and is forwarded to the network to be analyzed different body sensors for measuring physiological signs are summarized in table where depending on the captured physiological signal high or low data sampling rate might be needed more specifically some of the most important body sensors include the following accelerometer gyroscope accelerometers are used in the field of healthcare for recognizing body postures e g sitting kneeling crawling laying standing walking running and so on the accelerometer based posture monitoring for bans typically consists of three axis accelerome ters or tri axial accelerometers positioned on well defined locations on a human body they can table ambient sensors used in smart environments table body sensors acampora et al a survey on ambient intelligence in healthcare also be used to measure the vibration or acceler ation due to the gravity useful for recognizing for example elderly falls gyroscopes are used for measuring orientation based on the principle of conservation of angular momentum gyroscopes are typically used together with accelerometers for physical movement monitoring blood glucose glucose also referred to as blood sugar refers to the amount of glucose circulating in the blood traditionally glucose measurements are done by pricking a finger and extracting a drop of blood which is applied to a test strip composed of chemicals sensitive to the glucose in the blood sample an optical meter glucometer is used to analyze the blood sample and gives a numerical glucose reading recently noninvasive glucose monitoring has become available through infrared technology and optical sensing blood pressure the blood pressure sensor is a non invasive sensor designed to measure systolic and diastolic human blood pressure utilizing the oscillometric technique co gas sensor this sensor measures gaseous carbon dioxide levels to monitor changes in co levels as well as to monitor oxygen concentration during human respiration ecg sensor ecg is a graphic record of the heart electrical activity healthcare providers use it to help diagnose a heart disease they can also use it to monitor how well different heart medications are working in order to obtain an ecg signal several electrodes are attached at specific sites on the skin e g arms and chest and the potential differences between these electrodes are measured eeg sensor this sensor measures electrical activity within the brain by attaching small electrodes to the human scalp at multiple locations then in formation of the brain electrical activities sensed by the electrodes is forwarded to an amplifier for producing a pattern of tracings synchronous elec trical activities in different brain regions are gene rally assumed to imply functional relationships between these regions in a hospital the patient may be asked to breathe deeply or to look at a flashing light during the recording of eeg emg sensor emg measures electrical signals pro duced by muscles during contractions or at rest nerve conduction studies are often done together while measuring the electrical activity in muscles since nerves control the muscles in the body by electrical signals impulses and these impulses make the muscles react in specific ways nerve and muscle disorders cause the muscles to react in abnormal ways pulse oximetry this sensor measures oxygen satu ration using a noninvasive probe a small clip with vol no december proceedings of the ieee acampora et al a survey on ambient intelligence in healthcare a sensor is attached to the person finger earlobe or toe the sensor gives off a light signal that passes through the skin according to the light absorption of oxygenated hemoglobin and total hemoglobin in arterial blood the measurement is expressed as a ratio of oxygenated hemoglobin to the total amount of hemoglobin humidity and temperature sensors they are used for measuring the temperature of the human body and or the humidity of the immediate environment around a person an alarm signal can be issued if a certain amount of changes are measured recent trends in sensor technology since body sen sors are in direct contact with body tissue or might even be implanted their size and physical compatibility with hu man tissues are crucial this motivates the research and synthesis of novel materials and technologies such as the mems the mems is an innovative technology for sensors design based on miniaturized mechanical and electromechanical elements i e devices and structures that are made using the techniques of microfabrication the physical dimensions of mems devices can vary from well below m on the lower end of the dimensional spectrum all the way to several millimeters as a conse quence they open up new scenarios for ubiquitous health care applications recently mems technology has been used for design of different kinds of sensors such as acce lerometer blood glucose blood pressure carbon dioxide ments are allowing cameras to be made so small as to be embedded into eye glasses as a consequence enhancing the capabilities of bans with vision features the captured images can be mapped to audible outputs in order to assist people who have eyesight problems the images can even be translated to other kinds of formats e g gentle elec trical impulses on the tongue together with a lollipop sized electrode array in their mouths blind people can also be trained to regain vision co gas sensor ecg eeg emg gyroscope pulse oxi metry as well as some sensors typically used in wsns for iii algorithms and methods example in case of ecg bedside monitoring disposable electrodes are traditionally made of silver chloride agcl however long term usage of these types of electrodes may cause failure of electrical contacts as well as skin irritation problems mems technology can alleviate this problem by using textile structured electrodes which are embedded in this section we introduce the set of computational methodologies that combined with technologies pre sented in section ii enable systems designers to develop enhanced ami healthcare applications as summarized in tables and into clothes fabrics these textile structure electrodes possibly woven into clothes will not cause any skin irri tation and thus are comfortable and suitable for long term monitoring compared to the conventional electrodes a activity recognition ami systems focus on the needs of a human and therefore require information about the activities being they are also much more flexible since their shape can be adapted to human motion other research directions are also considering the pos sibility of innovative and noninvasive sensors for example the massachusetts institute of technology mit cambridge ma usa researchers have designed a scalable electronic sensate skin by using a collection of flexibly interconnected small rigid circuit boards fig each board contains an embedded processor together with a suite of sensors providing dense multimodal capture of proximate and contact phenomena other important results have been obtained in the field of computer vision thanks to the advancement of charge coupled devices ccd and complementary metal oxide semiconductor cmos active pixel sensors the recent advance proceedings of the ieee vol no december fig the scalable electronic sensate skin from mit table ambient health algorithms and methods performed by the human 34 at the core of such tech nologies is activity recognition which is a challenging and well researched problem the goal of activity recognition is to identify activities as they occur based on data col lected by sensors there exist a number of approaches to activity recognition that vary depending on the underlying sensor technologies that are used to monitor activities the alternative machine learning algorithms that are used to model the activities and the complexity of the activities that are being modeled sensor modalities advances in pervasive computing and sensor networks have resulted in the development of a wide variety of sensor modalities that are useful for gathering information about human activities in the case of wearable sensors sensors are attached to the body or woven into garments when three axis acce lerometers are distributed over an individual body then each sensor can provide information about the orientation table ami applications in healthcare acampora et al a survey on ambient intelligence in healthcare and movement of the corresponding body part research ers commonly use these inertial measurement units to recognize ambulatory movements e g walking running sitting climbing and falling posture and gestures environment sensors such as infrared motion detec tors magnetic door sensors break beam sensors and pressure mats have been used to gather infor mation about more complex activities such as cooking sleeping and eating these sensors are adept in perform ing location based activity recognition in indoor environ ments and their long battery life supports long term data collection because this approach embeds sensors within environments it is well suited to creating ambient intelligent applications such as smart environments and has been widely adopted for health monitoring and ambient assisted living some activities such as washing dishes taking medi cine and using the phone are characterized by interacting vol no december proceedings of the ieee acampora et al a survey on ambient intelligence in healthcare with unique objects in response researchers have ex concurrent activities humans often make efficient plored the usage of rfid tags and acceler use of time by performing a step for one activity while still ometers or shake sensors for tagging these objects and in the middle of another activity causing the sensor using the data for activity recognition the challenge with streams to interweave concurrent activities may occur if a this modality is deciding which objects to tag with sensors single sensor event contributes to more than one activity one approach that has been investigated is to this situation may also indicate that multiple residents are mine web page description of activities to determine which in the space which can be a challenge for activity recogni objects are instrumental to the activity and help differen tion algorithms tiate the activity from others other modalities that have been researched for activity b behavioral pattern discovery recognition include video cameras microphones ami systems focus on the needs of a human and there and gps locators each of these does fore require information about the activities being face a unique challenge for use in healthcare applications performed while recognizing predefined activities often cameras and microphones need to be carefully positioned relies on supervised learning techniques unsupervised and robust in the presence of occlusion furthermore learning is valuable for its ability to discover recurring se these technologies are not always well accepted because of quences of unlabeled sensor activities that may comprise privacy concerns smartphones are increasing in popular activities of interest methods for activity discovery build ity for activity recognition because sensors in the on a rich history of discovery research including methods for phone collect all of the gyro accelerometer gps acoustic mining frequent sequences mining frequent pat and video data found the other methods as long as they are terns using regular expressions constraint based min on the individual while they perform activities ing and frequent periodic pattern mining more recent work extends these early approaches to activity models the methods that are used to model look for more complex patterns ruotsalainen et al and recognize activities are as varied as the sensor modali design the gais genetic algorithm to detect interleaved ties used to observe activities existing methods can be patterns in an unsupervised learning fashion other ap broadly categorized into template matching transductive proaches have been proposed to mine more complex dis techniques generative and discriminative approaches continuous patterns from streaming data over template matching techniques employ a nearest neighbor time in different types of sequence data sets and to classifier based on euclidean distance or dynamic time allow variations in occurrences of the patterns dis warping generative approaches such as naive covered behavioral patterns are valuable to interpret bayes classifiers where activity samples are modeled using sensor data and models can be constructed from the dis gaussian mixtures have yielded promising results for batch covered patterns to recognize instances of the patterns learning generative probabilistic graphical when they occur in the future models such as hidden markov models 72 and dynamic bayesian networks have been used to c anomaly detection model activity sequences and to smooth recognition results while it is value to characterize and recognize common of an ensemble classifier decision trees as well as normal activities that account for the majority of the sen bagging and boosting methods have been tested sor events that are generated for health applications we discriminative approaches including support vector ma are also very interested in abnormal events these abnor chines and conditional random fields mal events may indicate a crisis or an abrupt change in a which attempt to maximally separate activity clusters have regimen that is associated with health difficulties also been effective abnormal activity detection or anomaly detection is also important in security monitoring where suspicious activity complexity many of these methods analyze activities need to be flagged and handled anomaly detec presegmented activity sequences that have been collected tion is most accurate when it is based on behaviors that are in controlled settings more recently attempts have been frequent and predictable there are common statistical made to perform automatic segmentation of the data into methods to automatically detect and analyze anomalies sensor events that belong to the same activity class including the box plot the chart and the cusum chart still others have focused on recognizing acti anomalies can be captured at different population vities in real time from continuous sensor streams in scales for example while most of the population may addition researchers have also investigated methods of perform activity a one person carries out activity b which leveraging information or models in one setting to boost pinpoints a condition needing further investigation activity recognition for a new sensor network a new anomalies may also be discovered at different temporal environmental setting or new activity la scales including single events days or weeks bels another level of complexity for activity recogni little attention has been devoted to anomaly detection tion is analyzing data for interwoven activities or in ambient environments this is partly because the notion proceedings of the ieee vol no december acampora et al a survey on ambient intelligence in healthcare of an anomaly is somewhat ill defined many possible cially to help dementia patients coach is one such interpretations of anomalies have been offered and use system which provides task guidance to alzheimer cases have even been generated for ami environments disease patients it uses a hand coded representation 97 some algorithmic approaches have been suggested of detailed steps of hand washing and relies on vision that build on the notion of expected temporal relationships techniques to recognize user steps if a user is unable to between events and activities others tag events as complete a particular step detailed instructions are pro anomalies if they occur rarely and they are not anticipated vided another example is peat which also provides task for the current context guidance to the user it maintains a detailed model of the daily plan in terms of hierarchal events and tracks d planning and scheduling their execution peat has the capability of rescheduling automatic planning and scheduling can be useful in activities in case of unexpected events however it lacks many ami applications automatic planning techniques any real sensory information from the world except for achieve a goal state by starting from an initial known state user feedback autominder by pollack et al is and choosing among possible actions at each state plan another system which provides users with reminders about ning can be useful in a number of different ami care their daily activities by reasoning about any disparities related scenarios for example planning can be used to between what the client is supposed to do and what she is schedule daily activities in a flexible manner for reminding doing and makes decisions about whether and when to dementia patients about their daily activities it also can be issue reminders used in order to detect any possible deficiencies in task execution and to help dementia patients to complete those e decision support steps another use of planning is in automating daily decision support systems dsss have routines in order to allow users with physical limitations been widely used in the field of healthcare for assisting to live a more independent lifestyle physicians and other healthcare professionals with in the past many planning techniques have been pro decision making tasks for example for analyzing patient posed some techniques include decision theoretic techni data dsss are mainly based on two main ques e g markov decision processes search stream approaches knowledge based and nonknowledge methods e g forward and backward search based graph based techniques e g graphplan hierarchal the knowledge based dss consists of two principal techniques e g o plan and reactive planning components the knowledge database and the inference techniques e g for example graph based plan engine the knowledge database contains the rules and ning techniques represent search space of possible actions in associations of compiled data which often take the form of the form of a graph hierarchal planning techniques use hie if then rules whereas the inference engine combines the rarchies to predefine groups of actions and reactive planning rules from the knowledge database with the real patients techniques adjust the plan based on sensed information data in order to generate new knowledge and to propose a ami applications pose many new challenges to the set of suitable actions different methodologies have been classical planning techniques for example the planner has proposed for designing healthcare knowledge databases to be functional in a dynamic environment where the and inference engines such as the ontological represen outcome of the actions and their duration is not determin tation of information istic also the availability of resources might change due to the nonknowledge based dsss have no direct clinical user mobility or other factors therefore more advanced knowledge about a particular healthcare process however planning techniques have been proposed by extending they learn clinical rules from past experiences and by classical planning techniques one example is the finding patterns in clinical data for example various ma distributed hierarchal task network d htn technique chine learning algorithms such as decision trees represent which extends the hierarchal task network htn it methodologies for learning healthcare and clinical uses a centralized approach to manage the distributed ca knowledge pabilities provided by the distributed devices the distrib both of these approaches could be used in conjunction uted devices might be available in a permanent or transient with ami technologies indeed the sensitive adaptive and manner d htn has been studied in the context of care for unobtrusive nature of ami is particularly suitable for de diabetic patients at home where different home devices signing decision support systems capable of supporting communicate and coordinate plans with each other in a medical staffs in critical decisions in particular ami distributed manner for example data from monitoring technology enables the design of the third generation of devices might require actions such as adjusting the room telecare systems the first generation was the panic alarms temperature suggesting insulin injection or contacting gadgets often worn as pendants or around the wrist to medical help allow a person to summon help in the case of a fall or other several ami systems have been reported in the litera kinds of health emergency the second generation of tele ture which use automated planning and scheduling espe care systems uses sensors to automatically detect situations vol no december proceedings of the ieee acampora et al a survey on ambient intelligence in healthcare where assistance or medical decisions are needed finally e textile or smart fabrics the majority of these sensors the third generation represents ami based systems which allow for noninvasive monitoring of physiological signs move away from the simple reactive approach and adopt a though some physiological measurements such as eeg proactive strategy capable of anticipating emergency still require the use of invasive devices and sensors e g situations as a result dsss could be used with multimodal measuring eeg requires the use of electrodes regardless sensing and wearable computing technologies for con of the form of the sensors such sensors allow the patients stantly monitoring all vital signs of a patient and for with chronic diseases to be in control of their health con analyzing such data in order to take real time decisions and dition by benefiting from continuous monitoring and opportunely support people anomalous situation detection achieving continuous finally dsss are jointly used with the ami paradigm monitoring is almost impossible in conventional healthcare for enhancing communications among health personnel settings where typical measures are taken only during such as doctors and nurses for example anya et al have occasional doctor visits the use of such sensors will also introduced a dss based on context aware knowledge allow the healthy adults to keep track of their health status modeling aimed at facilitating the communication and and to take the necessary steps for enhancing their lifestyle improving the capability to take decisions among health gouaux et al describe a wearable personal ecg care personal located in different geographical sites monitoring device pem for early detection of cardiac events which detects and reports anomalies by generating f anonymization and privacy preserving techniques different alarm levels another example is amon which is as ambient intelligent systems become more ubiqui in the form of a wristband and measures various physiolo tous more information will be collected about individuals gical signals today there are several commercially and their lives while the information is intended to available health monitoring devices such as healthbuddy promote the wellbeing of individuals it may be considered by bosch telestation by philips healthguide an invasion of privacy and if intercepted by other parties by intel and genesis by honeywell a number could be used for malicious purposes of academic projects also have tried to integrate monitoring while some privacy concerns focus on the perception of devices with clothing fabrics including the wealthy intrusive monitoring many heavily deployed internet project biotex project and magic project gadgets and current ambient intelligent systems are nearly for example biotex monitors sore conditions devoid of security against adversaries and many others based on ph changes and inflammatory proteins concen employ only crude methods for securing the system from tration other projects have tried to provide a variety internal or external attacks the definition of privacy will of accessible medical implants for example the healthy continue to evolve as ambient intelligent systems mature aims project focuses on developing a range of medical this is highlighted by the fact that even if personal implants to help the aging population developing information is not directly obtained by an unwanted party completely noninvasive methods for health monitoring is much of the information can be inferred even from aggre another active research area for example masuda et al gated data for this reason a number of approaches are measure physiological signs such as respiration rate being developed to ensure that important information and heart beat by measuring perturbations in the pressure cannot be gleaned from mined patterns of an air filled mattress and relying on the low frequency characteristics of heart and respiration similarly iv applications andoh et al have developed a sleep monitoring mattress to analyze respiration rate heart rate snoring and body different kinds of ami applications for healthcare have movement the self smart home project also moni been developed in academia and industry as summarized tors various factors such as posture body movement in table this section discusses each application class breathing oxygen in the blood airflow at mouth and nose by presenting both scientific and real world frameworks and apnea using pressure sensor arrays cameras and and highlights the benefits provided to patients elderly microphones and so on continuous behavioral monitoring in addition to a continuous monitoring monitoring physiological measures another potential monitoring application is behavioral monitoring behav continuous health monitoring in the past decade a ioral monitoring especially can be useful in assisted living variety of noninvasive sensors have been developed for settings and monitoring of individuals with mental measuring and monitoring various physiological param disabilities such systems can assess mental health and eters such as ecg eeg eda respiration and even bio cognitive status of inhabitants in a continual and natural chemical processes such as wound healing some of those istic manner they can also provide automated assistance sensors are in the form of wearable devices such as wrist and can decrease the caregiver burden in some cases a bands while others are embedded into textile known as single activity is monitored for example nambuet al proceedings of the ieee vol no december acampora et al a survey on ambient intelligence in healthcare monitor watching tv for diagnosing health conditions the assistance if needed those services especially can be majority of research projects monitor a subset of daily tasks useful for the older adults who are suffering from physical for example the casas project monitors a subset of and cognitive decline daily tasks to identify consistency and completeness in daily we already have discussed how behavioral monitoring activities of dementia patients the immed project and fall detection methods can be useful for the elderly monitors instrumented activities of daily living iadl in medication management is another area which can dementia patients by using a wearable camera to monitor provide great benefit to the elderly the ma the loss of motor or cognitive capabilities other re jority of older adults take many different medications and searchers have worked on recognizing social activity espe they usually forget medication dosage and timing due to cially in nursing homes identifying any cognitive decline using appropriate contextual informa changes in activities might be an indicator of cognitive or tion obtained from various sensors medication reminders physical decline for example indicators such as changes in can be delivered in a context aware and flexible manner movement patterns walking speed number of outgoings care personnel can be contacted if noncompliance is de and sleep rhythm have been identified as early signs of tected for example john will be reminded about his dementia medications right after finishing his breakfast but he will not be reminded if he is watching his favorite program on monitoring for emergency detection there also have tv or if he is talking on the phone if john forgets to take been some projects to monitor emergency situations in the his medication more than a certain number of times united kingdom british telecom bt and liverpool city depending on the medication his doctor will be auto council have developed a project on telecare technology matically contacted current medication management sys which monitors residents using a variety of sensors such as tems are not yet fully context aware though there has passive infrared pir sensors in case of any detected been some great progress for example imat is a user hazards the system asks the residents if they are ok other friendly medication management system an imat wise the selected personnel are notified another impor user has no need to understand the directions of her his tant area of emergency detection is fall detection which medications rather imat enables the pharmacist of each can be especially useful for the elderly as falls contribute to user to extract a machine readable medication schedule a high rate of morbidity and mortality in elderly fall specification from the user prescriptions or over the detection techniques rely on several technologies wearable counter descriptions once loaded into an imat dispenser devices ambient sensors and cameras wearable fall or schedule manager the tool automatically generates a detection systems measure posture and motion using sen medication schedule other medication management tools sors such as accelerometer and gyroscope and by measuring also have been proposed by researchers such as the orientation and acceleration ambient fall magic medicine cabinet which can provide reminder detection systems use ambient sensors such as pir sensors and can interact with healthcare professionals or the and pressure sensors to detect falls they also rely on smart medicine cabinet which uses rfid tags to techniques such as floor vibration detection and ambient monitor medication usage and can communicate with a audio analysis to detect possible fall finally cellphone vision based fall detection systems extract video features besides medication management other cognitive or such as d motion shape and inactivity to detect falls thotics tools can be quite useful for people with mental there are also some preventive fall detection disabilities especially older adults suffering from demen tools such as the smart cane developed by wu et al which tia coach is a cognitive orthotics tool which relies on classifies cane usage and walking patterns and informs the planning and vision techniques to guide a user through elderly in case of high risk of falling hand washing task other cognitive orthotics tools it should be noted that there is a huge potential for such as peat and autominder also use auto combining and fusing data from various sensors such as mated planning to provide generic reminders about daily physiological sensors with electronic health records activities they can adjust their schedules in case of any ehrs or daily activity information this will allow changes in the observed activities cognitive orthotics the healthcare to shift from cure to prevention by early tools also can be used for cognitive rehabilitation detection of diseases using continuous monitoring as well sensecam is a small wearable camera developed by as to reduce the need for institutional care by shifting the microsoft which captures a digital record of the wearer care to a personalized level day in terms of images in addition and a log of sensor data it has been shown to help dementia patients to b assisted living recollect aspects of earlier experiences that have subse ami technology can allow individuals with disabilities quently been forgotten thereby acting as a retrospective to maintain a more independent lifestyle using home au memory aid hoey et al also describe the develop tomation it can offer them continuous cognitive and phy ment of a cognitive rehabilitation tool to assist art thera sical monitoring and can provide them with real time pists working with older adults with dementia vol no december proceedings of the ieee acampora et al a survey on ambient intelligence in healthcare ami tools also can be useful for preventing wandering ogies for example the alarm net project is an behavior of older adults who suffer from dementia there assisted living and residential monitoring network for per are a number of outdoor wandering prevention tools vasive healthcare developed at the university of virginia kopal and outcare support issues related to blacksburg va usa it integrates environmental and disorientation by contacting the caregiver in case of leaving physiological sensors in a scalable heterogeneous archi predefined routes or deviating from daily signature routes tecture to support real time data collection and processing a number of tools have also been developed for preventing the alarm net network creates a continuous medical indoor wandering for example lin et al 167 use rfid history while preserving resident comfort and privacy by technology to detect if people prone to disorientation e g using unobtrusive ambient sensors combined with wear children or elderly have approached a dangerous area and able interactive devices the project complete crombag proposes using virtual indoor fencing some ambient assisted living experiment caalyx is commercially available products for wandering prevention another project for increasing elderly autonomy and self include safedoor and safetybed for example confidence by developing a wearable light device capable safedoor raises an alarm if a person walks out a door with of measuring specific vital signs and detecting falls and for out opening it to prevent nighttime wandering navigation communicating in real time with care providers in case of assistance tools also have been developed to help elderly an emergency myheart is an integrated project for suffering from early signs of dementia opportunity developing smart electronic and textile systems and ser knocks is a mobile application which provides public vices that empower the users to take control of their own transit system guidance by learning user routes health status the system uses wearable technology a number of ami projects try to provide comprehen and smart fabrics to monitor patients vital body signs in sive assistance through a variety of services robocare is order to provide proper wellbeing recommendations to the an assisted living project providing assistance to people user the saphire project develops an intelligent with disabilities using a combination of software robots healthcare monitoring and decision support system by intelligent sensors and humans it uses a tracking integrating the wireless medical sensor data with hospital system for tracking people and robots by exploiting vision information systems in the saphire project the techniques to determine various d positions it also re patient monitoring will be achieved by using agent tech lies on a task execution and monitoring component to nology complemented with intelligent decision support recognize current situation and to compare it with the systems based on clinical practice guidelines the observa expected schedule the aware home research initiative tions received from wireless medical sensors together with ahri at georgia tech atlanta ga usa includes a the patient medical history will be used in the reasoning number of different projects focused on providing assis process the patient history stored in medical informa tance to elderly such as the independent lifestyle assis tion systems will be accessed through semantically tant project which monitors the behavior of elderly in a enriched web services passive manner and alerts caregivers in case of emergency e g fall the technology coach is another ahri c therapy and rehabilitation project which watches the use of home medical devices by according to the disability and rehabilitation team at the elderly and provides appropriate feedback and guid the world health organization who the estimated ance for better use smart home projects such as number of people who require rehabilitation services is casas also try to provide comprehensive monitoring and continuously growing of the entire world popula assistance services in a noninvasive manner by relying on tion nevertheless the current healthcare solutions various machine learning and data mining techniques to and technologies are not nearly sufficient to fulfill the make sense of sensor data rehabilitation needs in such scenarios ami can shape ami systems also can provide great help to visually innovative rehabilitative approaches that support indivi impaired people a number of different systems have been duals to have access to rehabilitation resources this can be proposed for blind navigation relying on various sensors achieved by developing ad hoc rehabilitation systems based such as rfid tags infrared sensors and gps technology on sensor networks and other technological approaches chumkamon et al used rfid tags to develop a such as robotics and brain computer interfaces bci tracking system for indoor guidance of blind persons sensor networks have the potential to greatly impact chen et al embed rfid tags in the tiles of a blind many aspects of medical care including rehabilitation path for better navigation some systems also use audio for example jarochowski et al propose the interface to communicate the name of important locations implementation of a system the ubiquitous rehabilitation to the user e g the sawn system there are also center which integrates a zigbee based wireless network applications to facilitate daily tasks such as shopping e g with sensors that monitor patients and rehabilitation the shoptalk project machines these sensors interface with zigbee motes finally several ami based assisted living environments which in turn interface with a server application that have been designed by using decision support methodol manages all aspects of the rehabilitation center and allows proceedings of the ieee vol no december rehabilitation specialists to assign prescriptions to patients another systems proposed by piotrowicz et al de scribes the requirements of a system for cardiac telereh abilitation at home and in particular it discusses the different components controlling a physical exercise train ing session which needs to recognize and identify critical patient states through a continuous monitoring based on ami technology and react accordingly as a side effect the health related data gathered during the telerehabilita tion session are used for providing cardiologists with useful information for patient care the rehabilitation systems proposed by helmer et al improve the quality of life for patients suffering from the chronic obstructive pulmonary disease copd the system includes a component for monitoring the rehabilitation training and automatically as a consequence it controls the target load for the exercise on the basis of his or her vital data moreover by equipping patients with wireless wear able or environmental vital sign sensors collecting de tailed real time data on physiological status can enable innovative activities as autonomous rehabilitation and the rapy the stroke rehab exerciser by philips research guides the patient through a sequence of exercises for motor retraining which are prescribed by the physiotherapist and uploaded to a patient unit the system lies on a wireless inertial sensor system aimed at recording the patient movements analyzes the data for deviations from a personal movement target and provides feedback to the patient and the therapist the stroke rehab exerciser coaches the patient through a sequence of exer cises for motor retraining which are prescribed by the physiotherapist and uploaded to a patient unit a wireless inertial sensor system records the patient movements analyzes the data for deviations from a personal movement target and provides feedback to the patient and the therapist the hocoma ag valedo system see fig is a medical back training device which improves patient compliance and allows one to achieve increased fig the hocoma ag valedo at work acampora et al a survey on ambient intelligence in healthcare motivation by real time augmented feedback based on trunk movements it transfers trunk movements from two wireless sensors into a motivating game environment and guides the patient through exercises specifically designed for low back pain therapy in order to challenge the patient and to achieve more efficient training the exercises can be adjusted according to the patient specific needs finally ge healthcare is developing a wireless medical monitoring system that is expected to allow one to gather physiological and movement data thus facilitating rehabil itation interventions in the home setting several other systems are currently under research and development as an example jovanov et al have developed a computer assisted physical rehabilitation applications and ambulatory monitoring based on a wireless body area network wban this system performs real time analysis of sensors data providing guidance and feedback to the user in different therapy fields such as stroke rehabilita tion physical rehabilitation after hip or knee surgeries myocardial infarction rehabilitation and traumatic brain injury rehabilitation a practical application example is given by the tril project that by means of its sub component named base provides a home based interactive technology solution to deliver and validate the correctness of a personalized physiotherapist prescribed exercise program to older adults base uses a sensor net works to gather data necessary to deliver the exercise program and it exploits computer vision algorithms for validating the correctness of these rehabilitation experi ences one of the main aims of the active care project is related to the support of at risk elders this project exploits two environmental cameras for extracting human silhouettes and investigating the human gait by analyzing shoulder level spinal incline and silhouette centroid this analysis could be precious for remotely or autonomously aiding elder or impaired people other interesting work based on sensors networks is re lated to the design of rehabilitation systems for degener ative pathologies such as parkinson disease giansanti et al present the results of a pilot study to assess the feasibility of using accelerometer data to esti mate the severity of symptoms and motor complications in patients with parkinson disease this system is based on a support vector machine svm classifier used for estimat ing the severity of tremor bradykinesia and dyskinesia from accelerometer data features and as a consequence optimizing the patient therapy bachlin et al also introduce a wearable assistant for parkinson disease pa tients with the freezing of gait fog symptom this wearable system uses on body acceleration sensors to measure the patients movements to detect fog and auto matically provide a rhythmic auditory signal that stimu lates the patient to resume walking in the future by using the wearable sensor networks together with haptic hard ware it will be possible to design medical training systems based on augmented reality frameworks for improving vol no december proceedings of the ieee acampora et al a survey on ambient intelligence in healthcare medical staff capabilities to support elderly or patients during their rehabilitation the combination of sensor network technology and robots is also a very recent development in the field of rehabilitation systems interest in this ap proach originates from the observation that subjects with chronic conditions such as hemiparesis following a stroke could benefit from therapeutic interventions that can be facilitated by robotic systems and enhanced by wearable technology indeed these integrated sys tems could be used in a variety of healthcare scenarios a concrete application of these concepts is the human friendly assistive home environment intelligent sweet home ish developed at kaist korea 209 the system considers the residents lifestyle by continuously checking their intention or health status the home itself is seen as an intelligent robot supporting actively the with appropriate services people with disabilities kubota et al also propose a similar hybrid ami robotic systems for aiding disabled people with quadriplegia recently there has been some attempt to further im prove the sensor networks rehabilitation capabilities by sensing electroencephalography eeg signals directly fig the persuasive mirror project at works using bci technology bci systems represent a natural extension for ami environments indeed they are envi sioned to be typically used for allowing smart environ ments habitants to deal with their surrounding space in a transparent way this effortlessly interaction approach is particularly suitable for enhanced rehabilitation systems the aspice and dat projects are examples of this kind of technology which allow the temporary or permanent neuromotor disabled persons to improve or recover their mobility directly or by emulation as well as their communication skills d persuasive wellbeing applications persuasive technology represents computing systems devices or applications intentionally designed to change a person attitude or behavior in a predeter mined way in order to motivate people to lead a healthier life style by mediating prevention and treatment although the field of persuasive technologies has lately attracted lots of attention only recently the notion of am bient persuasive technology was introduced 216 ambient persuasive technology constitutes a radi cally new category of relationships between human beings and technological artifacts by blending insights from the behavioral sciences with computing technology one of the first examples of computerized persuasion system for healthy living is the persuasive mirror this system uses ubiquitous sensors for continuously gathering information about human behaviors and provides the user with continuous visual and nonintrusive feedback match ing the psychological strategy see fig other appli cations of the ambient persuasive mirror are introduced in 2484 proceedings of the ieee vol no december another seminal application of ambient persuasive technology is provided by the hygieneguard projector this environmental persuasive system is used in restaurants and hospitals to motivate employees or work ers to wash their hands before leaving a restroom the equipment is installed in restrooms and every employee is required to wear a badge whenever the employee goes to the restroom she has to use the sink for a period of time de carolis et al presents an approach to ambient persuasion based on a combination of pervasive and distri buted computation to motivate users at a fitness center the user is surrounded by several connected devices that cooperate during persuasion process another interesting ami application based on persuasive technologies is percues different from the previous applications percues is oriented to achieve a collective human well being by persuading users to reach a common goal like decreasing environmental pollution the project perframe implements a persuasive interface in the form of an interactive picture frame which integrates unobtrusively into the working environment and provides affective feed back in order to persuade employees to adopt more healthy habits while working with a computer etiobe is another project devoted to treat child obesity its architecture merges ubiquitous intelligent and persuasive features for implementing a cybertherapy approach it is based on virtual and augmented reality and attempts to persuade children to avoid poor eating habits the system uses a collection of environmental sensors for capturing important information such as contextual phy siological and psychological data acampora et al a survey on ambient intelligence in healthcare last some game based ambient persuasive systems for particular the system consists of a sensor enabled mobile well being have been introduced or they are under devel phone a wrist worn activity monitor a physiological sen opment for example the project dance revolution sor gsr pulse a stationary eeg system for periodic connects a sensor enabled dance floor with a video inter measurements and a home gateway combining the sen face and provides stimulating exercise as dance competi sor information with patients medical records and estab tion a recent trend is the use of motion sensing lished psychiatric knowledge a prediction of depressive controllers such as wiimote or kinect sensor allowing the and manic episodes is given individuals to naturally manipulate digital worlds in per the emo pain project is an intelligent system suasive games taken together this body of work demon that enables ubiquitous monitoring and assessment of strates that games and social competition can be used to patient pain related mood and movements specifically establish long term commitments for example such this system aims to develop a collection of methods for games can be used by the elders or individuals with physi automatically recognizing audiovisual cues related to pain cal impairments during their rehabilitation sessions behavioral patterns typical of low back pain and affective states influencing pain aziz et al also propose an e emotional wellbeing animated conversational agent providing emotional sup recent advances in neurology and psychology have port and companionship in order to promote the emotional demonstrated the importance of emotions in various as wellbeing of patients and enhance patient care and out pects of our lives and in particular in the field of health comes during a hospital stay care and wellbeing indeed it has been demonstrated that negative emotions have adverse effects on the immune f smart hospitals system of a person emotions are typically commu ami technology can be also useful for other stakehold nicated through three channels audio speech face and ers such as nurses doctors and other healthcare personnel body gestures visual and internal physiological changes especially for facilitating communication among them such as blood pressure heart beat rate or respiration sánchez et al have developed the ihospital project which ami sensor based infrastructures may represent a provides context aware communication based on activity suitable tool for recognizing and managing emotions as recognition various pieces of contextual information well as for improving wellbeing mcnaney et al have are collected and used including location time the roles of designed a wearable acoustic monitor wam device the people present and rfid tagged artifacts in order to which provides support in various aspects of social and help in decision making and communication emotional wellbeing by inferring levels of social interac there have been some efforts to create ami based tion and vocal features of emotionality it can monitor and middleware for healthcare rodrı guez et al describe evaluate the level of the wearer voice by identifying vocal development of salsa an agent based middleware to features such as amplitude pitch rate of speech and pause facilitate responding to the particular demands of patients length in order to provide insight into the emotionality of and hospital personnel salsa takes into account the the wearer at a given time this feature allows the indi distributed access nature of doctors which is a result of vidual to reflect on the contexts or situations which prove their high mobility a doctor has to access patients clinical particularly stressful or pleasurable and may affect future records access medical devices distributed throughout the behaviors another interesting application of environmen premises and communicate with colleagues spread tal sensors and ami to emotional wellbeing is affectaura throughout the hospital in order to track the location of project this system continuously predicts users people rodriguez et al use rf signal strength between valence arousal and engagement based on information mobile devices and aps and build a signal propagation gathered from webcam kinect sensor microphone elec model to estimate the distance trodermal activity sensor gps file activity sensor and favela et al describe several possible scenarios calendar scraper the users were allowed to leverage cues for using ami in hospitals and build their frameworks from affectaura to construct stories about their days even around those scenarios for example dr garcia is check after they had forgotten particular incidents or their re ing the patient in bed when he is alerted that a new lated emotional tones another project emosonet message has arrived his handheld device displays a introduces an emotion aware social network for the pur hospital floor map informing him that the x ray results of pose of increasing emotional wellbeing the framework another patient are available he approaches the nearest uses sensors and behavior analysis methods in order to public display that detects his presence and provides him infer users stress level automatically with minimal user with a personalized view dr garcia selects the message effort it also uses audio animation and vibrotactile feed on bed which opens windows displaying the patient back for enhanced engagement another system named medical record and the x ray image recently taken aware monarca 230 develops and validates solutions for of the context of the situation the system automatically multiparametric long term monitoring of behavioral and opens a window with the hospital medical guide that physiological information relevant to bipolar disorder in relates to the patient current diagnosis and an additional vol no december proceedings of the ieee acampora et al a survey on ambient intelligence in healthcare one with previous similar cases to support the physicians content and yielding performances better than conven analysis while dr garcia is analyzing the x ray image he tional data mining approaches thanks to these innovative notices on the map that a resident physician is nearby and features in the future it will be possible to design and calls him up to show him this interesting clinical case develop smart environments characterized by an intelli kofod petersen et al also describe using ami gence similar to hal more specifically these technology to support healthcare workers cooperating in environments will be able to not only instantly recognize patient diagnosis and treatment by using context informa who he was interacting with but also whether he could lip tion goal recognition and case based reasoning geriatric read judge aesthetics of visual sketches recognize emo ami gerami is another ami hospital project which helps tions subtly expressed by people and respond to these doctors and nurses to monitor patients and to better man emotions in an adaptive personalized way consequently age theirs tasks for example it keeps track of the they will increase the human health by providing therapy patients locations using rfid technology and generates and health support without intervention of medical staff alarms if needed it also allocates tasks to nurses based on various contextual information such as the availability of nearby nurses and their profile information b design and human factors the next generation of ami systems will try to improve the quality of human life by means of increasingly inno v ongoing challenges and future research directions proceedings of the ieee vol no december vative applications in the healthcare domain or in an implicit way by assuring high comfort together with an intelligent usage of scarce resources this benefit will be a artificial intelligence since the birth of modern computing around the scientists and doctors alike have always been cap tivated by the potential that ai methodologies might have in medicine and healthcare applications indeed conventional ai methods such as machine learning expert systems knowledge representation techniques have been strongly exploited for designing and implementing medical applications belonging to the following areas diagnosis prognosis medical training and so on expert or knowledge based systems are the most common type of ai application in medicine they contain medical knowl edge usually about a very specifically defined task and are able to reason with data from individual patients to come up with reasoned conclusions by using typically a set of rules based on some logic inference when there is not enough knowledge for designing medical expert systems machine learning approaches can be used for analyzing a collection of clinical cases producing a systematic descrip tion of that clinical knowledge that characterizes the clinical conditions of a give patient or a disease moreover mainly achieved by designing intelligent environments completely pervaded by different kind of sensors whose wireless interconnection could result in a larger human exposure to rf electromagnetic fields although the elec tromagnetic fields from wireless sensors are weak a long time exposition can raise concerns about possible health hazards indeed some recent studies indicated a direct link between exposure to radiation from wireless devices and an increased risk of acoustic neuronal a cancer of the nerve connecting the ear to the brain for this reason the next generation of ami systems must be designed by taking into account additional factors such as sensor positioning sensor mobility sensor radiation and so on by following these design guidelines future ami frameworks will be fully suitable for enhancing human health without provid ing dangerous side effects this challenge can only be tackled with the joint efforts of computer scientists archi tects doctors physicists and telecommunication engi neers moreover government regulatory authorities need to direct policy decisions intelligently to maintain a useful and safe future of humans techniques for knowledge representation e g ontologies have been used to formally gather medical knowledge and implement tutoring systems for young doctors or nurses these are only some samples of ai applied to healthcare domain but in general all of ai healthcare systems have to deal with manipulations and transformation of data and knowledge in this scenario approaches based on perva sive or ubiquitous hardware will be very useful for enhancing the current state of the art of ai in healthcare 247 indeed ami features could allow systems designers to develop complex software architectures capa ble of analyzing knowledge spread everywhere in sur rounding environments and as a consequence learning c security and infrastructure the wealth of data that is collected in ami systems can be beneficial in many aspects however it also opens up many security issues particularly for healthcare systems privacy and security are already very complex issues and the addition of a large number of sensors and devices will result in additional challenges wright et al discuss several ami security issues and call for the need to develop safeguards they describe various scenarios where ami might result in serious security breaches for example burglars might obtain various details about an older adult living alone and his habits for leaving home at a particular distributed expert systems classifying diseases or other health disorders depending upon the environmental hal computer the superlative star of the classic kubrick and clarke film a space odyssey acampora et al a survey on ambient intelligence in healthcare hour and use such information for breaking into the smart acceptance of ami system in general facilitate human home system potentially even endangering the elderly contact be oriented toward community and cultural life lack of interpretability and overly restricted access enhancement inspire trust and confidence and be might also pose some issues for example out of date controllable by ordinary peoplevthere should be an off health monitoring devices might result in fatal misdiag switch within reach nosis by paramedics in an accident or they might not allow overreliance on ami systems might have its own dangers the paramedics to access such information in case of in for the individuals with health needs and might result in the compatible devices used in different geographical regions early loss of the ability and confidence to manage their life bohn et al as well as van heerde discuss care must be taken to ensure that ami technology is not the potential security dangers of ami due to the ability of limited to the affluent individuals because less privileged such systems to collects large amounts of data about each individuals can also benefit from the benefits of ami individual they also warn against the future possibility of fear of decreased communication and patient isolation an invisible and comprehensive surveillance network is another ethical issue that many researchers have brought friedewald et al note that the unidirectional flow of up identifying where the problem lies in a misdiagnosis will information in ami systems might result in an asymmetric become more and more difficult in such a complex system data distribution for example health providers have and will result in many ethical and legal discussions access to every detail of a daily life of an individual but the individuals might not have enough information to decide and choose among their providers or even if they have vi conclusion access they might lack the sophisticated means necessary ami paradigm represents the vision of the next wave of to process such complex web of information computing by relying on various computing and network sensors which are used in ami systems might also be a ing techniques ami promises the successful acquisition source of concern for example rfid tags have already and interpretation of contextual information and offers an been subject of many security discussions the fact that an unobtrusive and intuitive user interface thanks to these object with an rfid tag can be uniquely identified and features ami systems have the potential to enhance our tracked back to its user might bring up many privacy issues everyday life in many different aspects and in particular also many ami sensors and devices will rely on wireless one of the areas which promises the widespread use of protocols for communication and as wireless communica this innovative paradigm is the healthcare domain tions are easier to intercept all communications should be in this survey we explored the application of ami in encrypted and made secure personal monitoring devices healthcare from various perspectives we discussed the use should authenticate the identity of their users using of ami in healthcare based on individuals medical unobtrusive biometrics or possibly key physiological signs conditions such as physical or mental disabilities chronic to avoid any data tampering owner aware devices disease or rehabilitation situations from a different in general kotz et al identify three particular perspective we discussed current technology and infra features of remote home healthcare that have implications structure such as smart environments assistive robots for privacy more medical data will be collected about a wearable sensors and smart fabrics more importantly we patient over extended periods broader health data will provided a high level description of various ami method be collected about the patient physiological data infor ologies in the healthcare domain such as automated mation about an individual lifestyle activities etc and decision making planning techniques activity recogni a broader range of applications will use the collected tion and other numerous techniques data to allow for provision of healthcare in such a system however we are aware that the goals set up for ami in and at the same time to maintain privacy a privacy by healthcare are not easily reachable and there are still design pbd approach can be considered pbd many challenges to be faced and consequently this re advocates full privacy provisions during design in a search field is getting more and more impetus researchers proactive manner in other words once the user privacy with different backgrounds are enhancing the current state requirements have been determined then the design of of the art of ami in healthcare by addressing fundamental the sensor system itself can be completed problems related to human factors intelligence design and implementation and security social and ethical issues as d social and ethical issues a result we are confident that this synergic approach will the initial european commission istag report materialize the complete vision of ami and its full identified several characteristics necessary for social application in healthcare and human wellbeing the collective intelligent behaviour of insect or animal groups in nature such as flocks of birds colonies of ants schools of fish swarms of bees and termites have attracted the attention of researchers the aggregate behaviour of insects or animals is called swarm behaviour entomologists have studied this collective behaviour to model biological swarms and engineers applied these models as a framework for solving complex real world problems this branch of artificial intelligence which deals with the collective behaviour of swarms through complex interaction of individuals without supervision is referred to as swarm intelligence bonabeau defined swarm intelligence as any attempt to design algorithms or distributed problem solving devices inspired by the collective behaviour of the social insect colonies and other animal societies swarm intel ligence has some advantages such as scalability fault tolerance adaptation speed modularity autonomy and parallelism the key components of swarm intelligence are self organization and division of labour in a self organising system each of the covered units may respond to local stimuli individually and act together to accomplish a global task via division of labour without a centralized supervision the entire system can adapt to internal and external changes efficiently bonabeau et al have characterized four basic properties on which self organization relies positive feedback negative feedback fluc tuations and multiple interactions positive feedback means that an individual recruits other individuals by some direc tive such as dancing of bees in order to lead some other bees onto a specific food source site negative feedback avoids all individuals accumulating on the same task by counterbalancing the attraction negatively such as abandoning the exhausted food source fluctuations are random behaviours of individuals in order to explore new states such as random flights of scouts in a bee swarm multiple interactions are the basis of the tasks to be carried out by certain rules bee swarms exhibit many intelligent behaviours in their tasks such as nest site building marriage foraging navigation and task selection there is an efficient task selection mechanism in a bee swarm that can be adaptively changed by the state of the hive and the environment foraging is another crucial task for bees forage selection depends on recruitment for and abandonment of food sources there are three types of bees associated with the foraging task with respect to their selection mechanisms employed bees fly onto the sources which they are exploiting onlooker bees choose the sources by watching corresponding author tel 352 fax 352 e mail addresses bahriye erciyes edu tr b akay karaboga erciyes edu tr d karaboga see front matter elsevier inc all rights reserved doi j ins contents lists available at sciencedirect information sciences journal homepage www elsevier com locate ins b akay d karaboga information sciences the dances performed by employed bees and scouts choose sources randomly by means of some internal motivation or pos sible external clue the exchange of information among bees is the most important occurrence in the formation of the col lective knowledge the most important part of the hive in terms of exchanging information is the dancing area communication among bees related to the quality of food sources takes place in the dancing area various dances are per formed on the dancing area such as waggle round tremble depending on the distance of the discovered source although the ant colony optimization aco algorithm simulating the behaviour of ant colonies and particle swarm optimization pso algorithm mimicking flocks of birds are the most popular intelligence based optimization algorithms there are some algorithms presented in the literature based on the foraging behaviour of a bee swarm tereshko developed a model of foraging behaviour of a honeybee colony based on reaction diffusion equations lucic and teodorovic developed the bee system based on the foraging behaviour of a bee colony for solving difficult combinatorial optimization problems another algorithm is beeadhoc proposed by wedde and farooq which is a routing algorithm for energy efficient routing in mobile ad hoc networks beeadhoc is also inspired by the foraging principles of honey bees yang presented a virtual bee algorithm vba to solve numerical optimization problems karaboga introduced a bee swarm algorithm called the artificial bee colony abc algorithm that simulates the foraging behaviour of bees for multimodal and multi dimensional numerical optimization problems pham et al also described the bees algorithm which mimics the foraging behaviour of honey bees ghosh and marshall proposed a model of learning and collective decision making in honey bees engaged in foraging they employed their model for a swarm of robots drias and yahi introduced a meta heuristic named bees swarm optimization bso based on the behaviour of real bees for solving maximum weight satisfiability problems chong et al described a bee colony optimization algorithm based on the foraging bahaviour and the waggle dance the algorithm was applied to job shop scheduling baig and rashid presented honey bee foraging hbf algorithm which simulates the foraging behaviour of the honey bees and performs swarm based collective foraging in promising neighborhoods with individual scouting searches in other areas quijano and passino introduced a foraging model of honey bees for solving a class of optimal resource allocation problems lu and zhou developed bee collecting pollen algorithm bcpa by simulating the honeybees pollen collecting behaviour for solving the travelling salesman problem ko et al proposed a self adaptive grid computing protocol called honeyadapt which is based on adaptive bee foraging behaviour in nature in this work some modifications to the standard abc algorithm are introduced and the performance of the modified abc algorithm is investigated for real parameter optimization on both basic and composite functions presented at the congress of evolutionary computation effects of the perturbation rate that controls the frequency of param eter change the scaling factor step size that determines the magnitude of change in parameters while producing a neigh boring solution and the limit parameter on the performance of the abc algorithm are investigated on real parameter optimization the rest of the paper is organized as follows in section the abc algorithm is described in section the works related to the abc algorithm are summarized and then the modifications to the basic abc algorithm are introduced in section in section experiments are presented and the results are discussed and in section a thorough comparative analysis includ ing the algorithms considered in this study is presented artificial bee colony algorithm in a real bee colony some tasks are performed by specialized individuals these specialized bees try to maximize the nec tar amount stored in the hive using efficient division of labour and self organization the artificial bee colony abc algo rithm proposed by karaboga in for real parameter optimization is a recently introduced optimization algorithm which simulates the foraging behaviour of a bee colony the minimal model of swarm intelligent forage selection in a honey bee colony which the abc algorithm simulates consists of three kinds of bees employed bees onlooker bees and scout bees half of the colony consists of employed bees and the other half includes onlooker bees employed bees are responsible for exploiting the nectar sources explored before and giving information to the waiting bees onlooker bees in the hive about the quality of the food source sites which they are exploiting onlooker bees wait in the hive and decide on a food source to exploit based on the information shared by the employed bees scouts either randomly search the envi ronment in order to find a new food source depending on an internal motivation or based on possible external clues this emergent intelligent behaviour in foraging bees can be summarized as follows at the initial phase of the foraging process the bees start to explore the environment randomly in order to find a food source after finding a food source the bee becomes an employed forager and starts to exploit the discovered source the employed bee returns to the hive with the nectar and unloads the nectar after unloading the nectar she can go back to her discovered source site directly or she can share information about her source site by performing a dance on the dance area if her source is exhausted she becomes a scout and starts to randomly search for a new source onlooker bees waiting in the hive watch the dances advertising the profitable sources and choose a source site depending on the frequency of a dance proportional to the quality of the source b akay d karaboga information sciences in the abc algorithm proposed by karaboga the position of a food source represents a possible solution to the optimiza tion problem and the nectar amount of a food source corresponds to the profitability fitness of the associated solution each food source is exploited by only one employed bee in other words the number of employed bees is equal to the num ber of food sources existing around the hive number of solutions in the population the employed bee whose food source has been abandoned becomes a scout using the analogy between emergent intelligence in foraging of bees and the abc algorithm the units of the basic abc algorithm can be explained as follows producing initial food source sites if the search space is considered to be the environment of the hive that contains the food source sites the algorithm starts with randomly producing food source sites that correspond to the solutions in the search space initial food sources are pro duced randomly within the range of the boundaries of the parameters x ij xmin j þ j à xmin j þ where i sn j d sn is the number of food sources and d is the number of optimization parameters in addition counters which store the numbers of trials of solutions are reset to in this phase after initialization the population of the food sources solutions is subjected to repeat cycles of the search processes of the employed bees the onlooker bees and the scout bees termination criteria for the abc algorithm might be reaching a maximum cycle number mcn or meeting an error tolerance sending employed bees to the food source sites as mentioned earlier each employed bee is associated with only one food source site hence the number of food source sites is equal to the number of employed bees an employed bee produces a modification on the position of the food source solution in her memory depending on local information visual information and finds a neighboring food source and then evaluates its quality in abc finding a neighboring food source is defined by t ij x ij þ ij ðx ij à x kj þ within the neigbourhood of every food source site represented by x i a food source t i is determined by changing one param eter of x i in eq j is a random integer in the range d and k sn is a randomly chosen index that has to be different from i ij is a uniformly distributed real random number in the range as can be seen from eq as the difference between the parameters of the x i j and x k j decreases the perturbation on the position x i j decreases thus as the search approaches to the optimal solution in the search space the step length is adap tively reduced if a parameter value produced by this operation exceeds its predetermined boundaries the parameter can be set to an acceptable value in this work the value of the parameter exceeding its boundary is set to its boundaries if x i xmax i then x i xmax i after if x i producingt xmin i then x i i within xmin i the boundaries a fitness value for a minimization problem can be assigned to the solutiont i by fitness i where f i þ f i þ þ absðf i if f i p þ if f i is the cost value of the solution t i for maximization problems the cost function can be directly used as a fitness function senting a the greedy selection is applied between x i and nectar amount of the food sources at x i and t i t i then if the better one is selected depending on fitness values repre the source at t i is superior to that of x i in terms of profitability the employed bee memorizes the new position and forgets the old one otherwise the previous position is kept in memory if x i cannot be improved its counter holding the number of trials is incremented by otherwise the counter is reset to calculating probability values involved in probabilistic selection after all employed bees complete their searches they share their information related to the nectar amounts and the posi tions of their sources with the onlooker bees on the dance area this is the multiple interaction feature of the artificial bees of abc an onlooker bee evaluates the nectar information taken from all employed bees and chooses a food source site with a probability related to its nectar amount this probabilistic selection depends on the fitness values of the solutions in the pop ulation a fitness based selection scheme might be a roulette wheel ranking based stochastic universal sampling tourna ment selection or another selection scheme in basic abc roulette wheel selection scheme in which each slice is proportional in size to the fitness value is employed p i p sn fitness fitness i i b akay d karaboga information sciences in this probabilistic selection scheme as the nectar amount of food sources the fitness of solutions increases the number of onlookers visiting them increases too this is the positive feedback feature of abc food source site selection by onlookers based on the information provided by employed bees in the abc algorithm a random real number within the range is generated for each source if the probability value p i in eq associated with that source is greater than this random number then the onlooker bee produces a modification on initial food positions calculate nectar amounts determine neighbors of the chosen food sources by the employed bees calculate nectar amounts calculate nectar amount selection determine a neighbor of the chosen food source by the onlooker no all onlookers distributed yes memorize the position of best food source find the abandoned food sources produce new positions for the abandoned food sources are termination criteria satisfied yes final food positions no fig flowchart of the artificial bee colony algorithm b akay d karaboga information sciences the position of this food source site by using eq as in the case of the employed bee after the source is evaluated greedy selection is applied and the onlooker bee either memorizes the new position by forgetting the old one or keeps the old one if solution x i cannot be improved its counter holding trials is incremented by otherwise the counter is reset to this pro cess is repeated until all onlookers are distributed onto food source sites abandonment criteria limit and scout production in a cycle after all employed bees and onlooker bees complete their searches the algorithm checks to see if there is any exhausted source to be abandoned in order to decide if a source is to be abandoned the counters which have been updated during search are used if the value of the counter is greater than the control parameter of the abc algorithm known as the limit then the source associated with this counter is assumed to be exhausted and is abandoned the food source aban doned by its bee is replaced with a new food source discovered by the scout which represents the negative feedback mech anism and fluctuation property in the self organization of abc this is simulated by producing a site position randomly and replacing it with the abandoned one assume that the abandoned source is x i then the scout randomly discovers a new food source to be replaced with x i this operation can be defined as in in basic abc it is assumed that only one source can be exhausted in each cycle and only one employed bee can be a scout if more than one counter exceeds the limit value one of the maximum ones might be chosen programmatically all these units and interactions between them are shown as a flowchart on fig previous work on the abc algorithm the abc algorithm was first applied to numerical optimization performance of the abc algorithm was compared to those of the genetic algorithm ga particle swarm inspired evolutionary algorithm ps ea and to those of differ ential evolution de pso and evolutionary algorithm ea on a limited number of basic test problems the effect of region scaling on algorithms including abc de and pso algorithms was studied in the abc algorithm was extended for constrained optimization problems in and was applied to train neural networks to medical pattern classification and clustering problems to solve tsp problems fenglei et al also studied the control mechanism of local opti mal solution in order to improve the global search ability of the algorithm singh used the artificial bee colony algo rithm for the leaf constrained minimum spanning tree lcmst problem called abc lcmst and compared the approach against ga aco and tabu search ts in it is reported that abc lcmst outperforms the other approaches in terms of the best and average solution qualities and computational time rao et al applied the abc algorithm to network recon figuration problem in a radial distribution system in order to minimize the real power loss improve voltage profile and bal ance feeder load subject to the radial network structure in which all loads must be energized the results obtained by the abc algorithm were better than the other methods compared in the study in terms of quality of the solution and computa tion efficiency bendes and ozkan used the abc algorithm for solving direct linear transformation dlt which is one of the camera calibration methods by establishing a relation between object coordinate and image plane linearly results produced by the abc algorithm were compared against those of the de algorithm karaboga used the abc algorithm in the signal processing area for designing digital iir filters qingxian and haijun proposed a modification in the initiali zation scheme by making the initial group symmetrical and the boltzmann selection mechanism was employed instead of roulette wheel selection for improving the convergence ability of the abc algorithm hemamalini and simon proposed an economic load dispatch with valve point effect by using the abc algorithm quan and shi integrated a search iter ation operator based on the fixed point theorem of contractive mapping in banach spaces with the abc algorithm in order to improve convergence rate pawar et al applied the abc algorithm to some problems in mechanical engineering includ ing multi objective optimization of electro chemical machining process parameters optimization of process parameters of the abrasive flow machining process and the milling process in order to maximize the exploitation capacity of the onlooker stage tsai et al introduced the newtonian law of universal gravitation in the onlooker phase of the basic abc algo rithm in which onlookers are selected based on a roulette wheel interactive abc iabc baykasoglu et al incorporated the abc algorithm with shift neighborhood searches and greedy randomized adaptive search heuristic and applied it to the generalized assignment problem modified artificial bee colony algorithm the basic version of the artificial bee colony algorithm has only one control parameter limit apart from the common control parameters of the population based algorithms such as population size or colony size sn and maximum generation number or maximum cycle number mcn the basic version of the abc algorithm is very efficient for multimodal and multi dimensional basic functions however the convergence rate of the algorithm is poorer when working with constrained prob lems composite functions and some non separable functions this issue arises from the stochastic variation process in which new solutions are produced from the parent solutions in this process some search parameters such as perturbation frequency or magnitude of the perturbation are important since b akay d karaboga information sciences they affect the distribution of new solutions in order to improve the convergence rate some modifications have been intro duced in the perturbation process of the basic abc algorithm frequency of the perturbation one of the modifications in the abc algorithm is controlling the frequency of perturbation in the basic version of abc this frequency is fixed in basic abc while producing a new solution t i changing only one parameter of the parent solution x i results in a slow convergence rate in order to overcome this issue the abc algorithm is modified by introducing a control parameter modification rate mr by means of this modification for each parameter x ij an uniformly distributed random table basic unimodal and multimodal test functions employed in the first part of the experiments f f x search range initialization range formulae sphere fð d d f ðxþ p d i rosenbrock fð d 048 048 d f ðxþ p þ ðx i ackley fð à i þ à d 768 d f exp q ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi d p d à exp d þ þ e griewank fð i p d p x i þ d d f ðxþ p d þ weierstrass fð p d i à q d cos d d f ðxþ p k max h p x i ffi i i àd ak cos p b k ðx i þ p k max k a b k max rastrigin fð d d f ðxþ p d 2x2 i à p x i þ þ noncontinuous rastrigin fð d d f ðxþ p d i à p y i þ þ y i schwefel fð x i i j þ jx i d d f ðxþ 9829xd jx i j à p p d à x i sin p ffiffiffiffiffiffiffi jx i j table mean of best results obtained through independent runs on basic functions d max eval mm multimodal um unimodal um um mm mm sphere rosenbrock ackley griewank pso w 56eà050 pso cf 21eà104 11eà002 pso w local 17eà035 04eà015 79eà002 pso cf local 60eà079 56e upso 56eà117 04eà001 fdr 88eà090 63e fips 56eà030 cpso h 00eà044 49eà014 07eà002 clpso 16eà028 46e 70e 55eà014 81eà003 abc mr sf limit basic 11eà017 016 95eà017 31eà009 26eà002 sf mr limit asf 48eà012 70eà006 14eà001 limit mr sf 25eà001 99eà017 07eà016 32eà002 99eà017 78eà016 46eà002 59eà002 is number r ij modified as in the eq t ij x x ij ij þ ij ðx ij à x kj þ if r ij mr otherwise b akay d karaboga information sciences table mean of best results obtained through independent runs on basic functions d max eval mm multimodal um unimodal mm mm mm mm weierstrass ncrastrigin schwefel pso w 04eà003 pso cf 17eà001 25e 20e pso w local 31eà006 26e pso cf local 16eà002 95e upso 17e 85e fdr 20eà003 51e fips 40eà003 10e cpso h 67eà015 10eà001 clpso abc mr sf limit basic 61eà005 20eà016 96eà017 15eà007 54eà013 58eà 34eà003 sf mr limit 95eà001 00eà001 00eà001 asf 06eà008 85eà001 80eà001 000 000 limit mr sf 07eà006 16eà007 04eà005 000 000 000 88eà006 32eà002 22eà004 15eà001 000 000 000 000 002 e e mr sphere rosenbrock a effect of mr on unimodal func tions sphere rosenbrock r o r r e c effect of sf on unimodal functions ackley griewank weierstrass rastrigin noncontinuous rastrigin schwefel d effect of sf on multimodal func tions r o r r ackley griewank r o r r weierstrass rastrigin noncontinuous rastrigin schwefel mr b effect of mr on multimodal func tions mr e e e mr sphere rosenbrock r o r r r o r r r o r r e effect of limit on unimodal func tions ackley griewank weierstrass rastrigin noncontinuous rastrigin schwefel limit f effect of limit on multimodal functions fig mean of best function values for different control parameter settings limit is produced and if the random number is less than mr then the parameter x ij b akay d karaboga information sciences sphere rosenbrock r o r r e cycle cycle a convergence graphs of unimodal functions r o r r e ackley griewank weierstrass rastrigin noncontinuous rastrigin schwefel 1000 1000 1500 3000 b convergence graphs of multimodal functions fig convergence graphs of the abc algorithm on basic functions table mean and standard deviations of error values obtained from the abc algorithm with different colony sizes cs for basic functions with different dimensions d cs functions d sphere 03eà017 13eà014 15eà005 rosenbrock 000 000 002 ackley 86eà001 000 griewank 34eà002 91eà002 000 weierstrass 000 000 000 000 000 rastrigin 29eà010 000 000 000 ncrastrigin 80eà001 000 000 000 schwefel 01eà002 002 002 sphere 20eà017 79eà006 rosenbrock 000 000 002 001 ackley 69eà016 59eà017 04eà010 25eà003 griewank 78eà003 14eà003 55eà003 weierstrass 000 000 70eà009 42eà003 rastrigin 99eà017 03eà001 001 000 ncrastrigin 40eà017 79eà001 001 000 schwefel 43eà008 002 002 003 002 sphere 71eà018 59eà017 56eà008 rosenbrock 71eà001 001 001 002 001 ackley 30eà016 96eà017 82eà012 98eà004 griewank 02eà003 22eà011 weierstrass 000 000 70eà013 71eà003 rastrigin 22eà018 19eà012 000 000 ncrastrigin 79eà018 06eà011 001 000 schwefel 27eà004 000 001 001 003 002 where k sn is randomly chosen index that has to be different from i and mr is the modification rate which takes value between and a lower value of mr may cause solutions to improve slowly while a higher one may cause too much diversity in a solution and hence in the population magnitude of the perturbation another modification is related to the ratio of the variance operator of the basic abc algorithm in basic abc a random perturbation which avoids getting stuck at local minima is added to the current solution in order to produce a new solution this random perturbation is the difference of the solutions x i and x k weighted by a random real number ij the value of ij varies within the range in the basic abc while it varies within the range àsf sf in the modified abc algorithm hence magnitude of the perturbation is controlled by a control parameter called the scaling factor sf this value is set be b akay d karaboga information sciences table time complexity of the abc algorithm on rosenbrock function d dimension t t b t complexity ðð t b à t þ t þ d d d table values for the control parameters of the abc algorithm used for hybrid functions d dimension of the problem colony size d max cycle number 000 limit mr sf table time complexity of the abc algorithm on composite function d dimension t t t b complexity ðð t b à t þ t þ d d d table d colony size limit cycle 000 max fes 000 mr mte mean of term err of runs ste std of term err of runs fes prob best median worst mean std best 82eà02 58eà01 median 72eà01 worst mean std 02eà01 best median worst mean std mte 65eà09 ste 40eà09 fore running the algorithm a lower value of sf allows the search to fine tuning the process in small steps while causing slow convergence a larger value of sf speeds up the search but it reduces the exploitation capability of the perturbation process for some classes of problems lower values of sf are appropriate while for some higher ones are convenient for this reason the modified algorithm may change sf automatically during the search called adaptive sf asf automatic tuning of sf is conducted by using rechenberg mutation rule which states that the ratio of successful mutations to all mutations should be changing step size according to rule in every m number of cycles is performed as in the eq b akay d karaboga information sciences table d colony size limit cycle 000 max fes 000 mr mte mean of term err of runs ste std of term err of runs fes prob best median worst mean std 80eà01 best median worst mean std best median worst mean std mte 90eà01 ste 92eà01 table d colony size limit cycle 000 max fes 000 mr mte mean of term err of runs ste std of term err of runs fes prob best median worst mean std best median worst mean std best median worst mean std 95eà03 mte ste 95eà03 sfðtþ ã if uðmþ sfðt þ sfðtþ if uðmþ sfðtþ if uðmþ if the algorithm cannot improve the solution with respect to rechenberg rule that is the ratio of successful mutations to all mutations u m is less than sf is decreased if u m is greater than then sf is increased in order to speed up the search b akay d karaboga information sciences table d colony size limit cycle 000 max fes 000 mr mte mean of term err of runs ste std of term err of runs fes prob best median worst mean std best median 33eà01 worst mean std 64e best 11eà03 median 03eà02 61e worst 56eà01 mean 92eà01 std best 69eà01 median worst mean std mte 06eà09 ste 80eà10 53e table d colony size limit cycle 000 max fes 000 mr mte mean of term err of runs ste std of term err of runs fes prob best median worst mean std best median worst mean std 61e 05 best 05 median 05 05 worst 05 mean 05 std 05 best median 53e 64e 05 worst 05 mean std mte ste b akay d karaboga information sciences table d colony size limit cycle 000 max fes 000 mr mte mean of term err of runs ste std of term err of runs fes prob best 21e 21e median worst mean std best median worst 08e mean std 92eà02 05 best median worst 28e mean std 05eà02 05 best median worst mean std mte 42e ste table d colony size limit cycle 000 max fes 000 mr mte mean of term err of runs ste std of term err of runs fes prob best 21e 13e median 13e 05 worst 05 53e mean 13e std 05 05 08e best 05 05 79e median 05 94e 05 13e worst 95e 05 13e mean 05 std 05 19e 84e 05 best 00e 00e 19e 13e median 00e 00e 04 45e 00e 00e 63e 42e 04 28e 04 worst 00e 00e 54e 84e 04 04 mean 00e 00e 13e 04 05eà01 std 00e 00e 05 53e 00e 05 best 00e 00e 05 49e 18eà01 00e 00e 67e 81eà01 11e median 00e 00e 04 11e 11e 00e 00e 04 worst 00e 00e 04 mean 00e 00e 13e 03e 04 10eà01 11e std 00e 00e 04 98e 13e mte 28eà09 13e 03e 04 10eà01 11e ste 05eà10 19eà10 04 98e 13e b akay d karaboga information sciences table d colony size limit cycle 000 max fes 000 mr mte mean of term err of runs ste std of term err of runs fes prob 12 00e best 08e 11e 03e median 11e 08e 03e 19th 06 05e 27e worst 06 19e 08e mean 11e 06 91e std 79e 05 00e 04 best 06 68e 53e 61e 06 00e 36e median 61e 68e 06 83e 19th 68e 67e 06 worst 98e 13e 94e 06 61e 19e mean 62e 65e 06 35e 37e 15e 17e std 60e 05 23e 49e 00e 05 best 36e 49e 95e 05 21e 00e 98e 11e 05 median 23e 06 81e 00e 19th 06 91e 05e 54e worst 98e 21e 06 28e 36e mean 21e 06 81e 97e std 60e 05 91e 00e 05 best 36e 62e 05 91e 00e 68e 81e 7th 51e 49e 99e 05 42e 00e median 61e 57e 03e 91e 05 54e 00e 19th 64e 68e 05 00e 57e 17e worst 05e 06 00e 86e mean 03e 88e 05 52e 08e std 91e 44e 77e 04 98e mte 03e 88e 05 50e 52e 39e 08e ste 91e 44e 77e 04 98e table d colony size limit cycle 000 max fes 000 mr mte mean of term err of runs ste std of term err of runs fes prob 00e best 24e 29e 19e 36e 10e 7th 29e 37e 23e 39e 39e 13e median 33e 36e 36e 26e 24e 19th 37e 42e 43e 44e 44e worst 42e 35e 48e 21e mean 36e 36e 27e 03 03 std 75e 39e 05e 78e 00e 04 best 03 04e 03 03e 03 08e 00e 03 7th 03 05e 03 03 03e 03 06e 03 median 03 05e 03 07e 03 04e 03 54e 03 19th 03 07e 03 09e 03 05e 03 65e 11e 03 03e worst 03 10e 03 10e 03 06e 03 84e 03 05e mean 08e 03 06e 03 07e 03 04e 03 52e 09e 03 std 54e 69e 92eà01 16eà01 00e 05 best 88e 77e 76e 00e 77e 00e 7th 00e 86e 01e 01e median 91e 81e 89e 98e 01e 01e 19th 93e 84e 01e 03 07e 01e 01e worst 00e 03 88e 89e 01e 03 06e 12e 01e mean 82e 82e 35e 97e 01e 01e std 00e 07e 95e 01e 19eà02 00e 05 best 67e 00e 58e 00e 00e 7th 84e 69e 69e 00e 74e 01e 01e median 86e 69e 76e 70e 01e 01e 19th 70e 01e 03 78e 74e 01e 01e worst 90e 72e 01e 03 84e 86e 01e mean 70e 70e 75e 01e 01e std 12e 83e 12e 54eà01 mte 70e 70e 75e 01e 01e ste 12e 38e 22e 83e 12e 54eà01 b akay d karaboga information sciences e e e b convergence of functions r o r r o r r o r r r r 12 function evaluations x function evaluations x function evaluations x a convergence of functions c convergence of functions r o r r e r o r r e function evaluations x d convergence of functions function evaluations x e convergence of functions fig logarithmic scaled best function error of median runs of runs for dimension of functions in the set numbers in the legend correspond to the function number table results of state of art algorithms for hybrid functions dimension colony size cycle for abc pso rdl recombination with dynamic linkage discovery in pso dms pso dynamic multi swarm particle swarm optimizer with local search spc pnx de differential evolution sade self adaptive differential evolution restart cma es restart covariance matrix adaptation evolution strategy with increasing population size f1 f5 pso rdl mean 50eà14 77eà13 68eà02 47eà07 09eà07 std 88eà14 03eà13 07eà07 22eà07 dms pso mean 00e std 00e spc pnx mean 90eà09 63eà09 08e 05 std 39eà10 30eà10 72e 04 de mean 00e 00e 94eà06 09eà15 00e std 00e 00e 63eà06 15eà14 00e sade mean 00e std 00e restart cma es mean 20eà09 70eà09 60eà09 02eà09 58eà09 std 94eà09 56eà09 93eà09 71eà09 17eà09 abc mean 89eà17 81eà17 50e 03 82e std 23eà18 89eà18 68e 11e abc sf mean 85eà17 04eà16 37e 03 32e std 36eà17 90eà17 54e 09e abc sf mean 16eà16 14eà16 90e 26e std 91eà17 60eà17 62e 30e abc sf mean 52eà16 83eà16 88e 07e std 43eà17 43eà17 79e 12e abc mr mean 64eà17 62eà17 52e 03 std 91eà18 16eà18 67e abc mr mean 80eà17 10eà16 57e 03 8 std 84eà17 66eà17 11e 03 abc mr mean 75eà17 58eà17 72e 03 std 79eà18 75eà18 29e 03 abc mr mean 58e 03 48eà17 std 8 91eà18 77eà18 60e 03 abc mr mean 80eà17 89eà17 22e 03 66e std 46eà18 62eà18 03 61e abc asf mr mean 94eà17 76eà17 23eà04 41eà12 12eà04 std 29eà18 8 58eà18 23eà04 32eà11 31eà04 134 b akay d karaboga information sciences the pseudo code of the abc algorithm is given below initialize the population of solutions x i j i sn j d trial i trial i is the non improvement number of the solution x i used for abandonment evaluate the population cycle repeat produce a new food source population for employed bee for i to sn do produce a new food source t i for the employed bee of the food source x i by using in the case of modified abc algorithm by using and evaluate its quality 8 apply a greedy selection process between t i and x i and select the better one if solution x i does not improve trial i trial i otherwise trial i end for calculate the probability values p i by for the solutions using fitness values produce a new food source population for onlookers 12 t i repeat if random p i produce a new then t ij food source by 2 in the case of the modified abc algorithm by using for the onlooker bee apply a greedy selection process between t i and x i and select the better one if solution x i does not improve trial i trial i otherwise trial i t t end if until t sn determine scout if max trial i limit then replace x i with a new randomly produced solution by end if memorize the best solution achieved so far cycle cycle until cycle maximum cycle number experiments and discussion we have tested the abc algorithm and its variants in two groups of functions the first group consists of basic functions and the second one has one set of composite functions experiments on basic functions in the first part of the experiments in order to assess the performance of the abc algorithm we considered basic functions used in and given in table there are two groups of functions in the table the first group consists of unimodal functions sphere and rosenbrock sphere function is a continuous convex and unimodal function since the global optimum of the rosenbrock function is inside a long narrow parabolic shaped flat valley and the variables are dependent the gradients gen erally do not point towards the optimum and it is difficult to converge the global optimum the second group consists of mul timodal functions ackley griewank weierstrass rastrigin noncontinuous rastrigin and schwefel the ackley function has a surface with many local optima due to its exponential term the variables of griewank function have interdependence since the function has a product term the multimodality is removed by the increment in dimensionality n and the problem seems unimodal the weierstrass function is continuous everywhere but differentiable nowhere non continuous and con tinuous rastrigin functions are based on the sphere function with the addition of cosine modulation to produce many local minima the surface of schwefel function is composed of numerous peaks and valleys the second best minimum of the func tion is far from the global minimum and the global minimum is near the boundaries of the search domain in the experiments the population size was and the maximum number of function evaluations was 000 for dimensional problems all experiments were conducted times independently for each function while making this comprehensive study we examined the abc algorithm under different control parameter settings number of parameters changed in each cycle step size in the production of neighboring solutions and limit the results obtained by abc algorithm with different control parameters are compared against the results of pso variants presented in settings of the pso variants can be found in pso w pso with inertia factor pso cf pso with b akay d karaboga information sciences table results of state of art algorithms for hybrid functions dimension colony size cycle for abc pso rdl recombination with dynamic linkage discovery in pso dms pso dynamic multi swarm particle swarm optimizer with local search spc pnx de differential evolution sade self adaptive differential evolution restart cma es restart covariance matrix adaptation evolution strategy with increasing population size f6 f10 pso rdl mean 57eà01 2 00e 86e std 74e 2 8 17e 80e dms pso mean 89eà08 52eà02 2 00e 00e 62e std 19eà07 00e 8 55eà01 spc pnx mean 89e 8 2 10e 30e std 00e 2 27e 21e de mean 2 04e 9 55eà01 std 97eà01 38eà01 58eà02 9 73eà01 sade mean 20eà08 99eà02 2 00e 00e 97e std 93eà08 00e 69e restart cma es mean 87eà09 31eà09 2 00e 2 std 66eà09 2 02eà09 89eà03 34eà01 2 75eà01 abc sf mean 2 2 03e 2 22e std 9 32e abc sf mean 8 89e 2 2 03e 77e std 8 98e abc sf mean 9 48e 2 2 03e 21eà01 13e std 9 8 8 abc sf mean 37e 2 2 03e 28e 50e std 36e 88eà01 2 32e 65e abc mr mean 27e 2 04e 2 std 38e 9 56e 33e abc mr 8 mean 2 03e 2 35e std 62e 8 09e abc mr mean 95e 2 04e 2 std 00e 37e abc mr mean 2 76e 2 04e 29eà17 43e std 62e 2 2 2 61e abc mr 2 mean 2 51e 9 2 04e 69e std 61e 62e abc asf mr 9 mean 36e 52eà02 2 00e 65e std 2 49e 2 15e 2 93e constriction factor pso w local pso with inertia factor using a local neighborhood pso cf local pso with constric tion factor using local neighborhood upso unified pso combining local and global neighborhood topologies fdr pso fitness distance ratio based pso uses the neighbor with higher fitness fips fully informed particle swarm cpso h cooperative pso clpso comprehensive learning pso are the versions of pso which are investi gated in while making an experimental comparison if the difference of error rates is less than this difference is consid ered as insignificant in a practical sense the winner algorithm for each problem was given in boldface in the tables if it was significantly different comparison results obtained by pso variants and the abc algorithm with different con trol parameters set for sphere rosenbrock ackley and griewank functions are given in table 2 and for weierstrass rastrigin non continuous rastrigin and schwefel functions are given in table from the results for the rosenbrock function the abc algorithm that adjusts step size automatically produces the best result for the griewank and schwefel functions clpso performs the best of the other functions sphere ackley rastrigin ncrastrigin clpso and basic abc algorithms exhibit similar performance for all functions except the rosenbrock function initialization ranges for the algorithms were different from the search ranges initial ranges and search ranges for functions can be found in table the abc algorithm is robust against initialization conditions since the scout unit helps the search space to be explored efficiently the modification rate mr scaling factor sf and limit are control parameters of the abc algorithm which needed to be tuned for better performance we investigated the effect of the control parameters on the performance of the abc algo rithm by manually trying some different values before the run the results presented in tables 2 were demonstrated in the fig 2 a f from these figures generally the basic version of the abc algorithm in which mr just one parameter is changed and sf is is in the range is a better choice than other structures tried out for both unimodal and multimodal basic functions mr parameter is more important for the abc algorithm on hybrid functions which are rotated and shifted versions of function combinations for limit control parameter is more appropriate than other values for both unimodal and multimodal functions this value can change depending on the dimension of the problem convergence rates of the basic abc algorithm are shown in fig a for unimodal basic functions and in fig b for multimodal basic functions b akay d karaboga information sciences table results of state of art algorithms for hybrid functions dimension colony size cycle for abc pso rdl recombination with dynamic linkage discovery in pso dms pso dynamic multi swarm particle swarm optimizer with local search spc pnx de differential evolution sade self adaptive differential evolution restart cma es restart covariance matrix adaptation evolution strategy with increasing population size f11 f15 pso rdl mean 58e 8 87eà01 78e 2 71e std 42e 50e 06eà01 59e dms pso mean 62e 2 69eà01 2 36e std 84eà01 36e 38eà01 spc pnx mean 91e 2 60e 8 38eà01 05e 2 54e std 89e 2 69eà01 37eà01 51e de mean 8 47eà01 17e 9 45e 2 59e std 40e 67eà01 40eà01 83e sade mean 89e 2 2 std 62eà01 8 51eà07 11eà02 2 06eà01 11e restart cma es mean 9 34eà01 2 93e 01e 2 28e std 9 00eà01 42e 80e abc sf mean 46e 9 2 96eà02 std 84eà01 2 34eà01 abc sf 0 mean 67e 16e 87eà02 44e 2 25e std 24eà01 2 73eà01 9 89e 00 abc sf 0 mean 75e 00 10e 03eà01 36e 00 std 21eà01 8 68e 2 50e 00 abc sf 0 mean 09e 00 52e 64eà01 53e 00 17e std 9 60eà01 8 2 08e abc mr mean 8 38e 00 30e 77e 00 66e 00 01e std 64eà01 76e 02 66e abc mr 0 8 mean 88e 00 1 72e 03 1 48e 00 59e 00 2 87e 02 std 20eà01 35e 02 1 49e 01 abc mr 0 mean 79e 00 22e 02 52eà01 56e 00 1 99e 02 std 19eà01 2 10e 01 1 73e 01 abc mr 0 mean 58e 00 2 22e 02 2 05eà01 49e 00 1 08e 02 std 6 38eà01 1 80e 02 9 1 34e 01 abc mr 0 2 mean 26e 00 1 02 6 91eà02 35e 00 1 71e 01 std 84eà01 6 58e 01 2 15e 01 abc asf mr 0 9 mean 2 39e 00 1 71e 01 6 32eà01 24e 00 2 41e 02 std 1 04e 00 1 46e 01 1 74eà01 2 95eà01 8 64e 01 2 on the scalability and time complexity of abc it is generally difficult for optimization algorithms to solve high dimensional problems performance of an algorithm dete riorates as the problem dimension increases in order to cope with this problem the algorithm needs more information about the search space to direct the solutions to the optima increasing the population size or the number of evaluations exponentially might improve the performance but the performance in case of a high dimension problem is also related to the landscape of the problem moreover it is more difficult for the algorithm to solve a high dimensional problem when there is an epistatic interaction between the parameters many local optima misleadingness and hard structural properties of the search space this is defined as scalability problem in order to analyze the scalability of abc we investigated the performance of abc with respect to growing dimensions for this experiment functions given in table 1 with different dimensions including and were used the scalability test was repeated for the colony sizes of and mean and standard deviation of errors for the functions for each case were reported in table when the problem dimension was increased from to and then to the performance of the abc algorithm was influenced from this change as expected however as seen from the table an increase in problem dimen sion did not require exponential increment in population size or evaluation number therefore it can be stated that the abc algorithm is not very sensitive to increments in problem dimensions and has a good scalability the effect of scalability on the computational complexity of the abc algorithm was also analyzed for this purpose time complexity of the abc algorithm for rosenbrock function with different dimensions was calculated as described in rosenbrock function was chosen since it has interaction between its parameters in order to determine the time complexity after code execution time t 0 and execution time of rosenbrock function for 000 evaluations t 1 were calculated mean of five executions time of abc on rosenbrock function ity of the algorithm was determined by ðð t b 2 à t 1 þ t 0 þ through and given 000 in table evaluations table ð shows b t 2 þ was that computed t b 2 increases then by the less complex than a factor of dimension increment consequently it can be stated that the time complexity of the abc algorithm does not depend on the problem dimension excessively and it scales with o n by onlookers a new population is formed by searching the neighborhoods of the solutions chosen depending on their quality since the number of onlookers is equal to sn in each cycle searches are conducted by employed bees and on looker bees hence when the maximum cycle number mcn is reached totally searches are carried out so the search complexity of abc is proportional to b akay d karaboga information sciences 120 table results of state of art algorithms for hybrid functions dimension colony size cycle for abc pso rdl recombination with dynamic linkage discovery in pso dms pso dynamic multi swarm particle swarm optimizer with local search spc pnx de differential evolution sade self adaptive differential evolution restart cma es restart covariance matrix adaptation evolution strategy with increasing population size 1 f16 f20 pso rdl mean 2 02 2 22e 02 1 03 9 02 9 59e 02 std 1 74e 02 1 00e 02 1 19e 02 1 02 1 06e 02 dms pso mean 9 48e 01 1 10e 02 61e 02 02 8 22e 02 std 1 01e 01 35e 00 1 02 2 01e 02 59e 01 spc pnx mean 1 10e 02 1 19e 02 40e 02 80e 02 40e 02 std 9 87e 00 1 07e 01 2 25e 02 1 87e 02 2 29e 02 de mean 1 13e 02 1 15e 02 00e 02 02 60e 02 std 1 80e 01 2 01e 01 2 04e 02 2 18e 02 2 38e 02 sade mean 1 01e 02 1 14e 02 19e 02 05e 02 7 13e 02 std 6 17e 00 9 97e 00 2 09e 02 1 90e 02 2 01e 02 restart cma es mean 9 13e 01 1 23e 02 32e 02 2 26e 02 00e 02 std 49e 00 2 09e 01 1 12e 02 9 93e 01 0 00e 00 abc sf 1 mean 1 75e 02 1 02 46e 02 51e 02 38e 02 std 2 11e 01 2 25e 01 83e 01 09e 01 30e 01 abc sf 0 7 mean 1 77e 02 2 04e 02 86e 02 02 09e 02 std 1 64e 01 2 47e 01 9 60e 01 16e 01 2 66e 01 abc sf 0 mean 2 12e 02 2 33e 02 16e 02 31e 02 39e 02 std 2 49e 01 2 87e 01 40e 01 66e 01 2 97e 01 abc sf 0 3 mean 2 93e 02 3 06e 02 75e 02 78e 02 61e 02 std 40e 01 80e 01 1 32e 02 73e 01 80e 01 abc mr 1 mean 1 98e 02 2 02 50e 02 16e 02 04e 02 std 1 20e 01 1 75e 01 1 13e 02 1 19e 02 1 07e 02 abc mr 0 8 mean 2 25e 02 2 41e 02 62e 02 83e 02 62e 02 std 1 17e 01 2 31e 01 2 15e 01 2 74e 01 3 12e 01 abc mr 0 6 mean 2 02 2 18e 02 03e 02 5 03e 02 5 19e 02 std 1 79e 01 1 55e 01 60e 01 6 14e 01 5 19e 01 abc mr 0 mean 1 75e 02 1 90e 02 61e 02 5 20e 02 5 18e 02 std 1 80e 01 1 44e 01 14e 01 2 76e 01 3 23e 01 abc mr 0 2 mean 1 59e 02 1 86e 02 33e 02 34e 02 24e 02 std 1 71e 01 1 66e 01 21e 01 5 99e 01 3 22e 01 abc asf mr 0 9 mean 1 85e 02 1 75e 02 3 71e 02 3 02 30e 02 std 3 62e 01 3 27e 01 5 78e 01 6 43e 01 7 00e 01 5 3 experiments on composite functions in the second part of the experiments we tested the abc algorithm and its modified versions on the real parameter opti mization problems defined in some algorithms produce good results on some functions while they cannot achieve de sired performance on some others if an algorithm has an operator for producing neighboring solutions by copying one parameter to another it can converge to the global optima quickly when the global optimum lies on symmetric dimensions a similar situation is when the global optima are at the origin when an algorithm has local search capability finding the global optimum is simpler 34 for this reason in composite problems are constructed by combining simple functions via the gaussian function in order to obtain more challenging problems composition functions are randomly located asym metrical and multimodal problems functions in the set have different characteristics and they are categorized in a system atic manner that will determine how the algorithms behave under the common evaluation criteria specified for this set contains total functions comprising unimodal multimodal shifted rotated and hybrid composition func tions detailed information about these functions is available in the abc algorithm was initialized uniformly within the search space except for 7th and problems initial ranges for these two problems are specified in the report other problems except 7 and have the global optimum within bounds for dimension d values of and were employed the abc algorithm was terminated when the number of function evaluations reached the maxfes or the error of function value was equal to or less in maximum function eval uation sizes were 000 000 and 000 for problem dimensions and respectively for fair comparison the same evaluation numbers as in were employed the abc algorithm was run through 000 cycles for all dimensions therefore the colony sizes were and for the dimensions and respectively for each function the algo rithm was run times error values were sorted from the best to worst and beside the best and the worst 7th median and 19th function values are reported in tables 8 for d tables for d tables for d in fig a e logarithmic scaled convergence graphs of problems for d of median run are presented in the experiments we tried different modification rates for some functions functions 7 9 12 and better results were obtained with lower values of mr while for some functions functions and using higher mr values produced bet ter results for this reason we have chosen an average value for mr and set it to 0 limit was set to for all functions b akay d karaboga information sciences 120 table results of state of art algorithms for hybrid functions dimension colony size cycle for abc pso rdl recombination with dynamic linkage discovery in pso dms pso dynamic multi swarm particle swarm optimizer with local search spc pnx 4 de differential evolution sade self adaptive differential evolution restart cma es restart covariance matrix adaptation evolution strategy with increasing population size 1 f21 f25 pso rdl mean 9 94e 02 8 87e 02 1 08e 03 7 20e 02 1 76e 03 std 3 27e 02 7 12e 01 2 87e 02 3 96e 02 1 54e 01 dms pso mean 5 36e 02 6 02 7 30e 02 2 24e 02 3 66e 02 std 2 18e 02 1 56e 02 1 66e 02 8 31e 01 1 51e 02 spc pnx mean 6 80e 02 7 49e 02 5 76e 02 2 00e 02 4 06e 02 std 2 69e 02 9 37e 01 8 22e 01 0 00e 00 2 38eà01 de mean 4 92e 02 7 18e 02 5 72e 02 2 00e 02 9 23e 02 std 4 00e 01 1 58e 02 4 48e 01 0 00e 00 3 40eà01 sade mean 4 64e 02 7 32e 02 6 64e 02 2 00e 02 3 76e 02 std 1 58e 02 9 15e 01 1 53e 02 0 00e 00 3 15e 00 restart cma es mean 5 00e 02 7 29e 02 5 59e 02 2 00e 02 3 74e 02 std 0 00e 00 3 6 86e 00 3 3 22e 00 abc sf 1 mean 4 07e 02 8 59e 02 4 98e 02 2 02 2 00e 02 std 5 89e 01 7 29e 01 4 44e 01 5 4 abc sf 0 7 mean 3 82e 02 7 79e 02 5 05e 02 2 02 2 00e 02 std 6 89e 01 1 73e 02 3 78e 01 8 2 abc sf 0 5 mean 3 80e 02 8 26e 02 5 08e 02 2 02 2 00e 02 std 6 82e 01 1 64e 02 3 49e 01 7 3 abc sf 0 3 mean 3 84e 02 9 20e 02 5 01e 02 2 02e 02 2 00e 02 std 6 43e 01 1 31e 02 3 87e 01 8 2 abc mr 1 mean 7 76e 02 8 38e 02 8 00e 02 2 02e 02 2 00e 02 std 8 00e 01 3 40e 00 1 39e 02 4 4 abc mr 0 8 mean 7 58e 02 8 40e 02 8 20e 02 2 02e 02 2 00e 02 std 7 62e 01 4 41e 00 9 93e 01 5 7 abc mr 0 6 mean 7 08e 02 8 41e 02 7 49e 02 2 02e 02 2 00e 02 std 1 14e 02 3 85e 00 1 31e 02 5 9 abc mr 0 4 mean 5 56e 02 8 43e 02 5 77e 02 2 02e 02 2 00e 02 std 9 68e 01 5 97e 00 6 66e 01 6 1 abc mr 0 2 mean 4 39e 02 7 89e 02 5 09e 02 2 02e 02 2 00e 02 std 1 13e 02 1 32e 02 3 86e 01 5 2 abc asf mr 0 9 mean 5 81e 02 8 20e 02 5 66e 02 2 02e 02 2 00e 02 std 1 46e 02 6 11e 00 4 07e 01 8 4 and dimensions number of solutions in the population was and for dimensions and respectively values of control parameters are listed on table 6 in order to point out the relation between dimension and complexity for different dimensions algorithm complexity was calculated as in basic functions as for basic functions code execution time t 0 execution time of function 3 for 000 evaluations culated the t complexity 1 and for five of the runs algorithm mean of was the then algorithm determined execution by ðð times b t 2 à on function b t 1 þ t 0 þ and 3 for 000 evaluations t 2 were cal given in table 7 our system was win dows xp on pentium r m 1 ghz processor with 1 gb of ram and the programming language used was delphi 7 from the results in tables 8 for d the abc algorithm reached the given accuracy on functions 1 2 4 and 9 in case where dimension was the abc algorithm reached the given accuracy on functions 1 2 and 4 while when d it reached that just on functions 1 and 2 the abc algorithm was not executed using an optimal control parameter set while producing the results given in tables 8 we tried different parameter values for mr sf and limit and compared the performance of abc against other algo rithms that are included in the special session of on real parameter optimization recombination with dynamic link age discovery in particle swarm optimization pso rdl dynamic multi swarm particle swarm optimizer with local search dms pso spc pnx 4 differential evolution self adaptive differential evolution sade restart covariance matrix adaptation evolution strategy restart cma es 1 the dimension was colony size was and max imum evaluation number was 000 for control parameter mr values 1 0 8 0 6 0 4 0 2 for sf values 1 0 7 0 5 0 3 and adaptive scaling were employed comparison results are given in the tables in order to demonstrate the results better values in the tables are pre sented in fig 5 a fig 6 l from the results in the tables and figures all algorithms show similar performances on functions 1 and 2 on function 3 the abc algorithm shows better performance by the increment in mr parameter and adaptive scaling this means that the abc algorithm needs more parameters to be mutated in the neighborhood of the current solution for function 3 because the function is non separable for function 4 reducing scaling factor affects the performance of the abc algorithm negatively other algorithms show similar performance for function 4 for function 5 the abc algorithm pro duces the best results when mr is incremented in addition decreasing the step size worsens the performance of the abc algorithm for function 6 other algorithms outperform abc in all cases for function 8 dms pso pso rdl restart cma es and abc algorithm with asf demonstrate equal performances performance of the abc algorithm on function 9 is affected fig 6 comparison of the state of art algorithms and the variants of the abc algorithm for composite functions f25 and restart cmaes is better while on functions and the dms pso algorithm is better on function spc pnx de sade and restart cmaes algorithms perform equally on function abc produces the best performance however it should be noted that lower values of sf produce better results 6 a comparative discussion on evolutionary computing paradigms vs abc in the previous sections comparative results of pso de es and abc variants were presented in this section we offer a thorough comparative analysis by considering standard versions of these algorithms exploration which is the ability to search the solution space to find promising new solutions and exploitation which is the ability to find the optimum solution in the neighborhood of a good solution are two important aspects in evolutionary computing paradigms however different algorithms in evolutionary computing employ different operators for exploration and exploitation in es a mutant vector is created by adding a normally distributed random step size to each vector component in basic abc a step size which is a randomly weighted difference of the current solution and a solution randomly selected is b akay d karaboga information sciences 120 applied to only one component of the current solution to produce a neighboring solution recent versions of es such as cma es use self adaptive mechanisms for step size earlier versions of es and abc do not have a recombination operator a fitness based probabilistic selection scheme used in the abc algorithm does not exist in es one advantage of abc over es is the diversification controlled by the random selection process in the scout bees phase which makes abc escape from local minima in pso a new position vector is calculated using the particle current and best solution and the swarm best solution while in abc a new solution vector is calculated using the employed bee current solution and a randomly chosen solution in pso the new solution is replaced with the old one without considering which one is better however in abc a greedy selection scheme is applied between the new solution and the old one and the better one is preferred for inclusion in the population in this way the information of a good member of the population is distributed among the other members due to the greedy selection mechanism employed abc also uses a probabilistic selection scheme in the onlooker bees phase in addition to this greedy selection scheme the abc algorithm also has a scout phase which provides diversity in the pop ulation by allowing new random solutions to be inserted into the population instead of the solutions which do not provide improvements while the pso algorithm does not have such a process moreover pso has more control parameters than abc the neighboring solution production mechanism used in abc is similar to the self adapting mutation process of de from this point of view in de and abc algorithms the solutions in the population directly affect the mutation operation since the operation is based on the difference between them however in de the difference is weighted by a constant scaling factor while in abc it is weighted by a random step size unlike de in abc there is no explicit crossover although both algorithms employ greedy selection between the current solution and a new solution in de there is no operation as in the scout bees phase of abc to insert a random solution into the population during a search therefore although the local convergence speed of a standard de is quite good it might result in the premature convergence in optimizing multimodal problems if a sufficient diversity is not provided within the initial population the performance of abc is very good in terms of the local and the global optimization due to the selection schemes em ployed and the neighboring production mechanism used abc balances exploration and exploitation efficiently 7 conclusion in this work we investigated the performance of standard and modified versions of the artificial bee colony algorithm and compared their performances against state of the art algorithms presented in the literature besides comparing the arti ficial bee colony algorithm against some other algorithms comparisons between its own versions were also conducted although the standard abc algorithm modifies only one parameter while producing a new neighboring solution the mod ified abc algorithm employs a control parameter that determines how many parameters to be modified for the production of a neighboring solution a scaling factor that tunes the step size adaptively was introduced from the results it can be con cluded that the standard abc algorithm can efficiently solve basic functions while the modified abc algorithm produces promising results on hybrid functions compared to state of the art algorithms big data has been one of the current and future research frontiers in this year gartner listed the top strategic tech nology trends for and top critical tech trends for the next five years and big data is listed in the both two it is right to say that big data will revolutionize many fields including business the scientific research public administration and so on for the definition of the big data there are various different explanations from to doug laney used volume velocity and variety known as to characterize the concept of big data the term volume is the size of the data set velocity indicates the speed of data in and out and variety describes the range of data types and sources sometimes people extend another v according to their special requirements the fourth v can be value variability or virtual more commonly big data is a collection of very huge data sets with a great diversity of types so that it becomes dif ficult to process by using state of the art data processing approaches or traditional data processing platforms in gart ner retrieved and gave a more detailed definition as big data are high volume high velocity and or high variety corresponding author e mail addresses philip chen ieee org c l philip chen cyzhangfst gmail com c y zhang see front matter elsevier inc all rights reserved http dx doi org 1016 j ins 01 c l philip chen contents lists available at sciencedirect information sciences journal homepage www elsevier com locate ins c l philip chen c y zhang information sciences information assets that require new forms of processing to enable enhanced decision making insight discovery and process optimization more generally a data set can be called big data if it is formidable to perform capture curation analysis and visualization on it at the current technologies with diversified data provisions such as sensor networks telescopes scientific experiments and high throughput instru ments the datasets increase at exponential rate 110 as demonstrated in fig 1 source from the off the shelf techniques and technologies that we ready used to store and analyse data cannot work efficiently and satisfactorily the challenges arise from data capture and data curation to data analysis and data visualization in many instances science is legging behind the real world in the capabilities of discovering the valuable knowledge from massive volume of data based on precious knowledge we need to develop and create new techniques and technologies to excavate big data and benefit our specified purposes big data has changed the way that we adopt in doing businesses managements and researches data intensive science especially in data intensive computing is coming into the world that aims to provide the tools that we need to handle the big data problems data intensive science is emerging as the fourth scientific paradigm in terms of the previous three namely empirical science theoretical science and computational science thousand years ago scientists describing the nat ural phenomenon only based on human empirical evidences so we call the science at that time as empirical science it is also the beginning of science and classified as the first paradigm then theoretical science emerged hundreds years ago as the second paradigm such as newton motion laws and kepler laws however in terms of many complex phenomenon and problems scientists have to turn to scientific simulations since theoretical analysis is highly complicated and some times unavailable and infeasible afterwards the third science paradigm was born as computational branch simulations in large of fields generate a huge volume of data from the experimental science at the same time more and more large data sets are generated in many pipelines there is no doubt that the world of science has changed just because of the increasing data intensive applications the techniques and technologies for this kind of data intensive science are totally distinct with the previous three therefore data intensive science is viewed as a new and fourth science paradigm for scientific discov eries in section 2 we will discuss several transparent big data applications around three fields the opportunities and chal lenges aroused from big data problems will be introduced in section 3 then we give a detailed demonstration of state of the art techniques and technologies to handle data intensive applications in section 4 where big data tools discussed there will give a helpful guide for expertise users in section 5 a number of principles for designing effective big data sys tems are listed one of the most important parts of this paper which provides several underlying techniques to settle big data problems is ranged in section 6 in the last section we draw a conclusion 2 big data problems as more and more fields involve big data problems ranging from global economy to society administration and from scientific researches to national security we have entered the era of big data recently a report from mckinsey insti tute gives transformative potentials of big data in five domains health care of the united states public sector administration of european union retail of the united states global manufacturing and personal location data their research claims that fig 1 data deluge the increase of data size has surpassed the capabilities of computation 316 c l philip chen c y zhang information sciences 347 big data can make prominent growth of the world economy by enhancing the productivity and competitiveness of enter prises and also the public administrations big data has a deep relationship with e science which is computationally intensive science which usually is imple mented in distributed computing systems many issues on big data applications can be resolved by e science which require grid computing e sciences include particle physics bio informatics earth sciences and social simulations it also pro vides technologies that enable distributed collaboration such as the access grid particle physics has a well developed e sci ence infrastructure in particular because of its need for adequate computing facilities for the analysis of results and storage of data originating from the european organization for nuclear research cern large hadron collider which started taking data in e science is a big concept with many sub fields such as e social science which can be regarded as a higher development in e science it plays a role as a part of social science to collect process and analyse the social and behavioral data other big data applications lies in many scientific disciplines like astronomy atmospheric science medicine genomics biologic biogeochemistry and other complex and interdisciplinary scientific researches web based applications encounter big data frequently such as recent hot spots social computing including social network analysis online communities rec ommender systems reputation systems and prediction markets internet text and documents internet search indexing alternatively there are countless sensor around us they generate sumless sensor data that need to be utilized for instance intelligent transportation systems its are based on the analysis of large volumes of complex sensor data large scale e commerce are particularly data intensive as it involves large number of customers and transactions in the following subsections we will briefly introduce several applications of the big data problems in commerce and business society administration and scientific research fields 2 1 big data in commerce and business according to estimates the volume of business data worldwide across almost companies doubles every 1 2 years taking retail industry as an example we try to give a brief demonstration for the functionalities of big data in commercial activities there are around million transactions per day in wal mart stores worldwide for seeking for higher competitiveness in retail wal mart recently collaborated with hewlett packard to establish a data warehouse which has a capability to store 4 petabytes see the size of data unit in appendix a of data i e trillion bytes tracing every pur chase record from their point of sale terminals taking advantage of sophisticated machine learning techniques to exploit the knowledge hidden in this huge volume of data they successfully improve efficiency of their pricing strategies and adver tising campaigns the management of their inventory and supply chains also significantly benefits from the large scale warehouse in the era of information almost every big company encounters big data problems especially for multinational corpora tions on the one hand those companies mostly have a large number of customers around the world on the other hand there are very large volume and velocity of their transaction data for instance fico falcon credit card fraud detection sys tem manages over 2 1 billion valid accounts around the world there are above 3 billion pieces of content generated on face book every day the same problem happens in every internet companies the list could go on and on as we witness the future businesses battle fields focusing on big data 2 2 big data in society administration public administration also involves big data problems 30 on one side the population of one country usually is very large for another people in each age level need different public services for examples kids and teenagers need more edu cation the elders require higher level of health care every person in one society generates a lot of data in each public section so the total number of data about public administration in one nation is extremely huge for instance there are almost 3 terabytes of data collected by the us library of congress by the obama administration announced the big data re search and development initiative in which investigate addressing important problems facing the government by make use of big data the initiative was constitutive of different big data programs involving six departments 1 the sim ilar thing also happened in europe governments around the world are facing adverse conditions to improve their productivity namely they are required to be more effective in public administration particularly in the recent global recession many gov ernments have to provide a higher level of public services with significant budgetary constraints therefore they should take big data as a potential budget resource and develop tools to get alternative solutions to decrease big budget deficits and reduce national debt levels according to mckinsey report big data functionalities such as reserving informative patterns and knowledge provide the public sector a chance to improve productivity and higher levels of efficiency and effectiveness european pub lic sector could potentially reduce expenditure of administrative activities by percent increasing billion to billion values or even more this estimate is under efficiency gains and a reduction in the difference between actual and 1 http www whitehouse gov blog 03 big data big deal c l philip chen c y zhang information sciences 347 potential aggregate of tax revenue these functionalities could speed up year productivity growth by up to 0 5 percentage points over the next decade 2 3 big data in scientific research many scientific fields have already become highly data driven with the development of computer sciences for instance astronomy meteorology social computing bioinformatics and computational biology are greatly based on data intensive scientific discovery as large volume of data with various types generated or produced in these sci ence fields how to probe knowledge from the data produced by large scale scientific simulation it is a certain big data problem which the answer is still unsatisfiable or unknown for instances a sophisticated telescope is regarded as a very large digital camera which generate huge number of uni versal images for example the large synoptic survey telescope lsst will record 30 trillion bytes of image data in a single day the size of the data equals to two entire sloan digital sky surveys daily astronomers will utilize computing facilities and advanced analysis methods to this data to investigate the origins of the universe the large hadron collider lhc is a particle accelerator that can generate terabytes of data per day the patterns in those data can give us an unprecedented understanding the nature of the universe petabytes of climate observations and simulations were con served on the discovery supercomputing cluster in the nasa center for climate simulation nccs the volume of human genome information is also so large that decoding them originally took a decade to process otherwise a lot of other e science projects are proposed or underway in a wide variety of other research fields range from environmental sci ence oceanography and geology to biology and sociology one common point exists in these disciplines is that they gen erate enormous data sets that automated analysis is highly required additionally centralized repository is necessary as it is impractical to replicate copies for remote individual research groups therefore centralized storage and analysis ap proaches drive the whole system designs 3 big data opportunities and challenges 3 1 opportunities recently several us government agencies such as the national institutes of health nih and the national science foundation nsf ascertain that the utilities of big data to data intensive decision making have profound influences in their future developments 1 consequently they are trying to developing big data technologies and techniques to facil itate their missions after us government passed a large scale big data initiative this initiative is very helpful for building new capabilities for exploiting informative knowledge and facilitate decision makers from the networking information technology research and development nitrd program which is recently recognized by president council of advisors on science and technology pcast we know that the bridges between big data and knowledge hidden in it are highly crucial in all areas of national priority this initiative will also lay the groundwork for com plementary big data activities such as big data infrastructure projects platforms development and techniques in settling complex data driven problems in sciences and engineering finally they will be put into practice and benefit society according to the report from mckinsey institute the effective use of big data has the underlying benefits to transform economies and delivering a new wave of productive growth taking advantages of valuable knowledge beyond big data will become the basic competition for today enterprises and will create new competitors who are able to attract employees that have the critical skills on big data researchers policy and decision makers have to recognize the potential of harnessing big data to uncover the next wave of growth in their fields there are many advantages in business section that can be obtained through harnessing big data as illustrated in fig 2 including increasing operational efficiency informing strategic direction developing better customer service identifying and developing new products and services fig 2 big data opportunities above of enterprises think big data will help them in increasing operational efficiency etc identifying new customers and markets etc the vertical axis denotes the percentages that how many enterprises think big data can help them with respect to specific purposes by liberal estimates big data could produce billion potential annual value to us health care and bil lion to european public administration there will be billion potential annual consumer surplus from using personal location data globally and give a potential increase with only in united states big data produce 000 to 000 deep analytical talent positions and 1 5 million data savvy managers undoubtedly big data is usually juicy and lucrative if explored correctly 3 2 challenges opportunities are always followed by challenges on the one hand big data bring many attractive opportunities on the other hand we are also facing a lot of challenges 137 when handle big data problems difficulties lie in data capture storage searching sharing analysis and visualization if we cannot surmount those challenges big data will become a gold ore but we do not have the capabilities to explore it especially when information surpass our capability to harness one challenge is existing in computer architecture for several decades that is cpu heavy but i o poor this system imbalance still restraint the development of the discovery from big data the cpu performance is doubling each months following the moore law and the performance of disk drives is also doubling at the same rate however the disks rotational speed has slightly improved over the last decade the conse quence of this imbalance is that random i o speeds have improved moderately while sequential i o speeds increase with density slowly moreover information is increasing at exponential rate simultaneously but the improvement of informa tion processing methods is also relatively slower in a lot of important big data applications the state of the art tech niques and technologies cannot ideally solve the real problems especially for real time analysis so partially speaking until now we do not have the proper tools to exploit the gold ores completely typically the analysis process is shown in fig 3 where the knowledge is discovered in data mining challenges in big data analysis include data inconsistence and incompleteness scalability timeliness and data security 8 92 as the prior step to data analysis data must be well constructed however considering variety of data sets in big data problems it is still a big challenge for us to purpose efficient representation access and analysis of unstructured or semi structured data in the further researches how can the data be preprocessed in order to improve the quality data and the analysis results before we begin data analysis as the sizes of data set are often very huge sometimes several gigabytes or more and their origin from heterogeneous sources current real world databases are severely susceptible to inconsistent incom plete and noisy data therefore a number of data preprocessing techniques including data cleaning data integration data transformation and date reduction can be applied to remove noise and correct inconsistencies different challenges arise in each sub process when it comes to data driven applications in the following subsections we will give a brief dis cussion about challenges we are facing for each sub process c l philip chen c y zhang information sciences 347 fig 3 knowledge discovery process c l philip chen c y zhang information sciences 347 3 2 1 data capture and storage data sets grow in size because they are increasingly being gathered by ubiquitous information sensing mobile devices aerial sensory technologies remote sensing software logs cameras microphones radio frequency identification readers wireless sensor networks and so on there are 2 5 quintillion bytes of data created every day and this number keeps increas ing exponentially the world technological capacity to store information has roughly doubled about every 3 years since the in many fields like financial and medical data often be deleted just because there is no enough space to store these data these valuable data are created and captured at high cost but ignored finally the bulk storage requirements for experimental data bases array storage for large scale scientific computations and large output files are reviewed in big data has changed the way we capture and store data including data storage device data storage architecture data access mechanism as we require more storage mediums and higher i o speed to meet the challenges there is no doubt that we need great innovations firstly the accessibility of big data is on the top priority of the knowledge discovery process big data should be accessed easily and promptly for further analysis fully or partially break the restraint cpu heavy but i o poor in addition the under developing storage technologies such as solid state drive ssd and phase change memory pcm may help us alleviate the difficulties but they are far from enough one significant shift is also under way that is the transformative change of the traditional i o subsystems in the past decades the persistent data were stored by using hard disk drives hdds as we known hdds had much slower random i o performance than sequential i o performance and data processing engines formatted their data and designed their query processing methods to work around this limitation but hdds are increasingly being replaced by ssds today and other technologies such as pcm are also around the corner 8 these current storage technologies cannot possess the same high performance for both the sequential and random i o simultaneously which requires us to rethink how to design storage subsystems for big data processing systems direct attached storage das network attached storage nas and storage area network san are the enterprise stor age architectures that were commonly used however all these existing storage architectures have severe drawbacks and limitations when it comes to large scale distributed systems aggressive concurrency and per server throughput are the essential requirements for the applications on highly scalable computing clusters and today storage systems lack the both optimizing data access is a popular way to improve the performance of data intensive computing 77 these techniques include data replication migration distribution and access parallelism in the performance reliability and scalability in data access platforms were discussed data access platforms such as castor dcache gpfs and scalla xrootd are employed to demonstrate the large scale validation and performance measurement data storage and search schemes also lead to high overhead and latency distributed data centric storage is a good approach in large scale wireless sen sor networks wsns shen zhao and li proposed a distributed spatial temporal similarity data storage scheme to provide efficient spatial temporal and similarity data searching service in wsns the collective behavior of individuals that cooper ate in a swarm provide approach to achieve self organization in distributed systems 184 3 2 2 data transmission cloud data storage is popularly used as the development of cloud technologies we know that the network bandwidth capacity is the bottleneck in cloud and distributed systems especially when the volume of communication is large on the other side cloud storage also lead to data security problems as the requirements of data integrity checking many schemes were proposed under different systems and security models 134 3 2 3 data curation data curation is aimed at data discovery and retrieval data quality assurance value addition reuse and preservation over time this field specifically involves a number of sub fields including authentication archiving management preservation retrieval and representation the existing database management tools are unable to process big data that grow so large and complex this situation will continue as the benefits of exploiting big data allowing researchers to analyse business trends prevent diseases and combat crime though the size of big data keeps increasing exponentially current capability to work with is only in the relatively lower levels of petabytes exabytes and zettabytes of data the classical approach of managing structured data includes two parts one is a schema to storage the data set and another is a relational database for data re trieval for managing large scale datasets in a structured way data warehouses and data marts are two popular approaches a data warehouse is a relational database system that is used to store and analyze data also report the results to users the data mart is based on a data warehouse and facilitate the access and analysis of the data warehouse a data warehouse is mainly responsible to store data that is sourced from the operational systems the preprocessing of the data is necessary before it is stored such as data cleaning transformation and cataloguing after these preprocessing the data is available for higher level online data mining functions the data warehouse and marts are standard query language sql based dat abases systems nosql database also called not only sql is a current approach for large and distributed data management and database design its name easily leads to misunderstanding that nosql means not sql on the contrary nosql does not avoid sql while it is true that some nosql systems are entirely non relational others simply avoid selected relational functionality such as fixed table schemas and join operations the mainstream big data platforms adopt nosql to break and transcend the rigidity of normalized rdbms schemas for instance hbase is one of the most famous used nosql databases see fig 4 however many big data analytic platforms like sqlstream and cloudera impala series still use sql in its data base systems because sql is more reliable and simpler query language with high performance in stream big data real time analytics to store and manage unstructured data or non relational data nosql employs a number of specific approaches firstly data storage and management are separated into two independent parts this is contrary to relational databases which try to meet the concerns in the two sides simultaneously this design gives nosql databases systems a lot of advantages in the storage part which is also called key value storage nosql focuses on the scalability of data storage with high performance in the management part nosql provides low level access mechanism in which data management tasks can be implemented in the application layer rather than having data management logic spread across in sql or db specific stored procedure lan guages therefore nosql systems are very flexible for data modeling and easy to update application developments and deployments 60 most nosql databases have an important property namely they are commonly schema free indeed the biggest advan tage of schema free databases is that it enables applications to quickly modify the structure of data and does not need to rewrite tables additionally it possesses greater flexibility when the structured data is heterogeneously stored in the data management layer the data is enforced to be integrated and valid the most popular nosql database is apache cassandra cassandra which was once facebook proprietary database was released as open source in other nosql implementa tions include simpledb google bigtable apache hadoop mapreduce memcachedb and voldemort companies that use nosql include twitter linkedin and netflix 3 2 4 data analysis the first impression of big data is its volume so the biggest and most important challenge is scalability when we deal with the big data analysis tasks in the last few decades researchers paid more attentions to accelerate analysis algorithms to cope with increasing volumes of data and speed up processors following the moore law for the former it is necessary to develop sampling on line and multiresolution analysis methods in the aspect of big data analytical techniques incre ment algorithms have good scalability property not for all machine learning algorithms some researchers devote into this area 72 as the data size is scaling much faster than cpu speeds there is a natural dramatic shift 8 in processor technology although the clock cycle frequency of processors is doubling following moore law the clock speeds still highly lag behind alternatively processors are being embedded with increasing numbers of cores this shift in processors leads to the development of parallel computing for those real time big data applications like navigation social networks finance biomedicine astronomy intelligent transport systems and internet of thing timeliness is at the top priority how can we grantee the timeliness of response when the volume of data will be processed is very large it is still a big challenge for stream processing involved by big data it is right to say that big data not only have produced many challenge and changed the directions of the development of the hardware but also in software architectures that is the swerve to cloud computing 186 7 which aggregates multiple disparate workloads into a large cluster of processors in this direction distributed computing is being developed at high speed recently we will give a more detail discussion about it in next section c l philip chen c y zhang information sciences 347 fig 4 hbase nosql database system architecture source from apache hadoop c l philip chen c y zhang information sciences 347 data security surfaces with great attentions significant security problems include data security protection intellectual property protection personal privacy protection commercial secrets and financial information protection most devel oped and developing countries have already made related data protection laws to enhance the security research groups and individuals need to carefully consider the legislation of where they store and process data to make sure that they are in com pliance with the regulations for big data related applications data security problems are more awkward for several reasons firstly the size of big data is extremely large channelling the protection approaches secondly it also leads to much heavier workload of the security otherwise most big data are stored in a distributed way and the threats from networks also can aggravate the problems 3 2 5 data visualization the main objective of data visualization is to represent knowledge more intuitively and effectively by using dif ferent graphs to convey information easily by providing knowledge hidden in the complex and large scale data sets both aesthetic form and functionality are necessary information that has been abstracted in some schematic forms including attributes or variables for the units of information is also valuable for data analysis this way is much more intuitive than sophisticated approaches online marketplace ebay have hundreds of million active users and billions of goods sold each month and they generate a lot of data to make all that data understandable ebay turned to big data visualization tool tableau which has capability to transform large complex data sets into intuitive pictures the results are also inter active based on them ebay employees can visualize search relevance and quality to monitor the latest customer feedback and conduct sentiment analysis for big data applications it is particularly difficult to conduct data visualization because of the large size and high dimen sion of big data however current big data visualization tools mostly have poor performances in functionalities scalability and response time what we need to do is rethinking the way we visualize big data not like the way we adopt before for example the history mechanisms for information visualization also are data intensive and need more efficient ap proaches uncertainty can lead to a great challenge to effective uncertainty aware visualization and arise in any stage of a visual analytics process new framework for modeling uncertainty and characterizing the evolution of the uncer tainty information are highly necessary through analytical processes the shortage of talent will be a significant constraint to capture values from big data in the united states big data is expected to rapidly become a key determinant of competition across many sectors however this area demands for deep analytical positions on big data could exceed the supply being produced on current trends by 000 to 000 positions furthermore this kind of human resource is more difficult to educate it usually takes many years to train big data analysts that must have intrinsic mathematical abilities and related professional knowledge we believe that the same sit uation also happened in other nations not matter developed countries or developing countries around the world it is fore seeable that there will be another hot competition about human resources in big data developments after review a number of challenges the optimists take a broad view challenges and hidden benefits they have enough confidence that we have the capabilities to overcome all the obstacles as new techniques and technologies are developed there are many critiques and negative opinions 167 from the pessimists some researchers think big data will lead to the end of theory and doubt whether it can help us to make better decisions whatever the mainstream perspectives are most positive so a large number of big data techniques and technologies have been developed or under developing 4 big data tools techniques and technologies to capture the value from big data we need to develop new techniques and technologies for analyzing it until now sci entists have developed a wide variety of techniques and technologies to capture curate analyze and visualize big data even so they are far away from meeting variety of needs these techniques and technologies cross a number of discipline includ ing computer science economics mathematics statistics and other expertises multidisciplinary methods are needed to dis covery the valuable information from big data we will discuss current techniques and technologies for exploiting data intensive applications we need tools platforms to make sense of big data current tools concentrate on three classes namely batch processing tools stream processing tools and interactive analysis tools most batch processing tools are based on the apache hadoop infrastructure such as mahout and dryad the latter is more like necessary for real time analytic for stream data applica tions storm and are good examples for large scale streaming data analytic platforms the interactive analysis processes the data in an interactive environment allowing users to undertake their own analysis of information the user is directly connected to the computer and hence can interact with it in real time the data can be reviewed compared and analyzed in tabular or graphic format or both at the same time google dremel and apache drill are big data platforms based on inter active analysis in the following sub sections we ll discuss several tools for each class more information about big data tools can be found in appendix c 4 1 big data techniques big data needs extraordinary techniques to efficiently process large volume of data within limited run times reasonably big data techniques are driven by specified applications for example wal mart applies machine learning and statistical techniques to explore patterns from their large volume of transaction data these patterns can produce higher competitive ness in pricing strategies and advertising campaigns taobao a chinese company like ebay adopts large stream data mining techniques on users browse data recorded on its website and exploits a good deal of valuable information to support their decision making big data techniques involve a number of disciplines including statistics data mining machine learning neural networks social network analysis signal processing pattern recognition optimization methods and visualization approaches there are many specific techniques in these disciplines and they overlap with each other hourly illustrated as fig 5 optimization methods have been applied to solve quantitative problems in a lot of fields such as physics biology engi neering and economics in several computational strategies for addressing global optimization problems are dis cussed such as simulated annealing adaptive simulated annealing quantum annealing as well as genetic algorithm which naturally lends itself to parallelism and therefore can be highly efficient stochastic optimization including genetic programming evolutionary programming and particle swarm optimization are useful and specific optimization techniques inspired by the process of nature however they often have high complexity in memory and time consumption many re search works have been done to scale up the large scale optimization by cooperative co evolutionary algo rithms real time optimization is also required in many big data application such as wsns and itss data reduction and parallelization are also alternative approaches in optimization problems statistics is the science to collect organize and interpret data statistical techniques are used to exploit correlationships and causal relationships between different objectives numerical descriptions are also provided by statistics however stan dard statistical techniques are usually not well suited to manage big data and many researchers have proposed extensions of classical techniques or completely new methods 41 authors in proposed efficient approximate algorithm for large scale multivariate monotonic regression which is an approach for estimating functions that are monotonic with re spect to input variables another trend of data driven statistical analysis focuses on scale and parallel implementation of sta tistical algorithms a survey of parallel statistics can be found in and several parallel statistics algorithms are discussed in statistical computing 193 and statistical learning are the two hot research sub fields data mining is a set of techniques to extract valuable information patterns from data including clustering analysis classification regression and association rule learning it involves the methods from machine learning and statistics big data mining is more challenging compared with traditional data mining algorithms taking clustering as an example a natural way of clustering big data is to extend existing methods such as hierarchical clustering k mean and fuzzy c mean so that they can cope with the huge workloads most extensions usually rely on analyzing a certain amount of samples of big data and vary in how the sample based results are used to derive a partition for the overall data this kind of clustering algorithms include clara clustering large applications algorithm clarans cluster ing large applications based upon randomized search birch balanced iterative reducing using cluster hierarchies algorithm and so on genetic algorithms are also applied to clustering as optimization criterion to reflect the goodness 322 c l philip chen c y zhang information sciences 347 fig 5 big data techniques c l philip chen c y zhang information sciences 347 clustering big data is also developing to distributed and parallel implementation taking discriminant analysis as another example researchers try to develop effective algorithm for large scale discriminant analysis 165 the empha sis is on the reduction of computational complexity taking bioinformatics as another example it becomes increasingly data driven that leads to paradigm change from traditional single gene biology to the approaches that combine integrative database analysis and data mining this new paradigm enables the synthesis of large scale portraits of genome function machine learning is an important subjection of artificial intelligence which is aimed to design algorithms that allow computers to evolve behaviors based on empirical data the most obvious characteristic of machine learning is to discovery knowledge and make intelligent decisions automatically when big data is concerned we need to scale up machine learning algorithms both supervised learning and unsupervised learning to cope with it deep machine learning has become a new research frontier in artificial intelligence in addition there are several frameworks like map reduce dryadlinq and ibm parallel machine learning toolbox that have capabilities to scale up machine learning for example sup port vector machine svm which is a very fundamental algorithm used in classification and regression problems suffers from serious scalability problem in both memory use and computation time parallel svm psvm 121 are introduced recently to reduce memory and time consumption there are many scale machine learning algorithms 106 85 but many important specific sub fields in large scale machine learning such as large scale recommender systems natural language processing association rule learning ensemble learning still face the scalability problems artificial neural network ann is a mature techniques and has a wide range of application coverage its successful applications can be found in pattern recognition image analysis adaptive control and other areas most of the currently employed anns for artificial intelligence are based on statistical estimations classification optimization and control theory it is generally acknowledged the more hidden layers and nodes in a neural network the higher accu racy they can produce however the complexity in a neural network also increases the learning time therefore the learn ing process in a neural networks over big data is severely time and memory consuming 206 neural processing of large scale data sets often leads to very large networks then there are two main challenges in this situation one is that the conventional training algorithms perform very poorly and the other is that the training time and memory limitations are increasingly intractable 49 naturally two common approaches can be employed in this situation one is to re duce the data size by some sampling methods and the structure of the neural network maybe remains the same the other one is to scale up neural networks in parallel and distributed ways 119 9 for example the combination of deep learning and parallel training implementation techniques provides potential ways to process big data 97 visualization approaches are the techniques used to create tables images diagrams and other intuitive display ways to understand data big data visualization 64 is not that easy like traditional relative small data sets because of the complexity in or the extension of traditional visualization approaches are already emerged but far away from enough when it comes to large scale data visualization many researchers use feature extraction and a geometric modeling to significantly reduce the data size before the actual data rendering for more closely and intuitively data inter pretation some researchers try to run batch mode software rendering of the data at the highest possible resolution in a par allel way 112 choosing proper data representation is also very important when we try to visualize big data in 182 author tried to compact data and give a good approximation to large scale data social network analysis sna which has emerged as a key technique in modern sociology views social relationships in terms of network theory and it consists of nodes and ties it has also gained a significant following in anthropology biology communication studies economics geography history information science organizational studies social psychology development studies and sociolinguistics and is now commonly available as a consumer tool sna include social system de sign human behavior modeling social network visualization social networks evolution analysis and graph query and mining recently online social networks and social media analysis have become popular one of the main obstacles regarding sna is the vastness of big data analysis of a network consisting of millions or billions of connected objects is usually computationally costly two hot research frontiers social computing and cloud computing are in favor of sna to some degree higher level big data technologies include distributed file systems distributed computational systems massively parallel processing mpp systems data mining based on grid computing 34 cloud based storage and computing resources as well as granular computing and biological computing these technologies will be intro duced in the following sub sections many researchers regard the curse of dimensionality as one aspect of big data problems indeed big data should not be constricted in data volume but all take the high dimension characteristic of data into consideration in fact processing high dimensional data is already a tough task in current scientific research the state of the art techniques for handling high dimensional data intuitively fall into dimension reduction namely we try to map the high dimensional data space into lower dimensional space with less loss of information as possible there are a large number of methods to reduce dimension linear mapping methods such as principal component analysis pca and factor analysis are popular linear dimension reduction techniques non linear techniques include kernel pca manifold learning techniques such as isomap locally linear embedding lle hessian lle laplacian eigenmaps and ltsa recently a generative deep networks called autoencoder perform very well as non linear dimensionality reduction random projection in dimensionality reduction also have been well developed c l philip chen c y zhang information sciences 347 4 2 big data tools based on batch processing one of the most famous and powerful batch process based big data tools is apache hadoop it provides infrastructures and platforms for other specific big data applications a number of specified big data systems table 1 are built on hadoop and have special usages in different domains for example data mining and machine learning used in business and commerce 4 2 1 apache hadoop and map reduce apache hadoop is one of the most well established software platforms that support data intensive distributed applica tions it implements the computational paradigm named map reduce apache hadoop see fig 6 platform consists of the hadoop kernel map reduce and hadoop distributed file system hdfs as well as a number of related projects including apache hive apache hbase and so on map reduce which is a programming model and an execution for processing and generating large volume of data sets was pioneered by google and developed by yahoo and other web companies map reduce is based on the divide and conquer method and works by recursively breaking down a complex problem into many sub problems until these sub problems is scalable for solving directly after that these sub problems are assigned to a cluster of working notes and solved in separate and parallel ways finally the solutions to the sub problems are then combined to give a solution to the original problem the divide and conquer method is implemented by two steps map step and reduce step in terms of hadoop cluster there are two kinds of nodes in hadoop infrastructure they are master nodes and worker nodes the master node takes the input divides it into smaller sub problems and distributes them to worker nodes in map step afterwards the master node collects the an swers to all the sub problems and combines them in some way to form the output in reduce step with the addition of map reduce hadoop works as a powerful software framework 149 54 for easily writing applica tions which process vast quantities of data in parallel on large clusters perhaps thousands of nodes of commodity hard ware in a reliable fault tolerant manner we give a famous and prototypical example that counts the occurrence number of each word in a set of documents for map reduce framework where the two main functions map and reduce are given in the following more detailed java code from hadoop is attached in appendix b the map steps are implemented on hadoop cluster in a parallel way and a large number of lists of intermediate data pairs with the form key c are produced where key represents a specified word and the parameter c indicates the count of the word appearance in reduce steps those lists of data pairs are integrated to the final results recursively in the main function as illustrated in fig 7 there are a master jobtracker and a number of slaves tasktracker in the map reduce frame work the master node is in charge of job scheduling and task distribution for the slaves the slaves implement the tasks exactly as assigned by the master as long as the systems start to run the master node keeps monitoring all the data nodes if there is a data nodes failed to execute the related tasks the master node will ask the data node or another data node to re execute the failed tasks in practice applications specify the input files and output locations and submit their map and re duce functions via interactions of client interfaces these parameters are important to construct a job configuration after that the hadoop job client submits the job and configuration to the jobtracker once jobtracker receive all the necessary information it will distribute the software configuration to the tasktrackers schedule tasks and monitor them provide sta tus and diagnostic information to the job client from the foregoing we know that coordination plays a very important role in hadoop it ensures the performance of a hadoop job in andrew pavlo gave an overall discussion on properties of map reduce framework as well as other approaches to large scale data analysis many data mining algorithms have been designed to accommodate map reduce for example data cube materialization and mining efficient skyline computation and scalable boosting methods 138 4 2 2 dryad dryad is another popular programming models for implementing parallel and distributed programs that can scale up capability of processing from a very small cluster to a large cluster it bases on dataflow graph processing the infra table 1 big data tools based on batch processing name specified use advantage apache hadoop infrastructure and platform high scalability reliability completeness dryad infrastructure and platform high performance distributed execution engine good programmability apache mahout machine learning algorithms in business good maturity jaspersoft bi suite business intelligence software cost effective self service bi at scale pentaho business analytics business analytics platform robustness scalability flexibility in knowledge discovery skytree server machine learning and advanced analytics process massive datasets accurately at high speeds tableau data visualization business analytics faster smart fit beautiful and ease of use dashboards karmasphere studio and analyst big data workspace collaborative and standards based unconstrained analytics and self service talend open studio data management and application integration easy to use eclipse based graphical environment structure for running dryad consists of a cluster of computing nodes and a programmer use the resources of a computer cluster to running their programs in a distributed way indeed dryad programmers can use thousands of machines each of them with multiple processors or cores one bonus is that programmers does not need to know anything about concurrent programming a dryad application runs a computational directed graph which is composed of computational vertices and communication channels the computation is structured as illustrated in fig 8 graph vertices represent the programs while graph edges denote the channels a dryad programmer writes several sequential programs and connects them using one way channels a dryad job is to generator a graph and it has capability to synthesize any directed acyclic graph these generated graphs can also be updated after execution in order to deal with the unexpected events in the computation dryad provides a large number of functionality including generating the job graph scheduling the processes on the avail able machines handling transient failures in the cluster collecting performance metrics visualizing the job invoking user defined policies and dynamically updating the job graph in response to these policy decisions without awareness of the semantics of the vertices fig 9 schematically shows the implementation schema of dryad there is a centralized job manager to supervise every dryad job it uses a small set of cluster services to control the execution of the vertices on the cluster because dryad encompasses other computational frameworks like map reduce and the relational algebra it is more com plex and powerful in some degree otherwise dryad is a self contained system with complete functions including job cre ation and management resource management job monitoring and visualization fault tolerance re execution therefore fig 7 map reduce overview solid arrows are for map flows and feint arrows are for reduce flows c l philip chen c y zhang information sciences 347 fig 6 hadoop system architecture fig 8 the structure of dryad jobs many software have been built on top dryad including microsoft server integration services ssis 120 and dryad linq 4 2 3 apache mahout the apache mahout aims to provide scalable and commercial machine learning techniques for large scale and intel ligent data analysis applications many renowned big companies such as google amazon yahoo ibm twitter and face book have implemented scalable machine learning algorithms in their projects many of their projects involve with big data problems and apache mahout provides a tool to alleviate the big challenges mahout core algorithms including clustering classification pattern mining regression dimension reduction evolution ary algorithms and batch based collaborative filtering run on top of hadoop platform via the map reduce framework these algorithms in the libraries have been well designed and optimized to have good performance and capabilities a num ber of non distributed algorithms are also contained the goal of mahout is to build a vibrant responsive diverse community to facilitate discussions not only on the project itself but also on potential use cases the business users need to purchase apache software license for mahout more detailed content can be found on the web site http mahout apache org 4 2 4 jaspersoft bi suite the jaspersoft package is an open source software that produce reports from database columns the software has already been installed in many business information systems it is an scalable big data analyticical platform and fast to get started with no need for etl jaspersoft has a capability of fast data visualization on popular storage platforms including mon godb cassandra redis riak and couchdb hadoop is also represented very well by jasperreports which provides a hive connector to hbase because it is integrated with all the leading big data platforms users have the flexibility to choose what works best for their own projects one important property of jaspersoft is that it can quickly explore big data without extraction transformation etl and loading as connected directly to mainstream big data stores whether or not they have a sql interface it explores large scale data using visualizations powered by a terabyte scale columnar based in memory engine optimizing perfor mance through an in memory engine that can push down query processing to the underlying data store if necessary jasper soft also have a ability to build powerful reports and dashboards interactively and directly from big data store without etl requirement these reports can be shared with anyone inside or outside user organizations or embedded in user applications 4 2 5 pentaho business analytics pentaho 4 is another software platform for big data it also generate reports from both structured and unstructured large volume of data pentaho plays as a business analytic platform for big data to provide professional services for business men with facile access integration visualization and exploration of data therefore pentaho can enable business users to make data driven decisions that have a positive effect on the performance of their organization the techniques embedded in it have several properties including good security scalability and accessibility similar with jaspersoft there is a chain between pentaho tool and many of the most popular nosql databases such as mongodb and cassandra once the connection to databases is established users can drill up and drill down the columns into different information granules business users can access their data though a web based interface that pentaho provided with its easy way to use wiz ard based approach business users can turn their data into insight and make information driven decisions very fast the graphical programming interface developed by pentaho such as kettle and pentaho data integration are very powerful tools to process massive data pentaho also develops softwares that are based on hadoop clusters to draw hdfs file data and 326 c l philip chen c y zhang information sciences 347 fig 9 dryad architecture c l philip chen c y zhang information sciences 347 hbase data so users just need to write their code and send them out to execute on the cluster by this way the data ana lytical processes are highly escalated 4 2 6 skytree server skytree server is the first general purpose machine learning and advanced analytics system designed to accurately process massive datasets at high speeds it offers many sophisticated machine learning algorithms it is easy to use and users just need to type the right command into a command line skytree server has five specific use cases namely recommenda tion systems anomaly outlier identification predictive analytics clustering and market segmentation and similarity search skytree is more focused on real time analytics therefore it is optimized to implement a number of sophisticated ma chine learning algorithms on big data via a mechanism which the company claims can be 000 times faster than other congeneric platforms it also can handle structured and unstructured data from relational databases hdfs flat files common statistical packages and machine learning libraries 4 2 7 tableau tableau has three main products to process large scale data set including tableau desktop tableau sever and tab leau public tableau desktop is a visualization tool that makes it easy to visualize data and look at it in a different and intu itive way the tool is optimized to give user all the columns for the data and let users mix them tableau server is a business intelligence system that provides browser based analytics and tableau public is used to create interactive visuals tableau also embed hadoop infrastructure it employs hive to structure the queries and cache the information for in memory analytics caching helps to reduce the latency of a hadoop cluster therefore it can provide an interactive mecha nism between users and big data applications 4 2 8 karmasphere studio and analyst karmasphere 3 is another hadoop based big data platform for business data analysis it provides a new approach for self service access and analytics to big data in a fast efficient and collaborative way karmasphere is natively designed for hadoop platform it provides users an integrated and user friendly workspace for processing their big data applications and presenting the workflows from the point of its performance it has capability to discovery business insight from huge amounts of data including data ingestion iterative analysis visualization and reporting karmasphere studio is a set of plug ins built on top of eclipse in this well designed integrated development environment users can easily write and implement their hadoop jobs on the platform karmasphere analyst is a big data tool which is designed by karmasphere to escalate the analytical process on hadoop clusters in addition karmasphere analyst also embeds hive project for processing structured and unstructured data on ha doop clusters technical analysts sql programmers and database administrator can experiment with hadoop in graphical environment this also makes karmasphere analyst to be an enterprise class big data platform 4 2 9 talend open studio talend open studio is an open source software for big data applications that provides users graphical environment to conduct their analysis visually it is developed from apache hadoop and involves hdfs pig hcatalog hbase sqoop or hive users can resolve their big data problems in this platform without the need to write complicated java code which cannot be avoided in hadoop by using talend studio users can build up their own tasks through dragging and dropping varieties of icons onto a canvas stringing together blocks visually can be simple after users get a feel for what the components actually do and do not do visual programming seems like a superordinate goal but the icons can never represent the mechanisms with enough detail to make it possible to deeply understand talend open studio also provides really simple syndication rss feed and its components maybe collect the rss and add proxying if needed 4 3 stream processing big data tools hadoop does well in processing large amount of data in parallel it provides a general partitioning mechanism to distrib ute aggregation workload across different machines nevertheless hadoop is designed for batch processing it is a multi pur pose engine but not a real time and high performance engine since there are high throughout latency in its implementations for certain stream data applications such as processing log files industry with sensor machine to ma chine and telematics requires real time response for processing large amount of stream data in those applications stream processing for real time analytics is mightily necessary stream big data has high volume high velocity and complex data types indeed when the high velocity and time dimension are concerned in applications that involve real time process ing there are a number of different challenges to map reduce framework therefore the real time big data platforms such as sqlstream 6 storm and streamcloud 57 are designed specially for real time stream data analytics real time processing means that the ongoing data processing highly requires a very low latency of response hence there is not too much data accumulation at the time dimension for processing in general big data may be collected and stored in a distributed environment not in one data center in the general map reduce framework the reduce phase starts to work only after the map phase finish up but most of all all the intermediate data generated in map phase is saved in the table 2 big data tools based on stream processing name specified use advantages storm realtime computation system scalable fault tolerant and is easy to set up and operate processing continuous unbounded streams of data proven distributed scalable fault tolerant pluggable platform sqlstream server sensor and telematics applications sql based real time streaming big data platform splunk collect and harness machine data fast and easy to use dynamic environments scales from laptop to datacenter apache kafka distributed publish subscribe messaging system high throughput stream of immutable activity data sap hana platform for real time business fast in memory computing and realtime analytic disk before submit to the reducers for next phase all these lead to significant latency of the processing the high latency characteristic of hadoop makes it almost impossible for real time analytics several big data tools based on stream processing have been developed or under developing one of the most famous platforms is storm and others include sqlstream 5 splunk apache kafka and sap hana see table 2 4 3 1 storm storm 6 is a distributed and fault tolerant real time computation system for processing limitless streaming data it is released as open source and free for remoulding storm is specifically designed for real time processing contrasts with hadoop which is for batch processing it is also very easy to set up and operate and guarantees all the data will be processed it is also scalable and fault tolerant to provide competitive performances storm is efficient that a benchmark clocked it at over a million tuples processed per second per node therefore it has many applications such as real time analytics inter active operation system on line machine learning continuous computation distributed rpc and etl a storm cluster is ostensibly similar to a hadoop cluster whereas on storm users run different topologies for different storm tasks however hadoop platform implements map reduce jobs for corresponding applications there are a number c l philip chen c y zhang information sciences 347 fig a storm topology example fig a storm cluster of differences between map reduce jobs and topologies the key one is that a map reduce job eventually finishes whereas a topology processes messages all the time or until users terminate it 86 to implement real time computation on storm users need to create different topologies a topology illustrated in fig is a graph of computation and can be created and submitted in any programming language there are two kinds of node in topologies namely spouts and bolts a spout is one of the starting points in the graph which denotes source of streams a bolt processes input streams and outputs new streams each node in a topology contains processing logic and links between nodes indicate how data should be processed between nodes therefore a topology is a graph represent ing the transformations of the stream and each node in the topology executes in parallel a storm cluster consists of two kinds of working nodes as illustrated in fig they are only one master node and several worker nodes the master node and worker nodes implement two kinds of daemons nimbus and supervisor respectively the two daemons have similar functions with according jobtracker and tasktracker in map reduce framework nimbus is in charge of distributing code across the storm cluster scheduling works assigning tasks to worker nodes monitoring the whole system if there is a failure in the cluster the nimbus will detect it and re execute the corresponding task the super visor complies with tasks assigned by nimbus and starts or stops worker processes as necessary based on the instructions of nimbus the whole computational topology is partitioned and distributed to a number of worker processes each worker process implements a part of the topology how can nimbus and the supervisors work swimmingly and complete the job fast another kind of daemon called zookeeper play an important role to coordinate the system it records all states of the nimbus and supervisors on local disk 4 3 2 128 is a general purpose distributed scalable fault tolerant pluggable computing platform for processing contin uous unbounded streams of data 115 it was initially released by yahoo in and has become an apache incubator project since allows programmers to easily develop applications and possesses has several competitive properties including robustness decentralization scalability cluster management and extensibility the core platform of is written in java the implementation of a job is designed to be modular and pluggable for easily and dynamically processing large scale stream data also employs apache zookeeper to manage its cluster like storm does has been put to use in production systems at yahoo for processing thousands of search queries and good performances show up in other applications 4 3 3 sqlstream server sqlstream 5 is another big data platform that is designed for processing large scale streaming data in real time it fo cuses on intelligent and automatic operations of streaming big data sqlstream is appropriate to discovery patterns from large amounts of unstructured log file sensor network and other machine generated data the new release sqlstream ser ver 3 0 has good performances in real time data collection transformation and sharing which is in favor of real time big data management and analytics the standard sql language are still adopted in the underlying operations sqlstream works very fast as it uses in memory processing also called nodatabase technology the data will not be stored in the disks instead of the arriving data are regarded as streams and processed in memory using streaming sql que ries streaming sql is developed from strand sql by taking advantage of multi core computing and achieves massively par allel streaming data processing c l philip chen c y zhang information sciences 347 fig 12 big data platforms 4 3 4 splunk splunk is a real time and intelligent big data platform for exploiting informations from machine generated big data it has been used in many famous companies such as amazen heroku and senthub splunk combines the up to the moment cloud technologies and big data to help users to search monitor and analyze their machine generated data via a web inter face it exhibits the results in an intuitive way such as graphs reports and alerts splunk is designed to provide metrics for many application diagnose problems for system and it infrastructures and also provide intelligence for business operations splunk storm is a cloud version of splunk big data analytics splunk is very different from the other stream processing tools its peculiarities include indexing structured or unstruc tured machine generated data real time searching reporting analytical results and dashboards therefore log files are a great application for it 4 3 5 apache kafka kafka is a high throughput messaging system that was incipiently developed at linkedin it works as a tool to man age streaming and operational data via in memory analytical techniques for obtaining real time decision making as a dis tributed publish subscribe messaging system kafka has four main characteristics persistent messaging with o 1 disk structures high throughput support for distributed processing and support for parallel data load into hadoop it already has wide usages in a number of different companies as data pipelines and messaging tools in recent years activity and operational data play an important role to extract features of websites activity data is the record of various human actions on line such as webpage content copy content clicklist and searching key words it is valu able to log these activities out into canned file and aggregate them for subsequent analysis operational data is data to de scribe the performance of servers for instances cpu and io usage request times service logs etc the knowledge discovery of operational data is helpful for real time operation management kafka combines off line and on line processing to provide real time computation and produce ad hoc solution for these two kinds of data 4 3 6 sap hana sap hana is an in memory analytics platform that aims to provide real time analysis on business processes predic tive analysis and sentiment data processing sap hana database is the core part of the real time platform it is a little bit different from other database systems operational reporting data warehousing and predictive and text analysis on big data are three hana specific real time analytics sap hana works with very large scope of applications whether or not they are from sap such as demographics and social media interactions 4 4 big data tools based on interactive analysis in recent years open source big data systems have emerged to address the need not only for scalable batch processing and stream processing but also interactive analysis processing the interactive analysis presents the data in an interactive environment allowing users to undertake their own analysis of information user are directly connected to the computer and hence can interact with it in real time the data can be reviewed compared and analyzed in tabular or graphic format or both at the same time c l philip chen c y zhang information sciences 347 fig 13 lambda architecture c l philip chen c y zhang information sciences 347 4 4 1 google dremel in google proposed an interactive analysis system named dremel which is scalable for processing nested data dremel has a very different architecture compared with well known apache hadoop and acts as a successful comple ment of map reduce based computations it has capability to run aggregation queries over trillion row tables in seconds by means of combining multi level execution trees and columnar data layout the system scales to thousands of cpus and pet abytes of data and has thousands of users at google 4 4 2 apache drill apache drill is another distributed system for interactive analysis of big data 89 it is similar to google dremel for drill there are more flexibility to support a various of different query languages data formats and data sources like dremel drill is also specifically designed to efficiently exploit nested data it has an objective to scale up on 000 servers or more and reaches the capability to process petabyes of data and trillions of records in seconds drill and dremel are experts in large scale ad hoc querying of data they use hdfs for storage and the map reduce to perform batch analysis by searching data either stored in columnar form or within a distributed file system it is possible to scan over petabytes of data in seconds to response ad hoc queries drill can be viewed as the open source version of dre mel google also provides dremel as a service with its bigquery offering other companies can design their own big data tools according to their special usages every big data platform has its focus some of them are designed for batch processing some are good at real time ana lytic each big data platform also has specific functionality for example statistical analysis machine learning and data stream processing we use fig 12 to illustrate their disadvantages and advantages in which the capability of real time pro cessing increase response time decrease from left to right and handling capability of batch processing increase from bottom to up 5 principles for designing big data systems big data analytics are doomed to be more complicated than traditional data analysis systems how do we implement complex data intensive tasks with satisfactory efficiency especially in real time the answer is the capability to massively parallelize the analytical algorithms in such a way that all the processing happen entirely in memory and can linearly scale up and down on demand when trying to exploit big data we not only need to develop new technologies but also new thinking ways in designing big data analytics systems we summarize seven necessary principles to guide the development of this kind of burning issues big data analytics in a highly distributed system cannot be achievable without the following principles principle 1 good architectures and frameworks are necessary and on the top priority big data cannot be solved effectively and approvingly if there are no good and proper architecture for the whole big data systems in traditional information architecture data sources that use integration techniques to transfer data into a dbms data warehouse or operational data store and then offer a wide variety of analytical techniques to reveal the data then some organizations have applied oversight and standardization across projects and perhaps have matured the information architecture capability through managing it however big data systems need high level architecture than traditional one many distributed and parallel processing architectures have already been proposed to address big data problems there are distinct technology strategies for real time and batch processing requirements for real time key value data stores such as nosql allow for high performance index based retrieval for batch processing map reduce can be applied according to a specific data discovery strategy for different data intensive applications we should design different and appropriate architectures in the beginning of everything for example the lambda architecture solves the problem of computing arbitrary functions on arbitrary data in real time by decomposing the problem into three layers the batch layer the serving layer and the speed layer fig 13 but this architecture cannot fit all the big data applications principle 2 support a variety of analytical methods big data applications often produce complex tasks that make it impossible to be resolved by using one or a few of disciplines and analytical methods the modern data science constantly involves a wide range of subjects and approaches they range from data mining statistical analysis machine learning distributed programming and visualization to real time analysis in memory analysis and human computer interaction these methods are often synchronously employed in different big data platform principle 3 no size fits all when it comes to big data analytics there is no one size which can fit all solutions according to ibm latin america big data sales leader leonardo gonzález every big data tool has its own limitations but if users use the proper tools for different tasks they also can partially obtain significant benefits by using those tools as the information keeps increasing at 332 c l philip chen c y zhang information sciences 347 exponential rate today big data problem will surely become the small data set problem in the future therefore how can we deal with the big big data problems until now we cannot answer this question but this question will give us some directions when we try to design data intensive systems especially for real time analytics principle 4 bring the analysis to data as the size of big data set is extremely large it is unadvisable and infeasible to collect and move data to only one or several centers for analysis data driven analysis needs contrary analysis direction which needs to bring the analysis tasks to data sites in data intensive computation problems data is the driver not analytical human or machines together with systematic thinking this principle is also resonant with the following ones principle 5 processing must be distributable for in memory computation if the fourth principle holds as expected we naturally require the processing must be distributable since the analysis which will be carried out on different data sites must be distributed to data locations in memory analytic which probes data stored in ram rather than on disk as is traditionally the case is becoming popular because it speeds up the analysis process even as data volumes explode in memory analytic is also highly necessary for real time analytic with the development of hard disk drives there are almost no differences between memory and hard disk in i o speed thinking about that we believe that applications based on real time analytic will highly benefit from in memory analytic or in memory like analytic principle 6 data storage must be distributable for in memory storage as a great portion of big data problems involve with the data and the information is generated at different addresses and different time this principle is already met but for the case that data generated or accumulated at data center the data also need to be partitioned into a number of parts for in memory analytic the popular and potential technology cloud computing make the data storage in cloud this is very appulsive in the big data problem solving once the data and services are stored in the cloud users just like carry out their big data calculations on an unimaginative and powerful supercomputer the real infrastructures are hidden in the cloud therefore some data driven applications can be realized principle 7 coordination is needed between processing and data units to improve scalability as well as efficiency and fault tolerance of big data systems coordination between different processing units and data units on a cluster is highly necessary and essential that is why both storm and employ independent and specialized cluster management frameworks zookeeper to control the whole data process this principle guarantees the low latency of response which is particularly required in real time analytics 6 underlying technologies and future researches the advanced techniques and technologies for developing big data science is with the purpose of advancing and invent ing the more sophisticated and scientific methods of managing analyzing visualizing and exploiting informative knowledge from large diverse distributed and heterogeneous data sets the ultimate aims are to promote the development and inno vation of big data sciences finally to benefit economic and social evolutions in a level that is impossible before big data techniques and technologies should stimulate the development of new data analytic tools and algorithms and to facilitate scalable accessible and sustainable data infrastructure so as to increase understanding of human and social processes and interactions as we discussed the novel big data tools techniques and infrastructures will enable breakthrough discov eries and innovation in science engineering medicine commerce education and national security laying the foundations for competitiveness for many decades to come a paradigm shift in scientific investigation is on the way as novel mathematical and statistical techniques new data min ing tools advanced machine learning algorithms as well as other data analytical disciplines are well established in the fu ture consequently a number of agencies are developing big data strategies to facilitate their missions they focus on common interests in big data researches across the us national institutes of health and the us national science foundation in the following subsections we will discuss several ongoing or underlying techniques and technologies to harness big data including granular computing cloud computing biological computing systems and quantum computing 6 1 granular computing when we talk about big data the first property of it is its size as granular computing grc is a general computation theory for effectively using granules such as classes clusters subsets groups and intervals to build an efficient computa tional model for complex applications with huge amounts of data information and knowledge therefore it is very natural to employ granular computing techniques to explore big data intuitively granular computing can reduce the data size into different level of granularity under certain circumstances some big data problems can be readily solved in such way grc is a burgeoning conceptual and computing paradigm of knowledge discovery in some degree it has been motivated by the urgent need for efficient processing of big data although the concept of big data have not be proposed when grc is developing in fact grc leads a significant transform from the current machine centric to human centric approach to infor mation and knowledge theoretical foundations of granular computing are exceptionally sound and involve set theory such as interval box and ball fuzzy sets 71 rough sets and random sets linked together in a highly comprehensive treatment of this paradigm in piecewise interval approximation and granular box regression are discussed to conduct data analysis su and his co authors introduced a new structure of radial basis function networks rbfns that can suc cessfully model symbolic interval valued data if big data can be transform into respective symbolic data some algorithms in neural networks and machine learning come into play as well known grc is called by a joint name for a variety of algorithms rather than one exact algorithm that is called granular computing grc is concerned with constructing and processing carried out at different level of information granules the information represented by different level of granules show up distinct knowledge features and patterns where the irrelevant features are hidden and valuable ones are highlighted taking satellite images as a example the interests of re searches within the low resolution images may are the cloud patterns that present typhoons or other weather phenomena however in high resolution satellite images these large scale atmospheric phenomena are ignored and small targets ap pear such as a map of a city or a scene of a street the same is generally true for all data in different granularities of infor mation different features and patterns emerge hence based on this fact grc is significantly useful to design more effective machine learning algorithms and data mining approaches there are a few of types of granularity that are often adopted in data mining and machine learning including variable granulation variable transformation variable aggregation system granulation aggregation concept granulation compo nent analysis equivalence class granulation and component granulation as you known the information hidden in big data maybe will lose partially if the data size is reduced to small ones not all the big data applications can use the grc techniques as the process in fig it depends the confidence and accuracy of results the system required for example financial data in banks and government are very sensitive and require high accu racy in some special analysis and the sensor data generated by users in its need to be processed and responded one by one in these cases grc dose not work well and we need other solutions 6 2 cloud computing the development of virtualization technologies have made supercomputing more accessible and affordable powerful computing infrastructures hidden in virtualization software make systems to be like a true physical computer but with the flexible specification of details such as number of processors memory and disk size and operating system the use of these virtual computers is known as cloud computing which has been one of the most robust big data techniques the name of cloud computing comes from the use of a cloud shaped symbol as an abstraction for the complex infrastructure it contains in system diagrams it entrusts remote services with a user data software and computation the combination of virtual machines and large numbers of affordable processors has made it possible for internet based companies to invest in large scale computational clusters and advanced data storage systems as illustrated in fig cloud computing not only delivers applications and services over the internet it also has been extended to infrastructure as a service for example amazon and platform as a service such as google appengine and microsoft azure infrastructure vendors provide hardware and a software stack including operating system database middleware and perhaps single instance of a conventional application therefore it shows out illusion of infinite resources without up front cost and fine grained billing it leads to the utility computing i e pay as you go computing surprisingly the cloud computing options available today are already well matched to the major themes of need though some of us might not see it big data forms a framework for discussing cloud computing options depending on special need users can go into the marketplace and buy infrastructure services from providers like google and amazon software as a ser vice saas from a whole crew of companies starting at salesforce and proceeding through netsuite jobscience and zuora a list that is almost never ending another bonus brought by cloud environments is cloud storage which provides a possible tool for storing big data cloud storage have good extensibility and scalability in storing information as demon strated in fig cloud computing is a highly feasible technology and attract a large number of researchers to develop it and try to apply to big data problems usually we need to combine the distributed mapreduce and cloud computing to get an effective answer c l philip chen c y zhang information sciences 347 fig 14 grc can be a option in some degree for providing petabyte scale computing cloudview is a framework for storage processing and analysis of massive machine maintenance data in a cloud computing environment which is formulated using the map reduce model and reaches real time response in the authors extended map reduce filtering aggregation programming model in cloud environment and boosts the performance of complex analysis queries apart from its flexibility cloud computing addresses one of the challenges relating to transferring and sharing data be cause data sets and analysis results held in the cloud can be shared with others there are a few disadvantages in cloud computing the obvious one is the time and cost that are required to upload and download large quantities of data in the cloud environment otherwise it becomes more difficult to control over the distribution of the computation and the under lying hardware furthermore there are privacy concerns relating to the hosting of data sets on publicly accessible servers as well as issues related to storage of data from human studies 159 it is right to say that big data problems will push the cloud computing to a high level of development c l philip chen c y zhang information sciences 347 fig cloud computing logical diagram fig cloud storage 6 3 bio inspired computing human brain maybe can give a hand to help us to rethink the way we interact with big data it captures and processes myriad of sensory data received moment of every day in an efficient and robust way human brain manages around a thou sand tb of data and no neuron in the brain runs faster than 1 khz which is about the speed of a general pc in how ever human being does not feel heavy headed while our brain boots up that is because the biological computing system of our brain works in a distinct way compared with today computer our brain does not need to locate and view large files with complex information sets the information is partitioned and individually stored as simple data elements in the brain tissue the processing for information in human brain is also executed in highly distributed and parallel ways the multi located storage schema and synchronous parallel processing approaches make our brain working so fast and efficiently biological computing models illustrated in fig are better appropriate for big data because they have mechanisms with high efficiency to organize access and process data in ways that are more practical for the ranging and nearly infinite inputs we deal with every day for today technology all of information is locked away in backward style data collections that are fixed and unwieldy however if we can store all that information in a system which is modeled more on biology rather than traditional ways and then apply significant and increasing processing power and intelligent algorithms to ana lyze rather than just move it around mechanically then we have the possibility of generating and interacting with the world and the characters of supernatural computational intelligence which is inspired by nature is a set of computational methodologies and approaches to address complex real world problems we have reason to believe that computational systems can also be illuminated by biological systems biologically inspired computing maybe provides tools to solve big data problems from hardware design to soft ware design in analogy to nature bio inspired hardware systems can be classified as three axes phylogeny ontogeny and epigenesis in authors give a review an emerging engineering discipline to program cell behaviors by embedding synthetic gene networks that perform computation communications and signal processing wang and sun proposed a bio inspired cost minimization mechanism for data intensive service provision it utilizes bio inspired mechanisms to search and find the optimal data service solution considering cost of data management and service maintenance tadashi gave a review for biological communication molecular communication inspired by the cell and cell to cell commu nication the data transformation and the communication between different computing units in big data systems maybe borrow some useful ideas from cells in two hardware processing architecture for modeling large networks of lea ky integrate and fire neurons that integrate bio inspired neural processing models into real world control environments sergio demonstrated self synchronization mechanism which borrowed from biological systems as the basic tool for achieving globally optimal distributed decisions in a wireless sensor network biocomputers is inspired and developed by biological molecules such as dna and proteins to conduct computational cal culations involving storing retrieving and processing data a significant feature of biocomputer is that it integrates biolog ically derived materials to perform computational functions and receive intelligent and efficient performance as demonstrated in fig a biocomputer is composed of a pathway or series of metabolic pathways involving biological mate rials that are engineered to behave in a certain manner based upon the conditions as input of the system the resulting path way of reactions that takes place constitutes an output which is based on the engineering design of the biocomputer and can be interpreted as a form of computational analysis there are three kinds of distinguishable biocomputers including bio chemical computers biomechanical computers and bioelectronic computers once the big data technologies and techniques get mature enough the following information revolution will incredibly change the way we process data the computing systems become exponentially faster compared with current status and novel data storage systems using biological models provide smarter interactions inevitable data losses and ambiguity gen uine computational intelligence enables human like analysis of massive quantities of data it is true that the future con c l philip chen c y zhang information sciences 347 fig biology computing paradigm structed by bio inspired technologies are so remarkable that a large amount of funds and human resources are poured into related research activities 6 4 quantum computing a quantum computer has memory that is exponentially larger than its apparent physical size and can manipulate an expo nential set of inputs simultaneously it also can compute in the twilight zone of hilbert space this exponential improvement in computer systems might be possible and real powerful computer is emerging if a real quantum computer existed now we could solve problems that are exceptionally difficult on current computers of course including today big data prob lems although it is very hard to develop quantum computer the main technical difficulty in building a quantum computer could soon be the thing that makes it possible to build one for example d ware systems company developed their quan tum computer called d wave one with 128 qubits processor and d wave two with 512 qubits processor on and respectively in essence quantum computing is to harness and exploit the powerful laws of quantum mechanics to process infor mation in a traditional computer information is presented by long strings of bits which encode either a zero or a one dif c l philip chen c y zhang information sciences 347 fig 18 biocomputers source from 12 table a 3 sizes of data units name equals to size in bytes bit 1 bit 1 8 nibble 4 bits 1 2 byte 8 bits 1 kilobyte bytes megabyte kilobytes 1 048 gigabyte megabytes 1 073 terrabyte gigabytes 1 099 627 petabyte terrabytes 1 125 624 exabyte petabytes 1 152 504 846 zettabyte exabytes 1 180 411 424 yottabyte zettabytes 1 208 629 706 c l philip chen c y zhang information sciences 347 ferently a quantum computer uses quantum bits or qubits the difference between qubit and bit is that a qubit is a quantum system that encodes the zero and the one into two distinguishable quantum states because qubits behave quantumly we can capitalize on the phenomena of superposition and entanglement 2 for example qubits in quantum systems require 2 complex values to be stored in classical computer systems nielsen and chuang pointed out that trying to store all these complex numbers would not be possible on any conceivable classical computer many certain problems can be solved much faster by larger scale quantum computers compared with classical comput ers that is because that quantum algorithms such as simon algorithm shor algorithm and other algorithms for simu lating quantum systems are more efficient and faster than traditional ones quantum computation does not violate the church turing thesis as classical computers also can simulate an arbitrary quantum algorithm with unlimited resources despite quantum computing is still in a fledging period quantum computational operations have been executed under a small number of quantum bits in practical experiments as the theoretical research continues to be advanced indeed there are a number of university institutes national governments and military funding research groups are working on quantum computing studies to develop quantum computers for both civilian and national security purposes 7 conclusion as we have entered an era of big data which is the next frontier for innovation competition and productivity a new wave of scientific revolution is about to begin fortunately we will witness the coming technological leapfrogging in this survey paper we give a brief overview on big data problems including big data opportunities and challenges current techniques and technologies we also propose several potential techniques to solve the problem including cloud computing quantum computing and biological computing although those technologies are still under development we have confidence that in the coming future we will receive several great breakthroughs in those areas undoubtedly today and future big data prob lems will benefit from those progresses there is no doubt that big data analytics is still in the initial stage of development since existing big data techniques and tools are very limited to solve the real big data problems completely in which some of them even cannot be viewed as big data tools in the true sense therefore more scientific investments from both governments and enterprises should be poured into this scientific paradigm to capture huge values from big data from hardware to software we imminently require more advanced storage and i o techniques more favorable computer architectures more efficient data intensive techniques cloud computing social computing and biological computing etc and more progressive technologies big data platforms with sound architecture infrastructure approach and properties big data also means big systems big challenges and big profits so more research works in these sub fields are necessary to resolve it we are fortunately witnessing the birth and development of big data and no person can settle it alone human resources capital investments and creative ideas are fundamental components of development of big data acknowledgments this work was supported in part by the national basic research program of china under grant and the macau science and technology development fund under grant and university of macau multiyear re search grants appendix a sizes of data units see table a 3 appendix b wordcount java 1 package org myorg 2 3 import java io ioexception 4 import java util 5 6 import org apache hadoop fs path 7 import org apache hadoop conf 8 import org apache hadoop io 9 import org apache hadoop mapred import org apache hadoop util continued on next page 338 c l philip chen c y zhang information sciences 347 12 public class wordcount 13 14 public staticclass map extends mapreducebase implements mapper longwritable text text intwritable private final static intwritable one new intwritable 1 private text word new text 18 public void map longwritable key text value outputcollector text intwritabl output reporter reporter throws ioexception string line value tostring stringtokenizer tokenizer new stringtokenizer line while tokenizer hasmoretokens word set tokenizer nexttoken output collect word one public static class reduce extends mapreducebase implements reducer text intwritable text intwritable public void reduce text key iterator intwritable values outputcollector text intwritable output reporter reporter throws ioexception 30 int sum 0 while values hasnext sum values next get 34 output collect key new intwritable sum 37 public static void main string args throws exception 39 jobconf conf new jobconf wordcount class conf setjobname wordcount 41 42 conf setoutputkeyclass text class conf setoutputvalueclass intwritable class 45 conf setmapperclass map class conf setcombinerclass reduce class 47 conf setreducerclass reduce class 49 conf setinputformat textinputformat class conf setoutputformat textoutputformat class 51 fileinputformat setinputpaths conf new path args 0 53 fileoutputformat setoutputpath conf new path args 1 54 jobclient runjob conf 57 source from 58 c l philip chen c y zhang information sciences 347 appendix c big data vendors big data vendors vendor data location new york ny website http www com index php featured big data products hosted analytical platform for big data using big table type data structures for consolidation and analysis vendor location new york ny palo alto ca london great britain dublin ireland website http www com featured big data products commercial support and services for mongodb vendor acxiom location various global locations website http acxiom com featured big data products data analytics and processing with an emphasis on marketing data and services vendor amazon web services location global website http aws amazon com featured big data products provider of cloud based database storage processing and virtual networking services vendor aster data location san carlos ca website http www asterdata com featured big data products data analytic services using map reduce technology vendor calpont location frisco tx website http www calpont com featured big data products infinidb enterprise is a column sorted database that also provides massively parallel processing capabilities vendor cloudera location palo alto and san francisco ca website http www cloudera com featured big data products distributor of commercial implementation of apache hadoop with services and support vendor couchbase location mountain view ca website http www couchbase com featured big data products commercial sponsor of the couchbase server map reduce oriented database as well as apache couchdb and memcached vendor datameer location san mateo ca website http www datameer com featured big data products data visualization services for apache hadoop data stores vendor datasift location san francisco ca reading united kingdom website http datasift com featured big data products social media data analytical services licensed re syndicator of twitter vendor datastax location san mateo ca austin tx website http www datastax com featured big data products distributor of commercial implementation of apache cassandra with services and support continued on next page 340 c l philip chen c y zhang information sciences 347 vendor digital reasoning location franklin tn website http www digitalreasoning com featured big data products synthesys a hosted and local business intelligence data analysics tool vendor emc location various global locations website http www emc com featured big data products makers of greenplum a massively parallel processing data store analytics solution vendor esri location various global locations website http www esri com featured big data products gis data analytical services vendor feedzai location united kingdom website http www feedzai com featured big data products feedzai pulse a real time business intelligence appliance vendor hadapt location cambridge ma website http www hadapt com featured big data products data analytic services for apache hadoop data stores vendor hortonworks location sunnyvale ca website http hortonworks com featured big data products distributor of commercial implementation of apache hadoop with services and support vendor hpcc systems location alpharetta ga website http hpccsystems com featured big data products hpcc high performance computing cluster an open source massive parallel processing computing database vendor ibm location various global locations website http www ibm com featured big data products hardware data analytical services and a massive parallel processing database vendor impetus location san jose ca noida india indore india bangalore india website http impetus com featured big data products data analytic and management services for apache hadoop data stores vendor infobright location toronto on dublin ireland chicago il website http www infobright com featured big data products infobright a column store database with services and support vendor jaspersoft location various global locations website http www jaspersoft com featured big data products data analytic services for apache hadoop data stores vendor karmasphere location cupertino ca website http www karmasphere com featured big data products data analytic and development services for apache hadoop data stores vendor lucid imagination location redwood city ca website http www lucidimagination com c l philip chen c y zhang information sciences 2014 347 341 featured big data products distributor of commercial implementation of apache lucene and apache solr with services and support provider of lucidworks enterprise search software vendor mapr technologies location san jose ca hyderabad india website http www mapr com featured big data products distributor of commercial implementation of apache hadoop with services and support vendor marklogic location various global locations website http www marklogic com featured big data products data analyic and visualization services vendor netezza corp location various global locations website http www netezza com featured big data products massively parallel processing data appliances analytic services vendor oracle location various global locations website http www oracle com featured big data products various hardware and software offerings including big data appliance mysql cluster exadata database machine vendor paraccel location campbell ca san diego ca wokingham united kingdom website http www paraccel com featured big data products data analyics using column store technology vendor pentaho location various global locations website http www pentaho com featured big data products data analytic services for apache hadoop data stores vendor pervasive software location austin tx website http www pervasive com featured big data products data analytic services for apache hadoop data stores based on hive vendor platform computing location various global locations website http www platform com featured big data products distributor of commercial implementation of apache hadoop with services and support vendor rackspace location global website http www rackspace com featured big data products provider of cloud based database storage and processing services vendor revolution analytics location palo alto ca seattle wa website http www revolutionanalytics com featured big data products data analytic and visualization services using r based software vendor splunk location various global locations website http www splunk com featured big data products data analytic and visualization services using logging oriented software vendor tableau software location seattle wa kirkland wa san mateo ca surrey united kingdom paris france website http www tableausoftware com featured big data products business intelligence and data analytic software continued on next page since its original publication utaut has served as a base line model and has been applied to the study of a variety of technologies in both organizational and non organizational settings there have been many applications and replications of the entire model or part of the model in organizational settings that have contributed to fortifying its generalizability e g neufeld et al there are three broad types of utaut extensions integrations the first type of extension integration examined utaut in new contexts such as new technologies e g collaborative technology health informa tion systems chang et al new user populations e g healthcare professionals consumers yi et al and new cultural settings e g china india gupta et al the second type is the addition of new constructs in order to expand the scope of the endogenous theoretical mechanisms outlined in utaut e g chan et al sun et al finally the third type is the inclusion of exogenous predictors of the utaut variables e g neufeld et al yi et al these extensive replications applications and exten sions integrations of utaut have been valuable in expanding our understanding of technology adoption and extending the theoretical boundaries of the theory however our review of this body of work revealed that most studies using utaut employed only a subset of the constructs particularly by dropping the moderators see al gahtani et al armida thus while the various studies con tribute to understanding the utility of utaut in different contexts there is still the need for a systematic investigation and theorizing of the salient factors that would apply to a consumer technology use context building on the past extensions to utaut the objective of our work is to pay particular attention to the consumer use context and develop compared to general theories in more recent years theories that focus on a specific context and identify relevant predictors and mechanisms are con sidered to be vital in providing a rich understanding of a focal phenomenon and to meaningfully extend theories speci fically both johns and alvesson and kärreman note that new contexts can result in several types of important changes in theories such as rendering originally theorized relationships to be nonsignificant changing the direction of relationships altering the magnitude of relationships and creating new relationships each change can reveal the break down of theories that results in the creation of new knowledge alvesson and kärreman in the case of utaut which was originally developed to explain employee tech nology acceptance and use it will be critical to examine how it can be extended to other contexts such as the context of consumer technologies which is a multibillion dollar industry given the number of technology devices applications and services targeted at consumers stofega and llamas 158 mis quarterly vol no 1 march against this backdrop the study of the boundary conditions and extensions to utaut in a consumer context represents an opportunity to make an important theoretical contribution specifically in the context of technology adoption detractors and proponents of models such as the technology acceptance model там have noted the need to expand the space of theoretical mechanisms see bagozzi benbasat and barki venkatesh et al this paper presents by identifying key additional constructs and relationships to be integrated into utaut thus tailoring it to a consumer use context in keeping with the general ideas outlined by alvesson and kärreman and by johns about how to extend a theory by lever aging a new context and the ideas presented in the journal of the ais special issue on там e g bagozzi venkatesh et al we accomplish this goal by 1 identifying three key constructs from prior research on both general adoption and use of technologies and consumer adoption and use of technologies 2 altering some of the existing relationships in the original conceptualization of utaut and 3 introducing new relationships first both consumer behavior and is research have theorized and found various constructs related to hedonic motivation e g enjoyment are important in consumer product and or technology use e g brown and venkatesh holbrook and hirschman nysveen et al van der heijden integrating hedonic motiva tion will complement utaut strongest predictor that emphasizes utility second in consumer contexts unlike workplace contexts users are responsible for the costs and such costs besides being important can dominate consumer adoption decisions see brown and venkatesh chan et al coulter and coulter dodds et al adding a construct related to price cost will complement utaut existing resource considerations that focus only on time and effort finally recent work has challenged the role of behavioral intention as the key predictor of technology use and introduced a new theoretical construct i e habit as another critical predictor of technology use e g davis and venkatesh kim and malhotra kim et al limayem et al integrating habit into utaut will complement the theory focus on intentionality as the overarching mechanism and key driver of behavior in fact habit as a key alternative mechanism has been lauded as a valuable next step in the jais special issue on там the collection of these works examining the role of habit albeit operationalized differently in each of the papers concludes that habit has a direct effect on technology use and or habit weakens or limits the strength of the relationship between behavioral intention and technology use such an integration of multiple streams of work to shed light on phenomena of interest is important from a scientific standpoint gioia and this content downloaded from 244 5 34 on mon jul 01 utc all use subject to http about jstor org terms venkatesh et al consumer acceptance and use of it petri greenwood beyond these changes relative sumer context then we discuss the new constructs added to to the original ut aut conceptualization we will drop extend utaut i e hedonic motivation price value and voluntariness which is one of the moderators and add a link habit to formulate between facilitating conditions moderated by age gender and experience and behavioral intention we will also include moderated relationships moderated by age gender unified theory of acceptance and use of and experience per the original ut aut pertaining to the technology utaut three new constructs based on a review of the extant literature venkatesh et al this work is expected to make important theoretical and developed utaut as a comprehensive synthesis of managerial contributions it sits at the confluence of several prior technology acceptance research utaut has four key sub streams related to technology acceptance and use constructs i e performance expectancy effort expectancy research там and utaut e g venkatesh et al social influence and facilitating conditions that influence extensions to там e g van der heijden questions behavioral intention to use a technology and or technology and criticisms about там e g benbasat and barki use we adapt these constructs and definitions from utaut venkatesh et al technology use e g burton jones to the consumer technology acceptance and use context and straub and is continuance e g bhattacherjee here performance expectancy is defined as the degree to hong et al thong et al and habit e g which using a technology will provide benefits to consumers limayem et al by building on and extending prior in performing certain activities effort expectancy is the work within this broad stream we expect to make three key degree of ease associated with consumers use of technology contributions first by incorporating three salient constructs social influence is the extent to which consumers perceive into utaut we expand the overall nomological network that important others e g family and friends believe they related to technology use the importance of the habit exten should use a particular technology and facilitating conditions sion for instance is even endorsed by detractors such as refer to consumers perceptions of the resources and support benbasat and barki who noted that it has been largely available to perform a behavior e g brown and venkatesh overlooked in this stream of work more broadly both venkatesh et al according to utaut perfor bagozzi and venkatesh et al have called for mance expectancy effort expectancy and social influence are alternative theoretical mechanisms in order to foster progress theorized to influence behavioral intention to use a tech in this mature stream of work the integration of hedonic nology while behavioral intention and facilitating conditions motivation price value and habit brings such new mech determine technology use also individual difference vari anisms i e affect monetary constraints and automaticky ables namely age gender and experience note that we drop tied to the new constructs into the largely cognition and voluntariness which is part of the original utaut 2 are intention based utaut second by adapting and extending theorized to moderate various utaut relationships the utaut to include new constructs and altering existing lighter lines in figure 1 show the original utaut along with relationships this work furthers the generalizability of the one modification noted above that was necessary to make utaut to a different context i e consumer it that is an the theory applicable to this context important step to advance a theory see alvesson and kärreman johns finally from a practical stand point the rich understanding gained can help organizations in the consumer technology industry better design and market to the original conceptualization of utaut we drop voluntariness technologies to consumers in various demographic groups at as a moderating variable this change is necessary to make utaut appli various stages of the use curve cable in the context of a voluntary behavior such as the one we are studying i e voluntary technology acceptance and use among consumers while in general voluntariness can be perceived as a continuum from absolutely mandatory to absolutely voluntary consumers have no organizational man theory date and thus most consumer behaviors are completely voluntary resulting in no variance in the voluntariness construct thus we drop voluntariness as background a relevant construct from the model this will only affect one relationship i e the social influence behavioral intention relationship this social influence to behavioral intention relationship thus reduces to a four way in this section we present an overview of the unified theory interaction effect of social influence x gender x age x experience on of acceptance and use of technology utaut and explain the basic modifications we make to fit utaut to the con behavioral intention instead of the original five way interaction in utaut there is evidence of such a four way interaction in the voluntary users sub sample in the split sample analysis reported in morris et al mis quarterly vol no 1 march 159 this content downloaded from 244 5 34 on mon jul 01 utc all use subject to http about jstor org terms venkatesh et ai consumer acceptance and use of it in order to examine the prior research on ut aut we reviewed papers published in the ais senior scholars basket of eight journals and then expanded our search to include other journals and conference proceedings this led us to over 500 articles that we then carefully examined for patterns we found that many of the articles cited the original ut aut article as a general reference to the body of work on adoption and neither did they apply nor extend ut aut our review and synthesis confirm that there has been some work in furthering ut aut despite these contributions it is worth noting that most published studies have only studied a subset of the ut aut constructs the extensions particularly the addition of new constructs have been helpful to expand the theoretical horizons of ut aut however the addition of constructs has been on an ad hoc basis without careful theo retical consideration to the context being studied and the works have not necessarily attempted to systematically choose theoretically complementary mechanisms to what is already captured in ut aut such complementary constructs can help expand the scope and generalizability of ut aut identifying constructs to incorporate into utaut building on our discussion in the introduction here we present an overview of the three constructs we add to utaut and discuss the details of the three constructs we adopt an approach that complements the current constructs in utaut first utaut takes an approach that emphasizes the impor tance of utilitarian value extrinsic motivation the construct tied to utility namely performance expectancy has consis tently been shown to be the strongest predictor of behavioral intention see venkatesh et al complementing this perspective from motivation theory is intrinsic or hedonic motivation vallerand hedonic motivation has been included as a key predictor in much consumer behavior research holbrook and hirschman and prior is research in the consumer technology use context brown and venkatesh second from the perspective of effort expectancy in organizational settings employees assess time and effort in forming views about the overall effort associated mis quarterly vol no 1 march this content downloaded from 244 5 34 on mon jul 01 utc all use subject to http about jstor org terms venkatesh et al consumer acceptance and use of it with the acceptance and use of technologies in a consumer cost for using them dodds et al the price value is technology use context price is also an important factor as positive when the benefits of using a technology are perceived unlike workplace technologies consumers have to bear the to be greater than the monetary cost and such price value has costs associated with the purchase of devices and services a positive impact on intention thus we add price value as a consistent with this argument much consumer behavior predictor of behavioral intention to use a technology research has included constructs related to cost to explain consumers actions dodds et al finally ut aut and related models hinge on intentionality as a key underlying experience and habit theoretical mechanism that drives behavior many including detractors of this class of models have argued that the inclu finally we add habit to utaut prior research on tech sion of additional theoretical mechanisms is important in a nology use has introduced two related yet distinct constructs use rather than initial acceptance context habit has been namely experience and habit experience as conceptualized shown to be a critical factor predicting technology use e g in prior research e g kim and malhotra venkatesh et kim and malhotra kim et al limayem et al al reflects an opportunity to use a target technology based on the above gaps in utaut and the asso and is typically operationalized as the passage of time from ciated theoretical explanation provided we integrate hedonic the initial use of a technology by an individual for instance motivation price value and habit into utaut in order to kim et al measure has five categories with different tailor it to the consumer technology use context periods of experience venkatesh et al operationa lized experience as three levels based on passage of time post training was when the system was initially available for hedonic motivation use 1 month later and 3 months later habit has been defined as the extent to which people tend to perform hedonic motivation is defined as the fun or pleasure derived behaviors automatically because of learning limayem et al from using a technology and it has been shown to play an while kim et al equate habit with automaticity important role in determining technology acceptance and although use conceptualized rather similarly habit has been brown and venkatesh in is research such hedonic operationalized in two distinct ways first habit is viewed as motivation conceptualized as perceived enjoyment has prior been behavior see kim and malhotra and second found to influence technology acceptance and use directly habit is measured as the extent to which an individual believes e g van der heijden thong et al in the con the behavior to be automatic e g limayem et al sumer context hedonic motivation has also been found consequently to be there are at least two key distinctions between an important determinant of technology acceptance and experience use and habit one distinction is that experience is a e g brown and venkatesh childers et al necessary but not sufficient condition for the formation of thus we add hedonic motivation as a predictor of consumers habit a second distinction is that the passage of chrono behavioral intention to use a technology logical time i e experience can result in the formation of differing levels of habit depending on the extent of interaction and familiarity that is developed with a target technology f or price value instance in a specific period of time say 3 months different individuals can form different levels of habit depending on an important difference between a consumer use setting their and use of a target technology this is perhaps what the organizational use setting where utaut was developed prompted limayem et al to include prior use as a is that consumers usually bear the monetary cost of such predictor use of habit and likewise kim and malhotra whereas employees do not the cost and pricing structure controlled for experience with the target technology in their may have a significant impact on consumers technology attempt use to understand the impact of habit on technology use for instance there is evidence that the popularity of ajzen short and fishbein also noted that feedback from messaging services sms in china is due to the low previous pricing experiences will influence various beliefs and of sms relative to other types of mobile internet applications consequently future behavioral performance in this context chan et al in marketing research the monetary habit cost is a perceptual construct that reflects the results of prior price is usually conceptualized together with the quality experiences of products or services to determine the perceived value of pro ducts or services zeithaml we follow these ideas the empirical and findings about the role of habit in technology define price value as consumers cognitive tradeoff between use have delineated different underlying processes by which the perceived benefits of the applications and the monetary habit influences technology use related to the operation mis quarterly vol no 1 march 161 this content downloaded from 244 5 34 on mon jul 01 utc all use subject to http about jstor org terms venkatesh et ai consumer acceptance and use of it alization of habit as prior use kim and malhotra found that prior use was a strong predictor of future technology use given that there are detractors to the operationalization of habit as prior use see ajzen some work such as that of limayem et al has embraced a survey and perception based approach to the measurement of habit such an operationalization of habit has been shown to have a direct effect on technology use over and above the effect of intention and also to moderate the effect of intention on technology use such that intention is less important with increasing habit limayem et al similar findings in the context of other behaviors have been reported in psychology research see ouellette and wood in this work we adopt the above discussed conceptual defini tions of experience and habit as we will also note later we operationalize experience in keeping with much prior research as the passage of time from the initial use of a target tech nology and we operationalize habit in keeping with limayem et al as a self reported perception hypothesis development in this section we present the hypotheses that we incorporate to extend ut aut to the consumer context figure 1 shows the original utaut and our proposed extensions impact of facilitating conditions moderated by age gender and experience the first change that we make to tailor utaut to the con sumer technology use context is the addition of a direct rela tionship from facilitating conditions to behavioral intention over and above the existing relationship between facilitating conditions and technology use in utaut facilitating condi tions is hypothesized to influence technology use directly based on the idea that in an organizational environment facilitating conditions can serve as the proxy for actual behavioral control and influence behavior directly ajzen this is because many aspects of facilitating condi tions such as training and support provided will be freely available within an organization and fairly invariant across users in contrast the facilitation in the environment that is available to each consumer can vary significantly across application vendors technology generations mobile devices and so on in this context facilitating conditions will act more like perceived behavioral control in the theory of planned behavior tpb and influence both intention and behavior ajzen specifically a consumer who has access to a favorable set of facilitating conditions is more mis quarterly vol no 1 march likely to have a higher intention to use a technology for instance if we were to consider mobile internet consumers have different levels of access to information and other resources that facilitate their use such as online tutorials in general all things being equal a consumer with a lower level of facilitating conditions will have lower intention to use mobile internet also consumers with different phones may experience different rates of data transfer and consequently have different levels of intention to use mobile internet thus in the consumer context we follow the general model of tpb and link facilitating conditions to both behavioral intention and behavior we expect the effect of facilitating conditions on behavioral intention to be moderated by age gender and experience older consumers tend to face more difficulty in processing new or complex information thus affecting their learning of new technologies morris et al plude and hoy er this difficulty may be attributed to the decline in cognitive and memory capabilities associated with the aging process posner hence compared to younger con sumers older consumers tend to place greater importance on the availability of adequate support hall and mansfield moreover men more than women are willing to spend more effort to overcome different constraints and diffi culties to pursue their goals with women tending to focus more on the magnitude of effort involved and the process to achieve their objectives henning and jardim rotter and portugal venkatesh and morris thus men tend to rely less on facilitating conditions when considering use of a new technology whereas women tend to place greater emphasis on external supporting factors this can also be explained partly by the cognitions related to gender roles in society where men tend to be more task oriented e g lynott and mccandless experience can also moderate the relationship between facilitating conditions and behavioral intention greater experience can lead to greater familiarity with the technology and better knowledge structures to facili tate user learning thus reducing user dependence on external support alba and hutchinson likewise a meta analysis showed that users with less experience or familiarity will depend more on facilitating conditions notani moreover gender age and experience have a joint impact on the link between facilitating conditions and intention gender differences in task orientation and emphasis on instrumen tality will become more pronounced with increasing age morris et al as people become older particularly from teenagers to adults the differentiation of their gender roles will be more significant thus older women will place more of an emphasis on facilitating conditions indeed there is empirical evidence that gender differences in the impor this content downloaded from 244 5 34 on mon jul 01 utc all use subject to http about jstor org terms venkatesh et al consumer acceptance and use of it tance of facilitating conditions become more pronounced with nology use decisions consequently the moderating effect of increasing age morris et al venkatesh et al in experience will differ across age and gender thus we concert with age and gender experience can further moderate hypothesize the relationship between facilitating conditions and behavioral intention this is because when consumers have not devel age gender and experience will moderate the effect of oped their knowledge and skills i e when they have less hedonic motivation on behavioral intention such that the experience the impacts of age and gender on consumer effect will be stronger among younger men in early learning will be more significant than when they have stages of experience with a technology acquired enough knowledge or expertise about the technology i e when they have more experience the dependence on facilitating conditions is of greater importance to older impact of price value moderated by women in the early stages of technology use because as age and gender discussed earlier they place greater emphasis on reducing the learning effort required in using new technology thus we we expect the effect of price value on behavioral intention to hypothesize be moderated by age and gender again we draw from theories about social roles e g bakan deaux and hi age gender and experience will moderate the effect of lewis 1 984 in theorizing about the differential importance of facilitating conditions on behavioral intention such that price value among men versus women and among younger the effect will be stronger among older women in early versus older individuals this literature suggests that men and stages of experience with a technology women typically take on different social roles and exhibit different role behaviors particularly men tend to be indepen dent competitive and make decisions based on selective impact of hedonic motivation moderated by information and heuristics while women are more inter age gender and experience dependent cooperative and consider more details bakan deaux and kite consequently in a consumer we expect the effect of hedonic motivation on behavioral context women are likely to pay more attention to the prices intention to be moderated by age gender and experience due of products and services and will be more cost conscious than to differences in consumers innovativeness novelty seeking men further women are typically more involved in pur and perceptions of novelty of a target technology innova chasing and thus more responsible and careful with money tiveness is the degree to which an individual is receptive to than men are slama and tashchian given the new ideas and makes innovation decisions independently penchant of men to play with technologies the price value midgley and dowling p novelty seeking is the assigned by men to technologies will likely be higher than the tendency of an individual to seek out novel information or value assigned by women to the same technologies more stimuli hirschman such innovativeness and novelty over this gender difference induced by social role stereotypes seeking can add to the hedonic motivation to use any product will be amplified with aging because older women are more holbrook and hirschman when consumers begin to likely to engage in such activities as taking care of their use a particular technology they will pay more attention to its families deaux and lewis 1 984 thus older women will be novelty e g the new interface and functionality of iphone more price sensitive due to their social role as gatekeepers of and may even use it for the novelty holbrook and hirschman family expenditures this implies that the monetary value of as experience increases the attractiveness of the products and services bears greater importance to older novelty that contributes to the effect of hedonic motivation on women thus we hypothesize technology use will diminish and consumers will use the technology for more pragmatic purposes such as gains in age and gender will moderate the effect of price value on efficiency or effectiveness thus hedonic motivation will behavioral intention such that the effect will be stronger play a less important role in determining technology use with among women particularly older women increasing experience further age and gender have been found to be associated with consumer technology innovative ness lee et al 1 0 in the early stages of using a new tech impacts of habit moderated by nology younger men tend to exhibit a greater tendency to age gender and experience seek novelty and innovativeness e g chau and hui this greater tendency will in turn increase the relative impor the issue of whether the effect of habit operates directly on tance of hedonic motivation in younger men early tech behavior or through behavioral intention has been extensively mis quarterly vol no 1 march 163 this content downloaded from 244 5 34 on mon jul 01 utc all use subject to http about jstor org terms venkatesh et ai consumer acceptance and use of it discussed in prior research e g aarts and dijksterhuis ajzen kim et al in the current work we follow the naming convention by kim et al referring to the habituation proposition as the habit automaticity perspective hap and the one consistent with tpb as the instant activa tion perspective iap staying faithful to tpb iap assumes that repeated performance of a behavior can result in well established attitudes and intentions that can be triggered by attitude objects or cues in the environment ajzen and fishbein once activated attitudes and intentions will automatically guide behavior without the need for conscious mental activities such as belief formation or retrieval fazio for instance after an extended period of repeated checking of e mail on mobile devices during commuting a consumer may have developed a positive view toward mobile internet technology e g checking e mail using mobile internet during commuting is useful and an associated behavioral intention e g i will check e mail using mobile internet during my commute this intention is thus stored in the conscious mind of the consumer when entering a car or taxi the environment or context can spontaneously trigger the positive view and intention that in turn results in the behavior e g pulling out the mobile device and checking e mail following this line of reasoning stronger habit will lead to a stored intention that in turn will influence behavior in contrast the hap assumes that repeated performance of a behavior produces habituation and behavior can be activated directly by stimulus cues ouellette and wood 1 998 ronis et al verplanken et al on future occasions being in a similar situation is sufficient to trigger the automatic response without conscious cognitive mediation i e attitude or intention unlike the iap the hap suggests that habit is established mainly through the reinforcement of the stimulus action link similar to that in conditioning ajzen for instance if habit is established as hap suggests a consumer will without thinking react immediately to the context of entering a subway car or taxi by pulling out his her mobile phone and check e mail here the context cue i e trans portation vehicle has been directly associated with the action i e checking e mail on a mobile device and no attitudes or intentions are involved thus the key difference between the iap and the hap is whether conscious cognitive processing for the makeup of intention is involved between the stimulus and the action as we have discussed while there are competing perspectives on how habit affects behavior there is some agreement at an abstract level that suggests a critical role played by informa tion and cue processing basically consumers need to first perceive and process the contextual cues from the environ 164 mis quarterly vol no 1 march ment once familiar cues are observed the association between the cues and the response either direct action or stored intention will be automatically established the behavior is performed as a result of the automatic association thus both the hap and the iap require a stable environment so long as the context remains relatively unchanged routinized behavior is performed in a largely automatic fashion with minimal conscious control ajzen however rapid change is the defining character of the environment especially in the consumer technology market mehrmann both the information appliances and the context in which consumers use them change rapidly and constantly for example mobile devices have evolved im mensely since both in design and function 3 from early analog models that could only be used to make phone calls to the latest mobile computing devices such as iphone that can take pictures and videos play videos and run one of the thousands of applications available from the apple app store consumer interaction with mobile devices has also changed dramatically from being mainly based on a phone paradigm in the early days to touch screens nowadays thus instead of a stable environment the environment surrounding consumer technology use is constantly changing in this regard the triggering process of habit i e cue pro cessing and association becomes important in determining the subsequent effects of habit on either behavioral intention or use if consumers perceive the changing environment as relatively stable the association between the stimulus cues and intentions or actions can be established and triggered if not consumer behavior may be less or not subject to the control of habit here individual differences in information processing and association in memory may play an important role in moderating the effect of habit if a consumer is less sensitive to changes in the context or has less tendency cognitive capacity to process environmental information in a controlled and detailed manner he or she will depend more on established habit to guide his or her behavior verplanken and wood for instance when in a subway car where the environmental cues keep changing consumers who are more sensitive to the changes in the environment will be less likely to maintain their old behavioral pattern related to the use of a mobile device to access the internet e g they may be distracted by people around them and may not use their blackberry devices to read e mail while in the subway car in contrast consumers who are less aware of the environment will tend to ignore the variety of environmental cues and stick www webdesignerdepot com 05 the evolution of cell phone design between 1 983 this content downloaded from 244 5 34 on mon jul 01 utc all use subject to http about jstor org terms venkatesh et ai consumer acceptance and use of it to their routinized behavior i e always checking e mail judgments or decisions e g farina meyers levy and using their blackberry devices upon entering a subway car tybout this is mainly due to the fact that men tend to process stimuli and information in a schema based manner in sum there are two causal pathways by which habit and tend to ignore some relevant details while women tend to ultimately influences use both hinge on information and cue process information in a piece meal and more detailed manner processing across individuals we expect both pathways to meyers levy and maheswaran thus it follows that be operational to varying extents we next discuss three women will be more sensitive to new cues or cue changes in individual difference variables that we expect to affect con the environment and pay attention to such changes that will in sumers cue processing and association process thus moder turn weaken the effect of habit on intention or behavior ating the effects of habit on behavioral intention and use finally experience will work in tandem with age and gender first experience mainly affects the strength of the association to moderate the effect of habit on behavior the strength between contextual cues and intention or behavior the ening effect of experience on habit varies across different relationship between experience and habit is formed and cohorts defined by age and gender as age increases gender strengthened as a result of repeated behavior limayem et al differences in learning about technologies from experience newell and rosenbloom habit is a learned out become more pronounced aging leads to a decreasing capa come and only after a relatively long period of extensive bility of information processing as women tend to process practice can it be stored in long term memory and override information in a more detailed and subtle manner than men do other behavior patterns lustig et al although it is darley and smith older men tend to rely more on possible for a habit to be formed through repetition in a short heuristics and schema acquired from usage experiences to period of time the longer the elapsed time the more oppor determine their behavioral intention paying little attention to tunities i e number of cue occurrences consumers have to environment cues therefore older men with more usage create an association between cues and behavior consumers experience will rely most on their habits again returning to with more experience of using a particular technology will our earlier example after forming the habit of checking develop a cognitive lock in that creates a barrier to behavioral mobile e mail that resulted from prior experience older men changes murray and haubl the response to cues then who use mobile e mail for a longer period of time will pay the becomes stronger with increasing experience with a tech least attention to most of the new cues or cue changes in the nology i e passage of time thus habit will have stronger subway car environment such as passengers entering leaving effect on intention and use for more experienced consumers the subway car and focus only on their habitual action of checking e mail on their mobile devices in contrast women second age and gender reflect people differences in infor particularly younger women with less experience of using mation processing i e cue perception and processing pro mobile e mail are more likely to immediately notice changes cess that in turn can affect their reliance on habit to guide in their environment and pay attention to the cues this will behavior it has been found that older people tend to rely weaken the automatic association between the subway car largely on automatic information processing hasher and environment and checking e mail using the mobile device zacks jennings and jacoby with their habits thus decreasing the effect of habit on intention and the preventing or suppressing new learning lustig et al consequent behavior among younger women with less once older consumers have formed a habit by repeated use of experience in sum we expect the effect of habit to be a particular technology it is difficult for them to override their strongest among older men especially when they have habit to adapt to a changed environment in the earlier significant experience with a technology thus we example of habitual behavior of using their mobile devices to hypothesize check e mail when entering a subway car older people are less likely to be distracted by changes in the subway car than a age gender and experience will moderate the effect younger people and will revert to their habitual action of of habit on behavioral intention such that the effect checking e mail using their mobile devices moreover will be stronger for older men with high levels of gender differences will further moderate the effect of habit experience with the technology research has shown that women tend to pay more attention to details and elaborate on details in their messages than men do b age gender and experience will moderate the effect e g gilligan krugman in the context of con of habit on technology use such that the effect will sumer decision making women have been found to exhibit be stronger for older men with high levels of greater sensitivity to details than men exhibit when making experience with the technology mis quarterly vol no 1 march 165 this content downloaded from 244 5 34 on mon jul 01 utc all use subject to http about jstor org terms venkatesh et ai consumer acceptance and use of it impact of behavioral intention moderated by experience with increasing experience consumers have more oppor tunities to reinforce their habit because they have more time to encounter the cues and perform the associated behavior kim and malhotra with increasing experience rou tine behavior becomes automatic and is guided more by the associated cues jasperson et al as a result the effect of behavioral intention on technology use will decrease as experience increases studies in psychology have found that experience can moderate the effect of behavioral intention on behavior for example verplanken et al showed in a field study that the frequency of car use reduces the effect of behavioral intention on future car use following the hap rationale greater usage experience implies more opportunities to strengthen the link between cues and behavior which then facilitates habitualization ouellette and wood and weakens the link between behavioral intention and use kim et al thus we hypothesize experience will moderate the effect of behavioral intention on use such that the effect will be stronger for consumers with less experience method mobile internet technology our target population was the current users of mobile internet technology our study was conducted in hong kong in the context of consumer use of mobile internet technology mobile internet supports an assortment of digital data services that can be accessed using a mobile device over a wide geo graphic area mobile internet enables people to exchange messages pictures and e mail check flight schedules book concert tickets and enjoy games while on the road in a consumer context the use of mobile internet is a voluntary decision measurement all of the scales were adapted from prior research the items are included in the appendix the scales for the ut aut constructs i e performance expectancy effort expectancy social influence facilitating conditions and behavioral inten tion were adapted from venkatesh et al the habit scale was drawn from limayem and hirt the scale for mis quarterly vol no 1 march hedonic motivation was adapted from kim et al and the price value scale was adapted from dodds et al all items were measured using a seven point likert scale with the anchors being strongly disagree and strongly agree age was measured in years gender was coded using a 0 or 1 dummy variable where 0 represented women experience was measured in months use was measured as a formative composite index of both variety and frequency of mobile internet use a list of six popular mobile internet applications in hong kong was provided and respondents were asked to indicate their usage frequency for each application the anchors of the seven point scale ranged from never to many times per day according to sharma et al our measurement of technology use essentially con sists of behavior anchored scales that may be subject to relatively high common method variance cmv that is high item characteristics effects however as also noted in sharma et al the temporal separation between two measures can reduce the effect of cmv that is low measure ment context effects as we measured use four months after we obtained the data for the key predictors the overall impact of cmv is reduced we created a questionnaire in english that was reviewed for content validity by a group of university staff and a group of is academics as the questionnaire was administered in chinese the language used predominantly by the local resi dents in hong kong we translated the english questionnaire to chinese and then back to english to ensure translation equivalence brislin 1 970 a professional translator and two research assistants independently translated the original items in english into chinese they analyzed the independently translated chinese versions of the items and came to an agree ment on the final version for the questionnaire the question naire was then translated back into english by another professional translator to confirm translation equivalence the questionnaire was pilot tested among a group of con sumers who were not included in the main survey we found preliminary evidence that the scales were reliable and valid participants and data collection procedure in hong kong had a mobile phone penetration rate of over 100 percent this high penetration rate suggests that every resident in hong kong is a potential consumer of mobile internet the diffusion rate of mobile internet in hong kong reached percent in to reach out to as many residents as possible we conducted an online survey through a popular web portal this web portal pro vides residents with a wide array of e government services this content downloaded from 244 5 34 on mon jul 01 utc all use subject to http about jstor org terms venkatesh et ai consumer acceptance and use of it such as filing tax returns booking public facilities checking nant validity with two exceptions one performance expec traffic information appointment booking for various govern tancy item and one habit item were deleted due to their low ment services and renewal of driving licenses loadings and high cross loadings use which was modeled using six formative indicators had weights between we conducted a two stage online survey during the first and stage we collected data on the exogenous variables and intention to use mobile internet a banner advertisement for the survey was placed on the web portal for four weeks as structural model an incentive respondents were entered into a lucky draw to win various prizes to eliminate respondents who partici we used two methods to assess cmv we first followed the pated in the survey more than once they were required to approach of liang et al using pls we specified a provide their mobile phone number and identity card number method factor together with the original latent variables in the later those respondents with repeated entries were dropped measurement model and calculated the squared factor from data analysis there were 4 1 27 valid respondents to the loadings for both the method factor and the substantive factors first stage of the online survey in the second stage of the i e original latent variables the average variance ex online survey we contacted the previous respondents four plained by the substantive factors was around 0 while that months later to collect their mobile internet use we received by the method factor was under 02 thus suggesting that com 2 220 responses to the second stage of the online survey as mon method bias is not a concern in our study next we only current users of mobile internet could respond to ques followed richardson et al suggestion of the cfa tions about habit and experience we removed the respondents marker technique that involves the addition of a theoretically with no prior experience of mobile internet leaving us with irrelevant marker variable in the analysis see also lindell and a final sample of 1 512 consumers women to test for whitney malhotra et al we followed malhotra nonresponse bias we compared the demographic character et al approach for the post hoc estimation of cmv istics of the respondents in the two waves of data collection and found no significant differences likewise a comparison of the demographic characteristics of the respondents and the and chose the second smallest positive correlation between two manifest variables 0 02 as a conservative estimate after the deduction of this value from all correlations we nonrespondents in the second wave showed no significant reran our analysis no significant difference was found be differences tween the original correlation estimates and the adjusted ones thus this test also showed that cmv is less of a concern in our study results we examined the correlation table for evidence of multi collinearity among the exogenous constructs see table 2 we used partial least squares pls to test our model because the highest correlation between the exogenous constructs was we capable have of quite testing a number these effects of interaction chin et al terms and using terms 0 58 pls the is to the reduce variables multicollinearity used to create interaction among the terms interaction were smart pls software we first examined the measurement mean centered before creating the interaction terms jaccard various model structural to assess models reliability and validity before testing et original the al utaut this paper method venkatesh is consistent et al with that to used further in the test for multicollinearity we computed variance inflation factors vifs and they were found to be around 4 and less measurement model than the conservative threshold of 5 thus suggesting that multicollinearity was not a major issue in our study tables 1 and 2 present the measurement model we results ran four separate models to test the support for baseline including information about reliability validity correlations utaut direct effects only baseline utaut direct and and factor loadings the internal consistency reliabilities moderated effects direct effects only and icrs of multi item scales modeled with reflective indicators direct and moderated effects table 3 reports the was or greater suggesting that the scales were results reliable of predicting behavioral intention and use in keeping the average variance extracted ave was greater than with utaut in we computed cohen s f square all cases and greater than the square of the correlations to check thus the effect size of each of the main effect variables suggesting discriminant validity the pattern of loadings and the and interaction terms by convention f square effect cross loadings supported internal consistency and sizes discrimi of 0 02 0 and 0 are termed small medium mis quarterly vol no 1 march 167 this content downloaded from 244 5 34 on mon jul 01 utc all use subject to http about jstor org terms ________________ venkatesh et ai consumer acceptance and use of it construct factor 1 factor 2 factor 3 factor 4 factor 5 factor 6 factor 7 factor 8 performance 14 15 05 09 expectancy 82 14 07 icr 0 88 ì5 4 ш 14 ëëï 8 j8 ш effort expectancy 82 15 15 09 15 icr 0 91 ш ee4 js sii ïô íõ tš m лб a f uence 30 7п т 5 ш icr o oz 30 15 15 09 tel 1ô lì 14 facilitating conditions 18 icr 0 m 17 li 17 85 tš нм1 tl lì tš 25 rd nicnrvatíon icr 0 86 нмз 15 25 10 õ5 10 т5 1ã 10 alue 30 õ5 10 15 06 10 htî 15 9 25 ь ш тэ õ9 õ5 10 нтз 12 22 08 05 ви tï 22 12 ã5 m ti rda ìo ao ntentìon 14 19 20 19 24 10 21 85 mean sd 1 2 3 4 5 6 7 8 9 10 12 1 pe ï 75 2 ee 5 25 3 si ï 20 50 71 4 fc 5 8 1 08 58 5 hm 4 60 1 л4 tš 6 pv 11 ï4 tí 73 7 bi 4 89 tiã 37 29 lì 8 gdr h 22 no na 9 age 30 68 го5 г03 17 06 03 na 10 exp 86 12 тз 1š 21 1з ôî na 11 ht 33 28 37 21 бб 03 05 19 12 use 30 20 20 30 28 24 42 m 25 49 na notes 1 pe performance expectancy ee effort expectancy si social influence fc facilitating conditions hm hedonic motivatio pv price value bl behavioral intention gdr gender age age exp experience ht habit 2 p 0 05 p 0 01 p 0 001 all other correlations are insignificant 3 diagonal elements are aves and off diagonal elements are correlations 168 mis quarterly vol no 1 march this content downloaded from 244 5 34 on mon jul 19 01 utc all use subject to http about jstor org terms ________________ venkatesh et al consumer acceptance and use of it dv behavioral intention utaut d only d i d only d i w í56 adj 55 performance expectancy pe 04 21 03 effort expectancy ee at 1 m 10 20 social influence si 20 facilitating conditions fc 17 hedonic motivation hm 23 03 price value pv 14 02 habit ht 32 m gender gdr 00 00 age age 02 01 experience exp 01 01 gdr x age age x exp m gdr x exp no ъз gdr x age x exp 01 02 pe x gdr no ee x gdr si x gdr 03 00 fc x gdr хю hm x gdr 02 pv x gdr 03 ht x gdr no pe x age 03 no ee x age m si x age fc x age hm x age m pv x age m ht x age no ee x exp 01 02 si x exp no m fc x exp no hm x exp õí ht x exp хю pe x gdr x age 31 22 ee x gdr x age si x gdr x age m fc x gdr x age 22 hm x gdr x age m pv x gdr x age ht x gdr x age 1 0i ee x gdr x exp 10 si x gdr x exp fc x gdr x exp 3 hm x gdr x exp ж ht x gdr x exp ee x age x exp 05 04 mis quarterly this content downloaded from 244 5 34 on mon jul 19 01 utc all use subject to http about jstor org terms ________________ venkatesh et ai consumer acceptance and use of it dv behavioral intention utaut si x age x exp tf fc x age x exp õí hm x age x exp no ht x age x exp m ee x gdr x age x exp tvf si x gdr x age x exp 21 17 fc x gdr x age x exp xpi hm x gdr x age x exp ht x gdr x age x exp dv technology use d only d l d only d l ж adj 40 52 behavioral intention bl 33 08 habit ht 24 гр facilitating conditions fc 17 03 15 08 age age m gender gdr 01 04 experience exp 04 02 bl x exp m age x exp õ3 gdr x exp 03 õ4 gdr x age x exp no ht x gdr m fc x age 04 õ2 ht x age хю ht x exp m _ _ fc x age x exp tf ht x age x exp ôï ht x gdr x age x exp 34 notes 1 d only direct effects only 2 p 0 001 p 0 01 p 0 0 and large respectively higher order cohen interac variables had effect x gdr x age x exp sizes and si x gdr x between age x exp when me based on a power predicting analysis bl and fc x age x exp when predicting we use conclu sample size we the results would support the applicability and have validity of utaut quite effects as a theoretical base to predict consumers behavioral inten tions and technology use the variance in behavioral as shown in table intention explained 3 by utaut the with direct basic effects only and struc firmed when interaction utaut with moderated effects also was terms quite good at 35 wer significant effects percent and percent for respectively performance and the variance expectancy ee explained in technology and use was 26 social percent and 40 percent influ intention bl respectively and we reran both the tests with only bl significant and paths in faci had significant the impacts model to examine the change in on we found use that whe included significant decreased by less than 2 percent path coefficie mis quarterly vol no 1 marc this content downloaded from 244 5 34 on mon jul 19 40 01 utc all use subject to http about jstor org terms ________________ venkatesh et ai consumer acceptance and use of it our hypotheses pertained to new moderated relationships as number of is research streams related to individual use of noted earlier about the role of facilitating conditions fc technology while the existence of many tam based studies hedonic motivation hm price value pv habit ht and do prompt the view that this is an over researched area interaction terms as predictors given the complexity of the benbasat and barki our study shows that utaut is proposed relationships beyond the beta coefficients reported a powerful framework goodhue and when it is in table 3 we conducted split sample analyses and plots to extended with relevant constructs bagozzi it can understand the pattern of results we report the support for contribute to the understanding of important phenomena here our hypotheses based on the cumulative evidence from the consumer use of technologies in general various tests we conducted most of our hypotheses were supported the direct effects only explained percent of the variance in behavioral intention and the theoretical contributions including interaction terms explained percent of the variance in behavioral intention likewise in explaining our major theoretical contribution is in modifying utaut technology use s direct effects only model and for the consumer technology acceptance and use context by moderated model explained 35 percent and 52 percent of the doing so we extend the generalizability of utaut from an variance respectively all of these represent significant jumps organizational to a consumer context prior technology in variance explained compared to the baseline original acceptance and use research has investigated the phenomenon utaut in organizational contexts where performance expectancy is the main driver of employees technology use intentions and the first two hypotheses pertain to the moderated effects behaviors of in the case of consumers acceptance and use of facilitating conditions and hedonic motivation on behavioral technology other drivers come to the fore two such drivers intention hi which predicted that age gender and included experi in are hedonic motivation and price value ence will moderate the effect of fc on bi was partially hedonic sup motivation is a critical determinant of behavioral ported as only gender and age were significant moderators intention but and was found to be a more important driver than experience was not the pattern related to these two performance modera expectancy is in non organizational contexts tors was consistent with hi in that fc was most important further to we delineated how various individual characteristics older women it is quite likely that as fc deals with namely broader gender age and experience jointly moderate the infrastructure and support issues it will always be important effect of hedonic motivation on behavioral intention some to those who value it even if they have significant experience interesting results are that the effect of hedonic motivation on with the target technology which predicted that behavioral age intention is stronger for younger men with less gender and experience will moderate the effect of experience hm on bi with a technology while the effect of price value such that it will be stronger among younger men was in early more important to older women thus the addition of stages of experience was supported which predicted hedonic motivation price value and their interactions with that age and gender would moderate the effect of pv utaut on bi moderators are crucial in expanding the scope and such that it will be stronger for older women was supported generalizability of utaut to the consumer environment the next set of hypotheses relate to the role of habit on bi and use it was theorized in a and b that habit s as effect hedonic is are ubiquitous in the consumer it market such will be stronger among older men in later stages as mobile of ex games and videos on iphones hedonic motivation perience this pattern was borne out the last hypothesis plays an important role in predicting intentions for hedonic is was to complement the role of habit as a predictor e g speci van der heijden we integrated hedonic motiva fically stated that the effect of bi on use will decline tion into with utaut and theorized the moderating effects of increasing experience we found that this hypothesis consumer was demographics on the relationship between hedonic also supported motivation and intention while van der heijden focused solely on hedonic is in our context of consumer use of mobile internet both utilitarian features e g the business discussion and productivity applications on iphone such as quickoffice and hedonic features e g mobile games and entertainment applications on iphone coexist our empirical results suggest our paper contributes to is research by providing the that logical in such a context of consumer use of it in general both companion in a consumer use setting to utaut utilitarian venka benefits and hedonic benefits are important drivers tesh et al that was developed for an employee of accept technology use future work can examine other key con ance and use setting our model sits at the confluence structs of that a are salient to different research contexts when mis quarterly vol no 1 march 171 this content downloaded from 244 5 34 on mon jul 19 40 01 utc all use subject to http about jstor org terms ________________ venkatesh et ai consumer acceptance and use of it building the models for instance in the context of social computing social outcomes such as higher status in the com munity or being unique in the group may be important addi tional drivers of it use we integrate price value into the ut aut framework to address the cost issue of technology use in the consumer setting while there are studies that have examined the role of value in consumer adoption of it e g kim et al we extend it to continued use and we theorized the moder ating effects of age and gender on the relationship between price value and intention our research highlights the impor tance of price value in consumer decision making regarding technology use and the moderating effects of the consumer demographic profile that is rooted in mechanisms related to social roles future research may build on our study to examine how the pricing of applications and the consequent value structure of the application portfolio can influence consumer technology use patterns i e the relative frequency of use of different applications for instance researchers may study how perceived value of applications can influence consumer use patterns when different bundling strategies such as pure bundling versus mixed bundling are adopted by it application vendors e g hitt and chen another important aspect of the extension of ut aut to the consumer context involves the influence of facilitating condi tions while the original ut aut only proposed a path from facilitating conditions to actual behavior in a consumer con text we theorized facilitating conditions moderated by gender and age to also influence behavioral intention in particular we found that the effect of facilitating conditions on behavioral intention is more pronounced for older women this particular group of consumers views availability of resources knowledge and support as essential to acceptance of a new technology we also found empirical support for the original ut aut with the remaining constructs performing as expected in the consumer context the effects of performance expectancy effort expectancy and social influence on behavioral intention were all moderated by individual characteristics i e different combinations of age gender and experience similarly the effect of facilitating conditions on technology use was moderated by age and experience one notable difference between the findings related to ut aut and is the effect of behavioral intention on technology use while behavioral intention had a positive direct effect on use in utaut in the consumer context in the effect was moderated by experience with the target technology another major theoretical contribution of this work is in the integration of habit into utaut researchers such as ben mis quarterly vol no 1 march basat and barki have called for more research into habit which is under studied in the is literature while others e g bagozzi have called for alternative theoretical mechanisms in predicting technology use in order to further the progress in this mature stream of work while limayem et al have integrated habit into expectation confirmation theory ест we have integrated habit into utaut which reflects an earlier unification of eight prior models of technology acceptance and use our treatment of habit reflects the two main theoretical perspectives of habit ouellette and wood the stored intention view e g ajzen and the automaticity view e g limayem et al age gender and experience were hypothesized to moderate the effects of habit on intention and use our research has demonstrated that when predicting continued use of it utaut predictors hedonic motivation price value and habit play important roles future research can extend our model and examine potential interventions to foster or break habits in the context of continued it use for example according to the automaticity view changes in the environ mental or context cues can already break the automatic cue behavior link in contrast following the stored intention view changes in the beliefs that formally led to the stored intention are more effective in changing habits in we modeled habit as having both a direct effect on use and an indirect effect through behavioral intention this is the first study of which we are aware that theorized the moderating effects of demographic characteristics on the habit intention and habit use relationships we have devel oped hypotheses regarding how age gender and experience jointly moderate the effect of habit on technology use based on the underlying process of habit activation and enforcement we found that older men with extensive usage experience tend to rely more on habit to drive technology use through both the stored intention path and the instant activation path we thus extend the nomological network related to tech nology use to include a new set of constructs and associated theoretical mechanisms in summary incorporates not only the main rela tionships from utaut but also new constructs and relation ships that extend the applicability of utaut to the consumer context we have provided empirical support for the appli cability of to the consumer context via a two stage online survey of 1 512 mobile internet consumers the variance explained in both behavioral intention 74 percent and technology use 52 percent are substantial compared to the baseline utaut that explained 56 percent and 40 percent of the variance in intention and use respectively the results from are also comparable to those obtained in venkatesh et al s study of utaut in the organiza this content downloaded from 142 244 5 34 on mon jul 19 40 01 utc all use subject to http about jstor org terms ________________ venkatesh et ai consumer acceptance and use of it tional context 70 percent and 48 percent respectively this applications features and depth of use i e the frequency of suggests that the proposed extensions are critical to making use future research can build on our study by including the predictive validity of ut aut in a consumer context com more structural elements of use such as those related to user parable to what was found in the original ut aut studies in and tasks burton jones and straub to examine the an organizational context in light of these findings com explanatory power of behavioral intention and habit for parisons to other models such as the model of adoption of instance the predictive power of habit may increase relative technology in the household math brown and venkatesh to that of behavioral intention when users daily tasks are will be of value our model incorporates ideas from included in the measurement of use as daily routine tasks are math but an empirical comparison and incorporation of the more subject to the influence of habit household lifecycle as in brown and venkatesh could be a fruitful future study the issue of common method variance cmv has been identified as a major methodological concern associated with tam based research e g malhotra et al sharma et al limitations and future research straub and burton jones meta analysis based investigations have revealed mixed results while malhotra the first limitation concerns generalizability of the findings et al found cmv was not a serious issue in the as our study was conducted in hong kong which has general a very там framework sharma et al suggested that high penetration rate for mobile phones the findings under may not certain conditions the link between perceived apply to countries that are less technologically advanced usefulness and technology use is subject to relatively high second as our sample is somewhat skewed with a mean cmv age while examining the cmv in tam utaut based around 31 the findings may not apply to those research who are is not the major goal of our paper we did not find significantly older third we have studied only one cmv type to of be a concern in our study even so future research technology i e mobile internet future research can should build adopt a more rigorous design to reduce measurement on our study by testing in different countries and method biases future research using different objective different age groups and different technologies finally measures we of use can help further rule out cmv future included hedonic motivation price value and habit research as pre using experiments that manipulate the predictors dictors based on key complementary theoretical perspectives and using the scales as manipulation checks can further help to the theoretical mechanisms in utaut future research reduce cmv concerns can identify other relevant factors that may help increase the applicability of utaut to a wide range of consumer technology use contexts managerial implications our measure of behavior is self reported there is not only our empirical finding about price value has implications for significant variance across studies in how technology use is the pricing strategy of consumer it application vendors conceptualized and measured but also continuing conceptual particularly our study suggests that perceived benefits over measurement progress with respect to the use construct monetary sacrifice i e the price value of it applications can according to burton jones and straub technology use influence consumers technology use for instance the cur has been conceptualized and measured as extent of use e g rent cost structure of mobile internet applications is mainly venkatesh and davis breadth of use e g saga and based on the network traffic generated by each type of appli zmud variety of use e g igbaria et al thong cation with multimedia contents priced at the highest level users cognitive absorption into the system agarwal however this pricing pattern may not reflect the relative and karahanna etc thus the interpretation of a value attached to different applications by consumers it study s results and comparison across studies on the variance application vendors should first focus on the real value of in use explained is contingent upon the conceptualization of their offerings for consumers for instance while mobile the use construct following the convention of burton jones video applications such as movie episodes are priced high and straub our focus of technology use in the current due to the network traffic they generate the real value i e study is at the system element i e breadth and extent depth hedonic benefits of these type of applications is still ques of the use accordingly our measurement of technology use tionable as consumers may not be able to concentrate fully on is a formative index of six questions see appendix on con the movies when watching them on the small screen of the sumers usage frequencies of the six most popular mobile mobile devices while on the move xu et al 2010 thus internet applications in hong kong thus our measure incor from a consumer s perspective the hedonic benefits of mobile porates both the breadth of use i e number of different movies may not be high enough to justify the price thus mis quarterly vol no 1 march 2012 173 this content downloaded from 142 244 5 34 on mon 16 jul 19 40 01 utc all use subject to http about jstor org terms ________________ venkatesh et ai consumer acceptance and use of it having low or even negative price value in contrast appli cations less rich in media such as mobile picture sharing that emphasize immediate experience sharing among friends may be of higher price value to consumers because of their social value and timeliness our study suggests that to maximize profit vendors should optimize the pricing of different appli cations based on their utilitarian hedonic or other types of value to consumers our results suggest that there is a significant impact of con sumers habit on personal technology use when they face an environment that is diversified and ever changing in addition to the direct and automatic effect of habit on technology use habit also operates as a stored intention path to influence behavior this demands more marketing communication efforts to strengthen both the stored intention and its link to behavior for instance when multimedia messaging service mms was introduced mobile service providers rolled out advertisements to emphasize a variety of scenarios where the service can be used such as experience sharing with friends sending greeting cards to family members field workers taking pictures on the spot etc these advertisements helped to enhance the stored intention i e i can use mms in a variety of contexts and its link to the behavior in different usage contexts in retrospect emphasizing the application of mobile internet in varied contexts and occasions may be a useful strategy to potentially increase the habitual use of it applications moreover our results suggest that the impact of habit on behavior differs with age gender and experience specifically older men with extensive experience more than others tend to be driven by habit thus when the goal is to facilitate changes in consumers habitual usage as in the case of launching a new technology more resources may need to be targeted at older men with significant experience because they may have great difficulty in changing their habits in contrast when it application providers want to maintain consumers habitual use more attention should be paid to younger women as they are most sensitive to changes in the environment finally the significance of the moderated effects in our model suggests that managers can use a market segmentation strategy to facilitate consumer technology use our results show that different cohorts of consumers attach different weights to various factors that influence their technology use which can potentially be attributed to the differential learning abilities and social roles across age experience and gender first we found that when older women are in the early stages of using a particular technology they rely more on external resources to facilitate their continued use of the technology this suggests that on going facilitations designed for older women should be provided by it application vendors if they mis quarterly vol no 1 march 2012 want to keep this group of consumers on track for instance customer help through a call center instant messaging ser vices or a consumer community can take special care of older women users who are new to it applications second we found that younger men in the early stages of experience are motivated more by the hedonic benefits gained from using a technology this implies that hedonic applications of the technology that are interesting to younger men such as mobile gaming music and videos in the case of mobile data services can be bundled together with special promotions to attract younger men new to the technology finally we found that older women more than others emphasize price value of the technology this suggests that older women are more price sensitive than other cohorts of consumers thus from the perspective of it application vendors relatively simple and utilitarian technology applications can be promoted with special discounts to older women users while premium pricing of hedonic applications may be adopted and targeted at younger men in summary our study suggests that the consumer technology industry should better design and market technologies to consumers in various demographic groups at various stages of the use curve conclusions the current study showed that in the context of consumers use of technology the effects of hedonic motivation price value and habit are complex first the impact of hedonic motivation on behavioral intention is moderated by age gender and experience second the effect of price value on behavioral intention is moderated by age and gender finally habit has both direct and mediated effects on technology use and these effects are moderated by individual differences thus both the tpb based view of habit i e as stored inten tion and the more recent automatic activation view of habit i e as a direct link between stimulus and behavior are functioning together in determining consumer use of tech nology moreover the strength and activation of habit differs across age gender and experience overall our study con firmed the important roles of hedonic motivation price value and habit in influencing technology use and in which is tailored to the context of consumer acceptance and use of technology python sequence types in this chapter we explore python various sequence classes namely the built in list tuple and str classes there is signiﬁcant commonality between these classes most notably each supports indexing to access an individual element of a sequence using a syntax such as seq k and each uses a low level concept known as an array to represent the sequence however there are signiﬁcant differences in the abstractions that these classes represent and in the way that instances of these classes are represented internally by python because these classes are used so widely in python programs and because they will become building blocks upon which we will develop more complex data structures it is imperative that we estab lish a clear understanding of both the public behavior and inner workings of these classes public behaviors a proper understanding of the outward semantics for a class is a necessity for a good programmer while the basic usage of lists strings and tuples may seem straightforward there are several important subtleties regarding the behaviors as sociated with these classes such as what it means to make a copy of a sequence or to take a slice of a sequence having a misunderstanding of a behavior can easily lead to inadvertent bugs in a program therefore we establish an accurate men tal model for each of these classes these images will help when exploring more advanced usage such as representing a multidimensional data set as a list of lists implementation details a focus on the internal implementations of these classes seems to go against our stated principles of object oriented programming in section we emphasized the principle of encapsulation noting that the user of a class need not know about the internal details of the implementation while it is true that one only needs to understand the syntax and semantics of a class public interface in order to be able to write legal and correct code that uses instances of the class the efﬁciency of a program depends greatly on the efﬁciency of the components upon which it relies asymptotic and experimental analyses in describing the efﬁciency of various operations for python sequence classes we will rely on the formal asymptotic analysis notations established in chapter we will also perform experimental analyses of the primary operations to provide empirical evidence that is consistent with the more theoretical asymptotic analyses low level arrays low level arrays to accurately describe the way in which python represents the sequence types we must ﬁrst discuss aspects of the low level computer architecture the primary memory of a computer is composed of bits of information and those bits are typ ically grouped into larger units that depend upon the precise system architecture such a typical unit is a byte which is equivalent to bits a computer system will have a huge number of bytes of memory and to keep track of what information is stored in what byte the computer uses an abstraction known as a memory address in effect each byte of memory is associated with a unique number that serves as its address more formally the binary representation of the number serves as the address in this way the computer system can refer to the data in byte versus the data in byte for example memory addresses are typically coordinated with the physical layout of the memory system and so we often portray the numbers in sequential fashion figure provides such a diagram with the designated memory address for each byte figure a representation of a portion of a computer memory with individual bytes labeled with consecutive memory addresses despite the sequential nature of the numbering system computer hardware is designed in theory so that any byte of the main memory can be efﬁciently accessed based upon its memory address in this sense we say that a computer main mem ory performs as random access memory ram that is it is just as easy to retrieve byte as it is to retrieve byte in practice there are complicating factors including the use of caches and external memory we address some of those issues in chapter using the notation for asymptotic analysis we say that any individual byte of memory can be stored or retrieved in o time in general a programming language keeps track of the association between an identiﬁer and the memory address in which the associated value is stored for example identiﬁer x might be associated with one value stored in memory while y is associated with another value stored in memory a common programming task is to keep track of a sequence of related objects for example we may want a video game to keep track of the top ten scores for that game rather than use ten different variables for this task we would prefer to use a single name for the group and use index numbers to refer to the high scores in that group chapter array based sequences a group of related variables can be stored one after another in a contiguous portion of the computer memory we will denote such a representation as an array as a tangible example a text string is stored as an ordered sequence of individual characters in python each character is represented using the unicode character set and on most computing systems python internally represents each unicode character with bits i e bytes therefore a six character string such as sample would be stored in consecutive bytes of memory as diagrammed in figure figure a python string embedded as an array of characters in the computer memory we assume that each unicode character of the string requires two bytes of memory the numbers below the entries are indices into the string we describe this as an array of six characters even though it requires bytes of memory we will refer to each location within an array as a cell and will use an integer index to describe its location within the array with cells numbered starting with and so on for example in figure the cell of the array with index has contents l and is stored in bytes and of memory each cell of an array must use the same number of bytes this requirement is what allows an arbitrary cell of the array to be accessed in constant time based on its index in particular if one knows the memory address at which an array starts e g in figure the number of bytes per element e g for a unicode character and a desired index within the array the appropriate memory address can be computed using the calculation start cellsize index by this formula the cell at index begins precisely at the start of the array the cell at index begins precisely cellsize bytes beyond the start of the array and so on as an example cell of figure begins at memory location of course the arithmetic for calculating memory addresses within an array can be handled automatically therefore a programmer can envision a more typical high level abstraction of an array of characters as diagrammed in figure s a m p l e figure a higher level abstraction for the string portrayed in figure low level arrays referential arrays as another motivating example assume that we want a medical information system to keep track of the patients currently assigned to beds in a certain hospital if we assume that the hospital has beds and conveniently that those beds are num bered from to we might consider using an array based structure to maintain the names of the patients currently assigned to those beds for example in python we might use a list of names such as rene joseph janet jonas helen virginia to represent such a list with an array python must adhere to the requirement that each cell of the array use the same number of bytes yet the elements are strings and strings naturally have different lengths python could attempt to reserve enough space for each cell to hold the maximum length string not just of currently stored strings but of any string we might ever want to store but that would be wasteful instead python represents a list or tuple instance using an internal storage mechanism of an array of object references at the lowest level what is stored is a consecutive sequence of memory addresses at which the elements of the se quence reside a high level diagram of such a list is shown in figure figure an array storing references to strings although the relative size of the individual elements may vary the number of bits used to store the memory address of each element is ﬁxed e g bits per address in this way python can support constant time access to a list or tuple element based on its index in figure we characterize a list of strings that are the names of the patients in a hospital it is more likely that a medical information system would manage more comprehensive information on each patient perhaps represented as an in stance of a patient class from the perspective of the list implementation the same principle applies the list will simply keep a sequence of references to those ob jects note as well that a reference to the none object can be used as an element of the list to represent an empty bed in the hospital chapter array based sequences the fact that lists and tuples are referential structures is signiﬁcant to the se mantics of these classes a single list instance may include multiple references to the same object as elements of the list and it is possible for a single object to be an element of two or more lists as those lists simply store references back to that object as an example when you compute a slice of a list the result is a new list instance but that new list has references to the same elements that are in the original list as portrayed in figure figure the result of the command temp primes when the elements of the list are immutable objects as with the integer in stances in figure the fact that the two lists share elements is not that signiﬁ cant as neither of the lists can cause a change to the shared object if for example the command temp were executed from this conﬁguration that does not change the existing integer object it changes the reference in cell of the temp list to reference a different object the resulting conﬁguration is shown in figure figure the result of the command temp upon the conﬁguration por trayed in figure the same semantics is demonstrated when making a new list as a copy of an existing one with a syntax such as backup list primes this produces a new list that is a shallow copy see section in that it references the same elements as in the ﬁrst list with immutable elements this point is moot if the contents of the list were of a mutable type a deep copy meaning a new list with new elements can be produced by using the deepcopy function from the copy module low level arrays as a more striking example it is a common practice in python to initialize an array of integers using a syntax such as counters this syntax produces a list of length eight with all eight elements being the value zero technically all eight cells of the list reference the same object as portrayed in figure counters figure the result of the command data at ﬁrst glance the extreme level of aliasing in this conﬁguration may seem alarming however we rely on the fact that the referenced integer is immutable even a command such as counters does not technically change the value of the existing integer instance this computes a new integer with value and sets cell to reference the newly computed value the resulting conﬁguration is shown in figure counters figure the result of command data upon the list from figure as a ﬁnal manifestation of the referential nature of lists we note that the extend command is used to add all elements from one list to the end of another list the extended list does not receive copies of those elements it receives references to those elements figure portrays the effect of a call to extend extras primes figure the effect of command primes extend extras shown in light gray chapter array based sequences compact arrays in python in the introduction to this section we emphasized that strings are represented using an array of characters not an array of references we will refer to this more direct representation as a compact array because the array is storing the bits that represent the primary data characters in the case of strings s a m p l e compact arrays have several advantages over referential structures in terms of computing performance most signiﬁcantly the overall memory usage will be much lower for a compact structure because there is no overhead devoted to the explicit storage of the sequence of memory references in addition to the primary data that is a referential structure will typically use bits for the memory address stored in the array on top of whatever number of bits are used to represent the object that is considered the element also each unicode character stored in a compact array within a string typically requires bytes if each character were stored independently as a one character string there would be signiﬁcantly more bytes used as another case study suppose we wish to store a sequence of one million bit integers in theory we might hope to use only million bits however we estimate that a python list will use four to ﬁve times as much memory each element of the list will result in a bit memory address being stored in the primary array and an int instance being stored elsewhere in memory python allows you to query the actual number of bytes being used for the primary storage of any object this is done using the getsizeof function of the sys module on our system the size of a typical int object requires bytes of memory well beyond the bytes needed for representing the actual bit number in all the list will be using bytes per entry rather than the bytes that a compact list of integers would require another important advantage to a compact structure for high performance com puting is that the primary data are stored consecutively in memory note well that this is not the case for a referential structure that is even though a list maintains careful ordering of the sequence of memory addresses where those elements reside in memory is not determined by the list because of the workings of the cache and memory hierarchies of computers it is often advantageous to have data stored in memory near other data that might be used in the same computations despite the apparent inefﬁciencies of referential structures we will generally be content with the convenience of python lists and tuples in this book the only place in which we consider alternatives will be in chapter which focuses on the impact of memory usage on data structures and algorithms python provides several means for creating compact arrays of various types low level arrays primary support for compact arrays is in a module named array that module deﬁnes a class also named array providing compact storage for arrays of primitive data types a portrayal of such an array of integers is shown in figure figure integers stored compactly as elements of a python array the public interface for the array class conforms mostly to that of a python list however the constructor for the array class requires a type code as a ﬁrst parameter which is a character that designates the type of data that will be stored in the array as a tangible example the type code i designates an array of signed integers typically represented using at least bits each we can declare the array shown in figure as primes array i the type code allows the interpreter to determine precisely how many bits are needed per element of the array the type codes supported by the array module as shown in table are formally based upon the native data types used by the c programming language the language in which the the most widely used distri bution of python is implemented the precise number of bits for the c data types is system dependent but typical ranges are shown in the table code c data type typical number of bytes b signed char b unsigned char u unicode char h signed short int h unsigned short int i signed int i unsigned int l signed long int l unsigned long int f ﬂoat d ﬂoat table type codes supported by the array module the array module does not provide support for making compact arrays of user deﬁned data types compact arrays of such structures can be created with the lower level support of a module named ctypes see section for more discussion of the ctypes module chapter array based sequences dynamic arrays and amortization when creating a low level array in a computer system the precise size of that array must be explicitly declared in order for the system to properly allocate a consecutive piece of memory for its storage for example figure displays an array of bytes that might be stored in memory locations through figure an array of bytes allocated in memory locations through because the system might dedicate neighboring memory locations to store other data the capacity of an array cannot trivially be increased by expanding into sub sequent cells in the context of representing a python tuple or str instance this constraint is no problem instances of those classes are immutable so the correct size for an underlying array can be ﬁxed when the object is instantiated python list class presents a more interesting abstraction although a list has a particular length when constructed the class allows us to add elements to the list with no apparent limit on the overall capacity of the list to provide this abstraction python relies on an algorithmic sleight of hand known as a dynamic array the ﬁrst key to providing the semantics of a dynamic array is that a list instance maintains an underlying array that often has greater capacity than the current length of the list for example while a user may have created a list with ﬁve elements the system may have reserved an underlying array capable of storing eight object references rather than only ﬁve this extra capacity makes it easy to append a new element to the list by using the next available cell of the array if a user continues to append elements to a list any reserved capacity will eventually be exhausted in that case the class requests a new larger array from the system and initializes the new array so that its preﬁx matches that of the existing smaller array at that point in time the old array is no longer needed so it is reclaimed by the system intuitively this strategy is much like that of the hermit crab which moves into a larger shell when it outgrows its previous one we give empirical evidence that python list class is based upon such a strat egy the source code for our experiment is displayed in code fragment and a sample output of that program is given in code fragment we rely on a func tion named getsizeof that is available from the sys module this function reports the number of bytes that are being used to store an object in python for a list it reports the number of bytes devoted to the array and other instance variables of the list but not any space devoted to elements referenced by the list dynamic arrays and amortization import sys provides getsizeof function data for k in range n note must ﬁx choice of n a len data number of elements b sys getsizeof data actual size in bytes print length size in bytes format a b data append none increase length by one code fragment an experiment to explore the relationship between a list length and its underlying size in python length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes length size in bytes code fragment sample output from the experiment of code fragment chapter array based sequences in evaluating the results of the experiment we draw attention to the ﬁrst line of output from code fragment we see that an empty list instance already requires a certain number of bytes of memory on our system in fact each object in python maintains some state for example a reference to denote the class to which it belongs although we cannot directly access private instance variables for a list we can speculate that in some form it maintains state information akin to n the number of actual elements currently stored in the list capacity the maximum number of elements that could be stored in the currently allocated array a the reference to the currently allocated array initially none as soon as the ﬁrst element is inserted into the list we detect a change in the underlying size of the structure in particular we see the number of bytes jump from to an increase of exactly bytes our experiment was run on a bit machine architecture meaning that each memory address is a bit number i e bytes we speculate that the increase of bytes reﬂects the allocation of an underlying array capable of storing four object references this hypothesis is consistent with the fact that we do not see any underlying change in the memory usage after inserting the second third or fourth element into the list after the ﬁfth element has been added to the list we see the memory usage jump from bytes to bytes if we assume the original base usage of bytes for the list the total of suggests an additional bytes that provide capacity for up to eight object references again this is consistent with the experiment as the memory usage does not increase again until the ninth insertion at that point the bytes can be viewed as the original plus an additional byte array to store object references the insertion pushes the overall memory usage to hence enough to store up to element references because a list is a referential structure the result of getsizeof for a list instance only includes the size for representing its primary structure it does not account for memory used by the objects that are elements of the list in our experiment we repeatedly append none to the list because we do not care about the contents but we could append any type of object without affecting the number of bytes reported by getsizeof data if we were to continue such an experiment for further iterations we might try to discern the pattern for how large of an array python creates each time the ca pacity of the previous array is exhausted see exercises r and c before exploring the precise sequence of capacities used by python we continue in this section by describing a general approach for implementing dynamic arrays and for performing an asymptotic analysis of their performance dynamic arrays and amortization implementing a dynamic array although the python list class provides a highly optimized implementation of dy namic arrays upon which we rely for the remainder of this book it is instructive to see how such a class might be implemented the key is to provide means to grow the array a that stores the elements of a list of course we cannot actually grow that array as its capacity is ﬁxed if an element is appended to a list at a time when the underlying array is full we perform the following steps allocate a new array b with larger capacity set b i a i for i n where n denotes current number of items set a b that is we henceforth use b as the array supporting the list insert the new element in the new array an illustration of this process is shown in figure a a b b a b c figure an illustration of the three steps for growing a dynamic array a create new array b b store elements of a in b c reassign reference a to the new array not shown is the future garbage collection of the old array or the insertion of the new element the remaining issue to consider is how large of a new array to create a com monly used rule is for the new array to have twice the capacity of the existing array that has been ﬁlled in section we will provide a mathematical analysis to justify such a choice in code fragment we offer a concrete implementation of dynamic arrays in python our dynamicarray class is designed using ideas described in this sec tion while consistent with the interface of a python list class we provide only limited functionality in the form of an append method and accessors len and getitem support for creating low level arrays is provided by a module named ctypes because we will not typically use such a low level structure in the remain der of this book we omit a detailed explanation of the ctypes module instead we wrap the necessary command for declaring the raw array within a private util ity method make array the hallmark expansion procedure is performed in our nonpublic resize method chapter array based sequences import ctypes provides low level arrays class dynamicarray a dynamic array class akin to a simpliﬁed python list def init self create an empty array self n count actual elements self capacity default array capacity self a self make array self capacity low level array def len self return number of elements stored in the array return self n def getitem self k return element at index k if not k self n raise indexerror invalid index return self a k retrieve from array def append self obj add object to end of the array if self n self capacity not enough room self resize self capacity so double capacity self a self n obj self n def resize self c nonpublic utitity resize internal array to capacity c b self make array c new bigger array for k in range self n for each existing value b k self a k self a b use the bigger array self capacity c def make array self c nonpublic utitity return new array with capacity c return c ctypes py object see ctypes documentation code fragment an implementation of a dynamicarray class using a raw array from the ctypes module as storage dynamic arrays and amortization amortized analysis of dynamic arrays in this section we perform a detailed analysis of the running time of operations on dynamic arrays we use the big omega notation introduced in section to give an asymptotic lower bound on the running time of an algorithm or step within it the strategy of replacing an array with a new larger array might at ﬁrst seem slow because a single append operation may require n time to perform where n is the current number of elements in the array however notice that by doubling the capacity during an array replacement our new array allows us to add n new elements before the array must be replaced again in this way there are many simple append operations for each expensive one see figure this fact allows us to show that performing a series of operations on an initially empty dynamic array is efﬁcient in terms of its total running time using an algorithmic design pattern called amortization we can show that per forming a sequence of such append operations on a dynamic array is actually quite efﬁcient to perform an amortized analysis we use an accounting technique where we view the computer as a coin operated appliance that requires the payment of one cyber dollar for a constant amount of computing time when an operation is executed we should have enough cyber dollars available in our current bank account to pay for that operation running time thus the total amount of cyber dollars spent for any computation will be proportional to the total time spent on that computation the beauty of using this analysis method is that we can overcharge some operations in order to save up cyber dollars to pay for others current number of elements figure running times of a series of append operations on a dynamic array chapter array based sequences proposition let s be a sequence implemented by means of a dynamic array with initial capacity one using the strategy of doubling the array size when full the total time to perform a series of n append operations in s starting from s being empty is o n justiﬁcation let us assume that one cyber dollar is enough to pay for the execu tion of each append operation in s excluding the time spent for growing the array also let us assume that growing the array from size k to size requires k cyber dollars for the time spent initializing the new array we shall charge each append operation three cyber dollars thus we overcharge each append operation that does not cause an overﬂow by two cyber dollars think of the two cyber dollars proﬁted in an insertion that does not grow the array as being stored with the cell in which the element was inserted an overﬂow occurs when the array s has elements for some integer i and the size of the array used by the array representing s is thus doubling the size of the array will require cyber dollars fortunately these cyber dollars can be found stored in cells through see figure note that the previous overﬂow occurred when the number of elements became larger than for the ﬁrst time and thus the cyber dollars stored in cells through have not yet been spent therefore we have a valid amortization scheme in which each operation is charged three cyber dollars and all the comput ing time is paid for that is we can pay for the execution of n append operations using cyber dollars in other words the amortized running time of each append operation is o hence the total running time of n append operations is o n a b figure illustration of a series of append operations on a dynamic array a an cell array is full with two cyber dollars stored at cells through b an append operation causes an overﬂow and a doubling of capacity copying the eight old elements to the new array is paid for by the cyber dollars already stored in the table inserting the new element is paid for by one of the cyber dollars charged to the current append operation and the two cyber dollars proﬁted are stored at cell dynamic arrays and amortization geometric increase in capacity although the proof of proposition relies on the array being doubled each time we expand the o amortized bound per operation can be proven for any geo metrically increasing progression of array sizes see section for discussion of geometric progressions when choosing the geometric base there exists a trade off between run time efﬁciency and memory usage with a base of i e doubling the array if the last insertion causes a resize event the array essentially ends up twice as large as it needs to be if we instead increase the array by only of its current size i e a geometric base of we do not risk wasting as much memory in the end but there will be more intermediate resize events along the way still it is possible to prove an o amortized bound using a constant factor greater than the cyber dollars per operation used in the proof of proposition see exercise c the key to the performance is that the amount of additional space is proportional to the current size of the array beware of arithmetic progression to avoid reserving too much space at once it might be tempting to implement a dynamic array with a strategy in which a constant number of additional cells are reserved each time an array is resized unfortunately the overall performance of such a strategy is signiﬁcantly worse at an extreme an increase of only one cell causes each append operation to resize the array leading to a familiar n summation and n overall cost using increases of or at a time is slightly better as portrayed in figure but the overall cost remains quadratic current number of elements current number of elements a b figure running times of a series of append operations on a dynamic array using arithmetic progression of sizes a assumes increase of in size of the array while b assumes increase of chapter array based sequences using a ﬁxed increment for each resize and thus an arithmetic progression of intermediate array sizes results in an overall time that is quadratic in the number of operations as shown in the following proposition intuitively even an increase in cells per resize will become insigniﬁcant for large data sets proposition performing a series of n append operations on an initially empty dynamic array using a ﬁxed increment with each resize takes time justiﬁcation let c represent the ﬁxed increment in capacity that is used for each resize event during the series of n append operations time will have been spent initializing arrays of size c mc for m fn and therefore the overall time would be proportional to c mc by proposition this sum is m m m m n n ci c i c c c i i therefore performing the n append operations takes time a lesson to be learned from propositions and is that a subtle difference in an algorithm design can produce drastic differences in the asymptotic performance and that a careful analysis can provide important insights into the design of a data structure memory usage and shrinking an array another consequence of the rule of a geometric increase in capacity when append ing to a dynamic array is that the ﬁnal array size is guaranteed to be proportional to the overall number of elements that is the data structure uses o n memory this is a very desirable property for a data structure if a container such as a python list provides operations that cause the removal of one or more elements greater care must be taken to ensure that a dynamic array guarantees o n memory usage the risk is that repeated insertions may cause the underlying array to grow arbitrarily large and that there will no longer be a propor tional relationship between the actual number of elements and the array capacity after many elements are removed a robust implementation of such a data structure will shrink the underlying array on occasion while maintaining the o amortized bound on individual op erations however care must be taken to ensure that the structure cannot rapidly oscillate between growing and shrinking the underlying array in which case the amortized bound would not be achieved in exercise c we explore a strategy in which the array capacity is halved whenever the number of actual element falls below one fourth of that capacity thereby guaranteeing that the array capacity is at most four times the number of elements we explore the amortized analysis of such a strategy in exercises c and c dynamic arrays and amortization python list class the experiments of code fragment and at the beginning of section provide empirical evidence that python list class is using a form of dynamic arrays for its storage yet a careful examination of the intermediate array capacities see exercises r and c suggests that python is not using a pure geometric progression nor is it using an arithmetic progression with that said it is clear that python implementation of the append method exhibits amortized constant time behavior we can demonstrate this fact experi mentally a single append operation typically executes so quickly that it would be difﬁcult for us to accurately measure the time elapsed at that granularity although we should notice some of the more expensive operations in which a resize is per formed we can get a more accurate measure of the amortized cost per operation by performing a series of n append operations on an initially empty list and deter mining the average cost of each a function to perform that experiment is given in code fragment from time import time import time function from time module def compute average n perform n appends to an empty list and return average time elapsed data start time record the start time in seconds for k in range n data append none end time record the end time in seconds return end start n compute average per operation code fragment measuring the amortized cost of append for python list class technically the time elapsed between the start and end includes the time to manage the iteration of the for loop in addition to the append calls the empirical results of the experiment for increasingly large values of n are shown in table we see higher average cost for the smaller data sets perhaps in part due to the over head of the loop range there is also natural variance in measuring the amortized cost in this way because of the impact of the ﬁnal resize event relative to n taken as a whole there seems clear evidence that the amortized time for each append is independent of n table average running time of append measured in microseconds as observed over a sequence of n calls starting with an empty list chapter array based sequences eﬃciency of python sequence types in the previous section we began to explore the underpinnings of python list class in terms of implementation strategies and efﬁciency we continue in this section by examining the performance of all of python sequence types python list and tuple classes the nonmutating behaviors of the list class are precisely those that are supported by the tuple class we note that tuples are typically more memory efﬁcient than lists because they are immutable therefore there is no need for an underlying dynamic array with surplus capacity we summarize the asymptotic efﬁciency of the nonmutating behaviors of the list and tuple classes in table an explanation of this analysis follows table asymptotic performance of the nonmutating behaviors of the list and tuple classes identiﬁers data and designate instances of the list or tuple class and n and their respective lengths for the containment check and index method k represents the index of the leftmost occurrence with k n if there is no occurrence for comparisons between two sequences we let k denote the leftmost index at which they disagree or else k min constant time operations the length of an instance is returned in constant time because an instance explicitly maintains such state information the constant time efﬁciency of syntax data j is assured by the underlying access into an array efﬁciency of python sequence types searching for occurrences of a value each of the count index and contains methods proceed through iteration of the sequence from left to right in fact code fragment of section demonstrates how those behaviors might be implemented notably the loop for computing the count must proceed through the entire sequence while the loops for checking containment of an element or determining the index of an element immediately exit once they ﬁnd the leftmost occurrence of the desired value if one exists so while count always examines the n elements of the sequence index and contains examine n elements in the worst case but may be faster empirical evidence can be found by setting data list range and then comparing the relative efﬁciency of the test in data relative to the test in data or even the failed test in data lexicographic comparisons comparisons between two sequences are deﬁned lexicographically in the worst case evaluating such a condition requires an iteration taking time proportional to the length of the shorter of the two sequences because when one sequence ends the lexicographic result can be determined however in some cases the result of the test can be evaluated more efﬁciently for example if evaluating it is clear that the result is true without examining the re mainders of those lists because the second element of the left operand is strictly less than the second element of the right operand creating new instances the ﬁnal three behaviors in table are those that construct a new instance based on one or more existing instances in all cases the running time depends on the construction and initialization of the new result and therefore the asymptotic be havior is proportional to the length of the result therefore we ﬁnd that slice data can be constructed almost immediately because it has only eight elements while slice data has one million elements and thus is more time consuming to create mutating behaviors the efﬁciency of the mutating behaviors of the list class are described in table the simplest of those behaviors has syntax data j val and is supported by the special setitem method this operation has worst case o running time be cause it simply replaces one element of a list with a new value no other elements are affected and the size of the underlying array does not change the more inter esting behaviors to analyze are those that add or remove elements from the list chapter array based sequences amortized table asymptotic performance of the mutating behaviors of the list class iden tiﬁers data and designate instances of the list class and n and their respective lengths adding elements to a list in section we fully explored the append method in the worst case it requires n time because the underlying array is resized but it uses o time in the amor tized sense lists also support a method with signature insert k value that inserts a given value into the list at index k n while shifting all subsequent elements back one slot to make room for the purpose of illustration code fragment pro vides an implementation of that method in the context of our dynamicarray class introduced in code fragment there are two complicating factors in analyzing the efﬁciency of such an operation first we note that the addition of one element may require a resizing of the dynamic array that portion of the work requires n worst case time but only o amortized time as per append the other expense for insert is the shifting of elements to make room for the new item the time for def insert self k value insert value at index k shifting subsequent values rightward for simplicity we assume k n in this verion if self n self capacity not enough room self resize self capacity so double capacity for j in range self n k shift rightmost ﬁrst self a j self a j self a k value store newest element self n code fragment implementation of insert for our dynamicarray class efﬁciency of python sequence types k n figure creating room to insert a new element at index k of a dynamic array that process depends upon the index of the new element and thus the number of other elements that must be shifted that loop copies the reference that had been at index n to index n then the reference that had been at index n to n continuing until copying the reference that had been at index k to k as illus trated in figure overall this leads to an amortized o n k performance for inserting at index k when exploring the efﬁciency of python append method in section we performed an experiment that measured the average cost of repeated calls on varying sizes of lists see code fragment and table we have repeated that experiment with the insert method trying three different access patterns in the ﬁrst case we repeatedly insert at the beginning of a list for n in range n data insert none in a second case we repeatedly insert near the middle of a list for n in range n data insert n none in a third case we repeatedly insert at the end of the list for n in range n data insert n none the results of our experiment are given in table reporting the average time per operation not the total time for the entire loop as expected we see that inserting at the beginning of a list is most expensive requiring linear time per operation inserting at the middle requires about half the time as inserting at the beginning yet is still n time inserting at the end displays o behavior akin to append n k k n k n 422 table average running time of insert k val measured in microseconds as observed over a sequence of n calls starting with an empty list we let n denote the size of the current list as opposed to the ﬁnal list chapter array based sequences removing elements from a list python list class offers several ways to remove an element from a list a call to pop removes the last element from a list this is most efﬁcient because all other elements remain in their original location this is effectively an o operation but the bound is amortized because python will occasionally shrink the underlying dynamic array to conserve memory the parameterized version pop k removes the element that is at index k n of a list shifting all subsequent elements leftward to ﬁll the gap that results from the removal the efﬁciency of this operation is o n k as the amount of shifting depends upon the choice of index k as illustrated in figure note well that this implies that pop is the most expensive call using n time see experiments in exercise r k n figure removing an element at index k of a dynamic array the list class offers another method named remove that allows the caller to specify the value that should be removed not the index at which it resides for mally it removes only the ﬁrst occurrence of such a value from a list or raises a valueerror if no such value is found an implementation of such behavior is given in code fragment again using our dynamicarray class for illustration interestingly there is no efﬁcient case for remove every call requires n time one part of the process searches from the beginning until ﬁnding the value at index k while the rest iterates from k to the end in order to shift elements leftward this linear behavior can be observed experimentally see exercise c def remove self value remove ﬁrst occurrence of value or raise valueerror note we do not consider shrinking the dynamic array in this version for k in range self n if self a k value found a match for j in range k self n shift others to ﬁll gap self a j self a j self a self n none help garbage collection self n we have one less item return exit immediately raise valueerror value not found only reached if no match code fragment implementation of remove for our dynamicarray class efﬁciency of python sequence types extending a list python provides a method named extend that is used to add all elements of one list to the end of a second list in effect a call to data extend other produces the same outcome as the code for element in other data append element in either case the running time is proportional to the length of the other list and amortized because the underlying array for the ﬁrst list may be resized to accom modate the additional elements in practice the extend method is preferable to repeated calls to append because the constant factors hidden in the asymptotic analysis are signiﬁcantly smaller the greater efﬁciency of extend is threefold first there is always some advantage to using an appropriate python method because those methods are often implemented natively in a compiled language rather than as interpreted python code second there is less overhead to a single function call that accomplishes all the work versus many individual function calls finally increased efﬁciency of extend comes from the fact that the resulting size of the updated list can be calculated in advance if the second data set is quite large there is some risk that the underlying dynamic array might be resized multiple times when using repeated calls to append with a single call to extend at most one resize operation will be performed exercise c explores the relative efﬁciency of these two approaches experimentally constructing new lists there are several syntaxes for constructing new lists in almost all cases the asymp totic efﬁciency of the behavior is linear in the length of the list that is created how ever as with the case in the preceding discussion of extend there are signiﬁcant differences in the practical efﬁciency section introduces the topic of list comprehension using an example such as squares k k for k in range n as a shorthand for squares for k in range n squares append k k experiments should show that the list comprehension syntax is signiﬁcantly faster than building the list by repeatedly appending see exercise c similarly it is a common python idiom to initialize a list of constant values using the multiplication operator as in n to produce a list of length n with all values equal to zero not only is this succinct for the programmer it is more efﬁcient than building such a list incrementally chapter array based sequences python string class strings are very important in python we introduced their use in chapter with a discussion of various operator syntaxes in section a comprehensive sum mary of the named methods of the class is given in tables a through a of appendix a we will not formally analyze the efﬁciency of each of those behav iors in this section but we do wish to comment on some notable issues in general we let n denote the length of a string for operations that rely on a second string as a pattern we let m denote the length of that pattern string the analysis for many behaviors is quite intuitive for example methods that produce a new string e g capitalize center strip require time that is linear in the length of the string that is produced many of the behaviors that test boolean conditions of a string e g islower take o n time examining all n characters in the worst case but short circuiting as soon as the answer becomes evident e g islower can immediately return false if the ﬁrst character is uppercased the comparison operators e g fall into this category as well pattern matching some of the most interesting behaviors from an algorithmic point of view are those that in some way depend upon ﬁnding a string pattern within a larger string this goal is at the heart of methods such as contains ﬁnd index count replace and split string algorithms will be the topic of chapter and this particular problem known as pattern matching will be the focus of section a naive im plementation runs in o mn time case because we consider the n m possible starting indices for the pattern and we spend o m time at each starting position checking if the pattern matches however in section we will develop an al gorithm for ﬁnding a pattern of length m within a longer string of length n in o n time composing strings finally we wish to comment on several approaches for composing large strings as an academic exercise assume that we have a large string named document and our goal is to produce a new string letters that contains only the alphabetic characters of the original string e g with spaces numbers and punctuation removed it may be tempting to compose a result through repeated concatenation as follows warning do not do this letters start with empty string for c in document if c isalpha letters c concatenate alphabetic character efﬁciency of python sequence types while the preceding code fragment accomplishes the goal it may be terribly inefﬁcient because strings are immutable the command letters c would presumably compute the concatenation letters c as a new string instance and then reassign the identiﬁer letters to that result constructing that new string would require time proportional to its length if the ﬁnal result has n characters the series of concatenations would take time proportional to the familiar sum n and therefore o time inefﬁcient code of this type is widespread in python perhaps because of the somewhat natural appearance of the code and mistaken presumptions about how the operator is evaluated with strings some later implementations of the python interpreter have developed an optimization to allow such code to complete in linear time but this is not guaranteed for all python implementations the op timization is as follows the reason that a command letters c causes a new string instance to be created is that the original string must be left unchanged if another variable in a program refers to that string on the other hand if python knew that there were no other references to the string in question it could imple ment more efﬁciently by directly mutating the string as a dynamic array as it happens the python interpreter already maintains what are known as reference counts for each object this count is used in part to determine if an object can be garbage collected see section but in this context it provides a means to detect when no other references exist to a string thereby allowing the optimization a more standard python idiom to guarantee linear time composition of a string is to use a temporary list to store individual pieces and then to rely on the join method of the str class to compose the ﬁnal result using this technique with our previous example would appear as follows temp start with empty list for c in document if c isalpha temp append c append alphabetic character letters join temp compose overall result this approach is guaranteed to run in o n time first we note that the series of up to n append calls will require a total of o n time as per the deﬁnition of the amortized cost of that operation the ﬁnal call to join also guarantees that it takes time that is linear in the ﬁnal length of the composed string as we discussed at the end of the previous section we can further improve the practical execution time by using a list comprehension syntax to build up the temporary list rather than by repeated calls to append that solution appears as letters join c for c in document if c isalpha better yet we can entirely avoid the temporary list with a generator comprehension letters join c for c in document if c isalpha chapter array based sequences using array based sequences storing high scores for a game the ﬁrst application we study is storing a sequence of high score entries for a video game this is representative of many applications in which a sequence of objects must be stored we could just as easily have chosen to store records for patients in a hospital or the names of players on a football team nevertheless let us focus on storing high score entries which is a simple application that is already rich enough to present some important data structuring concepts to begin we consider what information to include in an object representing a high score entry obviously one component to include is an integer representing the score itself which we identify as score another useful thing to include is the name of the person earning this score which we identify as name we could go on from here adding ﬁelds representing the date the score was earned or game statistics that led to that score however we omit such details to keep our example simple a python class gameentry representing a game entry is given in code fragment class gameentry represents one entry of a list of high scores def init self name score self name name self score score def get name self return self name def get score self return self score def str self return format self name self score e g bob code fragment python code for a simple gameentry class we include meth ods for returning the name and score for a game entry object as well as a method for returning a string representation of this entry using array based sequences a class for high scores to maintain a sequence of high scores we develop a class named scoreboard a scoreboard is limited to a certain number of high scores that can be saved once that limit is reached a new score only qualiﬁes for the scoreboard if it is strictly higher than the lowest high score on the board the length of the desired scoreboard may depend on the game perhaps or since that limit may vary depending on the game we allow it to be speciﬁed as a parameter to our scoreboard constructor internally we will use a python list named board in order to manage the gameentry instances that represent the high scores since we expect the score board to eventually reach full capacity we initialize the list to be large enough to hold the maximum number of scores but we initially set all entries to none by allocating the list with maximum capacity initially it never needs to be resized as entries are added we will maintain them from highest to lowest score starting at index of the list we illustrate a typical state of the data structure in figure figure an illustration of an ordered list of length ten storing references to six gameentry objects in the cells from index to with the rest being none a complete python implementation of the scoreboard class is given in code fragment the constructor is rather simple the command self board none capacity creates a list with the desired length yet all entries equal to none we maintain an additional instance variable n that represents the number of actual entries currently in our table for convenience our class supports the getitem method to retrieve an entry at a given index with a syntax board i or none if no such entry exists and we support a simple str method that returns a string representation of the entire scoreboard with one entry per line chapter array based sequences class scoreboard fixed length sequence of high scores in nondecreasing order def init self capacity initialize scoreboard with given maximum capacity all entries are initially none self board none capacity reserve space for future scores self n number of actual entries def getitem self k return entry at index k return self board k def str self return string representation of the high score list return n join str self board j for j in range self n def add self entry consider adding entry to high scores score entry get score does new entry qualify as a high score answer is yes if board not full or score is higher than last entry good self n len self board or score self board get score if good if self n len self board no score drops from list self n so overall number increases shift lower scores rightward to make room for new entry j self n while j and self board j get score score self board j self board j shift entry from j to j j and decrement j self board j entry when done add new entry code fragment python code for a scoreboard class that maintains an ordered series of scores as gameentry objects using array based sequences adding an entry the most interesting method of the scoreboard class is add which is responsible for considering the addition of a new entry to the scoreboard keep in mind that every entry will not necessarily qualify as a high score if the board is not yet full any new entry will be retained once the board is full a new entry is only retained if it is strictly better than one of the other scores in particular the last entry of the scoreboard which is the lowest of the high scores when a new score is considered we begin by determining whether it qualiﬁes as a high score if so we increase the count of active scores n unless the board is already at full capacity in that case adding a new high score causes some other entry to be dropped from the scoreboard so the overall number of entries remains the same to correctly place a new entry within the list the ﬁnal task is to shift any in ferior scores one spot lower with the least score being dropped entirely when the scoreboard is full this process is quite similar to the implementation of the insert method of the list class as described on pages in the context of our score board there is no need to shift any none references that remain near the end of the array so the process can proceed as diagrammed in figure figure adding a new gameentry for jill to the scoreboard in order to make room for the new reference we have to shift the references for game entries with smaller scores than the new one to the right by one cell then we can insert the new entry with index to implement the ﬁnal stage we begin by considering index j self n which is the index at which the last gameentry instance will reside after complet ing the operation either j is the correct index for the newest entry or one or more immediately before it will have lesser scores the while loop at line checks the compound condition shifting references rightward and decrementing j as long as there is another entry at index j with a score less than the new score chapter array based sequences sorting a sequence in the previous subsection we considered an application for which we added an ob ject to a sequence at a given position while shifting other elements so as to keep the previous order intact in this section we use a similar technique to solve the sorting problem that is starting with an unordered sequence of elements and rearranging them into nondecreasing order the insertion sort algorithm we study several sorting algorithms in this book most of which are described in chapter as a warm up in this section we describe a nice simple sorting al gorithm known as insertion sort the algorithm proceeds as follows for an array based sequence we start with the ﬁrst element in the array one element by itself is already sorted then we consider the next element in the array if it is smaller than the ﬁrst we swap them next we consider the third element in the array we swap it leftward until it is in its proper order with the ﬁrst two elements we then consider the fourth element and swap it leftward until it is in the proper order with the ﬁrst three we continue in this manner with the ﬁfth element the sixth and so on until the whole array is sorted we can express the insertion sort algorithm in pseudo code as shown in code fragment algorithm insertionsort a input an array a of n comparable elements output the array a with elements rearranged in nondecreasing order for k from to n do insert a k at its proper location within a a a k code fragment high level description of the insertion sort algorithm this is a simple high level description of insertion sort if we look back to code fragment of section we see that the task of inserting a new en try into the list of high scores is almost identical to the task of inserting a newly considered element in insertion sort except that game scores were ordered from high to low we provide a python implementation of insertion sort in code frag ment using an outer loop to consider each element in turn and an inner loop that moves a newly considered element to its proper location relative to the sorted subarray of elements that are to its left we illustrate an example run of the insertion sort algorithm in figure the nested loops of insertion sort lead to an o running time in the worst case the most work is done if the array is initially in reverse order on the other hand if the initial array is nearly sorted or perfectly sorted insertion sort runs in o n time because there are few or no iterations of the inner loop using array based sequences def insertion sort a sort list of comparable elements into nondecreasing order for k in range len a from to n cur a k current element to be inserted j k ﬁnd correct index j for current while j and a j cur element a j must be after current a j a j j a j cur cur is now in the right place code fragment python code for performing insertion sort on a list cur no move move no move no move move move move done figure execution of the insertion sort algorithm on an array of eight charac ters each row corresponds to an iteration of the outer loop and each copy of the sequence in a row corresponds to an iteration of the inner loop the current element that is being inserted is highlighted in the array and shown as the cur value chapter array based sequences simple cryptography an interesting application of strings and lists is cryptography the science of secret messages and their applications this ﬁeld studies ways of performing encryp tion which takes a message called the plaintext and converts it into a scrambled message called the ciphertext likewise cryptography also studies corresponding ways of performing decryption which takes a ciphertext and turns it back into its original plaintext arguably the earliest encryption scheme is the caesar cipher which is named after julius caesar who used this scheme to protect important military messages all of caesar messages were written in latin of course which already makes them unreadable for most of us the caesar cipher is a simple way to obscure a message written in a language that forms words with an alphabet the caesar cipher involves replacing each letter in a message with the letter that is a certain number of letters after it in the alphabet so in an english message we might replace each a with d each b with e each c with f and so on if shifting by three characters we continue this approach all the way up to w which is replaced with z then we let the substitution pattern wrap around so that we replace x with a y with b and z with c converting between strings and character lists given that strings are immutable we cannot directly edit an instance to encrypt it instead our goal will be to generate a new string a convenient technique for per forming string transformations is to create an equivalent list of characters edit the list and then reassemble a new string based on the list the ﬁrst step can be per formed by sending the string as a parameter to the constructor of the list class for example the expression list bird produces the result b i r d conversely we can use a list of characters to build a string by invoking the join method on an empty string with the list of characters as the parameter for exam ple the call join b i r d returns the string bird using characters as array indices if we were to number our letters like array indices so that a is b is c is and so on then we can write the caesar cipher with a rotation of r as a simple formula replace each letter i with the letter i r mod where mod is the modulo operator which returns the remainder after performing an integer division this operator is denoted with in python and it is exactly the operator we need to easily perform the wrap around at the end of the alphabet for mod is mod is and mod is the decryption algorithm for the caesar cipher is just the opposite we replace each letter with the one r places before it with wrap around that is letter i is replaced by letter i r mod using array based sequences we can represent a replacement rule using another string to describe the trans lation as a concrete example suppose we are using a caesar cipher with a three character rotation we can precompute a string that represents the replacements that should be used for each character from a to z for example a should be re placed by d b replaced by e and so on the replacement characters in order are defghijklmnopqrstuvwxyzabc we can subsequently use this translation string as a guide to encrypt a message the remaining challenge is how to quickly locate the replacement for each character of the original message fortunately we can rely on the fact that characters are represented in unicode by integer code points and the code points for the uppercase letters of the latin alphabet are consecutive for simplicity we restrict our encryption to uppercase letters python supports functions that convert between integer code points and one character strings speciﬁcally the function ord c takes a one character string as a parameter and returns the integer code point for that character conversely the function chr j takes an integer and returns its associated one character string in order to ﬁnd a replacement for a character in our caesar cipher we need to map the characters a to z to the respective numbers to the formula for doing that conversion is j ord c ord a as a sanity check if character c is a we have that j when c is b we will ﬁnd that its ordinal value is pre cisely one more than that for a so their difference is in general the integer j that results from such a calculation can be used as an index into our precomputed translation string as illustrated in figure encoder array d e f g h i j k l m n o p q r s t u v w x y z a b c using t as an index in unicode ord t ord a here is the replacement for t figure illustrating the use of uppercase characters as indices in this case to perform the replacement rule for caesar cipher encryption in code fragment we develop a python class for performing the caesar cipher with an arbitrary rotational shift and demonstrate its use when we run this program to perform a simple test we get the following output secret wkh hdjoh lv lq sodb phhw dw mrh v message the eagle is in play meet at joe s the constructor for the class builds the forward and backward translation strings for the given rotation with those in hand the encryption and decryption algorithms are essentially the same and so we perform both by means of a nonpublic utility method named transform chapter array based sequences class caesarcipher class for doing encryption and decryption using a caesar cipher def init self shift construct caesar cipher using given integer shift for rotation encoder none temp array for encryption decoder none temp array for decryption for k in range encoder k chr k shift ord a decoder k chr k shift ord a self forward join encoder will store as string self backward join decoder since ﬁxed def encrypt self message return string representing encripted message return self transform message self forward def decrypt self secret return decrypted message given encrypted secret return self transform secret self backward def transform self original code utility to perform transformation based on given code string msg list original for k in range len msg if msg k isupper j ord msg k ord a index from to msg k code j replace this character return join msg if name main cipher caesarcipher message the eagle is in play meet at joe s coded cipher encrypt message print secret coded answer cipher decrypt coded print message answer code fragment a complete python class for the caesar cipher multidimensional data sets multidimensional data sets lists tuples and strings in python are one dimensional we use a single index to access each element of the sequence many computer applications involve mul tidimensional data sets for example computer graphics are often modeled in either two or three dimensions geographic information may be naturally repre sented in two dimensions medical imaging may provide three dimensional scans of a patient and a company valuation is often based upon a high number of in dependent ﬁnancial measures that can be modeled as multidimensional data a two dimensional array is sometimes also called a matrix we may use two indices say i and j to refer to the cells in the matrix the ﬁrst index usually refers to a row number and the second to a column number and these are traditionally zero indexed in computer science figure illustrates a two dimensional data set with integer values this data might for example represent the number of stores in various regions of manhattan figure illustration of a two dimensional integer data set which has rows and columns the rows and columns are zero indexed if this data set were named stores the value of stores is and the value of stores is a common representation for a two dimensional data set in python is as a list of lists in particular we can represent a two dimensional array as a list of rows with each row itself being a list of values for example the two dimensional data 750 might be stored in python as follows data 750 an advantage of this representation is that we can naturally use a syntax such as data to represent the value that has row index and column index as data the second entry in the outer list is itself a list and thus indexable chapter array based sequences constructing a multidimensional list to quickly initialize a one dimensional list we generally rely on a syntax such as data n to create a list of n zeros on page we emphasized that from a technical perspective this creates a list of length n with all entries referencing the same integer instance but that there was no meaningful consequence of such aliasing because of the immutability of the int class in python we have to be considerably more careful when creating a list of lists if our goal were to create the equivalent of a two dimensional list of integers with r rows and c columns and to initialize all values to zero a ﬂawed approach might be to try the command data c r warning this is a mistake while c is indeed a list of c zeros multiplying that list by r unfortunately cre ates a single list with length r c just as results in list a better yet still ﬂawed attempt is to make a list that contains the list of c zeros as its only element and then to multiply that list by r that is we could try the command data c r warning still a mistake this is much closer as we actually do have a structure that is formally a list of lists the problem is that all r entries of the list known as data are references to the same instance of a list of c zeros figure provides a portrayal of such aliasing figure a ﬂawed representation of a data set as a list of lists created with the command data for simplicity we overlook the fact that the values in the secondary list are referential this is truly a problem setting an entry such as data would change the ﬁrst entry of the secondary list to reference a new value yet that cell of the secondary list also represents the value data because row data and row data refer to the same secondary list multidimensional data sets figure a valid representation of a data set as a list of lists for simplic ity we overlook the fact that the values in the secondary lists are referential to properly initialize a two dimensional list we must ensure that each cell of the primary list refers to an independent instance of a secondary list this can be accomplished through the use of python list comprehension syntax data c for j in range r this command produces a valid conﬁguration similar to the one shown in fig ure by using list comprehension the expression c is reevaluated for each pass of the embedded for loop therefore we get r distinct secondary lists as desired we note that the variable j in that command is irrelevant we simply need a for loop that iterates r times two dimensional arrays and positional games many computer games be they strategy games simulation games or ﬁrst person conﬂict games involve objects that reside in a two dimensional space software for such positional games need a way of representing such a two dimensional board and in python the list of lists is a natural choice tic tac toe as most school children know tic tac toe is a game played in a three by three board two players x and o alternate in placing their respective marks in the cells of this board starting with player x if either player succeeds in getting three of his or her marks in a row column or diagonal then that player wins this is admittedly not a sophisticated positional game and it not even that much fun to play since a good player o can always force a tie tic tac toe saving grace is that it is a nice simple example showing how two dimensional arrays can be used for positional games software for more sophisticated positional games such as checkers chess or the popular simulation games are all based on the same approach we illustrate here for using a two dimensional array for tic tac toe chapter array based sequences our representation of a board will be a list of lists of characters with x or o designating a player move or designating an empty space for example the board conﬁguration o x o x o x will be stored internally as o x o x o x we develop a complete python class for maintaining a tic tac toe board for two players that class will keep track of the moves and report a winner but it does not perform any strategy or allow someone to play tic tac toe against the computer the details of such a program are beyond the scope of this chapter but it might nonetheless make a good course project see exercise p before presenting the implementation of the class we demonstrate its public interface with a simple test in code fragment game tictactoe x moves o moves game mark game mark game mark game mark game mark game mark game mark game mark game mark print game winner game winner if winner is none print tie else print winner wins code fragment a simple test for our tic tac toe class the basic operations are that a new game instance represents an empty board that the mark i j method adds a mark at the given position for the current player with the software managing the alternating of turns and that the game board can be printed and the winner determined the complete source code for the tictactoe class is given in code fragment our mark method performs error checking to make sure that valid indices are sent that the position is not already occupied and that no further moves are made after someone wins the game multidimensional data sets class tictactoe management of a tic tac toe game does not do strategy def init self start a new game self board for j in range self player x def mark self i j put an x or o mark at position i j for next player turn if not i and j raise valueerror invalid board position if self board i j raise valueerror board position occupied if self winner is not none raise valueerror game is already complete self board i j self player if self player x self player o else self player x def is win self mark check whether the board conﬁguration is a win for the given player board self board local variable for shorthand return mark board board board or row mark board board board or row mark board board board or row mark board board board or column mark board board board or column mark board board board or column mark board board board or diagonal mark board board board rev diag def winner self return mark of winning player or none to indicate a tie for mark in xo if self is win mark return mark return none def str self return string representation of current game board rows join self board r for r in range return n n join rows code fragment a complete python class for managing a tic tac toe game chapter array based sequences exercises for help with exercises please visit the site www wiley com college goodrich reinforcement r execute the experiment from code fragment and compare the results on your system to those we report in code fragment r in code fragment we perform an experiment to compare the length of a python list to its underlying memory usage determining the sequence of array sizes requires a manual inspection of the output of that program redesign the experiment so that the program outputs only those values of k at which the existing capacity is exhausted for example on a system consistent with the results of code fragment your program should output that the sequence of array capacities are r modify the experiment from code fragment in order to demonstrate that python list class occasionally shrinks the size of its underlying array when elements are popped from a list r our dynamicarray class as given in code fragment does not support use of negative indices with getitem update that method to better match the semantics of a python list r redo the justiﬁcation of proposition assuming that the the cost of growing the array from size k to size is cyber dollars how much should each append operation be charged to make the amortization work r our implementation of insert for the dynamicarray class as given in code fragment has the following inefﬁciency in the case when a re size occurs the resize operation takes time to copy all the elements from an old array to a new array and then the subsequent loop in the body of insert shifts many of those elements give an improved implementation of the insert method so that in the case of a resize the elements are shifted into their ﬁnal position during that operation thereby avoiding the subsequent shifting r let a be an array of size n containing integers from to n inclu sive with exactly one repeated describe a fast algorithm for ﬁnding the integer in a that is repeated r experimentally evaluate the efﬁciency of the pop method of python list class when using varying indices as a parameter as we did for insert on page report your results akin to table exercises r explain the changes that would have to be made to the program of code fragment so that it could perform the caesar cipher for messages that are written in an alphabet based language other than english such as greek russian or hebrew r the constructor for the caesarcipher class in code fragment can be implemented with a two line body by building the forward and back ward strings using a combination of the join method and an appropriate comprehension syntax give such an implementation r use standard control structures to compute the sum of all numbers in an n n data set represented as a list of lists r describe how the built in sum function can be combined with python comprehension syntax to compute the sum of all numbers in an n n data set represented as a list of lists creativity c in the experiment of code fragment we begin with an empty list if data were initially constructed with nonempty length does this affect the sequence of values at which the underlying array is expanded perform your own experiments and comment on any relationship you see between the initial length and the expansion sequence c the shuﬄe method supported by the random module takes a python list and rearranges it so that every possible ordering is equally likely implement your own version of such a function you may rely on the randrange n function of the random module which returns a random number between and n inclusive c consider an implementation of a dynamic array but instead of copying the elements into an array of double the size that is from n to when its capacity is reached we copy the elements into an array with fn additional cells going from capacity n to capacity n fn prove that performing a sequence of n append operations still runs in o n time in this case c implement a pop method for the dynamicarray class given in code frag ment that removes the last element of the array and that shrinks the capacity n of the array by half any time the number of elements in the array goes below n c prove that when using a dynamic array that grows and shrinks as in the previous exercise the following series of operations takes o n time n append operations on an initially empty array followed by n pop oper ations chapter array based sequences c give a formal proof that any sequence of n append or pop operations on an initially empty dynamic array takes o n time if using the strategy described in exercise c c consider a variant of exercise c in which an array of capacity n is resized to capacity precisely that of the number of elements any time the number of elements in the array goes strictly below n give a formal proof that any sequence of n append or pop operations on an initially empty dynamic array takes o n time c consider a variant of exercise c in which an array of capacity n is resized to capacity precisely that of the number of elements any time the number of elements in the array goes strictly below n show that there exists a sequence of n operations that requires time to execute c in section we described four different ways to compose a long string repeated concatenation appending to a temporary list and then joining using list comprehension with join and using genera tor comprehension with join develop an experiment to test the efﬁciency of all four of these approaches and report your ﬁndings c develop an experiment to compare the relative efﬁciency of the extend method of python list class versus using repeated calls to append to accomplish the equivalent task c based on the discussion of page develop an experiment to compare the efﬁciency of python list comprehension syntax versus the construc tion of a list by means of repeated calls to append c perform experiments to evaluate the efﬁciency of the remove method of python list class as we did for insert on page use known values so that all removals occur either at the beginning middle or end of the list report your results akin to table c the syntax data remove value for python list data removes only the ﬁrst occurrence of element value from the list give an implementation of a function with signature remove all data value that removes all occur rences of value from the given list such that the worst case running time of the function is o n on a list with n elements not that it is not efﬁcient enough in general to rely on repeated calls to remove c let b be an array of size n containing integers from to n inclu sive with exactly ﬁve repeated describe a good algorithm for ﬁnding the ﬁve integers in b that are repeated c given a python list l of n positive integers each represented with k flog bits describe an o n time method for ﬁnding a k bit integer not in l c argue why any solution to the previous problem must run in n time chapter notes c a useful operation in databases is the natural join if we view a database as a list of ordered pairs of objects then the natural join of databases a and b is the list of all ordered triples x y z such that the pair x y is in a and the pair y z is in b describe and analyze an efﬁcient algorithm for computing the natural join of a list a of n pairs and a list b of m pairs c when bob wants to send alice a message m on the internet he breaks m into n data packets numbers the packets consecutively and injects them into the network when the packets arrive at alice computer they may be out of order so alice must assemble the sequence of n packets in order before she can be sure she has the entire message describe an efﬁcient scheme for alice to do this assuming that she knows the value of n what is the running time of this algorithm c describe a way to use recursion to add all the numbers in an n n data set represented as a list of lists projects p write a python function that takes two three dimensional numeric data sets and adds them componentwise p write a python program for a matrix class that can add and multiply two dimensional arrays of numbers assuming the dimensions agree appropri ately for the operation p write a program that can perform the caesar cipher for english messages that include both upper and lowercase characters p implement a class substitutioncipher with a constructor that takes a string with the uppercase letters in an arbitrary order and uses that for the forward mapping for encryption akin to the self forward string in our caesarcipher class of code fragment you should derive the backward mapping from the forward version p redesign the caesarcipher class as a subclass of the substitutioncipher from the previous problem p design a randomcipher class as a subclass of the substitutioncipher from exercise p so that each instance of the class relies on a random permutation of letters for its mapping chapter notes the fundamental data structures of arrays belong to the folklore of computer science they were ﬁrst chronicled in the computer science literature by knuth in his seminal book on fundamental algorithms chapter stacks queues and deques contents stacks the stack abstract data type simple array based stack implementation reversing data using a stack matching parentheses and html tags queues the queue abstract data type array based queue implementation double ended queues the deque abstract data type implementing a deque with a circular array deques in the python collections module exercises stacks stacks a stack is a collection of objects that are inserted and removed according to the last in ﬁrst out lifo principle a user may insert objects into a stack at any time but may only access or remove the most recently inserted object that remains at the so called top of the stack the name stack is derived from the metaphor of a stack of plates in a spring loaded cafeteria plate dispenser in this case the fundamental operations involve the pushing and popping of plates on the stack when we need a new plate from the dispenser we pop the top plate off the stack and when we add a plate we push it down on the stack to become the new top plate perhaps an even more amusing example is a pez candy dispenser which stores mint candies in a spring loaded container that pops out the topmost candy in the stack when the top of the dispenser is lifted see figure stacks are a fundamental data structure they are used in many applications including the following example internet web browsers store the addresses of recently visited sites in a stack each time a user visits a new site that site address is pushed onto the stack of addresses the browser then allows the user to pop back to previously visited sites using the back button example text editors usually provide an undo mechanism that cancels re cent editing operations and reverts to former states of a document this undo oper ation can be accomplished by keeping text changes in a stack figure a schematic drawing of a pez dispenser a physical implementation of the stack adt pez is a registered trademark of pez candy inc chapter stacks queues and deques the stack abstract data type stacks are the simplest of all data structures yet they are also among the most important they are used in a host of different applications and as a tool for many more sophisticated data structures and algorithms formally a stack is an abstract data type adt such that an instance s supports the following two methods s push e add element e to the top of stack s s pop remove and return the top element from the stack s an error occurs if the stack is empty additionally let us deﬁne the following accessor methods for convenience s top return a reference to the top element of stack s without removing it an error occurs if the stack is empty s is empty return true if stack s does not contain any elements len s return the number of elements in stack s in python we implement this with the special method len by convention we assume that a newly created stack is empty and that there is no a priori bound on the capacity of the stack elements added to the stack can have arbitrary type example the following table shows a series of stack operations and their effects on an initially empty stack s of integers operation return value stack contents s push s push len s s pop s is empty false s pop s is empty true s pop error s push s push s top s push len s s pop s push s push s pop stacks simple array based stack implementation we can implement a stack quite easily by storing its elements in a python list the list class already supports adding an element to the end with the append method and removing the last element with the pop method so it is natural to align the top of the stack at the end of the list as shown in figure top figure implementing a stack with a python list storing the top element in the rightmost cell although a programmer could directly use the list class in place of a formal stack class lists also include behaviors e g adding or removing elements from arbitrary positions that would break the abstraction that the stack adt represents also the terminology used by the list class does not precisely align with traditional nomenclature for a stack adt in particular the distinction between append and push instead we demonstrate how to use a list for internal storage while providing a public interface consistent with a stack the adapter pattern the adapter design pattern applies to any context where we effectively want to modify an existing class so that its methods match those of a related but different class or interface one general way to apply the adapter pattern is to deﬁne a new class in such a way that it contains an instance of the existing class as a hidden ﬁeld and then to implement each method of the new class using methods of this hidden instance variable by applying the adapter pattern in this way we have created a new class that performs some of the same functions as an existing class but repackaged in a more convenient way in the context of the stack adt we can adapt python list class using the correspondences shown in table table realization of a stack s as an adaptation of a python list l chapter stacks queues and deques implementing a stack using a python list we use the adapter design pattern to deﬁne an arraystack class that uses an un derlying python list for storage we choose the name arraystack to emphasize that the underlying storage is inherently array based one question that remains is what our code should do if a user calls pop or top when the stack is empty our adt suggests that an error occurs but we must decide what type of error when pop is called on an empty python list it formally raises an indexerror as lists are index based sequences that choice does not seem appropriate for a stack since there is no assumption of indices instead we can deﬁne a new exception class that is more appropriate code fragment deﬁnes such an empty class as a trivial subclass of the python exception class class empty exception error attempting to access an element from an empty container pass code fragment deﬁnition for an empty exception class the formal deﬁnition for our arraystack class is given in code fragment the constructor establishes the member self data as an initially empty python list for internal storage the rest of the public stack behaviors are implemented using the corresponding adaptation that was outlined in table example usage below we present an example of the use of our arraystack class mirroring the operations at the beginning of example on page s arraystack contents s push contents s push contents print len s contents outputs print s pop contents outputs print s is empty contents outputs false print s pop contents outputs print s is empty contents outputs true s push contents s push contents print s top contents outputs s push contents print len s contents outputs print s pop contents outputs s push contents stacks class arraystack lifo stack implementation using a python list as underlying storage def init self create an empty stack self data nonpublic list instance def len self return the number of elements in the stack return len self data def is empty self return true if the stack is empty return len self data def push self e add element e to the top of the stack self data append e new item stored at end of list def top self return but do not remove the element at the top of the stack raise empty exception if the stack is empty if self is empty raise empty stack is empty return self data the last item in the list def pop self remove and return the element from the top of the stack i e lifo raise empty exception if the stack is empty if self is empty raise empty stack is empty return self data pop remove last item from list code fragment implementing a stack using a python list as storage chapter stacks queues and deques analyzing the array based stack implementation table shows the running times for our arraystack methods the analysis di rectly mirrors the analysis of the list class given in section the implementa tions for top is empty and len use constant time in the worst case the o time for push and pop are amortized bounds see section a typical call to either of these methods uses constant time but there is occasionally an o n time worst case where n is the current number of elements in the stack when an operation causes the list to resize its internal array the space usage for a stack is o n amortized table performance of our array based stack implementation the bounds for push and pop are amortized due to similar bounds for the list class the space usage is o n where n is the current number of elements in the stack avoiding amortization by reserving capacity in some contexts there may be additional knowledge that suggests a maximum size that a stack will reach our implementation of arraystack from code fragment begins with an empty list and expands as needed in the analysis of lists from section we emphasized that it is more efﬁcient in practice to construct a list with initial length n than it is to start with an empty list and append n items even though both approaches run in o n time as an alternate model for a stack we might wish for the constructor to accept a parameter specifying the maximum capacity of a stack and to initialize the data member to a list of that length implementing such a model requires signiﬁcant changes relative to code fragment the size of the stack would no longer be synonymous with the length of the list and pushes and pops of the stack would not require changing the length of the list instead we suggest maintaining a separate integer as an instance variable that denotes the current number of elements in the stack details of such an implementation are left as exercise c stacks reversing data using a stack as a consequence of the lifo protocol a stack can be used as a general tool to reverse a data sequence for example if the values and are pushed onto a stack in that order they will be popped from the stack in the order and then this idea can be applied in a variety of settings for example we might wish to print lines of a ﬁle in reverse order in order to display a data set in decreasing order rather than increasing order this can be accomplished by reading each line and pushing it onto a stack and then writing the lines in the order they are popped an implementation of such a process is given in code fragment def reverse ﬁle ﬁlename overwrite given ﬁle with its contents line by line reversed s arraystack original open ﬁlename for line in original s push line rstrip n we will re insert newlines when writing original close now we overwrite with contents in lifo order output open ﬁlename w reopening ﬁle overwrites original while not s is empty output write s pop n re insert newline characters output close code fragment a function that reverses the order of lines in a ﬁle one technical detail worth noting is that we intentionally strip trailing newlines from lines as they are read and then re insert newlines after each line when writing the resulting ﬁle our reason for doing this is to handle a special case in which the original ﬁle does not have a trailing newline for the ﬁnal line if we exactly echoed the lines read from the ﬁle in reverse order then the original last line would be fol lowed without newline by the original second to last line in our implementation we ensure that there will be a separating newline in the result the idea of using a stack to reverse a data set can be applied to other types of sequences for example exercise r explores the use of a stack to provide yet another solution for reversing the contents of a python list a recursive solution for this goal was discussed in section a more challenging task is to reverse the order in which elements are stored within a stack if we were to move them from one stack to another they would be reversed but if we were to then replace them into the original stack they would be reversed again thereby reverting to their original order exercise c explores a solution for this task chapter stacks queues and deques matching parentheses and html tags in this subsection we explore two related applications of stacks both of which involve testing for pairs of matching delimiters in our ﬁrst application we consider arithmetic expressions that may contain various pairs of grouping symbols such as parentheses and braces and brackets and each opening symbol must match its corresponding closing symbol for example a left bracket must match a corresponding right bracket as in the expression x y z the following examples further illustrate this concept correct correct incorrect incorrect incorrect we leave the precise deﬁnition of a matching group of symbols to exercise r an algorithm for matching delimiters an important task when processing arithmetic expressions is to make sure their delimiting symbols match up correctly code fragment presents a python im plementation of such an algorithm a discussion of the code follows def is matched expr return true if all delimiters are properly match false otherwise lefty opening delimiters righty respective closing delims s arraystack for c in expr if c in lefty s push c push left delimiter on stack elif c in righty if s is empty return false nothing to match with if righty index c lefty index s pop return false mismatched return s is empty were all symbols matched code fragment function for matching delimiters in an arithmetic expression stacks we assume the input is a sequence of characters such as x y z we perform a left to right scan of the original sequence using a stack s to facilitate the matching of grouping symbols each time we encounter an opening symbol we push that symbol onto s and each time we encounter a closing symbol we pop a symbol from the stack s assuming s is not empty and check that these two symbols form a valid pair if we reach the end of the expression and the stack is empty then the original expression was properly matched otherwise there must be an opening delimiter on the stack without a matching symbol if the length of the original expression is n the algorithm will make at most n calls to push and n calls to pop those calls run in a total of o n time even con sidering the amortized nature of the o time bound for those methods given that our selection of possible delimiters has constant size auxiliary tests such as c in lefty and righty index c each run in o time combining these operations the matching algorithm on a sequence of length n runs in o n time matching tags in a markup language another application of matching delimiters is in the validation of markup languages such as html or xml html is the standard format for hyperlinked documents on the internet and xml is an extensible markup language used for a variety of structured data sets we show a sample html document and a possible rendering in figure body center the little boat center p the storm tossed the little boat like a cheap sneaker in an old washing machine the three drunken fishermen were used to such treatment of course but not the tree salesman who even as a stowaway now felt that he had overpaid for the voyage p ol li will the salesman die li li what color is the boat li li and what about naomi li ol body the little boat the storm tossed the little boat like a cheap sneaker in an old washing machine the three drunken ﬁshermen were used to such treatment of course but not the tree salesman who even as a stowaway now felt that he had overpaid for the voyage will the salesman die what color is the boat and what about naomi a b figure illustrating html tags a an html document b its rendering chapter stacks queues and deques in an html document portions of text are delimited by html tags a simple opening html tag has the form name and the corresponding closing tag has the form name for example we see the body tag on the ﬁrst line of figure a and the matching body tag at the close of that document other commonly used html tags that are used in this example include body document body section header center center justify p paragraph ol numbered ordered list li list item ideally an html document should have matching tags although most browsers tolerate a certain number of mismatching tags in code fragment we give a python function that matches tags in a string representing an html document we make a left to right pass through the raw string using index j to track our progress and the ﬁnd method of the str class to locate the and characters that deﬁne the tags opening tags are pushed onto the stack and matched against closing tags as they are popped from the stack just as we did when matching delimiters in code fragment by similar analysis this algorithm runs in o n time where n is the number of characters in the raw html source def is matched html raw return true if all html tags are properly match false otherwise s arraystack j raw ﬁnd ﬁnd ﬁrst character if any while j k raw ﬁnd j ﬁnd next character if k return false invalid tag tag raw j k strip away if not tag startswith this is opening tag s push tag else this is closing tag if s is empty return false nothing to match with if tag s pop return false mismatched delimiter j raw ﬁnd k ﬁnd next character if any return s is empty were all opening tags matched code fragment function for testing if an html document has matching tags queues queues another fundamental data structure is the queue it is a close cousin of the stack as a queue is a collection of objects that are inserted and removed according to the ﬁrst in ﬁrst out fifo principle that is elements can be inserted at any time but only the element that has been in the queue the longest can be next removed we usually say that elements enter a queue at the back and are removed from the front a metaphor for this terminology is a line of people waiting to get on an amusement park ride people waiting for such a ride enter at the back of the line and get on the ride from the front of the line there are many other applications of queues see figure stores theaters reservation centers and other similar services typically process customer requests according to the fifo principle a queue would therefore be a logical choice for a data structure to handle calls to a customer service center or a wait list at a restaurant fifo queues are also used by many computing devices such as a networked printer or a web server responding to requests a a figure real world examples of a ﬁrst in ﬁrst out queue a people waiting in line to purchase tickets b phone calls being routed to a customer service center chapter stacks queues and deques the queue abstract data type formally the queue abstract data type deﬁnes a collection that keeps objects in a sequence where element access and deletion are restricted to the ﬁrst element in the queue and element insertion is restricted to the back of the sequence this restriction enforces the rule that items are inserted and deleted in a queue accord ing to the ﬁrst in ﬁrst out fifo principle the queue abstract data type adt supports the following two fundamental methods for a queue q q enqueue e add element e to the back of queue q q dequeue remove and return the ﬁrst element from queue q an error occurs if the queue is empty the queue adt also includes the following supporting methods with ﬁrst being analogous to the stack top method q ﬁrst return a reference to the element at the front of queue q without removing it an error occurs if the queue is empty q is empty return true if queue q does not contain any elements len q return the number of elements in queue q in python we implement this with the special method len by convention we assume that a newly created queue is empty and that there is no a priori bound on the capacity of the queue elements added to the queue can have arbitrary type example the following table shows a series of queue operations and their effects on an initially empty queue q of integers queues array based queue implementation for the stack adt we created a very simple adapter class that used a python list as the underlying storage it may be very tempting to use a similar approach for supporting the queue adt we could enqueue element e by calling append e to add it to the end of the list we could use the syntax pop as opposed to pop to intentionally remove the ﬁrst element from the list when dequeuing as easy as this would be to implement it is tragically inefﬁcient as we dis cussed in section when pop is called on a list with a non default index a loop is executed to shift all elements beyond the speciﬁed index to the left so as to ﬁll the hole in the sequence caused by the pop therefore a call to pop always causes the worst case behavior of n time we can improve on the above strategy by avoiding the call to pop entirely we can replace the dequeued entry in the array with a reference to none and main tain an explicit variable f to store the index of the element that is currently at the front of the queue such an algorithm for dequeue would run in o time after several dequeue operations this approach might lead to the conﬁguration portrayed in figure f figure allowing the front of the queue to drift away from index unfortunately there remains a drawback to the revised approach in the case of a stack the length of the list was precisely equal to the size of the stack even if the underlying array for the list was slightly larger with the queue design that we are considering the situation is worse we can build a queue that has relatively few elements yet which are stored in an arbitrarily large list this occurs for example if we repeatedly enqueue a new element and then dequeue another allowing the front to drift rightward over time the size of the underlying list would grow to o m where m is the total number of enqueue operations since the creation of the queue rather than the current number of elements in the queue this design would have detrimental consequences in applications in which queues have relatively modest size but which are used for long periods of time for example the wait list for a restaurant might never have more than entries at one time but over the course of a day or a week the overall number of entries would be signiﬁcantly larger chapter stacks queues and deques using an array circularly in developing a more robust queue implementation we allow the front of the queue to drift rightward and we allow the contents of the queue to wrap around the end of an underlying array we assume that our underlying array has ﬁxed length n that is greater that the actual number of elements in the queue new elements are enqueued toward the end of the current queue progressing from the front to index n and continuing at index then figure illustrates such a queue with ﬁrst element e and last element m f n figure modeling a queue with a circular array that wraps around the end implementing this circular view is not difﬁcult when we dequeue an element and want to advance the front index we use the arithmetic f f n re call that the operator in python denotes the modulo operator which is computed by taking the remainder after an integral division for example divided by has a quotient of with remainder that is so in python evaluates to the quotient while evaluates to the remainder the modulo operator is ideal for treating an array circularly as a concrete example if we have a list of length and a front index we can advance the front by formally computing which is simply as divided by is with a remainder of similarly advancing index results in index but when we advance from index the last one in the array we compute which evaluates to index as divided by has a remainder of zero a python queue implementation a complete implementation of a queue adt using a python list in circular fashion is presented in code fragments and internally the queue class maintains the following three instance variables data is a reference to a list instance with a ﬁxed capacity size is an integer representing the current number of elements stored in the queue as opposed to the length of the data list front is an integer that represents the index within data of the ﬁrst element of the queue assuming the queue is not empty we initially reserve a list of moderate size for storing data although the queue formally has size zero as a technicality we initialize the front index to zero when front or dequeue are called with no elements in the queue we raise an instance of the empty exception deﬁned in code fragment for our stack queues class arrayqueue fifo queue implementation using a python list as underlying storage default capacity moderate capacity for all new queues def init self create an empty queue self data none arrayqueue default capacity self size self front def len self return the number of elements in the queue return self size def is empty self return true if the queue is empty return self size def ﬁrst self return but do not remove the element at the front of the queue raise empty exception if the queue is empty if self is empty raise empty queue is empty return self data self front def dequeue self remove and return the ﬁrst element of the queue i e fifo raise empty exception if the queue is empty if self is empty raise empty queue is empty answer self data self front self data self front none help garbage collection self front self front len self data self size return answer code fragment array based implementation of a queue continued in code fragment chapter stacks queues and deques def enqueue self e add an element to the back of queue if self size len self data self resize len self data double the array size avail self front self size len self data self data avail e self size def resize self cap we assume cap len self resize to a new list of capacity len self old self data keep track of existing list self data none cap allocate list with new capacity walk self front for k in range self size only consider existing elements self data k old walk intentionally shift indices walk walk len old use old size as modulus self front front has been realigned code fragment array based implementation of a queue continued from code fragment the implementation of len and is empty are trivial given knowledge of the size the implementation of the front method is also simple as the front index tells us precisely where the desired element is located within the data list assuming that list is not empty adding and removing elements the goal of the enqueue method is to add a new element to the back of the queue we need to determine the proper index at which to place the new element although we do not explicitly maintain an instance variable for the back of the queue we compute the location of the next opening based on the formula avail self front self size len self data note that we are using the size of the queue as it exists prior to the addition of the new element for example consider a queue with capacity current size and ﬁrst element at index the three elements of such a queue are stored at indices and the new element should be placed at index front size in a case with wrap around the use of the modular arithmetic achieves the desired circular semantics for example if our hypothetical queue had elements with the ﬁrst at index our computation of evaluates to which is perfect since the three existing elements occupy indices and queues when the dequeue method is called the current value of self front designates the index of the value that is to be removed and returned we keep a local refer ence to the element that will be returned setting answer self data self front just prior to removing the reference to that object from the list with the assignment self data self front none our reason for the assignment to none relates to python mechanism for reclaiming unused space internally python maintains a count of the number of references that exist to each object if that count reaches zero the object is effectively inaccessible thus the system may reclaim that mem ory for future use for more details see section since we are no longer responsible for storing a dequeued element we remove the reference to it from our list so as to reduce that element reference count the second signiﬁcant responsibility of the dequeue method is to update the value of front to reﬂect the removal of the element and the presumed promotion of the second element to become the new ﬁrst in most cases we simply want to increment the index by one but because of the possibility of a wrap around conﬁguration we rely on modular arithmetic as originally described on page resizing the queue when enqueue is called at a time when the size of the queue equals the size of the underlying list we rely on a standard technique of doubling the storage capacity of the underlying list in this way our approach is similar to the one used when we implemented a dynamicarray in section however more care is needed in the queue resize utility than was needed in the corresponding method of the dynamicarray class after creating a temporary reference to the old list of values we allocate a new list that is twice the size and copy references from the old list to the new list while transferring the contents we intentionally realign the front of the queue with index in the new array as shown in figure this realignment is not purely cosmetic since the modular arith metic depends on the size of the array our state would be ﬂawed had we transferred each element to its same index in the new array f i j k e f g h e f g h i j k f figure resizing the queue while realigning the front element with index chapter stacks queues and deques shrinking the underlying array a desirable property of a queue implementation is to have its space usage be n where n is the current number of elements in the queue our arrayqueue imple mentation as given in code fragments and does not have this property it expands the underlying array when enqueue is called with the queue at full ca pacity but the dequeue implementation never shrinks the underlying array as a consequence the capacity of the underlying array is proportional to the maximum number of elements that have ever been stored in the queue not the current number of elements we discussed this very issue on page in the context of dynamic arrays and in subsequent exercises c through c of that chapter a robust approach is to reduce the array to half of its current size whenever the number of elements stored in it falls below one fourth of its capacity we can implement this strategy by adding the following two lines of code in our dequeue method just after reducing self size at line of code fragment to reﬂect the loss of an element if self size len self data self resize len self data analyzing the array based queue implementation table describes the performance of our array based implementation of the queue adt assuming the improvement described above for occasionally shrinking the size of the array with the exception of the resize utility all of the methods rely on a constant number of statements involving arithmetic operations comparisons and assignments therefore each method runs in worst case o time except for enqueue and dequeue which have amortized bounds of o time for reasons similar to those given in section amortized table performance of an array based implementation of a queue the bounds for enqueue and dequeue are amortized due to the resizing of the array the space usage is o n where n is the current number of elements in the queue double ended queues double ended queues we next consider a queue like data structure that supports insertion and deletion at both the front and the back of the queue such a structure is called a double ended queue or deque which is usually pronounced deck to avoid confusion with the dequeue method of the regular queue adt which is pronounced like the abbreviation d q the deque abstract data type is more general than both the stack and the queue adts the extra generality can be useful in some applications for example we described a restaurant using a queue to maintain a waitlist occassionally the ﬁrst person might be removed from the queue only to ﬁnd that a table was not available typically the restaurant will re insert the person at the ﬁrst position in the queue it may also be that a customer at the end of the queue may grow impatient and leave the restaurant we will need an even more general data structure if we want to model customers leaving the queue from other positions the deque abstract data type to provide a symmetrical abstraction the deque adt is deﬁned so that deque d supports the following methods d add ﬁrst e add element e to the front of deque d d add last e add element e to the back of deque d d delete ﬁrst remove and return the ﬁrst element from deque d an error occurs if the deque is empty d delete last remove and return the last element from deque d an error occurs if the deque is empty additionally the deque adt will include the following accessors d ﬁrst return but do not remove the ﬁrst element of deque d an error occurs if the deque is empty d last return but do not remove the last element of deque d an error occurs if the deque is empty d is empty return true if deque d does not contain any elements len d return the number of elements in deque d in python we implement this with the special method len chapter stacks queues and deques example the following table shows a series of operations and their effects on an initially empty deque d of integers operation return value deque d add last d add ﬁrst d add ﬁrst d ﬁrst d delete last len d d delete last d delete last d add ﬁrst d last d add ﬁrst d is empty false d last implementing a deque with a circular array we can implement the deque adt in much the same way as the arrayqueue class provided in code fragments and of section so much so that we leave the details of an arraydeque implementation to exercise p we recommend maintaining the same three instance variables data size and front whenever we need to know the index of the back of the deque or the ﬁrst available slot beyond the back of the deque we use modular arithmetic for the computation for example our implementation of the last method uses the index back self front self size len self data our implementation of the arraydeque add last method is essentially the same as that for arrayqueue enqueue including the reliance on a resize utility like wise the implementation of the arraydeque delete ﬁrst method is the same as arrayqueue dequeue implementations of add ﬁrst and delete last use similar techniques one subtlety is that a call to add ﬁrst may need to wrap around the beginning of the array so we rely on modular arithmetic to circularly decrement the index as self front self front len self data cyclic shift the efﬁciency of an arraydeque is similar to that of an arrayqueue with all operations having o running time but with that bound being amortized for op erations that may change the size of the underlying list double ended queues deques in the python collections module an implementation of a deque class is available in python standard collections module a summary of the most commonly used behaviors of the collections deque class is given in table it uses more asymmetric nomenclature than our adt our deque adt collections deque description len d len d number of elements d add ﬁrst d appendleft add to beginning d add last d append add to end d delete ﬁrst d popleft remove from beginning d delete last d pop remove from end d ﬁrst d access ﬁrst element d last d access last element table comparison of our deque adt and the collections deque class the collections deque interface was chosen to be consistent with established naming conventions of python list class for which append and pop are presumed to act at the end of the list therefore appendleft and popleft designate an opera tion at the beginning of the list the library deque also mimics a list in that it is an indexed sequence allowing arbitrary access or modiﬁcation using the d j syntax the library deque constructor also supports an optional maxlen parameter to force a ﬁxed length deque however if a call to append at either end is invoked when the deque is full it does not throw an error instead it causes one element to be dropped from the opposite side that is calling appendleft when the deque is full causes an implicit pop from the right side to make room for the new element the current python distribution implements collections deque with a hybrid ap proach that uses aspects of circular arrays but organized into blocks that are them selves organized in a doubly linked list a data structure that we will introduce in the next chapter the deque class is formally documented to guarantee o time operations at either end but o n time worst case operations when using index notation near the middle of the deque chapter stacks queues and deques exercises for help with exercises please visit the site www wiley com college goodrich reinforcement r what values are returned during the following series of stack operations if executed upon an initially empty stack push push pop push push pop pop push push pop push push pop pop push pop pop r suppose an initially empty stack s has executed a total of push opera tions top operations and pop operations of which raised empty errors that were caught and ignored what is the current size of s r implement a function with signature transfer s t that transfers all ele ments from stack s onto stack t so that the element that starts at the top of s is the ﬁrst to be inserted onto t and the element at the bottom of s ends up at the top of t r give a recursive method for removing all the elements from a stack r implement a function that reverses a list of elements by pushing them onto a stack in one order and writing them back to the list in reversed order r give a precise and complete deﬁnition of the concept of matching for grouping symbols in an arithmetic expression your deﬁnition may be recursive r what values are returned during the following sequence of queue opera tions if executed on an initially empty queue enqueue enqueue dequeue enqueue enqueue dequeue dequeue enqueue enqueue dequeue enqueue enqueue dequeue dequeue enqueue dequeue dequeue r suppose an initially empty queue q has executed a total of enqueue operations ﬁrst operations and dequeue operations of which raised empty errors that were caught and ignored what is the current size of q r had the queue of the previous problem been an instance of arrayqueue that used an initial array of capacity and had its size never been greater than what would be the ﬁnal value of the front instance variable r consider what happens if the loop in the arrayqueue resize method at lines of code fragment had been implemented as for k in range self size self data k old k rather than old walk give a clear explanation of what could go wrong exercises r give a simple adapter that implements our queue adt while using a collections deque instance for storage r what values are returned during the following sequence of deque adt op erations on initially empty deque add ﬁrst add last add last add ﬁrst back delete ﬁrst delete last add last ﬁrst last add last delete ﬁrst delete ﬁrst r suppose you have a deque d containing the numbers in this order suppose further that you have an initially empty queue q give a code fragment that uses only d and q and no other variables and results in d storing the elements in the order r repeat the previous problem using the deque d and an initially empty stack s creativity c suppose alice has picked three distinct integers and placed them into a stack s in random order write a short straight line piece of pseudo code with no loops or recursion that uses only one comparison and only one variable x yet that results in variable x storing the largest of alice three integers with probability argue why your method is correct c modify the arraystack implementation so that the stack capacity is lim ited to maxlen elements where maxlen is an optional parameter to the constructor that defaults to none if push is called when the stack is at full capacity throw a full exception deﬁned similarly to empty c in the previous exercise we assume that the underlying list is initially empty redo that exercise this time preallocating an underlying list with length equal to the stack maximum capacity c show how to use the transfer function described in exercise r and two temporary stacks to replace the contents of a given stack s with those same elements but in reversed order c in code fragment we assume that opening tags in html have form name as with li more generally html allows optional attributes to be expressed as part of an opening tag the general form used is name value2 for example a table can be given a border and additional padding by using an opening tag of table border cellpadding modify code frag ment so that it can properly match tags even when an opening tag may include one or more such attributes c describe a nonrecursive algorithm for enumerating all permutations of the numbers n using an explicit stack chapter stacks queues and deques c show how to use a stack s and a queue q to generate all possible subsets of an n element set t nonrecursively c postﬁx notation is an unambiguous way of writing an arithmetic expres sion without parentheses it is deﬁned so that if op is a normal fully parenthesized expression whose operation is op the postﬁx version of this is op where is the postﬁx version of and is the postﬁx version of the postﬁx version of a sin gle number or variable is just that number or variable for example the postﬁx version of is describe a nonrecursive way of evaluating an expression in postﬁx notation c suppose you have three nonempty stacks r s and t describe a sequence of operations that results in s storing all elements originally in t below all of s original elements with both sets of those elements in their original order the ﬁnal conﬁguration for r should be the same as its original conﬁguration for example if r s and t the ﬁnal conﬁguration should have r and s c describe how to implement the stack adt using a single queue as an instance variable and only constant additional local memory within the method bodies what is the running time of the push pop and top methods for your design c describe how to implement the queue adt using two stacks as instance variables such that all queue operations execute in amortized o time give a formal proof of the amortized bound c describe how to implement the double ended queue adt using two stacks as instance variables what are the running times of the methods c suppose you have a stack s containing n elements and a queue q that is initially empty describe how you can use q to scan s to see if it contains a certain element x with the additional constraint that your algorithm must return the elements back to s in their original order you may only use s q and a constant number of other variables c modify the arrayqueue implementation so that the queue capacity is limited to maxlen elements where maxlen is an optional parameter to the constructor that defaults to none if enqueue is called when the queue is at full capacity throw a full exception deﬁned similarly to empty c in certain applications of the queue adt it is common to repeatedly dequeue an element process it in some way and then immediately en queue the same element modify the arrayqueue implementation to in clude a rotate method that has semantics identical to the combina tion q enqueue q dequeue however your implementation should be more efﬁcient than making two separate calls for example because there is no need to modify size exercises c alice has two queues q and r which can store integers bob gives alice odd integers and even integers and insists that she store all integers in q and r they then play a game where bob picks q or r at random and then applies the round robin scheduler described in the chapter to the chosen queue a random number of times if the last number to be processed at the end of this game was odd bob wins otherwise alice wins how can alice allocate integers to queues to optimize her chances of winning what is her chance of winning c suppose bob has four cows that he wants to take across a bridge but only one yoke which can hold up to two cows side by side tied to the yoke the yoke is too heavy for him to carry across the bridge but he can tie and untie cows to it in no time at all of his four cows mazie can cross the bridge in minutes daisy can cross it in minutes crazy can cross it in minutes and lazy can cross it in minutes of course when two cows are tied to the yoke they must go at the speed of the slower cow describe how bob can get all his cows across the bridge in minutes projects p give a complete arraydeque implementation of the double ended queue adt as sketched in section p give an array based implementation of a double ended queue supporting all of the public behaviors shown in table for the collections deque class including use of the maxlen optional parameter when a length limited deque is full provide semantics similar to the collections deque class whereby a call to insert an element on one end of a deque causes an element to be lost from the opposite side p implement a program that can input an expression in postﬁx notation see exercise c and output its value p the introduction of section notes that stacks are often used to provide undo support in applications like a web browser or text editor while support for undo can be implemented with an unbounded stack many applications provide only limited support for such an undo history with a ﬁxed capacity stack when push is invoked with the stack at full capacity rather than throwing a full exception as described in exercise c a more typical semantic is to accept the pushed element at the top while leaking the oldest element from the bottom of the stack to make room give an implementation of such a leakystack abstraction using a circular array with appropriate storage capacity this class should have a public interface similar to the bounded capacity stack in exercise c but with the desired leaky semantics when full chapter stacks queues and deques p when a share of common stock of some company is sold the capital gain or sometimes loss is the difference between the share selling price and the price originally paid to buy it this rule is easy to under stand for a single share but if we sell multiple shares of stock bought over a long period of time then we must identify the shares actually be ing sold a standard accounting principle for identifying which shares of a stock were sold in such a case is to use a fifo protocol the shares sold are the ones that have been held the longest indeed this is the de fault method built into several personal ﬁnance software packages for example suppose we buy shares at each on day shares at on day shares at on day and then sell shares on day at each then applying the fifo protocol means that of the shares sold were bought on day were bought on day and were bought on day the capital gain in this case would therefore be or write a program that takes as input a sequence of transactions of the form buy x share at y each or sell x share at y each assuming that the transactions oc cur on consecutive days and the values x and y are integers given this input sequence the output should be the total capital gain or loss for the entire sequence using the fifo protocol to identify shares p design an adt for a two color double stack adt that consists of two stacks one red and one blue and has as its operations color coded versions of the regular stack adt operations for example this adt should support both a red push operation and a blue push operation give an efﬁcient implementation of this adt using a single array whose ca pacity is set at some value n that is assumed to always be larger than the sizes of the red and blue stacks combined chapter notes we were introduced to the approach of deﬁning data structures ﬁrst in terms of their adts and then in terms of concrete implementations by the classic books by aho hopcroft and ullman exercises c and c are similar to interview questions said to be from a well known software company for further study of abstract data types see liskov and guttag cardelli and wegner or demurjian chapter linked lists contents singly linked lists implementing a stack with a singly linked list implementing a queue with a singly linked list circularly linked lists round robin schedulers implementing a queue with a circularly linked list doubly linked lists basic implementation of a doubly linked list implementing a deque with a doubly linked list the positional list adt the positional list abstract data type doubly linked list implementation sorting a positional list case study maintaining access frequencies using a sorted list using a list with the move to front heuristic link based vs array based sequences exercises chapter linked lists in chapter we carefully examined python array based list class and in chapter we demonstrated use of that class in implementing the classic stack queue and dequeue adts python list class is highly optimized and often a great choice for storage with that said there are some notable disadvantages the length of a dynamic array might be longer than the actual number of elements that it stores amortized bounds for operations may be unacceptable in real time systems insertions and deletions at interior positions of an array are expensive in this chapter we introduce a data structure known as a linked list which provides an alternative to an array based sequence such as a python list both array based sequences and linked lists keep elements in a certain order but us ing a very different style an array provides the more centralized representation with one large chunk of memory capable of accommodating references to many elements a linked list in contrast relies on a more distributed representation in which a lightweight object known as a node is allocated for each element each node maintains a reference to its element and one or more references to neighboring nodes in order to collectively represent the linear order of the sequence we will demonstrate a trade off of advantages and disadvantages when con trasting array based sequences and linked lists elements of a linked list cannot be efﬁciently accessed by a numeric index k and we cannot tell just by examining a node if it is the second ﬁfth or twentieth node in the list however linked lists avoid the three disadvantages noted above for array based sequences singly linked lists a singly linked list in its simplest form is a collection of nodes that collectively form a linear sequence each node stores a reference to an object that is an element of the sequence as well as a reference to the next node of the list see figures and msp element next figure example of a node instance that forms part of a singly linked list the node element member references an arbitrary object that is an element of the se quence the airport code msp in this example while the next member references the subsequent node of the linked list or none if there is no further node singly linked lists lax msp atl bos head tail figure example of a singly linked list whose elements are strings indicating airport codes the list instance maintains a member named head that identiﬁes the ﬁrst node of the list and in some applications another member named tail that identiﬁes the last node of the list the none object is denoted as ø the ﬁrst and last node of a linked list are known as the head and tail of the list respectively by starting at the head and moving from one node to another by following each node next reference we can reach the tail of the list we can identify the tail as the node having none as its next reference this process is commonly known as traversing the linked list because the next reference of a node can be viewed as a link or pointer to another node the process of traversing a list is also known as link hopping or pointer hopping a linked list representation in memory relies on the collaboration of many objects each node is represented as a unique object with that instance storing a reference to its element and a reference to the next node or none another object represents the linked list as a whole minimally the linked list instance must keep a reference to the head of the list without an explicit reference to the head there would be no way to locate that node or indirectly any others there is not an absolute need to store a direct reference to the tail of the list as it could otherwise be located by starting at the head and traversing the rest of the list however storing an explicit reference to the tail node is a common convenience to avoid such a traversal in similar regard it is common for the linked list instance to keep a count of the total number of nodes that comprise the list commonly described as the size of the list to avoid the need to traverse the list to count the nodes for the remainder of this chapter we continue to illustrate nodes as objects and each node next reference as a pointer however for the sake of simplicity we illustrate a node element embedded directly within the node structure even though the element is in fact an independent object for example figure is a more compact illustration of the linked list from figure head tail figure a compact illustration of a singly linked list with elements embedded in the nodes rather than more accurately drawn as references to external objects chapter linked lists inserting an element at the head of a singly linked list an important property of a linked list is that it does not have a predetermined ﬁxed size it uses space proportionally to its current number of elements when using a singly linked list we can easily insert an element at the head of the list as shown in figure and described with pseudo code in code fragment the main idea is that we create a new node set its element to the new element set its next link to refer to the current head and then set the list head to point to the new node head newest head a newest head b b figure insertion of an element at the head of a singly linked list a before the insertion b after creation of a new node c after reassignment of the head reference algorithm add ﬁrst l e newest node e create new node instance storing reference to element e newest next l head set new node next to reference the old head node l head newest set variable head to reference the new node l size l size increment the node count code fragment inserting a new element at the beginning of a singly linked list l note that we set the next pointer of the new node before we reassign variable l head to it if the list were initially empty i e l head is none then a natural consequence is that the new node has its next reference set to none singly linked lists inserting an element at the tail of a singly linked list we can also easily insert an element at the tail of the list provided we keep a reference to the tail node as shown in figure in this case we create a new node assign its next reference to none set the next reference of the tail to point to this new node and then update the tail reference itself to this new node we give the details in code fragment tail a tail newest b tail newest c figure insertion at the tail of a singly linked list a before the insertion b after creation of a new node c after reassignment of the tail reference note that we must set the next link of the tail in b before we assign the tail variable to point to the new node in c algorithm add last l e newest node e create new node instance storing reference to element e newest next none set new node next to reference the none object l tail next newest make old tail node point to new node l tail newest set variable tail to reference the new node l size l size increment the node count code fragment inserting a new node at the end of a singly linked list note that we set the next pointer for the old tail node before we make variable tail point to the new node this code would need to be adjusted for inserting onto an empty list since there would not be an existing tail node chapter linked lists removing an element from a singly linked list removing an element from the head of a singly linked list is essentially the reverse operation of inserting a new element at the head this operation is illustrated in figure and given in detail in code fragment head a head b head c figure removal of an element at the head of a singly linked list a before the removal b after linking out the old head c ﬁnal conﬁguration algorithm remove ﬁrst l if l head is none then indicate an error the list is empty l head l head next make head point to next node or none l size l size decrement the node count code fragment removing the node at the beginning of a singly linked list unfortunately we cannot easily delete the last node of a singly linked list even if we maintain a tail reference directly to the last node of the list we must be able to access the node before the last node in order to remove the last node but we cannot reach the node before the tail by following next links from the tail the only way to access this node is to start from the head of the list and search all the way through the list but such a sequence of link hopping operations could take a long time if we want to support such an operation efﬁciently we will need to make our list doubly linked as we do in section singly linked lists implementing a stack with a singly linked list in this section we demonstrate use of a singly linked list by providing a complete python implementation of the stack adt see section in designing such an implementation we need to decide whether to model the top of the stack at the head or at the tail of the list there is clearly a best choice here we can efﬁciently insert and delete elements in constant time only at the head since all stack operations affect the top we orient the top of the stack at the head of our list to represent individual nodes of the list we develop a lightweight node class this class will never be directly exposed to the user of our stack class so we will formally deﬁne it as a nonpublic nested class of our eventual linkedstack class see section for discussion of nested classes the deﬁnition of the node class is shown in code fragment class node lightweight nonpublic class for storing a singly linked node slots streamline memory usage def init self element next initialize node ﬁelds self element element reference to user element self next next reference to next node code fragment a lightweight node class for a singly linked list a node has only two instance variables element and next we intentionally deﬁne slots to streamline the memory usage see page of section for discussion because there may potentially be many node instances in a single list the constructor of the node class is designed for our convenience allowing us to specify initial values for both ﬁelds of a newly created node a complete implementation of our linkedstack class is given in code frag ments and each stack instance maintains two variables the head mem ber is a reference to the node at the head of the list or none if the stack is empty we keep track of the current number of elements with the size instance variable for otherwise we would be forced to traverse the entire list to count the number of elements when reporting the size of the stack the implementation of push essentially mirrors the pseudo code for insertion at the head of a singly linked list as outlined in code fragment when we push a new element e onto the stack we accomplish the necessary changes to the linked structure by invoking the constructor of the node class as follows self head self node e self head create and link a new node note that the next ﬁeld of the new node is set to the existing top node and then self head is reassigned to the new node chapter linked lists class linkedstack lifo stack implementation using a singly linked list for storage nested node class class node lightweight nonpublic class for storing a singly linked node slots streamline memory usage def init self element next initialize node ﬁelds self element element reference to user element self next next reference to next node stack methods def init self create an empty stack self head none reference to the head node self size number of stack elements def len self return the number of elements in the stack return self size def is empty self return true if the stack is empty return self size def push self e add element e to the top of the stack self head self node e self head create and link a new node self size def top self return but do not remove the element at the top of the stack raise empty exception if the stack is empty if self is empty raise empty stack is empty return self head element top of stack is at head of list code fragment implementation of a stack adt using a singly linked list con tinued in code fragment singly linked lists def pop self remove and return the element from the top of the stack i e lifo raise empty exception if the stack is empty if self is empty raise empty stack is empty answer self head element self head self head next bypass the former top node self size return answer code fragment implementation of a stack adt using a singly linked list con tinued from code fragment when implementing the top method the goal is to return the element that is at the top of the stack when the stack is empty we raise an empty exception as originally deﬁned in code fragment of chapter when the stack is nonempty self head is a reference to the ﬁrst node of the linked list the top element can be identiﬁed as self head element our implementation of pop essentially mirrors the pseudo code given in code fragment except that we maintain a local reference to the element that is stored at the node that is being removed and we return that element to the caller of pop the analysis of our linkedstack operations is given in table we see that all of the methods complete in worst case constant time this is in contrast to the amortized bounds for the arraystack that were given in table table performance of our linkedstack implementation all bounds are worst case and our space usage is o n where n is the current number of elements in the stack chapter linked lists implementing a queue with a singly linked list as we did for the stack adt we can use a singly linked list to implement the queue adt while supporting worst case o time for all operations because we need to perform operations on both ends of the queue we will explicitly maintain both a head reference and a tail reference as instance variables for each queue the natural orientation for a queue is to align the front of the queue with the head of the list and the back of the queue with the tail of the list because we must be able to enqueue elements at the back and dequeue them from the front recall from the introduction of section that we are unable to efﬁciently remove elements from the tail of a singly linked list our implementation of a linkedqueue class is given in code fragments and class linkedqueue fifo queue implementation using a singly linked list for storage class node lightweight nonpublic class for storing a singly linked node omitted here identical to that of linkedstack node def init self create an empty queue self head none self tail none self size number of queue elements def len self return the number of elements in the queue return self size def is empty self return true if the queue is empty return self size def ﬁrst self return but do not remove the element at the front of the queue if self is empty raise empty queue is empty return self head element front aligned with head of list code fragment implementation of a queue adt using a singly linked list continued in code fragment singly linked lists def dequeue self remove and return the ﬁrst element of the queue i e fifo raise empty exception if the queue is empty if self is empty raise empty queue is empty answer self head element self head self head next self size if self is empty special case as queue is empty self tail none removed head had been the tail return answer def enqueue self e add an element to the back of queue newest self node e none node will be new tail node if self is empty self head newest special case previously empty else self tail next newest self tail newest update reference to tail node self size code fragment implementation of a queue adt using a singly linked list continued from code fragment many aspects of our implementation are similar to that of the linkedstack class such as the deﬁnition of the nested node class our implementation of dequeue for linkedqueue is similar to that of pop for linkedstack as both remove the head of the linked list however there is a subtle difference because our queue must accurately maintain the tail reference no such variable was maintained for our stack in general an operation at the head has no effect on the tail but when dequeue is invoked on a queue with one element we are simultaneously removing the tail of the list we therefore set self tail to none for consistency there is a similar complication in our implementation of enqueue the newest node always becomes the new tail yet a distinction is made depending on whether that new node is the only node in the list in that case it also becomes the new head otherwise the new node must be linked immediately after the existing tail node in terms of performance the linkedqueue is similar to the linkedstack in that all operations run in worst case constant time and the space usage is linear in the current number of elements chapter linked lists circularly linked lists in section we introduced the notion of a circular array and demonstrated its use in implementing the queue adt in reality the notion of a circular array was artiﬁcial in that there was nothing about the representation of the array itself that was circular in structure it was our use of modular arithmetic when advancing an index from the last slot to the ﬁrst slot that provided such an abstraction in the case of linked lists there is a more tangible notion of a circularly linked list as we can have the tail of the list use its next reference to point back to the head of the list as shown in figure we call such a structure a circularly linked list head tail figure example of a singly linked list with circular structure a circularly linked list provides a more general model than a standard linked list for data sets that are cyclic that is which do not have any particular notion of a beginning and end figure provides a more symmetric illustration of the same circular list structure as figure figure example of a circular linked list with current denoting a reference to a select node a circular view similar to figure could be used for example to describe the order of train stops in the chicago loop or the order in which players take turns during a game even though a circularly linked list has no beginning or end per se we must maintain a reference to a particular node in order to make use of the list we use the identiﬁer current to describe such a designated node by setting current current next we can effectively advance through the nodes of the list circularly linked lists round robin schedulers to motivate the use of a circularly linked list we consider a round robin scheduler which iterates through a collection of elements in a circular fashion and services each element by performing a given action on it such a scheduler is used for example to fairly allocate a resource that must be shared by a collection of clients for instance round robin scheduling is often used to allocate slices of cpu time to various applications running concurrently on a computer a round robin scheduler could be implemented with the general queue adt by repeatedly performing the following steps on queue q see figure e q dequeue service element e q enqueue e figure the three iterative steps for round robin scheduling using a queue if we use of the linkedqueue class of section for such an application there is unnecessary effort in the combination of a dequeue operation followed soon after by an enqueue of the same element one node is removed from the list with appropriate adjustments to the head of the list and the size decremented and then a new node is created to reinsert at the tail of the list and the size is incremented if using a circularly linked list the effective transfer of an item from the head of the list to the tail of the list can be accomplished by advancing a reference that marks the boundary of the queue we will next provide an implementation of a circularqueue class that supports the entire queue adt together with an ad ditional method rotate that moves the ﬁrst element of the queue to the back a similar method is supported by the deque class of python collections module see table with this operation a round robin schedule can more efﬁciently be implemented by repeatedly performing the following steps service element q front q rotate chapter linked lists implementing a queue with a circularly linked list to implement the queue adt using a circularly linked list we rely on the intuition of figure in which the queue has a head and a tail but with the next reference of the tail linked to the head given such a model there is no need for us to explicitly store references to both the head and the tail as long as we keep a reference to the tail we can always ﬁnd the head by following the tail next reference code fragments and provide an implementation of a circularqueue class based on this model the only two instance variables are tail which is a reference to the tail node or none when empty and size which is the current number of elements in the queue when an operation involves the front of the queue we recognize self tail next as the head of the queue when enqueue is called a new node is placed just after the tail but before the current head and then the new node becomes the tail in addition to the traditional queue operations the circularqueue class supports a rotate method that more efﬁciently enacts the combination of removing the front element and reinserting it at the back of the queue with the circular representation we simply set self tail self tail next to make the old head become the new tail with the node after the old head becoming the new head class circularqueue queue implementation using circularly linked list for storage class node lightweight nonpublic class for storing a singly linked node omitted here identical to that of linkedstack node def init self create an empty queue self tail none will represent tail of queue self size number of queue elements def len self return the number of elements in the queue return self size def is empty self return true if the queue is empty return self size code fragment implementation of a circularqueue class using a circularly linked list as storage continued in code fragment circularly linked lists def ﬁrst self return but do not remove the element at the front of the queue raise empty exception if the queue is empty if self is empty raise empty queue is empty head self tail next return head element def dequeue self remove and return the ﬁrst element of the queue i e fifo raise empty exception if the queue is empty if self is empty raise empty queue is empty oldhead self tail next if self size removing only element self tail none queue becomes empty else self tail next oldhead next bypass the old head self size return oldhead element def enqueue self e add an element to the back of queue newest self node e none node will be new tail node if self is empty newest next newest initialize circularly else newest next self tail next new node points to head self tail next newest old tail points to new node self tail newest new node becomes the tail self size def rotate self rotate front element to the back of the queue if self size self tail self tail next old head becomes new tail code fragment implementation of a circularqueue class using a circularly linked list as storage continued from code fragment chapter linked lists doubly linked lists in a singly linked list each node maintains a reference to the node that is immedi ately after it we have demonstrated the usefulness of such a representation when managing a sequence of elements however there are limitations that stem from the asymmetry of a singly linked list in the opening of section we empha sized that we can efﬁciently insert a node at either end of a singly linked list and can delete a node at the head of a list but we are unable to efﬁciently delete a node at the tail of the list more generally we cannot efﬁciently delete an arbitrary node from an interior position of the list if only given a reference to that node because we cannot determine the node that immediately precedes the node to be deleted yet that node needs to have its next reference updated to provide greater symmetry we deﬁne a linked list in which each node keeps an explicit reference to the node before it and a reference to the node after it such a structure is known as a doubly linked list these lists allow a greater variety of o time update operations including insertions and deletions at arbitrary posi tions within the list we continue to use the term next for the reference to the node that follows another and we introduce the term prev for the reference to the node that precedes it header and trailer sentinels in order to avoid some special cases when operating near the boundaries of a doubly linked list it helps to add special nodes at both ends of the list a header node at the beginning of the list and a trailer node at the end of the list these dummy nodes are known as sentinels or guards and they do not store elements of the primary sequence a doubly linked list with such sentinels is shown in figure header next prev next next next prev prev prev trailer figure a doubly linked list representing the sequence jfk pvd sfo using sentinels header and trailer to demarcate the ends of the list when using sentinel nodes an empty list is initialized so that the next ﬁeld of the header points to the trailer and the prev ﬁeld of the trailer points to the header the remaining ﬁelds of the sentinels are irrelevant presumably none in python for a nonempty list the header next will refer to a node containing the ﬁrst real element of a sequence just as the trailer prev references the node containing the last element of a sequence doubly linked lists advantage of using sentinels although we could implement a doubly linked list without sentinel nodes as we did with our singly linked list in section the slight extra space devoted to the sentinels greatly simpliﬁes the logic of our operations most notably the header and trailer nodes never change only the nodes between them change furthermore we can treat all insertions in a uniﬁed manner because a new node will always be placed between a pair of existing nodes in similar fashion every element that is to be deleted is guaranteed to be stored in a node that has neighbors on each side for contrast look back at our linkedqueue implementation from section its enqueue method given in code fragment adds a new node to the end of the list however its implementation required a conditional to manage the special case of inserting into an empty list in the general case the new node was linked after the existing tail but when adding to an empty list there is no existing tail instead it is necessary to reassign self head to reference the new node the use of a sentinel node in that implementation would eliminate the special case as there would always be an existing node possibly the header before a new node inserting and deleting with a doubly linked list every insertion into our doubly linked list representation will take place between a pair of existing nodes as diagrammed in figure for example when a new element is inserted at the front of the sequence we will simply add the new node between the header and the node that is currently after the header see figure header a trailer trailer header b trailer c figure adding an element to a doubly linked list with header and trailer sen tinels a before the operation b after creating the new node c after linking the neighbors to the new node chapter linked lists header a trailer trailer header b trailer c figure adding an element to the front of a sequence represented by a dou bly linked list with header and trailer sentinels a before the operation b after creating the new node c after linking the neighbors to the new node the deletion of a node portrayed in figure proceeds in the opposite fash ion of an insertion the two neighbors of the node to be deleted are linked directly to each other thereby bypassing the original node as a result that node will no longer be considered part of the list and it can be reclaimed by the system because of our use of sentinels the same implementation can be used when deleting the ﬁrst or the last element of a sequence because even such an element will be stored at a node that lies between two others header a trailer trailer header b trailer c figure removing the element pvd from a doubly linked list a before the removal b after linking out the old node c after the removal and garbage collection doubly linked lists basic implementation of a doubly linked list we begin by providing a preliminary implementation of a doubly linked list in the form of a class named doublylinkedbase we intentionally name the class with a leading underscore because we do not intend for it to provide a coherent public interface for general use we will see that linked lists can support general insertions and deletions in o worst case time but only if the location of an operation can be succinctly identiﬁed with array based sequences an integer index was a convenient means for describing a position within a sequence however an index is not convenient for linked lists as there is no efﬁcient way to ﬁnd the jth element it would seem to require a traversal of a portion of the list when working with a linked list the most direct way to describe the location of an operation is by identifying a relevant node of the list however we prefer to encapsulate the inner workings of our data structure to avoid having users di rectly access nodes of a list in the remainder of this chapter we will develop two public classes that inherit from our doublylinkedbase class to provide more coherent abstractions speciﬁcally in section we provide a linkeddeque class that implements the double ended queue adt introduced in section that class only supports operations at the ends of the queue so there is no need for a user to identify an interior position within the list in section we introduce a new positionallist abstraction that provides a public interface that allows arbitrary insertions and deletions from a list our low level doublylinkedbase class relies on the use of a nonpublic node class that is similar to that for a singly linked list as given in code fragment except that the doubly linked version includes a prev attribute in addition to the next and element attributes as shown in code fragment class node lightweight nonpublic class for storing a doubly linked node slots _next streamline memory def init self element prev next initialize node ﬁelds self element element user element self prev prev previous node reference self next next next node reference code fragment a python node class for use in a doubly linked list the remainder of our doublylinkedbase class is given in code fragment the constructor instantiates the two sentinel nodes and links them directly to each other we maintain a size member and provide public support for len and is empty so that these behaviors can be directly inherited by the subclasses chapter linked lists class doublylinkedbase a base class providing a doubly linked list representation class node lightweight nonpublic class for storing a doubly linked node omitted here see previous code fragment def init self create an empty list self header self node none none none self trailer self node none none none self header next self trailer trailer is after header self trailer prev self header header is before trailer self size number of elements def len self return the number of elements in the list return self size def is empty self return true if list is empty return self size def insert between self e predecessor successor add element e between two existing nodes and return new node newest self node e predecessor successor linked to neighbors predecessor next newest successor prev newest self size return newest def delete node self node delete nonsentinel node from the list and return its element predecessor node prev successor node next predecessor next successor successor prev predecessor self size element node element record deleted element node prev node next node element none deprecate node return element return deleted element code fragment a base class for managing a doubly linked list doubly linked lists the other two methods of our class are the nonpublic utilities insert between and delete node these provide generic support for insertions and deletions re spectively but require one or more node references as parameters the implemen tation of the insert between method is modeled upon the algorithm that was previ ously portrayed in figure it creates a new node with that node ﬁelds initial ized to link to the speciﬁed neighboring nodes then the ﬁelds of the neighboring nodes are updated to include the newest node in the list for later convenience the method returns a reference to the newly created node the implementation of the delete node method is modeled upon the algorithm portrayed in figure the neighbors of the node to be deleted are linked directly to each other thereby bypassing the deleted node from the list as a formality we intentionally reset the prev next and element ﬁelds of the deleted node to none after recording the element to be returned although the deleted node will be ignored by the rest of the list setting its ﬁelds to none is advantageous as it may help python garbage collection since unnecessary links to the other nodes and the stored element are eliminated we will also rely on this conﬁguration to recognize a node as deprecated when it is no longer part of the list implementing a deque with a doubly linked list the double ended queue deque adt was introduced in section with an array based implementation we achieve all operations in amortized o time due to the occasional need to resize the array with an implementation based upon a doubly linked list we can achieve all deque operation in worst case o time we provide an implementation of a linkeddeque class code fragment that inherits from the doublylinkedbase class of the preceding section we do not provide an explicit init method for the linkeddeque class as the inherited version of that method sufﬁces to initialize a new instance we also rely on the inherited methods len and is empty in meeting the deque adt with the use of sentinels the key to our implementation is to remember that the header does not store the ﬁrst element of the deque it is the node just after the header that stores the ﬁrst element assuming the deque is nonempty similarly the node just before the trailer stores the last element of the deque we use the inherited insert between method to insert at either end of the deque to insert an element at the front of the deque we place it immediately between the header and the node just after the header an insertion at the end of deque is placed immediately before the trailer node note that these operations succeed even when the deque is empty in such a situation the new node is placed between the two sentinels when deleting an element from a nonempty deque we rely upon the inherited delete node method knowing that the designated node is assured to have neighbors on each side chapter linked lists class linkeddeque doublylinkedbase note the use of inheritance double ended queue implementation based on a doubly linked list def ﬁrst self return but do not remove the element at the front of the deque if self is empty raise empty deque is empty return self header next element real item just after header def last self return but do not remove the element at the back of the deque if self is empty raise empty deque is empty return self trailer prev element real item just before trailer def insert ﬁrst self e add an element to the front of the deque self insert between e self header self header next after header def insert last self e add an element to the back of the deque self insert between e self trailer prev self trailer before trailer def delete ﬁrst self remove and return the element from the front of the deque raise empty exception if the deque is empty if self is empty raise empty deque is empty return self delete node self header next use inherited method def delete last self remove and return the element from the back of the deque raise empty exception if the deque is empty if self is empty raise empty deque is empty return self delete node self trailer prev use inherited method code fragment implementation of a linkeddeque class that inherits from the doublylinkedbase class the positional list adt the positional list adt the abstract data types that we have considered thus far namely stacks queues and double ended queues only allow update operations that occur at one end of a sequence or the other we wish to have a more general abstraction for example although we motivated the fifo semantics of a queue as a model for customers who are waiting to speak with a customer service representative or fans who are waiting in line to buy tickets to a show the queue adt is too limiting what if a waiting customer decides to hang up before reaching the front of the customer service queue or what if someone who is waiting in line to buy tickets allows a friend to cut into line at that position we would like to design an abstract data type that provides a user a way to refer to elements anywhere in a sequence and to perform arbitrary insertions and deletions when working with array based sequences such as a python list integer in dices provide an excellent means for describing the location of an element or the location at which an insertion or deletion should take place however numeric in dices are not a good choice for describing positions within a linked list because we cannot efﬁciently access an entry knowing only its index ﬁnding an element at a given index within a linked list requires traversing the list incrementally from its beginning or end counting elements as we go furthermore indices are not a good abstraction for describing a local position in some applications because the index of an entry changes over time due to inser tions or deletions that happen earlier in the sequence for example it may not be convenient to describe the location of a person waiting in line by knowing precisely how far away that person is from the front of the line we prefer an abstraction as characterized in figure in which there is some other means for describing a position we then wish to model situations such as when an identiﬁed person leaves the line before reaching the front or in which a new person is added to a line immediately behind another identiﬁed person me figure we wish to be able to identify the position of an element in a sequence without the use of an integer index chapter linked lists as another example a text document can be viewed as a long sequence of characters a word processor uses the abstraction of a cursor to describe a position within the document without explicit use of an integer index allowing operations such as delete the character at the cursor or insert a new character just after the cursor furthermore we may be able to refer to an inherent position within a doc ument such as the beginning of a particular section without relying on a character index or even a section number that may change as the document evolves a node reference as a position one of the great beneﬁts of a linked list structure is that it is possible to perform o time insertions and deletions at arbitrary positions of the list as long as we are given a reference to a relevant node of the list it is therefore very tempting to develop an adt in which a node reference serves as the mechanism for describing a position in fact our doublylinkedbase class of section has methods insert between and delete node that accept node references as parameters however such direct use of nodes would violate the object oriented design principles of abstraction and encapsulation that were introduced in chapter there are several reasons to prefer that we encapsulate the nodes of a linked list for both our sake and for the beneﬁt of users of our abstraction it will be simpler for users of our data structure if they are not bothered with unnecessary details of our implementation such as low level manipulation of nodes or our reliance on the use of sentinel nodes notice that to use the insert between method of our doublylinkedbase class to add a node at the beginning of a sequence the header sentinel must be sent as a parameter we can provide a more robust data structure if we do not permit users to directly access or manipulate the nodes in that way we ensure that users cannot invalidate the consistency of a list by mismanaging the linking of nodes a more subtle problem arises if a user were allowed to call the insert between or delete node method of our doublylinkedbase class sending a node that does not belong to the given list as a parameter go back and look at that code and see why it causes a problem by better encapsulating the internal details of our implementation we have greater ﬂexibility to redesign the data structure and improve its performance in fact with a well designed abstraction we can provide a notion of a non numeric position even if using an array based sequence for these reasons instead of relying directly on nodes we introduce an inde pendent position abstraction to denote the location of an element within a list and then a complete positional list adt that can encapsulate a doubly linked list or even an array based sequence see exercise p the positional list adt the positional list abstract data type to provide for a general abstraction of a sequence of elements with the ability to identify the location of an element we deﬁne a positional list adt as well as a simpler position abstract data type to describe a location within a list a position acts as a marker or token within the broader positional list a position p is unaf fected by changes elsewhere in a list the only way in which a position becomes invalid is if an explicit command is issued to delete it a position instance is a simple object supporting only the following method p element return the element stored at position p in the context of the positional list adt positions serve as parameters to some methods and as return values from other methods in describing the behaviors of a positional list we being by presenting the accessor methods supported by a list l l ﬁrst return the position of the ﬁrst element of l or none if l is empty l last return the position of the last element of l or none if l is empty l before p return the position of l immediately before position p or none if p is the ﬁrst position l after p return the position of l immediately after position p or none if p is the last position l is empty return true if list l does not contain any elements len l return the number of elements in the list iter l return a forward iterator for the elements of the list see sec tion for discussion of iterators in python the positional list adt also includes the following update methods l add ﬁrst e insert a new element e at the front of l returning the position of the new element l add last e insert a new element e at the back of l returning the position of the new element l add before p e insert a new element e just before position p in l returning the position of the new element l add after p e insert a new element e just after position p in l returning the position of the new element l replace p e replace the element at position p with element e returning the element formerly at position p l delete p remove and return the element at position p in l invalidat ing the position for those methods of the adt that accept a position p as a parameter an error occurs if p is not a valid position for list l chapter linked lists note well that the ﬁrst and last methods of the positional list adt return the associated positions not the elements this is in contrast to the corresponding ﬁrst and last methods of the deque adt the ﬁrst element of a positional list can be determined by subsequently invoking the element method on that position as l ﬁrst element the advantage of receiving a position as a return value is that we can use that position to navigate the list for example the following code fragment prints all elements of a positional list named data cursor data ﬁrst while cursor is not none print cursor element print the element stored at the position cursor data after cursor advance to the next position if any this code relies on the stated convention that the none object is returned when after is called upon the last position that return value is clearly distinguishable from any legitimate position the positional list adt similarly indicates that the none value is returned when the before method is invoked at the front of the list or when ﬁrst or last methods are called upon an empty list therefore the above code fragment works correctly even if the data list is empty because the adt includes support for python iter function users may rely on the traditional for loop syntax for such a forward traversal of a list named data for e in data print e more general navigational and update methods of the positional list adt are shown in the following example example the following table shows a series of operations on an initially empty positional list l to identify position instances we use variables such as p and q for ease of exposition when displaying the list contents we use subscript notation to denote its positions operation return value l l add last p l ﬁrst p l add after p q l before q p l add before q r r element l after p r l before p none 5q l add ﬁrst 5q l delete l last 8p l replace p 7p 3r the positional list adt doubly linked list implementation in this section we present a complete implementation of a positionallist class using a doubly linked list that satisﬁes the following important proposition proposition each method of the positional list adt runs in worst case o time when implemented with a doubly linked list we rely on the doublylinkedbase class from section for our low level representation the primary responsibility of our new class is to provide a public interface in accordance with the positional list adt we begin our class deﬁnition in code fragment with the deﬁnition of the public position class nested within our positionallist class position instances will be used to represent the locations of elements within the list our various positionallist methods may end up creating redundant position instances that reference the same underlying node for example when ﬁrst and last are the same for that reason our position class deﬁnes the eq and ne special methods so that a test such as p q evaluates to true when two positions refer to the same node validating positions each time a method of the positionallist class accepts a position as a parameter we want to verify that the position is valid and if so to determine the underlying node associated with the position this functionality is implemented by a non public method named validate internally a position maintains a reference to the associated node of the linked list and also a reference to the list instance that con tains the speciﬁed node with the container reference we can robustly detect when a caller sends a position instance that does not belong to the indicated list we are also able to detect a position instance that belongs to the list but that refers to a node that is no longer part of that list recall that the delete node of the base class sets the previous and next references of a deleted node to none we can recognize that condition to detect a deprecated node access and update methods the access methods of the positionallist class are given in code fragment and the update methods are given in code fragment all of these methods trivially adapt the underlying doubly linked list implementation to support the pub lic interface of the positional list adt those methods rely on the validate utility to unwrap any position that is sent they also rely on a make position utility to wrap nodes as position instances to return to the user making sure never to return a position referencing a sentinel for convenience we have overridden the inherited insert between utility method so that ours returns a position associated with the newly created node whereas the inherited version returns the node itself chapter linked lists class positionallist doublylinkedbase a sequential container of elements allowing positional access nested position class class position an abstraction representing the location of a single element def init self container node constructor should not be invoked by user self container container self node node def element self return the element stored at this position return self node element def eq self other return true if other is a position representing the same location return type other is type self and other node is self node def ne self other return true if other does not represent the same location return not self other opposite of eq utility method def validate self p return position node or raise appropriate error if invalid if not isinstance p self position raise typeerror p must be proper position type if p container is not self raise valueerror p does not belong to this container if p node next is none convention for deprecated nodes raise valueerror p is no longer valid return p node code fragment a positionallist class based on a doubly linked list contin ues in code fragments and the positional list adt utility method def make position self node return position instance for given node or none if sentinel if node is self header or node is self trailer return none boundary violation else return self position self node legitimate position accessors def ﬁrst self return the ﬁrst position in the list or none if list is empty return self make position self header next def last self return the last position in the list or none if list is empty return self make position self trailer prev def before self p return the position just before position p or none if p is ﬁrst node self validate p return self make position node prev def after self p return the position just after position p or none if p is last node self validate p return self make position node next def iter self generate a forward iteration of the elements of the list cursor self ﬁrst while cursor is not none yield cursor element cursor self after cursor code fragment a positionallist class based on a doubly linked list contin ued from code fragment continues in code fragment chapter linked lists mutators override inherited version to return position rather than node def insert between self e predecessor successor add element between existing nodes and return new position node super insert between e predecessor successor return self make position node def add ﬁrst self e insert element e at the front of the list and return new position return self insert between e self header self header next def add last self e insert element e at the back of the list and return new position return self insert between e self trailer prev self trailer def add before self p e insert element e into list before position p and return new position original self validate p return self insert between e original prev original def add after self p e insert element e into list after position p and return new position original self validate p return self insert between e original original next def delete self p remove and return the element at position p original self validate p return self delete node original inherited method returns element def replace self p e replace the element at position p with e return the element formerly at position p original self validate p old value original element temporarily store old element original element e replace with new element return old value return the old element value code fragment a positionallist class based on a doubly linked list contin ued from code fragments and sorting a positional list sorting a positional list in section we introduced the insertion sort algorithm in the context of an array based sequence in this section we develop an implementation that operates on a positionallist relying on the same high level algorithm in which each element is placed relative to a growing collection of previously sorted elements we maintain a variable named marker that represents the rightmost position of the currently sorted portion of a list during each pass we consider the position just past the marker as the pivot and consider where the pivot element belongs relative to the sorted portion we use another variable named walk to move leftward from the marker as long as there remains a preceding element with value larger than the pivot a typical conﬁguration of these variables is diagrammed in figure a python implementation of this strategy is given in code walk pivot marker figure overview of one step of our insertion sort algorithm the shaded elements those up to and including marker have already been sorted in this step the pivot element should be relocated immediately before the walk position def insertion sort l sort positionallist of comparable elements into nondecreasing order if len l otherwise no need to sort it marker l ﬁrst while marker l last pivot l after marker next item to place value pivot element if value marker element pivot is already sorted marker pivot pivot becomes new marker else must relocate pivot walk marker ﬁnd leftmost item greater than value while walk l ﬁrst and l before walk element value walk l before walk l delete pivot l add before walk value reinsert value before walk code fragment python code for performing insertion sort on a positional list chapter linked lists case study maintaining access frequencies the positional list adt is useful in a number of settings for example a program that simulates a game of cards could model each person hand as a positional list exercise p since most people keep cards of the same suit together inserting and removing cards from a person hand could be implemented using the methods of the positional list adt with the positions being determined by a natural order of the suits likewise a simple text editor embeds the notion of positional insertion and deletion since such editors typically perform all updates relative to a cursor which represents the current position in the list of characters of text being edited in this section we consider maintaining a collection of elements while keeping track of the number of times each element is accessed keeping such access counts allows us to know which elements are among the most popular examples of such scenarios include a web browser that keeps track of a user most accessed urls or a music collection that maintains a list of the most frequently played songs for a user we model this with a new favorites list adt that supports the len and is empty methods as well as the following access e access the element e incrementing its access count and adding it to the favorites list if it is not already present remove e remove element e from the favorites list if present top k return an iteration of the k most accessed elements using a sorted list our ﬁrst approach for managing a list of favorites is to store elements in a linked list keeping them in nonincreasing order of access counts we access or remove an element by searching the list from the most frequently accessed to the least frequently accessed reporting the top k most accessed elements is easy as they are the ﬁrst k entries of the list to maintain the invariant that elements are stored in nonincreasing order of access counts we must consider how a single access operation may affect the order the accessed element count increases by one and so it may become larger than one or more of its preceding neighbors in the list thereby violating the invariant fortunately we can reestablish the sorted invariant using a technique similar to a single pass of the insertion sort algorithm introduced in the previous section we can perform a backward traversal of the list starting at the position of the element whose access count has increased until we locate a valid position after which the element can be relocated case study maintaining access frequencies using the composition pattern we wish to implement a favorites list by making use of a positionallist for storage if elements of the positional list were simply elements of the favorites list we would be challenged to maintain access counts and to keep the proper count with the associated element as the contents of the list are reordered we use a general object oriented design pattern the composition pattern in which we deﬁne a single object that is composed of two or more other objects speciﬁcally we deﬁne a nonpublic nested class item that stores the element and its access count as a single instance we then maintain our favorites list as a positionallist of item instances so that the access count for a user element is embedded alongside it in our representation an item is never exposed to a user of a favoriteslist class favoriteslist list of elements ordered from most frequently accessed to least nested item class class item slots streamline memory usage def init self e self value e the user element self count access count initially zero nonpublic utilities def ﬁnd position self e search for element e and return its position or none if not found walk self data ﬁrst while walk is not none and walk element value e walk self data after walk return walk def move up self p move item at position p earlier in the list based on access count if p self data ﬁrst consider moving cnt p element count walk self data before p if cnt walk element count must shift forward while walk self data ﬁrst and cnt self data before walk element count walk self data before walk self data add before walk self data delete p delete reinsert code fragment class favoriteslist continues in code fragment chapter linked lists public methods def init self create an empty list of favorites self data positionallist will be list of item instances def len self return number of entries on favorites list return len self data def is empty self return true if list is empty return len self data def access self e access element e thereby increasing its access count p self ﬁnd position e try to locate existing element if p is none p self data add last self item e if new place at end p element count always increment count self move up p consider moving forward def remove self e remove element e from the list of favorites p self ﬁnd position e try to locate existing element if p is not none self data delete p delete if found def top self k generate sequence of top k elements in terms of access count if not k len self raise valueerror illegal value for k walk self data ﬁrst for j in range k item walk element element of list is item yield item value report user element walk self data after walk code fragment class favoriteslist continued from code fragment case study maintaining access frequencies using a list with the move to front heuristic the previous implementation of a favorites list performs the access e method in time proportional to the index of e in the favorites list that is if e is the kth most popular element in the favorites list then accessing it takes o k time in many real life access sequences e g web pages visited by a user once an element is accessed it is more likely to be accessed again in the near future such scenarios are said to possess locality of reference a heuristic or rule of thumb that attempts to take advantage of the locality of reference that is present in an access sequence is the move to front heuristic to apply this heuristic each time we access an element we move it all the way to the front of the list our hope of course is that this element will be accessed again in the near future consider for example a scenario in which we have n elements and the following series of accesses element is accessed n times element is accessed n times element n is accessed n times if we store the elements sorted by their access counts inserting each element the ﬁrst time it is accessed then each access to element runs in o time each access to element runs in o time each access to element n runs in o n time thus the total time for performing the series of accesses is proportional to n n n n n n which is o n n on the other hand if we use the move to front heuristic inserting each element the ﬁrst time it is accessed then each subsequent access to element takes o time each subsequent access to element takes o time each subsequent access to element n runs in o time so the running time for performing all the accesses in this case is o thus the move to front implementation has faster access times for this scenario still the move to front approach is just a heuristic for there are access sequences where using the move to front approach is slower than simply keeping the favorites list ordered by access counts chapter linked lists the trade oﬀs with the move to front heuristic if we no longer maintain the elements of the favorites list ordered by their access counts when we are asked to ﬁnd the k most accessed elements we need to search for them we will implement the top k method as follows we copy all entries of our favorites list into another list named temp we scan the temp list k times in each scan we ﬁnd the entry with the largest access count remove this entry from temp and report it in the results this implementation of method top takes o kn time thus when k is a constant method top runs in o n time this occurs for example when we want to get the top ten list however if k is proportional to n then top runs in o time this occurs for example when we want a top list in chapter we will introduce a data structure that will allow us to implement top in o n k log n time see exercise p and more advanced techniques could be used to perform top in o n k log k time we could easily achieve o n log n time if we use a standard sorting algorithm to reorder the temporary list before reporting the top k see chapter this ap proach would be preferred to the original in the case that k is log n recall the big omega notation introduced in section to give an asymptotic lower bound on the running time of an algorithm there is a more specialized sorting algorithm see section that can take advantage of the fact that access counts are integers in order to achieve o n time for top for any value of k implementing the move to front heuristic in python we give an implementation of a favorites list using the move to front heuristic in code fragment the new favoriteslistmtf class inherits most of its func tionality from the original favoriteslist as a base class by our original design the access method of the original class relies on a non public utility named move up to enact the potential shifting of an element forward in the list after its access count had been incremented therefore we implement the move to front heuristic by simply overriding the move up method so that each accessed element is moved directly to the front of the list if not already there this action is easily implemented by means of the positional list adt the more complex portion of our favoriteslistmtf class is the new deﬁnition for the top method we rely on the ﬁrst of the approaches outlined above inserting copies of the items into a temporary list and then repeatedly ﬁnding reporting and removing an element that has the largest access count of those remaining case study maintaining access frequencies class favoriteslistmtf favoriteslist list of elements ordered with move to front heuristic we override move up to provide move to front semantics def move up self p move accessed item at position p to front of list if p self data ﬁrst self data add ﬁrst self data delete p delete reinsert we override top because list is no longer sorted def top self k generate sequence of top k elements in terms of access count if not k len self raise valueerror illegal value for k we begin by making a copy of the original list temp positionallist for item in self data positional lists support iteration temp add last item we repeatedly ﬁnd report and remove element with largest count for j in range k ﬁnd and report next highest from temp highpos temp ﬁrst walk temp after highpos while walk is not none if walk element count highpos element count highpos walk walk temp after walk we have found the element with highest count yield highpos element value report element to user temp delete highpos remove from temp list code fragment class favoriteslistmtf implementing the move to front heuristic this class extends favoriteslist code fragments and and overrides methods move up and top chapter linked lists link based vs array based sequences we close this chapter by reﬂecting on the relative pros and cons of array based and link based data structures that have been introduced thus far the dichotomy between these approaches presents a common design decision when choosing an appropriate implementation of a data structure there is not a one size ﬁts all so lution as each offers distinct advantages and disadvantages advantages of array based sequences arrays provide o time access to an element based on an integer index the ability to access the kth element for any k in o time is a hallmark advantage of arrays see section in contrast locating the kth element in a linked list requires o k time to traverse the list from the beginning or possibly o n k time if traversing backward from the end of a doubly linked list operations with equivalent asymptotic bounds typically run a constant factor more efﬁciently with an array based structure versus a linked structure as an example consider the typical enqueue operation for a queue ignoring the issue of resizing an array this operation for the arrayqueue class see code fragment involves an arithmetic calculation of the new index an increment of an integer and storing a reference to the element in the array in contrast the process for a linkedqueue see code fragment requires the instantiation of a node appropriate linking of nodes and an increment of an integer while this operation completes in o time in either model the actual number of cpu operations will be more in the linked version especially given the instantiation of the new node array based representations typically use proportionally less memory than linked structures this advantage may seem counterintuitive especially given that the length of a dynamic array may be longer than the number of elements that it stores both array based lists and linked lists are referential structures so the primary memory for storing the actual objects that are elements is the same for either structure what differs is the auxiliary amounts of memory that are used by the two structures for an array based container of n ele ments a typical worst case may be that a recently resized dynamic array has allocated memory for object references with linked lists memory must be devoted not only to store a reference to each contained object but also explicit references that link the nodes so a singly linked list of length n already requires references an element reference and next reference for each node with a doubly linked list there are references link based vs array based sequences advantages of link based sequences link based structures provide worst case time bounds for their operations this is in contrast to the amortized bounds associated with the expansion or contraction of a dynamic array see section when many individual operations are part of a larger computation and we only care about the total time of that computation an amortized bound is as good as a worst case bound precisely because it gives a guarantee on the sum of the time spent on the individual operations however if data structure operations are used in a real time system that is de signed to provide more immediate responses e g an operating system web server air trafﬁc control system a long delay caused by a single amortized operation may have an adverse effect link based structures support o time insertions and deletions at arbi trary positions the ability to perform a constant time insertion or deletion with the positionallist class by using a position to efﬁciently describe the location of the operation is perhaps the most signiﬁcant advantage of the linked list this is in stark contrast to an array based sequence ignoring the issue of resizing an array inserting or deleting an element from the end of an array based list can be done in constant time however more general insertions and deletions are expensive for example with python array based list class a call to insert or pop with index k uses o n k time because of the loop to shift all subsequent elements see section as an example application consider a text editor that maintains a document as a sequence of characters although users often add characters to the end of the document it is also possible to use the cursor to insert or delete one or more characters at an arbitrary position within the document if the charac ter sequence were stored in an array based sequence such as a python list each such edit operation may require linearly many characters to be shifted leading to o n performance for each edit operation with a linked list rep resentation an arbitrary edit operation insertion or deletion of a character at the cursor can be performed in o worst case time assuming we are given a position that represents the location of the cursor chapter linked lists exercises for help with exercises please visit the site www wiley com college goodrich reinforcement r give an algorithm for ﬁnding the second to last node in a singly linked list in which the last node is indicated by a next reference of none r describe a good algorithm for concatenating two singly linked lists l and m given only references to the ﬁrst node of each list into a single list ll that contains all the nodes of l followed by all the nodes of m r describe a recursive algorithm that counts the number of nodes in a singly linked list r describe in detail how to swap two nodes x and y and not just their con tents in a singly linked list l given references only to x and y repeat this exercise for the case when l is a doubly linked list which algorithm takes more time r implement a function that counts the number of nodes in a circularly linked list r suppose that x and y are references to nodes of circularly linked lists although not necessarily the same list describe a fast algorithm for telling if x and y belong to the same list r our circularqueue class of section provides a rotate method that has semantics equivalent to q enqueue q dequeue for a nonempty queue implement such a method for the linkedqueue class of sec tion without the creation of any new nodes r describe a nonrecursive method for ﬁnding by link hopping the middle node of a doubly linked list with header and trailer sentinels in the case of an even number of nodes report the node slightly left of center as the middle note this method must only use link hopping it cannot use a counter what is the running time of this method r give a fast algorithm for concatenating two doubly linked lists l and m with header and trailer sentinel nodes into a single list ll r there seems to be some redundancy in the repertoire of the positional list adt as the operation l add ﬁrst e could be enacted by the alter native l add before l ﬁrst e likewise l add last e might be per formed as l add after l last e explain why the methods add ﬁrst and add last are necessary exercises r implement a function with calling syntax max l that returns the max imum element from a positionallist instance l containing comparable elements r redo the previously problem with max as a method of the positionallist class so that calling syntax l max is supported r update the positionallist class to support an additional method ﬁnd e which returns the position of the ﬁrst occurrence of element e in the list or none if not found r repeat the previous process using recursion your method should not contain any loops how much space does your method use in addition to the space used for l r provide support for a reversed method of the positionallist class that is similar to the given iter but that iterates the elements in reversed order r describe an implementation of the positionallist methods add last and add before realized by using only methods in the set is empty ﬁrst last prev next add after and add ﬁrst r in the favoriteslistmtf class we rely on public methods of the positional list adt to move an element of a list at position p to become the ﬁrst ele ment of the list while keeping the relative order of the remaining elements unchanged internally that combination of operations causes one node to be removed and a new node to be inserted augment the positionallist class to support a new method move to front p that accomplishes this goal more directly by relinking the existing node r given the set of element a b c d e f stored in a list show the ﬁnal state of the list assuming we use the move to front heuristic and access the el ements according to the following sequence a b c d e f a c f b d e r suppose that we have made kn total accesses to the elements in a list l of n elements for some integer k what are the minimum and maximum number of elements that have been accessed fewer than k times r let l be a list of n items maintained according to the move to front heuris tic describe a series of o n accesses that will reverse l r suppose we have an n element list l maintained according to the move to front heuristic describe a sequence of accesses that is guaranteed to take time to perform on l r implement a clear method for the favoriteslist class that returns the list to empty r implement a reset counts method for the favoriteslist class that resets all elements access counts to zero while leaving the order of the list unchanged chapter linked lists creativity c give a complete implementation of the stack adt using a singly linked list that includes a header sentinel c give a complete implementation of the queue adt using a singly linked list that includes a header sentinel c implement a method concatenate for the linkedqueue class that takes all elements of linkedqueue and appends them to the end of the original queue the operation should run in o time and should result in being an empty queue c give a recursive implementation of a singly linked list class such that an instance of a nonempty list stores its ﬁrst element and a reference to a list of remaining elements c describe a fast recursive algorithm for reversing a singly linked list c describe in detail an algorithm for reversing a singly linked list l using only a constant amount of additional space and not using any recursion c exercise p describes a leakystack abstraction implement that adt using a singly linked list for storage c design a forward list adt that abstracts the operations on a singly linked list much as the positional list adt abstracts the use of a doubly linked list implement a forwardlist class that supports such an adt c design a circular positional list adt that abstracts a circularly linked list in the same way that the positional list adt abstracts a doubly linked list with a notion of a designated cursor position within the list c modify the doublylinkedbase class to include a reverse method that re verses the order of the list yet without creating or destroying any nodes c modify the positionallist class to support a method swap p q that causes the underlying nodes referenced by positions p and q to be exchanged for each other relink the existing nodes do not create any new nodes c to implement the iter method of the positionallist class we relied on the convenience of python generator syntax and the yield statement give an alternative implementation of iter by designing a nested iterator class see section for discussion of iterators c give a complete implementation of the positional list adt using a doubly linked list that does not include any sentinel nodes c implement a function that accepts a positionallist l of n integers sorted in nondecreasing order and another value v and determines in o n time if there are two elements of l that sum precisely to v the function should return a pair of positions of such elements if found or none otherwise exercises c there is a simple but inefﬁcient algorithm called bubble sort for sorting a list l of n comparable elements this algorithm scans the list n times where in each scan the algorithm compares the current element with the next one and swaps them if they are out of order implement a bubble sort function that takes a positional list l as a parameter what is the running time of this algorithm assuming the positional list is implemented with a doubly linked list c to better model a fifo queue in which entries may be deleted before reaching the front design a positionalqueue class that supports the com plete queue adt yet with enqueue returning a position instance and sup port for a new method delete p that removes the element associated with position p from the queue you may use the adapter design pattern section using a positionallist as your storage c describe an efﬁcient method for maintaining a favorites list l with move to front heuristic such that elements that have not been accessed in the most recent n accesses are automatically purged from the list c exercise c introduces the notion of a natural join of two databases describe and analyze an efﬁcient algorithm for computing the natural join of a linked list a of n pairs and a linked list b of m pairs c write a scoreboard class that maintains the top scores for a game ap plication using a singly linked list rather than the array that was used in section c describe a method for performing a card shufﬂe of a list of elements by converting it into two lists a card shufﬂe is a permutation where a list l is cut into two lists and where is the ﬁrst half of l and is the second half of l and then these two lists are merged into one by taking the ﬁrst element in then the ﬁrst element in followed by the second element in the second element in and so on projects p write a simple text editor that stores and displays a string of characters using the positional list adt together with a cursor object that highlights a position in this string a simple interface is to print the string and then to use a second line of output to underline the position of the cursor your editor should support the following operations left move cursor left one character do nothing if at beginning right move cursor right one character do nothing if at end insert c insert the character c just after the cursor delete delete the character just after the cursor do nothing at end chapter linked lists p an array a is sparse if most of its entries are empty i e none a list l can be used to implement such an array efﬁciently in particular for each nonempty cell a i we can store an entry i e in l where e is the element stored at a i this approach allows us to represent a using o m storage where m is the number of nonempty entries in a provide such a sparsearray class that minimally supports methods getitem j and setitem j e to provide standard indexing operations analyze the efﬁciency of these methods p although we have used a doubly linked list to implement the positional list adt it is possible to support the adt with an array based imple mentation the key is to use the composition pattern and store a sequence of position items where each item stores an element as well as that ele ment current index in the array whenever an element place in the array is changed the recorded index in the position must be updated to match given a complete class providing such an array based implementation of the positional list adt what is the efﬁciency of the various operations p implement a cardhand class that supports a person arranging a group of cards in his or her hand the simulator should represent the sequence of cards using a single positional list adt so that cards of the same suit are kept together implement this strategy by means of four ﬁngers into the hand one for each of the suits of hearts clubs spades and diamonds so that adding a new card to the person hand or playing a correct card from the hand can be done in constant time the class should support the following methods add card r add a new card with rank r and suit to the hand play remove and return a card of suit from the player hand if there is no card of suit then remove and return an arbitrary card from the hand iter iterate through all cards currently in the hand all of suit iterate through all cards of suit that are currently in the hand chapter notes a view of data structures as collections and other principles of object oriented design can be found in object oriented design books by booch budd goldberg and robson and liskov and guttag our positional list adt is derived from the position abstraction introduced by aho hopcroft and ullman and the list adt of wood implementations of linked lists are discussed by knuth chapter trees contents general trees binary trees the binary tree abstract data type properties of binary trees implementing trees linked structure for binary trees array based representation of a binary tree linked structure for general trees tree traversal algorithms preorder and postorder traversals of general trees breadth first tree traversal inorder traversal of a binary tree implementing tree traversals in python applications of tree traversals euler tours and the template method pattern case study an expression tree exercises chapter trees general trees productivity experts say that breakthroughs come by thinking nonlinearly in this chapter we discuss one of the most important nonlinear data structures in computing trees tree structures are indeed a breakthrough in data organization for they allow us to implement a host of algorithms much faster than when using linear data structures such as array based lists or linked lists trees also provide a natural organization for data and consequently have become ubiquitous structures in ﬁle systems graphical user interfaces databases web sites and other computer systems it is not always clear what productivity experts mean by nonlinear thinking but when we say that trees are nonlinear we are referring to an organizational relationship that is richer than the simple before and after relationships be tween objects in sequences the relationships in a tree are hierarchical with some objects being above and some below others actually the main terminology for tree data structures comes from family trees with the terms parent child ancestor and descendant being the most common words used to describe rela tionships we show an example of a family tree in figure figure a family tree showing some descendants of abraham as recorded in genesis chapters general trees tree deﬁnitions and properties a tree is an abstract data type that stores elements hierarchically with the excep tion of the top element each element in a tree has a parent element and zero or more children elements a tree is usually visualized by placing elements inside ovals or rectangles and by drawing the connections between parents and children with straight lines see figure we typically call the top element the root of the tree but it is drawn as the highest element with the other elements being connected below just the opposite of a botanical tree figure a tree with nodes representing the organization of a ﬁctitious cor poration the root stores electronics r us the children of the root store r d sales purchasing and manufacturing the internal nodes store sales interna tional overseas electronics r us and manufacturing formal tree deﬁnition formally we deﬁne a tree t as a set of nodes storing elements such that the nodes have a parent child relationship that satisﬁes the following properties if t is nonempty it has a special node called the root of t that has no parent each node v of t different from the root has a unique parent node w every node with parent w is a child of w note that according to our deﬁnition a tree can be empty meaning that it does not have any nodes this convention also allows us to deﬁne a tree recursively such that a tree t is either empty or consists of a node r called the root of t and a possibly empty set of subtrees whose roots are the children of r chapter trees other node relationships two nodes that are children of the same parent are siblings a node v is external if v has no children a node v is internal if it has one or more children external nodes are also known as leaves example in section we discussed the hierarchical relationship be tween ﬁles and directories in a computer ﬁle system although at the time we did not emphasize the nomenclature of a ﬁle system as a tree in figure we revisit an earlier example we see that the internal nodes of the tree are associ ated with directories and the leaves are associated with regular ﬁles in the unix and linux operating systems the root of the tree is appropriately called the root directory and is represented by the symbol figure tree representing a portion of a ﬁle system a node u is an ancestor of a node v if u v or u is an ancestor of the parent of v conversely we say that a node v is a descendant of a node u if u is an ancestor of v for example in figure is an ancestor of papers and is a descendant of the subtree of t rooted at a node v is the tree consisting of all the descendants of v in t including v itself in figure the subtree rooted at consists of the nodes grades homeworks programs pr1 and edges and paths in trees an edge of tree t is a pair of nodes u v such that u is the parent of v or vice versa a path of t is a sequence of nodes such that any two consecutive nodes in the sequence form an edge for example the tree in figure contains the path projects demos market general trees example the inheritance relation between classes in a python program forms a tree when single inheritance is used for example in section we provided a summary of the hierarchy for python exception types as portrayed in figure originally figure the baseexception class is the root of that hierarchy while all user deﬁned exception classes should conventionally be declared as descendants of the more speciﬁc exception class see for example the empty class we intro duced in code fragment of chapter figure a portion of python hierarchy of exception types in python all classes are organized into a single hierarchy as there exists a built in class named object as the ultimate base class it is a direct or indirect base class of all other types in python even if not declared as such when deﬁning a new class therefore the hierarchy pictured in figure is only a portion of python complete class hierarchy as a preview of the remainder of this chapter figure portrays our own hierarchy of classes for representing various forms of a tree figure our own inheritance hierarchy for modeling various abstractions and implementations of tree data structures in the remainder of this chapter we provide implementations of tree binarytree and linkedbinarytree classes and high level sketches for how linkedtree and arraybinarytree might be designed chapter trees ordered trees a tree is ordered if there is a meaningful linear order among the children of each node that is we purposefully identify the children of a node as being the ﬁrst second third and so on such an order is usually visualized by arranging siblings left to right according to their order example the components of a structured document such as a book are hier archically organized as a tree whose internal nodes are parts chapters and sections and whose leaves are paragraphs tables ﬁgures and so on see figure the root of the tree corresponds to the book itself we could in fact consider expanding the tree further to show paragraphs consisting of sentences sentences consisting of words and words consisting of characters such a tree is an example of an ordered tree because there is a well deﬁned order among the children of each node figure an ordered tree associated with a book let look back at the other examples of trees that we have described thus far and consider whether the order of children is signiﬁcant a family tree that de scribes generational relationships as in figure is often modeled as an ordered tree with siblings ordered according to their birth in contrast an organizational chart for a company as in figure is typically considered an unordered tree likewise when using a tree to describe an inher itance hierarchy as in figure there is no particular signiﬁcance to the order among the subclasses of a parent class finally we consider the use of a tree in modeling a computer ﬁle system as in figure although an operating system often displays entries of a directory in a particular order e g alphabetical chrono logical such an order is not typically inherent to the ﬁle system representation general trees the tree abstract data type as we did with positional lists in section we deﬁne a tree adt using the concept of a position as an abstraction for a node of a tree an element is stored at each position and positions satisfy parent child relationships that deﬁne the tree structure a position object for a tree supports the method p element return the element stored at position p the tree adt then supports the following accessor methods allowing a user to navigate the various positions of a tree t root return the position of the root of tree t or none if t is empty t is root p return true if position p is the root of tree t t parent p return the position of the parent of position p or none if p is the root of t t num children p return the number of children of position p t children p generate an iteration of the children of position p t is leaf p return true if position p does not have any children len t return the number of positions and hence elements that are contained in tree t t is empty return true if tree t does not contain any positions t positions generate an iteration of all positions of tree t iter t generate an iteration of all elements stored within tree t any of the above methods that accepts a position as an argument should generate a valueerror if that position is invalid for t if a tree t is ordered then t children p reports the children of p in the natural order if p is a leaf then t children p generates an empty iteration in similar regard if tree t is empty then both t positions and iter t generate empty iter ations we will discuss general means for iterating through all positions of a tree in sections we do not deﬁne any methods for creating or modifying trees at this point we prefer to describe different tree update methods in conjunction with speciﬁc implementations of the tree interface and speciﬁc applications of trees chapter trees a tree abstract base class in python in discussing the object oriented design principle of abstraction in section we noted that a public interface for an abstract data type is often managed in python via duck typing for example we deﬁned the notion of the public interface for a queue adt in section and have since presented several classes that implement the queue interface e g arrayqueue in section linkedqueue in section circularqueue in section however we never gave any formal deﬁnition of the queue adt in python all of the concrete implementations were self contained classes that just happen to adhere to the same public interface a more formal mechanism to designate the relationships between different implementations of the same abstraction is through the deﬁnition of one class that serves as an abstract base class via inheritance for one or more concrete classes see section we choose to deﬁne a tree class in code fragment that serves as an ab stract base class corresponding to the tree adt our reason for doing so is that there is quite a bit of useful code that we can provide even at this level of abstraction al lowing greater code reuse in the concrete tree implementations we later deﬁne the tree class provides a deﬁnition of a nested position class which is also abstract and declarations of many of the accessor methods included in the tree adt however our tree class does not deﬁne any internal representation for stor ing a tree and ﬁve of the methods given in that code fragment remain abstract root parent num children children and len each of these methods raises a notimplementederror a more formal approach for deﬁning abstract base classes and abstract methods using python abc module is described in section the subclasses are responsible for overriding abstract methods such as children to provide a working implementation for each behavior based on their chosen internal representation although the tree class is an abstract base class it includes several concrete methods with implementations that rely on calls to the abstract methods of the class in deﬁning the tree adt in the previous section we declare ten accessor methods five of those are the ones we left as abstract in code fragment the other ﬁve can be implemented based on the former code fragment provides concrete implementations for methods is root is leaf and is empty in section we will explore general algorithms for traversing a tree that can be used to provide concrete implementations of the positions and iter methods within the tree class the beauty of this design is that the concrete methods deﬁned within the tree abstract base class will be inherited by all subclasses this promotes greater code reuse as there will be no need for those subclasses to reimplement such behaviors we note that with the tree class being abstract there is no reason to create a direct instance of it nor would such an instance be useful the class exists to serve as a base for inheritance and users will create instances of concrete subclasses general trees class tree abstract base class representing a tree structure nested position class class position an abstraction representing the location of a single element def element self return the element stored at this position raise notimplementederror must be implemented by subclass def eq self other return true if other position represents the same location raise notimplementederror must be implemented by subclass def ne self other return true if other does not represent the same location return not self other opposite of eq abstract methods that concrete subclass must support def root self return position representing the tree root or none if empty raise notimplementederror must be implemented by subclass def parent self p return position representing p parent or none if p is root raise notimplementederror must be implemented by subclass def num children self p return the number of children that position p has raise notimplementederror must be implemented by subclass def children self p generate an iteration of positions representing p children raise notimplementederror must be implemented by subclass def len self return the total number of elements in the tree raise notimplementederror must be implemented by subclass code fragment a portion of our tree abstract base class continued in code fragment chapter trees concrete methods implemented in this class def is root self p return true if position p represents the root of the tree return self root p def is leaf self p return true if position p does not have any children return self num children p def is empty self return true if the tree is empty return len self code fragment some concrete methods of our tree abstract base class computing depth and height let p be the position of a node of a tree t the depth of p is the number of ancestors of p excluding p itself for example in the tree of figure the node storing international has depth note that this deﬁnition implies that the depth of the root of t is the depth of p can also be recursively deﬁned as follows if p is the root then the depth of p is otherwise the depth of p is one plus the depth of the parent of p based on this deﬁnition we present a simple recursive algorithm depth in code fragment for computing the depth of a position p in tree t this method calls itself recursively on the parent of p and adds to the value returned def depth self p return the number of levels separating position p from the root if self is root p return else return self depth self parent p code fragment method depth of the tree class the running time of t depth p for position p is o dp where dp denotes the depth of p in the tree t because the algorithm performs a constant time recur sive step for each ancestor of p thus algorithm t depth p runs in o n worst case time where n is the total number of positions of t because a position of t may have depth n if all nodes form a single branch although such a running time is a function of the input size it is more informative to characterize the running time in terms of the parameter dp as this parameter may be much smaller than n general trees height the height of a position p in a tree t is also deﬁned recursively if p is a leaf then the height of p is otherwise the height of p is one more than the maximum of the heights of p children the height of a nonempty tree t is the height of the root of t for example the tree of figure has height in addition height can also be viewed as follows proposition the height of a nonempty tree t is equal to the maximum of the depths of its leaf positions we leave the justiﬁcation of this fact to an exercise r we present an algorithm implemented in code fragment as a nonpublic method of the tree class it computes the height of a nonempty tree t based on proposition and the algorithm depth from code fragment def self works but o worst case time return the height of the tree return max self depth p for p in self positions if self is leaf p code fragment method of the tree class note that this method calls the depth method unfortunately algorithm is not very efﬁcient we have not yet deﬁned the positions method we will see that it can be implemented to run in o n time where n is the number of positions of t because calls algorithm depth p on each leaf of t its running time is o n p l dp where l is the set of leaf positions of t in the worst case the sum p l dp is proportional to see exercise c thus algorithm runs in o worst case time we can compute the height of a tree more efﬁciently in o n worst case time by relying instead on the original recursive deﬁnition to do this we will param eterize a function based on a position within the tree and calculate the height of the subtree rooted at that position algorithm shown as nonpublic method in code fragment computes the height of tree t in this way def self p time is linear in size of subtree return the height of the subtree rooted at position p if self is leaf p return else return max self c for c in self children p code fragment method for computing the height of a subtree rooted at a position p of a tree chapter trees it is important to understand why algorithm is more efﬁcient than the algorithm is recursive and it progresses in a top down fashion if the method is initially called on the root of t it will eventually be called once for each position of t this is because the root eventually invokes the recursion on each of its children which in turn invokes the recursion on each of their children and so on we can determine the running time of the algorithm by summing over all the positions the amount of time spent on the nonrecursive part of each call review section for analyses of recursive processes in our implementation there is a constant amount of work per position plus the overhead of computing the maximum over the iteration of children although we do not yet have a concrete implementation of children p we assume that such an iteration is generated in o cp time where cp denotes the number of children of p algorithm spends o cp time at each position p to compute the maximum and its overall running time is o p cp o n p cp in order to complete the analysis we make use of the following property proposition let t be a tree with n positions and let cp denote the number of children of a position p of t then summing over the positions of t p cp n justiﬁcation each position of t with the exception of the root is a child of another position and thus contributes one unit to the above sum by proposition the running time of algorithm when called on the root of t is o n where n is the number of positions of t revisiting the public interface for our tree class the ability to compute heights of subtrees is beneﬁcial but a user might expect to be able to compute the height of the entire tree without explicitly designating the tree root we can wrap the non public in our implementation with a public height method that provides a default interpretation when invoked on tree t with syntax t height such an implementation is given in code fragment def height self p none return the height of the subtree rooted at position p if p is none return the height of the entire tree if p is none p self root return self p start recursion code fragment public method tree height that computes the height of the entire tree by default or a subtree rooted at given position if speciﬁed binary trees binary trees a binary tree is an ordered tree with the following properties every node has at most two children each child node is labeled as being either a left child or a right child a left child precedes a right child in the order of children of a node the subtree rooted at a left or right child of an internal node v is called a left subtree or right subtree respectively of v a binary tree is proper if each node has either zero or two children some people also refer to such trees as being full binary trees thus in a proper binary tree every internal node has exactly two children a binary tree that is not proper is improper example an important class of binary trees arises in contexts where we wish to represent a number of different outcomes that can result from answering a series of yes or no questions each internal node is associated with a question starting at the root we go to the left or right child of the current node depending on whether the answer to the question is yes or no with each decision we follow an edge from a parent to a child eventually tracing a path in the tree from the root to a leaf such binary trees are known as decision trees because a leaf position p in such a tree represents a decision of what to do if the questions associated with p ancestors are answered in a way that leads to p a decision tree is a proper binary tree figure illustrates a decision tree that provides recommendations to a prospective investor figure a decision tree providing investment advice chapter trees example an arithmetic expression can be represented by a binary tree whose leaves are associated with variables or constants and whose internal nodes are associated with one of the operators and see figure each node in such a tree has a value associated with it if a node is leaf then its value is that of its variable or constant if a node is internal then its value is deﬁned by applying its operation to the values of its children an arithmetic expression tree is a proper binary tree since each operator and takes exactly two operands of course if we were to allow unary operators like negation as in x then we could have an improper binary tree figure a binary tree representing an arithmetic expression this tree represents the expression the value associated with the internal node labeled is a recursive binary tree deﬁnition incidentally we can also deﬁne a binary tree in a recursive way such that a binary tree is either empty or consists of a node r called the root of t that stores an element a binary tree possibly empty called the left subtree of t a binary tree possibly empty called the right subtree of t binary trees the binary tree abstract data type as an abstract data type a binary tree is a specialization of a tree that supports three additional accessor methods t left p return the position that represents the left child of p or none if p has no left child t right p return the position that represents the right child of p or none if p has no right child t sibling p return the position that represents the sibling of p or none if p has no sibling just as in section for the tree adt we do not deﬁne specialized update meth ods for binary trees here instead we will consider some possible update methods when we describe speciﬁc implementations and applications of binary trees the binarytree abstract base class in python just as tree was deﬁned as an abstract base class in section we deﬁne a new binarytree class associated with the binary tree adt we rely on inheritance to deﬁne the binarytree class based upon the existing tree class however our binarytree class remains abstract as we still do not provide complete speciﬁca tions for how such a structure will be represented internally nor implementations for some necessary behaviors our python implementation of the binarytree class is given in code frag ment by using inheritance a binary tree supports all the functionality that was deﬁned for general trees e g parent is leaf root our new class also inherits the nested position class that was originally deﬁned within the tree class deﬁnition in addition the new class provides declarations for new abstract methods left and right that should be supported by concrete subclasses of binarytree our new class also provides two concrete implementations of methods the new sibling method is derived from the combination of left right and parent typ ically we identify the sibling of a position p as the other child of p parent however if p is the root it has no parent and thus no sibling also p may be the only child of its parent and thus does not have a sibling finally code fragment provides a concrete implementation of the children method this method is abstract in the tree class although we have still not speci ﬁed how the children of a node will be stored we derive a generator for the ordered children based upon the implied behavior of abstract methods left and right chapter trees class binarytree tree abstract base class representing a binary tree structure additional abstract methods def left self p return a position representing p left child return none if p does not have a left child raise notimplementederror must be implemented by subclass def right self p return a position representing p right child return none if p does not have a right child raise notimplementederror must be implemented by subclass concrete methods implemented in this class def sibling self p return a position representing p sibling or none if no sibling parent self parent p if parent is none p must be the root return none root has no sibling else if p self left parent return self right parent possibly none else return self left parent possibly none def children self p generate an iteration of positions representing p children if self left p is not none yield self left p if self right p is not none yield self right p code fragment a binarytree abstract base class that extends the existing tree abstract base class from code fragments and binary trees properties of binary trees binary trees have several interesting properties dealing with relationships between their heights and number of nodes we denote the set of all nodes of a tree t at the same depth d as level d of t in a binary tree level has at most one node the root level has at most two nodes the children of the root level has at most four nodes and so on see figure in general level d has at most nodes level nodes figure maximum number of nodes in the levels of a binary tree we can see that the maximum number of nodes on the levels of a binary tree grows exponentially as we go down the tree from this simple observation we can derive the following properties relating the height of a binary tree t with its number of nodes a detailed justiﬁcation of these properties is left as exercise r proposition let t be a nonempty binary tree and let n ne ni and h denote the number of nodes number of external nodes number of internal nodes and height of t respectively then t has the following properties h n ne h ni log n h n also if t is proper then t has the following properties n h ne h ni log n h n chapter trees relating internal nodes to external nodes in a proper binary tree in addition to the earlier binary tree properties the following relationship exists between the number of internal nodes and external nodes in a proper binary tree proposition in a nonempty proper binary tree t with ne external nodes and ni internal nodes we have ne ni justiﬁcation we justify this proposition by removing nodes from t and divid ing them up into two piles an internal node pile and an external node pile until t becomes empty the piles are initially empty by the end we will show that the external node pile has one more node than the internal node pile we consider two cases case if t has only one node v we remove v and place it on the external node pile thus the external node pile has one node and the internal node pile is empty case otherwise t has more than one node we remove from t an arbitrary external node w and its parent v which is an internal node we place w on the external node pile and v on the internal node pile if v has a parent u then we reconnect u with the former sibling z of w as shown in figure this operation removes one internal node and one external node and leaves the tree being a proper binary tree repeating this operation we eventually are left with a ﬁnal tree consisting of a single node note that the same number of external and internal nodes have been removed and placed on their respective piles by the sequence of operations leading to this ﬁnal tree now we remove the node of the ﬁnal tree and we place it on the external node pile thus the the external node pile has one more node than the internal node pile u u u a b c figure operation that removes an external node and its parent node used in the justiﬁcation of proposition note that the above relationship does not hold in general for improper binary trees and nonbinary trees although there are other interesting relationships that do hold see exercises c through c implementing trees implementing trees the tree and binarytree classes that we have deﬁned thus far in this chapter are both formally abstract base classes although they provide a great deal of support neither of them can be directly instantiated we have not yet deﬁned key imple mentation details for how a tree will be represented internally and how we can effectively navigate between parents and children speciﬁcally a concrete imple mentation of a tree must provide methods root parent num children children len and in the case of binarytree the additional accessors left and right there are several choices for the internal representation of trees we describe the most common representations in this section we begin with the case of a binary tree since its shape is more narrowly deﬁned linked structure for binary trees a natural way to realize a binary tree t is to use a linked structure with a node see figure that maintains references to the element stored at a position p and to the nodes associated with the children and parent of p if p is the root of t then the parent ﬁeld of p is none likewise if p does not have a left child respectively right child the associated ﬁeld is none the tree itself maintains an instance variable storing a reference to the root node if any and a variable called size that represents the overall number of nodes of t we show such a linked structure representation of a binary tree in figure parent left element right a b figure a linked structure for representing a a single node b a binary tree chapter trees python implementation of a linked binary tree structure in this section we deﬁne a concrete linkedbinarytree class that implements the binary tree adt by subclassing the binarytree class our general approach is very similar to what we used when developing the positionallist in section we deﬁne a simple nonpublic node class to represent a node and a public position class that wraps a node we provide a validate utility for robustly checking the validity of a given position instance when unwrapping it and a make position utility for wrapping a node as a position to return to a caller those deﬁnitions are provided in code fragment as a formality the new position class is declared to inherit immediately from binarytree position tech nically the binarytree class deﬁnition see code fragment does not formally declare such a nested class it trivially inherits it from tree position a minor ben eﬁt from this design is that our position class inherits the ne special method so that syntax p q is derived appropriately relative to eq our class deﬁnition continues in code fragment with a constructor and with concrete implementations for the methods that remain abstract in the tree and binarytree classes the constructor creates an empty tree by initializing root to none and size to zero these accessor methods are implemented with careful use of the validate and make position utilities to safeguard against boundary cases operations for updating a linked binary tree thus far we have provided functionality for examining an existing binary tree however the constructor for our linkedbinarytree class results in an empty tree and we have not provided any means for changing the structure or content of a tree we chose not to declare update methods as part of the tree or binarytree ab stract base classes for several reasons first although the principle of encapsula tion suggests that the outward behaviors of a class need not depend on the internal representation the efﬁciency of the operations depends greatly upon the representa tion we prefer to have each concrete implementation of a tree class offer the most suitable options for updating a tree the second reason we do not provide update methods in the base class is that we may not want such update methods to be part of a public interface there are many applications of trees and some forms of update operations that are suitable for one application may be unacceptable in another however if we place an update method in a base class any class that inherits from that base will inherit the update method consider for example the possibility of a method t replace p e that replaces the element stored at position p with another element e such a general method may be unacceptable in the context of an arithmetic expression tree see example on page and a later case study in section because we may want to enforce that internal nodes store only operators as elements implementing trees for linked binary trees a reasonable set of update methods to support for gen eral usage are the following t add root e create a root for an empty tree storing e as the element and return the position of that root an error occurs if the tree is not empty t add left p e create a new node storing element e link the node as the left child of position p and return the resulting position an error occurs if p already has a left child t add right p e create a new node storing element e link the node as the right child of position p and return the resulting position an error occurs if p already has a right child t replace p e replace the element stored at position p with element e and return the previously stored element t delete p remove the node at position p replacing it with its child if any and return the element that had been stored at p an error occurs if p has two children t attach p attach the internal structure of trees and respec tively as the left and right subtrees of leaf position p of t and reset and to empty trees an error condition occurs if p is not a leaf we have speciﬁcally chosen this collection of operations because each can be implemented in o worst case time with our linked representation the most complex of these are delete and attach due to the case analyses involving the various parent child relationships and boundary conditions yet there remains only a constant number of operations to perform the implementation of both methods could be greatly simpliﬁed if we used a tree representation with a sentinel node akin to our treatment of positional lists see exercise c to avoid the problem of undesirable update methods being inherited by sub classes of linkedbinarytree we have chosen an implementation in which none of the above methods are publicly supported instead we provide nonpublic ver sions of each for example providing the underscored delete in lieu of a public delete our implementations of these six update methods are provided in code fragments and in particular applications subclasses of linkedbinarytree can invoke the non public methods internally while preserving a public interface that is appropriate for the application a subclass may also choose to wrap one or more of the non public update methods with a public method to expose it to the user we leave as an exercise r the task of deﬁning a mutablelinkedbinarytree subclass that provides public methods wrapping each of these six update methods chapter trees class linkedbinarytree binarytree linked representation of a binary tree structure class node lightweight nonpublic class for storing a node slots _right def init self element parent none left none right none self element element self parent parent self left left self right right class position binarytree position an abstraction representing the location of a single element def init self container node constructor should not be invoked by user self container container self node node def element self return the element stored at this position return self node element def eq self other return true if other is a position representing the same location return type other is type self and other node is self node def validate self p return associated node if position is valid if not isinstance p self position raise typeerror p must be proper position type if p container is not self raise valueerror p does not belong to this container if p node parent is p node convention for deprecated nodes raise valueerror p is no longer valid return p node def make position self node return position instance for given node or none if no node return self position self node if node is not none else none code fragment the beginning of our linkedbinarytree class continued in code fragments through implementing trees binary tree constructor def init self create an initially empty binary tree self root none self size public accessors def len self return the total number of elements in the tree return self size def root self return the root position of the tree or none if tree is empty return self make position self root def parent self p return the position of p parent or none if p is root node self validate p return self make position node parent def left self p return the position of p left child or none if no left child node self validate p return self make position node left def right self p return the position of p right child or none if no right child node self validate p return self make position node right def num children self p return the number of children of position p node self validate p count if node left is not none left child exists count if node right is not none right child exists count return count code fragment public accessors for our linkedbinarytree class the class begins in code fragment and continues in code fragments and chapter trees def add root self e place element e at the root of an empty tree and return new position raise valueerror if tree nonempty if self root is not none raise valueerror root exists self size self root self node e return self make position self root def add left self p e create a new left child for position p storing element e return the position of new node raise valueerror if position p is invalid or p already has a left child node self validate p if node left is not none raise valueerror left child exists self size node left self node e node node is its parent return self make position node left def add right self p e create a new right child for position p storing element e return the position of new node raise valueerror if position p is invalid or p already has a right child node self validate p if node right is not none raise valueerror right child exists self size node right self node e node node is its parent return self make position node right def replace self p e replace the element at position p with e and return old element node self validate p old node element node element e return old code fragment nonpublic update methods for the linkedbinarytree class continued in code fragment implementing trees def delete self p delete the node at position p and replace it with its child if any return the element that had been stored at position p raise valueerror if position p is invalid or p has two children node self validate p if self num children p raise valueerror p has two children child node left if node left else node right might be none if child is not none child parent node parent child grandparent becomes parent if node is self root self root child child becomes root else parent node parent if node is parent left parent left child else parent right child self size node parent node convention for deprecated node return node element def attach self p attach trees and as left and right subtrees of external p node self validate p if not self is leaf p raise valueerror position must be leaf if not type self is type is type all trees must be same type raise typeerror tree types must match self size len len if not is empty attached as left subtree of node root parent node node left root root none set instance to empty size if not is empty attached as right subtree of node root parent node node right root root none set instance to empty size code fragment nonpublic update methods for the linkedbinarytree class continued from code fragment chapter trees performance of the linked binary tree implementation to summarize the efﬁciencies of the linked structure representation we analyze the running times of the linkedbinarytree methods including derived methods that are inherited from the tree and binarytree classes the len method implemented in linkedbinarytree uses an instance variable storing the number of nodes of t and takes o time method is empty inherited from tree relies on a single call to len and thus takes o time the accessor methods root left right parent and num children are imple mented directly in linkedbinarytree and take o time the sibling and children methods are derived in binarytree based on a constant number of calls to these other accessors so they run in o time as well the is root and is leaf methods from the tree class both run in o time as is root calls root and then relies on equivalence testing of positions while is leaf calls left and right and veriﬁes that none is returned by both methods depth and height were each analyzed in section the depth method at position p runs in o dp time where dp is its depth the height method on the root of the tree runs in o n time the various update methods add root add left add right replace delete and attach that is their nonpublic implementations each run in o time as they involve relinking only a constant number of nodes per operation table summarizes the performance of the linked structure implementation of a binary tree table running times for the methods of an n node binary tree implemented with a linked structure the space usage is o n implementing trees array based representation of a binary tree an alternative representation of a binary tree t is based on a way of numbering the positions of t for every position p of t let f p be the integer deﬁned as follows if p is the root of t then f p if p is the left child of position q then f p f q if p is the right child of position q then f p f q the numbering function f is known as a level numbering of the positions in a binary tree t for it numbers the positions on each level of t in increasing order from left to right see figure note well that the level numbering is based on potential positions within the tree not actual positions of a given tree so they are not necessarily consecutive for example in figure b there are no nodes with level numbering or because the node with level numbering has no children a b figure binary tree level numbering a general scheme b an example chapter trees the level numbering function f suggests a representation of a binary tree t by means of an array based structure a such as a python list with the element at position p of t stored at index f p of the array we show an example of an array based representation of a binary tree in figure figure representation of a binary tree by means of an array one advantage of an array based representation of a binary tree is that a posi tion p can be represented by the single integer f p and that position based meth ods such as root parent left and right can be implemented using simple arithmetic operations on the number f p based on our formula for the level numbering the left child of p has index f p the right child of p has index f p and the parent of p has index l f p 2j we leave the details of a complete im plementation as an exercise r the space usage of an array based representation depends greatly on the shape of the tree let n be the number of nodes of t and let fm be the maximum value of f p over all the nodes of t the array a requires length n fm since elements range from a to a fm note that a may have a number of empty cells that do not refer to existing nodes of t in fact in the worst case n the justiﬁcation of which is left as an exercise r in section we will see a class of binary trees called heaps for which n n thus in spite of the worst case space usage there are applications for which the array representation of a binary tree is space efﬁcient still for general binary trees the exponential worst case space requirement of this representation is prohibitive another drawback of an array representation is that some update operations for trees cannot be efﬁciently supported for example deleting a node and promoting its child takes o n time because it is not just the child that moves locations within the array but all descendants of that child implementing trees linked structure for general trees when representing a binary tree with a linked structure each node explicitly main tains ﬁelds left and right as references to individual children for a general tree there is no a priori limit on the number of children that a node may have a natural way to realize a general tree t as a linked structure is to have each node store a single container of references to its children for example a children ﬁeld of a node can be a python list of references to the children of the node if any such a linked representation is schematically illustrated in figure parent element children a b figure the linked structure for a general tree a the structure of a node b a larger portion of the data structure associated with a node and its children table summarizes the performance of the implementation of a general tree using a linked structure the analysis is left as an exercise r but we note that by using a collection to store the children of each position p we can implement children p by simply iterating that collection table running times of the accessor methods of an n node general tree im plemented with a linked structure we let cp denote the number of children of a position p the space usage is o n chapter trees tree traversal algorithms a traversal of a tree t is a systematic way of accessing or visiting all the posi tions of t the speciﬁc action associated with the visit of a position p depends on the application of this traversal and could involve anything from increment ing a counter to performing some complex computation for p in this section we describe several common traversal schemes for trees implement them in the con text of our various tree classes and discuss several common applications of tree traversals preorder and postorder traversals of general trees in a preorder traversal of a tree t the root of t is visited ﬁrst and then the sub trees rooted at its children are traversed recursively if the tree is ordered then the subtrees are traversed according to the order of the children the pseudo code for the preorder traversal of the subtree rooted at a position p is shown in code fragment algorithm preorder t p perform the visit action for position p for each child c in t children p do preorder t c recursively traverse the subtree rooted at c code fragment algorithm preorder for performing the preorder traversal of a subtree rooted at position p of a tree t figure portrays the order in which positions of a sample tree are visited during an application of the preorder traversal algorithm figure preorder traversal of an ordered tree where the children of each posi tion are ordered from left to right tree traversal algorithms postorder traversal another important tree traversal algorithm is the postorder traversal in some sense this algorithm can be viewed as the opposite of the preorder traversal be cause it recursively traverses the subtrees rooted at the children of the root ﬁrst and then visits the root hence the name postorder pseudo code for the postorder traversal is given in code fragment and an example of a postorder traversal is portrayed in figure algorithm postorder t p for each child c in t children p do postorder t c recursively traverse the subtree rooted at c perform the visit action for position p code fragment algorithm postorder for performing the postorder traversal of a subtree rooted at position p of a tree t figure postorder traversal of the ordered tree of figure running time analysis both preorder and postorder traversal algorithms are efﬁcient ways to access all the positions of a tree the analysis of either of these traversal algorithms is similar to that of algorithm given in code fragment of section at each position p the nonrecursive part of the traversal algorithm requires time o cp where cp is the number of children of p under the assumption that the visit itself takes o time by proposition the overall running time for the traversal of tree t is o n where n is the number of positions in the tree this running time is asymptotically optimal since the traversal must visit all the n positions of the tree chapter trees breadth first tree traversal although the preorder and postorder traversals are common ways of visiting the positions of a tree another common approach is to traverse a tree so that we visit all the positions at depth d before we visit the positions at depth d such an algorithm is known as a breadth ﬁrst traversal a breadth ﬁrst traversal is a common approach used in software for playing games a game tree represents the possible choices of moves that might be made by a player or computer during a game with the root of the tree being the initial conﬁguration for the game for example figure displays a partial game tree for tic tac toe figure partial game tree for tic tac toe with annotations displaying the or der in which positions are visited in a breadth ﬁrst traversal a breadth ﬁrst traversal of such a game tree is often performed because a computer may be unable to explore a complete game tree in a limited amount of time so the computer will consider all moves then responses to those moves going as deep as computational time allows pseudo code for a breadth ﬁrst traversal is given in code fragment the process is not recursive since we are not traversing entire subtrees at once we use a queue to produce a fifo i e ﬁrst in ﬁrst out semantics for the order in which we visit nodes the overall running time is o n due to the n calls to enqueue and n calls to dequeue algorithm breadthﬁrst t initialize queue q to contain t root while q not empty do p q dequeue p is the oldest entry in the queue perform the visit action for position p for each child c in t children p do q enqueue c add p children to the end of the queue for later visits code fragment algorithm for performing a breadth ﬁrst traversal of a tree tree traversal algorithms inorder traversal of a binary tree the standard preorder postorder and breadth ﬁrst traversals that were introduced for general trees can be directly applied to binary trees in this section we intro duce another common traversal algorithm speciﬁcally for a binary tree during an inorder traversal we visit a position between the recursive traver sals of its left and right subtrees the inorder traversal of a binary tree t can be informally viewed as visiting the nodes of t from left to right indeed for every position p the inorder traversal visits p after all the positions in the left subtree of p and before all the positions in the right subtree of p pseudo code for the inorder traversal algorithm is given in code fragment and an example of an inorder traversal is portrayed in figure algorithm inorder p if p has a left child lc then inorder lc recursively traverse the left subtree of p perform the visit action for position p if p has a right child rc then inorder rc recursively traverse the right subtree of p code fragment algorithm inorder for performing an inorder traversal of a subtree rooted at position p of a binary tree figure inorder traversal of a binary tree the inorder traversal algorithm has several important applications when using a binary tree to represent an arithmetic expression as in figure the inorder traversal visits positions in a consistent order with the standard representation of the expression as in albeit without parentheses chapter trees binary search trees an important application of the inorder traversal algorithm arises when we store an ordered sequence of elements in a binary tree deﬁning a structure we call a binary search tree let s be a set whose unique elements have an order relation for example s could be a set of integers a binary search tree for s is a binary tree t such that for each position p of t position p stores an element of s denoted as e p elements stored in the left subtree of p if any are less than e p elements stored in the right subtree of p if any are greater than e p an example of a binary search tree is shown in figure the above properties assure that an inorder traversal of a binary search tree t visits the elements in nondecreasing order figure a binary search tree storing integers the solid path is traversed when searching successfully for the dashed path is traversed when searching un successfully for we can use a binary search tree t for set s to ﬁnd whether a given search value v is in s by traversing a path down the tree t starting at the root at each internal position p encountered we compare our search value v with the element e p stored at p if v e p then the search continues in the left subtree of p if v e p then the search terminates successfully if v e p then the search continues in the right subtree of p finally if we reach an empty subtree the search terminates unsuccessfully in other words a binary search tree can be viewed as a binary decision tree recall example where the question asked at each internal node is whether the element at that node is less than equal to or larger than the element being searched for we illustrate several examples of the search operation in figure note that the running time of searching in a binary search tree t is proportional to the height of t recall from proposition that the height of a binary tree with n nodes can be as small as log n or as large as n thus binary search trees are most efﬁcient when they have small height chapter is devoted to the study of search trees tree traversal algorithms implementing tree traversals in python when ﬁrst deﬁning the tree adt in section we stated that tree t should include support for the following methods t positions generate an iteration of all positions of tree t iter t generate an iteration of all elements stored within tree t at that time we did not make any assumption about the order in which these iterations report their results in this section we demonstrate how any of the tree traversal algorithms we have introduced could be used to produce these iterations to begin we note that it is easy to produce an iteration of all elements of a tree if we rely on a presumed iteration of all positions therefore support for the iter t syntax can be formally provided by a concrete implementation of the special method iter within the abstract base class tree we rely on python generator syntax as the mechanism for producing iterations see section our implementation of tree iter is given in code fragment 75 def iter self generate an iteration of the tree elements for p in self positions use same order as positions yield p element but yield each element code fragment iterating all elements of a tree instance based upon an iter ation of the positions of the tree this code should be included in the body of the tree class to implement the positions method we have a choice of tree traversal algo rithms given that there are advantages to each of those traversal orders we will provide independent implementations of each strategy that can be called directly by a user of our class we can then trivially adapt one of those as a default order for the positions method of the tree adt preorder traversal we begin by considering the preorder traversal algorithm we will support a public method with calling signature t preorder for tree t which generates a preorder iteration of all positions within the tree however the recursive algorithm for gen erating a preorder traversal as originally described in code fragment must be parameterized by a speciﬁc position within the tree that serves as the root of a subtree to traverse a standard solution for such a circumstance is to deﬁne a non public utility method with the desired recursive parameterization and then to have the public method preorder invoke the nonpublic method upon the root of the tree our implementation of such a design is given in code fragment chapter trees def preorder self generate a preorder iteration of positions in the tree if not self is empty for p in self subtree preorder self root start recursion yield p def subtree preorder self p generate a preorder iteration of positions in subtree rooted at p yield p visit p before its subtrees for c in self children p for each child c for other in self subtree preorder c do preorder of c subtree yield other yielding each to our caller code fragment support for performing a preorder traversal of a tree this code should be included in the body of the tree class formally both preorder and the utility subtree preorder are generators rather than perform a visit action from within this code we yield each position to the caller and let the caller decide what action to perform at that position the subtree preorder method is the recursive one however because we are relying on generators rather than traditional functions the recursion has a slightly different form in order to yield all positions within the subtree of child c we loop over the positions yielded by the recursive call self subtree preorder c and re yield each position in the outer context note that if p is a leaf the for loop over self children p is trivial this is the base case for our recursion we rely on a similar technique in the public preorder method to re yield all positions that are generated by the recursive process starting at the root of the tree if the tree is empty nothing is yielded at this point we have provided full support for the preorder generator a user of the class can therefore write code such as for p in t preorder visit position p the ofﬁcial tree adt requires that all trees support a positions method as well to use a preorder traversal as the default order of iteration we include the deﬁnition shown in code fragment within our tree class rather than loop over the results returned by the preorder call we return the entire iteration as an object def positions self generate an iteration of the tree positions return self preorder return entire preorder iteration code fragment an implementation of the positions method for the tree class that relies on a preorder traversal to generate the results tree traversal algorithms postorder traversal we can implement a postorder traversal using very similar technique as with a preorder traversal the only difference is that within the recursive utility for a post order we wait to yield position p until after we have recursively yield the positions in its subtrees an implementation is given in code fragment def postorder self generate a postorder iteration of positions in the tree if not self is empty for p in self subtree postorder self root start recursion yield p def subtree postorder self p generate a postorder iteration of positions in subtree rooted at p for c in self children p for each child c for other in self subtree postorder c do postorder of c subtree yield other yielding each to our caller yield p visit p after its subtrees code fragment support for performing a postorder traversal of a tree this code should be included in the body of the tree class breadth first traversal in code fragment we provide an implementation of the breadth ﬁrst traversal algorithm in the context of our tree class recall that the breadth ﬁrst traversal algorithm is not recursive it relies on a queue of positions to manage the traver sal process our implementation uses the linkedqueue class from section although any implementation of the queue adt would sufﬁce inorder traversal for binary trees the preorder postorder and breadth ﬁrst traversal algorithms are applicable to all trees and so we include their implementations within the tree abstract base class those methods are inherited by the abstract binarytree class the concrete linkedbinarytree class and any other dependent tree classes we might develop the inorder traversal algorithm because it explicitly relies on the notion of a left and right child of a node only applies to binary trees we therefore include its deﬁnition within the body of the binarytree class we use a similar technique to implement an inorder traversal code fragment as we did with preorder and postorder traversals chapter trees def breadthﬁrst self generate a breadth ﬁrst iteration of the positions of the tree if not self is empty fringe linkedqueue known positions not yet yielded fringe enqueue self root starting with the root while not fringe is empty p fringe dequeue remove from front of the queue yield p report this position for c in self children p fringe enqueue c add children to back of queue code fragment an implementation of a breadth ﬁrst traversal of a tree this code should be included in the body of the tree class def inorder self generate an inorder iteration of positions in the tree if not self is empty for p in self subtree inorder self root yield p def subtree inorder self p generate an inorder iteration of positions in subtree rooted at p if self left p is not none if left child exists traverse its subtree for other in self subtree inorder self left p yield other yield p visit p between its subtrees if self right p is not none if right child exists traverse its subtree for other in self subtree inorder self right p yield other code fragment support for performing an inorder traversal of a binary tree this code should be included in the binarytree class given in code fragment for many applications of binary trees an inorder traversal provides a natural iteration we could make it the default for the binarytree class by overriding the positions method that was inherited from the tree class see code fragment override inherited version to make inorder the default def positions self generate an iteration of the tree positions return self inorder make inorder the default code fragment deﬁning the binarytree position method so that positions are reported using inorder traversal tree traversal algorithms applications of tree traversals in this section we demonstrate several representative applications of tree traversals including some customizations of the standard traversal algorithms table of contents when using a tree to represent the hierarchical structure of a document a preorder traversal of the tree can naturally be used to produce a table of contents for the doc ument for example the table of contents associated with the tree from figure is displayed in figure part a of that ﬁgure gives a simple presentation with one element per line part b shows a more attractive presentation produced by indenting each element based on its depth within the tree a similar presentation could be used to display the contents of a computer ﬁle system based on its tree representation as in figure paper paper title title abstract abstract a b figure table of contents for a document represented by the tree in figure a without indentation b with indentation based on depth within the tree the unindented version of the table of contents given a tree t can be produced with the following code for p in t preorder print p element to produce the presentation of figure b we indent each element with a number of spaces equal to twice the element depth in the tree hence the root ele ment was unindented although we could replace the body of the above loop with the statement print t depth p str p element such an approach is unnecessarily inefﬁcient although the work to produce the preorder traversal runs in o n time based on the analysis of section the calls to depth incur a hid den cost making a call to depth from every position of the tree results in o worst case time as noted when analyzing the algorithm in section chapter trees a preferred approach to producing an indented table of contents is to redesign a top down recursion that includes the current depth as an additional parameter such an implementation is provided in code fragment this implementation runs in worst case o n time except technically the time it takes to print strings of increasing lengths def preorder indent t p d print preorder representation of subtree of t rooted at p at depth d print d str p element use depth for indentation for c in t children p preorder indent t c d child depth is d code fragment efﬁcient recursion for printing indented version of a pre order traversal on a complete tree t the recursion should be started with form preorder indent t t root in the example of figure we were fortunate in that the numbering was embedded within the elements of the tree more generally we might be interested in using a preorder traversal to display the structure of a tree with indentation and also explicit numbering that was not present in the tree for example we might display the tree from figure beginning as electronics r us r d sales domestic international canada s america this is more challenging because the numbers used as labels are implicit in the structure of the tree a label depends on the index of each position relative to its siblings along the path from the root to the current position to accomplish the task we add a representation of that path as an additional parameter to the recursive signature speciﬁcally we use a list of zero indexed numbers one for each position along the downward path other than the root we convert those numbers to one indexed form when printing at the implementation level we wish to avoid the inefﬁciency of duplicating such lists when sending a new parameter from one level of the recursion to the next a standard solution is to share the same list instance throughout the recursion at one level of the recursion a new entry is temporarily added to the end of the list before making further recursive calls in order to leave no trace that same block of code must remove the extraneous entry from the list before completing its task an implementation based on this approach is given in code fragment tree traversal algorithms def preorder label t p d path print labeled representation of subtree of t rooted at p at depth d label join str j for j in path displayed labels are one indexed print d label p element path append path entries are zero indexed for c in t children p preorder label t c d path child depth is d path path pop code fragment efﬁcient recursion for printing an indented and labeled pre sentation of a preorder traversal parenthetic representations of a tree it is not possible to reconstruct a general tree given only the preorder sequence of elements as in figure a some additional context is necessary for the structure of the tree to be well deﬁned the use of indentation or numbered labels provides such context with a very human friendly presentation however there are more concise string representations of trees that are computer friendly in this section we explore one such representation the parenthetic string representation p t of tree t is recursively deﬁned as follows if t consists of a single position p then p t str p element otherwise it is deﬁned recursively as p t str p element p p tk where p is the root of t and tk are the subtrees rooted at the children of p which are given in order if t is an ordered tree we are using here to denote string concatenation as an example the parenthetic representation of the tree of figure would appear as follows line breaks are cosmetic electronics r us r d sales domestic international canada s america overseas africa europe asia australia purchasing manufacturing tv cd tuner although the parenthetic representation is essentially a preorder traversal we cannot easily produce the additional punctuation using the formal implementation of preorder as given in code fragment the opening parenthesis must be produced just before the loop over a position children and the closing parenthesis must be produced just after that loop furthermore the separating commas must be produced the python function parenthesize shown in code fragment is a custom traversal that prints such a parenthetic string representation of a tree t chapter trees def parenthesize t p print parenthesized representation of subtree of t rooted at p print p element end use of end avoids trailing newline if not t is leaf p ﬁrst time true for c in t children p sep if ﬁrst time else determine proper separator print sep end ﬁrst time false any future passes will not be the ﬁrst parenthesize t c recur on child print end include closing parenthesis code fragment function that prints parenthetic string representation of a tree computing disk space in example we considered the use of a tree as a model for a ﬁle system struc ture with internal positions representing directories and leaves representing ﬁles in fact when introducing the use of recursion back in chapter we speciﬁcally examined the topic of ﬁle systems see section although we did not explic itly model it as a tree at that time we gave an implementation of an algorithm for computing the disk usage code fragment the recursive computation of disk space is emblematic of a postorder traversal as we cannot effectively compute the total space used by a directory until after we know the space that is used by its children directories unfortunately the formal implementation of postorder as given in code fragment does not sufﬁce for this purpose as it visits the position of a directory there is no easy way to discern which of the previous positions represent children of that directory nor how much recursive disk space was allocated we would like to have a mechanism for children to return information to the parent as part of the traversal process a custom solution to the disk space prob lem with each level of recursion providing a return value to the parent caller is provided in code fragment def disk space t p return total disk space for subtree of t rooted at p subtotal p element space space used at position p for c in t children p subtotal disk space t c add child space to subtotal return subtotal code fragment recursive computation of disk space for a tree we assume that a space method of each tree element reports the local space used at that position tree traversal algorithms euler tours and the template method pattern the various applications described in section demonstrate the great power of recursive tree traversals unfortunately they also show that the speciﬁc imple mentations of the preorder and postorder methods of our tree class or the inorder method of the binarytree class are not general enough to capture the range of computations we desire in some cases we need more of a blending of the ap proaches with initial work performed before recurring on subtrees additional work performed after those recursions and in the case of a binary tree work performed between the two possible recursions furthermore in some contexts it was impor tant to know the depth of a position or the complete path from the root to that position or to return information from one level of the recursion to another for each of the previous applications we were able to develop a custom implementa tion to properly adapt the recursive ideas but the great principles of object oriented programming introduced in section include adaptability and reusability in this section we develop a more general framework for implementing tree traversals based on a concept known as an euler tour traversal the euler tour traversal of a general tree t can be informally deﬁned as a walk around t where we start by going from the root toward its leftmost child viewing the edges of t as being walls that we always keep to our left see figure figure euler tour traversal of a tree the complexity of the walk is o n because it progresses exactly two times along each of the n edges of the tree once going downward along the edge and later going upward along the edge to unify the concept of preorder and postorder traversals we can think of there being two notable visits to each position p a pre visit occurs when ﬁrst reaching the position that is when the walk passes immediately left of the node in our visualization a post visit occurs when the walk later proceeds upward from that position that is when the walk passes to the right of the node in our visualization chapter trees the process of an euler tour can easily be viewed recursively in between the pre visit and post visit of a given position will be a recursive tour of each of its subtrees looking at figure as an example there is a contiguous portion of the entire tour that is itself an euler tour of the subtree of the node with element that tour contains two contiguous subtours one traversing that position left subtree and another traversing the right subtree the pseudo code for an euler tour traversal of a subtree rooted at a position p is shown in code fragment algorithm eulertour t p perform the pre visit action for position p for each child c in t children p do eulertour t c recursively tour the subtree rooted at c perform the post visit action for position p code fragment algorithm eulertour for performing an euler tour traversal of a subtree rooted at position p of a tree the template method pattern to provide a framework that is reusable and adaptable we rely on an interesting object oriented software design pattern the template method pattern the template method pattern describes a generic computation mechanism that can be specialized for a particular application by redeﬁning certain steps to allow customization the primary algorithm calls auxiliary functions known as hooks at designated steps of the process in the context of an euler tour traversal we deﬁne two separate hooks a pre visit hook that is called before the subtrees are traversed and a postvisit hook that is called after the completion of the subtree traversals our implementation will take the form of an eulertour class that manages the process and deﬁnes trivial deﬁ nitions for the hooks that do nothing the traversal can be customized by deﬁning a subclass of eulertour and overriding one or both hooks to provide specialized behavior python implementation our implementation of an eulertour class is provided in code fragment the primary recursive process is deﬁned in the nonpublic tour method a tour instance is created by sending a reference to a speciﬁc tree to the constructor and then by calling the public execute method which beings the tour and returns a ﬁnal result of the computation tree traversal algorithms class eulertour abstract base class for performing euler tour of a tree hook previsit and hook postvisit may be overridden by subclasses def init self tree prepare an euler tour template for given tree self tree tree def tree self return reference to the tree being traversed return self tree def execute self perform the tour and return any result from post visit of root if len self tree return self tour self tree root start the recursion def tour self p d path perform tour of subtree rooted at position p p position of current node being visited d depth of p in the tree path list of indices of children on path from root to p self hook previsit p d path pre visit p results path append add new index to end of path before recursion for c in self tree children p results append self tour c d path recur on child subtree path increment index path pop remove extraneous index from end of path answer self hook postvisit p d path results post visit p return answer def hook previsit self p d path can be overridden pass def hook postvisit self p d path results can be overridden pass code fragment an eulertour base class providing a framework for perform ing euler tour traversals of a tree chapter trees based on our experience of customizing traversals for sample applications sec tion we build support into the primary eulertour for maintaining the re cursive depth and the representation of the recursive path through a tree using the approach that we introduced in code fragment we also provide a mechanism for one recursive level to return a value to another when post processing formally our framework relies on the following two hooks that can be specialized method hook previsit p d path this function is called once for each position immediately before its subtrees if any are traversed parameter p is a position in the tree d is the depth of that position and path is a list of indices using the convention described in the discussion of code fragment no return value is expected from this function method hook postvisit p d path results this function is called once for each position immediately after its subtrees if any are traversed the ﬁrst three parameters use the same convention as did hook previsit the ﬁnal parameter is a list of objects that were provided as return values from the post visits of the respective subtrees of p any value returned by this call will be available to the parent of p during its postvisit for more complex tasks subclasses of eulertour may also choose to initialize and maintain additional state in the form of instance variables that can be accessed within the bodies of the hooks using the euler tour framework to demonstrate the ﬂexibility of our euler tour framework we revisit the sample applications from section as a simple example an indented preorder traver sal akin to that originally produced by code fragment can be generated with the simple subclass given in code fragment class preorderprintindentedtour eulertour def hook previsit self p d path print d str p element code fragment a subclass of eulertour that produces an indented preorder list of a tree elements such a tour would be started by creating an instance of the subclass for a given tree t and invoking its execute method this could be expressed as follows tour preorderprintindentedtour t tour execute tree traversal algorithms a labeled version of an indented preorder presentation akin to code frag ment could be generated by the new subclass of eulertour shown in code fragment class preorderprintindentedlabeledtour eulertour def hook previsit self p d path label join str j for j in path labels are one indexed print d label p element code fragment a subclass of eulertour that produces a labeled and indented preorder list of a tree elements to produce the parenthetic string representation originally achieved with code fragment we deﬁne a subclass that overrides both the previsit and postvisit hooks our new implementation is given in code fragment class parenthesizetour eulertour def hook previsit self p d path if path and path p follows a sibling print end so preface with comma print p element end then print element if not self tree is leaf p if p has children print end print opening parenthesis def hook postvisit self p d path results if not self tree is leaf p if p has children print end print closing parenthesis code fragment a subclass of eulertour that prints a parenthetic string repre sentation of a tree notice that in this implementation we need to invoke a method on the tree instance that is being traversed from within the hooks the public tree method of the eulertour class serves as an accessor for that tree finally the task of computing disk space as originally implemented in code fragment can be performed quite easily with the eulertour subclass shown in code fragment the postvisit result of the root will be returned by the call to execute class diskspacetour eulertour def hook postvisit self p d path results we simply add space associated with p to that of its subtrees return p element space sum results code fragment a subclass of eulertour that computes disk space for a tree chapter trees the euler tour traversal of a binary tree in section we introduced the concept of an euler tour traversal of a general graph using the template method pattern in designing the eulertour class that class provided methods hook previsit and hook postvisit that could be overrid den to customize a tour in code fragment we provide a binaryeulertour specialization that includes an additional hook invisit that is called once for each position after its left subtree is traversed but before its right subtree is traversed our implementation of binaryeulertour replaces the original tour utility to specialize to the case in which a node has at most two children if a node has only one child a tour differentiates between whether that is a left child or a right child with the in visit taking place after the visit of a sole left child but before the visit of a sole right child in the case of a leaf the three hooks are called in succession class binaryeulertour eulertour abstract base class for performing euler tour of a binary tree this version includes an additional hook invisit that is called after the tour of the left subtree if any yet before the tour of the right subtree if any note right child is always assigned index in path even if no left sibling def tour self p d path results none none will update with results of recursions self hook previsit p d path pre visit for p if self tree left p is not none consider left child path append results self tour self tree left p d path path pop self hook invisit p d path in visit for p if self tree right p is not none consider right child path append results self tour self tree right p d path path pop answer self hook postvisit p d path results post visit p return answer def hook invisit self p d path pass can be overridden code fragment a binaryeulertour base class providing a specialized tour for binary trees the original eulertour base class was given in code fragment tree traversal algorithms figure an inorder drawing of a binary tree to demonstrate use of the binaryeulertour framework we develop a subclass that computes a graphical layout of a binary tree as shown in figure the geometry is determined by an algorithm that assigns x and y coordinates to each position p of a binary tree t using the following two rules x p is the number of positions visited before p in an inorder traversal of t y p is the depth of p in t in this application we take the convention common in computer graphics that x coordinates increase left to right and y coordinates increase top to bottom so the origin is in the upper left corner of the computer screen code fragment provides an implementation of a binarylayout subclass that implements the above algorithm for assigning x y coordinates to the element stored at each position of a binary tree we adapt the binaryeulertour framework by introducing additional state in the form of a count instance variable that repre sents the number of in visits that we have performed the x coordinate for each position is set according to that counter class binarylayout binaryeulertour class for computing x y coordinates for each node of a binary tree def init self tree super init tree must call the parent constructor self count initialize count of processed nodes def hook invisit self p d path p element setx self count x coordinate serialized by count p element sety d y coordinate is depth self count advance count of processed nodes code fragment a binarylayout class that computes coordinates at which to draw positions of a binary tree we assume that the element type for the original tree supports setx and sety methods chapter trees case study an expression tree in example we introduced the use of a binary tree to represent the structure of an arithmetic expression in this section we deﬁne a new expressiontree class that provides support for constructing such trees and for displaying and evaluating the arithmetic expression that such a tree represents our expressiontree class is de ﬁned as a subclass of linkedbinarytree and we rely on the nonpublic mutators to construct such trees each internal node must store a string that deﬁnes a binary op erator e g and each leaf must store a numeric value or a string representing a numeric value our eventual goal is to build arbitrarily complex expression trees for compound arithmetic expressions such as however it sufﬁces for the expressiontree class to support two basic forms of initialization expressiontree value create a tree storing the given value at the root expressiontree op create a tree storing string op at the root e g and with the structures of existing expressiontree instances and as the left and right subtrees of the root respectively such a constructor for the expressiontree class is given in code fragment the class formally inherits from linkedbinarytree so it has access to all the non public update methods that were deﬁned in section we use add root to cre ate an initial root of the tree storing the token provided as the ﬁrst parameter then we perform run time checking of the parameters to determine whether the caller invoked the one parameter version of the constructor in which case we are done or the three parameter form in that case we use the inherited attach method to incorporate the structure of the existing trees as subtrees of the root composing a parenthesized string representation a string representation of an existing expression tree instance for example as can be produced by displaying tree elements us ing an inorder traversal but with opening and closing parentheses inserted with a preorder and postorder step respectively in the context of an expressiontree class we support a special str method see section that returns the appropriate string because it is more efﬁcient to ﬁrst build a sequence of individ ual strings to be joined together see discussion of composing strings in sec tion the implementation of str relies on a nonpublic recursive method named parenthesize recur that appends a series of strings to a list these methods are included in code case study an expression tree class expressiontree linkedbinarytree an arithmetic expression tree def init self token left none right none create an expression tree in a single parameter form token should be a leaf value e g and the expression tree will have that value at an isolated node in a three parameter version token should be an operator and left and right should be existing expressiontree instances that become the operands for the binary operator super init linkedbinarytree initialization if not isinstance token str raise typeerror token must be a string self add root token use inherited nonpublic method if left is not none presumably three parameter form if token not in x raise valueerror token must be valid operator self attach self root left right use inherited nonpublic method def str self return string representation of the expression pieces sequence of piecewise strings to compose self parenthesize recur self root pieces return join pieces def parenthesize recur self p result append piecewise representation of p subtree to resulting list if self is leaf p result append str p element leaf value as a string else result append opening parenthesis self parenthesize recur self left p result left subtree result append p element operator self parenthesize recur self right p result right subtree result append closing parenthesis code fragment the beginning of an expressiontree class chapter trees expression tree evaluation the numeric evaluation of an expression tree can be accomplished with a simple application of a postorder traversal if we know the values represented by the two subtrees of an internal position we can calculate the result of the computation that position designates pseudo code for the recursive evaluation of the value repre sented by a subtree rooted at position p is given in code fragment algorithm evaluate recur p if p isa leaf then return the value stored at p else let be the operator stored at p x evaluate recur left p y evaluate recur right p return x y code fragment algorithm evaluate recur for evaluating the expression rep resented by a subtree of an arithmetic expression tree rooted at position p to implement this algorithm in the context of a python expressiontree class we provide a public evaluate method that is invoked on instance t as t evaluate code fragment provides such an implementation relying on a nonpublic evaluate recur method that computes the value of a designated subtree def evaluate self return the numeric result of the expression return self evaluate recur self root def evaluate recur self p return the numeric result of subtree rooted at p if self is leaf p return ﬂoat p element we assume element is numeric else op p element left val self evaluate recur self left p right val self evaluate recur self right p if op return left val right val elif op return left val right val elif op return left val right val else return left val right val treat x or as multiplication code fragment support for evaluating an expressiontree instance case study an expression tree building an expression tree the constructor for the expressiontree class from code fragment provides basic functionality for combining existing trees to build larger expression trees however the question still remains how to construct a tree that represents an ex pression for a given string such as to automate this process we rely on a bottom up construction algorithm as suming that a string can ﬁrst be tokenized so that multidigit numbers are treated atomically see exercise r and that the expression is fully parenthesized the algorithm uses a stack s while scanning tokens of the input expression e to ﬁnd values operators and right parentheses left parentheses are ignored when we see an operator we push that string on the stack when we see a literal value v we create a single node expression tree t storing v and push t on the stack when we see a right parenthesis we pop the top three items from the stack s which represent a subexpression we then construct a tree t using trees for and as subtrees of the root storing and push the resulting tree t back on the stack we repeat this until the expression e has been processed at which time the top element on the stack is the expression tree for e the total running time is o n an implementation of this algorithm is given in code fragment in the form of a stand alone function named build expression tree which produces and returns an appropriate expressiontree instance assuming the input has been tokenized def build expression tree tokens returns an expressiontree based upon by a tokenized expression s we use python list as stack for t in tokens if t in x t is an operator symbol s append t push the operator symbol elif t not in consider t to be a literal s append expressiontree t push trivial tree storing value elif t compose a new tree from three constituent parts right s pop right subtree as per lifo op s pop operator symbol left s pop left subtree s append expressiontree op left right repush tree we ignore a left parenthesis return s pop code fragment implementation of a build expression tree that produces an expressiontree from a sequence of tokens representing an arithmetic expression chapter trees exercises for help with exercises please visit the site www wiley com college goodrich reinforcement r the following questions refer to the tree of figure a which node is the root b what are the internal nodes c how many descendants does node have d how many ancestors does node have e what are the siblings of node homeworks f which nodes are in the subtree rooted at node projects g what is the depth of node papers h what is the height of the tree r show a tree achieving the worst case running time for algorithm depth r give a justiﬁcation of proposition r what is the running time of a call to t p when called on a position p distinct from the root of t see code fragment r describe an algorithm relying only on the binarytree operations that counts the number of leaves in a binary tree that are the left child of their respective parent r let t be an n node binary tree that may be improper describe how to represent t by means of a proper binary tree t l with o n nodes r what are the minimum and maximum number of internal and external nodes in an improper binary tree with n nodes r answer the following questions so as to justify proposition a what is the minimum number of external nodes for a proper binary tree with height h justify your answer b what is the maximum number of external nodes for a proper binary tree with height h justify your answer c let t be a proper binary tree with height h and n nodes show that log n h n d for which values of n and h can the above lower and upper bounds on h be attained with equality r give a proof by induction of proposition r give a direct implementation of the num children method within the class binarytree exercises r find the value of the arithmetic expression associated with each subtree of the binary tree of figure r draw an arithmetic expression tree that has four external nodes storing the numbers and with each number stored in a distinct external node but not necessarily in this order and has three internal nodes each storing an operator from the set so that the value of the root is the operators may return and act on fractions and an operator may be used more than once r draw the binary tree representation of the following arithmetic expres sion r justify table summarizing the running time of the methods of a tree represented with a linked structure by providing for each method a de scription of its implementation and an analysis of its running time r the linkedbinarytree class provides only nonpublic versions of the up date methods discussed on page implement a simple subclass named mutablelinkedbinarytree that provides public wrapper functions for each of the inherited nonpublic update methods r let t be a binary tree with n nodes and let f be the level numbering function of the positions of t as given in section a show that for every position p of t f p b show an example of a binary tree with seven nodes that attains the above upper bound on f p for some position p r show how to use the euler tour traversal to compute the level number f p as deﬁned in section of each position in a binary tree t r let t be a binary tree with n positions that is realized with an array rep resentation a and let f be the level numbering function of the positions of t as given in section give pseudo code descriptions of each of the methods root parent left right is leaf and is root r our deﬁnition of the level numbering function f p as given in sec tion began with the root having number some authors prefer to use a level numbering g p in which the root is assigned number be cause it simpliﬁes the arithmetic for ﬁnding neighboring positions redo exercise r but assuming that we use a level numbering g p in which the root is assigned number r draw a binary tree t that simultaneously satisﬁes the following each internal node of t stores a single character a preorder traversal of t yields examfun an inorder traversal of t yields mafxuen r in what order are positions visited during a preorder traversal of the tree of figure chapter trees r in what order are positions visited during a postorder traversal of the tree of figure r let t be an ordered tree with more than one node is it possible that the preorder traversal of t visits the nodes in the same order as the postorder traversal of t if so give an example otherwise explain why this cannot occur likewise is it possible that the preorder traversal of t visits the nodes in the reverse order of the postorder traversal of t if so give an example otherwise explain why this cannot occur r answer the previous question for the case when t is a proper binary tree with more than one node r consider the example of a breadth ﬁrst traversal given in figure using the annotated numbers from that ﬁgure describe the contents of the queue before each pass of the while loop in code fragment to get started the queue has contents before the ﬁrst pass and contents before the second pass r the collections deque class supports an extend method that adds a col lection of elements to the end of the queue at once reimplement the breadthﬁrst method of the tree class to take advantage of this feature r give the output of the function parenthesize t t root as described in code fragment when t is the tree of figure r what is the running time of parenthesize t t root as given in code fragment for a tree t with n nodes r describe in pseudo code an algorithm for computing the number of de scendants of each node of a binary tree the algorithm should be based on the euler tour traversal r the build expression tree method of the expressiontree class requires input that is an iterable of string tokens we used a convenient exam ple in which each character is its own to ken so that the string itself sufﬁced as input to build expression tree in general a string such as must be explicitly tokenized into list so as to ignore whitespace and to recognize multidigit numbers as a single token write a utility method tokenize raw that returns such a list of tokens for a raw string creativity c deﬁne the internal path length i t of a tree t to be the sum of the depths of all the internal positions in t likewise deﬁne the external path length e t of a tree t to be the sum of the depths of all the external positions in t show that if t is a proper binary tree with n positions then e t i t n exercises c let t be a not necessarily proper binary tree with n nodes and let d be the sum of the depths of all the external nodes of t show that if t has the minimum number of external nodes possible then d is o n and if t has the maximum number of external nodes possible then d is o n log n c let t be a possibly improper binary tree with n nodes and let d be the sum of the depths of all the external nodes of t describe a conﬁguration for t such that d is such a tree would be the worst case for the asymptotic running time of method code fragment c for a tree t let ni denote the number of its internal nodes and let ne denote the number of its external nodes show that if every internal node in t has exactly children then ne c two ordered trees t l and t ll are said to be isomorphic if one of the fol lowing holds both t l and t ll are empty the roots of t l and t ll have the same number k of subtrees and the ith such subtree of t l is isomorphic to the ith such subtree of t ll for i k design an algorithm that tests whether two given ordered trees are iso morphic what is the running time of your algorithm c show that there are more than improper binary trees with n internal nodes such that no pair are isomorphic see exercise c c if we exclude isomorphic trees see exercise c exactly how many proper binary trees exist with exactly leaves c add support in linkedbinarytree for a method delete subtree p that removes the entire subtree rooted at position p making sure to maintain the count on the size of the tree what is the running time of your imple mentation c add support in linkedbinarytree for a method swap p q that has the effect of restructuring the tree so that the node referenced by p takes the place of the node referenced by q and vice versa make sure to properly handle the case when the nodes are adjacent c we can simplify parts of our linkedbinarytree implementation if we make use of of a single sentinel node referenced as the sentinel member of the tree instance such that the sentinel is the parent of the real root of the tree and the root is referenced as the left child of the sentinel fur thermore the sentinel will take the place of none as the value of the left or right member for a node without such a child give a new imple mentation of the update methods delete and attach assuming such a representation chapter trees c describe how to clone a linkedbinarytree instance representing a proper binary tree with use of the attach method c describe how to clone a linkedbinarytree instance representing a not necessarily proper binary tree with use of the add left and add right methods c we can deﬁne a binary tree representation t l for an ordered general tree t as follows see figure for each position p of t there is an associated position pl of t l if p is a leaf of t then pl in t l does not have a left child otherwise the left child of pl is ql where q is the ﬁrst child of p in t if p has a sibling q ordered immediately after it in t then ql is the right child of pl in t otherwise pl does not have a right child given such a representation t l of a general ordered tree t answer each of the following questions a is a preorder traversal of t l equivalent to a preorder traversal of t b is a postorder traversal of t l equivalent to a postorder traversal of t c is an inorder traversal of t l equivalent to one of the standard traver sals of t if so which one a b figure 23 representation of a tree with a binary tree a tree t b binary tree t l for t the dashed edges connect nodes of t l that are siblings in t c give an efﬁcient algorithm that computes and prints for every position p of a tree t the element of p followed by the height of p subtree c 45 give an o n time algorithm for computing the depths of all positions of a tree t where n is the number of nodes of t c the path length of a tree t is the sum of the depths of all positions in t describe a linear time method for computing the path length of a tree t c the balance factor of an internal position p of a proper binary tree is the difference between the heights of the right and left subtrees of p show how to specialize the euler tour traversal of section to print the balance factors of all the internal nodes of a proper binary tree exercises c given a proper binary tree t deﬁne the reﬂection of t to be the binary tree t l such that each node v in t is also in t l but the left child of v in t is v right child in t l and the right child of v in t is v left child in t l show that a preorder traversal of a proper binary tree t is the same as the postorder traversal of t reﬂection but in reverse order c let the rank of a position p during a traversal be deﬁned such that the ﬁrst element visited has rank the second element visited has rank and so on for each position p in a tree t let pre p be the rank of p in a preorder traversal of t let post p be the rank of p in a postorder traversal of t let depth p be the depth of p and let desc p be the number of descendants of p including p itself derive a formula deﬁning post p in terms of desc p depth p and pre p for each node p in t c design algorithms for the following operations for a binary tree t preorder next p return the position visited after p in a preorder traversal of t or none if p is the last node visited inorder next p return the position visited after p in an inorder traversal of t or none if p is the last node visited postorder next p return the position visited after p in a postorder traversal of t or none if p is the last node visited what are the worst case running times of your algorithms c to implement the preorder method of the linkedbinarytree class we re lied on the convenience of python generator syntax and the yield state ment give an alternative implementation of preorder that returns an ex plicit instance of a nested iterator class see section for discussion of iterators c algorithm preorder draw draws a binary tree t by assigning x and y coordinates to each position p such that x p is the number of nodes pre ceding p in the preorder traversal of t and y p is the depth of p in t a show that the drawing of t produced by preorder draw has no pairs of crossing edges b redraw the binary tree of figure using preorder draw c redo the previous problem for the algorithm postorder draw that is simi lar to preorder draw except that it assigns x p to be the number of nodes preceding position p in the postorder traversal c design an algorithm for drawing general trees using a style similar to the inorder traversal approach for drawing binary trees c exercise p described the walk function of the os module this func tion performs a traversal of the implicit tree represented by the ﬁle system read the formal documentation for the function and in particular its use of an optional boolean parameter named topdown describe how its be havior relates to tree traversal algorithms described in this chapter chapter trees sales domestic international canada s america overseas africa europe asia australia a b figure 24 a tree t b indented parenthetic representation of t c the indented parenthetic representation of a tree t is a variation of the parenthetic representation of t see code fragment that uses inden tation and line breaks as illustrated in figure 24 give an algorithm that prints this representation of a tree c 57 let t be a binary tree with n positions deﬁne a roman position to be a position p in t such that the number of descendants in p left subtree differ from the number of descendants in p right subtree by at most describe a linear time method for ﬁnding each position p of t such that p is not a roman position but all of p descendants are roman c let t be a tree with n positions deﬁne the lowest common ancestor lca between two positions p and q as the lowest position in t that has both p and q as descendants where we allow a position to be a descendant of itself given two positions p and q describe an efﬁcient algorithm for ﬁnding the lca of p and q what is the running time of your algorithm c let t be a binary tree with n positions and for any position p in t let dp denote the depth of p in t the distance between two positions p and q in t is dp dq where a is the lowest common ancestor lca of p and q the diameter of t is the maximum distance between two positions in t describe an efﬁcient algorithm for ﬁnding the diameter of t what is the running time of your algorithm c suppose each position p of a binary tree t is labeled with its value f p in a level numbering of t design a fast method for determining f a for the lowest common ancestor lca a of two positions p and q in t given f p and f q you do not need to ﬁnd position a just value f a c give an alternative implementation of the build expression tree method of the expressiontree class that relies on recursion to perform an implicit euler tour of the tree that is being built exercises c 62 note that the build expression tree function of the expressiontree class is written in such a way that a leaf token can be any string for exam ple it parses the expression a b c however within the evaluate method an error would occur when attempting to convert a leaf token to a number modify the evaluate method to accept an optional python dic tionary that can be used to map such string variables to numeric values with a syntax such as t evaluate a b c in this way the same algebraic expression can be evaluated using different values c as mentioned in exercise c postﬁx notation is an unambiguous way of writing an arithmetic expression without parentheses it is deﬁned so that if op is a normal inﬁx fully parenthesized expres sion with operation op then its postﬁx equivalent is op where is the postﬁx version of and is the postﬁx ver sion of the postﬁx version of a single number or variable is just that number or variable so for example the postﬁx version of the inﬁx expression is implement a postﬁx method of the expressiontree class of section that produces the postﬁx notation for the given expression projects p implement the binary tree adt using the array based representation de scribed in section p implement the tree adt using a linked structure as described in sec tion provide a reasonable set of update methods for your tree p 66 the memory usage for the linkedbinarytree class can be streamlined by removing the parent reference from each node and instead having each position instance keep a member path that is a list of nodes representing the entire path from the root to that position this generally saves mem ory because there are typically relatively few stored position instances reimplement the linkedbinarytree class using this strategy p 67 a slicing ﬂoor plan divides a rectangle with horizontal and vertical sides using horizontal and vertical cuts see figure 25a a slicing ﬂoor plan can be represented by a proper binary tree called a slicing tree whose internal nodes represent the cuts and whose external nodes represent the basic rectangles into which the ﬂoor plan is decomposed by the cuts see figure 25b the compaction problem for a slicing ﬂoor plan is deﬁned as follows assume that each basic rectangle of a slicing ﬂoor plan is assigned a minimum width w and a minimum height h the compaction problem is to ﬁnd the smallest possible height and width for each rectangle of the slicing ﬂoor plan that is compatible with the minimum dimensions chapter trees a b figure a slicing ﬂoor plan b slicing tree associated with the ﬂoor plan of the basic rectangles namely this problem requires the assignment of values h p and w p to each position p of the slicing tree such that w if p is a leaf whose basic rectangle has minimum width w if p is an internal position associated with w p max w f w r w f w r a horizontal cut with left child f and right child r if p is an internal position associated with a vertical cut with left child f and right child r h if p is a leaf node whose basic rectangle if p is an internal position associated with h p h f h r a horizontal cut with left child f and right child r if p is an internal position associated with max h f h r a vertical cut with left child f and right child r design a data structure for slicing ﬂoor plans that supports the operations create a ﬂoor plan consisting of a single basic rectangle decompose a basic rectangle by means of a horizontal cut decompose a basic rectangle by means of a vertical cut assign minimum height and width to a basic rectangle draw the slicing tree associated with the ﬂoor plan compact and draw the ﬂoor plan chapter notes p write a program that can play tic tac toe effectively see section to do this you will need to create a game tree t which is a tree where each position corresponds to a game conﬁguration which in this case is a representation of the tic tac toe board see section the root corresponds to the initial conﬁguration for each internal position p in t the children of p correspond to the game states we can reach from p game state in a single legal move for the appropriate player a the ﬁrst player or b the second player positions at even depths correspond to moves for a and positions at odd depths correspond to moves for b leaves are either ﬁnal game states or are at a depth beyond which we do not want to explore we score each leaf with a value that indicates how good this state is for player a in large games like chess we have to use a heuristic scoring function but for small games like tic tac toe we can construct the entire game tree and score leaves as indicating whether player a has a win draw or lose in that conﬁguration a good algorithm for choosing moves is minimax in this algorithm we assign a score to each internal position p in t such that if p represents a turn we compute p score as the maximum of the scores of p children which corresponds to a optimal play from p if an internal node p represents b turn then we compute p score as the minimum of the scores of p children which corresponds to b optimal play from p p 69 implement the tree adt using the binary tree representation described in exercise c 43 you may adapt the linkedbinarytree implementation p write a program that takes as input a general tree t and a position p of t and converts t to another tree with the same set of position adjacencies but now with p as its root chapter notes discussions of the classic preorder inorder and postorder tree traversal methods can be found in knuth fundamental algorithms book the euler tour traversal technique comes from the parallel algorithms community it is introduced by tarjan and vishkin and is discussed by ja ja 54 and by karp and ramachandran the algorithm for drawing a tree is generally considered to be a part of the folklore of graph drawing al gorithms the reader interested in graph drawing is referred to the book by di battista eades tamassia and tollis and the survey by tamassia and liotta the puzzle in exercise r was communicated by micha sharir chapter priority queues contents the priority queue abstract data type priorities the priority queue adt implementing a priority queue the composition design pattern implementation with an unsorted list implementation with a sorted list heaps the heap data structure implementing a priority queue with a heap array based representation of a complete binary tree python heap implementation analysis of a heap based priority queue bottom up heap construction python heapq module sorting with a priority queue selection sort and insertion sort heap sort adaptable priority queues locators implementing an adaptable priority queue exercises the priority queue abstract data type the priority queue abstract data type priorities in chapter we introduced the queue adt as a collection of objects that are added and removed according to the ﬁrst in ﬁrst out fifo principle a com pany customer call center embodies such a model in which waiting customers are told calls will be answered in the order that they were received in that setting a new call is added to the back of the queue and each time a customer service rep resentative becomes available he or she is connected with the call that is removed from the front of the call queue in practice there are many applications in which a queue like structure is used to manage objects that must be processed in some way but for which the ﬁrst in ﬁrst out policy does not sufﬁce consider for example an air trafﬁc control center that has to decide which ﬂight to clear for landing from among many approaching the airport this choice may be inﬂuenced by factors such as each plane distance from the runway time spent waiting in a holding pattern or amount of remaining fuel it is unlikely that the landing decisions are based purely on a fifo policy there are other situations in which a ﬁrst come ﬁrst serve policy might seem reasonable yet for which other priorities come into play to use another airline analogy suppose a certain ﬂight is fully booked an hour prior to departure be cause of the possibility of cancellations the airline maintains a queue of standby passengers hoping to get a seat although the priority of a standby passenger is inﬂuenced by the check in time of that passenger other considerations include the fare paid and frequent ﬂyer status so it may be that an available seat is given to a passenger who has arrived later than another if such a passenger is assigned a better priority by the airline agent in this chapter we introduce a new abstract data type known as a priority queue this is a collection of prioritized elements that allows arbitrary element insertion and allows the removal of the element that has ﬁrst priority when an element is added to a priority queue the user designates its priority by providing an associated key the element with the minimum key will be the next to be removed from the queue thus an element with key will be given priority over an element with key although it is quite common for priorities to be expressed numerically any python object may be used as a key as long as the object type supports a consistent meaning for the test a b for any instances a and b so as to deﬁne a natural order of the keys with such generality applications may develop their own notion of priority for each element for example different ﬁnancial analysts may assign different ratings i e priorities to a particular asset such as a share of stock chapter priority queues the priority queue adt formally we model an element and its priority as a key value pair we deﬁne the priority queue adt to support the following methods for a priority queue p p add k v insert an item with key k and value v into priority queue p p min return a tuple k v representing the key and value of an item in priority queue p with minimum key but do not re move the item an error occurs if the priority queue is empty p remove min remove an item with minimum key from priority queue p and return a tuple k v representing the key and value of the removed item an error occurs if the priority queue is empty p is empty return true if priority queue p does not contain any items len p return the number of items in priority queue p a priority queue may have multiple entries with equivalent keys in which case methods min and remove min may report an arbitrary choice of item having mini mum key values may be any type of object in our initial model for a priority queue we assume that an element key re mains ﬁxed once it has been added to a priority queue in section we consider an extension that allows a user to update an element key within the priority queue example the following table shows a series of operations and their effects on an initially empty priority queue p the priority queue column is somewhat deceiving since it shows the entries as tuples and sorted by key such an internal representation is not required of a priority queue implementing a priority queue implementing a priority queue in this section we show how to implement a priority queue by storing its entries in a positional list l see section we provide two realizations depending on whether or not we keep the entries in l sorted by key the composition design pattern one challenge in implementing a priority queue is that we must keep track of both an element and its key even as items are relocated within our data structure this is reminiscent of a case study from section in which we maintain access counts with each element in that setting we introduced the composition design pattern deﬁning an item class that assured that each element remained paired with its associated count in our primary data structure for priority queues we will use composition to store items internally as pairs consisting of a key k and a value v to implement this concept for all priority queue implementations we provide a priorityqueuebase class see code fragment that includes a deﬁnition for a nested class named item we deﬁne the syntax a b for item instances a and b to be based upon the keys class priorityqueuebase abstract base class for a priority queue class item lightweight composite to store priority queue items slots def init self k v self key k self value v def lt self other return self key other key compare items based on their keys def is empty self concrete method assuming abstract len return true if the priority queue is empty return len self code fragment a priorityqueuebase class with a nested item class that com poses a key and a value into a single object for convenience we provide a concrete implementation of is empty that is based on a presumed len impelementation chapter priority queues implementation with an unsorted list in our ﬁrst concrete implementation of a priority queue we store entries within an unsorted list our unsortedpriorityqueue class is given in code fragment inheriting from the priorityqueuebase class introduced in code fragment for internal storage key value pairs are represented as composites using instances of the inherited item class these items are stored within a positionallist identiﬁed as the data member of our class we assume that the positional list is implemented with a doubly linked list as in section so that all operations of that adt execute in o time we begin with an empty list when a new priority queue is constructed at all times the size of the list equals the number of key value pairs currently stored in the priority queue for this reason our priority queue len method simply returns the length of the internal data list by the design of our priorityqueuebase class we inherit a concrete implementation of the is empty method that relies on a call to our len method each time a key value pair is added to the priority queue via the add method we create a new item composite for the given key and value and add that item to the end of the list such an implementation takes o time the remaining challenge is that when min or remove min is called we must locate the item with minimum key because the items are not sorted we must inspect all entries to ﬁnd one with a minimum key for convenience we deﬁne a nonpublic ﬁnd min utility that returns the position of an item with minimum key knowledge of the position allows the remove min method to invoke the delete method on the positional list the min method simply uses the position to retrieve the item when preparing a key value tuple to return due to the loop for ﬁnding the minimum key both min and remove min methods run in o n time where n is the number of entries in the priority queue a summary of the running times for the unsortedpriorityqueue class is given in table table worst case running times of the methods of a priority queue of size n realized by means of an unsorted doubly linked list the space requirement is o n implementing a priority queue class unsortedpriorityqueue priorityqueuebase base class deﬁnes item a min oriented priority queue implemented with an unsorted list def ﬁnd min self nonpublic utility return position of item with minimum key if self is empty is empty inherited from base class raise empty priority queue is empty small self data ﬁrst walk self data after small while walk is not none if walk element small element small walk walk self data after walk return small def init self create a new empty priority queue self data positionallist def len self return the number of items in the priority queue return len self data 23 24 def add self key value add a key value pair self data add last self item key value 28 def min self return but do not remove k v tuple with minimum key p self ﬁnd min 31 item p element return item key item value def remove min self remove and return k v tuple with minimum key p self ﬁnd min item self data delete p return item key item value code fragment an implementation of a priority queue using an unsorted list the parent class priorityqueuebase is given in code fragment and the positionallist class is from section chapter priority queues implementation with a sorted list an alternative implementation of a priority queue uses a positional list yet main taining entries sorted by nondecreasing keys this ensures that the ﬁrst element of the list is an entry with the smallest key our sortedpriorityqueue class is given in code fragment the implemen tation of min and remove min are rather straightforward given knowledge that the ﬁrst element of a list has a minimum key we rely on the ﬁrst method of the posi tional list to ﬁnd the position of the ﬁrst item and the delete method to remove the entry from the list assuming that the list is implemented with a doubly linked list operations min and remove min take o time this beneﬁt comes at a cost however for method add now requires that we scan the list to ﬁnd the appropriate position to insert the new item our implementation starts at the end of the list walking backward until the new key is smaller than an existing item in the worst case it progresses until reaching the front of the list therefore the add method takes o n worst case time where n is the number of entries in the priority queue at the time the method is executed in summary when using a sorted list to implement a priority queue insertion runs in linear time whereas ﬁnding and removing the minimum can be done in constant time comparing the two list based implementations table compares the running times of the methods of a priority queue realized by means of a sorted and unsorted list respectively we see an interesting trade off when we use a list to implement the priority queue adt an unsorted list supports fast insertions but slow queries and deletions whereas a sorted list allows fast queries and deletions but slow insertions operation unsorted list sorted list len o o is empty o o add o o n min o n o remove min o n o table worst case running times of the methods of a priority queue of size n realized by means of an unsorted or sorted list respectively we assume that the list is implemented by a doubly linked list the space requirement is o n implementing a priority queue class sortedpriorityqueue priorityqueuebase base class deﬁnes item a min oriented priority queue implemented with a sorted list def init self create a new empty priority queue self data positionallist def len self return the number of items in the priority queue return len self data def add self key value add a key value pair newest self item key value make new item instance walk self data last walk backward looking for smaller key while walk is not none and newest walk element walk self data before walk if walk is none 19 self data add ﬁrst newest new key is smallest else self data add after walk newest newest goes after walk 23 def min self 24 return but do not remove k v tuple with minimum key if self is empty raise empty priority queue is empty p self data ﬁrst 28 item p element 29 return item key item value 31 def remove min self remove and return k v tuple with minimum key if self is empty raise empty priority queue is empty 35 item self data delete self data ﬁrst return item key item value code fragment an implementation of a priority queue using a sorted list the parent class priorityqueuebase is given in code fragment and the positionallist class is from section chapter priority queues heaps the two strategies for implementing a priority queue adt in the previous section demonstrate an interesting trade off when using an unsorted list to store entries we can perform insertions in o time but ﬁnding or removing an element with minimum key requires an o n time loop through the entire collection in contrast if using a sorted list we can trivially ﬁnd or remove the minimum element in o time but adding a new element to the queue may require o n time to restore the sorted order in this section we provide a more efﬁcient realization of a priority queue using a data structure called a binary heap this data structure allows us to perform both insertions and removals in logarithmic time which is a signiﬁcant improvement over the list based implementations discussed in section the fundamental way the heap achieves this improvement is to use the structure of a binary tree to ﬁnd a compromise between elements being entirely unsorted and perfectly sorted the heap data structure a heap see figure is a binary tree t that stores a collection of items at its positions and that satisﬁes two additional properties a relational property deﬁned in terms of the way keys are stored in t and a structural property deﬁned in terms of the shape of t itself the relational property is the following heap order property in a heap t for every position p other than the root the key stored at p is greater than or equal to the key stored at p parent as a consequence of the heap order property the keys encountered on a path from the root to a leaf of t are in nondecreasing order also a minimum key is always stored at the root of t this makes it easy to locate such an item when min or remove min is called as it is informally said to be at the top of the heap hence the name heap for the data structure by the way the heap data structure deﬁned here has nothing to do with the memory heap section used in the run time environment supporting a programming language like python for the sake of efﬁciency as will become clear later we want the heap t to have as small a height as possible we enforce this requirement by insisting that the heap t satisfy an additional structural property it must be what we term complete complete binary tree property a heap t with height h is a complete binary tree if levels h of t have the maximum number of nodes possible namely level i has nodes for i h and the remaining nodes at level h reside in the leftmost possible positions at that level heaps figure example of a heap storing entries with integer keys the last position is the one storing entry w the tree in figure is complete because levels and are full and the six nodes in level are in the six leftmost possible positions at that level in formalizing what we mean by the leftmost possible positions we refer to the discussion of level numbering from section in the context of an array based representation of a binary tree in fact in section we will discuss the use of an array to represent a heap a complete binary tree with n elements is one that has positions with level numbering through n for example in an array based representation of the above tree its entries would be stored consecutively from a to a the height of a heap let h denote the height of t insisting that t be complete also has an important consequence as shown in proposition proposition a heap t storing n entries has height h llog nj justiﬁcation from the fact that t is complete we know that the number of nodes in levels through h of t is precisely and that the number of nodes in level h is at least and at most therefore n and n by taking the logarithm of both sides of inequality n we see that height h log n by rearranging terms and taking the logarithm of both sides of inequality n we see that log n h since h is an integer these two inequalities imply that h llog nj chapter priority queues implementing a priority queue with a heap proposition has an important consequence for it implies that if we can perform update operations on a heap in time proportional to its height then those opera tions will run in logarithmic time let us therefore turn to the problem of how to efﬁciently perform various priority queue methods using a heap we will use the composition pattern from section to store key value pairs as items in the heap the len and is empty methods can be implemented based on examination of the tree and the min operation is equally trivial because the heap property assures that the element at the root of the tree has a minimum key the interesting algorithms are those for implementing the add and remove min methods adding an item to the heap let us consider how to perform add k v on a priority queue implemented with a heap t we store the pair k v as an item at a new node of the tree to maintain the complete binary tree property that new node should be placed at a position p just beyond the rightmost node at the bottom level of the tree or as the leftmost position of a new level if the bottom level is already full or if the heap is empty up heap bubbling after an insertion after this action the tree t is complete but it may violate the heap order property hence unless position p is the root of t that is the priority queue was empty before the insertion we compare the key at position p to that of p parent which we denote as q if key kp kq the heap order property is satisﬁed and the algorithm terminates if instead kp kq then we need to restore the heap order property which can be locally achieved by swapping the entries stored at positions p and q see figure and d this swap causes the new item to move up one level again the heap order property may be violated so we repeat the process going up in t until no violation of the heap order property occurs see figure and h the upward movement of the newly inserted entry by means of swaps is con ventionally called up heap bubbling a swap either resolves the violation of the heap order property or propagates it one level up in the heap in the worst case up heap bubbling causes the new entry to move all the way up to the root of heap t thus in the worst case the number of swaps performed in the execution of method add is equal to the height of t by proposition that bound is llog nj heaps a b c d e f g h figure insertion of a new entry with key into the heap of figure a initial heap b after performing operation add c and d swap to locally restore the partial order property e and f another swap g and h ﬁnal swap chapter priority queues removing the item with minimum key let us now turn to method remove min of the priority queue adt we know that an entry with the smallest key is stored at the root r of t even if there is more than one entry with smallest key however in general we cannot simply delete node r because this would leave two disconnected subtrees instead we ensure that the shape of the heap respects the complete binary tree property by deleting the leaf at the last position p of t deﬁned as the rightmost position at the bottommost level of the tree to preserve the item from the last position p we copy it to the root r in place of the item with minimum key that is being removed by the operation figure and b illustrates an example of these steps with minimal item c being removed from the root and replaced by item w from the last position the node at the last position is removed from the tree down heap bubbling after a removal we are not yet done however for even though t is now complete it likely violates the heap order property if t has only one node the root then the heap order property is trivially satisﬁed and the algorithm terminates otherwise we distin guish two cases where p initially denotes the root of t if p has no right child let c be the left child of p otherwise p has both children let c be a child of p with minimal key if key kp kc the heap order property is satisﬁed and the algorithm terminates if instead kp kc then we need to restore the heap order property this can be locally achieved by swapping the entries stored at p and c see figure and d it is worth noting that when p has two children we intentionally consider the smaller key of the two children not only is the key of c smaller than that of p it is at least as small as the key at c sibling this ensures that the heap order property is locally restored when that smaller key is promoted above the key that had been at p and that at c sibling having restored the heap order property for node p relative to its children there may be a violation of this property at c hence we may have to continue swapping down t until no violation of the heap order property occurs see figure h this downward swapping process is called down heap bubbling a swap either resolves the violation of the heap order property or propagates it one level down in the heap in the worst case an entry moves all the way down to the bottom level see figure thus the number of swaps performed in the execution of method remove min is in the worst case equal to the height of heap t that is it is log n by proposition 3 heaps 4 c w w a z a z k f q b k f q b x j e h s x 25 j e 12 h s a b c d e f g h figure 3 removal of the entry with the smallest key from a heap a and b deletion of the last node whose entry gets stored into the root c and d swap to locally restore the heap order property e and f another swap g and h ﬁnal swap chapter priority queues 3 3 array based representation of a complete binary tree the array based representation of a binary tree section 3 is especially suitable for a complete binary tree t we recall that in this implementation the elements of t are stored in an array based list a such that the element at position p in t is stored in a with index equal to the level number f p of p deﬁned as follows if p is the root of t then f p if p is the left child of position q then f p f q if p is the right child of position q then f p 2 f q 2 with this implementation the elements of t have contiguous indices in the range n and the last position of t is always at index n where n is the number of positions of t for example figure 4 illustrates the array based representation of the heap structure originally portrayed in figure 4 c a 6 z k f q 20 b x 25 j e 12 h s w 2 3 4 6 12 figure 4 an array based representation of the heap from figure implementing a priority queue using an array based heap representation allows us to avoid some complexities of a node based tree structure in particular the add and remove min operations of a priority queue both depend on locating the last index of a heap of size n with the array based representation the last position is at index n of the array locating the last position of a complete binary tree implemented with a linked structure requires more effort see exercise c if the size of a priority queue is not known in advance use of an array based representation does introduce the need to dynamically resize the array on occasion as is done with a python list the space usage of such an array based representation of a complete binary tree with n nodes is o n and the time bounds of methods for adding or removing elements become amortized see section 3 3 4 python heap implementation we provide a python implementation of a heap based priority queue in code frag ments 4 and we use an array based representation maintaining a python list of item composites although we do not formally use the binary tree adt code fragment 4 includes nonpublic utility functions that compute the level numbering of a parent or child of another this allows us to describe the rest of our algorithms using tree like terminology of parent left and right however the relevant vari ables are integer indexes not position objects we use recursion to implement the repetition in the upheap and downheap utilities 3 heaps class heappriorityqueue priorityqueuebase base class deﬁnes item 2 a min oriented priority queue implemented with a binary heap 3 nonpublic behaviors 4 def parent self j return j 2 6 def left self j return 2 j def right self j return 2 j 2 12 13 def has left self j return self left j len self data index beyond end of list def has right self j return self right j len self data index beyond end of list 19 def swap self i j 20 swap the elements at indices i and j of array self data i self data j self data j self data i 22 23 def upheap self j 24 parent self parent j 25 if j and self data j self data parent self swap j parent 27 self upheap parent recur at position of parent 28 29 def downheap self j 30 if self has left j 31 left self left j small child left although right may be smaller if self has right j right self right j 35 if self data right self data left small child right if self data small child self data j self swap j small child self downheap small child recur at position of small child code fragment 4 an implementation of a priority queue using an array based heap continued in code fragment the extends the priorityqueuebase class from code fragment chapter priority queues public behaviors 41 def init self create a new empty priority queue 43 self data 45 def len self return the number of items in the priority queue return len self data def add self key value add a key value pair to the priority queue self data append self item key value self upheap len self data upheap newly added position 54 def min self 55 return but do not remove k v tuple with minimum key 56 57 raise empty exception if empty 59 if self is empty raise empty priority queue is empty 61 item self data 62 return item key item value 64 def remove min self 65 remove and return k v tuple with minimum key 66 67 raise empty exception if empty 68 69 if self is empty 70 raise empty priority queue is empty self swap len self data put minimum item at the end item self data pop and remove it from the list self downheap then ﬁx new root return item key item value code fragment an implementation of a priority queue using an array based heap continued from code fragment 4 3 heaps 3 analysis of a heap based priority queue table 3 shows the running time of the priority queue adt methods for the heap implementation of a priority queue assuming that two keys can be compared in o time and that the heap t is implemented with an array based or linked based tree representation in short each of the priority queue adt methods can be performed in o or in o log n time where n is the number of entries at the time the method is exe cuted the analysis of the running time of the methods is based on the following the heap t has n nodes each storing a reference to a key value pair the height of heap t is o log n since t is complete proposition 2 the min operation runs in o because the root of the tree contains such an element locating the last position of a heap as required for add and remove min can be performed in o time for an array based representation or o log n time for a linked tree representation see exercise c 34 in the worst case up heap and down heap bubbling perform a number of swaps equal to the height of t amortized if array based table 3 performance of a priority queue p realized by means of a heap we let n denote the number of entries in the priority queue at the time an operation is executed the space requirement is o n the running time of operations min and remove min are amortized for an array based representation due to occasional re sizing of a dynamic array those bounds are worst case with a linked tree structure we conclude that the heap data structure is a very efﬁcient realization of the priority queue adt independent of whether the heap is implemented with a linked structure or an array the heap based implementation achieves fast running times for both insertion and removal unlike the implementations that were based on using an unsorted or sorted list chapter priority queues 3 6 bottom up heap construction if we start with an initially empty heap n successive calls to the add operation will run in o n log n time in the worst case however if all n key value pairs to be stored in the heap are given in advance such as during the ﬁrst phase of the heap sort algorithm there is an alternative bottom up construction method that runs in o n time heap sort however still requires n log n time because of the second phase in which we repeatedly remove the remaining element with smallest key in this section we describe the bottom up heap construction and provide an implementation that can be used by the constructor of a heap based priority queue for simplicity of exposition we describe this bottom up heap construction as suming the number of keys n is an integer such that n 2h that is the heap is a complete binary tree with every level being full so the heap has height h log n viewed nonrecursively bottom up heap construction consists of the following h log n steps in the ﬁrst step see figure we construct n 2 elementary heaps storing one entry each 2 in the second step see figure d we form n 4 heaps each storing three entries by joining pairs of elementary heaps and adding a new entry the new entry is placed at the root and may have to be swapped with the entry stored at a child to preserve the heap order property 3 in the third step see figure f we form n heaps each storing entries by joining pairs of 3 entry heaps constructed in the previous step and adding a new entry the new entry is placed initially at the root but may have to move down with a down heap bubbling to preserve the heap order property i in the generic ith step 2 i h we form n heaps each storing entries by joining pairs of heaps storing entries constructed in the previous step and adding a new entry the new entry is placed initially at the root but may have to move down with a down heap bubbling to preserve the heap order property h in the last step see figure h we form the ﬁnal heap storing all the n entries by joining two heaps storing n 2 entries constructed in the previous step and adding a new entry the new entry is placed initially at the root but may have to move down with a down heap bubbling to preserve the heap order property we illustrate bottom up heap construction in figure for h 3 3 heaps a b c d e f g h figure bottom up construction of a heap with 15 entries a and b we begin by constructing entry heaps on the bottom level c and d we combine these heaps into 3 entry heaps and then e and f entry heaps until g and h we create the ﬁnal heap the paths of the down heap bubblings are highlighted in d f and h for simplicity we only show the key within each node instead of the entire entry chapter priority queues python implementation of a bottom up heap construction implementing a bottom up heap construction is quite easy given the existence of a down heap utility function the merging of two equally sized heaps that are subtrees of a common position p as described in the opening of this section can be accomplished simply by down heaping p entry for example that is what happened to the key 14 in going from figure f to g with our array based representation of a heap if we initially store all n items in arbitrary order within the array we can implement the bottom up heap construction process with a single loop that makes a call to downheap from each position of the tree as long as those calls are ordered starting with the deepest level and ending with the root of the tree in fact that loop can start with the deepest nonleaf since there is no effect when down heap is called at a leaf position in code fragment 6 we augment the original heappriorityqueue class from section 3 4 to provide support for the bottom up construction of an initial col lection we introduce a nonpublic utility method heapify that calls downheap on each nonleaf position beginning with the deepest and concluding with a call at the root of the tree we have redesigned the constructor of the class to accept an optional parameter that can be any sequence of k v tuples rather than initializing self data to an empty list we use a list comprehension syntax see section 2 to create an initial list of item composites based on the given contents we de clare an empty sequence as the default parameter value so that the default syntax heappriorityqueue continues to result in an empty priority queue def init self contents create a new priority queue by default queue will be empty if contents is given it should be as an iterable sequence of k v tuples specifying the initial contents self data self item k v for k v in contents empty by default if len self data self heapify def heapify self start self parent len self start at parent of last leaf for j in range start going to and including the root self downheap j code fragment 6 revision to the heappriorityqueue class of code frag ments 4 and to support a linear time construction given an initial sequence of entries 3 heaps asymptotic analysis of bottom up heap construction bottom up heap construction is asymptotically faster than incrementally inserting n keys into an initially empty heap intuitively we are performing a single down heap operation at each position in the tree rather than a single up heap operation from each since more nodes are closer to the bottom of a tree than the top the sum of the downward paths is linear as shown in the following proposition proposition 3 bottom up construction of a heap with n entries takes o n time assuming two keys can be compared in o time justiﬁcation the primary cost of the construction is due to the down heap steps performed at each nonleaf position let v denote the path of t from nonleaf node v to its inorder successor leaf that is the path that starts at v goes to the right child of v and then goes down leftward until it reaches a leaf although v is not necessarily the path followed by the down heap bubbling step from v the length v its number of edges is proportional to the height of the subtree rooted at v and thus a bound on the complexity of the down heap operation at v we can bound the total running time of the bottom up heap construction algorithm based on the sum of the sizes of paths v v for intuition figure 6 illustrates the justiﬁcation visually marking each edge with the label of the nonleaf node v whose path v contains that edge we claim that the paths v for all nonleaf v are edge disjoint and thus the sum of the path lengths is bounded by the number of total edges in the tree hence o n to show this we consider what we term right leaning and left leaning edges i e those going from a parent to a right respectively left child a particular right leaning edge e can only be part of the path v for node v that is the parent in the relationship represented by e left leaning edges can be partitioned by considering the leaf that is reached if continuing down leftward until reaching a leaf each nonleaf node only uses left leaning edges in the group leading to that nonleaf node inorder successor since each nonleaf node must have a different inorder successor no two such paths can contain the same left leaning edge we conclude that the bottom up construction of heap t takes o n time figure 6 visual justiﬁcation of the linear running time of bottom up heap con struction each edge e is labeled with a node v for which v contains e if any chapter priority queues 3 python heapq module python standard distribution includes a heapq module that provides support for heap based priority queues that module does not provide any priority queue class instead it provides functions that allow a standard python list to be managed as a heap its model is essentially the same as our own with n elements stored in list cells l through l n based on the level numbering indices with the small est element at the root in l we note that heapq does not separately manage associated values elements serve as their own key the heapq module supports the following functions all of which presume that existing list l satisﬁes the heap order property prior to the call heappush l e push element e onto list l and restore the heap order property the function executes in o log n time heappop l pop and return the element with smallest value from list l and reestablish the heap order property the operation executes in o log n time heappushpop l e push element e on list l and then pop and return the smallest item the time is o log n but it is slightly more efﬁcient than separate calls to push and pop because the size of the list never changes if the newly pushed el ement becomes the smallest it is immediately returned otherwise the new element takes the place of the popped element at the root and a down heap is performed heapreplace l e similar to heappushpop but equivalent to the pop be ing performed before the push in other words the new element cannot be returned as the smallest again the time is o log n but it is more efﬁcient that two separate operations the module supports additional functions that operate on sequences that do not previously satisfy the heap order property heapify l transform unordered list to satisfy the heap order prop erty this executes in o n time by using the bottom up construction algorithm nlargest k iterable produce a list of the k largest values from a given iterable this can be implemented to run in o n k log n time where we use n to denote the length of the iterable see exercise c nsmallest k iterable produce a list of the k smallest values from a given it erable this can be implemented to run in o n k log n time using similar technique as with nlargest 4 sorting with a priority queue 4 sorting with a priority queue in deﬁning the priority queue adt we noted that any type of object can be used as a key but that any pair of keys must be comparable to each other and that the set of keys be naturally ordered in python it is common to rely on the operator to deﬁne such an order in which case the following properties must be satisﬁed irreﬂexive property k k transitive property if and then k3 formally such a relationship deﬁnes what is known as a strict weak order as it allows for keys to be considered equal to each other but the broader equivalence classes are totally ordered as they can be uniquely arranged from smallest to largest due to the transitive property as our ﬁrst application of priority queues we demonstrate how they can be used to sort a collection c of comparable elements that is we can produce a sequence of elements of c in increasing order or at least in nondecreasing order if there are duplicates the algorithm is quite simple we insert all elements into an initially empty priority queue and then we repeatedly call remove min to retrieve the elements in nondecreasing order an implementation of this algorithm is given in code fragment assuming that c is a positional list see chapter 4 we use an original element of the collection as both a key and value when calling p add element element def pq sort c 2 sort a collection of elements stored in a positional list 3 n len c 4 p priorityqueue for j in range n 6 element c delete c ﬁrst p add element element use element as key and value for j in range n k v p remove min c add last v store smallest remaining element in c code fragment an implementation of the pq sort function assuming an ap propriate implementation of a priorityqueue class note that each element of the input list c serves as its own key in the priority queue p with a minor modiﬁcation to this code we can provide more general sup port sorting elements according to an ordering other than the default for exam ple when working with strings the operator deﬁnes a lexicographic ordering which is an extension of the alphabetic ordering to unicode for example we have that 12 4 because of the order of the ﬁrst character of each string just as chapter priority queues apple banana suppose that we have an application in which we have a list of strings that are all known to represent integral values e g 12 and our goal is to sort the strings according to those integral values in python the standard approach for customizing the order for a sorting algo rithm is to provide as an optional parameter to the sorting function an object that is itself a one parameter function that computes a key for a given element see sections and 1 for a discussion of this approach in the context of the built in max function for example with a list of numeric strings we might wish to use the value of int as a key for a string of the list in this case the con structor for the int class can serve as the one parameter function for computing a key in that way the string 4 will be ordered before string 12 because its key int 4 int 12 we leave it as an exercise to support such an optional key parameter for the pq sort function see exercise c 4 1 selection sort and insertion sort our pq sort function works correctly given any valid implementation of the pri ority queue class however the running time of the sorting algorithm depends on the running times of the operations add and remove min for the given priority queue class we next discuss a choice of priority queue implementations that in effect cause the pq sort computation to behave as one of several classic sorting algorithms selection sort if we implement p with an unsorted list then phase 1 of pq sort takes o n time for we can add each element in o 1 time in phase 2 the running time of each remove min operation is proportional to the size of p thus the bottleneck com putation is the repeated selection of the minimum element in phase 2 for this reason this algorithm is better known as selection sort see figure as noted above the bottleneck is in phase 2 where we repeatedly remove an entry with smallest key from the priority queue p the size of p starts at n and incrementally decreases with each remove min until it becomes thus the ﬁrst operation takes time o n the second one takes time o n 1 and so on there fore the total time needed for the second phase is o n n 1 2 1 o n i by proposition 3 3 we have n i n n 1 2 thus phase 2 takes time o as does the entire selection sort algorithm 4 sorting with a priority queue collection c priority queue p 4 2 3 a 4 2 3 b 2 3 4 figure execution of selection sort on collection c 4 2 5 3 insertion sort if we implement the priority queue p using a sorted list then we improve the run ning time of phase 2 to o n for each remove min operation on p now takes o 1 time unfortunately phase 1 becomes the bottleneck for the running time since in the worst case each add operation takes time proportional to the current size of p this sorting algorithm is better known as insertion sort see figure in fact our implementation for adding an element to a priority queue is almost identical to a step of insertion sort as presented in section 5 the worst case running time of phase 1 of insertion sort is o 1 2 n 1 n o n i again by proposition 3 3 this implies a worst case o time for phase 1 and thus the entire insertion sort algorithm however unlike selection sort insertion sort has a best case running time of o n collection c priority queue p input 4 2 5 3 phase 1 a 4 2 5 3 b 2 5 3 4 c 2 5 3 4 d 5 3 2 4 e 3 2 4 5 8 f 2 3 4 5 8 phase 2 a 2 3 4 5 7 8 b 2 3 4 5 7 8 figure 8 execution of insertion sort on collection c 7 4 8 2 5 3 chapter priority queues 4 2 heap sort as we have previously observed realizing a priority queue with a heap has the advantage that all the methods in the priority queue adt run in logarithmic time or better hence this realization is suitable for applications where fast running times are sought for all the priority queue methods therefore let us again consider the pq sort scheme this time using a heap based implementation of the priority queue during phase 1 the ith add operation takes o log i time since the heap has i entries after the operation is performed therefore this phase takes o n log n time it could be improved to o n with the bottom up heap construction described in section 3 6 during the second phase of pq sort the jth remove min operation runs in o log n j 1 since the heap has n j 1 entries at the time the operation is performed summing over all j this phase takes o n log n time so the entire priority queue sorting algorithm runs in o n log n time when we use a heap to im plement the priority queue this sorting algorithm is better known as heap sort and its performance is summarized in the following proposition proposition 4 the heap sort algorithm sorts a collection c of n elements in o n log n time assuming two elements of c can be compared in o 1 time let us stress that the o n log n running time of heap sort is considerably better than the o running time of selection sort and insertion sort section 4 1 implementing heap sort in place if the collection c to be sorted is implemented by means of an array based se quence most notably as a python list we can speed up heap sort and reduce its space requirement by a constant factor using a portion of the list itself to store the heap thus avoiding the use of an auxiliary heap data structure this is accomplished by modifying the algorithm as follows 1 we redeﬁne the heap operations to be a maximum oriented heap with each position key being at least as large as its children this can be done by recoding the algorithm or by adjusting the notion of keys to be negatively oriented at any time during the execution of the algorithm we use the left portion of c up to a certain index i 1 to store the entries of the heap and the right portion of c from index i to n 1 to store the elements of the sequence thus the ﬁrst i elements of c at indices i 1 provide the array list representation of the heap 2 in the ﬁrst phase of the algorithm we start with an empty heap and move the boundary between the heap and the sequence from left to right one step at a time in step i for i 1 n we expand the heap by adding the element at index i 1 4 sorting with a priority queue 3 in the second phase of the algorithm we start with an empty sequence and move the boundary between the heap and the sequence from right to left one step at a time at step i for i 1 n we remove a maximum element from the heap and store it at index n i in general we say that a sorting algorithm is in place if it uses only a small amount of memory in addition to the sequence storing the objects to be sorted the variation of heap sort above qualiﬁes as in place instead of transferring elements out of the sequence and then back in we simply rearrange them we illustrate the second phase of in place heap sort in figure a b c d e f figure phase 2 of an in place heap sort the heap portion of each sequence representation is highlighted the binary tree that each sequence implicitly repre sents is diagrammed with the most recent path of down heap bubbling highlighted chapter priority queues 5 adaptable priority queues the methods of the priority queue adt given in section 1 2 are sufﬁcient for most basic applications of priority queues such as sorting however there are situations in which additional methods would be useful as shown by the scenarios below involving the standby airline passenger application a standby passenger with a pessimistic attitude may become tired of waiting and decide to leave ahead of the boarding time requesting to be removed from the waiting list thus we would like to remove from the priority queue the entry associated with this passenger operation remove min does not sufﬁce since the passenger leaving does not necessarily have ﬁrst priority instead we want a new operation remove that removes an arbitrary entry another standby passenger ﬁnds her gold frequent ﬂyer card and shows it to the agent thus her priority has to be modiﬁed accordingly to achieve this change of priority we would like to have a new operation update allowing us to replace the key of an existing entry with a new key we will see another application of adaptable priority queues when implementing certain graph algorithms in sections 14 6 2 and 14 7 1 in this section we develop an adaptable priority queue adt and demonstrate how to implement this abstraction as an extension to our heap based priority queue 5 1 locators in order to implement methods update and remove efﬁciently we need a mecha nism for ﬁnding a user element within a priority queue that avoids performing a linear search through the entire collection to support our goal when a new ele ment is added to the priority queue we return a special object known as a locator to the caller we then require the user to provide an appropriate locator as a parameter when invoking the update or remove method as follows for a priority queue p p update loc k v replace key and value for the item identiﬁed by locator loc p remove loc remove the item identiﬁed by locator loc from the priority queue and return its key value pair the locator abstraction is somewhat akin to the position abstraction used in our positional list adt from section 7 4 and our tree adt from chapter 8 however we differentiate between a locator and a position because a locator for a priority queue does not represent a tangible placement of an element within the structure in our priority queue an element may be relocated within our data structure during an operation that does not seem directly relevant to that element a locator for an item will remain valid as long as that item remains somewhere in the queue 5 adaptable priority queues 5 2 implementing an adaptable priority queue in this section we provide a python implementation of an adaptable priority queue as an extension of our heappriorityqueue class from section 3 4 to implement a locator class we will extend the existing item composite to add an additional ﬁeld designating the current index of the element within the array based representation of our heap as shown in figure token 4 c 5 a 1 6 z 2 15 k 3 f 4 7 q 5 20 b 6 x 7 1 2 3 4 5 6 7 figure representing a heap using a sequence of locators the third element of each locator instance corresponds to the index of the item within the array iden tiﬁer token is presumed to be a locator reference in the user scope the list is a sequence of references to locator instances each of which stores a key value and the current index of the item within the list the user will be given a reference to the locator instance for each inserted element as portrayed by the token identiﬁer in figure when we perform priority queue operations on our heap and items are relo cated within our structure we reposition the locator instances within the list and we update the third ﬁeld of each locator to reﬂect its new index within the list as an ex ample figure shows the state of the above heap after a call to remove min the heap operation caused the minimum entry 4 c to be removed and the en try 16 x to be temporarily moved from the last position to the root followed by a down heap bubble phase during the down heap element 16 x was swapped token 5 a f 1 6 z 2 15 k 3 16 x 4 7 q 5 20 b 6 1 2 3 4 5 6 7 figure the result of a call to remove min on the heap originally portrayed in figure identiﬁer token continues to reference the same locator instance as in the original conﬁguration but the placement of that locator in the list has changed as has the third ﬁeld of the locator chapter priority queues with its left child 5 a at index 1 of the list then swapped with its right child f at index 4 of the list in the ﬁnal conﬁguration the locator instances for all affected elements have been modiﬁed to reﬂect their new location it is important to emphasize that the locator instances have not changed iden tity the user token reference portrayed in figures 10 and continues to reference the same instance we have simply changed the third ﬁeld of that instance and we have changed where that instance is referenced within the list sequence with this new representation providing the additional support for the adaptable priority queue adt is rather straightforward when a locator instance is sent as a parameter to update or remove we may rely on the third ﬁeld of that structure to designate where the element resides in the heap with that knowledge the update of a key may simply require an up heap or down heap bubbling step to reestablish the heap order property the complete binary tree property remains intact to implement the removal of an arbitrary element we move the element at the last position to the vacated location and again perform an appropriate bubbling step to satisfy the heap order property python implementation code fragments 8 and present a python implementation of an adaptable pri ority queue as a subclass of the heappriorityqueue class from section 3 4 our modiﬁcations to the original class are relatively minor we deﬁne a public locator class that inherits from the nonpublic item class and augments it with an addi tional index ﬁeld we make it a public class because we will be using locators as return values and parameters however the public interface for the locator class does not include any other functionality for the user to update locators during the ﬂow of our heap operations we rely on an inten tional design decision that our original class uses a nonpublic swap method for all data movement we override that utility to execute the additional step of updating the stored indices within the two swapped locator instances we provide a new bubble utility that manages the reinstatement of the heap order property when a key has changed at an arbitrary position within the heap either due to a key update or the blind replacement of a removed element with the item from the last position of the tree the bubble utility determines whether to apply up heap or down heap bubbling depending on whether the given location has a parent with a smaller key if an updated key coincidentally remains valid for its current location we technically call downheap but no swaps result the public methods are provided in code fragment the existing add method is overridden both to make use of a locator instance rather than an item instance for storage of the new element and to return the locator to the caller the remainder of that method is similar to the original with the management of locator indices enacted by the use of the new version of swap there is no reason to over 5 adaptable priority queues ride the remove min method because the only change in behavior for the adaptable priority queue is again provided by the overridden swap method the update and remove methods provide the core new functionality for the adaptable priority queue we perform robust checking of the validity of a locator that is sent by a caller although in the interest of space our displayed code does not do preliminary type checking to ensure that the parameter is indeed a locator instance to ensure that a locator is associated with a current element of the given priority queue we examine the index that is encapsulated within the locator object and then verify that the entry of the list at that index is the very same locator in conclusion the adaptable priority queue provides the same asymptotic efﬁ ciency and space usage as the nonadaptive version and provides logarithmic per formance for the new locator based update and remove methods a summary of the performance is given in table 4 1 class adaptableheappriorityqueue heappriorityqueue 2 a locator based priority queue implemented with a binary heap 3 4 nested locator class 5 class locator heappriorityqueue item 6 token for locating an entry of the priority queue 7 slots add index as additional ﬁeld 8 def init self k v j 10 super init k v 11 self index j 12 13 nonpublic behaviors 14 override swap to record new indices 15 def swap self i j 16 super swap i j perform the swap 17 self data i index i reset locator index post swap self data j index j reset locator index post swap 19 20 def bubble self j if j 0 and self data j self data self parent j 22 self upheap j 23 else 24 self downheap j code fragment 8 an implementation of an adaptable priority queue continued in code fragment this extends the heappriorityqueue class of code frag ments 4 and 5 chapter priority queues 25 def add self key value 26 add a key value pair 27 token self locator key value len self data initiaize locator index 28 self data append token 29 self upheap len self data 1 30 return token 31 def update self loc newkey newval 33 update the key and value for the entry identiﬁed by locator loc 34 j loc index 35 if not 0 j len self and self data j is loc 36 raise valueerror invalid locator 37 loc key newkey 38 loc value newval self bubble j 40 41 def remove self loc 42 remove and return the k v pair identiﬁed by locator loc 43 j loc index if not 0 j len self and self data j is loc 45 raise valueerror invalid locator 46 if j len self 1 item at last position 47 self data pop just remove it 48 else 49 self swap j len self 1 swap item to the last position 50 self data pop remove it from the list self bubble j ﬁx item displaced by the swap 52 return loc key loc value code fragment an implementation of an adaptable priority queue continued from code fragment 8 amortized with dynamic array table 4 running times of the methods of an adaptable priority queue p of size n realized by means of our array based heap representation the space requirement is o n 6 exercises 6 exercises for help with exercises please visit the site www wiley com college goodrich reinforcement r 1 how long would it take to remove the flog smallest elements from a heap that contains n entries using the remove min operation r 2 suppose you label each position p of a binary tree t with a key equal to its preorder rank under what circumstances is t a heap r 3 what does each remove min call return within the following sequence of priority queue adt methods add 5 a add 4 b add 7 f add 1 d remove min add 3 j add 6 l remove min remove min add 8 g remove min add 2 h remove min remove min r 4 an airport is developing a computer simulation of air trafﬁc control that handles events such as landings and takeoffs each event has a time stamp that denotes the time when the event will occur the simulation program needs to efﬁciently perform the following two fundamental operations insert an event with a given time stamp that is add a future event extract the event with smallest time stamp that is determine the next event to process which data structure should be used for the above operations why r 5 the min method for the unsortedpriorityqueue class executes in o n time as analyzed in table 2 give a simple modiﬁcation to the class so that min runs in o 1 time explain any necessary modiﬁcations to other methods of the class r 6 can you adapt your solution to the previous problem to make remove min run in o 1 time for the unsortedpriorityqueue class explain your an swer r 7 illustrate the execution of the selection sort algorithm on the following input sequence 22 15 36 44 10 3 13 29 25 r 8 illustrate the execution of the insertion sort algorithm on the input se quence of the previous problem r give an example of a worst case sequence with n elements for insertion sort and show that insertion sort runs in time on such a sequence r 10 at which positions of a heap might the third smallest key be stored r 11 at which positions of a heap might the largest key be stored chapter priority queues r 12 consider a situation in which a user has numeric keys and wishes to have a priority queue that is maximum oriented how could a standard min oriented priority queue be used for such a purpose r 13 illustrate the execution of the in place heap sort algorithm on the follow ing input sequence 2 5 16 4 10 23 39 26 15 r 14 let t be a complete binary tree such that position p stores an element with key f p where f p is the level number of p see section 8 3 2 is tree t a heap why or why not r 15 explain why the description of down heap bubbling does not consider the case in which position p has a right child but not a left child r 16 is there a heap h storing seven entries with distinct keys such that a pre order traversal of h yields the entries of h in increasing or decreasing order by key how about an inorder traversal how about a postorder traversal if so give an example if not say why r 17 let h be a heap storing 15 entries using the array based representation of a complete binary tree what is the sequence of indices of the array that are visited in a preorder traversal of h what about an inorder traversal of h what about a postorder traversal of h r 18 show that the sum n log i i 1 which appears in the analysis of heap sort is n log n r 19 bill claims that a preorder traversal of a heap will list its keys in nonde creasing order draw an example of a heap that proves him wrong r 20 hillary claims that a postorder traversal of a heap will list its keys in non increasing order draw an example of a heap that proves her wrong r 21 show all the steps of the algorithm for removing the entry 16 x from the heap of figure 1 assuming the entry had been identiﬁed with a locator r 22 show all the steps of the algorithm for replacing key of entry 5 a with 18 in the heap of figure 1 assuming the entry had been identiﬁed with a locator r 23 draw an example of a heap whose keys are all the odd numbers from 1 to 59 with no repeats such that the insertion of an entry with key 32 would cause up heap bubbling to proceed all the way up to a child of the root replacing that child key with 32 r 24 describe a sequence of n insertions in a heap that requires n log n time to process r 25 complete figure by showing all the steps of the in place heap sort algorithm show both the array and the associated heap at the end of each step 6 exercises creativity c 26 show how to implement the stack adt using only a priority queue and one additional integer instance variable c 27 show how to implement the fifo queue adt using only a priority queue and one additional integer instance variable c 28 professor idle suggests the following solution to the previous problem whenever an item is inserted into the queue it is assigned a key that is equal to the current size of the queue does such a strategy result in fifo semantics prove that it is so or provide a counterexample c 29 reimplement the sortedpriorityqueue using a python list make sure to maintain remove min o 1 performance c 30 give a nonrecursive implementation of the upheap method for the class heappriorityqueue c 31 give a nonrecursive implementation of the downheap method for the class heappriorityqueue c 32 assume that we are using a linked representation of a complete binary tree t and an extra reference to the last node of that tree show how to update the reference to the last node after operations add or remove min in o log n time where n is the current number of nodes of t be sure and handle all possible cases as illustrated in figure 12 c 33 when using a linked tree representation for a heap an alternative method for ﬁnding the last node during an insertion in a heap t is to store in the last node and each leaf node of t a reference to the leaf node immedi ately to its right wrapping to the ﬁrst node in the next lower level for the rightmost leaf node show how to maintain such references in o 1 time per operation of the priority queue adt assuming that t is implemented with a linked structure a b figure 12 updating the last node in a complete binary tree after operation add or remove node w is the last node before operation add or after operation remove node z is the last node after operation add or before operation remove chapter priority queues c 34 we can represent a path from the root to a given node of a binary tree by means of a binary string where 0 means go to the left child and 1 means go to the right child for example the path from the root to the node storing 8 w in the heap of figure 12a is represented by design an o log n time algorithm for ﬁnding the last node of a complete binary tree with n nodes based on the above representation show how this algorithm can be used in the implementation of a complete binary tree by means of a linked structure that does not keep a reference to the last node c 35 given a heap t and a key k give an algorithm to compute all the entries in t having a key less than or equal to k for example given the heap of figure 12a and query k 7 the algorithm should report the entries with keys 2 4 5 6 and 7 but not necessarily in this order your algorithm should run in time proportional to the number of entries returned and should not modify the heap c 36 provide a justiﬁcation of the time bounds in table 4 c 37 give an alternative analysis of bottom up heap construction by showing the following summation is o 1 for any positive integer h h i i 1 c 38 suppose two binary trees and hold entries satisfying the heap order property but not necessarily the complete binary tree property describe a method for combining and into a binary tree t whose nodes hold the union of the entries in and and also satisfy the heap order prop erty your algorithm should run in time o where and are the respective heights of and c 39 implement a heappushpop method for the heappriorityqueue class with semantics akin to that described for the heapq module in section 3 7 c 40 implement a heapreplace method for the heappriorityqueue class with semantics akin to that described for the heapq module in section 3 7 c 41 tamarindo airlines wants to give a ﬁrst class upgrade coupon to their top log n frequent ﬂyers based on the number of miles accumulated where n is the total number of the airlines frequent ﬂyers the algorithm they currently use which runs in o n log n time sorts the ﬂyers by the number of miles ﬂown and then scans the sorted list to pick the top log n ﬂyers describe an algorithm that identiﬁes the top logn ﬂyers in o n time c 42 explain how the k largest elements from an unordered collection of size n can be found in time o n k log n using a maximum oriented heap c 43 explain how the k largest elements from an unordered collection of size n can be found in time o n log k using o k auxiliary space 6 exercises c 44 given a class priorityqueue that implements the minimum oriented pri ority queue adt provide an implementation of a maxpriorityqueue class that adapts to provide a maximum oriented abstraction with methods add max and remove max your implementation should not make any as sumption about the internal workings of the original priorityqueue class nor the type of keys that might be used c 45 write a key function for nonnegative integers that determines order based on the number of 1 in each integer s binary expansion c 46 give an alternative implementation of the pq sort function from code fragment 7 that accepts a key function as an optional parameter c 47 describe an in place version of the selection sort algorithm for an array that uses only o 1 space for instance variables in addition to the array c 48 assuming the input to the sorting problem is given in an array a describe how to implement the insertion sort algorithm using only the array a and at most a constant number of additional variables c 49 give an alternate description of the in place heap sort algorithm using the standard minimum oriented priority queue instead of a maximum oriented one c 50 an online computer system for trading stocks needs to process orders of the form buy shares at x each or sell shares at y each a buy order for x can only be processed if there is an existing sell order with price y such that y x likewise a sell order for y can only be processed if there is an existing buy order with price x such that y x if a buy or sell order is entered but cannot be processed it must wait for a future order that allows it to be processed describe a scheme that allows buy and sell orders to be entered in o log n time independent of whether or not they can be immediately processed c 51 extend a solution to the previous problem so that users are allowed to update the prices for their buy or sell orders that have yet to be processed c 52 a group of children want to play a game called unmonopoly where in each turn the player with the most money must give half of his her money to the player with the least amount of money what data structure s should be used to play this game efﬁciently why projects p 53 implement the in place heap sort algorithm experimentally compare its running time with that of the standard heap sort that is not in place p 54 use the approach of either exercise c 9 42 or c 9 43 to reimplement the top method of the favoriteslistmtf class from section 7 6 2 make sure that results are generated from largest to smallest chapter 9 priority queues p 9 55 write a program that can process a sequence of stock buy and sell orders as described in exercise c 9 50 p 9 56 let s be a set of n points in the plane with distinct integer x and y coordinates let t be a complete binary tree storing the points from s at its external nodes such that the points are ordered left to right by in creasing x coordinates for each node v in t let s v denote the subset of s consisting of points stored in the subtree rooted at v for the root r of t deﬁne top r to be the point in s s r with maximum y coordinate for every other node v deﬁne top r to be the point in s with highest y coordinate in s v that is not also the highest y coordinate in s u where u is the parent of v in t if such a point exists such labeling turns t into a priority search tree describe a linear time algorithm for turning t into a priority search tree implement this approach p 9 57 one of the main applications of priority queues is in operating systems for scheduling jobs on a cpu in this project you are to build a program that schedules simulated cpu jobs your program should run in a loop each iteration of which corresponds to a time slice for the cpu each job is assigned a priority which is an integer between 20 highest priority and 19 lowest priority inclusive from among all jobs waiting to be pro cessed in a time slice the cpu must work on a job with highest priority in this simulation each job will also come with a length value which is an integer between 1 and inclusive indicating the number of time slices that are needed to process this job for simplicity you may assume jobs cannot be interrupted once it is scheduled on the cpu a job runs for a number of time slices equal to its length your simulator must output the name of the job running on the cpu in each time slice and must process a sequence of commands one per time slice each of which is of the form add job name with length n and priority p or no new job this slice p 9 58 develop a python implementation of an adaptable priority queue that is based on an unsorted list and supports location aware entries chapter notes course syllabus cmpt principles of computer science note this is a preliminary version of the syllabus some details relating to the course offering e g the number of assignments or the weight of the final exam may change slightly with the final version this information is provided as a convenience for students and should not be taken as the final word as long as this notice appears note this course replaces cmpt in several degree programs but is not an exact equivalent catalogue description this course builds on cmpt by introducing additional problem solving methods and computer science principles to solve larger problems that are more data intensive or require more sophisticated techniques these principles include data structures for efficient storage and retrieval of data selection of appropriate data structures algorithmic paradigms for solving difficult problems and analysis of algorithms time and space requirements this course also emphasizes fundamental principles of coding style testing and top down design for writing robust maintainable software prerequisite cmpt or cmpt and permission of the department note students with credit for cmpt cannot take cmpt for credit be proficient in writing robust maintainable software in python design algorithms using pseudo code and analyze algorithms written in pseudo code analyze time and space complexity of algorithms and to compare and evaluate algorithms and data structures in terms of complexity analysis explain the concept of abstract data types atds in terms of interface and encapsulation describe the use and behavior of objects in python as examples of the adt concept apply specific data types arrays trees binary search trees as part of the solution to computational problems apply recursion to computational tasks involving data structures such as lists and trees apply skills in elementary software design including test driven design and iterative development apply skills in elementary software testing including black box testing and boundary conditions and debugging describe and apply strategies such as divide and conquer greedy algorithms and backtracking course overview this course can be taken as a science credit for arts science majors and is also a required course in computer science major programs and a few other programs lectures will be opportunities to apply the concepts covered in the course discuss them as well as to ask questions and receive guidance we will not waste class time reading powerpoint slides to you short readings will be assigned before each class and you will be expected to be prepared to discuss ask questions and participate laboratories are your opportunities to put the week material into practice under the guidance of a teaching assistant assignments are weekly to ensure that all the relevant material is put into regular consistent practice even a simple assignment can turn into a time consuming affair if you get stuck on something that blocks your progress working at the last minute is a guaranteed source of stress and burn out to manage your workload you must practice effective time management start every assignment early to allow yourself time to consult if you run into a problem please make use of the teaching resources instructors office hours tas labs lectures discussion forums etc available to you students who complete cmpt with diligence will be able to build substantial applications making use of python extensive libraries build computational solutions to a wide variety of problems using a range of algorithmic strategies and a range of data structures verify that python programs work correctly assess and mitigate computational efficiency concerns that may arise in practice continue their formal study of computer science in courses such as cmpt cmpt and cmpt acm learning outcomes this course achieves the following learning outcomes listed alphabetically from the acm computer science curriculum guidelines https www acm org education final report pdf implement test and debug simple recursive functions and procedures usage sdf ad determine whether a recursive or iterative solution is most appropriate for a problem assessment sdf ad identify the data components and behaviors of multiple abstract data types usage sdf ad implement a coherent abstract data type with loose coupling between components and behaviors usage sdf ad design implement test and debug a program that uses each of the following fundamental programming constructs basic computation simple i o standard conditional and iterative structures the definition of functions and parameter passing usage sdf fpc discuss the appropriate use of built in data structures familiarity describe common applications for each of the following data structures stack queue priority queue set and map familiarity write programs that use each of the following data structures arrays records structs strings linked lists stacks queues sets and maps usage compare alternative implementations of data structures with respect to performance assessment choose the appropriate data structure for modeling a given problem assessment explain why the creation of correct program components is important in the production of high quality software familiarity sdf dm apply a variety of strategies to the testing and debugging of simple programs usage sdf dm construct execute and debug programs using a modern ide and associated tools such as unit testing tools and visual debuggers usage sdf dm construct and debug programs using the standard libraries available with a chosen programming language usage sdf dm explain what is meant by best and worst case behavior of an algorithm partial al ba determine informally the time and space complexity of simple algorithms usage al ba list and contrast standard complexity classes familiarity al ba perform empirical studies to validate hypotheses about runtime stemming from mathematical analysis run algorithms on input of various sizes and compare performance assessment al ba for each of the strategies brute force greedy divide and conquer recursive backtracking and dynamic programming identify a practical example to which it would apply familiarity al as use a greedy approach to solve an appropriate problem and determine if the greedy rule chosen leads to an optimal solution assessment al as use a divide and conquer algorithm to solve an appropriate problem usage al as use recursive backtracking to solve a problem such as navigating a maze usage al as implement simple search algorithms and explain the differences in their time complexities assessment al fdsa explain how tree balance affects the efficiency of various binary search tree operations familiarity al fdsa student evaluation note all students must be properly registered in order to attend lectures and receive credit for this course grading scheme preliminary assignments each tutorial exercises each midterm exam final exam total there will be assignments in this course one approximately every week assignments will consist of two portions written questions that require written answers programming questions which require you to write computer programs submission instructions will be included with each assignment description generally you will upload your solutions as files to moodle unless you are instructed otherwise generally text files are preferred to documents that include formatting e g msword documents a document that cannot be opened will receive a grade of zero do not assume the markers will take the time to open your file if it is in a file format that is not standard tutorials have associated exercises which we expect you to do for the skills you learn by doing them you are expected to submit your tutorial exercises weekly along with your assignment solutions these exercises will be graded and will make up of your total course grade you should expect to complete these exercises in the time allotted for tutorials and should not require any time outside the tutorials the midterm exam will be held in mid february the midterm examination is written closed book only bring water your student card and writing instruments the midterm examination will have some multiple choice and some programming and short answer questions the mid term examination is intended to provide practice for the final exam and to provide feedback to students regarding their current performance please see the section on policies for important information concerning missed midterms and final exams in the case of a missed midterm the instructor in consultation with the student will determine how the missed work will be compensated for one potential alternative is transferring the weight of the midterm onto the final examination the final examination will be scheduled by central timetabling to occur during the usual final examination interval it will be three hours long written closed book bring only water your student card and writing instruments please see the section on policies for important information concerning missed midterms and final exams criteria that must be met to pass students must write the final exam a student who does not write the final exam will receive a grade of at most in the course attendance expectation attend every class and participate actively there will be short reading assignments for all classes see moodle webpage http moodle cs usask ca and students are expected to come to class having completed the readings there is no penalty for missed lectures attend all laboratory sessions these are opportunities to practice the course material with the guidance of a teaching assistant there is no penalty for missed lab sessions provided that the lab exercises are completed by the due date attend the midterm examination if you have part time work or other responsibilities please try to make arrangements asap that will allow you to write the midterm we will make alternative arrangements for students who cannot attend the evening seatings but obviously we would like to keep the special arrangements to a minimum a missed midterm will be counted as a score of zero excluding exemptions due to health or compassionate grounds final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted tutorials and help sessions tutorials tutorials in a laboratory setting are mandatory and include new material not presented in class lectures emphasize the data organization concepts using pseudocode tutorials focus on how to implement in c the concepts studied in lecture material presented in tutorials may appear on examinations if you miss a tutorial section you may try to attend another section during that week but there is considerable risk you will not be able to find a seat help sessions obstacles to progress and completion of assignments can sometimes be part of the homework i e something we want you to think about carefully and sometimes beyond the scope of the course i e a problem that you can t really be expected to manage in first year and it can be nearly impossible to tell the difference without some advice from a ta or instructor there are several help sessions in the spinks computer lab that are specifically for cmpt students the tas are all prepared for the assignments and lab questions we highly recommend you to work in the computer lab because it is very helpful if you can get help when you have difficulties the schedule will be announced in the first two weeks of the term on moodle textbook information extensive notes will be provided on the course moodle site required texts tba recommended text tba useful texts goodrich tamassia and goldwater data structures and algorithms in python wiley isbn sedgewick wayne and dondero introduction to programming in python addison wesly lecture schedule week introduction to software design principles week programming practice good style defensive programming week stacks and queues and their applications week algorithm analysis or why your code isn t as good as you thought week abstract data types for design and testing testing your code week trees binary trees and binary search trees and their applications week more software development techniques iterative design test driven design week objects week getting the most out of python apis and interfaces week techniques greedy algorithms and back tracking week looking beneath python understanding the machine leads to speed when you need it week looking beyond python other languages policies recording of lectures lecture videos will be available through a link on the course moodle site late assignments due to the weekly assignment schedule late submissions cannot be accepted be sure to start your assignments early and hand in partial solutions for partial credit if necessary we understand that legitimate exceptional circumstances sometimes prevent a deadline from being met if you feel you cannot submit an assignment on time please talk to the course instructor at least a day before the assignment is due extensions on assignments will be granted only by the course instructor all extension requests will require suitable documentation missed assignments students are expected to attempt and hopefully complete all assignments and all laboratory exercises it better to submit partially completed assignments than to submit nothing at all a missed assignment will receive a score of zero if you miss an assignment for medical or compassionate reasons contact your instructor as soon as possible missed examinations students who miss an exam should contact the instructor as soon as possible if it is known in advance that an exam will be missed the instructor should be contacted before the exam a student who is absent from a final examination due to medical compassionate or other valid reasons may apply to the college of arts and science undergraduate student office for a deferred exam application must be made within three business days of the missed examination and be accompanied by supporting documents http artsandscience usask ca students help success php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions past the final examination date for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the class which factors in the incomplete coursework as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the student has a passing percentile grade but the instructor has indicated in the course outline that failure to complete the required coursework will result in failure in the course a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised assigned final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed a student can pass a course on the basis of work completed in the course provided that any incomplete course work has not been deemed mandatory by the instructor in the course outline and or by college regulations for achieving a passing grade http policies usask ca policies academic affairs academic courses php for policies governing examinations and grading students are referred to the assessment of students section of the university policy academic courses class delivery examinations and assessment of student learning http policies usask ca policies academic affairs academic courses php academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals section of the university secretary website and avoid any behavior that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of complaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca students academic honesty index php for more information on what academic integrity means for students see the student conduct appeals section of the university secretary website at http www usask ca secretariat student conduct appeals forms integritydefined pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to register with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports students must follow dss policy and procedures for more information check http students usask ca health centres disability services for students php or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss course syllabus cmpt introduction to creative computing note this is a preliminary version of the syllabus some details relating to the course offering e g the number of assignments or the weight of the final exam may change slightly with the final version this information is provided as a convenience for students and should not be taken as the final word as long as this notice appears note this course replaces cmpt catalogue description concepts in computing such as algorithms problem solving and programming are explored using interactive multimedia systems as the creative focus basic skills in problem solving programming design and interaction event based behaviour and prototyping are developed restrictions recommended for students who do not have computer science cmpt can be taken for credit after the completion of cmpt but cmpt cannot be taken for credit after completion of cmpt students with credit for cmpt cmpt cmpt or cmpt cannot obtain credit for cmpt cmpt will count as a open elective for students in interactive systems design computer science and bioinformatics programs who is this course intended for the creative computing cmpt course is designed to be a challenging and fun introduction to computing for students who have little or no background in computer science or programming the course emphasizes building computer programs in a graphical and interactive environment using the python programming language which is a useful tool for everyone cmpt starts with no assumptions about your background in computer science if you never had the opportunity to study programming before this is an excellent place to start you won t be the only one in the class who has never done any programming in fact students who have done programming before have their own starting point cmpt students who finish cmpt can be confident moving on to cmpt in our experience students who learn computer science without any background in programming do as well as or better than students who have some background so if you have taken cmpt you will be perfectly prepared to take cmpt course overview we ve designed this course to be a challenging and fun introduction to computing for students who have little or no background in computer science or programming if you already have or cmpt you should take cmpt you ll start by learning to program using the python language in processing a friendly graphical interactive environment you ll be able to create programs that employ all the computational ideas in applications that emphasize graphics animations and interactive systems as you progress through the course you ll extend your understanding of computation using python you ll begin to see why it is used by scientists scholars software developers engineers you ll be very well prepared to take cmpt the next course which also teaches computer science using python course objectives after completing this course students will be able to design and implement simple python programs from scratch test and debug simple python programs employ conditionals and loops in simple python programs employ variables lists and dictionaries in simple python programs define and call python functions in python programs trace through the execution of simple python programs by hand implement simple numerical algorithms such as computing the average of a list of numbers finding the min max of a list acm learning outcomes this course achieves the following learning outcomes listed alphabetically from the acm computer science curriculum guidelines https www acm org education final report pdf explain the characteristics and defining properties of algorithms and how they relate to machine processing familiarity c p analyze simple problem statements to identify relevant information and select appropriate processing to solve the problem assessment c p discuss the importance of algorithms in the problem solving process familiarity sdf ad create algorithms for solving simple problems usage sdf ad analyze and explain the behavior of simple programs involving the fundamental programming constructs variables expressions assignments i o control constructs functions parameter passing and recursion assessment sdf fpc identify and describe uses of primitive data types familiarity sdf fpc write programs that use primitive data types usage sdf fpc modify and expand short programs that use standard conditional and iterative control structures and functions usage sdf fpc design implement test and debug a program that uses each of the following fundamental programming constructs basic computation simple i o standard conditional and iterative structures the definition of functions and parameter passing usage sdf fpc choose appropriate conditional and iteration constructs for a given programming task assessment sdf fpc write programs that use lists and maps partial sdf fpc implement basic numerical algorithms usage al fdsa student evaluation grading scheme lab exercises at each assignments at each midterm exam final exam total criteria that must be met to pass students must write the final exam a student who does not write the final exam will receive a grade of at most in the course attendance expectation attend every class and participate actively there is no penalty for missed lectures attend all laboratory tutorial sessions these are opportunities to practice the course material with the guidance of a teaching assistant attend the midterm examination final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information recommended texts tba topic schedule algorithms and computational thinking abstraction and encapsulation introduction to processing pythong and visual output functions part calling and defining colour in processing interaction and events the interaction cycle data expressions and variables functions part return values libraries conditional branching repetition nesting constructs and problem solving lists file i o dictionaries advanced problem solving and data management policies recording of lectures lecture videos will be available through a link on the course moodle site late assignments late assignments will only be accepted under exceptional circumstances by the permission of the instructor missed assignments students are expected to attempt and hopefully complete all assignments and all laboratory exercises it better to submit partially completed assignments than to submit nothing at all a missed assignment will receive a score of zero if you miss an assignment for medical or compassionate reasons contact your instructor as soon as possible missed examinations students who miss an exam should contact the instructor as soon as possible if it is known in advance that an exam will be missed the instructor should be contacted before the exam a student who is absent from a final examination due to medical compassionate or other valid reasons may apply to the college of arts and science undergraduate student office for a deferred exam application must be made within three business days of the missed examination and be accompanied by supporting documents http artsandscience usask ca students help success php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions past the final examination date for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the class which factors in the incomplete coursework as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the student has a passing percentile grade but the instructor has indicated in the course outline that failure to complete the required coursework will result in failure in the course a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised assigned final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed a student can pass a course on the basis of work completed in the course provided that any incomplete course work has not been deemed mandatory by the instructor in the course outline and or by college regulations for achieving a passing grade http policies usask ca policies academic affairs academic courses php for policies governing examinations and grading students are referred to the assessment of students section of the university policy academic courses class delivery examinations and assessment of student learning http policies usask ca policies academic affairs academic courses php academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals section of the university secretary website and avoid any behavior that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of complaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca students academic honesty index php for more information on what academic integrity means for students see the student conduct appeals section of the university secretary website at http www usask ca secretariat student conduct appeals forms integritydefined pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to register with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports students must follow dss policy and procedures for more information check http students usask ca health centres disability services for students php or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss course syllabus cmpt introduction to computer science note this is a preliminary version of the syllabus some details relating to the course offering e g the number of assignments or the weight of the final exam may change slightly with the final version this information is provided as a convenience for students and should not be taken as the final word as long as this notice appears note this course replaces cmpt in several degree programs but is not an exact equivalent catalogue description an introduction to computer science and problem solving using procedural programming this course introduces the basic computer science and computer programming principles of algorithms abstraction encapsulation variables conditional branching repetition functions recursion and elementary data structures these concepts are applied to problem solving applications such as data analysis and visualization simulation text processing and image processing the programming skills acquired in this course are applicable in all fields of study the work place and personal projects prerequisite one of computer science cmpt cmpt and one of mathematics or foundations of mathematics or pre calculus or math or math can be taken concurrently note recommended for students with computer science or cmpt or cmpt or for students in programs that require math or equivalent students with credit for cmpt of cmpt cannot take this course for credit students may not take cmpt or for credit concurrently with or after cmpt who is this course intended for this course introduces the basic computer science and computer programming principles these concepts are applied to problem solving applications such as data analysis and visualization simulation text processing and image processing the programming skills acquired in this course are applicable in all fields of study the work place and personal projects cmpt is the starting point for two kinds of students students with or roughly equivalent experience can start with cmpt some students may already have programming experience so we recommend that they start with cmpt instead of cmpt students who are pretty good in math should feel confident starting at cmpt even if they ve not done any computer science before by pretty good at math we mean that you re taking math or equivalent or you already have credit for it important note cmpt won t need any particular math knowledge we re far more interested in the by products of math class good analytical skills logical and deductive reasoning skills attention to detail which are all identified as useful in learning computer science of course you ll need a little math everyone always needs a little math but nothing to be intimidated by course objectives by the completion of this course students will be expected to apply python data structures including tuples lists and dictionaries in programs requiring basic data organization apply basic computer science strategies relating to practical forms of abstraction encapsulation generalization and specialization demonstrate problem solving skills applied to simple simulations data analysis text and image processing design and implement recursive functions in python compare and contrast linear search and binary search in terms of runtime and memory costs compare and contrast insertion sort quick sort and merge sort in terms of runtime and memory costs apply skills in elementary software testing debugging and tracing acm learning outcomes this course achieves the following learning outcomes listed alphabetically from the acm computer science curriculum guidelines https www acm org education final report pdf discuss the importance of algorithms in the problem solving process familiarity sdf ad discuss how a problem may be solved by multiple algorithms each with different properties familiarity sdf ad analyze and explain the behavior of simple programs involving the fundamental programming constructs variables expressions assignments i o control constructs functions parameter passing and recursion assessment sdf fpc identify and describe uses of primitive data types familiarity sdf fpc write programs that use primitive data types usage sdf fpc modify and expand short programs that use standard conditional and iterative control structures and functions usage sdf fpc design implement test and debug a program that uses each of the following fundamental programming constructs basic computation simple i o standard conditional and iterative structures the definition of functions and parameter passing usage sdf fpc choose appropriate conditional and iteration constructs for a given programming task assessment sdf fpc describe the concept of recursion and give examples of its use familiarity sdf fpc identify the base case and the general case of a recursively defined problem assessment sdf fpc discuss the appropriate use of built in data structures familiarity sdf fds write programs that use each of the following data structures arrays records structs strings linked lists stacks queues sets and maps usage sdf fds compare alternative implementations of data structures with respect to performance assessment sdf fds explain what is meant by best and worst case behavior of an algorithm partial al ba implement basic numerical algorithms usage al fdsa implement simple search algorithms and explain the differences in their time complexities assessment al fdsa be able to implement common quadratic and o n log n sorting algorithms partial al fdsa discuss the runtime and memory efficiency of principal algorithms for sorting searching partial al fdsa explain and give examples of the benefits of simulation and modeling in a range of important application areas partial cn ms demonstrate the ability to apply the techniques of modeling and simulation to a range of problem areas partial cn ms analyze simple problem statements to identify relevant information and select appropriate processing to solve the problem assessment cn p explain how simple data is represented in a machine partial cn p course overview this course can be taken as a science credit for arts science majors and is also a required course in computer science major programs and a few other programs lectures will be opportunities to apply the concepts covered in the course discuss them as well as to ask questions and receive guidance we will not waste class time reading powerpoint slides to you short readings will be assigned before each class and you will be expected to be prepared to discuss ask questions and participate laboratory times are listed below these are your opportunities to put into practice the week material under the guidance of a teaching assistant the midterm examination see above for the schedule will have some multiple choice and some programming and short answer questions the final examination is scheduled by the university assignments are weekly to ensure that all the relevant material is put into regular consistent practice even a simple assignment can turn into a time consuming affair if you get stuck on something that blocks your progress working at the last minute is a guaranteed source of stress and burn out start every assignment early to allow yourself time to consult if you run into a problem please make use of the teaching resources instructors office hours tas labs lectures discussion forums etc available to you students who complete cmpt with diligence will be able to apply their basic programming skills to build programs that solve simple computational problems that arise in science and engineering and sometimes in other fields of study requiring data analysis extend their knowledge of python programming language by self study apply their knowledge of computer science to learn other programming languages or scripting environments such as matlab or r a programming language focused on performing statistical analyses continue their formal study of computer science in courses such as cmpt student evaluation grading scheme assignments each lab exercises each midterm exam final exam total the midterm is tentatively scheduled for the week after thanksgiving for all sections in the evening there will be exam seatings tentatively starting at and students in any section may attend any of the seatings though there will be a sign up procedure about a week before the exam locations will be announced on the course moodle webpage http moodle cs usask ca closer to the actual date the midterm will consist of multiple choice questions some short answer questions and some programming questions criteria that must be met to pass students must write the final exam a student who does not write the final exam will receive a grade of at most in the course attendance expectation attend every class and participate actively there will be short reading assignments for all classes see moodle webpage http moodle cs usask ca and students are expected to come to class having completed the readings there is no penalty for missed lectures attend all laboratory tutorial sessions these are opportunities to practice the course material with the guidance of a teaching assistant there is no penalty for missed lab sessions provided that the lab exercises codelab are completed by the due date fridays attend the midterm examination if you have part time work or other responsibilities please try to make arrangements asap that will allow you to write the midterm we will make alternative arrangements for students who cannot attend the evening seatings but obviously we would like to keep the special arrangements to a minimum a missed midterm will be counted as a score of zero excluding exemptions due to health or compassionate grounds final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information required text course readings are provided free of charge and are available on the cmpt course moodle webpage http moodle cs usask ca codelab students are required to purchase access to codelab for cmpt please see codelab setup section for instructions reference and other useful texts dierbach charles introduction to computer science using python a computational problem solving focus wiley isbn codelab codelab turingscraft is a web based interactive programming exercise system for intro programming classes we will be using it for all of your lab exercises you are required to purchase access to codelab to be able to complete all of the lab exercises the procedure for registering for codelab will be posted on the course moodle webpage http moodle cs usask ca note that codelab is an external company that is not affiliated with the university of saskatchewan if you have troubles problems with your codelab account then you must contact codelab to have them resolved your instructors are not able to assist with codelab account problems registration for codelab will cost us this registration is non refundable so only register before sept if you are sure that you will not be withdrawing from cmpt before then if you do not have a credit card with which to purchase access then you can purchase a pre loaded credit card from most grocery stores in the city we suggest purchasing a card if you have to use a prepaid credit card note the balance on a prepaid credit card can be used at most all local stores to partially pay for something for example if you have on a prepaid credit card and buy something for then you can use the card to pay for and then pay the remainder by cash debit etc lecture schedule introduction to python hours variables expressions console i o conditionals loops file i o data structures hours strings tuples lists dictionaries sets arrays functions abstraction and recursion hours defining functions encapsulation abstraction generalization problem vs algorithm recursion software skills hours testing debugging tracing searching and sorting hours linear and binary search insertion sort quick sort merge sort elementary complexity analysis applications and problem solving hours simple simulations data analysis and visualization text processing image processing policies recording of lectures lecture videos will be available through a link on the course moodle site late assignments unless otherwise noted all lab assignments are due fridays at and regular assignments are due wednesdays at because of the weekly nature of assignments late lab exercises or regular assignments cannot be accepted yes that harsh but we have a schedule to keep we may make exceptions but only for emergencies or exceptional circumstances please contact your instructor in such cases missed assignments students are expected to attempt and hopefully complete all assignments and all laboratory exercises it better to submit partially completed assignments than to submit nothing at all a missed assignment will receive a score of zero if you miss an assignment for medical or compassionate reasons contact your instructor as soon as possible missed examinations students who miss an exam should contact the instructor as soon as possible if it is known in advance that an exam will be missed the instructor should be contacted before the exam a student who is absent from a final examination due to medical compassionate or other valid reasons may apply to the college of arts and science undergraduate student office for a deferred exam application must be made within three business days of the missed examination and be accompanied by supporting documents http artsandscience usask ca students help success php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions past the final examination date for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the class which factors in the incomplete coursework as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the student has a passing percentile grade but the instructor has indicated in the course outline that failure to complete the required coursework will result in failure in the course a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised assigned final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed a student can pass a course on the basis of work completed in the course provided that any incomplete course work has not been deemed mandatory by the instructor in the course outline and or by college regulations for achieving a passing grade http policies usask ca policies academic affairs academic courses php for policies governing examinations and grading students are referred to the assessment of students section of the university policy academic courses class delivery examinations and assessment of student learning http policies usask ca policies academic affairs academic courses php academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals section of the university secretary website and avoid any behavior that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of complaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca students academic honesty index php for more information on what academic integrity means for students see the student conduct appeals section of the university secretary website at http www usask ca secretariat student conduct appeals forms integritydefined pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to register with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports students must follow dss policy and procedures for more information check http students usask ca health centres disability services for students php or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss course syllabus cmpt introduction to computer science and programming catalogue description gives the fundamentals of programming including functions procedures and arrays it introduces object oriented programming and gui components also some basic numerical methods and engi neering applications are presented prerequisite mathematics or foundations of mathematics or pre calculus website http moodle cs usask ca learning objectives by the completion of this course students will be expected to read and write simple algorithms using pseudo code and flowcharts design and implement simple c programs from scratch test and debug simple c programs translate a simple pseudo code or c program into a flowchart employ conditionals and loops in simple c programs employ variables arrays and records in simple c programs define and call c functions in c programs design and implement simple recursive functions in c trace through the execution of simple c programs by hand compare and contrast linear search and binary search in terms of runtime and memory costs compare and contrast bubble sort insertion sort and selection sort in terms of runtime and memory costs student evaluation grading scheme assignments each midterm exam oct lab exercises each final exam date tba total the midterm is scheduled for thursday october in the evening the location will be announced on the course moodle webpage http moodle cs usask ca closer to the actual date the midterm will consist of multiple choice questions some short answer questions and some programming questions criteria that must be met to pass students must write the final exam a student who does not write the final exam will receive a grade of at most in the course attendance expectations attend every class and participate actively there will be short reading assignments for all classes see moodle webpage http moodle cs usask ca and students are expected to come to class having com pleted the readings there is no penalty for missed lectures attend all laboratory tutorial sessions these are opportunities to practice the course material with the guidance of a teaching assistant there is no penalty for missed lab sessions provided that the lab exercises codelab are completed by the due date fridays attend the midterm examination if you have part time work or other responsibilities please try to make arrangements asap that will allow you to write the midterm we will make alternative arrangements for students who cannot attend the evening seatings but obviously we would like to keep the special arrangements to a minimum a missed midterm will be counted as a score of zero excluding exemptions due to health or compassionate grounds note all students must be properly registered in order to attend lectures and receive credit for this course final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted course overview this course is required for certain engineering majors it is equivalent in credit to cmpt lectures will be opportunities to apply the concepts covered in the course discuss them as well as to ask questions and receive guidance we will not waste class time reading powerpoint slides to you short readings will be assigned before each class and you will be expected to be prepared to discuss ask questions and participate these readings are collected into a book like document available on the moodle webpage http moodle cs usask ca laboratory times are listed below these are your opportunities to put into practice the week material using a computer under the guidance of a teaching assistant we will have two midterm examinations see above for the schedule the final examination is scheduled by the university and the exam schedule is usually released in october assignments are weekly to ensure that all the relevant material is put into regular consistent practice some early assignments will seem easy and later assignments will definitely challenge you even a simple assignment can turn into a time consuming affair if you get stuck on something that blocks your progress working at the last minute is a guaranteed source of stress and burn out to manage your workload you must learn effective time management start every assignment early to allow yourself time to consult if you run into a problem students who complete cmpt with diligence will be able to apply their basic programming skills to build applications for practical situations extend their knowledge of the c programming language by self study build on their knowledge of computer science to learn the basics of any other computer programming language continue their formal study of computer science in courses such as cmpt and cmpt please make use of the teaching resources instructors office hours tas labs lectures discussion forums etc available to you textbook information required texts and resources there is no required textbook for the course codelab students are required to purchase access to codelab for cmpt please see codelab setup section for instructions course readings are provided free of charge and are available on the cmpt course moodle webpage http moodle cs usask ca recommended texts cay horstmann c for everyone edition wiley isbn 92713 lecture schedule the following schedule is approximate other topics may be added if time allows topic details introduction administrative details course overview what is computer science algorithms what is an algorithm pseudo code and flowcharts encapsulation and abstraction atomic data variables and types compound data c fundamentals variables console io expressions conditional branching functions while loops one dimensional arrays for loops and do while loops multi dimensional arrays records and record types recursion a detailed course schedule is available on the course moodle webpage http moodle cs usask ca laboratory sections laboratory sessions begin the week of september see the detailed course schedule which can be found on the course moodle webpage http moodle cs usask ca the lab sessions will be held in the teaching labs located on the floor of the spinks addition of the thorvaldson building section day time room tue 50am thorv tue 20pm thorv thu 50pm thorv thu 50am thorv tue 50pm thorv the laboratory sessions will be guided by teaching assistants the contact information for the teaching assistants will be made available on course moodle webpage http moodle cs usask ca a large open access lab on the floor of spinks is available for student use outside of lab time many tas and instructors for several cmpt courses will hold office hours in the open lab don t be shy if you see an instructor or ta who is not your ta or instructor in the lab don t hesitate to call them over to help you codelab codelab turingscraft is a web based interactive programming exercise system for intro programming classes we will be using it for all of your lab exercises you are required to purchase access to codelab to be able to complete all of the lab exercises the procedure for registering for codelab will be posted on the course moodle webpage http moodle cs usask ca note that codelab is an external company that is not affiliated with the university of saskatchewan if you have troubles problems with your codelab account then you must contact codelab to have them resolved your instructors are not able to assist with codelab account problems registration for codelab will cost us this registration is non refundable so only register before sept if you are sure that you will not be withdrawing from cmpt before then if you do not have a credit card with which to purchase access then you can purchase a pre loaded credit card from most grocery stores in the city we suggest purchasing a card if you have to use a prepaid credit card note the balance on a prepaid credit card can be used at most all local stores to partially pay for something for example if you have on a prepaid credit card and buy something for then you can use the card to pay for and then pay the remainder by cash debit etc policies late assignments unless otherwise noted all lab assignments are due fridays at and regular assignments are due tuesdays at because of the weekly nature of assignments late lab exercises or regular assignments cannot be accepted yes that harsh but we have a schedule to keep we may make exceptions but only for emergencies or exceptional circumstances please contact your instructor in such cases missed assignments students are expected to attempt and hopefully complete all assignments and all laboratory exercises it better to submit partially completed assignments than to submit nothing at all a missed assignment will receive a score of zero if you miss an assignment for medical or compassionate reasons contact your instructor as soon as possible missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the application must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses http www arts usask ca students transition tips php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the course outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations section of the calendar university of saskatchewan calendar academic courses policy academic honesty the university policy the university of saskatchewan is committed to the highest standards of academic integrity and honesty stu dents are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals section of the university secretary website and avoid any behaviour that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of com plaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca undergrad honesty php for more information on what academic integrity means for students see the student conduct appeals section of the university secretary website at http www usask ca secretariat student conduct appeals forms integritydefined pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to regis ter with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports stu dents must follow dss policy and procedures for more information check http www students usask ca disability or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss version history initial version one midterm tentatively scheduled course syllabus cmpt computing using excel and vba course description an introduction to the fundamentals of programming using the visual basic for applications vba language in excel an emphasis is placed on learning many important concepts used to create useful com puter programs in excel examples of some concepts include arrays procedures and functions this course is mostly intended for engineering students and therefore some introductory numerical methods and engineer ing applications are presented some graphical user interface gui design concepts are also introduced course learning outcomes by the end of this course students should be able to complete the following tasks students will complete a variety of tasks using microsoft excel given a problem students will be able to create a set of instructions in plain language that when followed will result in a correct solution to the problem students will design computer programs using vba in excel to execute a set of instructions to solve a given problem given a sample of vba code students will be able to correctly anticipate the outcome of the code or identify incorrect vba syntax student evaluation grading scheme equally weighted assignments midterm exam february in class final exam total criteria that must be met to pass students must write the final exam in order to pass the course if a student does not write the final exam the student will receive a total grade of at most attendance expectation attend every class and be prepared to participate there is no penalty for poor attendance however attendance is an important factor in successfully completing the course students will be expected to understand concepts introduced and discussed in class students will also be asked to participate in class activities using many of the course concepts these activities are designed to help improve the understanding of important course material attend all tutorial sessions these sessions provide students an opportunity to practice concepts learnt in class during the session tutorial instructors will review course concepts and answer student ques tions students will also be given an opportunity to work on tutorial exercises and assignments due to time constraints it is possible that some concepts will be introduced in tutorial sessions and not during lectures material introduced in tutorial sessions may be the subject of assignment questions or tested on either the mid term or final examination final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information required text j walkenbach micorsoft excel power programming with vba isbn lecture schedule week topics jan jan course outline and introduction overview of excel excel functions and excel macros excel file types jan jan elements of pseudo code and flowcharts solving problems with pseudo code and flowcharts jan jan vba editor components of a vba program using objects in vba variables and expressions input output jan jan string operations conditional statements if then else elseif loops feb feb arrays matrices multi dimensional arrays testing and debugging feb feb review if time permits midterm on february in class feb feb reading week feb feb sub procedures functions variable scope feb mar recursion user defined types in vba mar mar file input and output sort methods search methods mar mar some numerical methods in excel mar mar plots and charts of data mar apr user forms in excel if time permits active x controls in excel if time permits apr apr review for final exam if time permits course overview students who complete this course will be able to apply their basic programming skills to build applications for practical situations build on the knowledge of vba to learn the basics of any other computer programming language policies late assignments late assignments will not be accepted students should ensure all assignments are submitted by the due date posted for the assignments missed assignments assignment extensions will only be granted for medical reasons or exceptional circumstances students should be prepared to show evidence for reason given students should contact the instructor for an extension before the assignment due date requests for extensions after the assignment due date will not be considered missed examinations students who miss an exam should contact the instructor as soon as possible if it is known in advance that an exam will be missed the instructor should be contacted before the exam a student who is absent from a final examination due to medical compassionate or other valid rea sons may apply to the college of arts and science undergraduate student office for a deferred exam application must be made within three business days of the missed examination and be accompanied by supporting documents deferred exams are written during the february mid term break for term courses and in early june for term and full year courses http artsandscience usask ca undergraduate advising strategies php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions past the final examination date for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the class which factors in the incomplete coursework as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the student has a passing percentile grade but the instructor has indicated in the course outline that failure to complete the required coursework will result in failure in the course a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised assigned final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed a student can pass a course on the basis of work completed in the course provided that any incom plete course work has not been deemed mandatory by the instructor in the course outline and or by col lege regulations for achieving a passing grade http policies usask ca policies academic affairs academic courses php for policies governing examinations and grading students are referred to the assessment of students section of the university policy academic courses class delivery examinations and assessment of student learning http policies usask ca policies academic affairs academic courses php academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals subsection of the university secretary website and avoid any behaviour that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca honesty studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of com plaints and appeals http www usask ca honesty studentnon pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca students academic honesty index php for more information on what academic integrity means for students see the student conduct appeals subsection of the university secretary website at http www usask ca pdf pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to register with disability services for students dss if they have not already done so students who sus pect they may have disabilities should contact dss for advice and referrals in order to access dss pro grams and supports students must follow dss policy and procedures for more information check http www students usask ca disability or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss course syllabus cmpt introduction to computer science and programming catalogue description introduces basic concepts of computer science through the study of traditional elementary pro gramming object oriented programming debugging design of objects and standard algorithms with their analysis prerequisite mathematics or foundations of mathematics or pre calculus website http moodle cs usask ca learning objectives by the completion of this course students will be expected to read and write simple algorithms using pseudo code and flowcharts design and implement simple c programs from scratch test and debug simple c programs translate a simple pseudo code or c program into a flowchart employ conditionals and loops in simple c programs employ variables arrays and records in simple c programs define and call c functions in c programs design and implement simple recursive functions in c trace through the execution of simple c programs by hand compare and contrast linear search and binary search in terms of runtime and memory costs compare and contrast bubble sort insertion sort and selection sort in terms of runtime and memory costs student evaluation grading scheme assignments each midterm exam oct lab exercises each final exam date tba total the midterm is tentatively scheduled for thursday october the week of october the week after thanksgiving for all sections in the evening there will be exam seatings tentatively starting at and students in any section may attend any of the seatings though there will be a sign up procedure about a week before the exam locations will be announced on the course moodle webpage http moodle cs usask ca closer to the actual date the midterm will consist of multiple choice questions some short answer questions and some programming questions criteria that must be met to pass students must write the final exam a student who does not write the final exam will receive a grade of at most in the course attendance expectations attend every class and participate actively there will be short reading assignments for all classes see moodle webpage http moodle cs usask ca and students are expected to come to class having com pleted the readings there is no penalty for missed lectures attend all laboratory tutorial sessions these are opportunities to practice the course material with the guidance of a teaching assistant there is no penalty for missed lab sessions provided that the lab exercises codelab are completed by the due date fridays attend the midterm examination if you have part time work or other responsibilities please try to make arrangements asap that will allow you to write the midterm we will make alternative arrangements for students who cannot attend the evening seatings but obviously we would like to keep the special arrangements to a minimum a missed midterm will be counted as a score of zero excluding exemptions due to health or compassionate grounds note all students must be properly registered in order to attend lectures and receive credit for this course final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted course overview this course can be taken as a science credit for arts science majors and is also a required course in computer science major programs and a few other programs lectures will be opportunities to apply the concepts covered in the course discuss them as well as to ask questions and receive guidance we will not waste class time reading powerpoint slides to you short readings will be assigned before each class and you will be expected to be prepared to discuss ask questions and participate these readings are collected into a book like document available on the moodle webpage http moodle cs usask ca laboratory times are listed below these are your opportunities to put into practice the week material using a computer under the guidance of a teaching assistant we will have two midterm examinations see above for the schedule the final examination is scheduled by the university and the exam schedule is usually released in october assignments are weekly to ensure that all the relevant material is put into regular consistent practice some early assignments will seem easy and later assignments will definitely challenge you even a simple assignment can turn into a time consuming affair if you get stuck on something that blocks your progress working at the last minute is a guaranteed source of stress and burn out to manage your workload you must learn effective time management start every assignment early to allow yourself time to consult if you run into a problem students who complete cmpt with diligence will be able to apply their basic programming skills to build applications for practical situations extend their knowledge of the c programming language by self study build on their knowledge of computer science to learn the basics of any other computer programming language continue their formal study of computer science in courses such as cmpt and cmpt please make use of the teaching resources instructors office hours tas labs lectures discussion forums etc available to you textbook information required texts and resources there is no required textbook for the course codelab students are required to purchase access to codelab for cmpt please see codelab setup section for instructions course readings are provided free of charge and are available on the cmpt course moodle webpage http moodle cs usask ca recommended texts cay horstmann c for everyone edition wiley isbn 92713 lecture schedule the following schedule is approximate other topics may be added if time allows topic details introduction administrative details course overview what is computer science algorithms what is an algorithm pseudo code and flowcharts encapsulation and abstraction atomic data variables and types compound data c fundamentals variables console io expressions conditional branching functions while loops one dimensional arrays for loops and do while loops multi dimensional arrays records and record types recursion searching and sorting linear search binary search insertion sort selection sort bubble sort computational complexity a detailed course schedule is available on the course moodle webpage http moodle cs usask ca laboratory sections laboratory sessions begin the week of september see the detailed course schedule which can be found on the course moodle webpage http moodle cs usask ca the lab sessions will be held in the teaching labs located on the floor of the spinks addition of the thorvaldson building section day time room fri 50pm thorv mon 20pm thorv mon 50pm thorv mon 50pm thorv wed 50pm thorv wed 50pm thorv thu 20pm thorv thu 50pm thorv tue 50pm thorv thu 50pm thorv mon 50pm thorv the laboratory sessions will be guided by teaching assistants the contact information for the teaching assistants will be made available on course moodle webpage http moodle cs usask ca a large open access lab on the floor of spinks is available for student use outside of lab time many tas and instructors for several cmpt courses will hold office hours in the open lab don t be shy if you see an instructor or ta who is not your ta or instructor in the lab don t hesitate to call them over to help you codelab codelab turingscraft is a web based interactive programming exercise system for intro programming classes we will be using it for all of your lab exercises you are required to purchase access to codelab to be able to complete all of the lab exercises the procedure for registering for codelab will be posted on the course moodle webpage http moodle cs usask ca note that codelab is an external company that is not affiliated with the university of saskatchewan if you have troubles problems with your codelab account then you must contact codelab to have them resolved your instructors are not able to assist with codelab account problems registration for codelab will cost us this registration is non refundable so only register before sept if you are sure that you will not be withdrawing from cmpt before then if you do not have a credit card with which to purchase access then you can purchase a pre loaded credit card from most grocery stores in the city we suggest purchasing a card if you have to use a prepaid credit card note the balance on a prepaid credit card can be used at most all local stores to partially pay for something for example if you have on a prepaid credit card and buy something for then you can use the card to pay for and then pay the remainder by cash debit etc policies late assignments unless otherwise noted all lab assignments are due fridays at and regular assignments are due tuesdays at because of the weekly nature of assignments late lab exercises or regular assignments cannot be accepted yes that harsh but we have a schedule to keep we may make exceptions but only for emergencies or exceptional circumstances please contact your instructor in such cases missed assignments students are expected to attempt and hopefully complete all assignments and all laboratory exercises it better to submit partially completed assignments than to submit nothing at all a missed assignment will receive a score of zero if you miss an assignment for medical or compassionate reasons contact your instructor as soon as possible missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the application must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses http www arts usask ca students transition tips php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the course outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations section of the calendar university of saskatchewan calendar academic courses policy academic honesty the university policy the university of saskatchewan is committed to the highest standards of academic integrity and honesty stu dents are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals section of the university secretary website and avoid any behaviour that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of com plaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca undergrad honesty php for more information on what academic integrity means for students see the student conduct appeals section of the university secretary website at http www usask ca secretariat student conduct appeals forms integritydefined pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to regis ter with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports stu dents must follow dss policy and procedures for more information check http www students usask ca disability or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss version history initial version one midterm tentatively scheduled midterm scheduled course description this course focuses on the mechanics of computer games and students will learn about the design of games how to critically evaluate games and about strategies for playtesting games design topics include consideration of narrative game rules collaboration and animation cmpt can be taken as a science course by non science students cmpt cannot be taken as a science course by science students course overview and learning objectives please see the course schedule for a list of topics and lecture dates the following topics may be covered but are subject to change critical thinking for games formal elements players objectives rules procedures resources conflicts boundaries and outcomes dramatic elements challenge play premise character story world building and the dramatic arc low fidelity prototyping storyboarding digital prototyping unity playtesting techniques effective interface design in games advance topics functionality completeness game balance serious games gamification and casual gaming resources readings textbooks there is one optional textbooks for this course partial lecture notes and slides will be provided online via the course website however lecture notes are not a substitute for attending class class time will be used for content presentation examples case studies design exercises and group interaction the visual nature of the course content combined with the interactive nature of the content presentation means that class attendance is essential to success in this course optional textbook tracy fullerton game design workshop a playcentric approach to creating innovative games third edition crc press isbn website the course website is on moodle and can be accessed from http www cs usask ca courses course announcements regarding assignments and examinations as well as other information may and will be communicated to the class via this website the student is responsible for reading this website regularly grading scheme assignments project midterm exam final exam total evaluation components assignments value of final grade assignments will be used as tool to reinforce concepts learned in class all assignments must be completed individually unless otherwise stated due dates there will be a number of small assignments over the course assigned as the course material dictates students will be given a week to complete any assignment that is expected to take more than an hour to complete smaller assignments may be due the following class submission submission instructions for assignments will be given in the descriptions of individual assignments late assignment policy and extensions absolutely no late assignments will be accepted absolutely no extensions will be provided for assignment due dates at the end of the course the bottom mark for assignments will be dropped when calculating the average assignment grade project value of final grade this course requires completion of a single team project which has several marked deliverables throughout the term the goal is to provide students with practical experience in designing implementing and playtesting games students will participate in the same team throughout the course there are six stages to the team project each with a milestone and deliverable more detail for each component will become available in time please consult the course schedule early and often for timing of components grading scheme each project component will be graded and given a weight of the total project grade of grade proposal pitch paper prototype playtesting i digital prototype playtesting ii class presentation team peer evaluation group work is beneficial for the learning experience but has the drawback that some members of a group may not carry their weight in terms of group participation to mitigate this factor students will perform peer evaluations of their project group members these evaluations will be used to scale the project grade submission submission instructions for projects will be given in the descriptions of individual project components late project policy and extensions absolutely no late project components will be accepted for grading as the project components build upon each other feedback will be provided on late projects but the grade for the late component will be zero absolutely no extensions will be given for project components midterm exam value of final grade date thursday february tentative length hour a student who misses the midterm test due to illness must contact their instructor by email on the day of the missed test explaining the reason for their absence the student must subsequently provide appropriate medical documentation to the course instructor at which time the instructor and the student shall discuss how the missed exam will be made up a student who cannot attend a midterm test for religious reasons or due to a conflict with another class or examination must inform the instructor at least two weeks prior to the test date so that alternative arrangements can be made final exam value of final grade date to be determined length to be determined a student who misses the final examination for any reason has a conflict with another final examination or cannot attend the final examination for religious reasons must follow the appropriate procedures outlined in the university of saskatchewan calendar important regulations all students must be properly registered in order to attend lectures and receive credit for this course failure to write the final exam will result in failure of this course to obtain a passing grade in this course the weighted average of the student midterm test and final exam grades must be at least to be eligible to write the final examination the student must have a standing of at least in all other course work weighted average of all assignments and midterm integrity defined from the office of the university secretary the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals section of the university secretary website and avoid any behavior that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of complaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf for more information on what academic integrity means for students see the student conduct appeals section of the university secretary website at http www usask ca secretariat student conduct appeals forms integritydefined pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to register with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports students must follow dss policy and procedures for more information check http students usask ca health centres disability services for students php or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss course syllabus cmpt introduction to computing and interactive systems design description introduction to ideas and concepts in computer science and the design of interactive systems concepts in computing such as algorithms problem solving and programming are explored using interactive multimedia systems as the focus basic concepts in design and interaction such as the interaction cycle event based behaviour and prototyping are introduced restrictions cmpt can be taken as a science course by non science majors science majors may not receive science credit for this course cmpt can be taken for credit after the completion of cmpt or but cmpt and cannot be taken for credit after completion of cmpt cmpt cannot be taken for credit after cmpt class time location course objectives learn how to design build and talk about interactive systems student evaluation grading scheme assignments class project midterm exam mid oct final exam dec participation total final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course recommended textbook learn to program with scratch a visual introduction to programming with games art science and math by majed marji february pp isbn topics sprites motion and repetition conditional and arithmetic expressions interaction and events variables and lists messages subprograms design process animation problem solving personas scenarios and storyboards processing images and colour sound structure and navigation recursion policies missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science undergraduate student office the application must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses http www arts usask ca students transition tips php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the course outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations section of the calendar university of saskatchewan calendar academic courses policy academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals section of the university secretary website and avoid any behavior that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca honesty studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of complaints and appeals http www usask ca honesty studentnon pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca undergrad honesty php for more information on what academic integrity means for students see the student conduct appeals section of the university secretary website at http www usask ca pdf pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to register with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports students must follow dss policy and procedures for more information check http www students usask ca disability or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss course syllabus cmpt introduction to computing catalogue description a survey of major computer science areas combining a breadth of topics with depth via specific examples within each topic topics include history of computing computer applications analysis and design high level programming computer software computer hardware artificial intelligence and the social impact of computers prerequisite mathematics or or or foundations of mathematics or pre calculus note after cmpt students can take any of and students can receive credit for only one of cmpt cmpt cmpt cmpt students may not take cmpt for credit after taking cmpt also students may not take cmpt for credit concurrent with or following cmpt or cmpt students wishing to major in computer science are advised to take cmpt in addition students ma joring in computer science may not use cmpt as a course in their major but may count it as a junior elective as long as cmpt is taken before cmpt or cmpt class time and location lecture am am mwf arts tutorials am pm wednesday 11 am pm tuesday am am friday all in thor valdson building website moodle course objectives provide an introduction to the science and impact of computing student evaluation grading scheme assignments through each final project assignment midterm exam final exam total criteria that must be met to pass students should receive a cumulative passing mark to pass the class tentative schedule of midterm and assignment submissions assignment due 25 assignment due 9 assignment due 10 midterm exam 11 assignment due 11 assignment due note midterm and final cumulative marks will be scaled attendance expectation students will be expected to know all information passed on during lectures and tutorials and through the webpage moodle and email if they miss a lecture or tutorial they are responsible for acquiring material covered in the session some topics covered in class may not have corresponding notes associated with them students are expected to make their own notes in these cases students who miss a class in which such topics are covered are responsible for obtaining notes from other students final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information required text none recommended texts none lecture schedule tentative schedule week of topic sept introduction hardware and software systems sept hardware and software systems html and hci sept html and hci networking and the internet sept networking and the internet sept programming in scratch oct problem solving in scratch advanced scratch oct computer and information security oct software engineering oct new media nov e commerce nov artificial intelligence nov social and technical implications of ict dec conclusion additional topics may be added and some topics particularly related to the history of computer science may be removed based on student interest topics for possible addition include basics of binary arithmetic algorithms for searching and sorting computational complexity and concurrency there will be sessions of tutorials tentatively scheduled for the weeks of sept sept sept sept oct oct and nov 3 course overview is an introductory computer science course designed to provide a broad overview of computer science and a foundation for lifelong use of and learning about computers this course is intended for students majoring in areas other than computer science students who successfully complete and desire further experience in computing may consider taking and leading into the interactive system design basc degree or cmpt and leading to the computer science bsc degree you can also follow up with if you are interested in business applications check out http www cs usask ca undergrad programs index php for more information on these programs there are a number of other first year courses that can be taken by students who want an alternative to for students interested in the interactive system design basc cmpt for students wishing to take the computer science bsc programs cmpt for engineering students interested in com puting using vba in excel cmpt for engineering students considering a double degree option with computer science for students interested in digital document processing and cmpt a course similar to for business students if you take you cannot also get credit for or policies late assignments assignments must be turned in at the times and dates and locations they are due unless you have received permission in advance for an extension missed assignments students should submit early versions of their assignments frequently to avoid the possibility of missing a deadline students will receive a zero for assignments missed without prior permission from the instructor if there is a compelling reason why seeking prior permission is not possible the student should contact the instructor at the earliest opportunity for consideration of alternative arrangements the instructor will judge whether the reason is compelling missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the appli cation must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses http www arts usask ca students transition tips php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the cours e outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regu lations on examinations subsection of the calendar university of saskatchewan calendar academic courses policy academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals subsection of the university secretary website and avoid any behaviour that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of com plaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca undergrad honesty php for more information on what academic integrity means for students see the student conduct appeals subsection of the university secretary website at http www usask ca secretariat student conduct appeals forms integritydefined pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to regis ter with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports stu dents must follow dss policy and procedures for more information check http www students usask ca disability or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss the if statement the if statement is used to implement a decision when a condition is fulfilled one set of statements is executed otherwise another set of statements is executed see syntax here is an example using the if statement in many countries the number is considered unlucky rather than offending superstitious ten ants building owners sometimes skip the thirteenth floor floor is immediately followed by floor of course floor is not usually left empty or as some conspiracy theorists believe filled with secret offices and research labs it is simply called floor the computer that controls the building eleva tors needs to compensate for this foible and adjust all floor numbers above let simulate this process in c we will ask the user to type in the desired floor number and then compute the actual floor when the input is above then we need to decrement the input to obtain the actual floor for example if the user provides an input of the program determines the actual floor as otherwise we simply use the supplied floor number int if floor this elevator panel skips the thirteenth floor the floor is not actually missing the computer that controls the elevator adjusts the floor numbers above floor else floor the flowchart in figure shows the branching behavior in our example each branch of the if statement contains a single statement you can include as many statements in each branch as you like sometimes it happens that figure flowchart for if statement figure flowchart for if statement with no else branch there is nothing to do in the else branch of the statement in that case you can omit it entirely such as in this example int floor if floor no else needed see figure for the flowchart the following program puts the if statement to work this program asks for the desired floor and then prints out the actual floor an if statement is like a fork in the road depending upon a decision different parts of the program are executed cpp program run syntax if statement in some asian countries the number is considered unlucky some building owners play it safe and skip both the thirteenth and the fourteenth floor how would you modify the sample program to handle such a building consider the following if statement to compute a discounted price if else what is the discounted price if the original price is compare this if statement with the one in self check if else do the two statements always compute the same value if not when do the values differ consider the following statements to compute a discounted price if what is the discounted price if the original price is the variables and hold the actual amount of fuel and the size of the fuel tank of a vehicle if less than percent is remaining in the tank a status light should show a red color otherwise it shows a green color simulate this process by printing out either red or green practice it now you can try these exercises at the end of the chapter common error a semicolon after the if condition the following code fragment has an unfortunate error if floor error floor there should be no semicolon after the if condition the compiler interprets this statement as follows if floor is greater than execute the statement that is denoted by a single semicolon that is the do nothing statement the statement enclosed in braces is no longer a part of the if statement it is always executed even if the value of floor is not above it is decremented placing a semicolon after the else reserved word is also wrong if floor floor else floor in this case the do nothing statement is executed if floor is not fulfilled this is the end of the if statement the next statement enclosed in braces is executed in both cases that is is always set to floor programming tip tabs block structured code has the property that nested statements are indented by one or more levels int main int floor if floor floor return indentation level how do you move the cursor from the leftmost column to the appropriate indentation level a perfectly reasonable strategy is to hit the space bar a sufficient number of times however many pro grammers use the tab key instead a tab moves the cursor to the next indentation level you use the tab key to move the cursor to the next indentation level while the tab key is nice some editors use tab characters for alignment which is not so nice tab characters can lead to problems when you send your file to another person or a printer there is no universal agreement on the width of a tab character and some software will ignore tabs altogether it is therefore best to save your files with spaces instead of tabs most editors have a setting to automatically convert all tabs to spaces look at the documentation of your development environment to find out how to activate this useful setting programming tip avoid duplication in branches look to see whether you duplicate code in each branch if so move it out of the if statement here is an example of such duplication if floor floor cout actual floor endl else floor cout actual floor endl the output statement is exactly the same in both branches this is not an error the program will run correctly however you can simplify the program by moving the duplicated state ment like this if floor floor else floor cout actual floor endl removing duplication is particularly important when programs are maintained for a long time when there are two sets of statements with the same effect it can easily happen that a programmer modifies one set but not the other comparing numbers and strings every if statement contains a condi tion in many cases the condition involves comparing two values for example in the previous examples we tested floor the comparison is called a relational operator c has six relational operators see table as you can see only two c rela tional operators and look as you would expect from the mathematical notation computer keyboards do not have keys for or but the and operators are easy to remem ber because they look similar the operator is initially confusing to most newcomers to c in c already has a meaning namely assignment in c you use a relational operator to check whether one value is greater than another c math notation description greater than greater than or equal less than less than or equal equal not equal the operator denotes equality testing floor assign to floor if floor test whether floor equals you must remember to use inside tests and to use outside tests see common error on page for more information you can compare strings as well if input quit use to check whether two strings are different in c letter case matters for example quit and quit are not the same string syntax comparisons expression value comment true is less than tests for less than or equal error the less than or equal operator is not the less than symbol comes first false is the opposite of false the left hand side must be strictly smaller than the right hand side true both sides are equal tests for less than or equal true tests for equality true tests for inequality it is true that is not error use to test for equality false although the values are very close to one another they are not exactly equal see common error on page error you cannot compare a string to a number table summarizes how to use relational operators in c which of the following conditions are true provided a is and b is a a b b a b c a b give the opposite of the condition floor what is the error in this statement if cout tie endl supply a condition in this if statement to test whether the user entered a y string input cout enter y to quit endl cin input if cout goodbye endl return how do you test that a string str is not the empty string practice it now you can try these exercises at the end of the chapter common error confusing and the rule for the correct usage of and is very simple in tests always use and never use if it is so simple why can t the compiler be helpful and flag any errors actually the c language allows the use of inside tests to understand this we have to go back in time the creators of c the predecessor to c were very frugal they did not want to have special values true and false instead they allowed any numeric value inside a condition with the convention that denotes false and any non value denotes true fur thermore in c and c assignments have values for example the value of the assignment expression floor is these two features namely that numbers can be used as truth values and that assignments are expressions with values conspire to make a horrible pitfall the test if floor error is legal c but it does not test whether floor and are equal instead the code sets floor to and since that value is not zero the condition of the if statement is always fulfilled fortunately most compilers issue a warning when they encounter such a statement you should take such warnings seriously see programming tip for more advice about com piler warnings some shell shocked programmers are so nervous about using that they use even when they want to make an assignment floor floor error this statement tests whether floor equals floor it doesn t do anything with the outcome of the test but that is not an error some compilers will warn that the code has no effect but others will quietly accept the code common error exact comparison of floating point numbers floating point numbers have only a limited precision and calculations can introduce roundoff errors you must take these inevitable roundoffs into account when comparing floating point numbers for example the following code multiplies the square root of by itself ideally we expect to get the answer double r sqrt if r r cout sqrt squared is endl else take limited precision into account when comparing floating point numbers cout sqrt squared is not but setprecision r r endl this program displays sqrt squared is not but it does not make sense in most circumstances to compare floating point numbers exactly instead we should test whether they are close enough that is the magnitude of their differ ence should be less than some threshold mathematically we would write that x and y are close enough if x y for a very small number is the greek letter epsilon a letter used to denote a very small quantity it is common to set to when comparing double numbers const double epsilon double r sqrt if fabs r r epsilon cout sqrt squared is approximately include the cmath header to use the fabs function c a r c a r t c a t step decide upon the branching condition in our sample problem the obvious choice for the condition is original price that is just fine and we will use that condition in our solution but you could equally well come up with a cor rect solution if you choose the opposite condition is the original price at least you might choose this condition if you put yourself into the position of a shopper who wants to know when the bigger discount applies step give pseudocode for the work that needs to be done when the condition is true sales discounts are often higher for expensive products use the if state ment to implement such a decision in this step you list the action or actions that are taken in the positive branch the details depend on your problem you may want to print a message compute values or even exit the program in our example we need to apply an percent discount discounted price x original price step give pseudocode for the work if any that needs to be done when the condition is not true what do you want to do in the case that the condition of step is not fulfilled sometimes you want to do nothing at all in that case use an if statement without an else branch in our example the condition tested whether the price was less than if that condition is not true the price is at least so the higher discount of percent applies to the sale discounted price x original price step double check relational operators first be sure that the test goes in the right direction it is a common error to confuse and next consider whether you should use the operator or its close cousin the operator what should happen if the original price is exactly reading the problem carefully we find that the lower discount applies if the original price is less than and the higher dis count applies when it is at least a price of should therefore not fulfill our condition and we must use not step remove duplication check which actions are common to both branches and move them outside see program ming tip on page in our example we have two statements of the form discounted price x original price they only differ in the discount rate it is best to just set the rate in the branches and to do the computation afterwards if original price discount rate else discount rate discounted price discount rate x original price step test both branches formulate two test cases one that fulfills the condition of the if statement and one that does not ask yourself what should happen in each case then follow the pseudocode and act each of them out in our example let us consider two scenarios for the original price and we expect that the first price is discounted by the second by when the original price is then the condition is true and we get discount rate discounted price x when the original price is then the condition is false and discount rate discounted price x in both cases we get the expected answer step assemble the if statement in c type the skeleton if else and fill it in as shown in syntax on page omit the else branch if it is not needed in our example the completed statement is if 92 else original_price random fact the denver airport luggage handling system making decisions is an essential part of any computer program nowhere is this more obvious than in a computer system that helps sort luggage at an airport after scanning the luggage identification codes the system sorts the items and routes them to different conveyor belts human operators then place the items onto trucks when the city of denver built a huge airport to replace an outdated and congested facility the luggage system contractor went a step further the new system was designed to replace the human operators with robotic carts unfortu nately the system plainly did not work it was plagued by mechanical prob lems such as luggage falling onto the tracks and jamming carts equally frus trating were the software glitches carts would uselessly accumulate at some locations when they were needed elsewhere the airport had been scheduled to open in but without a func tioning luggage system the opening was delayed for over a year while the contractor tried to fix the problems the contractor never succeeded and ultimately a manual system was installed the delay cost the city and airlines close to a billion dollars and the contractor once the leading lug gage systems vendor in the united states went bankrupt clearly it is very risky to build a large system based on a technology that has never been tried on a smaller scale as robots and the software that controls them get better over time they will take on a larger share of lug gage handling in the future but it is likely that this will happen in an incre mental fashion the denver airport originally had a fully automatic system for moving lug gage replacing human operators with robotic carts unfortunately the sys tem never worked and was dismantled before the airport was opened available online at www wiley com college horstmann multiple alternatives in section you saw how to program a two way branch with an if statement in many situations there are more than two cases in this section you will see how to implement a decision with multiple alternatives for example consider a program that displays the effect of an earthquake as measured by the richter scale see table the loma prieta earthquake that damaged the bay bridge in san fran cisco and destroyed many buildings measured on the richter scale value effect most structures fall many buildings destroyed many buildings considerably damaged some collapse damage to poorly constructed buildings the richter scale is a measurement of the strength of an earthquake every step in the scale for example from to signifies a tenfold increase in the strength of the quake in this case there are five branches one each for the four descriptions of damage and one for no destruction figure shows the flowchart for this multiple branch statement you use multiple if statements to implement multiple alternatives like this if richter cout most structures fall else if richter cout many buildings destroyed else if richter cout many buildings considerably damaged some collapse else if richter cout damage to poorly constructed buildings else cout no destruction of buildings as soon as one of the four tests succeeds the effect is displayed and no further tests are attempted if none of the four cases applies the final else clause applies and a default message is printed see the richter cpp file for the full program figure multiple alternatives true here you must sort the conditions and test against the largest cutoff first suppose we reverse the order of tests if richter tests in wrong order cout damage to poorly constructed buildings else if richter cout many buildings considerably damaged some collapse else if richter cout many buildings destroyed else if richter cout most structures fall this does not work suppose the value of richter is that value is at least matching the first case the other tests will never be attempted in this example it is also important that we use a sequence of else if clauses not just multiple independent if statements consider this sequence of independent tests if richter didn t use else cout most structures fall if richter cout many buildings destroyed if richter cout many buildings considerably damaged some collapse if richter cout damage to poorly constructed buildings now the alternatives are no longer exclusive if richter is then the last three tests all match and three messages are printed in a game program the scores of players a and b are stored in variables and assuming that the player with the larger score wins write a sequence of conditional statements that prints out a won b won or game tied write a conditional statement with three branches that sets to if x is positive to if x is negative and to if x is zero how could you achieve the task of self check with only two branches beginners sometimes write statements such as the following if price price else if price price explain how this code can be improved suppose the user enters into the richter cpp program what is printed suppose we want to have the richter cpp program check whether the user entered a negative number what branch would you add to the if statement and where practice it now you can try these exercises at the end of the chapter special topic the switch statement a sequence of if statements that compares a single integer value against several constant alter natives can be implemented as a switch statement for example int digit switch digit case one break case two break case three break case four break case five break case six break case seven break case eight break case nine break default break this is a shortcut for int digit if digit one else if digit two else if digit three else if digit four else if digit five else if digit six else if digit seven else if digit eight else if digit nine else well it isn t much of a shortcut but it has one advan tage it is obvious that all branches test the same value namely digit it is possible to have multiple case clauses for a branch such as case case case case case odd true break the default branch is chosen if none of the case clauses match every branch of the switch must be terminated by a break instruction if the break is missing execution falls through to the next branch and so on until finally a break or the end of the switch is reached in practice this fall through behavior is rarely useful but it is a com the switch statement lets you choose from a fixed set of alternatives mon cause of errors if you accidentally forget the break statement your program compiles but executes unwanted code many programmers consider the switch statement somewhat dan gerous and prefer the if statement we leave it to you to use the switch statement for your own code or not at any rate you need to have a reading knowledge of switch in case you find it in other programmers code nested branches it is often necessary to include an if statement inside another such an arrangement is called a nested set of statements here is a typical example in the united states different tax rates are used depending on the taxpayer mari tal status there are different tax schedules for single and for married taxpayers mar ried taxpayers add their income together and pay taxes on the total table gives the tax rate computations using a simplification of the schedules in effect for the tax year a different tax rate applies to each bracket in this schedule the income at the first bracket is taxed at percent and the income at the second bracket is taxed at percent the income limits for each bracket depend on the marital status now compute the taxes due given a filing status and an income figure the key point is that there are two levels of decision making first you must branch on the marital status then for each filing status you must have another branch on income level if your status is single and if the taxable income is the tax is of the amount over at most over if your status is married and if the taxable income is the tax is of the amount over at most over the two level decision process is reflected in two levels of if statements in the pro gram at the end of this section see figure for a flowchart computing income taxes requires multiple levels of decisions figure income tax computation in theory nesting can go deeper than two levels a three level decision process first by state then by filing status then by income level requires three nesting levels tax cpp if if income income else income else if income income else income double cout the tax is endl return program run what is the amount of tax that a single taxpayer pays on an income of would that amount change if the first nested if statement changed from if income to if income suppose harry and sally each make per year would they save taxes if they married how would you modify the tax cpp program in order to check that the user entered a correct value for the marital status i e or m some people object to higher tax rates for higher incomes claiming that you might end up with less money after taxes when you get a raise for working hard what is the flaw in this argument practice it now you can try these exercises at the end of the chapter programming tip hand tracing a very useful technique for understanding whether a program works correctly is called hand tracing you simulate the pro gram activity on a sheet of paper you can use this method with pseudocode or c code get an index card a cocktail napkin or whatever sheet of paper is within reach make a column for each variable have the program code ready use a marker such as a paper clip to mark the current statement in your mind execute statements one at a time every time the value of a variable changes cross out the old value and write the new value below the old one for example let trace the tax program with the data from the program run on page in lines and and are initial ized to hand tracing helps you understand whether a program works correctly int main const double const double const double const double double double in lines and income and are initialized by input statements double income cout please enter your income cin income cout please enter for single m for married string cin because is not we move to the else branch of the outer if statement line marital income status marital income status m if if income rate1 income else rate1 tax2 rate2 income else since income is not we move to the else branch of the inner if statement line if income rate1 income else rate1 tax2 rate2 income m common error the dangling else problem when an if statement is nested inside another if statement the following error may occur double inside continental u s if country usa if state hi hawaii is more expensive else pitfall as are foreign shipments the indentation level seems to suggest that the else is grouped with the test country usa unfortunately that is not the case the compiler ignores all indentation and matches the else with the preceding if that is the code is actually double inside continental u s if country usa if state hi hawaii is more expensive else pitfall as are foreign shipments that isn t what you want you want to group the else with the first if the ambiguous else is called a dangling else you can avoid this pitfall if you always use braces as recommended in programming tip on page double inside continental u s if country usa if state hi hawaii is more expensive else as are foreign shipments problem solving flowcharts you have seen examples of flowcharts earlier in this chapter a flowchart shows the structure of decisions and tasks that are required to solve a problem when you have to solve a complex problem it is a good idea to draw a flowchart to visualize the flow of control the basic flowchart elements are shown in figure figure flowchart elements the basic idea is simple enough link tasks and input output boxes in the sequence in which they should be executed whenever you need to make a decision draw a dia mond with two outcomes see figure each branch can contain a sequence of tasks and even additional decisions if there are multiple choices for a value lay them out as in figure figure flowchart with two outcomes figure flowchart with multiple choices there is one issue that you need to be aware of when drawing flowcharts unconstrained branching and merging can lead to spaghetti code a messy net work of possible pathways through a program there is a simple rule for avoiding spaghetti code never point an arrow inside another branch to understand the rule consider this example shipping costs are inside the united states except that to hawaii and alaska they are international shipping costs are also you might start out with a flowchart like the following spaghetti code has so many pathways that it becomes impossible to understand now you may be tempted to reuse the shipping cost task don t do that the red arrow points inside a different branch instead add another task that sets the shipping cost to like this not only do you avoid spaghetti code but it is also a better design in the future it may well happen that the cost for international shipments is different from that to alaska and hawaii flowcharts can be very useful for getting an intuitive understanding of the flow of an algorithm however they get large rather quickly when you add more details at that point it makes sense to switch from flowcharts to pseudocode draw a flowchart for a program that reads a value temp and prints frozen if it is less than zero what is wrong with the flowchart at right how do you fix the flowchart of self check draw a flowchart for a program that reads a value x if it is less than zero print error otherwise print its square root draw a flowchart for a program that reads a value temp if it is less than zero print ice if it is greater than print steam otherwise print liquid practice it now you can try these exercises at the end of the chapter problem solving test cases consider how to test the tax computation program from section of course you cannot try out all possible inputs of filing status and income level even if you could there would be no point in trying them all if the program correctly computes one or two tax amounts in a given bracket then we have a good reason to believe that all amounts will be correct you want to aim for complete coverage of all decision points here is a plan for obtaining a comprehensive set of test cases there are two possibilities for the filing status and two tax brackets for each status yielding four test cases test a handful of boundary conditions such as an income that is at the boundary between two brackets and a zero income if you are responsible for error checking which is discussed in section also test an invalid input such as a negative income make a list of the test cases and the expected outputs test case expected output comment bracket of m bracket m of boundary case boundary case when you develop a set of test cases it is helpful to have a flowchart of your program see section check off each branch that has a test case include test cases for the boundary cases of each decision for example if a decision checks whether an input is less than test with an input of it is always a good idea to design test cases before starting to code working through the test cases gives you a better understanding of the algorithm that you are about to implement using figure on page as a guide follow the process described in section to design a set of test cases for the elevator cpp program in section what is a boundary test case for the algorithm in how to on page what is the expected output using figure on page as a guide follow the process described in section to design a set of test cases for the richter cpp program in section suppose you are designing a part of a program for a medical robot that has a sensor returning an x and y location measured in cm you need to check whether the sensor location is inside the circle outside the circle on the boundary specifically having a distance of less than mm from the boundary assume the circle has center and radius cm give a set of test cases practice it now you can try these exercises at the end of the chapter programming tip make a schedule and make time for unexpected problems commercial software is notorious for being delivered later than promised for example microsoft originally promised that its windows vista operating system would be available late in then in then in march it finally was released in january some of the early promises might not have been realistic it was in microsoft interest to let prospective customers expect the imminent availability of the product had customers known the actual delivery date they might have switched to a different product in the meantime undeniably though microsoft had not anticipated the full complexity of the tasks it had set itself to solve microsoft can delay the delivery of its product but it is likely that you cannot as a student or a programmer you are expected to manage your time wisely and to finish your assignments on time you can probably do simple programming exercises the night before the due date but an assignment that looks twice as hard may well take four times as long because more things can go wrong you should therefore make a schedule whenever you start a programming project first estimate realistically how much time it will take you to design the program logic develop test cases type in the program and fix syntax errors test and debug the program for example for the income tax program i might estimate an hour for the design minutes for developing test cases an hour for data entry and fixing syntax errors and an hour for testing and debugging that is a total of hours if i work two hours a day on this project it will take me almost two days then think of things that can go wrong your computer might break down you might be stumped by a problem with the computer sys tem that is a particularly important concern for make a schedule for your programming work and build in time for problems beginners it is very common to lose a day over a trivial problem just because it takes time to track down a person who knows the magic command to overcome it as a rule of thumb double the time of your estimate that is you should start four days not two days before the due date if nothing went wrong great you have the program done two days early when the inevitable problem occurs you have a cushion of time that protects you from embarrassment and failure boolean variables and operators sometimes you need to evaluate a logical condition in one part of a program and use it elsewhere to store a condition that can be true or false you use a boolean variable boolean variables are named after the mathematician george boole a pioneer in the study of logic in c the bool data type represents the boolean type variables of type bool can hold exactly two values denoted false and true these values are not strings or inte gers they are special values just for boolean variables here is a definition of a boolean variable bool failed true you can use the value later in your program to make a decision if failed only executed if failed has been set to true a boolean variable is also called a flag because it can be either up true or down false when you make complex decisions you often need to combine boolean values an operator that combines boolean conditions is called a boolean operator in c the operator called and yields true only when both conditions are true the opera tor called or yields the result true if at least one of the conditions is true suppose you write a program that processes temperature values and you want to test whether a given temperature corresponds to liquid water at sea level water freezes at degrees celsius and boils at degrees water is liquid if the tempera ture is greater than zero and less than if temp temp cout liquid the condition of the test has two parts joined by the operator as shown in table and appendix c the and operators have higher precedence than the operator each part is a boolean value that can be true or false the combined expression is true if both individual expressions are true if either one of the expressions is false then the result is also false see figure figure boolean truth tables at this geyser in iceland you can see ice liquid water and steam operator description unary unary increment decrement positive negative boolean not multiplication division remainder addition subtraction comparisons equal not equal boolean and boolean or conversely let test whether water is not liquid at a given temperature that is the case when the temperature is at most or at least use the or operator to combine the expressions if temp temp cout not liquid figure shows flowcharts for these examples sometimes you need to invert a condition with the not logical operator the operator takes a single condition and evaluates to true if that condition is false and to false if the condition is true in this example output occurs if the value of the boolean variable frozen is false if frozen cout not frozen figure flowcharts for and and or combinations table boolean operators expression value comment false only the first condition is true note that the operator has a higher precedence than the operator true the first condition is true true the is not a test for either or if both conditions are true the result is true true error the expression is true which is converted to the expression is true you never want to write such an expression see common error on page true error is not zero it is converted to true you never want to write such an expression see common error on page x x x x x x the operator has a higher precedence than the operator false is true therefore its negation is false frozen true frozen there is no need to compare a boolean variable with true frozen false frozen it is clearer to use than to compare with false table illustrates additional examples of evaluating boolean operators suppose x and y are two integers how do you test whether both of them are zero how do you test whether at least one of them is zero how do you test whether exactly one of them is zero what is the value of frozen what is the advantage of using the type bool rather than strings false true or integers practice it now you can try these exercises at the end of the chapter common error common error combining multiple relational operators consider the expression if temp error this looks just like the mathematical test temp unfortunately it is not let us dissect the expression temp the first half temp is a test with out come true or false depending on the value of temp the outcome of that test true or false is then compared against can one compare truth values and floating point numbers is true larger than or not unfortunately to stay compatible with the c language c converts false to and true to therefore the expression will always evaluate to true you must be careful not to mix logical and arithmetic expressions in your programs instead use and to combine two separate tests if temp temp another common error along the same lines is to write if x y error instead of if x y unfortunately the compiler will not issue an error message instead it converts x to true or false zero is converted to false and any nonzero value is converted to true if x is not zero then it tests whether y is greater than and finally it computes the and of these two truth val ues naturally that computation makes no sense confusing and conditions it is a surprisingly common error to confuse and and or conditions a value lies between and if it is at least and at most it lies outside that range if it is less than or greater than there is no golden rule you just have to think carefully often the and or or is clearly stated and then it isn t too hard to implement it but some times the wording isn t as explicit it is quite common that the individual conditions are nicely set apart in a bulleted list but with little indication of how they should be combined consider these instructions for filing a tax return you can claim single filing status if any one of the fol lowing is true you were never married you were legally separated or divorced on the last day of the tax year you were widowed and did not remarry since the test passes if any one of the conditions is true you must combine the conditions with or elsewhere the same instructions state that you may use the more advantageous status of married filing jointly if all five of the following conditions are true your spouse died less than two years ago and you did not remarry you have a child whom you can claim as dependent that child lived in your home for all of the tax year you paid over half the cost of keeping up your home for this child you filed a joint return with your spouse the year he or she died because all of the conditions must be true for the test to pass you must combine them with an and special topic short circuit evaluation of boolean operators when the and operators are computed evaluation stops as soon as the truth value is determined when an is evaluated and the first condition is false the second condition is not evaluated because it does not matter what the outcome of the second test is for example consider the expression quantity price quantity suppose the value of quantity is zero then the test quantity fails and the second test is not attempted that is just as well because it is illegal to divide by zero similarly when the first condition of an expression is true then the remainder is not evaluated since the result must be true this process is called short circuit evaluation in a short circuit electricity travels along the path of least resistance similarly short circuit evaluation takes the fastest path for computing the result of a boolean expression special topic de morgan law humans generally have a hard time comprehending logical conditions with not operators applied to and or expressions de morgan law named after the logician augustus de mor gan can be used to simplify these boolean expressions suppose we want to charge a higher shipping rate if we don t ship within the continental united states if country usa state ak state hi this test is a little bit complicated and you have to think carefully through the logic when it is not true that the country is usa and the state is not alaska and the state is not hawaii then charge huh it is not true that some people won t be confused by this code the computer doesn t care but it takes human programmers to write and maintain the code therefore it is useful to know how to simplify such a condition de morgan law has two forms one for the negation of an and expression and one for the negation of an or expression a b is the same as a b a b is the same as a b pay particular attention to the fact that the and and or operators are reversed by moving the not inward for example the negation of the state is alaska or it is hawaii state ak state hi is the state is not alaska and it is not hawaii state ak state hi application input validation an important application for the if statement is input validation whenever your program accepts user input you need to make sure that the user supplied values are valid before you use them in your computations consider our elevator program assume that the elevator panel has buttons labeled through but not the following are illegal inputs the number zero or a negative number like a quality control worker you want to make sure that user input is correct before processing it a number larger than an input that is not a sequence of digits such as five in each of these cases we will want to give an error message and exit the program it is simple to guard against an input of if floor cout error there is no thirteenth floor endl return the statement return immediately exits the main function and therefore terminates the program it is a con vention to return with the value if the program completed normally and with a non zero value when an error was encountered here is how you ensure that the user doesn t enter a number outside the valid range if floor floor cout error the floor must be between and endl return however dealing with an input that is not a valid integer is a more serious problem when the statement cin floor is executed and the user types in an input that is not an integer such as five then the integer variable floor is not set instead the input stream cin is set to a failed state you call the fail member function to test for that failed state if cin fail cout error not an integer endl return the order of the if statements is important you must first test for cin fail after all if the input failed no value has been assigned to floor and it makes no sense to compare it against other values input failure is quite serious in c once input has failed all subsequent attempts at input will fail as well you will learn in chapter how to write programs that are more tolerant of bad input for now our goal is simply to detect bad input and to exit the program when it occurs here is the complete elevator program with input validation cpp now we know that the input is valid int if floor floor else floor cout the elevator will travel to the actual floor endl return program run consider the cpp program what output do you get when the input is a b c d thirteen your task is to rewrite the cpp program so that there is a single if state ment with a complex condition if cout error bad input endl return what is the condition in the sherlock holmes story the adventure of the sussex vampire the inimitable detective uttered these words matilda briggs was not the name of a young woman watson it was a ship which is associated with the giant rat of sumatra a story for which the world is not yet prepared over a hundred years later researchers found giant rats in western new guinea another part of indonesia suppose you are charged with writing a program that processes rat weights it contains the statements double weight cout enter weight in kg cin weight what input checks should you supply when processing inputs you want to reject values that are too large but how large is too large these giant rats found in western new guinea are about five times the size of a city rat consider the following test program int main int m cout enter an integer cin m int n cout enter another integer cin n cout m n endl return run this program and enter three at the first prompt what happens why practice it now you can try these exercises at the end of the chapter random fact artificial intelligence when one uses a sophisticated com puter program such as a tax prepara tion package one is bound to attribute some intelligence to the computer the computer asks sensible questions and makes computations that we find a mental challenge after all if doing one taxes were easy we wouldn t need a computer to do it for us as programmers however we know that all this apparent intelligence is an illusion human programmers have carefully coached the software in all possible scenarios and it simply replays the actions and decisions that were programmed into it would it be possible to write com puter programs that are genuinely intelligent in some sense from the earliest days of computing there was a sense that the human brain might be nothing but an immense computer and that it might well be feasible to program computers to imitate some processes of human thought seri ous research into artificial intelligence began in the mid and the first twenty years brought some impres sive successes programs that play chess surely an activity that appears to require remarkable intellectual pow ers have become so good that they now routinely beat all but the best human players as far back as an expert system program called mycin gained fame for being better in diag nosing meningitis in patients than the average physician however there were serious set backs as well from to the japanese government embarked on a massive research project funded at over billion japanese yen it was known as the fifth generation project its goal was to develop new hardware and software to greatly improve the performance of expert system soft ware at its outset the project created fear in other countries that the japa nese computer industry was about to become the undisputed leader in the field however the end results were disappointing and did little to bring artificial intelligence applications to market from the very outset one of the stated goals of the ai community was to produce software that could trans late text from one language to another for example from english to russian that undertaking proved to be enor mously complicated human language appears to be much more subtle and interwoven with the human experience than had originally been thought even the grammar checking tools that come with word processing programs today are more of a gimmick than a useful tool and analyzing grammar is just the first step in translating sentences the cyc from encyclopedia proj ect started by douglas lenat in tries to codify the implicit assump tions that underlie human speech and writing the team members started out analyzing news articles and asked themselves what unmentioned facts chapter summary use the if statement to implement a decision the if statement allows a program to carry out different actions depending on the nature of the data to be processed implement comparisons of numbers and objects relational operators are used to compare numbers and strings lexicographic order is used to compare strings implement complex decisions that require multiple if statements multiple alternatives are required for decisions that have more than two cases when using multiple if statements pay attention to the order of the conditions are necessary to actually understand the sentences for example consider the sentence last fall she enrolled in michigan state the reader automati cally realizes that fall is not related to falling down in this context but refers to the season while there is a state of michigan here michigan state denotes the university a priori a com puter program has none of this knowl edge the goal of the cyc project is to extract and store the requisite facts that is people enroll in universities michigan is a state many states have universities named x state uni versity often abbreviated as x state most people enroll in a university in the fall by the project had codi ing examples is the out come of a series of grand challenges for autono mous vehicles posed by the defense advanced research projects agency darpa competitors were invited to submit a com puter controlled vehicle that had to complete an obstacle course without a human driver or remote control the first event in was a disappointment with none of the entrants finish ing the route in five vehicles completed a gru eling km course in the winner of the darpa urban challenge fied about common sense concepts and about a million facts of knowledge relating them even this massive amount of data has not proven sufficient for useful applications in recent years artificial intelli gence technology has seen substantial advances one of the most astound mojave desert stanford stanley came in first with an average speed of km h in darpa moved the com petition to an urban environment an abandoned air force base vehicles had to be able to interact with each other following california traffic laws as stanford sebastian thrun explained in the last grand challenge it didn t really matter whether an obstacle was a rock or a bush because either way you d just drive around it the current challenge is to move from just sensing the environment to understanding the environment implement decisions whose branches require further decisions when a decision statement is contained inside the branch of another decision statement the statements are nested nested decisions are required for problems that have two levels of decision making draw flowcharts for visualizing the control flow of a program flow charts are made up of elements for tasks input outputs and decisions each branch of a decision can contain tasks and further decisions never point an arrow inside another branch design test cases for your programs each branch of your program should be covered by a test case it is a good idea to design test cases before implementing a program use the boolean data type to store and combine conditions that can be true or false the boolean type bool has two values false and true c has two boolean operators that combine conditions and and or to invert a condition use the not operator the and operators are computed using short circuit evaluation as soon as the truth value is determined no further conditions are evaluated de morgan law tells you how to negate and conditions apply if statements to detect whether user input is valid when reading a value check that it is within the required range use the fail function to test whether the input stream has failed find the errors in the following if statements a if x then cout x b if x y else y c if x pow x sqrt y y x d if x y e cin x if cin fail y y x what do these code fragments print a int n int m if n m cout n else cout m b int n int m if n m cout n else cout m c double x double y if fabs x y cout x else cout y d double x sqrt double y if x x y cout x else cout y suppose x and y are variables of type double write a code fragment that sets y to x if x is positive and to otherwise suppose x and y are variables of type double write a code fragment that sets y to the absolute value of x without calling the fabs function use an if statement explain why it is more difficult to compare floating point numbers than integers write c code to test whether an integer n equals and whether a floating point number x equals common error on page explains that a c compiler will not report an error when you use an assignment operator instead of a test for equality but it may issue a warning write a test program containing a statement if floor what does your compiler do when you compile the program each square on a chess board can be described by a letter and number such as in this example a b c d e a b c d e f g h f g h the following pseudocode describes an algorithm that determines whether a square with a given letter and number is dark black or light white if the letter is an a c e or g if the number is odd color black else color white else if the number is even color black else color white using the procedure in programming tip on page trace this pseudocode with input give a set of four test cases for the algorithm of exercise that covers all branches in a scheduling program we want to check whether two appointments overlap for simplicity appointments start at a full hour and we use military time with hours the following pseudocode describes an algorithm that determines whether the appointment with start time and end time overlaps with the appoint ment with start time and end time if start1 else if e else e if e the appointments overlap else the appointments don t overlap trace this algorithm with an appointment from and one from then with an appointment from and one from draw a flow chart for the algorithm in exercise draw a flow chart for the algorithm in exercise draw a flow chart for the algorithm in exercise develop a set of test cases for the algorithm in exercise develop a set of test cases for the algorithm in exercise write pseudocode for a program that prompts the user for a month and day and prints out whether it is one of the following four holidays new year day january independence day july veterans day november christmas day december write pseudocode for a program that assigns letter grades for a quiz according to the following table score grade a b c d f explain how the lexicographic ordering of strings in c differs from the ordering of words in a dictionary or telephone book hint consider strings such as ibm wiley com century and while u wait of the following pairs of strings which comes first in lexicographic order a tom dick b tom tomato c church churchill d car manufacturer carburetor e harry hairy f c car g tom tom h car carl i car bar explain the difference between a sequence of else if clauses and nested if state ments give an example for each give an example of a sequence of else if clauses where the order of the tests does not matter give an example where the order of the tests matters rewrite the condition in section to use operators instead of operators what is the impact on the order of the comparisons give a set of test cases for the tax program in exercise manually compute the expected results make up another c code example that shows the dangling else problem using the following statement a student with a gpa of at least but less than is on probation with less than the student is failing complete the following truth table by finding the truth values of the boolean expressions for all combinations of the boolean inputs p q and r p q r p q r p q r false false false false false true false true false more combinations true or false a b is the same as b a for any boolean conditions a and b the advanced search feature of many search engines allows you to use boolean operators for complex queries such as cats or dogs and not pets contrast these search operators with the boolean operators in c suppose the value of b is false and the value of x is what is the value of each of the following expressions a b x b b x c b x d b x e b x f b x g b x h b x simplify the following expressions here b is a variable of type bool a b true b b false c b true d b false simplify the following statements here b is a variable of type bool and n is a variable of type int a if n b true else b false hint what is the value of n b if n b false else b true c b false if n if n b true d if n b true else b n what is wrong with the following program cout enter the number of quarters cin quarters total total quarters cout total total endl if cin fail cout input error reading numbers is surprisingly difficult because a c input stream looks at the input one character at a time first white space is skipped then the stream con sumes those input characters that can be a part of a number once the stream has recognized a number it stops reading if it finds a character that cannot be a part of a number however if the first non white space character is not a digit or a sign or if the first character is a sign and the second one is not a digit then the stream fails consider a program reading an integer cout enter the number of quarters int quarters cin quarters for each of the following user inputs circle how many characters have been read and whether the stream is in the failed state or not a b c d e fifteen f fifteen g h i write a program that reads a temperature value and the letter c for celsius or f for fahrenheit print whether water is liquid solid or gaseous at the given temperature at sea level the boiling point of water drops by about one degree centigrade for every meters or feet of altitude improve the program of exercise to allow the user to supply the altitude in meters or feet write a program that reads in three floating point numbers and prints the largest of the three inputs for example please enter three numbers the largest number is write a program that reads in three strings and sorts them lexicographically enter three strings charlie able baker able baker charlie write a program that reads an integer and prints how many digits the number has by checking whether the number is and so on assume that all integers are less than ten billion if the number is negative first multiply it with write a program that reads three numbers and prints all the same if they are all the same all different if they are all different and neither otherwise write a program that reads three numbers and prints increasing if they are in increasing order decreasing if they are in decreasing order and neither other wise here increasing means strictly increasing with each value larger than its predecessor the sequence would not be considered increasing repeat exercise but before reading the numbers ask the user whether increas ing decreasing should be strict or lenient in lenient mode the sequence is increasing and the sequence is both increasing and decreasing write a program that translates a letter grade into a number grade letter grades are a b c d and f possibly followed by or their numeric values are and there is no f or f a increases the numeric value by a decreases it by however an a has value enter a letter grade b the numeric value is write a program that translates a number between and into the closest letter grade for example the number which might have been the average of several grades would be converted to b break ties in favor of the better grade for example should be a b write a program that takes user input describing a playing card in the following shorthand notation a ace card values j jack q queen k king d diamonds h hearts s spades c clubs your program should print the full description of the card for example enter the card notation qs queen of spades when two points in time are compared each given as hours in military time ranging from and and minutes the following pseudocode determines which comes first if comes first else if and are the same if comes first else if and are the same and are the same else comes first else comes first write a program that prompts the user for two points in time and prints the time that comes first then the other time the following algorithm yields the season spring summer fall or winter for a given month and day if month is or season winter else if month is or season spring else if month is or season summer else if month is or season fall if month is divisible by and day if season is winter season spring else if season is spring season summer else if season is summer season fall else season winter write a program that prompts the user for a month and day and then prints the season as determined by this algorithm write a program that reads in two floating point numbers and tests whether they are the same up to two decimal places here are two sample runs enter two floating point numbers they are the same up to two decimal places enter two floating point numbers they are different write a program to simulate a bank transaction there are two bank accounts check ing and savings first ask for the initial balances of the bank accounts reject negative balances then ask for the transactions options are deposit withdrawal and trans fer then ask for the account options are checking and savings then ask for the amount reject transactions that overdraw an account at the end print the balances of both accounts write a program that reads in the name and salary of an employee here the salary will denote an hourly wage such as then ask how many hours the employee worked in the past week be sure to accept fractional hours any overtime work over hours per week is paid at percent of the regular wage compute the pay print a paycheck for the employee write a program that prompts for the day and month of the user birthday and then prints a horoscope make up fortunes for programmers like this please enter your birthday month and day gemini are experts at figuring out the behavior of complicated programs you feel where bugs are coming from and then stay one step ahead tonight your style wins approval from a tough critic each fortune should contain the name of the astrological sign you will find the names and date ranges of the signs at a distressingly large number of sites on the internet write a program that computes taxes for the following schedule if your status is single and if the taxable income is over but not over the tax is of the amount over if your status is married and if the taxable income is over but not over the tax is of the amount over 800 the original u s income tax of was quite simple the tax was percent on the first percent on the amount over up to percent on the amount over up to percent on the amount over up to percent on the amount over up to percent on the amount over there was no separate schedule for single or married taxpayers write a program that computes the income tax according to this schedule the tax cpp program uses a simplified version of the u s income tax schedule look up the tax brackets and rates for the current year for both single and married filers and implement a program that computes the actual income tax unit conversion write a unit conversion program that asks the users from which unit they want to convert fl oz gal oz lb in ft mi and to which unit they want to convert ml l g kg mm cm m km reject incompatible conversions such as gal km ask for the value to be converted then display the result convert from gal convert to ml value gal ml write a program that prompts the user to provide a single character from the alpha bet print vowel or consonant depending on the user input if the user input is not a letter between a and z or a and z or is a string of length print an error message roman numbers write a program that converts a positive integer into the roman number system the roman number system has digits i v x l c d m numbers are formed according to the following rules only numbers up to are represented as in the decimal system the thousands hundreds tens and ones are expressed separately the numbers to are expressed as i ii iii iv v vi vii viii ix as you can see an i preceding a v or x is subtracted from the value and you can never have more than three i in a row tens and hundreds are done the same way except that the letters x l c and c d m are used instead of i v x respectively your program should take an input such as and convert it to roman numerals mcmlxxviii write a program that asks the user to enter a month for january for february and so on and then prints the number of days in the month for february print or days enter a month days do not use a separate if else branch for each month use boolean operators a year with days is called a leap year a year is a leap year if it is divisible by four for example except that it is not a leap year if it is divisible by for example however it is a leap year if it is divisible by for example there were no exceptions before the introduction of the gregorian calendar on october was a leap year write a program that asks the user for a year and computes whether that year is a leap year add error handling to exercise if the user does not enter a number when expected or provides an invalid unit for the altitude print an error message and end the program engineering write a program that prompts the user for a wavelength value and prints a descrip tion of the corresponding part of the electromagnetic spectrum as given in table type wavelength m frequency hz radio waves microwaves to to infrared to to visible light to to ultraviolet to to x rays to to gamma rays engineering repeat exercise modifying the program so that it prompts for the frequency instead engineering repeat exercise modifying the program so that it first asks the user whether the input will be a wavelength or a frequency engineering a minivan has two sliding doors each door can be opened by either a dashboard switch its inside handle or its outside handle however the inside handles do not work if a child lock switch is activated in order for the sliding doors to open the gear shift must be in park and the master unlock switch must be activated this book author is the long suffering owner of just such a vehicle your task is to simulate a portion of the control software for the vehicle the input is a sequence of values for the switches and the gear shift in the following order dashboard switches for left and right sliding door child lock and master unlock for off or for activated inside and outside handles on the left and right sliding doors or the gear shift setting one of p n d r a typical input would be p print left door opens and or right door opens as appropriate if neither door opens print both doors stay closed engineering sound level l in units of decibel db is determined by l p where p is the sound pressure of the sound in pascals abbreviated pa and is a reference sound pressure equal to pa where l is db the following table gives descriptions for certain sound levels threshold of pain db possible hearing damage db jack hammer at m db traffic on a busy roadway at m db normal conversation db calm library db light leaf rustling db write a program that reads a value and a unit either db or pa and then prints the closest description from the list above engineering the electric circuit shown below is designed to measure the temperature of the gas in a chamber vs v the resistor r represents a temperature sensor enclosed in the chamber the resis tance r in is related to the temperature t in c by the equation r kt in this device assume and k the voltmeter displays the value of the voltage vm across the sensor this voltage vm indicates the temperature t of the gas according to the equation t r rs vm k k k vs vm k suppose the voltmeter voltage is constrained to the range vmin volts vm vmax volts write a program that accepts a value of vm and checks that it between and the program should return the gas temperature in degrees celsius when vm is between and and an error message when it isn t engineering crop damage due to frost is one of the many risks confronting farmers the figure below shows a simple alarm circuit designed to warn of frost the alarm circuit uses a device called a thermistor to sound a buzzer when the temperature drops below freezing thermistors are semiconductor devices that exhibit a temperature depen dent resistance described by the equation r e t where r is the resistance in at the temperature t in k and is the resistance in at the temperature in k is a constant that depends on the material used to make the thermistor v v thermistor r buzzer comparator the circuit is designed so that the alarm will sound when r the thermistor used in the alarm circuit has at c and k notice that has units of k recall that the temperature in k is obtained by adding to the temperature in c the resistors and have a resis tance of k write a c program that prompts the user for a temperature in f and prints a message indicating whether or not the alarm will sound at that temperature engineering a mass m kilograms is attached to the end of a rope of length r meters the mass is whirled around at high speed the rope can withstand a maximum tension of t newtons write a program that accepts a rotation speed v and determines if such a speed will cause the rope to break hint t r engineering a mass m is attached to the end of a rope of length r meters the rope can only be whirled around at speeds of or meters per second the rope can with stand a maximum tension of t newtons write a program where the user enters the value of the mass m and the program determines the greatest speed at which it can be whirled without breaking the rope hint t r engineering the average person can jump off the ground with a velocity of mph without fear of leaving the planet however if an astronaut jumps with this velocity while standing on halley comet will the astronaut ever come back down create a program that allows the user to input a launch velocity in mph from the surface of halley comet and determine whether a jumper will return to the surface if not the program should calculate how much more massive the comet must be in order to return the jumper to the surface hint escape velocity is vescape where g kg2 is the gravitational constant m kg is the mass of halley comet and r m is its radius change the if statement to if floor floor the only difference is if original_price is the statement in self check sets to this one sets it to if cout red endl else cout green endl a and b are both true c is false floor the values should be compared with not input y str or str length if cout a won else if cout b won else cout game tied if x else if x else you could first set to one of the three values if x else if x the if price can be omitted making it clear that the else branch is the sole alternative no destruction of buildings add a branch before the final else else if richter cout error negative input endl no then the computation is 32000 no their individual tax is each and if they married they would pay actually taxpayers in higher tax brackets which our program does not model may pay higher taxes when they marry a phenomenon known as the marriage penalty change else in line to else if m and add another branch after line else cout error marital status should be or m endl the higher tax rate is only applied on the income in the higher bracket suppose you are single and make should you try to get a raise absolutely you get to keep percent of the first and percent of the next the true arrow from the first decision points into the true branch of the sec ond decision creating spaghetti code here is one solution in section you will see how you can combine the condi tions for a more elegant solution test case expected output comment below floor above floor the specification is not clear see section for a version of this program with error handling a boundary test case is a price of a percent discount should apply because the problem statement states that the larger discount applies if the price is at least thus the expected output is test case expected output most structures fall comment many buildings destroyed many buildings considerably damage to poorly no destruction most structures fall boundary case in this program boundary cases are not as significant since the behavior of an earthquake changes gradually the specification is not clear see self check for a version of this program with error handling test case expected output comment inside outside on the boundary exactly on the boundary 414 on the boundary close to the boundary inside not less than mm from the boundary outside not less than mm from the boundary x y x y x y y x the same as the value of frozen you are guaranteed that there are no other values with strings or integers you would need to check that no values such as maybe or enter your calculations a error the floor must be between and b error the floor must be between and c the elevator will travel to the actual floor d error not an integer cin fail floor floor floor check for cin fail to make sure a researcher didn t supply an input such as oh my check for weight since any rat must surely have a positive weight we don t know how giant a rat could be but the new guinea rats weighed no more than kg a regular house rat rattus rattus weighs up to kg so we ll say that any weight kg was surely an input error perhaps confusing grams and kilograms thus the checks are if cin fail cout error not a number endl return if weight cout error weight cannot be negative endl return if weight cout error weight kg endl return the first input fails the value of m is unchanged because a previous input failed the next input doesn t even try to get additional keystrokes it also fails and n is un changed the program prints to understand nested loops to implement programs that read and process data sets to use a computer for simulations the while loop syntax while statement common error infinite loops common error don t think are we there yet common error off by one errors random fact the first bug problem solving hand tracing the for loop syntax for statement programming tip use for loops for their intended purpose only programming tip choose loop bounds that match your task programming tip count iterations the do loop programming tip flowcharts for loops processing input special topic clearing the failure state special topic the loop and a half problem and the break statement special topic redirection of input and output problem solving storyboards common loop algorithms how to writing a loop worked example credit card processing nested loops random numbers and simulations random fact software piracy the while loop in this section you will learn how to repeatedly execute statements until a goal has been reached recall the investment problem from chapter you put into a bank account that earns percent interest per year how many years does it take for the account balance to be double the origi nal investment in chapter we developed the following algo rithm for this problem start with a year value of a column for the interest and a balance of because the interest earned also earns interest a bank balance grows exponentially year interest balance repeat the following steps while the balance is less than add to the year value compute the interest as balance x i e percent interest add the interest to the balance report the final year value as the answer you now know how to define and update the variables in c what you don t yet know is how to carry out repeat steps while the balance is less than in a particle accelerator subatomic particles traverse a loop shaped tunnel multiple times gaining the speed required for physical experiments similarly in computer science statements in a loop are executed while a condition is true figure flowchart of a while loop in c the while statement implements such a repetition see syntax the code while condition statements keeps executing the statements while the condition is true in our case we want to increment the year counter and add interest while the balance is less than the target balance of while balance target year double interest balance rate balance balance interest a while statement is an example of a loop if you draw a flowchart the flow of execu tion loops again to the point where the condition is tested see figure syntax while statement when you define a variable inside the loop body the variable is created for each iteration of the loop and removed after the end of each iteration for example con sider the interest variable in this loop while balance target figure execution of the doublinv loop year double interest balance rate balance balance interest interest no longer defined here in contrast the balance and years variables were defined outside the loop body that way the same variable is used for all iterations of the loop check the loop condition balance year the condition is true while balance target year double interest balance rate balance balance interest execute the statements in the loop balance year interest while balance target year double interest balance rate balance balance interest check the loop condition again balance year the condition is still true while balance target year double interest balance rate balance balance interest after iterations the condition is while balance target no longer true balance year double interest balance rate year balance balance interest execute the statement following the loop while balance target balance year year double interest balance rate balance balance interest cout year endl here is the program that solves the investment problem figure illustrates the program execution doublinv cpp program run the investment doubled after years how many years does it take for the investment to triple modify the program and run it if the interest rate is percent per year how many years does it take for the investment to double modify the program and run it modify the program so that the balance after each year is printed how did you do that suppose we change the program so that the condition of the while loop is while balance target what is the effect on the program why what does the following loop print int n while n n n cout n practice it now you can try these exercises at the end of the chapter loop output explanation i while i when i is the loop condition is false and the loop ends cout i i i the i statement is an error causing an infinite loop see common error on page while i cout i i i while i no output the statement i is false and the loop is never executed cout i i i no output the programmer probably thought stop when i is less than however the loop condition controls when the loop is executed not when it ends see common error on page while i cout i i i while i no output program does not terminate note the semicolon before the this loop has an empty body it runs forever checking whether i and doing nothing in the body cout i i should year start at or at should you test for balance target or for balance target it is easy to be off by one in these expressions some people try to solve off by one errors by randomly inserting or until the pro gram seems to work a terrible strategy it can take a long time to compile and test all the vari ous possibilities expending a small amount of mental effort is a real time saver fortunately off by one errors are easy to avoid simply by working through a couple of test cases and using the information from the test cases to come up with a rationale for your decisions should year start at or at look at a scenario with simple val ues an initial balance of and an interest rate of percent after year the balance is and after year it is or over 200 so the investment doubled after years the loop executed two times incrementing year each time hence year must start at not at year balance in other words the balance variable denotes the balance after the end of the year at the outset the balance variable contains the balance after year and not after year next should you use a or comparison in the test if you want to settle this question with an example you need to find a scenario in which the final balance is exactly twice the initial balance this happens when the interest is percent the loop executes once now year is and balance is exactly equal to has the investment doubled after one year it has therefore the loop should not execute again if the test condition is balance target the loop stops as it should if the test condition had been balance target the loop would have executed once more in other words you keep adding interest while the balance has not yet doubled random fact the first bug according to legend the first bug was found in the mark ii a huge electrome chanical computer at harvard univer sity it really was caused by a bug a moth was trapped in a relay switch actually from the note that the operator left in the log book next to the moth see the photo it appears as if the term bug had already been in active use at the time the first bug the pioneering computer scientist maurice wilkes wrote somehow at the moore school and afterwards one had always assumed there would be no particular difficulty in getting pro grams right i can remember the exact instant in time at which it dawned on me that a great part of my future life would be spent finding mistakes in my own programs problem solving hand tracing in programming tip you learned about the method of hand tracing when you hand trace code or pseudocode you write the names of the variables on a sheet of paper mentally execute each step of the code and update the variables it is best to have the code written or printed on a sheet of paper use a marker such as a paper clip to mark the current line whenever a variable changes cross out the old value and write the new value below when a program produces output also write down the output in another column consider this example what value is displayed int n int sum while n int digit n sum sum digit n n cout sum endl there are three variables n sum and digit n sum digit the first two variables are initialized with and before the loop is entered int n int sum while n int digit n sum sum digit n n cout sum endl because n is greater than zero enter the loop the variable digit is set to the remain der of dividing by the variable sum is set to int n int sum while n int digit n sum sum digit n n cout sum endl finally n becomes recall that the remainder in the division is dis carded because both arguments are integers cross out the old values and write the new ones under the old ones int n int sum while n int digit n sum sum digit n n cout sum endl now check the loop condition again int n int sum while n int digit n sum sum digit n n cout sum endl because n is still greater than zero repeat the loop now digit becomes sum is set to and n is set to repeat the loop once again setting digit to sum to and n to enter the loop for one last time now digit is set to sum to and n becomes zero int n int sum while n because n equals zero int digit n sum sum digit n n this condition is not true cout sum endl the condition n is now false continue with the statement after the loop int n int sum while n int digit n sum sum digit n n cout sum endl this statement is an output statement the value that is output is the value of sum which is of course you can get the same answer by just running the code however hand tracing can give you an insight that you would not get if you simply ran the code consider again what happens in each iteration we extract the last digit of n we add that digit to sum we strip the digit off n in other words the loop forms the sum of the digits in n you now know what the loop does for any value of n not just the one in the example why would anyone want to form the sum of the digits operations of this kind are useful for checking the validity of credit card numbers and other forms of id numbers see exercise hand tracing does not just help you understand code that works correctly it is a powerful technique for finding errors in your code when a program behaves in a way that you don t expect get out a sheet of paper and track the values of the vari ables as you mentally step through the code you don t need a working program to do hand tracing you can hand trace pseudocode in fact it is an excellent idea to hand trace your pseudocode before you go to the trouble of translating it into actual code to confirm that it works correctly hand trace the following code showing the value of n and the output int n while n n cout n endl hand trace the following code showing the value of n and the output what potential error do you notice int n while n cout n n hand trace the following code assuming that a is and n is then explain what the code does for arbitrary values of a and n int r int i while i n r r a i trace the following code what error do you observe int n while n cout n endl n n the following pseudocode is intended to count the number of digits in the number n count temp n while temp increment count divide temp by trace the pseudocode for n and n what error do you find practice it now you can try these exercises at the end of the chapter the for loop it often happens that you want to execute a sequence of statements a given number of times you can use a while loop that is controlled by a counter as in the following example counter initialize the counter while counter check the counter cout counter endl counter update the counter because this loop type is so common there is a special form for it called the for loop see syntax for counter counter counter cout counter endl some people call this loop count controlled in contrast the while loop of the preceding section can be called an event controlled loop because it executes until an event occurs for example when the balance reaches the target another commonly used term for a count controlled loop is definite you know from the outset that the loop body will be executed a definite number of times ten times in our example in contrast you do not know how many iterations it takes to accumulate a target balance such a loop is called indefinite the for loop neatly groups the initialization condi tion and update expressions together however it is important to realize that these expressions are not exe cuted together see figure you can visualize the for loop as an orderly sequence of steps the initialization is executed once before the loop is entered the condition is checked before each iteration the update is executed after each iteration figure execution of a for loop syntax for statement a for loop can count down instead of up for counter counter counter the increment or decrement need not be in steps of for counter counter counter counter see table on page for additional variations so far we assumed that the counter variable had already been defined before the for loop alternatively you can define a variable in the loop initialization such a variable is defined only in the loop for int counter counter counter counter no longer defined here here is a typical use of the for loop we want to print the balance of our savings account over a period of years as shown in this table year balance the for loop pattern applies because the variable year starts at and then moves in constant increments until it reaches the target for int year year nyears year update balance print year and balance here is the complete program figure shows the corresponding flowchart figure flowchart of a for loop invtable cpp include iostream include iomanip using namespace std int main const double rate const double double balance int nyears cout enter number of years cin nyears cout fixed setprecision for int year year nyears year balance balance rate cout setw year setw balance endl return program run enter number of years 12155 13400 16288 loop values of i comment for i i i note that the loop is executed times see programming tip on page for i i i use i for decreasing values for i i i i use i i for a step size of for i i i i infinite loop you can use or instead of to avoid this problem for i i i i you can specify any rule for modifying i such as doubling it in every step for i i str length i until the last valid index of the string str in the loop body use the expression str substr i to get a string containing the ith character write the for loop of the invtable cpp program as a while loop how many numbers does this loop print for int n n n cout n endl write a for loop that prints all even numbers between and inclusive write a for loop that computes the sum of the integers from to n how would you modify the for loop of the invtable cpp program to print all bal ances until the investment has doubled practice it now you can try these exercises at the end of the chapter the do loop sometimes you want to execute the body of a loop at least once and perform the loop test after the body is executed the do loop serves that purpose do statements while condition the body of the do loop is executed first then the condition is tested some people call such a loop a post test loop because the condition is tested after completing the loop body in contrast while and for loops are pre test loops in those loop types the condition is tested before entering the loop body a typical example for such a loop is input validation suppose you ask a user to enter a value if the user didn t pay attention and entered a larger value you ask again until the value is correct of course you cannot test the value until the user has entered it this is a perfect fit for the do loop see figure int value do cout enter a value cin value while value the do loop figure flowchart of a do loop suppose that we want to check for inputs that are at least and at most modify the do loop for this check rewrite the input check using a while loop what is the disadvantage of your solution suppose c didn t have a do loop could you rewrite any do loop as a while loop write a do loop that reads integers and computes their sum stop when reading the value write a do loop that reads positive integers and computes their sum stop when reading the same value twice in a row for example if the input is then the sum is and the loop stops practice it now you can try these exercises at the end of the chapter processing input in this section you will learn how to read and pro cess a sequence of input values whenever you read a sequence of inputs you need to have some method of indicating the end of the sequence sometimes you are lucky and no input value can be zero then you can prompt the user to keep entering numbers or to finish the sequence if zero is allowed but negative numbers are not you can use to indicate termination a value that serves as a signal for termination is called a sentinel let put this technique to work in a program that computes the average of a set of salary values in our sample program we will use as a sentinel an employee would surely not work for a negative salary but there may be volunteers who work for free inside the loop we read an input if the input is not we process it in order to compute the aver age we need the total sum of all salaries and the number of inputs while in the military a sentinel guards a border or passage in computer science a sentinel value denotes the end of an input sequence or the border between input sequences cin salary if salary sum sum salary count we stay in the loop while the sentinel value is not detected while salary there is just one problem when the loop is entered for the first time no data value has been read be sure to initialize salary with some value other than the sentinel double salary any value other than will do alternatively use a do loop do while salary the following program reads inputs until the user enters the sentinel and then com putes and prints the average sentinel cpp program run numeric sentinels only work if there is some restriction on the input in many cases though there isn t suppose you want to compute the average of a data set that may contain or negative values then you cannot use or to indicate the end of the input in such a situation you can read input data until input fails as you have seen in section the condition cin fail is true if the preceding input has failed for example suppose that the input was read with these statements double value cin value if the user enters a value that is not a number such as q then the input fails we now encounter an additional complexity you only know that input failed after you have entered the loop and attempted to read it to remember the failure use a boolean variable cout enter values q to quit bool more true while more cin value if cin fail more false else process value some programmers dislike the introduction of a boolean variable to control a loop special topic on page shows an alternative mechanism for leaving a loop how ever when reading input there is an easier way the expression cin value can be used in a condition it evaluates to true if cin has not failed after reading value therefore you can read and process a set of inputs with the following loop cout enter values q to quit while cin value process value this loop is suitable for processing a single sequence of inputs you will learn more about reading inputs in chapter what does the sentinel cpp program print when the user immediately types when prompted for a value why does the sentinel cpp program have two checks of the form salary what would happen if the definition of the salary variable in sentinel cpp was changed to double salary we prompt the user enter values q to quit what happens when the user enters a different letter what is wrong with the following loop for reading a sequence of values cout enter values q to quit while cin fail double value cin value sum sum value count practice it now you can try these exercises at the end of the chapter special topic redirection of input and output consider the sentinel program that computes the average value of an input sequence if you use such a program then it is quite likely that you already have the values in a file and it seems a shame that you have to type them all in again the command line interface of your operating system provides a way to link a file to the input of a program as if all the characters in the file had actually been typed by a user if you type sentinel numbers txt the program is executed its input instructions no longer expect input from the keyboard all input commands get their input from the file numbers txt this process is called input redirection input redirection is an excellent tool for testing programs when you develop a program and fix its bugs it is boring to keep entering the same input every time you run the program spend a few minutes putting the inputs into a file and use redirection you can also redirect output in this program that is not terribly useful if you run sentinel numbers txt output txt the file output txt contains the input prompts and the output such as enter a value to finish enter a value to finish enter a value to finish enter a value to finish average however redirecting output is obviously useful for programs that produce lots of output you can print the file containing the output or edit it before you turn it in for grading problem solving storyboards when you design a program that interacts with a user you need to make a plan for that interaction what information does the user provide and in which order what information will your program display and in which format what should happen when there is an error when does the program quit this planning is similar to the development of a movie or a computer game where storyboards are used to plan action sequences a storyboard is made up of panels that show a sketch of each step annotations explain what is happening and note any spe cial situations storyboards are also used to develop software see figure making a storyboard is very helpful when you begin designing a program you need to ask yourself which information you need in order to compute the answers that the program user wants you need to decide how to present those answers these are important considerations that you want to settle before you design an algorithm for computing the answers let look at a simple example we want to write a program that helps users with questions such as how many tablespoons are in a pint or how many inches are centimeters what information does the user provide the quantity and unit to convert from the unit to convert to problem solving storyboards figure storyboard for the design of a web application what if there is more than one quantity a user may have a whole table of centimeter values that should be converted into inches what if the user enters units that our program doesn t know how to handle such as angstrom what if the user asks for impossible conversions such as inches to gallons let get started with a storyboard panel it is a good idea to write the user inputs in a different color underline them if you don t have a color pen handy the storyboard shows how we deal with a potential confusion a user who wants to know how many inches are centimeters may not read the first prompt carefully and specify inches but then the output is in cm alerting the user to the problem the storyboard also raises an issue how is the user supposed to know that cm and in are valid units would centimeter and inches also work what happens when the user enters a wrong unit let make another storyboard to demonstrate error handling to eliminate frustration it is better to list the units that the user can supply we switched to a shorter prompt to make room for all the unit names exercise explores a different alternative there is another issue that we haven t addressed yet how does the user quit the program the first storyboard gives the impression that the program will go on forever we can ask the user after seeing the sentinel that terminates an input sequence exiting the program from unit in ft mi mm cm m km oz lb g kg tsp tbsp pint gal cm to unit in enter values terminated by zero cm in more conversions y n n program exits sentinel triggers the prompt to exit as you can see from this case study a storyboard is essential for developing a work ing program you need to know the flow of the user interaction in order to structure your program provide a storyboard panel for a program that reads a number of test scores and prints the average score the program only needs to process one set of scores don t worry about error handling google has a simple interface for converting units you just type the question and you get the answer make storyboards for an equivalent interface in a c program show the happy day scenario in which all goes well and show the handling of two kinds of errors consider a modification of the program in self check drop the lowest score before computing the average provide a storyboard for the situation in which a user only provides one score what is the problem with implementing the following storyboard in c produce a storyboard for a program that compares the growth of a investment for a given number of years under two interest rates practice it now you can try these exercises at the end of the chapter common loop algorithms in the following sections we discuss some of the most common algorithms that are implemented as loops you can use them as starting points for your loop designs sum and average value computing the sum of a number of inputs is a very common task keep a running total a variable to which you add each input value of course the total should be ini tialized with double total double input while cin input total total input to compute an average count how many values you have and divide by the count be sure to check that the count is not zero double total int count double input while cin input total total input count double average if count average total count counting matches you often want to know how many values fulfill a particular condition for example you may want to count how many spaces are in a string keep a counter a variable that is initialized with and incremented whenever there is a match int spaces for int i i str length i string ch str substr i if ch spaces for example if str is the string my fair lady spaces is incremented twice when i is and note that the spaces variable is declared outside the loop we want the loop to update a single variable the ch variable is declared inside the loop a separate vari able is created for each iteration and removed at the end of each loop iteration this loop can also be used for scanning inputs the following loop reads text a word at a time and counts the number of words with at most three letters int string input while cin input if input length in a loop that counts matches a counter is incremented whenever a match is found finding the first match when you count the values that fulfill a con dition you need to look at all values how ever if your task is to find a match then you can stop as soon as the condition is fulfilled here is a loop that finds the first space in a string because we do not visit all elements in the string a while loop is a better choice than a for loop bool found false int position while found position str length string ch str substr position if ch found true else position when searching you look at items until a match is found if a match was found then found is true and position is the index of the first match if the loop did not find a match then found remains false after the end of the loop in the preceding example we searched a string for a character that matches a con dition you can apply the same process for user input suppose you are asking a user to enter a positive value keep asking until the user provides a correct input bool valid false double input while valid cout please enter a positive value cin input if input input valid true else cout invalid input endl note that the variable input is declared outside the while loop because you will want to use the input after the loop has finished if it had been declared inside the loop body you would not be able to use it outside the loop maximum and minimum to compute the largest value in a sequence keep a variable that stores the largest ele ment that you have encountered and update it when you find a larger one double largest cin largest double input while cin input if input largest largest input this algorithm requires that there is at least one input to find the height of the tallest bus rider remember the largest value so far and update it whenever you see a taller one to compute the smallest value simply reverse the comparison double smallest cin smallest double input while cin input if input smallest smallest input comparing adjacent values when processing a sequence of values in a loop you sometimes need to compare a value with the value that just preceded it for example suppose you want to check whether a sequence of inputs contains adjacent duplicates such as now you face a challenge consider the typical loop for reading a value double input while cin input now input contains the current input how can you compare the current input with the preceding one at any time input contains the current input overwriting the previous one the answer is to store the previous input like this double input double previous while cin input if input previous cout duplicate input endl previous input when comparing adjacent values store the previous value in a variable one problem remains when the loop is entered for the first time previous has not yet been set you can solve this problem with an initial input operation outside the loop double input double previous cin previous while cin input if input previous cout duplicate input endl previous input what total is computed when no user input is provided in the algorithm in section how do you compute the total of all positive inputs what is the value of position when no match is found in the algorithm in section what is wrong with the following loop for finding the position of the first space in a string bool found false for int position found position str length position string ch str substr position if ch found true how do you find the last space in a string what is wrong with the following loop for finding the smallest input value double smallest double input while cin input if input smallest smallest input what happens with the algorithm in section when no input is provided at all practice it now you can try these exercises at the end of the chapter step decide what work must be done inside the loop every loop needs to do some kind of repetitive work such as reading another item updating a value such as a bank balance or total incrementing a counter if you can t figure out what needs to go inside the loop start by writing down the steps that you would take if you solved the problem by hand for example with the temperature reading problem you might write read first value read second value if second value is higher than the first set highest temperature to that value highest month to read next value if value is higher than the first and second set highest temperature to that value highest month to read next value if value is higher than the highest temperature seen so far set highest temperature to that value highest month to now look at these steps and reduce them to a set of uniform actions that can be placed into the loop body the first action is easy read next value the next action is trickier in our description we used tests higher than the first higher than the first and second higher than the highest temperature seen so far we need to settle on one test that works for all iterations the last formulation is the most general similarly we must find a general way of setting the highest month we need a variable that stores the current month running from to then we can formulate the second loop action if value is higher than the highest temperature set highest temperature to that value highest month to current month altogether our loop is loop read next value if value is higher than the highest temperature set highest temperature to that value highest month to current month increment current month step specify the loop condition what goal do you want to reach in your loop typical examples are has the counter reached the final value have you read the last input value has a value reached a given threshold in our example we simply want the current month to reach step determine the loop type we distinguish between two major loop types a definite or count controlled loop is executed a definite number of times in an indefinite or event controlled loop the number of iterations is not known in advance the loop is executed until some event happens a typical example of the latter is a loop that reads data until a sentinel is encountered if you know in advance how many times a loop is repeated use a for statement for other loops consider the loop condition do you need to complete one iteration of the loop body before you can tell when to terminate the loop in that case you should choose a do loop oth erwise use a while loop in our example we read temperature values therefore we choose a for loop step set up variables for entering the loop for the first time list all variables that are used and updated in the loop and determine how to initialize them commonly counters are initialized with or totals with in our example the variables are current month highest value highest month we need to be careful how we set up the highest temperature value we can t simply set it to after all our program needs to work with temperature values from antarctica all of which may be negative a good option is to set the highest temperature value to the first input value of course then we need to remember to only read in another values with the current month starting at we also need to initialize the highest month with after all in an australian city we may never find a month that is warmer than january step process the result after the loop has finished in many cases the desired result is simply a variable that was updated in the loop body for example in our temperature program the result is the highest month sometimes the loop computes values that contribute to the final result for example suppose you are asked to average the temperatures then the loop should compute the sum not the average after the loop has completed you are ready to compute the average divide the sum by the number of inputs here is our complete loop read first value store as highest value highest month for current month current month current month read next value if value is higher than the highest value set highest value to that value highest month to current month step trace the loop with typical examples hand trace your loop code as described in section choose example values that are not too complex executing the loop times is enough to check for the most common errors pay special attention when entering the loop for the first and last time sometimes you want to make a slight modification to make tracing feasible for example when hand tracing the investment doubling problem use an interest rate of percent rather than percent when hand tracing the temperature loop use data values not let say the data are here is the walkthrough current month current value highest month highest value the trace demonstrates that highest month and highest value are properly set step implement the loop in c here the loop for our example exercise asks you to complete the program double cin int for int double cin if next_value cout endl available online at www wiley com college horstmann nested loops in section you saw how to nest two if statements similarly complex iterations sometimes require a nested loop a loop inside another loop statement when pro cessing tables nested loops occur naturally an outer loop iterates over all rows of the table an inner loop deals with the columns in the current row in this section you will see how to print a table for simplicity we will simply print powers xn as in the table at right here is the pseudocode for printing the table print table header for x from to print table row print endl how do you print a table row you need to print a value for each exponent this requires a second loop for n from to print xn this loop must be placed inside the preceding loop we say that the inner loop is nested inside the outer loop see figure figure flowchart of a nested loop the hour and minute displays in a digital clock are an example of nested loops the hours loop times and for each hour the minutes loop times there are rows in the outer loop for each x the program prints four columns in the inner loop thus a total of values are printed following is the complete program note that we also use loops to print the table header however those loops are not nested powtable cpp program run x x x x nested loops output explanation for i i i prints rows of for j j j cout cout endl asterisks each for i i i prints rows of for j j j cout asterisks each cout endl for i i i prints rows of for j j i j cout lengths and cout endl for i i i for j j j if j cout else cout cout endl prints asterisks in even columns dashes in odd columns for i i i for j j j if i j cout else cout cout endl prints a checkerboard pattern why is there a statement cout endl in the outer loop but not in the inner loop how would you change the program so that all powers from to are displayed if you make the change in self check how many values are displayed what do the following nested loops display for int i i i for int j j j cout i j cout endl write nested loops that make the following pattern of brackets practice it now you can try these exercises at the end of the chapter random numbers and simulations a simulation program uses the computer to simulate an activity in the real world or an imaginary one simulations are commonly used for predicting climate change analyzing traffic picking stocks and many other applications in science and busi ness in the following sections you will learn how to implement simulations that model phenomena with a degree of randomness generating random numbers many events in the real world are difficult to predict with absolute precision yet we can sometimes know the average behavior quite well for example a store may know from experience that a customer arrives every five minutes of course that is an aver age customers don t arrive in five minute intervals to accurately model customer traffic you want to take that random fluctuation into account now how can you run such a simulation in the computer the c library has a random number generator which produces numbers that appear to be completely random calling rand yields a random integer between and which is an implementation dependent constant typically but not always the largest valid int value call rand again and you get a different number the rand function is declared in the cstdlib header the following program calls the rand function ten times random cpp program run actually the numbers are not completely random they are drawn from sequences of numbers that don t repeat for a long time these sequences are actually computed from fairly simple formulas they just behave like random numbers for that reason they are often called pseudorandom numbers try running the program again you will get the exact same output this confirms that the random numbers are generated by formulas however when running simu lations you don t always want to get the same results to overcome this problem specify a seed for the random number sequence every time you use a new seed the random number generator starts generating a new sequence the seed is set with the srand function a simple value to use as a seed is the current time srand time simply make this call once in your program before generating any random numbers then the random numbers will be different in every program run also include the ctime header that declares the time function simulating die tosses in actual applications you need to transform the output from the random number generator into different ranges for exam ple to simulate the throw of a die you need random numbers between and here is the general recipe for computing random integers between two bounds a and b as you know from program ming tip on page there are b a values between a and b including the bounds themselves first compute rand b a to obtain a random value between and b a then add a yielding a random value between a and b int r rand b a a here is a program that simulates the throw of a pair of dice dice cpp program run the monte carlo method the monte carlo method is an inge nious method for finding approxi mate solutions to problems that can not be precisely solved the method is named after the famous casino in monte carlo here is a typical example it is difficult to compute the number but you can approxi mate it quite well with the following simulation simulate shooting a dart into a square surrounding a circle of radius that is easy generate random x and y coordinates between and if the generated point lies inside the circle we count it as a hit that is the case when because our shots are entirely random we expect that the ratio of hits tries is approximately equal to the ratio of the areas of the circle and the square that is therefore our y estimate for is hits tries this method yields an esti mate for using nothing but simple arithmetic to run the monte carlo simulation you have to work a little harder with random number generation when x you throw a die it has to come up with one of six faces when throwing a dart however there are many possible outcomes you must generate a random floating point number first generate the following value double r rand between and the value r is a random floating point value between and you have to multiply by to ensure that one of the operands of the operator is a floating point number the division rand would be an integer division see common error to generate a random value between and you compute double x r between and as r ranges from to x ranges from to here is the program that carries out the simulation montecarlo cpp program run estimate for pi how do you simulate a coin toss with the rand function how do you simulate the picking of a random playing card why does the dice cpp file include the ctime header in many games you throw a pair of dice to get a value between and what is wrong with this simulated throw of a pair of dice int sum rand how do you generate a random floating point number between and practice it now you can try these exercises at the end of the chapter random fact software piracy as you read this you have written a few computer programs and you have experienced firsthand how much effort it takes to write even the humblest of programs writing a real software prod uct such as a financial application or a computer game takes a lot of time and money few people and fewer compa nies are going to spend that kind of time and money if they don t have a rea sonable chance to make more money from their effort actually some com panies give away their software in the hope that users will upgrade to more elaborate paid versions or pay for con sulting other companies give away the software that enables users to read and use files but sell the software needed to create those files finally there are individuals who donate their time out of enthusiasm and produce programs that you can copy freely see random fact for more information when selling software a company must rely on the honesty of its cus tomers it is an easy matter for an unscrupulous person to make copies of computer programs without paying for them in most countries that is ille gal most governments provide legal protection such as copyright laws and patents to encourage the develop ment of new products countries that tolerate widespread piracy have found that they have an ample cheap supply of foreign software but no local man ufacturers willing to design good soft ware for their own citizens such as word processors in the local script or financial programs adapted to the local tax laws when a mass market for software first appeared vendors were enraged by the money they lost through piracy they tried to fight back by various schemes to ensure that only the legiti mate owner could use the software such as dongles devices that must be attached to a printer port before the software will run legitimate users hated these measures they paid for the software but they had to suffer through inconveniences such as hav ing multiple dongles stick out from their computer in the united states market pressures forced most vendors to give up on these copy protection schemes but they are still common place in other parts of the world because it is so easy and inexpen sive to pirate software and the chance of being found out is minimal you have to make a moral choice for your self if a package that you would really like to have is too expensive for your budget do you steal it or do you stay honest and get by with a more afford able product of course piracy is not limited to software the same issues arise for other digital products as well you may have had the opportunity to obtain copies of songs or movies without payment or you may have been frustrated by a copy protec tion device on your music player that made it difficult for you to listen to songs that you paid for admittedly it can be difficult to have a lot of sym pathy for a musical ensemble whose publisher charges a lot of money for what seems to have been very little effort on their part at least when compared to the effort that goes into designing and implementing a soft ware package nevertheless it seems only fair that artists and authors receive some compensation for their efforts how to pay artists authors and programmers fairly without burdening honest customers is an unsolved problem at the time of this writing and many computer scientists are engaged in research in this area chapter summary explain the flow of execution in a loop loops execute a block of code repeatedly while a condition remains true an off by one error is a common error when programming loops think through simple test cases to avoid this type of error use the technique of hand tracing to analyze the behavior of a program hand tracing is a simulation of code execution in which you step through instructions and track the values of the variables hand tracing can help you understand how an unfamiliar algorithm works hand tracing can show errors in code or pseudocode use for loops for implementing counting loops the for loop is used when a value runs from a starting point to an ending point with a constant increment or decrement choose between the while loop and the do loop the do loop is appropriate when the loop body must be executed at least once implement loops that read sequences of input data a sentinel value denotes the end of a data set but it is not part of the data you can use a boolean variable to control a loop set the variable to true before entering the loop then set it to false to leave the loop use input redirection to read input from a file use output redirection to capture program output in a file use the technique of storyboarding for planning user interactions a storyboard consists of annotated sketches for each step in an action sequence developing a storyboard helps you understand the inputs and outputs that are required for a program know the most common loop algorithms to compute an average keep a total and a count of all values to count values that fulfill a condition check all values and increment a counter for each match if your goal is to find a match exit the loop when the match is found to find the largest value update the largest value seen so far whenever you see a larger one to compare adjacent inputs store the preceding input in a variable use nested loops to implement multiple levels of iteration when the body of a loop contains another loop the loops are nested a typical use of nested loops is printing a table with rows and columns apply loops to the implementation of simulations in a simulation you use the computer to simulate an activity you can introduce randomness by calling the random number generator provide trace tables for these loops a int i int j int n while i j i j n b int i int j int n while i i n n i j j c int i int j int n while i i j n n i j d int i int j int n while i j i i j j n what do these loops print a for int i i i cout i b for int i i i cout i c for int i i i cout i d for int i i i cout i e for int i i i i cout i f for int i i i if i cout i what is an infinite loop on your computer how can you terminate a program that executes an infinite loop what is an off by one error give an example from your own programming experience write a program trace for the pseudocode in exercise assuming the input values are is the following code legal for int i i i for int i i i cout i cout endl what does it print is it good coding style if not how would you improve it how often do the following loops execute assume that i is not changed in the loop body a for int i i i b for int i i i c for int i i i d for int i i i e for int i i i f for int i i i i g for int i i i i write pseudocode for a program that prints a calendar such as the following su m t w th f sa write pseudocode for a program that prints a celsius fahrenheit conversion table such as the following celsius fahrenheit 212 write pseudocode for a program that reads a sequence of student records and prints the total score for each student each record has the student first and last name followed by a sequence of test scores and a sentinel of the sequence is termi nated by the word end here is a sample sequence harry morgan sally lin 95 end provide a trace table for this sample input rewrite the following for loop into a while loop int for int i i i i rewrite the following do while loop into a while loop int n cin n double x double do n n n x x while provide trace tables of the following loops a int int n while n n b int for int n n n n c int int n do n n while n what do the following loops print work out the answer by tracing the code not by using the computer a int for int n n n n cout b int for int n cout n n n c int int n for n n n n n cout n what do the following program segments print find the answers by tracing the code not by using the computer a int n for int i i i n n i cout n b int i double n for i i i n n i cout i c double x double y int i do y y x x y i while x cout i d double x double y int i while y x x y x y i cout i give an example of a for loop where symmetric bounds are more natural give an example of a for loop where asymmetric bounds are more natural add a storyboard panel for the conversion program in section on page that shows a scenario where a user enters incompatible units in section we decided to show users a list of all valid units in the prompt if the program supports many more units this approach is unworkable give a storyboard panel that illustrates an alternate approach if the user enters an unknown unit a list of all known units is shown change the storyboards in section to support a menu that asks users whether they want to convert units see program help or quit the program the menu should be displayed at the beginning of the program when a sequence of values has been converted and when an error is displayed draw a flow chart for a program that carries out unit conversions as described in section in section the code for finding the largest and smallest input initializes the largest and smallest variables with an input value why can t you initialize them with zero what are nested loops give an example where a nested loop is typically used the nested loops for int i i height i for int j j width j cout cout endl display a rectangle of a given width and height such as write a single for loop that displays the same rectangle suppose you design an educational game to teach children how to read a clock how do you generate random values for the hours and minutes in a travel simulation harry will visit one of his friends that are located in three states he has ten friends in california three in nevada and two in utah how do you produce a random number between and denoting the destination state with a probability that is proportional to the number of friends in each state write programs with loops that compute a the sum of all even numbers between and inclusive b the sum of all squares between and inclusive c all powers of from up to d the sum of all odd numbers between a and b inclusive where a and b are inputs e the sum of all odd digits of an input for example if the input is the sum would be write programs that read a sequence of integer inputs and print a the smallest and largest of the inputs b the number of even and odd inputs c cumulative totals for example if the input is the program should print d all adjacent duplicates for example if the input is the program should print write programs that read a line of input as a string and print a only the uppercase letters in the string b every second letter of the string c the string with all vowels replaced by an underscore d the number of vowels in the string e the positions of all vowels in the string complete the program in how to on page your program should read twelve temperature values and print the month with the highest temperature credit card number check the last digit of a credit card number is the check digit which protects against transcription errors such as an error in a single digit or switching two digits the following method is used to verify actual credit card numbers but for simplicity we will describe it for numbers with digits instead of starting from the rightmost digit form the sum of every other digit for example if the credit card number is then you form the sum double each of the digits that were not included in the preceding step add all digits of the resulting numbers for example with the number given above doubling the digits starting with the next to last one yields adding all digits in these values yields add the sums of the two preceding steps if the last digit of the result is the number is valid in our case so the number is valid write a program that implements this algorithm the user should supply an digit number and you should print out whether the number is valid or not if it is not valid you should print out the value of the check digit that would make the number valid currency conversion write a program that first asks the user to type today ex change rate between u s dollars and japanese yen then reads u s dollar values and converts each to yen use as a sentinel write a program that first asks the user to type in today exchange rate between u s dollars and japanese yen then reads u s dollar values and converts each to japanese yen use as the sentinel value to denote the end of dollar inputs then the program reads a sequence of yen amounts and converts them to dollars the second sequence is terminated by another zero value write a program that reads a set of floating point values ask the user to enter the values then print the average of the values the smallest of the values the largest of the values the range that is the difference between the smallest and largest of course you may only prompt for the values once translate the following pseudocode for finding the minimum value from a set of inputs into a c program set a boolean variable first to true while another value has been read successfully if first is true set the minimum to the value set first to false else if the value is less than the minimum set the minimum to the value print the minimum translate the following pseudocode for randomly permuting the characters in a string into a c program read a word repeat word length times pick a random position i in the word pick a random position j i in the word swap the letters at positions j and i print the word to swap the letters construct substrings as follows first i then replace the string with middle j last first word substr j middle word substr i last write a program that reads a word and prints each character of the word on a sepa rate line for example if the user provides the input harry the program prints h a r r y write a program that reads a word and prints the word in reverse for example if the user provides the input harry the program prints yrrah write a program that reads a word and prints the number of vowels in the word for this exercise assume that a e i o u y are vowels for example if the user provides the input harry the program prints vowels write a program that reads a word and prints the number of syllables in the word for this exercise assume that syllables are determined as follows each sequence of vowels a e i o u y except for the last e in a word is a vowel however if that algo rithm yields a count of change it to for example word syllables harry hairy hare the write a program that reads a word and prints all substrings sorted by length for example if the user provides the input rum the program prints r u m ru um rum write a program that reads a number and prints all of its binary digits print the remainder number then replace the number with number keep going until the number is for example if the user provides the input the output should be mean and standard deviation write a program that reads a set of floating point data values choose an appropriate mechanism for prompting for the end of the data set when all values have been read print out the count of the values the average and the standard deviation the average of a data set xn is x xi n where xi xn is the sum of the input values the standard deviation is however this formula is not suitable for the task by the time the program has computed x the individual xi are long gone until you know how to save these values use the numerically less stable formula you can compute this quantity by keeping track of the count the sum and the sum of squares as you process the input values the fibonacci numbers are defined by the sequence fn fn fn reformulate that as fnew fibonacci numbers describe the growth of a rabbit population after that discard which is no longer needed and set to and to fnew repeat fnew an appropriate number of times implement a program that computes the fibonacci numbers in that way factoring of integers write a program that asks the user for an integer and then prints out all its factors for example when the user enters the program should print prime numbers write a program that prompts the user for an integer and then prints out all prime numbers up to that integer for example when the user enters the program should print recall that a number is a prime number if it is not divisible by any number except and itself write a program that prints a multiplication table like this write a program that reads an integer and displays using asterisks a filled and hollow square placed next to each other for example if the side length is the program should display write a program that reads an integer and displays using asterisks a filled diamond of the given side length for example if the side length is the program should display the game of nim this is a well known game with a number of variants the following variant has an interesting winning strategy two players alternately take marbles from a pile in each move a player chooses how many marbles to take the player must take at least one but at most half of the marbles then the other player takes a turn the player who takes the last marble loses you will write a program in which the computer plays against a human opponent generate a random integer between and to denote the initial size of the pile generate a random integer between and to decide whether the computer or the human takes the first turn generate a random integer between and to decide whether the computer plays smart or stupid in stupid mode the computer simply takes a random legal value between and n from the pile whenever it has a turn in smart mode the computer takes off enough marbles to make the size of the pile a power of two minus that is or that is always a legal move except when the size of the pile is currently one less than a power of two in that case the computer makes a random legal move you will note that the computer cannot be beaten in smart mode when it has the first move unless the pile size happens to be or of course a human player who has the first turn and knows the winning strategy can win against the computer the drunkard walk a drunkard in a grid of streets randomly picks one of four directions and stumbles to the next intersection then again randomly picks one of four directions and so on you might think that on average the drunkard doesn t move very far because the choices cancel each other out but that is actually not the case represent locations as integer pairs x y implement the drunkard walk over intersections and print the beginning and ending location the monty hall paradox marilyn vos savant described the following problem loosely based on a game show hosted by monty hall in a popular magazine suppose you re on a game show and you re given the choice of three doors behind one door is a car behind the others goats you pick a door say no and the host who knows what behind the doors opens another door say no which has a goat he then says to you do you want to pick door no is it to your advan tage to switch your choice ms vos savant proved that it is to your advantage but many of her readers includ ing some mathematics professors disagreed arguing that the probability would not change because another door was opened your task is to simulate this game show in each iteration randomly pick a door number between and for placing the car randomly have the player pick a door randomly have the game show host pick one of the two doors having a goat now increment a counter for strategy if the player wins by switching to the third door and increment a counter for strategy if the player wins by sticking with the original choice run iterations and print both counters the buffon needle experiment the following experiment was devised by comte georges louis leclerc de buffon a french naturalist a needle of length inch is dropped onto paper that is ruled with lines inches apart if the needle drops onto a line we count it as a hit see figure buffon conjectured that the quotient tries hits approximates figure the buffon needle experiment figure a hit in the buffon needle experiment yhigh ylow for the buffon needle experiment you must generate two random numbers one to describe the starting position and one to describe the angle of the needle with the x axis then you need to test whether the needle touches a grid line generate the lower point of the needle its x coordinate is irrelevant and you may assume its y coordinate ylow to be any random number between and the angle between the needle and the x axis can be any value between degrees and degrees radians the upper end of the needle has y coordinate yhigh ylow sin the needle is a hit if yhigh is at least as shown in figure stop after tries and print the quotient tries hits this program is not suitable for computing the value of you need in the computation of the angle engineering in a predator prey simulation you compute the populations of predators and prey using the following equations preyn predn preyn a b predn predn c d preyn here a is the rate at which prey birth exceeds natural death b is the rate of predation c is the rate at which predator deaths exceed births without food and d represents predator increase in the presence of food write a program that prompts users for these rates the initial population sizes and the number of periods then print the populations for the given number of periods as inputs try a b c and d with initial prey and predator populations of and engineering projectile flight suppose a cannonball is propelled straight into the air with a starting velocity any calculus book will state that the position of the ball after t seconds is t where g m is the gravitational force of the earth no calculus book ever mentions why someone would want to carry out such an obviously dangerous experiment so we will do it in the safety of the computer in fact we will confirm the theorem from calculus by a simulation in our simula tion we will consider how the ball moves in very short time intervals t in a short time interval the velocity v is nearly constant and we can compute the distance the ball moves as v t in our program we will simply set const double and update the position by v the velocity changes constantly in fact it is reduced by the gravitational force of the earth in a short time interval v g t we must keep the velocity updated as v v g in the next iteration the new velocity is used to update the distance now run the simulation until the cannonball falls back to the earth get the initial velocity as an input m sec is a good value update the position and velocity times per second but print out the position only every full second also printout the values from the exact formula t for comparison note you may wonder whether there is a benefit to this simulation when an exact formula is available well the formula from the calculus book is not exact actually the gravitational force diminishes the farther the cannonball is away from the surface of the earth this complicates the algebra sufficiently that it is not possible to give an exact formula for the actual motion but the computer simulation can simply be extended to apply a variable gravitational force for cannonballs the calculus book formula is actually good enough but computers are necessary to compute accurate trajectories for higher flying objects such as ballistic missiles engineering a simple model for the hull of a ship is given by b z y l t where b is the beam l is the length and t is the draft note there are two values of y for each x and z because the hull is symmetric from starboard to port the cross sectional area at a point x is called the section in nautical parlance to compute it let z go from to t in n increments each of size t n for each value of z compute the value for y then sum the areas of trapezoidal strips at right are the strips where n write a program that reads in values for b l t x and n and then prints out the cross sectional area at x engineering radioactive decay of radioactive materials can be modeled by the equation a t log h where a is the amount of the material at time t is the amount at time and h is the half life technetium is a radioisotope that is used in imaging of the brain it has a half life of hours your program should display the relative amount a in a patient body every hour for hours after receiving a dose engineering the photo at left shows an electric device called a transformer transformers are often constructed by wrapping coils of wire around a ferrite core the figure below illustrates a situation that occurs in various audio devices such as cell phones and music players in this circuit a transformer is used to connect a speaker to the output of an audio amplifier the symbol used to represent the transformer is intended to suggest two coils of wire the parameter n of the transformer is called the turns ratio of the trans former the number of times that a wire is wrapped around the core to form a coil is called the number of turns in the coil the turns ratio is literally the ratio of the number of turns in the two coils of wire when designing the circuit we are concerned primarily with the value of the power delivered to the speakers that power causes the speakers to produce the sounds we want to hear suppose we were to connect the speakers directly to the amplifier without using the transformer some fraction of the power available from the amplifier would get to the speakers the rest of the available power would be lost in the amplifier itself the transformer is added to the circuit to increase the fraction of the amplifier power that is delivered to the speakers the power ps delivered to the speakers is calculated using the formula ps rs nvs n rs write a c program that models the circuit shown and varies the turns ratio from to in increments then determines the value of the turns ratio that maxi mizes the power delivered to the speakers years years add a statement cout balance endl as the last statement in the while loop the program prints the same output this is because the balance after years is slightly below and after years it is slightly above note that the value 128 is printed even though it is larger than n output n output there is a comma after the last value usually commas are between values only a n r i the code computes an n output this is an infinite loop n is never equal to count n this yields the correct answer the number has digits count n this yields the wrong answer the number also has digits the loop condition should have been while temp int year while year nyears balance balance rate cout setw year setw balance endl year numbers for int i i i i cout n endl int sum for int i i n i sum sum i for int year balance year however it is best not to use a for loop in this case because the loop condition does not relate to the year variable a while loop would be a better choice do cout enter a value between and cin value while value value int value while value cout enter a value cin value here the variable value had to be initialized with an artificial value to ensure that the loop is entered at least once yes the do loop do body while condition is equivalent to this while loop bool first true while first condition body first false int x int sum do cin x sum sum x while x int x int previous do previous x cin x sum sum x while previous x no data the first check ends the loop after the sentinel has been read the second check ensures that the sentinel is not processed as an input value the while loop would never be entered the user would never be prompted for in put since count stays the program would then print no data the stream also fails a more accurate prompt would have been enter values a key other than a digit to quit but that might be more confusing to the program user who would need now ponder which key to choose you don t know whether the input fails until after you try reading input computing the average simple conversion unknown unit program doesn t understand question syntax one score is not enough it would not be possible to implement this interface using the c features we have covered up to this point there is no way for the program to know when the first set of inputs ends when you read numbers with cin value it is your choice whether to put them on a single line or multiple lines comparing two interest rates year 10500 00 00 12155 14641 00 12762 the total is zero double total double input while cin input if input total total input position is str length the loop will stop when a match is found but you cannot access the match because position is not defined outside the loop start the loop at the end of string bool found false int position str length while found position string ch str substr position if ch found true else position unless the input contains zero or negative numbers the smallest value is incorrectly computed as when executing cin previous cin fails and previous is unchanged the statement cin input also fails and the while loop is never entered all values in the inner loop should be displayed on the same line change lines and to for int n n nmax n change nmax to the outer loop is executed times and the inner loop times 2345 for int i i i for int j j j cout cout endl compute rand and use for heads for tails or the other way around compute rand and associate the numbers with the four suits then com pute rand and associate the numbers with jack ace queen and king it is required for calling the time function the call will produce a value between and but all values have the same proba bility when throwing a pair of dice the number is six times as likely as the number the correct formula is int sum rand rand rand this page intentionally left blank to appreciate the importance of function comments to develop strategies for decomposing complex tasks into simpler ones to be able to determine the scope of a variable to recognize when to use value and reference parameters functions as black boxes implementing functions syntax function definition programming tip function comments parameter passing programming tip do not modify parameter variables return values common error missing return value special topic function declarations how to implementing a function worked example matching and replacing parts of a string worked example using a debugger functions without return values problem solving reusable functions problem solving stepwise refinement programming tip keep functions short programming tip tracing functions programming tip stubs worked example calculating a course grade variable scope and global variables programming tip avoid global variables reference parameters programming tip prefer return values to reference parameters special topic constant references recursive functions optional how to thinking recursively random fact the explosive growth of personal computers functions as black boxes a function is a sequence of instructions with a name you have already encountered several functions for example the function named pow which was introduced in chapter contains instructions to compute a power xy moreover every c pro gram has a function called main you call a function in order to execute its instructions for example consider the following program int main double z pow by using the expression pow main calls the pow function asking it to compute the power the main function is temporarily suspended the instructions of the pow function execute and compute the result the pow function returns its result that is the value back to main and the main function resumes execution see figure figure execution flow during a function call functions as black boxes figure the pow function as a black box when another function calls the pow function it provides inputs such as the expressions and in the call pow these expressions are called arguments this terminology avoids confusion with other inputs such as those provided by a human user similarly the output that the pow function computes is called the return value functions can have multiple arguments but they have only one return value note that the return value of a function is returned to the calling function not dis played on the screen for example suppose your program contains a statement double z pow when the pow function returns its result the return value is stored in the variable z if you want the value to be displayed you need to add a statement such as cout z at this point you may wonder how the pow function performs its job for example how does pow compute that is by multiplying with logarithms fortunately as a user of the function you don t need to know how the function is implemented you just need to know the specification of the function if you provide arguments x and y the function returns xy engineers use the term black box for a device with a given specification but unknown implementation you can think of pow as a black box as shown in figure when you design your own functions you will want to make them appear as black boxes to other programmers those programmers want to use your functions with out knowing what goes on inside even if you are the only person working on a pro gram making each function into a black box pays off there are fewer details that you need to keep in mind although a thermostat is usually white you can think of it as a black box the input is the desired temperature and the output is a signal to the heater or air conditioner consider the function call pow what are the arguments and return value what is the return value of the function call pow pow the ceil function in the c standard library takes a single argument x and returns the smallest integer x what is the return value of ceil it is possible to determine the answer to self check without knowing how the ceil function is implemented use an engineering term to describe this aspect of the ceil function practice it now you can try these exercises at the end of the chapter implementing functions in this section you will learn how to implement a function from a given specification we will use a very simple example a function to compute the volume of a cube with a given side length the function uses a given side length to compute the volume of a cube when writing this function you need to pick a name for the function declare a variable for each argument double these variables are called parameter variables specify the type of the return value double put all this information together to form the first line of the function definition double double next specify the body of the function the statements that are executed when the function is called the volume of a cube of side length is however for greater clarity our parameter variable has been called not so we need to compute length we will store this value in a variable called volume double volume in order to return the result of the function use the return statement return volume the body of a function is enclosed in braces here is the complete function double double double volume return volume implementing functions the return statement gives the function result to the caller let put this function to use we ll supply a main function that calls the function twice int main double double cout a cube with side length has volume endl cout a cube with side length has volume endl return when the function is called with different arguments the function returns different results consider the call the argument corresponds to the length parameter variable therefore in this call is the function com putes or when the function is called with a different argument say then the function computes now we combine both functions into a test program because main calls volume the function must be known before the main function is defined this is easily achieved by placing first and main last in the source file see special topic on page for an alternative syntax function definition here is the complete program note the comment that describes the behavior of the function programming tip on page describes the format of the comment cube cpp program run what is the value of what is the value of provide an alternate implementation of the body of the function by calling the pow function define a function that computes the area of a square of a given side length consider this function int mystery int x int y double result x y y x return result what is the result of the call mystery practice it now you can try these exercises at the end of the chapter programming tip function comments whenever you write a function you should comment its behavior comments are for human readers not compilers and there is no universal standard for the layout of a function com ment in this book we will use the following layout computes the volume of a cube param the side length of the cube return the volume double double double volume return volume this particular documentation style is borrowed from the java programming language it is widely supported by c tools as well for example by the doxygen tool www doxygen org the first line of the comment describes the purpose of the function each param clause describes a parameter variable and the return clause describes the return value note that the function comment does not document the implementation how the function does what it does but rather the design what the function does its inputs and its results the comment allows other programmers to use the function as a black box parameter passing in this section we examine the mechanism of passing arguments into functions when a function is called its parameter variables are created another commonly used term for a parameter variable is formal parameter in the function call an expression is supplied for each parameter variable called the argument another commonly used term for this expression is actual parameter each parameter vari able is initialized with the value of the corresponding argument consider the function call illustrated in figure double pie fruit pie fruit a recipe for a fruit pie may say to use any kind of fruit here fruit is an example of a parameter variable apples and cherries are examples of arguments figure parameter passing the parameter variable of the function is created the parameter variable is initialized with the value of the argument that was passed in the call in our case is set to the function computes the expression which has the value that value is stored in the variable volume the function returns all of its variables are removed the return value is trans ferred to the caller that is the function calling the function now consider what happens in a subsequent call a new parameter variable is created recall that the previous parameter variable was removed when the first call to returned it is initialized with the argument and the process repeats after the second function call is complete its variables are again removed like any other variables parameter variables can only be set to values of compat ible types for example the parameter variable of the function has type double it is valid to call or in the latter call the integer is automatically converted to the double value however a call volume two is not legal what does this program print use a diagram like figure to find the answer double mystery int x int y double z x y z z return z int main int a int b cout mystery a b endl what does this program print use a diagram like figure to find the answer int mystery int x int y x x return y int main int a cout mystery a endl what does the following program print use a diagram like figure to find the answer int mystery int n n n return n int main int a cout mystery a endl practice it now you can try these exercises at the end of the chapter return values you use the return statement to specify the result of a function when the return state ment is processed the function exits immediately this behavior is convenient for handling exceptional cases at the beginning double double if return double volume return volume if the function is called with a negative value for then the function returns and the remainder of the function is not executed see figure in the preceding example each return statement returned a constant or a variable actually the return statement can return the value of any expression instead of sav ing the return value in a variable and returning the variable it is often possible to eliminate the variable and return a more complex expression double double return it is important that every branch of a function return a value consider the following incorrect function double double if return error suppose you call with a negative value for the side length of course you aren t supposed to call that but it might happen as the result of a coding error because the if condition is not true the return statement is not executed however the func tion must return something depending on the circumstances the compiler might figure a return statement exits a function immediately flag this as an error or the function might return a random value protect against this problem by returning some safe value double double if return return the last statement of every function ought to be a return statement this ensures that some value gets returned when the function reaches the end special topic function declarations it is a compile time error to call a function that the compiler does not know just as it is an error to use an undefined variable you can avoid this error if you define all functions before they are first used first define lower level helper functions then the mid level workhorse functions and finally main in your program some programmers prefer to list the main function first in their programs if you share that preference you need to learn how to declare the other functions at the top of the program a declaration of a function lists the return type function name and parameter variables but it contains no body double double this is an advertisement that promises that the function is implemented elsewhere it is easy to distinguish declarations from definitions declarations end in a semicolon whereas defini tions are followed by a block declarations are also called prototypes in a function prototype the names of the parameters are optional you could also write double double however it is a good idea to include parameter names in order to document the purpose of each parameter the declarations of common functions such as pow are contained in header files if you have a look inside cmath you will find the declaration of pow and the other math functions step describe what the function should do provide a simple english description such as compute the volume of a pyramid whose base is a square step determine the function inputs make a list of all the parameters that can vary it is common for beginners to implement functions that are overly specific for example you may know that the great pyramid of giza the largest of the egyptian pyramids has a height of meters and a base length of meters you should not use these numbers in your calculation even if the original problem only asked about the great pyramid it is just as easy and far more useful to write a function that computes the volume of any pyramid in our case the parameters are the pyramid height and base length at this point we have enough information to document the function computes the volume of a pyramid whose base is a square param height the height of the pyramid param the length of one side of the pyramid base return the volume of the pyramid step determine the types of the parameter variables and the return value the height and base length can both be floating point numbers therefore we will choose the type double for both parameter variables the computed volume is also a floating point num ber yielding a return type of double therefore the function will be defined as double double height double step write pseudocode for obtaining the desired result in most cases a function needs to carry out several steps to find the desired answer you may need to use mathematical formulas branches or loops express your function in pseudocode an internet search yields the fact that the volume of a pyramid is computed as volume x height x base area since the base is a square we have base area base length x base length using these two equations we can compute the volume from the parameter variables step implement the function body in our example the function body is quite simple note the use of the return statement to return the result double return height step test your function after implementing a function you should test it in isolation such a test is called a unit test work out test cases by hand and make sure that the function produces the correct results for example for a pyramid with height and base length we expect the area to be if the height is we expect an area of int main cout volume endl cout expected cout volume endl cout expected return the output confirms that the function worked as expected volume expected volume expected functions without return values sometimes you need to carry out a sequence of instructions that does not yield a value if that instruction sequence occurs multiple times you will want to package it into a function in c you use the return type void to indicate the absence of a return value here is a typical example your task is to print a string in a box like this hello a void function returns no value but it can produce output however different strings can be substituted for hello a function for this task can be defined as follows void string str now you develop the body of the function in the usual way by formulating a general method for solving the task print a line that contains the character n times where n is the length of the string print a line containing the string surrounded with a to the left and right print another line containing the character n times here is the function implementation prints a string in a box param str the string to print void string str available online at www wiley com college horstmann functions without return values int n str length for int i i n i cout cout endl cout str endl for int i i n i cout cout endl note that this function doesn t compute any value it performs some actions and then returns to the caller see the sample program box cpp because there is no return value you cannot use in an expression you can call hello but not result hello error doesn t return a result if you want to return from a void function before reaching the end you use a return statement without a value for example void string str int n str length if n return return immediately how do you generate the following printout using the function hello world what is wrong with the following statement cout hello implement a function shout that prints a line consisting of a string followed by three exclamation marks for example shout hello should print hello the function should not return a value how would you modify the function to leave a space around the string that is being boxed like this hello the function contains the code for printing a line of characters twice place that code into a separate function and use that function to sim plify what is the code of both functions practice it now you can try these exercises at the end of the chapter problem solving reusable functions you have used many functions from the c standard library these functions have been provided as a part of standard c so that programmers need not recreate them of course the c library doesn t cover every conceivable need you will often be able to save yourself time by designing your own functions that can be used for mul tiple problems when you write nearly identical code or pseudocode multiple times either in the same program or in separate programs consider introducing a function here is a typical example of code replication int hours do cout enter a value between and cin hours while hours hours int minutes do cout enter a value between and cin minutes while minutes minutes this program segment reads two variables making sure that each of them is within a certain range it is easy to extract the common behavior into a function prompts a user to enter a value up to a given maximum until the user provides a valid input param high the largest allowable input return the value provided by the user between and high inclusive int int high int input do cout enter a value between and high cin input while input input high return input then use this function twice int hours int minutes we have now removed the replication of the loop it only occurs once inside the function note that the function can be reused in other programs that need to read integer values however we should consider the possibility that the smallest value need not always be zero problem solving reusable functions when carrying out the same task multiple times use a function here is a better alternative prompts a user to enter a value within a given range until the user provides a valid input param low the smallest allowable input param high the largest allowable input return the value provided by the user between low and high inclusive int int low int high int input do cout enter a value between low and high cin input while input low input high return input in our program we call int hours another program can call int month in general you will want to provide parameter variables for the values that vary when a function is reused consider the following statements int int total int int total introduce a function to reduce code duplication consider this code that prints a page number on the left or right side of a page if page cout page endl else cout setw page endl introduce a function with return type bool to make the condition in the if statement easier to understand consider the following function that computes compound interest for an account with an initial balance of and an interest rate of percent double balance int years return pow years how can you make this function more reusable the comment explains what the following loop does use a function instead counts the number of spaces int spaces for int i i input length i if input substr i spaces in self check you were asked to implement a function that counts spaces how can you generalize it so that it can count any character why would you want to do this practice it now you can try these exercises at the end of the chapter problem solving stepwise refinement one of the most powerful strategies for problem solving is the process of stepwise refinement to solve a difficult task break it down into simpler tasks then keep break ing down the simpler tasks into even simpler ones until you are left with tasks that you know how to solve here is an application of this process to a problem of everyday life you get up in the morning and simply must get coffee how do you get coffee you see whether you can get someone else such as your mother or mate to bring you some if that fails you must make coffee how do you make coffee a production process is broken down into sequences of assembly steps if there is instant coffee available you can make instant coffee how do you make instant coffee simply boil water and mix the boiling water with the instant coffee how do you boil water if there is a microwave then you fill a cup with water place it in the microwave and heat it for three minutes otherwise you fill a kettle with water and heat it on the stove until the water comes to a boil on the other hand if you don t have instant coffee you must brew coffee how do you brew coffee you add water to the coffee maker put in a filter grind coffee put the coffee in the filter and turn the cof fee maker on how do you grind coffee you add coffee beans to the coffee grinder and push the button for seconds figure shows a flowchart view of the coffee making solution refinements are shown as expanding boxes in c you implement a refinement as a function for example a function would call and it would be called from a function let us apply the process of stepwise refinement to a programming problem figure flowchart of coffee making solution when printing a check it is customary to write the check amount both as a number and as a text string two hundred seventy four dollars and cents doing so reduces the recipient temptation to add a few digits in front of the amount for a human this isn t particularly difficult but how can a computer do this there is no built in function that turns into two hundred seventy four we need to program this function here is the description of the function we want to write turns a number into its english name param number a positive integer return the name of number e g two hundred seventy four string int number how can this function do its job let look at a simple case first if the number is between and we need to compute one nine in fact we need the same computation again for the hundreds two hundred using the stepwise decomposition process we design another function for this simpler task again rather than imple menting the function we first write the comment turns a digit into its english name param digit an integer between and return the name of digit one nine string int digit this sounds simple enough to implement using an if statement with nine branches no further functions should be required for completing the function so we will worry about the implementation later numbers between and are special cases let have a separate function name that converts them into strings eleven twelve thirteen and so on turns a number between and into its english name param number an integer between and return the name of the number ten nineteen string int number next suppose that the number is between and then we show the tens as twenty thirty ninety for simplicity and consistency put that computation into a separate function gives the name of the tens part of a number between and param number an integer between and return the name of the tens part of the number twenty ninety string int number now suppose the number is at least and at most if the number is evenly divisi ble by we use and we are done otherwise we print the tens with name and the ones with if the number is between and then we show a digit the word hundred and the remainder as described previously here is the pseudocode of the algorithm part number the part that still needs to be converted name the name of the number if part name name of hundreds in part hundred remove hundreds from part if part append part to name remove tens from part else if part append part to name part if part append part to name this pseudocode has a number of important improvements over the verbal descrip tion it shows how to arrange the tests starting with the comparisons against the larger numbers and it shows how the smaller number is subsequently processed in further if statements on the other hand this pseudocode is vague about the actual conversion of the pieces just referring to name of hundreds and the like furthermore we were vague about spaces as it stands the code would produce strings with no spaces twohundredseventyfour for example compared to the complexity of the main problem one would hope that spaces are a minor issue it is best not to muddy the pseudocode with minor details now turn the pseudocode into real code the last three cases are easy because helper functions are already developed for them if part name name part part part else if part name name part part if part name name part finally let us tackle the case of numbers between and because part part is a single digit and we obtain its name by calling then we add the hundred suffix if part name part hundred part part now you have seen all the important building blocks for the function here is the complete program intname cpp if digit return one if digit return two if digit return three if digit return four if digit return five if digit return six if digit return seven if digit return eight if digit return nine return turns a number between and into its english name param number an integer between and return the name of the given number ten nineteen string int number if number return ten if number return eleven if number return twelve if number return thirteen if number return fourteen if number return fifteen if number return sixteen if number return seventeen if number return eighteen if number return nineteen return 45 gives the name of the tens part of a number between and param number an integer between and return the name of the tens part of the number twenty ninety string int number 52 if number return ninety if number return eighty if number return seventy if number return sixty if number return fifty if number return forty if number return thirty if number return twenty return turns a number into its english name param number a positive integer 000 return the name of the number e g two hundred seventy four string int number int part number the part that still needs to be converted string name the return value program run explain how you can improve the function so that it can handle argu ments up to why does line set part what happens when you call how can you change the function to handle this case correctly trace the function call as described in programming tip use the process of stepwise refinement to break down the task of printing the following table into simpler tasks i i i i practice it now you can try these exercises at the end of the chapter programming tip tracing functions when you design a complex set of functions it is a good idea to carry out a manual walk through before entrusting your program to the computer take an index card or some other piece of paper and write down the function call that you want to study write the name of the function and the names and values of the parameter vari ables like this then write the names and initial values of the function variables write them in a table since you will update them as you walk through the code we enter the test part part is and part is is easily seen to be four had been complicated you would have started another sheet of paper to figure out that function call it is quite common to accumulate several sheets in this way now name has changed to name part hundred that is four hun dred and part has changed to part or programming tip stubs when writing a larger program it is not always feasible to implement and test all functions at once you often need to test a function that calls another but the other function hasn t yet been implemented then you can temporarily replace the missing function with a stub a stub is a func tion that returns a simple value that is sufficient for testing another function here are examples of stub functions turns a digit into its english name param digit an integer between and return the name of digit one nine string int digit stubs are incomplete functions that can be used for testing return mumble gives the name of the tens part of a number between and param number an integer between and return the tens name of the number twenty ninety string int number return mumblety if you combine these stubs with the function and test it with an argument of you will get a result of mumble hundred mumblety mumble which indicates that the basic logic of the function is working correctly variable scope and global variables it is possible to define the same variable name more than once in a program when the variable name is used you need to know to which definition it belongs in this sec tion we discuss the rules for dealing with multiple definitions of the same name a variable that is defined within a function is visible from the point at which it is defined until the end of the block in which it was defined this area is called the scope of the variable consider the volume variables in the following example double double double volume return volume int main double volume cout volume endl return each volume variable is defined in a separate function and their scopes do not overlap in the same way that there can be a street named main street in different cities a c program can have multiple variables with the same name available online at www wiley com college horstmann variable scope and global variables it is not legal to define two variables with the same name in the same scope for example the following is not legal int main double volume double volume error cannot define another volume variable in this scope however you can define another variable with the same name in a nested block here we define two variables called amount the scope of the parameter variable amount is the entire function except inside the nested block inside the nested block amount refers to the variable that was defined in that block we say that the inner variable shadows the variable that is defined in the outer block you should avoid this potentially confusing situation in the functions that you write simply by renaming one of the variables variables that are defined inside functions are called local variables c also sup ports global variables variables that are defined outside functions a global variable is visible to all functions that are defined after it for example the iostream header defines global variables cin and cout here is an example of a global variable int balance a global variable void withdraw double amount if balance amount balance balance amount int main withdraw cout balance endl return the scope of the variable balance extends over both the withdraw and the main functions generally global variables are not a good idea when multiple functions update global variables the result can be difficult to predict particularly in larger programs that are developed by multiple programmers it is very important that the effect of each function be clear and easy to understand you should avoid global variables in your programs consider this sample program int x int mystery int x int for int i i x i int x i x return x int main x int mystery x cout endl which line defines a global variable which lines define local variables named x which lines are in the scope of the definition of x in line which variable is changed by the assignment in line this program defines two variables with the same name whose scopes don t overlap what are they practice it now you can try these exercises at the end of the chapter reference parameters if you want to write a function that changes the value of an argument you must use a reference parameter in order to allow the change we first explain why a different parameter type is necessary then we show you the syntax for reference parameters consider a function that simulates withdrawing a given amount of money from a bank account provided that sufficient funds are available if the amount of money is insufficient a penalty is deducted instead the function would be used as follows double withdraw now is withdraw insufficient funds now is here is a first attempt void withdraw double balance double amount does not work const double penalty if balance amount balance balance amount else balance balance penalty but this doesn t work let walk through the function call withdraw see figure as the function starts the parameter variable balance is created and set to the same value as and amount is set to then balance is modified of course that modification has no effect on because balance is a separate variable when the function returns balance is forgotten and no money was with drawn from figure when balance and account are value parameters main function withdraw function figure reference and value parameters the parameter variable balance is called a value parameter because it is initialized with the value of the supplied argument all functions that we have written so far use value parameters in this situation though we don t just want balance to have the same value as we want balance to refer to the actual variable account or or whatever variable is supplied in the call the contents of that variable should be updated you use a reference parameter when you want to update a variable that was sup plied in the function call when we make balance into a reference parameter then balance is not a new variable but a reference to an existing variable any change in bal ance is actually a change in the variable to which balance refers in that particular call figure shows the difference between value and reference parameters to indicate a reference parameter you place an after the type name void withdraw double balance double amount the type double is read a reference to a double or more briefly double ref the withdraw function now has two parameter variables one of type double ref and the other a value parameter of type double the body of the function is unchanged what has changed is the meaning of the assignments to the balance variable the assignment balance balance amount now changes the variable that was passed to the function see figure a reference parameter for a bank balance is like an atm card it allows you to change the balance in contrast a value parameter can only tell you the balance figure when balance is a reference parameter for example the call withdraw modifies the variable and the call withdraw modifies the variable the argument for a reference parameter must always be a variable it would be an error to supply a number withdraw error argument for reference parameter must be a variable the reason is clear the function modifies the reference parameter but it is impossi ble to change the value of a number for the same reason you cannot supply an expression withdraw 500 error argument for reference parameter must be a variable account cpp program run would the withdraw function work correctly if the amount parameter was defined as double instead of double the following function is intended to transfer the given amount of money from one account to another supply the function parameters void transfer if amount amount amount change the withdraw function so that it returns a bool value indicating whether the withdrawal was successful do not charge a penalty if the balance was insufficient write a function minmax so that the call minmax x y a b sets a to the smaller of x and y and b to the larger of x and y what does this program print void mystery int a int b a a b b b a a b a int main int x int y mystery x y cout x y endl practice it now you can try these exercises at the end of the chapter recursive functions optional a recursive function is a function that calls itself this is not as unusual as it sounds at first suppose you face the arduous task of cleaning up an entire house you may well say to yourself i ll pick a room and clean it and then i ll clean the other rooms in other words the cleanup task calls itself but with a simpler input eventually all the rooms will be cleaned in c a recursive function uses the same principle here is a typical example we want to print triangle patterns like this cleaning up a house can be solved recursively clean one room then clean up the rest specifically our task is to provide a function void int the triangle given above is printed by calling to see how recursion helps consider how a triangle with side length can be obtained from a triangle with side length print the triangle with side length print a line with four more generally for an arbitrary side length print the triangle with side length print a line with side length here is the pseudocode translated to c void int for int i i i cout cout endl there is just one problem with this idea when the side length is we don t want to call and so on the solution is simply to treat this as a special case and not to print anything when is less than void int if return for int i i i cout look at the function one more time and notice how utterly reasonable it is if the side length is nothing needs to be printed the next part is just as reason able print the smaller triangle and don t think about why that works then print a row of clearly the result is a triangle of the desired size there are two key requirements to make sure that the recursion is successful every recursive call must simplify the task in some way there must be special cases to handle the simplest tasks directly the function calls itself again with smaller and smaller side lengths eventually the side length must reach and the function stops calling itself here is what happens when we print a triangle with side length the call printtriangle calls printtriangle the call printtriangle calls printtriangle the call printtriangle calls printtriangle the call printtriangle calls printtriangle the call printtriangle returns doing nothing the call printtriangle prints the call printtriangle prints the call printtriangle prints the call prints the call pattern of a recursive function looks complicated and the key to the success ful design of a recursive function is not to think about it this set of russian dolls looks similar to the call pattern of a recursive function triangle cpp program run recursion is not really necessary to print triangle shapes you can use nested loops like this for int i i i for int j j i j cout cout endl however this pair of loops is a bit tricky many people find the recursive solution simpler to understand consider this slight modification of the function void int if return for int i i i cout what is the result of consider this recursive function int mystery int n if n return return n mystery n what is mystery consider this recursive function int mystery int n if n return return mystery n what is mystery write a recursive function for printing n box shapes in a row the function in section accepted arguments 000 using a recursive call extend its range to for example an input of 345 should return twelve thousand three hundred forty five practice it now you can try these exercises at the end of the chapter step break the input into parts that can themselves be inputs to the problem in your mind fix a particular input or set of inputs for the task that you want to solve and think how you can simplify the inputs look for simplifications that can be solved by the same task and whose solutions are related to the original task in the digit sum problem consider how we can simplify an input such as n would it help to subtract after all but consider n there seems to be no obvious relationship between and 999 a much more promising idea is to remove the last digit that is compute n the digit sum of is directly related to the digit sum of step combine solutions with simpler inputs into a solution of the original problem in your mind consider the solutions for the simpler inputs that you have discovered in step don t worry how those solutions are obtained simply have faith that the solutions are readily available just say to yourself these are simpler inputs so someone else will solve the problem for me in the case of the digit sum task ask yourself how you can obtain if you know you simply add the last digit and you are done how do you get the last digit as the remainder n the value n can therefore be obtained as n n don t worry how n is computed the input is smaller and therefore it just works step find solutions to the simplest inputs a recursive computation keeps simplifying its inputs to make sure that the recursion comes to a stop you must deal with the simplest inputs separately come up with special solutions for them that is usually very easy look at the simplest inputs for the test a number with a single digit random fact the explosive growth of personal computers in marcian e ted hoff an engi neer at intel corpo ration was working on a chip for a manufacturer of electronic calculators he realized that it would be a better idea to develop a general purpose chip that could be programmed to inter face with the keys and display of a cal culator rather than to do yet another custom design thus the microproces sor was born at the time its primary application was as a controller for cal culators washing machines and the like it took years for the computer industry to notice that a genuine cen tral processing unit was now available as a single chip hobbyists were the first to catch on in the first computer kit the altair was available from mits electronics for about the kit consisted of the microprocessor a cir cuit board a very small amount of memory toggle switches and a row of display lights purchasers had to sol der and assemble it then program it in machine language through the toggle switches it was not a big hit the first big hit was the apple ii it was a real computer with a keyboard a monitor and a floppy disk drive when it was first released users had a 000 machine that could play space invaders run a primitive bookkeep ing program or let users program it in basic the original apple ii did not even support lowercase letters mak ing it worthless for word processing the breakthrough came in with a new spreadsheet program visicalc in a spreadsheet you enter financial data and their relationships into a grid of rows and columns see the figure at right then you modify some of the data and watch in real time how the others change for example you can see how changing the mix of widgets in a manufacturing plant might affect estimated costs and profits middle managers in companies who under stood computers and were fed up with having to wait for hours or days to get their data runs back from the comput ing center snapped up visicalc and the computer that was needed to run it for them the computer was a spread sheet machine the next big hit was the ibm per sonal computer ever after known as the pc it was the first widely available personal computer that used intel bit processor the whose successors are still being used in per sonal computers today the success of the pc was based not on any engi neering breakthroughs but on the fact that it was easy to clone ibm published the computer specifications in order to encourage third parties to develop plug in cards perhaps ibm did not foresee that functionally equivalent versions of their computer could be recreated by others but a variety of pc clone vendors emerged and ulti mately ibm stopped selling personal computers ibm never produced an operating system for its pcs that is the soft ware that organizes the interaction between the user and the computer starts application programs and man ages disk storage and other resources instead ibm offered customers the option of three separate operating systems most customers couldn t care less about the operating system a number with a single digit is its own digit sum so you can stop the recursion when n and return n in that case or if you prefer you can be even lazier if n has a single digit then n n equals n you can simply terminate the recursion when n is zero step implement the solution by combining the simple cases and the reduction step now you are ready to implement the solution make separate cases for the simple inputs that you considered in step if the input isn t one of the simplest cases then implement the logic you discovered in step here is the complete function int int n special case for terminating the recursion if n return general case return n n they chose the system that was able to launch most of the few applications that existed at the time it happened to be dos disk operating system by microsoft microsoft licensed the same operating system to other hardware vendors and encouraged software companies to write dos applications a huge number of useful application programs for pc compatible machines was the result pc applications were certainly use ful but they were not easy to learn every vendor developed a different user interface the collection of key strokes menu options and settings that a user needed to master to use a software package effectively data exchange between applications was difficult because each program used a different data format the apple mac intosh changed all that in the designers of the macintosh had the vision to supply an intuitive user inter face with the computer and to force software developers to adhere to it it took microsoft and pc compatible manufacturers years to catch up most personal computers are used for accessing information from online sources entertainment word process ing and home finance some analysts predict that the personal computer will merge with the television set and cable network into an entertainment and information appliance the visicalc spreadsheet running on an apple ii understand the concepts of functions arguments and return values a function is a named sequence of instructions arguments are supplied when a function is called the return value is the result that the function computes be able to implement functions when defining a function you provide a name for the function a variable for each argument and a type for the result function comments explain the purpose of the function the meaning of the parameter variables and return value as well as any special requirements describe the process of parameter passing parameter variables hold the argument values supplied in the function call describe the process of returning a value from a function the return statement terminates a function call and yields the function result design and implement functions without return values use a return type of void to indicate that a function does not return a value develop functions that can be reused for multiple problems eliminate replicated code or pseudocode by defining a function design your functions to be reusable supply parameter variables for the values that can vary when the function is reused apply the design principle of stepwise refinement use the process of stepwise refinement to decompose complex tasks into simpler ones when you discover that you need a function write a description of the parameter variables and return values a function may require simpler functions to carry out its work determine the scope of variables in a program the scope of a variable is the part of the program in which it is visible a variable in a nested block shadows a variable with the same name in an outer block a local variable is defined inside a function a global variable is defined outside a function avoid global variables in your programs describe how reference parameters work modifying a value parameter has no effect on the caller a reference parameter refers to a variable that is supplied in a function call modifying a reference parameter updates the variable that was supplied in the call understand recursive function calls and implement simple recursive functions a recursive computation solves a problem by using the solution of the same problem with simpler inputs for a recursion to terminate there must be special cases for the simplest inputs the key to finding a recursive solution is reducing the input to a simpler input for the same problem when designing a recursive solution do not worry about multiple nested calls simply focus on reducing a problem to a slightly simpler one what is the difference between an argument and a return value how many argu ments can a function have how many return values in which sequence are the lines of the program cube cpp on page executed starting with the first line of main give examples of the following either from the c library or from the functions discussed in this chapter a a function with two double arguments and a double return value b a function with a double argument and a double return value c a function with two int arguments and an int return value d a function with an int argument and a string return value e a function with a string argument and no return value f a function with a reference parameter and no return value g a function with no arguments and an int return value true or false a a function has exactly one return statement b a function has at least one return statement c a function has at most one return value d a function with return value void never has a return statement e when executing a return statement the function exits immediately f a function with return value void must print a result g a function without arguments always returns the same value consider these functions double f double x return g x sqrt h x double g double x return h x double h double x return x x k x double k double x return x without actually compiling and running a program determine the results of the following function calls a double f b double g h c double k g h d double f f f e double f g h k write pseudocode for a function that translates a telephone number with letters in it such as 800 flowers into the actual phone number use the standard letters on a phone pad design a function that prints a floating point number as a currency value with a sign and two decimal digits a indicate how the programs cpp and invtable cpp should change to use your function b what change is required if the programs should show a different currency such as euro for each of the variables in the following program indicate the scope then deter mine what the program prints without actually running the program int a int b int f int c int n a c if n c n a b return n int g int c int n int a c if n f c n a b return n int main int i int b g i cout a b i endl return we have seen three kinds of variables in c global variables parameter variables and local variables classify the variables of exercise according to these categories use the process of stepwise refinement to describe the process of making scrambled eggs discuss what you do if you do not find eggs in the refrigerator how many parameters does the following function have how many return values does it have hint the c notions of parameter and return value are not the same as the intuitive notions of input and output void average double avg cout please enter two numbers double x double y cin x y avg x y perform a walkthrough of the function with the following arguments a b c d e f g consider the following function int f int n if n return if n n is even return f n else return f n perform traces of the computations f f f f f f f f f and f eliminate the global variable in the code at the end of section by a passing the balance to the withdraw function and returning the updated balance b passing the balance as a reference parameter to the withdraw function given the following functions trace the function call int i int isqrt int n i while i i n i return i void int n for i i n i cout isqrt i how can you fix the code so that the output is as expected that is consider the following function that is intended to swap the values of two integers void int a int b a b b a int main int x int y x y cout x y endl return why doesn t the function swap the contents of x and y how can you rewrite the function to work correctly consider the following function that is intended to swap the values of two integers void int a int b int temp a a b b temp int main int x int y x y cout x y endl return why doesn t the function swap the contents of x and y how can you rewrite the function to work correctly 18 the following function swaps two integers without requiring a temporary variable void int a int b a a b b a b a b a however it fails in one important case namely when calling x x explain what should happen and what actually happens give pseudocode for a recursive function for printing all substrings of a given string for example the substrings of the string rum are rum itself ru um r u m and the empty string you may assume that all letters of the string are different r5 give pseudocode for a recursive function that sorts all letters in a string for exam ple the string goodbye would be sorted into bdegooy the max function that is declared in the algorithm header returns the larger of its two arguments write a program that reads three floating point numbers uses the max function and displays the larger of the first two inputs the larger of the last two inputs the largest of all three inputs write a function that computes the balance of a bank account with a given initial balance and interest rate after a given number of years assume interest is com pounded yearly write the following functions and provide a program to test them a double smallest double x double y double z returning the smallest of the arguments b double average double x double y double z returning the average of the arguments write the following functions a bool double x double y double z returning true if the arguments are all the same b bool double x double y double z returning true if the arguments are all different c bool sorted double x double y double z returning true if the arguments are sorted with the smallest one coming first provide a program that tests your functions write the following functions a int int n returning the first digit of the argument b int int n returning the last digit of the argument c int digits int n returning the number of digits of the argument for example is is and digits 1729 is provide a program that tests your functions write a function string middle string str that returns a string containing the middle character in str if the length of str is odd or the two middle characters if the length is even for example middle middle returns dd write a function string repeat string str int n that returns the string str repeated n times for example repeat ho returns hohoho write a function int string str that returns a count of all vowels in the string str vowels are the letters a e i o and u and their uppercase variants write a function int string str that returns a count of all words in the string str words are separated by spaces for example mary had a little lamb should return it is a well known phenomenon that most people are easily able to read a text whose words have two characters flipped provided the first and last letter of each word are not changed for example i dn ot gvie a dman for a man taht can olny sepll a wrod one way mrak taiwn write a function string scramble string word that constructs a scrambled version of a given word randomly flipping two characters other than the first and last one then write a program that reads words from cin and prints the scrambled words write functions double double r double double r double double r double h double double r double h double double r double h double double r double h that compute the volume and surface area of a sphere with radius r a cylinder with a circular base with radius r and height h and a cone with a circular base with radius r and height h then write a program that prompts the user for the values of r and h calls the six functions and prints the results write functions double distance double double double double void midpoint double double double double double xmid double ymid void slope double double double double bool vertical double that compute the distance midpoint and slope of the line segment joining the points and the slope function should either set vertical to true and not set s or set vertical to false and set s to the slope write a function double string prompt that displays the prompt string followed by a space reads a floating point number in and returns it here is a typical usage salary please enter your salary what percentage raise would you like write a function void int a int b that swaps the values of a and b if a is greater than b and otherwise leaves a and b unchanged for example int u int v int w int x u v u is still v is still w x w is now x is now write a function int a int b int c that swaps its three arguments to arrange them in sorted order for example int v int w int x v w x v is now w is now x is now hint use sort2 of exercise enhance the function so that it works correctly for values 000 000 000 enhance the function so that it works correctly for negative values and zero caution make sure the improved function doesn t print as twenty zero 18 for some values for example the function returns a string with a leading space twenty repair that blemish and ensure that spaces are inserted only when necessary hint there are two ways of accomplishing this either ensure that leading spaces are never inserted or remove leading spaces from the result before returning it write a program that prints a paycheck ask the program user for the name of the employee the hourly rate and the number of hours worked if the number of hours exceeds the employee is paid time and a half that is percent of the hourly rate on the hours exceeding your check should look similar to that in the figure below use fictitious names for the payer and the bank be sure to use stepwise refinement and break your solution into several functions use the function to print the dollar amount of the check write a function that computes the weekday of a given date using a formula known as zeller s congruence let d the day of the month mm the modified month march december january february w the weekday monday tuesday sunday then w d mm year year here all denote integer division and denotes the remainder operation leap years write a function bool int year that tests whether a year is a leap year that is a year with days leap years are necessary to keep the calendar synchronized with the sun because the earth revolves around the sun once every days actually that figure is not entirely precise and for all dates after the gregorian correction applies usually years that are divisible by are leap years for example however years that are divisible by for example are not leap years but years that are divisible by are leap years for example write a program that converts a roman number such as mcmlxxviii to its decimal number representation hint first write a function that yields the numeric value of each of the letters then use the following algorithm total while the roman number string is not empty if the first character has a larger value than the second or the string has length add value first character to total remove the character else add value second character value first character to total remove both characters in exercise you were asked to write a program to convert a number to its representation in roman numerals at the time you did not know how to eliminate duplicate code and as a consequence the resulting program was rather long rewrite that program by implementing and using the following function string int n string one string five string ten that function translates one digit using the strings specified for the one five and ten values you would call the function as follows n i v x n n roman_digit n x l c postal bar codes for faster sorting of letters the united states postal service encour ages companies that send large volumes of mail to use a bar code denoting the zip code see figure ecrlot code john doe franklin blvd sunnyvale ca figure a postal bar code the encoding scheme for a five digit zip code is shown in figure there are full height frame bars on each side the five encoded digits are followed by a check digit which is computed as follows add up all digits and choose the check digit to make the sum a multiple of for example the zip code has a sum of so the check digit is to make the sum equal to frame bars digit digit digit digit digit check digit figure encoding for five digit bar codes each digit of the zip code and the check digit is encoded according to the following table where denotes a half bar and a full bar digit bar weight bar weight bar weight bar weight bar weight the digit can be easily computed from the bar code using the column weights for example is the only exception is which would yield according to the weight formula write a program that asks the user for a zip code and prints the bar code use for half bars for full bars for example becomes write a program that reads in a bar code with denoting half bars and denoting full bars and prints out the zip code it represents print an error message if the bar code is not correct write a program that prints instructions to get coffee asking the user for input whenever a decision needs to be made decompose each task into a function for example void cout add water to the coffee maker endl cout put a filter in the coffee maker endl cout put the coffee in the filter endl write a recursive function string reverse string str that computes the reverse of a string for example reverse flow should return wolf hint reverse the substring starting at the second character then add the first character at the end for example to reverse flow first reverse low to wol then add the f at the end 28 write a recursive function bool string str that returns true if str is a palindrome that is a word that is the same when reversed examples of palindrome are deed rotor or aibohphobia hint a word is a palindrome if the first and last letters match and the remainder is also a palindrome use recursion to implement a function bool find string str string match that tests whether match is contained in str bool b find mississippi sip sets b to true hint if str starts with match then you are done if not consider the string that you obtain by removing the first character use recursion to determine the number of digits in a number n hint if n is it has one digit otherwise it has one more digit than n use recursion to compute an where n is a positive integer hint if n is then an a otherwise an a an engineering the effective focal length f of a lens of thickness d that has surfaces with radii of curvature and is given by n n d f r r nr r where n is the refractive index of the lens medium write a function that computes f in terms of the other parameters engineering a laboratory container is shaped like the frustum of a cone write functions to compute the volume and surface area using these equations v h r r s r r engineering in a movie theater the angle at which a viewer sees the picture on the screen depends on the distance x of the viewer from the screen for a movie theater with the dimensions shown in the picture below write a function that computes the angle for a given distance x 24 ft ft next provide a more general function that works for theaters with arbitrary dimensions engineering electric wire like that in the photo is a cylindrical conductor covered by an insulat ing material the resistance of a piece of wire is given by the formula r l l a where is the resistivity of the conductor and l a and d are the length cross sectional area and diameter of the wire the resistivity of copper is m the wire diameter d is commonly specified by the american wire gauge awg which is an integer n the diameter of an awg n wire is given by the formula n d 92 mm write a c function double diameter int that accepts the wire gauge and returns the corresponding wire diameter write another c function double double length int that accepts the length and gauge of a piece of copper wire and returns the resistance of that wire the resistivity of aluminum is 82 m write a third c function double double length int that accepts the length and gauge of a piece of aluminum wire and returns the resistance of that wire write a c program to test these functions engineering the drag force on a car is given by fd where is the density of air kg v is the velocity in units of m s a is the projected area of the car and cd is the drag coefficient the amount of power in watts required to overcome such drag force is p fdv and the equivalent horsepower required is hp p write a program that accepts a car s velocity and computes the power in watts and in horsepower needed to over come the resulting drag force note mph m s the arguments are and the return value is the inner call to pow returns 22 therefore the outer call returns users of the function can treat it as a black box double volume pow return volume double double double area side_length return area when the function is called x is set to y is set to and z becomes then z is changed to and that value is returned and printed when the function is called x is set to then y is set to and that value is returned and printed when the function is called n is set to then n is incremented twice setting it to that value is returned and printed hello world the function does not return a value therefore you cannot use it in a expression void shout string str cout str endl void string str int n str length for int i i n i cout cout endl cout str endl for int i i n i cout cout endl void int count for int i i count i cout cout endl void string str int n str length n cout str endl n 18 int total int total where the function is defined as param amount an amount in dollars and cents return the amount in pennies rounded to the nearest penny int double amount return int amount if page where the function is defined as follows bool int n return n add parameter variables so you can pass the initial balance and interest rate to the function double balance double double rate int years return pow rate years int spaces input where the function is defined as follows param str any string return the number spaces in str int string str int count for int i i str length i if str substr i count return count 22 it is very easy to replace the space with any character param str any string param ch a string of length return the number of times that ch occurs in str int count string str string ch int count for int i i str length i if str substr i ch count return count this is useful if you want to count other characters for example count input counts how many commas are in the input change line to name name part hundred in line add the statement if part name digit_name part thousand part part in line change to in the comment 24 in the case of teens we already have the last digit as part of the name nothing is printed one way of dealing with this case is to add the following state ment before line if number return zero 26 here is the approximate trace note that the string starts with a blank space exercise 18 asks you to eliminate it here is one possible solution break up the task print table into print header and print body the print header task calls print separator prints the header cells and calls print separator again the print body task repeatedly calls print row and then calls print separator 28 lines but not through the global variable defined in line the variables s defined in lines and yes but since the function does not modify the amount parameter variable there is no need to do so void transfer double double double amount bool withdraw double balance double amount if balance amount balance balance amount return true else return false void minmax double x double y double a double b if x y a x b y else a y b x the program sets x to then y to then x to it prints 39 mystery mystery mystery mystery mystery the idea is to print one then print n of them void int n if n return cout n simply add the following to the beginning of the function if part return part thousand part chapter arrays and ve ctors to become familiar with using arrays and vectors to collect values to learn about common algorithms for processing arrays and vectors to write functions that receive and return arrays and vectors to be able to use two dimensional arrays arrays syntax defining an array common error bounds errors programming tip use arrays for sequences of related values random fact an early internet worm common array algorithms special topic sorting with the c library special topic a sorting algorithm special topic binary search arrays and functions special topic constant array parameters problem solving adapting algorithms how to working with arrays worked example rolling the dice problem solving discovering algorithms by manipulating physical objects two dimensional arrays syntax two dimensional array definition common error omitting the column size of a two dimensional array parameter worked example a world population table vectors syntax defining a vector programming tip prefer vectors over arrays random fact the first programmer arrays we start this chapter by introducing the array data type arrays are the fundamental mechanism in c for collecting multiple values in the following sections you will learn how to define arrays and how to access array elements defining arrays suppose you write a program that reads a sequence of values and prints out the sequence marking the largest value like this largest value you do not know which value to mark as the largest one until you have seen them all after all the last value might be the largest one therefore the program must first store all values before it can print them could you simply store each value in a separate variable if you know that there are ten inputs then you can store the values in ten variables value10 however such a sequence of variables is not very practical to use you would have to write quite a bit of code ten times once for each of the variables to solve this problem use an array a structure for storing a sequence of values figure an array of size 250 syntax defining an array here we define an array that can hold ten values double values this is the definition of a variable values whose type is array of double that is val ues stores a sequence of floating point numbers the indicates the size of the array see figure the array size must be a constant that is known at compile time when you define an array you can specify the initial values for example double values 80 44 when you supply initial values you don t need to specify the array size the com piler determines the size by counting the values int numbers an array of ten integers const int size int numbers size it is a good idea to use a named constant for the size int size int numbers size caution in standard c the size must be a constant this array definition will not work with all compilers int squares an array of five integers with initial values int squares you can omit the array size if you supply initial values the size is set to the number of initial values int squares if you supply fewer initial values than the size the remaining values are set to this array contains string names an array of three strings accessing array elements the values stored in an array are called its elements each element has a position number called an index to access a value in the values array you must specify which index you want to use that is done with the operator values now the element with index is filled with see figure figure filling an array element you can display the contents of the element with index with the following command cout values endl as you can see the element values can be used like any variable of type double in c array positions are counted in a way that you may find surprising if you look carefully at figure you will find that the fifth element was filled when we changed values in c the elements of arrays are numbered starting at that is the legal elements for the values array are values the first element values the second element values the third element values the fourth element values the fifth element values the tenth element you will see in chapter why this numbering scheme was chosen in c you have to be careful about index values trying to access a element that does not exist in the array is a serious error for example if values has twenty elements you are not allowed to access values attempting to access an element whose index is not within the valid index range is called a bounds error the compiler does not catch this type of error even the run ning program generates no error message if you make a bounds error you silently read or overwrite another memory location as a consequence your program may have random errors and it can even crash like a post office box that is identified by a box number an array element is identified by an index the most common bounds error is the following double values cout values there is no values in an array with ten elements the legal index values range from to to visit all elements of an array use a variable for the index suppose values has ten elements and the integer variable i takes values and so on up to then the expression values i yields each element in turn for example this loop displays all elements for int i i i cout values i endl note that in the loop condition the index is less than because there is no element corresponding to values partially filled arrays an array cannot change size at run time this is a problem when you don t know in advance how many elements you need in that situation you must come up with a good guess on the maximum number of elements that you need to store we call this quantity the capacity for example we may decide that we sometimes want to store more than ten values but never more than const int capacity double values capacity in a typical program run only part of the array will be occupied by actual elements we call such an array a partially filled array you must keep a companion variable that counts how many elements are actually used in figure we call the companion vari able with a partially filled array you need to remember how many elements are filled figure a partially filled array the following loop collects values and fills up the values array int double input while cin input capacity if capacity values input at the end of this loop contains the actual number of elements in the array note that you have to stop accepting inputs if the size of the array reaches the capacity to process the gathered array elements you again use the companion variable not the capacity this loop prints the partially filled array for int i i i cout values i endl define an array of integers containing the first five prime numbers assume the array primes has been initialized as described in self check what is its contents after executing the following loop for int i i i primes i primes i assume the array primes has been initialized as described in self check what is its contents after executing the following loop for int i i i primes i given the definition const int capacity double values capacity write statements to put a zero into the elements of the array values with the lowest and the highest valid index given the array defined in self check write a loop to print the elements of the array values in reverse order starting with the last element define an array called words that can hold ten values of type string define an array containing two strings yes and no practice it now you can try these exercises at the end of the chapter random fact an early internet worm in november robert morris a stu dent at cornell university launched a so called virus program that infected about 000 computers connected to the internet across the united states tens of thousands of computer users were unable to read their e mail or oth erwise use their computers all major universities and many high tech com panies were affected the internet was much smaller then than it is now the particular kind of virus used in this attack is called a worm the virus program crawled from one computer on the internet to the next the worm would attempt to connect to finger a program in the unix operating sys tem for finding information on a user who has an account on a particular computer on the network like many programs in unix finger was writ ten in the c language in c as in c arrays have a fixed size to store the user name to be looked up say wal ters cs sjsu edu the finger program allocated an array of characters under the assumption that nobody would ever provide such a long input unfortunately c like c does not check that an array index is less than the length of the array if you write into an array using an index that is too large you simply overwrite memory locations that belong to some other objects in some versions of the finger program the programmer had been lazy and had not checked whether the array holding the input characters was large enough to hold the input so the worm program purposefully filled the character array with bytes the excess 24 bytes would overwrite a return address which the attacker knew was stored just after the line buf fer when that function was finished it didn t return to its caller but to code supplied by the worm see figure that code ran under the same super user privileges as finger allowing the worm to gain entry into the remote sys tem had the programmer who wrote finger been more conscientious this particular attack would not be pos sible in c as in c all programmers must be very careful not to overrun array boundaries one may well speculate what would possess the virus author to spend many weeks to plan the antisocial act of breaking into thousands of comput ers and disabling them it appears that the break in was fully intended by the author but the disabling of the com puters was a bug caused by continu ous reinfection morris was sentenced to years probation hours of community service and fined 000 in recent years computer attacks have intensified and the motives have become more sinister instead of dis abling computers viruses often steal financial data or use the attacked com puters for sending spam e mail sadly many of these attacks continue to be possible because of poorly written pro grams that are susceptible to buffer overrun errors before the attack line buffer bytes return address after the attack overrun buffer bytes return address figure a buffer overrun attack common array algorithms in the following sections we discuss some of the most common algorithms for processing sequences of values we present the algorithms so that you can use them with fully and partially filled arrays as well as vectors which we will introduce in section when we use the expression size of values you should replace it with a constant or variable that yields the number of elements in the array or the expression values size if values is a vector filling this loop fills an array with zeroes for int i i size of values i values i next let us fill an array squares with the numbers and so on note that the element with index contains the element with index contains and so on for int i i size of squares i squares i i i copying consider two arrays int squares int now suppose you want to copy all values from the first array to the second the fol lowing assignment is an error squares error in c you cannot assign one array to another instead you must use a loop to copy all elements for int i i i i squares i squares 16 16 figure copying elements to copy an array sum and average value you have already encountered this algorithm in section here is the code for computing the sum of all elements in an array double total for int i i size of values i total total values i to obtain the average divide by the number of elements double average total size of values be sure to check that the size is not zero maximum and minimum use the algorithm from section that keeps a variable for the largest element that you have encountered so far here is the implementation for arrays double largest values for int i i size of values i if values i largest largest values i note that the loop starts at because we initialize largest with values to compute the smallest value reverse the comparison these algorithms require that the array contain at least one element element separators when you display the elements of a collection you usually want to separate them often with commas or vertical lines like this 16 note that there is one fewer separator than there are numbers print the separator before each element except the initial one with index for int i i size of values i if i cout cout values i to print five elements you need four separators linear search you often need to search for the position of an element so that you can replace or remove it visit all elements until you have found a match or you have come to the end of the array here we search for the position of the first element equal to int pos bool found false while pos size of values found if values pos found true to search for a specific element visit the elements and stop when you encounter the match else pos if found is true then pos is the position of the first match removing an element consider a partially filled array values whose current size is stored in the variable suppose you want to remove the element with index pos from values if the elements are not in any particular order that task is easy to accomplish simply overwrite the element to be removed with the last element then decrement the vari able tracking the size see figure values pos values the situation is more complex if the order of the elements matters then you must move all elements following the element to be removed to a lower index then decre ment the variable holding the size of the array see figure for int i pos i i values i values i pos pos figure removing an element in an unordered array figure removing an element in an ordered array inserting an element if the order of the elements does not matter you can simply insert new elements at the end incrementing the variable tracking the size see figure for a partially filled array if capacity values it is more work to insert an element at a particular position in the middle of a sequence first increase the variable holding the current size next move all elements above the insertion location to a higher index finally insert the new element here is the code for a partially filled array if capacity for int i i pos i values i values i values pos note the order of the movement when you remove an element you first move the next element down to a lower index then the one after that until you finally get to the end of the array when you insert an element you start at the end of the array move that element to a higher index then move the one before that and so on until you finally get to the insertion location see figure pos figure figure inserting an element in an unordered array inserting an element in an ordered array swapping elements you often need to swap elements of an array for example the sorting algorithm in special topic on page sorts an array by repeatedly swapping elements figure swapping array elements 34 34 consider the task of swapping the elements at posi tions i and j of an array values we d like to set values i to values j but that overwrites the value that is currently stored in values i so we want to save that first double temp values i values i values j now we can set values j to the saved value values j temp figure shows the process to swap two elements you need a temporary variable reading input if you know how many input values the user will supply it is simple to place them into an array double values for i i i cin values i however this technique does not work if you need to read an arbitrary number of inputs in that case add the values to an array until the end of the input has been reached double values capacity int double input while cin input if capacity values input now values is a partially filled array and the companion variable is set to the number of input values this loop discards any inputs that won t fit in the array a better approach would be to copy values to a new larger array when the capacity is reached see section the following program solves the task that we set ourselves at the beginning of this chapter to mark the largest value in an input sequence largest cpp program run what is the output of the largest cpp program with the following inputs q write a loop that counts how many elements in an array are equal to zero consider the algorithm to find the largest element in an array why don t we initialize largest and i with zero like this double largest for int i i size of values i if values i largest largest values i when printing separators we skipped the separator before the initial element rewrite the loop so that the separator is printed after each element except for the last element what is wrong with these statements for printing an array with separators cout values for int i i size of values i cout values i 13 when searching for a match we used a while loop not a for loop what is wrong with using this loop instead for pos pos size of values found pos if values pos found true when inserting an element into an array we moved the elements with larger index values starting at the end why is it wrong to start at the insertion loca tion like this for int i pos i size of values i values i values i practice it now you can try these exercises at the end of the chapter 13 16 special topic a sorting algorithm a sorting algorithm rearranges the elements of a sequence so that they are stored in sorted order here is a simple sorting algorithm called selection sort consider sorting the following array values an obvious first step is to find the smallest element in this case the smallest element is stored in values you should move the to the beginning of the array of course there is already an element stored in values namely therefore you cannot simply move val ues into values without moving the somewhere else you don t yet know where the should end up but you know for certain that it should not be in values simply get it out of the way by swapping it with values now the first element is in the correct place in the foregoing figure the darker color indicates the portion of the array that is already sorted next take the minimum of the remaining entries values values that minimum value is already in the correct place you don t need to do anything in this case simply extend the sorted area by one to the right repeat the process the minimum value of the unsorted region is which needs to be swapped with the first value of the unsorted region special topic binary search when an array is sorted there is a much faster search algorithm than the linear search of sec tion consider the following sorted array values 20 we would like to see whether the value is in the array let s narrow our search by finding whether the value is in the first or second half of the array the last point in the first half of the data set values is which is smaller than the value we are looking for hence we should look in the second half of the array for a match that is in the sequence now the last value of the first half of this sequence is hence the value must be located in the sequence the last value of the first half of this very short sequence is which is smaller than the value that we are searching so we must look in the second half arrays and functions in this section we will explore how to write functions that process arrays a function that processes the values in an array needs to know the number of valid elements in the array for example here is a sum function that computes the sum of all elements in an array double sum double values int size double total for int i i size i total total values i return total note the special syntax for array parameter variables when writing an array param eter variable you place an empty behind the parameter name do not specify the size of the array inside the brackets when you call the function supply both the name of the array and the size for example double double scores 54 67 29 34 80 44 double sum scores you can also pass a smaller size to the function double sum scores this call computes the sum of the first five elements of the scores array remember the function has no way of knowing how many elements the array has it simply relies on the size that the caller provides array parameters are always reference parameters you will see the reason in chapter functions can modify array arguments and those modifications affect the array that was passed into the function for example the following multiply func tion updates all elements in the array void multiply double values int size double factor for int i i size i values i values i factor you do not use an symbol to denote the reference parameter in this case although arrays can be function arguments they cannot be function return types if a function computes multiple values the caller of the function must provide an array parameter variable to hold the result void squares int n int result for int i i n i result i i i when a function changes the size of an array it should indicate to the caller how many elements the array has after the call the easiest way to do this is to return the new size here is an example a function that adds input values to an array int double inputs int capacity int double input while cin input if capacity inputs input return note that this function also needs to know the capacity of the array generally a function that adds elements to an array needs to know is capacity you would call this function like this const int double values int values values is a partially filled array the variable specifies its size alternatively you can pass the size as a reference parameter this is more appropriate for functions that modify an existing array void double inputs int capacity int double input while cin input if capacity inputs input this function is called as values after the call the variable contains the new size the following example program reads values from standard input doubles them and prints the result the program uses three functions the function fills an array with the input values it returns the number of elements that were read the multiply function modifies the contents of the array that it receives demon strating that arrays are passed by reference the print function does not modify the contents of the array that it receives functions cpp include iostream using namespace std 5 reads a sequence of floating point numbers param inputs an array containing the numbers param capacity the capacity of that array return the number of inputs stored in the array int double inputs int capacity 13 int cout please enter values q to quit endl bool more true 16 while more 18 double input cin input 20 if cin fail 22 more false 24 else if capacity 26 inputs input 28 29 30 return 31 32 33 34 multiplies all elements of an array by a factor param values a partially filled array param size the number of elements in values program run what happens if you call the sum function and you lie about the size for exam ple calling double result sum values even though values has size 16 how do you call the squares function to compute the first five squares and store the result in an array numbers 17 write a function that returns the first position of an element in an array or if the element is not present use the linear search algorithm of section 18 rewrite the function so that the array size is a reference parameter not a return value 19 write the header for a function that appends two arrays into another array do not implement the function practice it now you can try these exercises at the end of the chapter problem solving adapting algorithms in section you were introduced to a number of fundamental array algorithms these algorithms form the building blocks for many programs that process arrays in general it is a good problem solving strategy to have a repertoire of fundamental algorithms that you can combine and adapt consider this example problem you are given the quiz scores of a student you are to compute the final quiz score which is the sum of all scores after dropping the low est one for example if the scores are 5 5 then the final score is we do not have a ready made algorithm for this situation instead consider which algorithms may be related these include calculating the sum section finding the minimum value section removing an element section now we can formulate a plan of attack that combines these algorithms find the minimum remove it from the array calculate the sum let s try it out with our example the minimum of 5 5 5 7 is 4 how do we remove it now we have a problem the removal algorithm in section 2 7 locates the ele ment to be removed by using the position of the element not the value but we have another algorithm for that linear search section 2 we need to fix our plan of attack find the minimum value find its position remove that position from the array calculate the sum will it work let s continue with our example we found a minimum value of 4 linear search tells us that the value 4 occurs at position 5 2 4 5 we remove it 2 4 5 7 5 5 7 finally we compute the sum 7 5 5 7 this walkthrough demonstrates that our strategy works can we do better it seems a bit inefficient to find the minimum and then make another pass through the array to obtain its position we can adapt the algorithm for finding the minimum to yield the position of the minimum here is the original algorithm double smallest values for int i i size of values i if values i smallest smallest values i when we find the smallest value we also want to update the position if values i smallest smallest values i i in fact then there is no reason to keep track of the smallest value any longer it is sim ply values with this insight we can adapt the algorithm as follows int for int i i size of values i if values i values i with this adaptation our problem is solved with the following strategy find the position of the minimum remove it from the array calculate the sum in how to on page we develop a c program from this strategy the next section shows you a technique for discovering a new algorithm when none of the fundamental algorithms can be adapted to a task 20 section 2 7 has two algorithms for removing an element which of the two should be used to solve the task described in this section 21 it isn t actually necessary to remove the minimum in order to compute the total score describe an alternative 22 how can you print the number of positive and negative values in a given array using one or more of the algorithms in section 4 7 how can you print all positive values in an array separated by commas 24 consider the following algorithm for collecting all matches in an array int for int i i size of values i if values i fulfills the condition matches values i how can this algorithm help you with self check practice it now you can try these exercises at the end of the chapter 16 step decompose your task into steps you will usually want to break down your task into multiple steps such as reading the data into an array processing the data in one or more steps displaying the results in our sample problem this yields the following pseudocode read inputs compute the final score display the score when deciding how to process the data you should be familiar with the array algorithms in section 2 many processing tasks can be solved by combining or adapting one or more of these algorithms the preceding section showed you how to decompose compute the final score into fundamen tal algorithms find the position of the minimum remove it from the array calculate the sum step 2 determine functions arguments and return values for each step even though it may be possible to put all steps into the main function this is rarely a good idea the simplest and best approach is to make each nontrivial step into a separate function in our example we will implement four functions remove sum for each function that processes an array you will need to pass the array itself and the array size for example double sum double values int size if the function modifies the size it needs to tell the caller what the new size is the function can return the size or it can use a reference parameter for the size the second approach is a better choice for a function that modifies an existing array we use the first approach with the function that reads input values int double values int capacity returns the size the remove function modifies the parameter void remove double values int int pos at this point you should document each function like this removes an element from an array the order of the elements is not preserved param values a partially filled array param the number of elements in values will be reduced by if the position is valid param pos the position of the element to be removed void remove double values int int pos step implement each function using helper functions when needed we won t show the code for the function because you have seen it already let us implement the function it calls three helper functions remove and sum removes the smallest value of an array and returns the sum of the remaining values param values a partially filled array param the number of elements in values will be reduced by return the sum of the values excluding the minimum double double values int int pos values remove values pos return sum values we discussed the algorithm for in the preceding section gets the position of the minimum value from an array param values a partially filled array param size the number of elements in values return the position of the smallest element in values int double values int size int for int i i size i if values i values i return the remaining helper functions use the algorithms from section 2 you will find the imple mentations in the book s companion code step 4 consider boundary conditions for the functions that you are implementing most functions that operate on arrays are a bit intricate and you have to be careful that you handle both normal and exceptional situations what happens with an empty array an array that contains a single element when no match is found when there are multiple matches consider these boundary conditions and make sure that your functions work correctly here is one example of such a consideration how do we know that the func tion will be called with an array of size at least recall that you must have at least one ele ment in order to find the minimum that function is called from the function however the function could conceivably be called with an empty array we need to either include a test or add a restriction to the function comment we will opt for the latter and change the comment for the values parameter variable of the function to param values a partially filled array of size consider another potential problem what if there are multiple matches that means that a student had more than one test with a low score the function removes only one of the occurrences of that low score and that is the desired behavior step 5 assemble and test the complete program now we are ready to combine the individual functions into a complete program before doing this consider some test cases and their expected output test case expected output comment 7 5 5 7 5 see step 7 7 24 only one instance of the low score should be removed after removing the low score no score remains no inputs error that is not a legal input this main function completes the solution see scores cpp int main const int capacity double scores capacity int read_inputs scores capacity if cout at least one score is required endl else double score scores cout final score score endl return 5 problem solving discovering algorithms by manipulating physical objects in section 4 you saw how to solve a problem by combining and adapting known algorithms but what do you do when none of the standard algorithms is sufficient for your task in this section you will learn a technique for discovering algorithms by manipulating physical objects consider the following task you are given an array whose size is an even number and you are to switch the first and the second half for example if the array contains the eight numbers then you should change it to many students find it quite challenging to come up with an algorithm they may know that a loop is required and they may realize that elements should be inserted section 2 8 or swapped section 2 but they do not have sufficient intuition to draw diagrams describe an algorithm or write down pseudocode one useful technique for discovering an algorithm is to manipulate physical objects start by lining up some objects to denote an array coins playing cards or small toys are good choices available online at www wiley com college horstmann manipulating physical objects can give you ideas for discovering algorithms here we arrange eight coins visualizing the removal of an array element now let s step back and see what we can do to change the order of the coins we can remove a coin section 2 7 we can insert a coin section 2 8 visualizing the insertion of an array element or we can swap two coins section 2 visualizing the swapping of two coins go ahead line up some coins and try out these three operations right now so that you get a feel for them now how does that help us with our problem switching the first and the second half of the array let s put the first coin into place by swapping it with the fifth coin however as c programmers we will say that we swap the coins in positions and 4 next we swap the coins in positions and 5 two more swaps and we are done now an algorithm is becoming apparent i j we ll think about that in a minute while don t know yet swap elements at positions i and j i j where does the variable j start when we have eight coins the coin at position zero is moved to position 4 in general it is moved to the middle of the array or to position size 2 and how many iterations do we make we need to swap all coins in the first half that is we need to swap size 2 coins the pseudocode is i j size 2 while i size 2 swap elements at positions i and j i j it is a good idea to make a walkthrough of the pseudocode see section 4 2 you can use paper clips to denote the positions of the variables i and j if the walkthrough is successful then we know that there was no off by one error in the pseudocode self check asks you to carry out the walkthrough and exercise 7 asks you to translate the pseudocode to c exercise 17 suggests a different algorithm for switching the two halves of an array by repeatedly removing and inserting coins many people find that the manipulation of physical objects is less intimidating than drawing diagrams or mentally envisioning algorithms give it a try when you need to design a new algorithm walk through the algorithm that we developed in this section using two paper clips to indicate the positions for i and j explain why there are no bounds errors in the pseudocode 26 take out some coins and simulate the following pseudocode using two paper clips to indicate the positions for i and j i j size while i j swap elements at positions i and j i j what does the algorithm do consider the task of rearranging all values in an array so that the even numbers come first otherwise the order doesn t matter for example the array 4 2 5 could be rearranged to 4 2 14 5 23 using coins and paperclips discover an algorithm that solves this task by swapping elements then describe it in pseudocode 28 discover an algorithm for the task of self check 27 that uses removal and insertion of elements instead of swapping 29 consider the algorithm in section 4 7 4 that finds the largest element in a sequence of inputs not the largest element in an array why is this algorithm better visual ized by picking playing cards from a deck rather than arranging toy soldiers in a sequence practice it now you can try these exercises at the end of the chapter 17 18 7 two dimensional arrays it often happens that you want to store collections of values that have a two dimensional layout such data sets commonly occur in financial and scientific applications an arrangement consisting of rows and columns of values is called a two dimensional array or a matrix let s explore how to store the example data shown in figure the medal counts of the figure skating competitions at the winter olympics gold silver bronze canada china germany korea japan russia united states figure figure skating medal counts defining two dimensional arrays c uses an array with two subscripts to store a two dimensional array for exam ple here is the definition of an array with 7 rows and columns suitable for storing our medal count data const int countries 7 const int medals int counts countries medals you can initialize the array by grouping each row as follows int counts countries medals just as with one dimensional arrays you cannot change the size of a two dimen sional array once it has been defined syntax 2 two dimensional array definition 2 accessing elements to access a particular element in the two dimensional array you need to specify two subscripts in separate brackets to select the row and column respectively see syntax 2 and figure int value counts to access all values in a two dimensional array you use two nested loops for exam ple the following loop prints all elements of counts for int i i countries i process the ith row for int j j medals j process the jth column in the ith row cout setw 8 counts i j cout endl start a new line at the end of the row column index counts figure 12 accessing an element in a two dimensional array computing row and column totals a common task is to compute row or column totals in our example the row totals give us the total number of medals won by a particular country finding the right index values is a bit tricky and it is a good idea to make a quick sketch to compute the total of row i we need to visit the following elements medals row i as you can see we need to compute the sum of counts i j where j ranges from to medals the following loop computes the total int total for int j j medals j total total counts i j computing column totals is similar form the sum of counts i j where i ranges from to countries column j j j 2 j j 4 j 5 j j int total for int i i countries i countries total total counts i j 4 two dimensional array parameters when passing a two dimensional array to a function you must specify the number of columns as a constant with the parameter type for example this function computes the total of a given row const int columns int int table columns int row int total for int j j columns j total total table row j return total this function can compute row totals of a two dimensional array with an arbitrary number of rows but the array must have columns you have to write a different function if you want to compute row totals of a two dimensional array with 4 columns to understand this limitation you need to know how the array elements are stored in memory although the array appears to be two dimensional the elements are still stored as a linear sequence figure 13 shows how the counts array is stored row by row for example to reach counts the program must first skip past rows and 2 and then locate offset in row the offset from the start of the array is number of columns now consider the function the compiler generates code to find the element table i j by computing the offset i columns j the compiler uses the value that you supplied in the second pair of brackets when declaring the parameter int int table columns int row note that the first pair of brackets should be empty just as with one dimensional arrays row row row 2 row counts 3 figure 13 a two dimensional array is stored as a sequence of rows the function did not need to know the number of rows of the array if the number of rows is required pass it as a variable as in this example int int table columns int rows int column int total for int i i rows i total total table i column return total working with two dimensional arrays is illustrated in the following program the program prints out the medal counts and the row totals medals cpp cout country gold silver bronze total endl print countries counts and row totals for int i i countries i cout setw countries i process the ith row for int j j medals j cout setw 8 counts i j int total counts i cout setw 8 total endl return program run country gold silver bronze total canada 2 china 2 germany korea japan 2 russia 2 united states 2 30 what results do you get if you total the columns in our sample data 31 consider an 8 8 array for a board game int board 8 8 using two nested loops initialize the board so that zeroes and ones alternate as on a checkerboard 1 1 1 1 1 1 1 1 1 1 1 1 1 hint check whether i j is even 32 define a two dimensional array for representing a tic tac toe board the board has three rows and columns and contains strings x o and 33 write an assignment statement to place an x in the upper right corner of the tic tac toe board 34 which elements are on the diagonal joining the upper left and the lower right corners of the tic tac toe board practice it now you can try these exercises at the end of the chapter 23 19 20 7 vectors when you write a program that collects values from user input you don t always know how many values you will have unfortunately the size of the array has to be known when the program is compiled in section 1 3 you saw how you can address this problem with partially filled arrays the vec tor construct which we discuss in the following sections offers a more convenient solution a vec tor collects a sequence of values just like an array does but its size can change a vector expands to hold as many elements as needed available online at www wiley com college horstmann syntax 6 3 defining a vector 6 7 1 defining vectors when you define a vector you specify the type of the elements in angle brackets like this vector double values you can optionally specify the initial size for example here is a definition of a vector whose initial size is vector double values if you define a vector without an initial size it has size while there would be no point in defining an array of size zero it is often useful to have vectors with initial size zero and then grow them as needed in order to use vectors in your program you need to include the vector header you access the vector elements as values i just as you do with arrays the size member function returns the current size of a vector in a loop that visits all vector elements use the size member function like this for int i i values size i cout values i endl 6 7 2 growing and shrinking vectors if you need additional elements you use the function to add an element to the end of the vector thereby increasing its size by 1 the function is a mem ber function that you must call with the dot notation like this values 5 after this call the vector values in figure 14 has size 3 and values 2 contains the value 5 figure 14 adding an element with it is very common to start with an empty vector and use the function to fill it for example vector double values initially empty values 32 now values has size 1 and element 32 values 54 now values has size 2 and elements 32 54 values 5 now values has size 3 and elements 32 54 5 another common use for the member function is to fill a vector with input values vector double values initially empty double input while cin input values input note how this input loop is much simpler than the one in section 6 2 another member function removes the last element of a vector shrinking its size by one see figure values figure removing an element with 6 7 3 vectors and functions you can use vectors as function arguments in exactly the same way as any other val ues for example the following function computes the sum of a vector of floating point numbers double sum vector double values double total for int i i values size i total total values i return total this function visits the vector elements but it does not modify them if your func tion modifies the elements use a reference parameter the following function multi plies all values of a vector with a given factor void multiply vector double values double factor note the for int i i values size i values i values i factor some programmers use a constant reference see special topic 5 2 for vector param eters that are not modified for example double sum const vector double values const added for efficiency a function can return a vector again vectors are no different from any other values in this regard simply build up the result in the function and return it in this example the squares function returns a vector of squares from up to n 1 2 vector int squares int n vector int result for int i i n i result i i return result as you can see it is easy to use vectors with functions there are no special rules to keep in mind 6 7 4 vector algorithms most of the algorithms in section 6 2 apply without change to vectors simply replace size of values with values size in this section we discuss which of the algo rithms are different for vectors copying as discussed in section 6 2 2 you need an explicit loop to make a copy of an array it is much easier to make a copy of a vector you simply assign it to another vector con sider this example vector int squares for int i i 5 i squares i i vector int initially empty squares now contains the same elements as squares finding matches section 6 2 6 shows you how to find the first match but sometimes you want to have all matches this is tedious with arrays but simple using a vector that collects the matches here we collect all elements that are greater than vector double matches for int i i values size i if values i matches values i removing an element when you remove an element from a vector you want to adjust the size of the vector by calling the member function here is the code for removing an element at pos when the order doesn t matter int values size 1 values pos values replace element at pos with last element values delete last element when removing an element from an ordered vector first move the elements then reduce the size for int i pos 1 i values size i values i 1 values i values inserting an element inserting an element at the end of a vector requires no special code simply use the member function when you insert an element in the middle you still want to call so that the size of the vector is increased use the following code int values size 1 values values for int i i pos i values i values i 1 values pos define a vector of integers that contains the first five prime numbers 2 3 5 7 and use to add the elements 36 answer self check without using what is the contents of the vector names after the following statements vector string names names ann names bob names names cal suppose you want to store a set of temperature measurements that is taken every five minutes should you use a vector or an array 39 suppose you want to store the names of the weekdays should you use a vector or an array of seven strings write the header for a function that appends two vectors yielding a third vector do not implement the function 41 consider this partially completed function that appends the elements of one vector to another void append vector double target vector double source for int i i source size i target source i specify whether the parameters should be value or reference parameters practice it now you can try these exercises at the end of the chapter 11 25 26 27 random fact 6 2 the first programmer before pocket calcu lators and personal computers existed navigators and engineers used mechanical adding machines slide rules and tables of logarithms and trigonometric func tions to speed up computations unfor tunately the tables for which values had to be computed by hand were notoriously inaccurate the mathema tician charles babbage had the insight that if a machine could be constructed that produced printed tables automatically both calcula tion and typesetting errors could be avoided babbage set out to develop a machine for this purpose which he called a diference engine because it replica of babbage s diference engine used successive differences to com pute polynomials for example con sider the function f x write down the values for f 1 f 2 f 3 and so on then take the diferences between successive values 1 7 8 19 27 repeat the process taking the differ ence of successive values in the second column and then repeat once again 1 7 8 12 19 6 27 18 37 6 64 24 61 6 30 216 now the differences are all the same you can retrieve the function values by a pattern of additions you need to know the values at the fringe of the pattern and the constant difference you can try it out yourself write the highlighted numbers on a sheet of paper and fill in the others by adding the numbers that are in the north and northwest positions this method was very attractive because mechanical addition machines had been known for some time they consisted of cog wheels with cogs per wheel to represent digits and mechanisms to handle the carry from one digit to the next mechanical mul tiplication machines on the other hand were fragile and unreliable babbage built a successful prototype of the difference engine and with his own money and government grants proceeded to build the table printing machine however because of funding problems and the difficulty of building the machine to the required precision it was never completed while working on the difference engine babbage conceived of a much grander vision that he called the ana lytical engine the difference engine was designed to carry out a limited set of computations it was no smarter than a pocket calculator is today but babbage realized that such a machine could be made programmable by stor ing programs as well as data the inter nal storage of the analytical engine was to consist of 1 000 registers of 50 decimal digits each programs and con stants were to be stored on punched cards a technique that was at that time commonly used on looms for weaving patterned fabrics ada augusta countess of lovelace the only child of lord byron was a friend and sponsor of charles babbage ada lovelace was one of the first people to realize the potential of such a machine not just for computing mathematical tables but for processing data that were not num bers she is considered by many to be the world s first programmer use arrays for collecting values use an array to collect a sequence of values of the same type individual elements in an array values are accessed by an integer index i using the notation values i chapter summary overrun buffer bytes return address an array element can be used like any variable an array index must be at least zero and less than the size of the array a bounds error which occurs if you supply an invalid array index can corrupt data or cause your program to terminate with a partially filled array keep a companion variable for the current size be able to use common array algorithms to copy an array use a loop to copy its elements to a new array when separating elements don t place a separator before the first element a linear search inspects elements in sequence until a match is found before inserting an element move elements to the end of the array starting with the last one use a temporary variable when swapping two elements implement functions that process arrays when passing an array to a function also pass the size of the array array parameters are always reference parameters a function s return type cannot be an array when a function modifies the size of an array it needs to tell its caller a function that adds elements to an array needs to know its capacity be able to combine and adapt algorithms for solving a programming problem by combining fundamental algorithms you can solve complex programming tasks you should be familiar with the implementation of fundamental algorithms so that you can adapt them discover algorithms by manipulating physical objects use a sequence of coins playing cards or toys to visualize an array of values you can use paper clips as position markers or counters use two dimensional arrays for data that is arranged in rows and columns use a two dimensional array to store tabular data individual elements in a two dimensional array are accessed by using two subscripts array i j a two dimensional array parameter must have a fixed number of columns use vectors for managing collections whose size can change a vector stores a sequence of values whose size can change use the size member function to obtain the current size of a vector use the member function to add more elements to a vector use to reduce the size vectors can occur as function arguments and return values use a reference parameter to modify the contents of a vector a function can return a vector 1 write code that fills an array double values with each set of values below a 1 2 3 4 5 6 7 8 b 2 4 6 8 12 14 16 18 c 1 4 9 16 25 36 64 81 d 0 0 0 0 0 0 0 e 1 4 9 16 9 7 4 9 11 f 0 1 0 1 0 1 0 1 0 1 g 0 1 2 3 4 0 1 2 3 4 2 consider the following array int a 1 2 3 4 5 4 3 2 1 0 what is the value of total after the following loops complete a int total 0 for int i 0 i i total total a i b int total 0 for int i 0 i 10 i i 2 total total a i c int total 0 for int i 1 i 10 i i 2 total total a i d int total 0 for int i 2 i 10 i total total a i e int total 0 for int i 0 i 10 i 2 i total total a i f int total 0 for int i 9 i 0 i total total a i g int total 0 for int i 9 i 0 i i 2 total total a i h int total 0 for int i 0 i 10 i total a i total 3 consider the following array int a 1 2 3 4 5 4 3 2 1 0 review exercises what are the contents of the array a after the following loops complete a for int i 1 i 10 i a i a i 1 b for int i 9 i 0 i a i a i 1 c for int i 0 i 9 i a i a i 1 d for int i 8 i 0 i a i a i 1 e for int i 1 i 10 i a i a i a i 1 f for int i 1 i 10 i i 2 a i 0 g for int i 0 i 5 i a i 5 a i h for int i 1 i 5 i a i a 9 i 4 write a loop that fills an array int values 10 with ten random numbers between 1 and 100 write code for two nested loops that fill values with ten different random numbers between 1 and 100 5 write c code for a loop that simultaneously computes both the maximum and minimum of an array 6 what is wrong with the following loop int values 10 for int i 1 i 10 i values i i i introduction the game of minesweeper is a game of logic reasoning a by grid is created and mines are placed at random in the grid the player then clicks a button in the grid which either reveals a mine you lose or it reveals how many mines there are in the neighboring squares away in each of the directions your goal is to find the location of all mines without accidentally exploding one what i ve provided to get you started i ve produced a skeleton project this project contains the code that creates the grid of buttons it also has a method that informs you when and which button the user has placed your job is to modify the code in the subroutine and the grid click subroutine do not modify any of the code i ve provided except for the example code in grid click what you need to do you need to create a dimensional array that represents the playing board and in each appropriate array element you will either indicate it is a mine or you will place the number of mines in the surrounding squares in the subroutine you will do the following place mines at unique random coordinates fill in the array with the neighbor counts according to the following examples general case for all of the rows from to and columns from to each square has eight neighbors if the square you are examining marked with the below does not contain a mine count how many mines are in the surrounding squares first last rows columns in the first row last row first column and last column there are only neighbors corners in the top right top left bottom right and bottom left corners there are only neighbors note loops will greatly help you in most of these cases however there still is a lot of nearly identical code that will seem tedious it copy paste and modify for the most part tips and suggestions on every windows machine there is a copy of minesweeper installed play it to get a feel for how it should look you don t have to be good at the game just understand the rules i ve provided an example i wrote as well there no simple way to determine you won so don t worry about it just handle displaying a message if you click a mine get some graph paper and draw your special cases to understand them better try to follow the comments i ve provided to make sure you do the right things when start with the general case it a bit easier to understand remember to place the mines at unique places that means you need to check to see if there already a mine at the coordinates you picked and if so try again submission submission is done through the usual website http www cs pitt edu jmisurda submit remember to name them with your last name if there isn t a last name on the file or in the comment i will be unable to grade it since i will not see you again submission is after the final blackjack also known as is a multiplayer card game with fairly simple rules for this assignment you will be implementing a simplified version where a user can play against the computer who acts as dealer two cards are dealt to each player the dealer shows one card face up and the other is face down the player gets to see both of his or her cards and the total of them is added face cards kings queens and jacks are worth points aces are worth or points and all other cards are worth their face value the goal of the game is to get as close to blackjack without going over called busting the human player goes first making his or her decisions based on the single dealer card showing the player has two choices hit or stand hit means to take another card stand means that the player wishes no more cards and ends the turn allowing for the dealer to play the dealer must hit if their card total is less than and must stand if it is or higher whichever player gets closest to without exceeding it wins interface the interface should have a menu with a new and exit options when the new menu item is clicked the game should begin deal two cards to the player and show their total deal cards to the dealer however only show card in the interface enable the hit and stand buttons so the player can play the player should be able to play until they hit the stand button or they go over if they bust the hit and stand buttons get disabled and a message saying the game is over should be displayed when the stand button is hit the dealer cards are revealed and the dealer plays using the rules above when the dealer is done playing a winner is determined and displayed in a message box the game is then over until the user chooses new again from the menu requirements for this assignment you need to do the following write a program that plays blackjack have the program intelligently determine if an ace should be interpreted as a or an this is somewhat challenging you need to be able to also handle multiple aces if there are any aces in the hand and the total exceeds change the to a i e subtract until there are no more aces in the hand or the total is below hint keep a counter that says how many aces have been dealt so far to a player you don t need to be able to deal from a real deck just generate a random number where is an ace and and represent as well to get the right distribution of cards you can ignore suit you must have comments and good variable control names we will discuss this in class again this will be a portion of your grade hints and tips start early we will talk about the msgbox function again to display messages if you want the program to pause a little so you can see the dealer cards each turn you can use the code to pause half a second please play with the demo i ve provided on the webpage submission place your files into a zip folder as we did for the last assignment the url for submission is http www cs pitt edu jmisurda submit it will go live at class time tuesday no submissions accepted prior to that if you want help make sure you see me before the deadline algorithms are precise stepbystep instructions on how to accomplish a desired task a formal algorithm for use with computers or in mathematics must be very detailed and resolve the ambiguities that we take for granted in everyday life your first out of class assignment is to pick a common or interesting task and approach it algorithmically breaking it down to the level that a computer might understand assignment for this project you need to pick an everyday or interesting task and create an algorithm that explains how to do it you need to specify what if anything is input like ingredients in a recipe the process by which you operate on that input to produce or accomplish a task what the output of the algorithm is what if any assumptions are made in the algorithm what to turn in type your algorithm up using the following example as a template name the task your algorithm describes the input and output and then list the steps involved numbering each followed by what your algorithm assumes example algorithm calling a friend on the telephone input the telephone number of your friend output none steps pick up the phone and listen for a dial tone press each digit of the phone number on the phone if busy hang up phone wait minutes jump to step if no one answers leave a message then hang up if no answering machine hang up and wait hours then jump to step talk to friend hang up phone assumptions step assumes that you live alone and no one else could be on the phone the algorithm assumes the existence of a working phone and active service the algorithm assumes you are not deaf or mute the algorithm assumes a normal corded phone real life algorithms http people cs pitt edu jmisurda teaching htm advice please dont go overboard with this assignment keep the algorithm under steps however do try to make this interesting avoid recipes for food or the like since the book deals with these extensively also note that you are allowed to be somewhat vague without being imprecise note i say here to leave a message but not what that message is that would be dependent on the individual call and not affect the actual process of making a call reasonable assumptions are along the lines of assuming the presence of necessary materials unreasonable assumptions introduce ambiguity like assuming the universal knowledge of how much a pinch of salt is grading you will be graded on your originality and the precision of the algorithm the task should be clear to me and reproducible from the steps you list to get a second opinion on this you will spend a few minutes in the class on the duedate trading them with your classmates this allows you to act both as the designer of the algorithm and as the computer it runs on note i will note accept your assignment if it is not typed introduction the game of craps is a game played with dice they are rolled together and their points are totaled if you roll a or you immediately lose if you roll a or you immediately win any other value is called the point you then must roll the dice over until you roll the point again or you roll a if you roll the point again you win if you roll you lose interface a sample program can be downloaded from the website see the screenshots below for example behavior what you need to do cs project craps http people cs pitt edu jmisurda teaching htm make a craps program as shown above one important thing that the program should do is to only display the point groupbox when it is appropriate i e when you are trying to roll point make sure it handles the separate phases of the game correctly the first roll rolling point if necessary after the first roll hints and suggestions you can either remember the point in a separate variable or you can extract it from the label you store it in for later comparison if you set the visible property of a groupbox any controls inside it will appear disappear as well don t forget to seed the random number generator you need to determine whether this roll is in the first roll phase or the point roll phase there are several different ways to do this and you can use any submission place your files into a zip folder as we did for the last assignment the url for submission is http www cs pitt edu jmisurda submit it will go live at class time tuesday no submissions accepted prior to that if you want help make sure you see me before the deadline the username and password will again be and vbrocks introduction your first project will be making the game of rock paper scissors for those unfamiliar with the rules typically the game is played with two people who use hand gestures to represent a rock closed fist paper an open hand or scissor a v made with your fingers each person displays their choice at the same time and the winner is determined by winner in bold scissors cut paper paper covers rock rock breaks scissors your job will be to write a program where a human can play against the have the human player enter their choice and then have the computer randomly pick its choice for each game you will display what the computer has chosen and who the winner is interface a screenshot of the interface is shown below for this first project i want you to as closely replicate this as possible it doesnt need to be perfect no need to count dots or anything the font is point in the example and the text alignment the player should pick their guess by selecting a button from the row at the bottom when a button is chosen the computer should make its guess that guess should be displayed in the top label and the winner of the match should be shown in the center example run cs project http people cs pitt edu jmisurda teaching htm what you need to do and when there are two deadlines for this project the first is in week where i want you to bring your interface to class to show me load it onto a flash drive or disk and bring it to class ready to demonstrate the final deadline will be a week after that for the entire program submission details will be forthcoming you will be turning in your code and a working program for me to test hints and suggestions i have placed the program that i wrote up on the web for you to try just so you understand the interaction requirements remember that the computer does not know what a rock paper or scissors are so you should represent these things in a way that a computer would love follow the example we will do in class for how to generate random numbers and make decisions start early this project has some handholding later ones you will have more freedom on so develop good habits with this one description computers have become increasingly pervasive in todays society their power to do complicated and repetitive tasks quickly and efficiently has made them invaluable tools in modern society in this class you will learn to harness the power of a computer to do new tasks by creating your own software as opposed to using existing programs we will explore the limits of what a computer can and cannot do easily and provide you with the knowledge and experience to recognize those problems that you may find in life that can be solved with the help of a computer and the ability to make the computer do those tasks this class is meant as a first class in computer science anyone is welcome to take it as computers can be useful in a variety of fields it is also meant as an introduction to computer science for those students who wish to pursue the field further but lack prior exposure to the material prerequisites there are no prerequisites concepts from basic algebra will be drawn upon passing familiarity with the operation of a computer is also helpful if you are already proficient i e had or courses prior in computer programming this is not the course for you disability resources and services if you have a disability for which you are requesting an accommodation you are encouraged to contact both your instructor and disability resources and services william pitt union as early as possible in the term drs will verify your disability and determine reasonable accommodations for this course course purposes and goals cs introduction to computer programming in visual basic http people cs pitt edu jmisurda teaching htm introduction to computer programming is meant as a course to expose interested students how computer software is written and tested it seeks to illuminate the creative process that is computer programming from the ground up with an emphasis on good preplanning and style in this course we will be writing event driven programs for the windows operating system in the visual basic vb programming language goals for the course include being able to write computer programs in vb that can perform a variety of tasks recognizing and creating wellwritten programs in good programming style participating in the various stages of the lifecycle of a computer program including planning implementation and debugging textbook the text can be found in the bookstore schneider david i an introduction to programming using visual basic net fifth edition prentice hall new jersey isbn additional materials in addition to the textbook you will also need some type of removable storage media to store your programs on while you work on them i suggest a small inexpensive usb thumb drive anything over mb should be sufficient for the work in this course floppy disks or rewriteable cds are also useable but are a more brittle form of storage meaning you may lose your work during the term since this is a computer class with out of class assignments you will also need access to a computer that has visual basic net visual basic net or visual basic installed on it all campus lab computers are already installed with visual basic net if you want to work at home we can discuss ways of doing this visual studio net is available on cds for free from the campus labs visual studio net can be downloaded from http software pitt edu however it is huge and visual basic express can be downloaded for free from microsoft class policies exams there will be a midterm and a final in this class the final exam is scheduled for monday april from p m p m cheating on exams will not be tolerated anyone caught cheating will be given a zero for the test and reported to the department following university procedures quizzes there will be unannounced quizzes given throughout the term projects there will be four outofclass assignments given these are to be completed in the given time no extensions will be given without a valid excuse these are meant to be your own work anyone found to be collaborating will be given a zero for the assignment collaborating also means using code from previous terms other universities your friends or finding it on the internet participation attendance will not be taken but in a small class any absence will be noticed several unexcused missed classes will adversely affect your grade asking and answering questions will also be considered as part of the participation grade cs introduction to computer programming in visual basic http people cs pitt edu jmisurda teaching htm grading your grade will be based upon exams projects quizzes the lowest one will be dropped and participation exams each projects each quizzes participation total the scale for the term will be percentage letter or above a a b b c c d d dless than f term schedule the daily topics are subject to change depending on our pace they are there to assist you in the readings so you can focus on those concepts prior to class week readings for this week purchase the book topics introduction to the course demonstration of visual basic week readings for this week chapter topics event driven programming windows basics examination of dialogbased and viewbased applications common windows controls interface design in vb week readings for this week chapter topics cs introduction to computer programming in visual basic http people cs pitt edu jmisurda teaching htm problem solving algorithms flowcharts week readings for this week chapter appendix d topics controls and events under vb stepping through a program numbers and arithmetic strings week 1 readings for this week chapter topics reading the online documentation subroutines functions modules week readings for this week chapter topics linear control flow if then blocks select case blocks week readings for this week chapter topics repetition looping dowhile fornext week readings for this week chapters midterm exam february during class topics review for exam week readings for this week chapter topics arrays sorting and searching multidimensional arrays cs introduction to computer programming in visual basic http people cs pitt edu jmisurda teaching htm spring break no classes this week week readings for this week chapter topics files text and binary files sequential vs random access week readings for this week chapter topics additional windows controls list boxes common controls week readings for this week msdn online help topics interfacing with the api shellnotifyicondata week readings for this week tba topics good programming practices examples of dos and donts week 4 11 readings for this week tba topics visual basic for applications office macros week 4 readings for this week none topics review for the final exam finals week final exam monday april from 2 00 p m 50 p m in sennott square normal classroom 15 cs introduction to computer programming in visual basic project the game of craps http people cs pitt edu jmisurda teaching htm project the game of craps due june introduction craps is played with two dice which are rolled together the sum of these two dice is computed and if the sum is or the player loses or the player wins any other number is referred to as the point the goal then becomes to keep rolling the dice until o the player rolls a the player loses o the player rolls the point the player wins examples ex roll player rolls a player loses ex roll player rolls a player wins ex roll player rolls a the point becomes roll player rolls a roll player rolls a player loses ex roll player rolls a the point becomes roll player rolls a roll player rolls a player wins assignment your job on this assignment is to make a simple craps game in this game the player will enter his or her name the program will display a welcome message and ask if the player would like to play or quit the program will then roll two dice and display them and their total if the player has won display a message congratulating them if the player has lost report it to them otherwise store this first roll as the point roll and keep rolling until the player wins or loses when the game ends ask if the player would like to play again ex project the game of craps http people cs pitt edu jmisurda teaching htm note that you need not match this exactly it is only shown to give you an idea of how to proceed rolling dice since we havent studied random numbers yet heres the code to roll a single die it returns a number to inclusive class craps public static void main string args int rolldie your code here public static int rolldie return int math random welcome to jons casino please enter your name jonathan would you like to play or quit play you have rolled you win would you like to play again y n n goodbye jonathan for this assignment you will be implementing a game of higher lower the computer will pick a random number between and see provided code below the human player will then try to guess that number for each guess the computer will respond simply higher or lower if the number it has chosen is greater than or less than the guess respectively the player should get attempts to guess the number correctly if the player hasnt guessed correctly the computer should reveal its number and the game is over example run welcome to higher lower enter your name jon ive chosen my number jon enter guess higher enter guess lower enter guess you guessed my number provided code class higherlower public static void main string args int compnum computerguess place your game code here public static int computerguess return int math random introduction you may remember from high school science that while mass stays constant your weight is actually dependent on your mass multiplied how strong the gravitational pull of the planet is so if you were to travel to different planets your weight would change in your first programming project you are asked to compute just how much someone would weigh on the other planets in our solar system background knowledge here is the table of relative gravitational strength as compared to earth planet relative strength mercury venus mars jupiter saturn uranus neptune pluto what you need to do your task is to write a program which asks the user to input a weight displays that weight scaled appropriately on each of the other planets example welcome to the planetary weight converter please enter the weight youd like to convert here is your weight on other planets mercury lbs venus lbs the moon lbs mars lbs jupiter lbs saturn lbs uranus lbs neptune lbs pluto lbs submission submission guidelines will be handed out next week separately requirements advice hints project planetary weights http people cs pitt edu jmisurda teaching htm your program must compile and run on the unixs cis pitt edu machines style which we will be discussing shortly will also be graded on in addition to correctness dont forget to have appropriate comments in your code start early in case you have any questions dont forget that the ta can help you too do not collaborate with anyone other than me or the ta do not search for a solution on the web overview algorithms are precise stepbystep instructions on how to accomplish a desired task a formal algorithm for use with computers or in mathematics must be very detailed and resolve the ambiguities that we take for granted in everyday life your first out of class assignment is to pick a common or interesting task and approach it algorithmically breaking it down to the level that a computer might understand assignment for this project you need to pick an everyday or interesting task and create an algorithm that explains how to do it you need to specify what if anything is input like ingredients in a recipe the process by which you operate on that input to produce or accomplish a task what the output of the algorithm is what if any assumptions are made in the algorithm what to turn in type your algorithm up using the following example as a template name the task your algorithm describes the input and output and then list the steps involved numbering each followed by what your algorithm assumes example algorithm calling a friend on the telephone input the telephone number of your friend output none steps pick up the phone and listen for a dial tone press each digit of the phone number on the phone if busy hang up phone wait minutes jump to step if no one answers leave a message then hang up if no answering machine hang up and wait hours then jump to step talk to friend hang up phone assumptions step assumes that you live alone and no one else could be on the phone the algorithm assumes the existence of a working phone and active service the algorithm assumes you are not deaf or mute the algorithm assumes a normal corded phone project real life algorithms http people cs pitt edu jmisurda teaching htm advice please don t go overboard with this assignment keep the algorithm under steps however do try to make this interesting avoid recipes for food or the like since the book deals with these extensively also note that you are allowed to be somewhat vague without being imprecise note i say here to leave a message but not what that message is that would be dependent on the individual call and not affect the actual process of making a call reasonable assumptions are along the lines of assuming the presence of necessary materials unreasonable assumptions introduce ambiguity like assuming the universal knowledge of how much a pinch of salt is grading you will be graded on your originality and the precision of the algorithm the task should be clear to me and reproducible from the steps you list to get a second opinion on this you will spend a few minutes in the class on the duedate trading them with your classmates this allows you to act both as the designer of the algorithm and as the computer it runs on note i will note accept your assignment if it is not typed practice test what is wrong with the following code class wrong public static void main string args int x printx public static void printx system out println x is x fix the above using parameters why can t you write a swap method in java write a for loop that iterates times how would you in java get a random number between and practice final http people cs pitt edu jmisurda teaching htm write a method called cointoss that returns for heads and for tails for each of the following provide the recursive definition and the base case a sum b factorial c power pick one of the above and write a recursive method in java to calculate it practice final http people cs pitt edu jmisurda teaching htm what is an object in object oriented programming what does it mean for a program to be eventdriven given the following method that you do not need to implement but can use int min int a int start that returns the position of smallest element in a starting at position a start write the code for selection sort void selectionsort int data practice final lab http people cs pitt edu jmisurda teaching htm name lab introduction to unix and java this first lab is meant to be an introduction to computer environments we will be using this term you must have a pitt id to complete this lab note text in unix is casesensitive is different from is different from all filenames of concern in this lab are lowercase please follow the instructions as listed in this document here are a few common unix commands cd change directory ls list all files in current directory pwd display the current directory mv move or rename a file cp copy a file javac compile a java file java run a java program pico edit a text file part i to login to the computers you will need to use an ssh client the ssh client that we will be using is fsecure ssh client located under programs in the start menu when you login first you are placed in your home directory the command ls will list all of the files and directories there the one we are most concerned with is the private directory it is special in that only you can access files inside this directory it will keep your work safe from other people lets move into the private directory so we can work there cd private changes directory to the private directory for this class well keep all of our files organized into a directory make it by typing mkdir if you want to double check that it worked type ls to list we now want to move into the directory to do our actual work lab http people cs pitt edu jmisurda teaching htm cd now we need to do some things to set up java so we can use it the rest of the term this only needs to be done this one time type cp public tio jar again typing ls should show you a file named tio jar cp copies a file a jar file is a lot like a zip file and we will extract it here using the jar command jar xvf tio jar that should show a bunch of files extracted and we are done with part i part ii while still in the directory type pico java pico is a very simple text editor a lot like notepad on windows it is what we will be writing our programs in this term type the following text in exactly as it is shown class public static void main string args system out println hello world save the file by hitting ctrl o and then enter exit pico by typing ctrl x at the bottom of the pico window it shows what keys do special things the means to hold ctrl while pressing the key back at the prompt type javac java which will make our program a file named class will be in the directory if we type ls run it by typing java show the output to the ta and you are done with the first lab helpful hints lab http people cs pitt edu jmisurda teaching htm keep this sheet as a guide until you get comfortable in unix every time you want to write a program you will do the following after logging into your account cd private cs here you will write and save your files using pico remember case matters description computers have become increasingly pervasive in todays society their power to do complicated and repetitive tasks quickly and efficiently has made them invaluable tools in modern society in this class you will learn to harness the power of a computer to do new tasks by creating your own software as opposed to using existing programs we will explore the limits of what a computer can and cannot do easily and provide you with the knowledge and experience to recognize those problems that you may find in life that can be solved with the help of a computer and the ability to make the computer do those tasks this class is meant as a first class in computer science anyone is welcome to take it as computers can be useful in a variety of fields it is also meant as an introduction to computer science for those students who wish to pursue the field further but lack prior exposure to the material prerequisites there are no prerequisites concepts from basic algebra will be drawn upon passing familiarity with the operation of a computer is also helpful if you are already proficient had or courses prior in computer programming this is not the course for you course purposes and goals introduction to computer programming is meant as a course to expose interested students how computer software is written and tested it seeks to illuminate the creative process that is computer programming from the ground up with an emphasis on good preplanning and style in this course we will be writing procedural programs in the java programming language not only will you learn what that precisely means but you will also at the end of the term be able to write computer programs in java that can perform a variety of tasks cs introduction to computer programming http people cs pitt edu jmisurda teaching htm recognize and create wellwritten programs in good programming style participate in the various stages of the lifecycle of a computer program including planning implementation and debugging textbook the text is pohl ira and mcdowell charlie java by dissection addisonwesley it can be found in the bookstore class policies exams there will be a midterm and a final in this class the scheduled final exam is on the final day of class insert date here cheating on exams will not be tolerated anyone caught cheating will be given a zero for the test and reported to the department following university procedures labs and quizzes attending recitation is an important part of this course in recitation you will be able to work in a structured setting while completing small tasks labs concepts from class will be expanded upon and tested with unannounced quizzes projects there will be six outofclass assignments given these are to be completed in the given time no extensions will be given without a valid excuse these are meant to be your own work anyone found to be collaborating will be given a zero for the assignment collaborating also means using code from previous terms other universities your friends or finding it on the internet participation attendance will not be taken but in such a small class any absence will be noticed several unexcused missed classes will adversely affect your grade grading your grade will be based upon exams projects weekly labs and quizzes the lowest one of which will be dropped and participation exams each projects each labs quizzes participation total the scale for the term will be percentage letter or above a a b b cs introduction to computer programming http people cs pitt edu jmisurda teaching htm c c d d dless than f disability resources and services if you have a disability for which you are requesting an accommodation you are encouraged to contact both your instructor and disability resources and services william pitt union as early as possible in the term drs will verify your disability and determine reasonable accommodations for this course term schedule the daily topics are subject to change depending on our pace they are there to assist you in the readings so you can focus on those concepts prior to class week readings for this week jbd project assigned tuesday topics introduction to the course computers and the internet using the lab and recitations introduction to problem solving compiling and running programs algorithms week readings for this week jbd project due tuesday no class thursday out of town topics keywords comments primitive types identifiers variables expressions operators user input strings predefined methods week readings for this week jbd appendix 15 cs introduction to computer programming http people cs pitt edu jmisurda teaching htm project assigned tuesday topics numeric types characters arithmetic expressions type conversion assignment increment decrement operator precedence week readings for this week jbd project due thursday project assigned thursday topics more expressions blocks empty statement boolean expressions relational operators logical operators conditional statements week readings for this week topics advanced conditionals introduction to repetition while for break continue switch problemsolving strategies style more applets text fields labels week readings for this week project due tuesday midterm exam thursday 16 topics introduction to methods structured programming calling defining methods review for the midterm week readings for this week project assigned tuesday topics return types details of parameter passing callbyvalue recursion 15 cs introduction to computer programming http people cs pitt edu jmisurda teaching htm parameters scope topdown design week readings for this week project due thursday 30 topics mathematical functions overloading style javadoc comments week readings for this week project assigned tuesday topics introduction to arrays declaring initializing accessing length passing to methods finding min max searching simple sorting searching sorted arrays week readings for this week project due thursday project assigned thursday topics bigoh notation boolean arrays char arrays arrays initializing arrays of nonprimitive types introduction to files creating text files reading text files week 19 readings for this week 6 topics encrypting decrypting text binary files detecting endoffile introduction to guis swing buttons text numerical input events week readings for this week none project 6 due tuesday 7 26 final exam thursday 7 28 during class 15 cs introduction to computer programming http people cs pitt edu jmisurda teaching htm 6 6 topics drawing revisited components layout managers review for the final project 6 blackjack refinements due tuesday july no late penalty for july with project we had an assumption that there was an infinite amount of cards available to play with in this project you should rectify this assumption by making a card unique and once dealt is never dealt again to make a card unique you should keep track of whether you have seen that particular card before if you have reject it and try to deal again a good place to keep track of whether youve seen a card is with an array one assumption you can make is that the deck is reshuffled between each game what you need to do modify your project to deal unique cards use an array to maintain a history of what cards youve seen submit this by the due date blackjack also known as is a multiplayer card game with fairly simple rules for this assignment you will be implementing a simplified version where a user can play against the computer who acts as dealer two cards are dealt to each player the dealer shows one card face up and the other is face down the player gets to see both of his or her cards and the total of them is added face cards kings queens and jacks are worth points aces are worth or points and all other cards are worth their face value the goal of the game is to get as close to blackjack without going over called busting the human player goes first making his or her decisions based on the single dealer card showing the player has two choices hit or stand hit means to take another card stand means that the player wishes no more cards and ends the turn allowing for the dealer to play the dealer must hit if their card total is less than and must stand if it is or higher whichever player gets closest to without exceeding it wins example run welcome to jons casino please enter your name jonathan the dealer you 14 would you like to hit or stand hit the dealer you 14 busted you busted dealer wins requirements for this assignment you need to do the following write a program that plays blackjack have the program use at least functions for the dealer for the player 15 project 5 blackjack http people cs pitt edu jmisurda teaching htm 2 3 to deal a card have the program intelligently determine if an ace should be interpreted as a or an this is somewhat difficult you need to be able to also handle multiple aces if there are any aces in the hand and the total exceeds change the to a i e subtract until there are no more aces in the hand or the total is below hint keep a counter that says how many aces have been dealt so far to a player you dont need to be able to deal from a real deck just generate a random number where 1 is an ace and 11 and 13 represent 10 as well to get the right distribution of cards you can ignore suit course objective the course aims to provide a gentle introduction to data science principles methodologies technologies and tools for non cs majors the name of the course comes from the famous arthur c clarke quote any sufficiently advanced technology is indistinguishable from magic course description this course is an introduction to data science for beginners designed as a first course in computer science for non cs majors and cs minors aiming to introduce students to basic data management technologies and data analytics skills the course will consist of about of introduction to computer programming using python of introduction to data management technologies and of introduction to data analytics the course will adopt the point of view of a user of data e g who is just combining data and analyzing it using tools and not a provider of data e g who would be implementing a database driven web site as is typically be case for related courses for cs majors course format lectures two minute meetings per week lectures will be fairly interactive and will in clude quizzes and student presentations class participation discussions and in class quizzes will be a component of the students final grade recitations one minute meeting per week friday needs to be in a computer lab to provide hands on experience for the projects and assignments some of the recitations will follow the flipped classroom model where additional material was given as reading practice assignment ahead of time exams we will have two midterm exams roughly timed at the one third and the two thirds time points of the term but no final exam since there will be a term project assignments there will be six assignments all of them being hands on using real data sets whenever applicable at least one of the assignment will optionally be a group assignment for groups of two to three students at least one assignment will include an in class presentation component term project the term project will be a group project teams of two to three students and it will involve hypothesis formation data acquisition data analysis and data presentation textbooks there are two reference textbooks both available for free for students through the university sub scription to o reilly safari bookshelf prerequisites although there are no formal prerequisites some familiarity with computers in general and with computer programming in particular are highly recommended anti requisites this course should not be taken by cs majors or anyone who has passed four of the five courses class web page http data witchcraft org all handouts and class notes will be published on the class web page you are expected to check this page frequently at least twice a week class communications policies new please read carefully mailing list all students will be automatically subscribed to the class mailing list so that they receive time sensitive announcements from the instructor and ta in class student responses we will use the socrative system http www socrative com to capture student responses to questions and record attendance it is crucial that you provide your pitt user account name e g at the name prompt to properly record your answers email to instructor and ta instead of email we will use the piazza system which is essentially a web based bulletin board for questions and clarifications to assignments more instructions will be posted on the class web site confidential email in case you need to communicate with the instructor and ta outside of the piazza system i e for confidential matters you should send email to staff cs pitt edu we will make every effort to respond to all email requests within one business day at the latest due to spam filtering you should always use your pitt email address when sending email and include your full name e mail communication policy each student is issued a university e mail address username pitt edu upon admittance this e mail address may be used by the university for official communication with stu dents students are expected to read e mail sent to this account on a regular basis failure to read and react to university communications in a timely manner does not absolve the student from knowing and com plying with the content of the communications the university provides an e mail forwarding service that allows students to read their e mail via other service providers e g gmail yahoo hotmail students that choose to forward their e mail from their pitt edu address to another address do so at their own risk if e mail is lost as a result of forwarding it does not absolve the student from responding to official commu nications sent to their university e mail address to forward e mail sent to your university account go to http accounts pitt edu log into your account click on edit forwarding addresses and follow the instructions on the page be sure to log out of your account when you have finished for the full e mail communication policy go to www bc pitt edu policies policy html technology policy new please read carefully since this is the century the use of laptops tablets and other digital devices is allowed in class however when using digital devices in the classroom you must be mindful when you are emailing tweeting texting surfing etc you are not paying attention research shows that no one can multitask that well you included paying attention and taking good notes is essential to success in this course isn t that why you are here be respectful your use of digital devices should not distract other students in the class it is unlikely that taking notes or searching class related topics will be distracting to the other students however viewing videos of kittens or ice bucket challenges gone well or gone wrong will likely distract others complaints about inappropriate technology use in class will result in your privileges being curtailed or revoked be honest emailing surfing and the use of any other applications or technologies is not allowed during examinations doing so unless explicitly allowed is considered cheating in the exam and will be dealt accordingly cell phone use new please read carefully answering a cell phone or texting is very disruptive and hence any use of a cell phone to make or receive calls or text messages is not permitted in the class or recitation cell phones must be switched to silent mode and if you have a phone call which cannot wait until the end of the class you need to step out of the class and then answer it audio video recording to ensure the free and open discussion of ideas students may not record classroom lectures discussion and or activities without the advance written permission of the instructor and any such recording properly approved in advance can be used solely for the student own private use copyrighted material all material provided through this web site is subject to copyright this applies to class and recitation notes slides handouts assignments solutions project descriptions etc you are allowed and expected to use all the provided material for personal use however you are strictly prohibited from sharing the material with others in general and from posting the material on the web or other file sharing venues in particular grading policy unless explicitly noted otherwise the work in this course is to be done independently discus sions with other students on the assignments should be limited to understanding the statement of the problems except when assignments are to be done in groups in which case it is expected of members of the same group to work together cheating in any way including giving your work to someone else will result in an f for the course and a report to the appropriate university authority submissions that are alike in a substantive way will be considered to be cheating by all involved parties please protect yourselves by only storing your files in private directories and by retrieving all printouts promptly students are expected to abide by the dietrich school of arts and sciences academic integrity code of conduct posted at http www as pitt edu fac policies academic integrity all assignments must be submitted electronically grades can be appealed up to two weeks after they have been posted no appeals will be considered after that time academic integrity policy cheating plagiarism will not be tolerated students suspected of violating the uni versity of pittsburgh policy on academic integrity noted below will be required to participate in the outlined procedural process as initiated by the instructor a minimum sanction of a zero score for the quiz exam or paper will be imposed for the full academic integrity policy go to www provost pitt edu info html late policy a late assignment will receive a deduction of points if it is up to one day past the deadline and points if it is up to two days past the deadline assignments that are past two days late will not be accepted make up policy students are expected to be present for all exams and quizzes make up exams will only be given in the event of an emergency and only if the instructor is informed in advance failure to notify the instructor prior to missing an exam will result in a zero for the exam final exam conflict policy in case you have a final exam conflict i e have more than two exams scheduled on the same date during finals week you need to notify the instructors of all classes involved in order to resolve the conflict by the sixth week of classes according to the university policy posted at http www registrar pitt edu classroomscheduling html students with disabilities if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and the office of disability resources and services william pitt union as early as possible in the term disability resources and services will verify your disability and determine reasonable accommodations for this course their web site is http www drs pitt edu religious observances in order to accommodate the observance of religious holidays students should inform the instructor by email of any such days that conflict with scheduled class activities within the first two weeks of the term tentative schedule a detailed reading guide will be published on the web page along with the class notes and additional online articles and resources time permitting the tentative class schedule for lectures recita tions assignments and exams will be as follows week introduction to data science and big data week introduction to python programming control flow variables basic data structures week python programming user defined functions week python programming file input output week python programming parsing popular data exchange formats csv json rss week introduction to data mining clustering association rule mining week intro to data visualization powerpoint tableau google fusion tables week data visualization using python week introduction to recommendation systems week introduction to data warehousing and statistical data summarization week introduction to relational databases week introduction to sql select project join queries week sql programming using python week advanced topics graph databases rest apis week term project presentations an denotes an assignment that week whereas a denotes a midterm examination there is no final exam ination ideas for term projects utilizing open data from different cities e g pittsburgh chicago and washington dc identifying combining and analyzing interesting datasets from http data gov using publicly available challenge data e g from kaggle debs etc utilizing movie rating data from http grouplens org datasets movielens analyzing historical weather data execute the following python program that prints random integers between and from random import randint for idx in range randint print try idx number drawn write a python program that generates random integers between and but stops when the number is generated write a python program that computes the compound interest for a bank account assume your initial deposit is stored in variable deposit and your interest rate is stored in variable rate the interest to be paid at the end of one month would then be interest deposit rate the deposit at the end of one month would then become deposit deposit interest write a python program that takes input from the user for an initial deposit amount an interest rate e g to denote and the number of months to compute the compound interest for print the interest and the updated deposit total for every month and the final deposit total at the end given the following dictionary sun cream of potato mon tomato basil tue cream of broccoli wed cream of potato thu wedding fri clam chowder sat cream of broccoli write a python program that will ask for a day from the user and produce the matching soup of the day or the message invalid day of the week if there is no match update the above program to make it loop continuously asking for a day of the week until the user gives the word banana as input write a python script to swap two variables for instance you have the following two variables before swapping a b after swapping a b write a python script that convert a temperature between fahrenheit and celsius the formula to convert a temperature from fahrenheit to celsius is f x c the formula to convert a temperature from celsius to fahrenheit is c f write a python script to create a list of all the courses that you are taking in this semester print the second course in the list in addition print the list value that corresponds to data witchcraft write a python script to check if a number if positive or negative write a python script to check if a number is odd or even write a python script to print the pattern abcabcabcabcabcabcabcabc using a string with not more than characters setting up python version to get started you will first need to check if python is preinstalled in your machine by typing python on the command line if it then you are good to get started if not then here are some resources to help you setup and start coding in python please note that we will be using in this course a installation to install python visit https www python org downloads and download the latest version release for your machine once the installation process is complete type python in command line again if python does not fire up then you will have to add python to your system path variable you can set the path using set command in windows or through the system variables dialogue box and export command in mac if you have both and installed please make sure that appears first on the system path b ide you are free to use any text editor or ide one option is to use pycharm ide with a free community edition for both windows and mac available here https www jetbrains com pycharm download c package manager you can install python packages using pip pip comes pre installed with but you can also install it from here https pip pypa io en stable installing alternatively anaconda is a great package manager that comes with over a pre installed python packages for data science and can found here https www continuum io downloads setting up python version to get started you will first need to check if python is preinstalled in your machine by typing python on the command line if it then you are good to get started if not then here are some resources to help you setup and start coding in python please note that we will be using x in this course 7 version of python but not or higher a installation to install python visit https www python org downloads and download any 7 x release for your machine once the installation process is complete type python in command line again if python does not fire up then you will have to add python to your system path variable you can set the path using set command in windows or through the system variables dialogue box and export command in mac if you have both and installed please make sure that appears first on the system path b ide you are free to use any text editor or ide one option is to use pycharm ide with a free community edition for both windows and mac available here https www jetbrains com pycharm download c package manager you can install python packages using pip pip comes pre installed with 7 but you can also install it from here https pip pypa io en stable installing alternatively anaconda is a great package manager that comes with over a pre installed python packages for data science and can found here https www continuum io downloads goal gain familiarity with python description this assignment is meant to help you gain some experience with the python programming language you are asked to complete the following problem sets from http snakify org how to do your assignment you need to signup to http snakify org using the following link http pythonlab org complete as many of the above problems you can on http snakify org and submit a text file txt that says which problems you did not do using the web submis sion interface below a sample assign txt file is provided you should fill out your first last name and your pitt email address that you also used to register for http snakify org if you complete all the problems then your assign txt file should only have lines that start with academic honesty the work in this assignment is to be done independently by you and only you discussions with other students on the assignment should be limited to understanding the statement of the problem cheating in any way including giving your work to someone else will result in an f for the course and a report to the appropriate university authority for further disciplinary action how to submit your assignment we will use a web based assignment submission interface to submit your assignment go to the class web page http org and click the submit button you can submit up to five files at a time you can come back and submit more files please note that files with the same name will overwrite previously submitted files use your pittid as the username and the last four digits of your peoplesoft id as the password there is a reminder service via email if you forget your password upload your assignment file to the appropriate assignment from the drop down list check through the web interface to verify what is the file size that has been uploaded and make sure it has been submitted in full it is your responsibility to make sure the assignment was properly submitted you must submit your assignment before the due date sunday february to avoid getting any late penalty the timestamp of the electronic submission will determine if you have met the deadline there will be no late submissions allowed after tuesday february goal gain familiarity with the web based assignment submission interface and the class honor code description the assignment is trivial and is meant to familiarize you with the web based assignment submission interface in order to avoid last minute problems or cases where you thought you properly submitted your assignment when you did not you are simply asked to submit one file a plain text a doc or a pdf file named pledge txt or pledge doc or pledge pdf with the following pledge data scientist data wizard data witch commitment i read the syllabus including the policies about cheating and on my honor as a student in cs in the spring term i will neither give nor receive aid on my assignments and exams sign your name date please note that we will not grade your other assignments unless this assignment has been properly submitted academic honesty the work in this assignment is to be done independently by you and only you discussions with other students on the assignment should be limited to understanding the statement of the problem cheating in any way including giving your work to someone else will result in an f for the course and a report to the appropriate university authority for further disciplinary action how to submit your assignment we will use a web based assignment submission interface to submit your assignment go to the class web page http org and click the submit button you can submit up to five files at a time you can come back and submit more files please note that files with the same name will overwrite previously submitted files use your pittid as the username and the last four digits of your peoplesoft id as the password there is a reminder service via email if you forget your password upload your assignment file to the appropriate assignment from the drop down list check through the web interface to verify what is the file size that has been uploaded and make sure it has been submitted in full it is your responsibility to make sure the assignment was properly submitted you must submit your assignment before the due date thursday january to avoid getting any late penalty the timestamp of the electronic submission will determine if you have met the deadline there will be no late submissions allowed after saturday january 01 introduction cs spring data witchcraft alexandros labrinidis http labrinidis cs pitt edu university of pittsburgh grading assignments assignments with programming class participation will use socrative system midterm exam tue feb snow day thu feb midterm exam thu mar term project due mon apr presentations in class apr 20 final exam n a industries that are data driven now but were not years ago transportation multimodal real time traffic advertising personalized target demographics insurance consumer facing airline pricing consumer facing aggregators hiring assessment intelligence hunch vs algorithm healthcare personalized medicine pitt me initiative reviews for shopping restaurants etc objectives after reading this chapter and completing the exercises you will be able to explain what a control structure is explain the difference between sequential selection and iterative control describe and use boolean operators explain the notion of logically equivalent boolean expressions explain what is meant by an infinite loop explain the difference between a definite and indefinite loop explain the use of indentation in python effectively use if statements in python for selection control effectively implement multi way selection in python effectively use while statements in python for iterative control chapter contents motivation fundamental concepts what is a control structure boolean expressions conditionals selection control iterative control computational problem solving calendar month program chapter control structures motivation the first electronic computers over sixty years ago were referred to as electronic brains this gave the misleading impression that computers could think although very complex in their design computers are machines that simply do step by step instruction by instruction what they are told thus there is no more intelligence in a computer than what it is instructed to do what computers can do however is to exe cute a series of instructions very quickly and very reliably it is the speed in which instructions can be executed that gives computers their power see figure since the execution of many simple instructions can result in very complex behavior and thus this is the enticement of computing a computer can accomplish any task for which there is an algorithm for doing so the instructions could be for something as simple as sorting lists or as ambitious as performing intelligent tasks that as of now only humans are capable of performing in this chapter we look at how to control the order that instructions are executed in python figure processing speed floating point operations per second flops fundamental concepts what is a control structure control flow is the order that instructions are executed in a program a control statement is a statement that determines the control flow of a set of instructions there are three fundamental forms of control that programming languages provide sequential control selection control and iterative control sequential control is an implicit form of control in which instructions are executed in the order that they are written a program consisting of only sequential control is referred to as a straight line program the program examples in chapter are all straight line programs selec tion control is provided by a control statement that selectively executes instructions while iterative boolean expressions conditions control is provided by an iterative control statement that repeatedly executes instructions each is based on a given condition collectively a set of instructions and the control statements controlling their execution is called a control structure few programs are straight line programs most use all three forms of control depicted in figure we look at selection control and iterative control next figure if statement boolean expressions conditions the boolean data type contains two boolean values denoted as true and false in python a boolean expression is an expression that evaluates to a boolean value boolean expressions are used to denote the conditions for selection and iterative control statements we look at the use of boolean expressions next relational operators the relational operators in python perform the usual comparison operations shown in figure relational expressions are a type of boolean expression since they evaluate to a boolean result these operators not only apply to numeric values but to any set of values that has an ordering such as strings note the use of the comparison operator for determining if two values are equal this rather than the single equal sign is used since the equal sign is used as the assignment operator this is often a source of confusion for new programmers num variable num is assigned the value num variable num is compared to the value chapter control structures figure the relational operators also is used for inequality simply because there is no keyboard character for the ﬁ symbol string values are ordered based on their character encoding which normally follows a lexographical dictionary ordering for example alan is less than brenda since the unicode ascii value for a is and b is however alan is greater than comes after brenda since the unicode encoding of lowercase letters comes after the encoding of uppercase letters recall from chapter that the encoding of any char acter can be obtained by use of the ord function let s tr y it from the python shell enter the following and observe the results hello hello hello zebra hello zebra membership operators python provides a convenient pair of membership operators these operators can be used to easily determine if a particular value occurs within a specified list of values the membership operators are given in figure the in operator is used to determine if a specific value is in a given list returning true if found and false otherwise the not in operator returns the opposite result the list of values surrounded by matching parentheses in the figure are called tuples in python tuples and lists are covered in chapter boolean expressions conditions figure the membership operators the membership operators can also be used to check if a given string occurs within another string dr in dr madison true as with the relational operators the membership operators can be used to construct boolean expressions boolean operators george boole in the mid developed what we now call boolean algebra his goal was to develop an algebra based on true false rather than numerical values boolean algebra contains a set of boolean logical operators denoted by and or and not in python these logical op erators can be used to construct arbitrarily complex boolean expressions the boolean operators are shown in figure logical and is true only when both its operands are true otherwise it is false logical or is true when either or both of its operands are true and thus false only when both operands are false logical not simply reverses truth values not false equals true and not true equals false chapter control structures figure boolean logic truth table one must be cautious when using boolean operators for example in mathematics to denote that a value is within a certain range is written as num in most programming languages however this expression does not make sense to see why let assume that num has the value the expression would then be evaluated as follows num true it does not make sense to check if true is less than or equal to some programming languages would generate a mixed type expression error for this the correct way of denoting the condition is by use of the boolean and operator num and num in some languages such as python boolean values true and false have integer values and respectively in such cases the expression num would evaluate to true would evaluate to which equals true this would not be the correct result for this expression however let see what we get when we do evaluate this expression in the python shell num num false we actually get the correct result false so what is going on here the answer is that python is playing a trick here for boolean expressions of the particular form var python automatically rewrites this before performing the evaluation var and var thus it is important to note that expressions of this form are handled in a special way in python and would not be proper to use in most other programming languages one must also be careful in the use of and or boolean operators for example not num and num is true for any value of num as is num or num and boolean expressions conditions therefore are not useful expressions the boolean expression num and num is also useless since it is always false finally boolean literals true and false are never quoted doing so would cause them to be taken as string values true and as we saw boolean expressions do not necessarily contain boolean operators for example is a boolean expression by definition boolean literals true and false are boolean expressions as well let s tr y it from the python shell enter the following and observe the results true and false and true or false or not true and false not or not true and false not or operator precedence and boolean expressions the operator precedence and operator associativity of arithmetic operators was given in chapter operator precedence also applies to boolean operators since boolean expressions can contain arithmetic as well as relational and boolean operators the precedence of all operators needs to be collectively applied an updated operator precedence table is given in figure figure operator precedence of arithmetic relational and boolean operators chapter control structures as before in the table higher priority operators are placed above lower priority ones thus we see that all arithmetic operators are performed before any relational or boolean operator true in addition all of the relational operators are performed before any boolean operator and true and false false or true or false true and as with arithmetic operators boolean operators have various levels of precedence unary boolean operator not has higher precedence than and and boolean operator and has higher precedence than or and or true and false or true false or true true not or not true or false false or false false as with arithmetic expressions it is good programming practice to use parentheses even if not needed to add clarity and enhance readability thus the above expressions would be better written by denoting at least some of the subexpressions and or not or if not all subexpressions and or not or finally note from figure above that all relational and boolean operators associate from left to right let s tr y it from the python shell enter the following and observe the results not true and false and not not true and false or true not or short circuit lazy evaluation there are differences in how boolean expressions are evaluated in different programming languages for logical and if the first operand evaluates to false then regardless of the value of the second operand the expression is false similarly for logical or if the first operand evaluates to true regardless of the value of the second operand the expression is true because of this some programming languages boolean expressions conditions do not evaluate the second operand when the result is known by the first operand alone called short circuit lazy evaluation subtle errors can result if the programmer is not aware of this for example the expression if n and n tolerance would evaluate without error for all values of n when short circuit evaluation is used if program ming in a language not using short circuit evaluation however a divide by zero error would result when n is equal to in such cases the proper construction would be if n if n tolerance in the python programming language short circuit evaluation is used logically equivalent boolean expressions in numerical algebra there are arithmetically equivalent expressions of different form for example x y z and xy xz are equivalent for any numerical values x y and z similarly there are logically equivalent boolean expressions of different form we give some examples in figure figure logically equivalent conditional expressions chapter control structures the range of values satisfying each set of expressions is shaded in the figure both expressions in are true for any value except the expressions in are true for any value except and the expressions in are only true for values in the range through inclusive the expressions in are true for all values except through inclusive figure lists common forms of logically equivalent expressions figure forms of logically equivalent boolean expressions the last two equivalences above are referred to as de morgan laws self test questions three forms of control in programming are sequential selection and control which of the following expressions evaluate to true a b c d e which of the following boolean expressions evaluate to true a dave ed b dave ed c dave dale selection control what is the value of variable num after the following is executed num num num num num num what does the following expression evaluate to for name equal to ann name in jacob maryann thomas evaluate the following boolean expressions using the operator precedence rules of python a and b and or which one of the following boolean expressions is not logically equivalent to the other two a not num or num b num and num c num and num answers iterative a b d a false a true b false b selection control a selection control statement is a control statement providing selective execution of instructions a selection control structure is a given set of instructions and the selection control statement controlling their execution we look at the if statement providing selection control in python next if statement an if statement is a selection control statement based on the value of a given boolean expression the if statement in python is depicted in figure figure if statement note that if statements may omit the else part a version of the temperature conversion program from chapter using an if statement is given in figure this program extends the original program by converting celsius to fahrenheit as well as fahrenheit to celsius the if statement line selects the appropriate set of instructions to execute based on user input f for fahrenheit to celsius and c for celsius to fahrenheit a statement that contains other statements such as the if statement is called a compound statement we look at python use of indentation in compound statements next chapter control structures figure temperature conversion two way conversion indentation in python one fairly unique aspect of python is that the amount of indentation of each program line is signifi cant in most programming languages indentation has no affect on program logic it is simply used to align program lines to aid readability in python however indentation is used to associate and group statements as shown in figure figure compound statement in python a header in python is a specific keyword followed by a colon in the figure the if else statement contains two headers if which f containing keyword if and else consisting only of the keyword else headers that are part of the same compound statement must be indented the same amount otherwise a syntax error will result selection control the set of statements following a header in python is called a suite commonly called a block the statements of a given suite must all be indented the same amount a header and its associated suite are together referred to as a clause a compound statement in python may consist of one or more clauses while four spaces is commonly used for each level of indentation any num ber of spaces may be used as shown in figure figure compound statements and indentation in python both a and b in the figure are properly indented in a both suites have the same amount of indentation in b each suite has a different amount of indentation this is syntactically correct although not good practice since the amount of indentation within each suite is consistent both c and d are examples of invalid indentation and thus syntactically incorrect in c the if and else headers of the if statement are not indented the same amount in d the headers are indented the same amount however the statements within the second suite are not properly aligned finally note that the suite following a header can itself be a compound statement another if statement for example thus compound statements may be nested one within another we look at nested com pounded statements next from idle create and run a python program containing the code on the left and observe the results modify and run the code to match the version on the right and again observe the results make sure to indent the code exactly as shown grade if grade print passing grade else print failing grade grade if grade print passing grade else print failing grade multi way selection in this section we look at the two means of constructing multi way selection in python one involving multiple nested if statements and the other involving a single if statement and the use of elif headers chapter control structures nested if statements there are often times when selection among more than two sets of statements suites is needed for such situations if statements can be nested resulting in multi way selection an example of this is given in figure figure multi way selection using if statements the nested if statements on the right result in a way selection in the first if statement if variable grade is greater than or equal to then grade of a is displayed therefore its else suite is not executed containing the remaining if statements if grade is less than the else suite is executed if grade is greater than or equal to grade of b is displayed and the rest of the if statements in its else suite are skipped and so on the final else clause is executed only if all the previous conditions fail displaying grade of f this is referred to as a catch all case as an example use of nested if statements and a check for invalid input in a program we give a revised ver sion of the temperature conversion program from figure in figure figure temperature conversion program input error dectection selection control in this version there is a catch all clause line for handling invalid input we next look at a more concise means of denoting multi way selection in python the elif header in python if statements may contain only one else header thus if else statements must be nested to achieve multi way selection python however has another header called elif else if that provides multi way selection in a single if statement shown in figure all the headers of an if elif statement are indented the same amount thus avoiding the deeply nested levels of indentation with the use of if else statements a final else clause may be used for catch all situations we next look at iterative control in python figure the elif header in python chapter control structures let apply it number of days in month program the following python program figure prompts the user for a given month and year for february and displays how many days are in the month this program utilizes the following pro gramming features if statement elif header lines provide the program header and program greeting on line variable is initialized to true for the input error checking performed line prompts the user for the month read as an integer value and stores in variable month on line the month of february is checked for february is the only month that may have a different number of days for a regular year and for leap years thus when february is entered the user is also prompted for the year line if the year is a leap year then variable is set to otherwise it is set to generally if a year is evenly divisible by then it is a leap year however there are a couple of exceptions if the year is divisible by but is also divisible by then it is not a leap year unless it is also divisible by then it is for example and were leap years but was not this condition is given below year and not year or year thus the conditions for which this boolean expression is true are and year and not year year and year selection control figure number of days in month program chapter control structures line checks if month is equal to or if true then is assigned to if not true line checks if month is equal to or all the remaining months except february if true then is assigned to if not true then an invalid month number was entered and is set to false finally the number of days in the month is dis played only if the input is valid line self test questions all if statements must contain either an else or elif header true false a compound statement is a a statement that spans more than one line b a statement that contains other statements c a statement that contains at least one arithmetic expression which of the following statements are true regarding headers in python a headers begin with a keyword and end with a colon b headers always occur in pairs c all headers of the same compound statement must be indented the same amount which of the following statements is true a statements within a suite can be indented a different amount b statements within a suite can be indented a different amount as long as all headers in the statement that it occurs in are indented the same amount c all headers must be indented the same amount as all other headers in the same statement and all statements in a given suite must be indented the same amount the elif header allows for a multi way selection that cannot be accomplished otherwise b multi way selection as a single if statement c the use of a catch all case in multi way selection answers false b a c c b iterative control an iterative control statement is a control statement providing the repeated execution of a set of instructions an iterative control structure is a set of instructions and the iterative control statement controlling their execution because of their repeated execution iterative control structures are commonly referred to as loops we look at one specific iterative control state ment next the while statement iterative control while statement a while statement is an iterative control statement that repeatedly executes a set of statements based on a provided boolean expression condition all iterative control needed in a program can be achieved by use of the while statement figure contains an example of a while loop in python that sums the first n integers for a given positive value n entered by the user figure the while statement in python as long as the condition of a while statement is true the statements within the loop are re executed once the condition becomes false the iteration terminates and control continues with the first state ment after the while loop note that it is possible that the first time a loop is reached the condition may be false and therefore the loop would never be executed suppose for the example in the figure that the user enters the value since variable current is initialized to referred to as a counter variable the first time the while statement is reached current is true thus the statements within the loop are executed and sum is updated to sum current since sum is initialized to sum becomes similarly current is updated and assigned to after the first time through the loop control returns to the top of the loop the condi tion is again found to be true and thus the loop is executed a second time in this iteration both sum and current become in the next iteration the condition is still true and therefore the loop is executed a third time this time sum becomes and current becomes thus when control returns to the top of the loop the condition is false and the loop terminates the final value of sum therefore is this process is summarized in figure figure iterative steps for adding first three integers chapter control structures input error checking the while statement is well suited for input error checking in a program this is demonstrated in the revised version of the temperature conversion program from figure reproduced in figure figure temperature conversion program invalid input checking the difference in this program from the previous version is that rather than terminating on invalid input the program continues to prompt the user until a valid temperature conversion f or c is entered thus the associated input statement is contained within a while loop that keeps iterat ing as long as variable which contains an invalid value once the user enters a proper value the loop terminates allowing the program to continue in idle create and run a simple program containing the code below and observe the results make sure to indent the code exactly as shown n sum current while current n sum sum current current current print sum n sum current while current n sum sum current current current print sum iterative control infinite loops an infinite loop is an iterative control structure that never terminates or eventually terminates with a system error infinite loops are generally the result of programming errors for example if the condition of a while loop can never be false an infinite loop will result when executed consider if the program segment in figure reproduced in figure omitted the statement incrementing variable current since current is initialized to it would remain in all iterations causing the expression current n to be always be true thus the loop would never terminate figure infinite loop such infinite loops can cause a program to hang that is to be unresponsive to the user in such cases the program must be terminated by use of some special keyboard input such as ctrl c to interrupt the execution from idle create and run a simple program containing the code below and observe the results make sure to indent the code exactly as shown to terminate an executing loop hit ctrl c while true print looping n sum current while current n sum sum current print sum n sum current while current n sum sum current n n print sum chapter control structures definite vs indefinite loops a definite loop is a program loop in which the number of times the loop will iterate can be determined before the loop is executed for example the while loop introduced in figure is a definite loop sum current n input enter value while current n sum sum current current current although it is not known what the value of n will be until the input statement is executed its value is known by the time the while loop is reached thus it will execute n times an indefinite loop is a program loop in which the number of times that the loop will iterate cannot be determined before the loop is executed consider the while loop in the temperature con version program of figure which input enter selection while which f and which c which input please enter f or c in this case the number of times that the loop will be executed depends on how many times the user mistypes the input thus a while statement can be used to construct both definite and indefinite loops in the next chapter we look at the for statement specifically suited for the construction of definite loops boolean flags and indefinite loops often the condition of a given while loop is denoted by a single boolean variable called a boolean flag this is shown in figure boolean variable is a boolean flag controlling the while loop at line if the mileage of the last oil change is greater than the current mileage an error message is displayed lines and the while loop is re executed if the current mileage is greater than or equal to the mileage of the last oil change is set to this difference and entries is set to true causing the loop to terminate thus lines display either that they are due for an oil change an oil change will soon be needed or there is no immediate need for an oil change iterative control figure indefinite loop using a boolean flag let apply it coin change exercise program the python program in figure implements an exercise for children learning to count change it displays a random value between and cents and asks the user to enter a set of coins that sums exactly to the amount shown the program utilizes the following programming features while loop if statement boolean flag random number generator on line the random module is imported for use of function randint this function is called on line to randomly generate a coin value for the user to match stored in variable amount lines provide the program greeting on line variable terminate is initialized to false used to control when the main loop and thus the program terminates on line is initialized to the empty string literal used to determine when the user has entered an empty line to end the coin entries these two variables need only be initialized once and therefore are assigned before the main while loop the game begins on line since boolean flag terminate is initialized to false the while loop is executed besides variable amount is initialized to false and total is initialized to variable serves as another boolean flag to determine if the current game is to continue or not the coin entry ends if either the user enters a blank line indicat ing that they are done entering coins in which case the result is displayed and is set to true line or if the total amount accumulated exceeds the total amount to be matched on line at the top of the while loop a third boolean flag is used the value of this flag determines whether the user should be prompted again because of an invalid input a value chapter control structures figure coin change exercise program continued other than or or the empty string note the use of the membership operator in line thus once the user inputs an appropriate value is set to true line otherwise the message invalid entry is displayed and remains false causing the loop to execute again the list of valid entered values on line includes variable since this is the value input when the user hits return to terminate their entry of coin values when the empty string is found line the total coin value entered in variable total is compared with variable amount the amount to be matched if equal the message correct is displayed line otherwise a message is displayed indicating how much they entered this amount is always less than the required amount since whenever variable total exceeds amount the current game ends lines iterative control figure coin change exercise program chapter control structures when line is reached boolean flag may be either true or false it is true when the user has indicated that they have entered all their coin values by hitting return or if the total of the coin values entered exceeds the value in variable amount it is false otherwise therefore flag variable is used to determine whether the user should be prompted to play another game line if they choose to quit the program when prompted then boolean vari able terminate is set to true this causes the encompassing while loop at line to terminate leaving only the final goodbye message on line to be executed before the program terminates self test questions a while loop continues to iterate until its condition becomes false true false a while loop executes zero or more times true false all iteration can be achieved by a while loop true false an infinite loop is an iterative control structures that a loops forever and must be forced to terminate b loops until the program terminates with a system error c both of the above the terms definite loop and indefinite loop are used to indicate whether a a given loop executes at least once b the number of times that a loop is executed can be determined before the loop is executed c both of the above a boolean flag is a a variable b has the value true or false c is used as a condition for control statements d all of the above answers true true true c b d computational problem solving calendar month program the problem the problem is to display a calendar month for any given month between january and december the format of the month should be as shown in figure problem analysis two specific algorithms are needed for this problem first we need an algorithm for computing the first day figure calendar month display of a given month for years through this algorithm is given in chapter the second needed algorithm is for appropriately displaying the calendar month given the day of the week that calendar month program the first day falls on and the number of days in the month we shall develop this algorithm the data representation issues for this problem are straight forward program design meeting the program requirements we will develop and implement an algorithm that displays the month as given there is no require ment of how the month and year are to be entered we shall therefore request the user to enter the month and year as integer values with appropriate input error checking data description what needs to be represented in the program is the month and year entered whether the year is a leap year or not the number of days in the month and which day the first of the month falls on given that information the calendar month can be displayed the year and month will be entered and stored as integer values represented by variables year and month year month the remaining values will be computed by the program based on the given year and month as given below variable holds a boolean true false value variables and each hold integer values algorithmic approach first we need an algorithm for determining the day of the week that a given date falls on the algo rithm for this from chapter is reproduced in figure we also need to determine how many days are in a given month which relies on an algorithm for determining leap years for the month of february the code for this has already been developed in the number of days in month program in section we shall also reuse the portion of code from that program for determining leap years reproduced below if year and not year or year true else false let review how this algorithm works and try to determine the day of the week on which may falls first variable holding the first two digits of the year is set to and holding the last two digits of the year is set to steps and variable value in step is then set to value floor floor floor chapter control structures figure day of the week algorithm from chapter in step since is equal to value is incremented by value value in step since the month is equal to may value is incremented by value value in step value is updated based on the day of the month since we want to determine the day of the week for the of may value is updated as follows value value day of the month mod mod mod therefore by step of the algorithm the day of the week for may is a saturday a table for the interpretation of the day of the week for the final computed value is given in figure calendar month program figure interpretation of the day of the week algorithm results overall program steps the overall steps in this program design are given in figure program implementation and testing stage determining the number of days in the month leap years we develop and test the program in three stages first we implement and test the code that deter mines for a given month and year the number of days in the month and whether the year is a leap year or not given in figure figure overall steps of calendar month program chapter control structures figure first stage of calendar month program the month and year entered by the user are stored in variables month and year while loops are used at lines and to perform input error checking lines are adapted from the previous number of days in month program for determining leap years lines are similar to the previous program for determining the number of days in a month stored in variable lines contain added code for the purpose of testing these instructions will not be part of the final program the program continues to prompt for another month until is entered thus boolean flag termi nate is initialized to false line and set to true line when the program is to terminate stage testing we give output from the testing of this version of the program in figure calendar month program figure example output of first stage testing the set of test cases for this stage of the program is given in figure the test cases are selected such that each month is tested within the and the month of february has a number of test cases to ensure that the program is working for non leap years typical leap years and exception years and the test plan also includes the extreme cases of january and december the beginning and end of the range of valid months all test cases are shown to have passed and thus we can move on to stage of the program development figure results of execution of test plan for stage stage determining the day of the week we give the next stage of the program in figure this version includes the code for determining the day of the week for the first day of a given month and year lines with the final print statement line displaying the test results note that for testing purposes there is no need to convert the day number into the actual name e g monday this raw output is good enough also for this program we will need to determine only the day of the week for the first day of any chapter control structures figure second stage of calendar month program continued given month since all remaining days follow sequentially therefore the day value in the day of the week algorithm part of the code is hard coded to on line let look at the code that imple ments the day of the week algorithm the algorithm operates separately on the first two digits and last two digits of the year on line integer division is used to extract the first two digits of the year for example calendar month program figure second stage of calendar month program equals on line the modulus operator is used to extract the last two digits for example equals the rest of the program through line follows the day of the week algo rithm given above stage testing we give a sample test run of this version of the program in figure figure example output of second stage testing figure shows the results of the execution of the test plan for this version of the program it includes the same months as in the test plan for the first stage since all test cases passed we can move on to the final stage of program development final stage displaying the calendar month in the final stage of the program figure we add the code for displaying the calendar month the corresponding name for the month number is determined on lines and displayed line the while loop at line moves the cursor to the proper starting column by printing chapter control structures figure results of execution of test plan for stage figure final stage of calendar month program continued calendar month program figure final stage of calendar month program continued the number of blank characters for each column to be skipped the while loop at line displays the dates single digit dates are output line with three leading spaces and two digit dates with two line so that the columns line up each uses the newline suppression form of print print to prevent the cursor from moving to the next screen line until it is time to do so variable is incremented from to the number of days in the month variable is also incremented by to keep track of what column the current date is being chapter control structures figure final stage of calendar month program calendar month program displayed in when equals it is reset to line and print moves the cursor to the start of the next line line otherwise is simply incremented by line an example test run of this final version of the program is given in figure figure example output of final stage testing something is obviously wrong the calendar month is displayed with eight columns instead of seven the testing of all other months produces the same results since the first two stages of the program were successfully tested the problem must be in the code added in the final stage the code at line simply assigns the month name therefore we reflect on the logic of the code start ing on line lines is where the column is reset back to column and a new screen line is started based on the current value of variable if else print variable is initialized to at line and is advanced to the proper starting col umn on lines variable is set to the value for the day of the week for the particular month being displayed since the day of the week results have been successfully tested we can assume that will have a value between and with that assump tion we can step though lines and see if this is where the problem is stepping through a program on paper by tracking the values of variables is referred to as deskchecking we check what happens as the value of approaches as shown in figure now it is clear what the problem is the classic off by one error the condition of the while loop should be not should be reset to once the seventh column has been displayed when is using the chapter control structures figure deskchecking the value of variable operator causes to be reset to only after an eighth column is displayed thus we make this correction in the program if else print after re executing the program with this correction we get the current output depicted in figure figure display output of final stage of calendar month program although the column error has been corrected we find that the first of the month appears under the wrong column the month should start on a wednesday fourth column not a thursday column fifth column the problem must be in how the first row of the month is displayed other months are tested each found to be off by one day we therefore look at lines that are responsible for moving over the cursor to the correct starting column while print chapter summary we consider whether there is another off by one error reconsidering the condition of the while loop we realize that in fact this is the error if the correct starting column is wednesday then the cursor should move past three columns and place a in the fourth column the current condition however would move the cursor past four columns thus placing a in the fifth column thursday the corrected code is given below while print the month is now correctly displayed we complete the testing by executing the program on a set of test cases figure although the test plan is not as complete as it could be it includes test cases for months from each century including both leap years and non leap years figure results of execution of test plan for final stage chapter summar y general topics control statement control structure sequential selection and iterative control relational operators boolean operators boolean expressions operator precedence and boolean expressions logically equivalent boolean expressions short circuit lazy evaluation selection control statements if statement compound statement multi way selection while statements input error checking infinite loops definite vs indefinite loops boolean flags and indefinite loops deskchecking python speciﬁc programming topics membership operators in not in if statement in python else and elif headers indentation in python multi way selection in python while statement in python chapter control structures chapter exercises section which of the three forms of control is an implicit form of control what is meant by a straight line program what is the difference between a control statement and a control structure section the boolean data type contains two literal values denoted as and in python which of the following relational expressions evaluate to true a c b d jake brian which of the following relational expressions evaluate to false a c e b d give an appropriate expression for each of the following a to determine if the number does not appear in a given list of numbers assigned to variable nums b to determine if the name ellen appears in a list of names assigned to variable names c to determine if a single last name stored in variable is either morris or morrison evaluate the following python expressions a b what value for x makes each of the following boolean expressions true a x or false b x and true c not x or false d not x and true evaluate the boolean expressions below for n and k a n and k b n or k c not n and k d not n and not k e n or k or k give an appropriate boolean expression for each of the following a determine if variable num is greater than or equal to and less than b determine if variable num is less than and greater than or equal to or it is equal to c determine if either the name thompson or wu appears in a list of names assigned to variable d determine if the name thomson appears and the name wu does not appear in a list of last names assigned to variable evaluate the following boolean expressions for and a not and b not and or chapter exercises give a logically equivalent expression for each of the following a num or num b num and num c not num and not num d num or num section give an appropriate if statement for each of the following a an if statement that displays within range if num is between and inclusive b an if statement that displays within range if num is between and inclusive and dis plays out of range otherwise rewrite the following if else statements using a single if statement and elif headers if temperature and humidity print muggy day today else if temperature print warm but not muggy today else if temperature print pleasant today else if temperature print cold today else print cool today regarding proper indentation a explain the change in indentation needed in order for the following code to be syntactically correct b indicate other changes in the indentation of the code that is not strictly needed but would make the code more readable if level print value is well within range print recheck in one year elif level print value is within range print recheck within one month elif level print value is slightly high print recheck in one week elif level print value abnormally high print shut down system immediately section write a program segment that uses a while loop to add up all the even numbers between and inclusive chapter control structures the following while loop is meant to multiply a series of integers input by the user until a sentinel value of is entered indicate any errors in the code given product num input enter first number while num num input enter first number product product num print product product for each of the following indicate which is a definite loop and which is an indefinite loop a num input enter a non zero value while num num input enter a non zero value b num while n print n n n python programming exercises write a python program in which the user enters either a b or c if a is entered the pro gram should display the word apple if b is entered it displays banana and if c is entered it displays coconut use nested if statements for this as depicted in figure repeat question using an if statement with elif headers instead write a python program in which a student enters the number of college credits earned if the number of credits is greater than senior status is displayed if greater than junior status is displayed if greater than sophomore status is displayed else freshman status is displayed write a program that sums a series of positive integers entered by the user excluding all numbers that are greater than write a program in which the user can enter any number of positive and negative integer values that displays the number of positive values entered as well as the number of negative values write a program containing a pair of nested while loops that displays the integer values ten num bers per row with the columns aligned as shown below display the integer values as given in question using only one while loop program modification problems program modification problems temperature conversion program input error checking modify the temperature conversion program in figure to perform input error checking of entered temperatures on the fahrenheit scale absolute zero is therefore all valid fahrenheit tem peratures start at that value with no upper limit on the celsius scale absolute zero is the program should reprompt the user for any invalid entered temperatures temperature conversion program addition of kelvin scale modify the temperature conversion program in figure to add an additional option of converting to and from degrees kelvin the formula for conversion to kelvin k from celsius c is k c number of days in month program input error checking modify the number of days in month program of section so that the program prompts the user to re enter any month not in the range or year that is an invalid value number of days in month program indication of leap years modify the number of days in month program of section so that the program displays in addition to the number of days in the month that the year is a leap year or not as shown below enter the month please enter the year e g there are days in the month a leap year oil change notification program number of miles before change modify the oil change notification program in figure so that the program displays the number of miles left before the next oil change or the number of miles overdue for an oil change as appropriate coin change exercise program addition of half dollar coins modify the coin change exercise program in section to allow for the use of half dollar coins make all necessary changes in the program coin change exercise program raising the challenge modify the coin change exercise program in section so that the least possible number of coins must be entered for example the least number of coins that total to cents is one quarter one dime one nickel and three pennies calendar month program indication of leap year modify the final version of the calendar month program in section so that for leap years the month heading is displayed as in the following calendar month program user entry of month name modify the final version of the calendar month program to allow the user to enter a month name e g january rather than a number e g make all appropriate changes in the program as a result of this change chapter control structures calendar month program day of the week headings modify the final version of the calendar month program in section so that there is day heading for each of the columns as shown below sage program modification following is the output of an all knowing sage program that replies with random responses to ques tions posed by the user the responses generated have no meaningful connection to the questions asked program development problems in the a program called eliza was developed that was able to behave as a psychotherapist it did not really understand anything it only looked for certain words to turn the patient comments or ques tions back to the patient for example if a patient said my mom drives me crazy it might reply with tell me more about your mom modify this program so that it appears to have understanding by similar means of word recognition as used in the eliza program specifically incorporate a set of trigger words that if found causes a specific response to be given for example if the word i appears in the question for example will i ever be rich or am i always going to be happy the response may be you are in charge of your own destiny if the word new appears in the question for example will i find a new boyfriend soon or will i find a new life the response may be changes are up to you and the unpre dictable events in life be creative in order to determine if a given word or phrase appears in a given question make use of the in membership operator program development problems metric conversion develop and test a python program that converts pounds to grams inches to centimeters and kilometers to miles the program should allow conversions both ways leap years to come develop and test a python program that displays future leap years starting with the first occurring leap year from the current year until a final year entered by the user hint module datetime used in the age in seconds program of chapter will be needed here chapter control structures the first time home buyer tax credit develop and test a python program that determines if an individual qualifies for a government first time home buyer tax credit of the credit was only available to those that a bought a house that cost less than b had a combined income of under and c had not owned a primary resi dence in the last three years home loan amortization develop and test a python program that calculates the monthly mortgage payments for a given loan amount term number of years and range of interest rates from to the fundamental formula for determining this is a d where a is the original loan amount and d is the discount factor the discount factor is calculated as d r n r r n where n is the number of total payments times the number of years of the loan and r is the interest rate expressed in decimal form e g divided by a monthly payment table should be generated as shown below loan amount term years interest rate monthly payment 1878 check your results with an online mortgage calculator life signs develop and test a program that determines how many breaths and how many heartbeats a person has had in their life the average respiration breath rate of people varies with age use the breath rates given below for use in your program breaths per minute infant years years years for heart rate use an average of beats per second in this chapter we look at a means of structuring and accessing a collection of data in particular we look at a way of organizing data in a linear sequence generally referred to as a list objectives after reading this chapter and completing the exercises you will be able to explain what a list is in programming describe the typical operations performed on lists explain what is meant by list traversal effectively create and use lists in python explain the difference between lists and tuples in python explain what a sequence is in python describe the sequence operations common to lists tuples and strings in python effectively use nested lists and tuples in python effectively iterate over lists sequences in python effectively use for statements for iterative control in python use the range function in python explain how list representation relates to list assignment in python effectively use list comprehensions in python write python programs using sequences chapter contents motivation fundamental concepts list structures lists sequences in python chapter lists iterating over lists sequences in python more on python lists computational problem solving calendar year program motivation the way that data is organized has a significant impact on how effec tively it can be used one of the most obvious and useful ways to organize data is as a list we use lists in our everyday lives we make shopping lists to do lists and mental checklists various forms of lists are provided by programming languages differing in the ele ments they can store mixed type their size variable size whether they can be altered mutable and the operations that can be per formed on them see figure lists also occur in nature our dna is essentially a long list of molecules in the form of a double helix found in the nucleus of all human cells and all living organisms its purpose is also to store information specifically the instructions that are used to construct all other cells in the body that we call genes given the billion nucleotides that make up the human genome determining their sequencing and thus understanding our genetic makeup is fundamentally a computational problem in this chapter we look at the use of lists and other sequences in python figure list properties and common operations list structures fundamental concepts list structures in this section we introduce the use of lists in programming the concept of a list is similar to our everyday notion of a list we read off access items on our to do list add items cross off delete items and so forth we look at the use of lists next what is a list a list is a linear data structure meaning that its elements have a linear ordering that is there is a first element a second element and so on figure depicts a list storing the average temperature for each day of a given week in which each item in the list is identified by its index value the location at index stores the temperature for sunday the location at index stores the temperature for monday and so on it is customary in programming languages to begin number ing sequences of items with an index value of rather than this is referred to as zero based indexing this is important to keep in mind to avoid any off by one errors in programs as we shall see we next look at some common operations performed on lists figure indexed data structure common list operations operations commonly performed on lists include retrieve update insert delete remove and append figure depicts these operations on a list of integers chapter lists figure common list operations the operation depicted in a retrieves elements of a list by index value thus the value is retrieved at index the fifth item in the list the replace operation in b updates the current value at index with the insert operation in c inserts the new value at index thus shifting down all elements below that point and lengthening the list by one in d the remove operation deletes the element at index thus shifting up all elements below that point and shortening the list by one finally the append operation in e adds a new value to the end of the list in the following sections we will see how these operations are accomplished in python first we look at what is called list traversal a way of accessing each of the elements of a given list list traversal a list traversal is a means of accessing one by one the elements of a list for example to add up all the elements in a list of integers each element can be accessed one by one starting with the first and ending with the last element similarly the list could be traversed starting with the last element list structures and ending with the first to find a particular value in a list also requires traversal we depict the tasks of summing and searching a list in figure figure list traversal self test questions what would be the range of index values for a list of elements a b c which one of the following is not a common operation on lists a access b replace c interleave d append e insert f delete which of the following would be the resulting list after inserting the value at index answers a c b a b c chapter lists lists sequences in python next we look at lists and other sequence types in python python list type a list in python is a mutable linear data structure of variable length allowing mixed type elements mutable means that the contents of the list may be altered lists in python use zero based indexing thus all lists have index values n where n is the number of elements in the list lists are denoted by a comma separated list of elements within square brackets as shown below one two three apples true an empty list is denoted by an empty pair of square brackets we shall later see the useful ness of the empty list elements of a list are accessed by using an index value within square brackets lst lst access of first element lst access of second element lst access of third element thus for example the following prints the first element of list lst print lst the elements in list lst can be summed as follows sum lst lst lst for longer lists we would want to have a more concise way of traversing the elements we discuss this below elements of a list can be updated replaced or deleted removed as follows for lst lst del lst replacement of with at index removal of at index methods insert and append also provide a means of altering a list lst insert insertion of at index lst append appending of to end of list in addition methods sort and reverse reorder the elements of a given list these list modifying operations are summarized in figure lists sequences in python figure list modification operations in python methods and the associated dot notation used are fully explained in chapter on objects and their use we only mention methods here for the sake of completeness in covering the topic of list operations tuples a tuple is an immutable linear data structure thus in contrast to lists once a tuple is defined it cannot be altered otherwise tuples and lists are essentially the same to distinguish tuples from lists tuples are denoted by parentheses instead of square brackets as given below nums student john smith computer science chapter lists another difference between tuples and lists is that tuples of one element must include a comma following the element otherwise the parenthesized element will not be made into a tuple as shown below correct wrong an empty tuple is represented by a set of empty parentheses we shall later see the useful ness of the empty tuple the elements of tuples are accessed the same as lists with square brackets nums student john smith any attempt to alter a tuple is invalid thus delete update insert and append operations are not defined on tuples for now we can consider using tuples when the information to represent should not be altered we will see additional uses of tuples in the coming chapters sequences a sequence in python is a linearly ordered set of elements accessed by an index number lists tuples and strings are all sequences strings like tuples are immutable therefore they cannot be altered we give sequence operations common to strings lists and tuples in figure for any sequence len gives its length and k retrieves the element at index k the slice operation returns a subsequence of a sequence starting with the first index location up to but not including the second the index form of the slice operation returns a string containing all the list elements starting from the given index location to the end of the sequence the count method returns how many instances of a given value occur within a sequence and the find method returns the index location of the first occurrence of a specific item returning if not found for determining only if a given value occurs within a lists sequences in python figure sequence operations in python sequence without needing to know where the in operator introduced in chapter can be used instead the operator is used to denote concatenation since the plus sign also denotes addition python determines which operation to perform based on the operand types thus the plus sign is referred to as an overloaded operator if both operands are numeric types addition is performed if both operands are sequence types concatenation is performed if a mix of numeric and sequence operands is used an unsupported operand type for error message will occur operations min max return the smallest largest value of a sequence and sum returns the sum of all the elements when of numeric type finally the comparison operator returns true if the two sequences are the same length and their corresponding elements are equal to each other let s tr y it from the python shell enter the following and observe the results coconut count o count count index o index index juice chapter lists nested lists lists and tuples can contain elements of any type including other sequences thus lists and tuples can be nested to create arbitrarily complex data structures below is a list of exam grades for each student in a given class in this list for example equals 91 and equals thus the following would access the first exam grade of the first student in the list however there is no need for intermediate variables and the exam grade can be directly accessed as follows 91 to calculate the class average on the first exam a while loop can be constructed that iterates over the first grade of each student list of grades sum k while k len sum sum k k k sum float len if we wanted to produce a new list containing the exam average for each student in the class we could do the following k while k len avg k k k append avg k k each time through the loop the average of the exam grades for a student is computed and appended to list when the loop terminates will contain the corresponding exam average for each student in the class lists sequences in python let s tr y it from the python shell enter the following and observe the results lst lst lst lst lst let apply it a chinese zodiac program the following program figure determines the animal and associated characteristics from the chinese zodiac for a given year of birth this program utilizes the following programming features tuples datetime module example execution of the program is given in figure figure execution of the chinese zodiac program line imports the datetime module it provides the current year line used to check for invalid years of birth only years between and the current year are considered valid lines perform the initialization for the program the variables on lines are assigned the characteristics of each animal the set of characteristics is represented as a tuple chapter lists figure chinese zodiac program line and not a list type since the information is not meant to be altered it associates each set of characteristics with the corresponding year of the twelve year cycle of the zodiac based on their position in the tuple we could have defined characteristics to contain each of the twelve string descriptions without the use of variables rat ox and so on it was written this iterating over lists sequences in python way for the sake of readability variable terminate initialized to false is a boolean flag used to quit the program once set to true in response to the user being asked to continue with another month or not at line lines display the program greeting lines comprise the main loop of the program the while loop at line ensures that the entered year is valid on line the for the individual is assigned a value between based on their year of birth since the year was the year of the rat in the chinese zodiac the value of is lines then use the as an index into tuple to get the animal for that birth year and tuple characteristics to get the associated personal characteristics to display the results self test questions which of the following sequence types is a mutable type a strings b lists c tuples which of the following is true a lists and tuples are denoted by the use of square brackets b lists are denoted by use of square brackets and tuples are denoted by the use of parentheses c lists are denoted by use of parentheses and tuples are denoted by the use of square brackets lists and tuples must each contain at least one element true false for lst what is the result of the following operation lst insert a b c which of the following is the correct way to denote a tuple of one element a b c d which of the following set of operations can be applied to any sequence a len i w concatenation b max i sum c len i sort answers b b false a d a iterating over lists sequences in python python for statement provides a convenient means of iterating over lists and other sequences in this section we look at both for loops and while loops for list iteration for loops a for statement is an iterative control statement that iterates once for each element in a specified sequence of elements thus for loops are used to construct definite loops for example a for loop is given in figure that prints out the values of a specific list of integers chapter lists figure the for statement in python variable k is referred to as a loop variable since there are six elements in the provided list the for loop iterates exactly six times to contrast the use of for loops and while loops for list iteration the same iteration is provided as a while loop below k while k len nums print nums k k k in the while loop version loop variable k must be initialized to and incremented by each time through the loop in the for loop version loop variable k automatically iterates over the provided sequence of values the for statement can be applied to all sequence types including strings thus iteration over a string can be done as follows which prints each letter on a separate line for ch in hello print ch next we look at the use of the built in range function with for loops the built in range function python provides a built in range function that can be used for generating a sequence of integers that a for loop can iterate over as shown below sum for k in range sum sum k iterating over lists sequences in python the values in the generated sequence include the starting value up to but not including the ending value for example range generates the sequence thus this for loop adds up the integer values the range function is convenient when long sequences of integers are needed actually range does not create a sequence of integers it creates a generator function able to produce each next item of the sequence when needed this saves memory especially for long lists therefore typing range in the python shell does not produce a list as expected it simply echoes out the call to range by default the range function generates a sequence of consecutive integers a step value can be provided however for example range produces the sequence with a step value of a sequence can also be generated backwards when given a negative step value for example range produces the sequence note that since the generated sequence always begins with the provided starting value up to but not including the final value the final value here is and not iterating over list elements vs list index values when the elements of a list need to be accessed but not altered a loop variable that iterates over each list element is an appropriate approach however there are times when the loop variable must iterate over the index values of a list instead a comparison of the two approaches is shown in figure figure iterating over the elements vs the index values of a given sequence chapter lists suppose the average of a list of class grades named grades needs to be computed in this case a for loop can be constructed to iterate over the grades for k in grades sum sum k print class average is sum len grades however suppose that the instructor made a mistake in grading and a point needed to be added to each student grade in order to accomplish this the index value the location of each element must be used to update each grade value thus the loop variable of the for loop must iterate over the index values of the list for k in range len grades grades k grades k in such cases the loop variable k is also functioning as an index variable an index variable is a variable whose changing value is used to access elements of an indexed data structure note that the range function may be given only one argument in that case the starting value of the range defaults to thus range len grades is equivalent to range len grades while loops and lists sequences there are situations in which a sequence is to be traversed while a given condition is true in such cases a while loop is the appropriate control structure another approach for the partial traversal of a sequence is by use of a for loop containing break statements we avoid the use of break state ments in this text favoring the more structured while loop approach let say that we need to determine whether the value occurs in list nums equal to in this case once the value is found the traversal of the list is terminated an example of this is given in figure variable k is initialized to and used as an index variable thus the first time through the loop k is and nums with the value is compared to since they are not equal the second clause of the if statement is executed incrementing k to the loop continues until either the item is found or the complete list has been traversed the final if statement determines iterating over lists sequences in python figure list search in python which of the two possibilities for ending the loop occurred displaying either item found or item not found finally note that the correct loop condition is k len nums and not k len nums otherwise an index out of range error would result let apply it password encryption decryption program the following program figure allows a user to encrypt and decrypt passwords containing uppercase lowercase characters digits and special characters this program utilizes the following programming features for loop nested sequences tuples example program execution is given in figure lines perform the initialization needed for the program variable is used to hold the encrypted or decrypted output of the program since the output string is created by appending to it each translated character one at a time it is initialized to the empty string chapter lists figure execution of password encryption decryption program variable holds the tuple of tuples used to encrypt decrypt passwords this tuple contains as elements tuples of length two a m b h etc the first tuple a m for example is used to encode the letter a thus when encrypting a given file each occurrence of a is replaced by the letter m when decrypting the reverse is done all occurrences of letter m are replaced by the letter a line contains the program greeting line inputs from the user whether they wish to encrypt or decrypt a password based on the response variable encrypting is set to either true or false line the program section in lines performs the encryption and decryption if variable encrypting is equal to true then is set to and is set to causing the direction of the substitution of letters to go from the first in the pair to the second a re placed by m when encrypting is false and thus decryption should be performed the direc tion of the substitution is from the second of the pair to the first m replaced by a variable line is set to the difference between the encoding of the low ercase and the uppercase letters recall that the encoding of the lowercase letters is greater than that of the uppercase letters the for loop at line performs the iteration over the pairs of letters in the encryption key the first time through the loop t a m thus t and t refer to each of the characters in the pair since all characters in the encryption key are in lowercase when uppercase letters are found in the password they are converted to lowercase by use of variable line before being compared to the lowercase letters in the encryption key this works because the character encoding of all lowercase letters is greater than the corresponding uppercase version ord a ord a ord a ord a a similar approach is used for converting from lowercase back to uppercase finally on lines the encrypted and decrypted versions of the password are displayed to the user iterating over lists sequences in python figure password encryption decryption program the substitution occurs in the nested for loops in lines the outer for loop iterates variable ch over each character in the entered password to be encrypted or decrypted the first step of the outer for loop is to initialize to false this variable is used to indicate if each character is a uppercase or lowercase letter if so it is replaced by its corresponding encoding character if not it must be a digit or special character and thus appended as is line the code on lines and lines is similar to each other the only difference is that since the letters in chapter lists the encryption key are all lowercase any uppercase letters in the password need to be converted to lowercase before being compared to the letters in the key self test questions for nums what does the following for loop output for k in nums print k a b c for nums what does the following for loop output for k in range print nums k a b c for fruit strawberry what does the following for loop output for k in range len fruit print fruit k end a srwer b tabry for nums what is the value of k when the while loop terminates k while k len nums and nums k k k a b c answers b b a b more on python lists in this section we take a closer look at the assignment of lists we also introduce a useful and convenient means of generating lists that the range function cannot produce called list comprehensions assigning and copying lists because of the way that lists are represented in python when a variable is assigned to another vari able holding a list each variable ends up referring to the same instance of the list in memory this is depicted in figure more on python lists figure assignment of lists this has important implications for example if an element of is changed then the corre sponding element of will change as well change made in change in causes a change in knowing that variables and refer to the same list explains this behavior this issue does not apply to strings and tuples since they are immutable and therefore cannot be modified when needed a copy of a list can be made as given below list in this case we get the following results list change made in change in does not cause any change in when copying lists that have sublists another means of copying called deep copy may be needed we will discuss this further in chapter when discussing objects in python let s tr y it from the python shell enter the following and observe the results red blue green red blue green list yellow yellow chapter lists list comprehensions the range function allows for the generation of sequences of integers in fixed increments list comprehensions in python can be used to generate more varied sequences example list compre hensions are given in figure figure list comprehensions in the figure a generates a list of squares of the integers in list in b squares are generated for each value in range in c only positive elements of list nums are included in the resulting list in d a list containing the character encoding values in the string hello is created finally in e tuple vowels is used for generating a list containing only the vowels in string w list comprehensions are a very powerful feature of python calendar year program computational problem solving calendar year program in this section we extend the calendar month program given in the computational problem solving section of chapter to display a complete calendar year the problem the problem is to display a calendar year for any year between and inclusive the format of the displayed year should be as depicted in figure figure calendar month display problem analysis the computational issues for this problem are similar to the calendar month program of chapter we need an algorithm for computing the first day of a given month for years however since the complete year is being displayed only the day of the week for january of the given year needs be computed the rest of the days follow from knowing the number of days in each month chapter lists including february for leap years the algorithm previously developed to display a calendar month however is not relevant for this program instead the information will first be stored in a data structure allowing for the months to be displayed three across program design meeting the program requirements we will develop and implement an algorithm that displays the calendar year as shown in figure we shall request the user to enter the four digit year to display with appropriate input error checking data description the program needs to represent the year entered whether it is a leap year the day of the week for january of the year and the number of days in each month accounting for leap years the names of each of the twelve months will also be stored for display in the calendar year given this information the calendar year can be appropriately constructed and displayed we make use of nested lists for representing the calendar year the data structure will start out as an empty list and will be built incrementally as each new calendar month is computed the list structures for the calendar year and calendar month are given below etc week week week k each italicized month is represented as a list of four to six strings with each string storing a week of the month to be displayed or a blank line for alignment purposes february may lines the strings are formatted to contain all the spaces needed for proper alignment when displayed for example since the first week of may begins on a friday the string value for this week would be the complete representation for the calendar year is given below with the details shown for the months of february and may calendar year program january march april june july august september october november december typically yearly calendars combine the one or two remaining days of the month on the sixth line of a calendar month onto the previous week we shall not do that in this program however algorithmic approach we make use of the algorithm for determining the day of the week previously used for this program however the only date for which the day of the week needs to be determined is january of a given year thus the original day of the week algorithm can be simplified by removing variable day and replacing its occurrence on line with given in figure figure simplified day of the week algorithm overall program steps the overall steps in this program design are given in figure program implementation and testing stage determining the day of the week for january we first develop and test the code for determining the day of the week for january of a given year this modified code from the calendar month program is given in figure chapter lists figure overall steps of calendar year program line initializes boolean flag terminate to false if the user enters for the year in lines terminate is set to true and the while loop at line terminates thus terminating the program if a valid year is entered lines are executed lines determine if the year is a leap year using the same code as in the calendar month program assigning boolean variable accordingly lines implement the simpli fied day of the week algorithm for determining the day of the week for january of a given year in figure with the result displayed on line stage testing we show a sample test run of this stage of the program in figure figure displays the test cases used for this program since all test cases passed we can move on to the next stage of program development stage constructing the calendar year data structure next we develop the part of the program that constructs the data structure holding all of the calendar year information to be displayed the data structure begins empty and is incrementally built con sisting of nested lists as previously discussed calendar year program figure stage of the calendar year program figure example output of first stage testing chapter lists figure test cases for stage of the calendar year program figure shows an implementation of this stage of the program lines perform the required initialization tuples and names have been added to the program to store the number of days for each month with february handled as an exception and the month names on line is initialized to the empty list it will be constructed month by month for the twelve months of the year there is the need for strings of blanks of various lengths in the program initialized as and lines the year data structure will contain all the space characters needed for the calendar months to be properly displayed therefore there will be no need to develop code that determines how each month should be displayed as in the calendar month program the complete structure will sim ply be displayed row by row lines are the same as the first stage of the program for determining the day of the week of a given date once the day of the week for january of the given year is known the days of the week for all remaining dates simply follow thus there is no need to calculate the day of the week for any other date line begins the for loop for constructing each of the twelve months on line the month name is retrieved from tuple and assigned to variable day holding the current day of the month is initialized to for the new month line in lines determined by the day of the week algorithm is converted to the appropriate column number thus since denotes saturday if equals is set to otherwise is set to e g if is then is set to in lines the initialization for a new month finishes with the reassignment of and each calendar week of a given month is initially assigned to the empty string with each date appended one by one variable is used to keep track of the current column day of the week incremented from to since the first day of the month can fall on any day of the week the first week of any month may contain blank skipped columns this includes the columns from up to but not including the while loop in lines 69 appends any of these skipped columns to empty string lines assign to the number of days stored in tuple the exception for february based on whether the year is a leap year or not is handled as a special case the while loop at line increments variable from to the number of calendar year program figure stage of the calendar year program continued chapter lists figure stage of the calendar year program days in the month in lines 81 each date is appended to right justified as a string of length three by use of the format function thus a single digit date will be appended with two leading blanks and a double digit date with one leading blank so that the columns of dates align for each new date appended to a check is made on line as to whether the end of the week has been reached if the last column of the calendar week has been reached when equals then the constructed string is appended to the line in addition is re initialized to the empty string and is reset to lines if the last column of the calendar week has not yet been reached then is simply incremented by line then on line variable is incremented by whether or not a new week is started when the while loop at line eventually terminates variable holds the last week of the constructed month therefore as with the first week of the month the last week may contain empty columns this is handled by lines before appending to any remaining unfilled columns are appended to it the reason that these final calendar year program columns must be blank filled is because months are displayed side by side and therefore are needed to keep the whole calendar properly aligned current_col11 thus the substring of produced will end up as an empty string if the value of is for saturday the last column as it should line sets variable to since holds the column value of the next column that would have been used for the current month and thus is the first day of the following month on line the completed current month is appended to list and on line is reset to an empty list in anticipation of the next month to be constructed finally on line the complete list is displayed because the program prompts the user for other years to be constructed and displayed the list is reset to the empty list line stage testing the program terminates with an error on line enter year yyyy to quit traceback most recent call last file c my python programs py line in module indexerror tuple index out of range this line is within the for loop at line for in range for some reason index variable is out of range for tuple we look at the final value of by typing the variable name into the python shell since has index values since of length an index value of should not be out of range how then can this index out of range error happen just to make sure that names has the right values we display its length len this is not right the tuple should contain all twelve months of the year that is the way it was initialized on line and tuples unlike lists cannot be altered they are immutable this does not seem to make sense to continue our investigation we display the value of the tuple january february march april may junejuly august september october november december chapter lists now we see something that doesn t look right months june and july are concatenated into one string value junejuly making the length of the tuple and not as we discovered that would explain why the index out of range error occurred what then is the problem why were the strings june and july concatenated we need to look at the line of code that creates this tuple january february march april may june july august september october november december it looks ok strings june and july were written as separate strings we then decide to count the number of items in the tuple since items in tuples and lists are separated by commas we count the number of items between the commas we count the items up to may which is five items as it should be then june which is six items ah there is no comma after the string june that must be why strings june and july were concatenated and thus the source of the index out of range error we try to reproduce this in the shell june july junejuly that it we have found the problem and should feel good about it after making the correction and re executing the program we get the following results enter year yyyy to quit enter year yyyy to quit we can see if the output looks like the structure that we expect the first item in the list the structure for the month of january is as follows 13 calendar year program in checking against available calendar month calculators we see that the first day of the month for january is a thursday thus the first week of the month should have four skipped days followed by and each in a column width of we find that there are fourteen blank characters in the first line the first twelve are for the four skipped columns and the last two are for the right justified string in the column of the first day of the month blank chars blank chars since there are five weeks in the month there should be one extra blank week at the end of the list to match the vertical spacing of all other months we see in fact that the last sixth string is a string of blanks since the structure looks correct we now develop the final stage of the program that displays the complete calendar year stage displaying the calendar year data structure we now give the complete calendar year program in figure in this final version the only change at the start of the program is that a program greeting is added on line the rest of the program is the same up to line the point where the calendar year has been constructed the print line and re initialization of to the empty list have been removed from the previous version since they were only there for testing purposes the new code in this version of the program is in lines which displays the calendar year the calendar year output is given in figure on line the year is displayed because the months are displayed three across as shown in figure the for loop on line iterates variable over the values thus when is months january march are displayed when is months april june are displayed and so forth the for loop at line displays the month names for each row for example january february and march each is displayed left justified in a field width of a leading blank character is appended to the formatting string to align with the first column of numbers displayed for each month the print form of print is used which prevents the cursor from moving to the next line thus the months can be displayed side by side variable separator contains the appropriate number of blank spaces initialized at the top of the pro gram to provide the required amount of padding between the months as shown below january february march lines perform the initialization needed for the following while loop at line which displays each week one by one of the current three months variable week is initial ized to zero for each month and is used to keep count of the number of weeks displayed variable is initialized to true to start the execution of the following while loop at line within the while loop is initialized to false it is then set to true by any or all of the current three months being displayed only if they still have more calendar lines weeks to print thus causing the while loop to continue with another chapter lists figure final stage of the calendar year program continued calendar year program figure final stage of the calendar year program continued chapter lists figure final stage of the calendar year program iteration this occurs within the for loop at lines since variable indicates the current month being displayed the number of weeks in the month is determined by the length of the tuple of strings for the current month k len k note that some months may have no more weeks to display whereas others may this is the case for the first three months of january february march 12 13 12 13 14 12 13 14 17 17 17 21 28 28 25 27 28 in this case the while loop needs to continue to iterate in order to display the last lines of january and march even though the last line of february has been displayed therefore in cases where a given month has a line to print but another month doesn t a blank line is displayed in order to main tain the correct alignment of month weeks after the week of dates or blank week is output for each of the three months the cursor is moved to the start of the next line on line and variable week is incremented by one line before the loop begins the next iteration for displaying the next row of calendar weeks chapter summary figure calendar year program output finally the while loop at line continues to iterate until there are no more lines to display for all of the three months currently being displayed that is until is false figure 25 displays the results of testing this final version by using the test plan from the calendar month program of chapter the test plan passed for all test cases chapter summar y general topics linear data structures list operations list traversal the empty list and its use nested lists list iteration loop variable index variable chapter lists figure 25 final calendar year program testing python speciﬁc programming topics lists in python list operations in python empty lists and tuples in python lists tuples and strings as sequences in python additional sequence operations nested lists and tuples in python for while loops and list iteration in python built in range function in python iterating over list sequence elements vs iterating over index values in python assigning lists in python list comprehensions in python chapter exercises section a give the index values of all the odd numbers in the following list representation assuming zero based indexing 23 16 14 chapter exercises b how many elements would be looked at when the list is traversed from top to bottom until the value was found section which of the following lists are syntactically correct in python a four b c four for lst what is the result of each of the following list operations a lst b lst insert c del lst d lst append for fruit apple banana pear cherry use a list operation to change the list to apple banana cherry for a list of integers lst give the code to retrieve the maximum value of the second half of the list for variable containing a string of letters and digits a give an if statement that outputs verified if contains both a z and a and outputs failed otherwise b give a python instruction that prints out just the last three characters in which of the following are valid operations on tuples for tuples and a len b c append d insert for hello world answer the following a give an instruction that prints the fourth character of the string b give an instruction that finds the index location of the first occurrence of the letter o in the string for a nested list lst that contains sublists of integers of the form a give a python instruction that determines the length of the list b give python code that determines how many total integer values there are in list lst c give python code that totals all the values in list lst d given an assignment statement that assigns the third integer of the fourth element sublist of lst to the value 12 section for a list of integers named nums a write a while loop that adds up all the values in nums b write a for loop that adds up all the values in nums in which the loop variable is assigned each value in the list c write a for loop that adds up all the elements in nums in which the loop variable is assigned to the index value of each element in the list d write a for loop that displays the elements in nums backwards e write a for loop that displays every other element in nums starting with the first element section 11 for and give the values of and where indicated after the following assignments a b c d chapter lists 12 give an appropriate list comprehension for each of the following a producing a list of consonants that appear in string variable w b producing a list of numbers between and that are divisible by c producing a list of numbers from a list of floating point values that are within some distance epsilon from python programming exercises write a python program that prompts the user for a list of integers stores in another list only those values between and displays the resulting list write a python program that prompts the user for a list of integers stores in another list only those values that are in tuple and displays the resulting list write a python program that prompts the user for a list of integers and stores them in a list for all values that are greater than the string over should be stored instead the program should display the resulting list write a python program that prompts the user to enter a list of first names and stores them in a list the program should display how many times the letter a appears within the list write a python program that prompts the user to enter a list of words and stores in a list only those words whose first letter occurs again within the word for example baboon the program should display the resulting list write a python program that prompts the user to enter types of fruit and how many pounds of fruit there are for each type the program should then display the information in the form fruit weight listed in al phabetical order one fruit type per line as shown below apple lbs banana 11 lbs etc write a python program that prompts the user to enter integer values for each of two lists it then should displays whether the lists are of the same length whether the elements in each list sum to the same value and whether there are any values that occur in both lists program modification problems chinese zodiac program japanese and vietnamese variations modify the chinese zodiac program in the chapter to allow the user to select the chinese zodiac the japanese zodiac or the vietnamese zodiac the japanese zodiac is the same as the chinese zodiac ex cept that pig is substituted with wild boar the vietnamese zodiac is also the same except that the ox is substituted with water buffalo and rabbit is replaced with cat chinese zodiac program improved accuracy the true chinese zodiac does not strictly follow the year that a given person was born it also depends on the month and date as well which vary over the years following are the correct range of dates for each of the zodiac symbols for the years to which includes two full cycles of the zodiac modify program development problems the chinese zodiac program in the chapter so that the user is prompted to enter their date of birth includ ing month and day and displays the name and characteristics of the corresponding chinese zodiac sym bol based on the more accurate zodiac provided here password encryption decryption program multiple executions modify the password encryption decryption program in the chapter so that it allows the user to continue to encrypt and decrypt passwords until they quit password encryption decryption program secure password check modify the password encryption decryption program in the chapter so that the program rejects any en tered password for encryption that is not considered secure enough a password is considered secure if it contains at least eight characters with at least one digit and one special character etc password encryption decryption program random key generation modify the encryption decryption program in the chapter so that a new encryption key is randomly generated each time the program is executed see the python programmers reference for information on the random module calendar year program multilingual version modify the calendar year program so that the user can select the language with which the calendar months are labeled give the user the choice of at least three different languages from which to select find the month names for the other languages online calendar year program flexible calendar format the program as is always displays three months per row modify the calendar year program so that the user can select how many months are displayed per row allow the user to select either two three or four months per row program development problems morse code encryption decryption program develop and test a python program that allows a user to type in a message and have it converted into morse code and also enter morse code and have it converted back to the original message the encoding of morse code is given below chapter lists format the original message containing english words so that there is one sentence per line format the morse code file containing dots and dashes so that there is one letter per line with a blank line following the last letter of each word and two blank lines following the end of each sentence except the last holidays calendar develop and test a python program that displays the day of the week that the following holidays fall on for a year entered by the user new year eve valentine day st patrick day april fool day fourth of july labor day halloween user birthday note that labor day as opposed to the other holidays above does not fall on the same date each year it occurs each year on the first monday of september the game of battleship battleship is a game involving ships at sea for each of two players the ships are located in a grid in which each column of the grid is identified by a letter and each row by a number as shown below program development problems the top half of the board contains the ships of player and the bottom half the ships of player the darkened areas indicate the size and location of ships each player starts with the same number and types of ships the location of each player ships is determined by the player players take turns taking a shot at the opponent ships by calling out a particular grid location for example if player calls out no ship would be hit in this example if however they were to call out then player ship on the bottom half of the board would be hit each player calls out hit or miss when they are shot at by the other player when all grid locations of a given ship have been hit the ship is sunk and the op ponent gets the number of points based on the ship size given below the number of grid locations that a given ship takes up indicates its type and point value a typical set of ships is given below develop and test a python program that can play the game of battleship the user should be able to select the skill level the higher the skill level the larger the grid that is created for play all games start with exactly one of each type of ship for each player the locations of the computer ships will be randomly placed the user however must be able to enter the location of each of their ships the computer shots into the opponent grid area should be randomly generated heuristic play for the game of battleship a heuristic is a general rule of thumb for solving a problem modify the game of battleship program from the previous problem so that the locations of the shots that the computer makes into the opponent grid area are based on heuristics rather than being randomly generated include an explanation of the heuristics developed up until this point we have viewed a computer program as a single series of instructions most programs however consist of distinct groups of instructions each of which accomplishes a specific task such a group of instructions is referred to as a routine program routines called functions in python are fundamental building blocks in software development we take our first look at functions in this chapter objectives after reading this chapter and completing the exercises you will be able to explain the concept of a program routine explain the concept of parameter passing explain the concept of value returning and non value returning functions explain the notion of the side effects of a function call differentiate between local scope and global scope define and use functions in python explain the concept of keyword and default arguments in python write a python program using programmer defined functions effectively use trace statements for program testing chapter contents motivation fundamental concepts program routines more on functions computational problem solving credit card calculation program program routines motivation so far we have limited ourselves to using only the most fundamental features of python variables expressions control structures input print and lists in theory these are the only instructions needed to write any pro gram that is to perform any computation from a practical point of view however these instructions alone are not enough the problem is one of complexity some smart phones for example contain over million lines of code see figure imagine the effort needed to develop and debug software of that size it certainly cannot be implemented by any one person it takes a team of programmers to develop such a project in order to manage the complexity of a large problem it is broken down into smaller subprob lems then each subproblem can be focused on and solved separately in programming we do the same thing programs are divided into manageable pieces called program routines or simply rou tines doing so is a form of abstraction in which a more general less detailed view of a system can be achieved in addition program routines provide the opportunity for code reuse so that systems do not have to be created from scratch routines therefore are a fundamental building block in software development in this chapter we look at the definition and use of program routines in python figure measures of lines of program code fundamental concepts program routines we first introduce the notion of a program routine we then look in particular at program routines in python called functions we have already been using python built in functions such as len range and others we now look more closely at how functions are used in python as well as how to define our own what is a function routine a routine is a named group of instructions performing some task a routine can be invoked called as many times as needed in a given program as shown in figure chapter functions figure program routine when a routine terminates execution automatically returns to the point from which it was called such routines may be predefined in the programming language or designed and implemented by the programmer a function is python version of a program routine some functions are designed to return a value while others are designed for other purposes we look at these two types of functions next defining functions in addition to the built in functions of python there is the capability to define new functions such functions may be generally useful or specific to a particular program the elements of a function definition are given in figure figure example of python function definition the first line of a function definition is the function header a function header starts with the key word def followed by an identifier avg which is the function name the function name is followed by a comma separated possibly empty list of identifiers called formal parameters or simply parameters following the parameter list is a colon following the function header is the body of the function a suite program block containing the function instructions as with all suites the statements must be indented at the same level relative to the function header program routines the number of items in a parameter list indicates the number of values that must be passed to the function called actual arguments or simply arguments such as the variables and below 25 16 avg functions are generally defined at the top of a program however every function must be defined before it is called we discuss more about function definition and use in the following sections value returning functions a value returning function is a program routine called for its return value and is therefore similar to a mathematical function take the simple mathematical function f x in this notation x stands for any numeric value that function f may be applied to for example f 2x pro gram functions are similarly used as illustrated in figure function avg takes three arguments and and returns the average of the three the function call avg 25 16 therefore is an expression that evaluates to the returned function value this is indicated in the function return statement of the form return expr where expr may be any expression next we look at a second form of program routine called for a purpose other than a returned function value figure call to value returning function chapter functions non value returning functions a non value returning function is called not for a returned value but for its side effects a side effect is an action other than returning a function value such as displaying output on the screen there is a fundamental difference in the way that value returning and non value returning functions are called a call to a value returning function is an expression as for the call to function avg result avg 25 16 factor when non value returning functions are called however the function call is a statement as shown in figure since such functions do not have a return value it is incorrect to use a call to a non value returning function as an expression figure call to non value returning function in this example function displaywelcome is called only for the side effect of the screen output produced finally every function in python is technically a value returning function since any func tion that does not explicitly return a function value via a return statement automatically returns the special value none we will however consider such functions as non value returning functions program routines let apply it temperature conversion program function version the following is a program figure that allows a user to convert a range of values from fahren heit to celsius or celsius to fahrenheit as presented in chapter in this version however the program is designed with the use of functions this program utilizes the following programming features value returning functions non value returning functions example execution of the program is given in figure figure execution of temperature conversion program chapter functions figure temperature conversion program function version in lines are defined functions displaywelcome getconvertto displayfahren tocelsius and displaycelsiustofahren the functions are directly called from the main module of the program in lines on line the non value returning function displaywelcome is called its job is to dis play information about the program to the user it does not need to be passed any arguments since it performs the same output each time it is called next on line value returning function get convertto is called this function also is not passed any arguments it simply asks the user to program routines enter either f or c to indicate whether they want to convert from fahrenheit to celsius or celsius to fahrenheit the input value entered is returned as the function value the instructions on line then prompt the user for the start and end range of temperatures to be converted this task does not warrant the construction of a function since there are only two input instructions to accomplish this the final part of the program displays the converted range of temperatures two non value returning functions are defined for accomplishing this task displayfahrentocelsius and displaycelsiustofahren each is passed two arguments and which indicate the range of temperature values to be converted what is left to look at is the implementation of each of the individual functions the imple mentation of function displaywelcome lines is very straightforward it simply contains three print instructions function getconvertto lines 13 contains a call to input followed by a while loop that performs input validation the user is forced to enter either f or c and is continually prompted to re enter as long as a value other than these two values is entered when the loop terminates variable which is returned by the return statement in line 13 function displayfahrentocelsius lines 21 and function displaycelsius tofahren lines 23 29 are similar in design each contains two parameters start and end which are each passed actual arguments and in the main section of the program each first prints the appropriate column headings followed by a for statement that iterates variable temp over the requested temperature range the conversion formula is different in each however each has the same final print instruction to print out the original temperature and the converted temperature in each of the columns self test questions the values passed in a given function call in python are called a formal parameters b actual arguments the identifiers of a given python function providing names for the values passed to it are called a formal parameters b actual arguments functions can be called as many times as needed in a given program true false when a given function is called it is said to be a subrogated b invoked c activated which of the following types of functions must contain a return statement a value returning functions b non value returning functions value returning function calls are a expressions b statements non value returning function calls are a expressions b statements which of the following types of routines is meant to produce side effects a value returning functions b non value returning functions answers b a true b a a b b chapter functions more on functions in this section we further discuss issues related to function use including more on function invoca tion and parameter passing calling value returning functions calls to value returning functions can be used anywhere that a function return value is appropriate result max here we apply built in function max to a list of integers examples of additional allowable forms of function calls are given below a result max max b result abs max c if max d print largest value in is max the examples demonstrate that an expression may contain multiple function calls as in a a function call may contain function calls as arguments as in b conditional expressions may contain function calls as in c and the arguments in print function calls may contain function calls as in d what if a function is to return more than one value such as function maxmin to return both the maximum and minimum values of a list of integers in python we can do this by returning the two values as a single tuple function definition def maxmin return max min function use a maxmin b high low maxmin in a above the returned tuple is assigned to a single variable thus temps contains the maximum temperature and contains the mini mum temperature in b however a tuple assignment is used in this case variables high and low are each assigned a value of the tuple based on the order that they appear thus high is assigned to the tuple value at index and low the tuple value at index of the returned tuple note that it does not make sense for a call to a value returning function to be used as a state ment for example max such a function call does not have any utility because the expression would evaluate to a value that is never used and thus is effectively thrown away finally we can design value returning functions that do not take any arguments as we saw in the getconvertto function of the previous temperature conversion program empty parentheses more on functions are used in both the function header and the function call this is needed to distinguish the identifier as denoting a function name and not a variable calling non value returning functions as we have seen non value returning functions are called for their side effects and not for a returned function value thus such function calls are statements and therefore can be used anywhere that an executable statement is allowed consider such a function call to display welcome from figure displaywelcome it would not make sense to treat this function call as an expression since no meaningful value is returned only the default return value none thus for example the following assignment state ment would not serve any purpose displaywelcome finally as demonstrated by function displaywelcome functions called for their side effects can be designed to take no arguments the same as we saw for value returning functions parentheses are still included in the function call to indicate that identifier displaywelcome is a function name and not a variable chapter functions parameter passing now that we have discussed how functions are called we take a closer look at the passing of argu ments to functions actual arguments vs formal parameters parameter passing is the process of passing arguments to a function as we have seen actual argu ments are the values passed to a function formal parameters to be operated on this is illustrated in figure figure parameter passing more on functions here the values of birthyr the user year of birth and hsgradyr the user year of high school graduation are passed as the actual arguments to formal parameters and each call is part of the same boolean expression ordered birthyr hsgradyr and ordered hsgradyr colgradyr in the second function call of the expression a differ ent set of values hsgradyr and colgradyr are passed formal parameter names and however remain the same note that the correspondence of actual arguments and formal parameters is determined by the order of the arguments passed and not their names thus for example it is perfectly fine to pass an actual argument named to formal parameter and actual argument to formal parameter as given in figure figure parameter passing and argument names in this example function ordered is called once with arguments and a second time with arguments each is a proper function call and each is what is logically needed in this instance chapter functions mutable vs immutable arguments there is an issue related to parameter passing that we have yet to address we know that when a function is called the current values of the arguments passed become the initial values of their cor responding formal parameters def avg avg 25 40 in this case literal values are passed as the arguments to function avg when variables are passed as actual arguments however as shown below def avg avg there is the question as to whether any changes to formal parameters and in the function result in changes to the corresponding actual arguments and in this case func tion avg doesn t assign values to its formal parameters so there is no possibility of the actual argu ments being changed consider however the following function def countdown n while n if n print n end else print n n n this function simply displays a countdown of the provided integer parameter value for example function call countdown produces the following output what if the function call contained a variable as the argument for example countdown tics since function countdown alters the value of formal parameter n decrementing it until it reaches the value does the corresponding actual argument have value as well countdown more on functions if you try this you will see that is unchanged now consider the following function def sumpos nums 9 for k in range len nums total sumpos if nums k total nums k return sum nums 9 function sumpos returns the sum of only the positive numbers in the provided argument it does this by first replacing all negative values in parameter nums with then summing the list using built in function sum we see above that the corresponding actual argument has been al tered in this case with all of the original negative values set to the reason that there was no change in integer argument above but there was in list argument has to do with their types lists are mutable thus arguments of type list will be altered if passed to a function that alters its value integers floats booleans strings and tuples on the other hand are immutable thus arguments of these types cannot be altered as a result of any function call it is generally better to design functions that do not return results through their arguments in most cases the result should be returned as the function return value what if a function needs to return more than one function value the values can be returned in a tuple as discussed above let s tr y it enter the following and observe the results num def incr n def update nums update n n nums nums incr num update num keyword arguments in python the functions we have looked at so far were called with a fixed number of positional arguments a positional argument is an argument that is assigned to a particular parameter based on its posi tion in the argument list as illustrated below def amount rate term 182 chapter functions this function computes and returns the monthly mortgage payment for a given loan amount amount interest rate rate and number of years of the loan term python provides the option of calling any function by the use of keyword arguments a key word argument is an argument that is specified by parameter name rather than as a positional argu ment as shown below note that keyword arguments by convention do not have a space before or after the equal sign def amount rate term this can be a useful way of calling a function if it is easier to remember the parameter names than it is to remember their order it is possible to call a function with the use of both positional and key word arguments however all positional arguments must come before all keyword arguments in the function call as shown below def amount rate term 06 this form of function call might be useful for example if you remember that the first argument is the loan amount but you are not sure of the order of the last two arguments rate and term more on functions default arguments in python python also provides the ability to assign a default value to any function parameter allowing for the use of default arguments a default argument is an argument that can be optionally provided as shown here def amount rate 35000 in this case the third argument in calls to function is optional if omitted pa rameter term will default to the value years as shown if on the other hand a third argument is provided the value passed replaces the default parameter value all positional arguments must come before any default arguments in a function definition variable scope looking back at the temperature conversion program in section we see that functions displayfahrentocelsius and displaycelsiustofahren each contain variables named temp and we ask do these identifiers refer to common entities or does each function have its own distinct entities the answer is based on the concept of identifier scope which we discuss next local scope and local variables a local variable is a variable that is only accessible from within a given function such variables are said to have local scope in python any variable assigned a value in a function becomes a local vari able of the function consider the example in figure chapter functions figure defining local variables both and contain identifier n function assigns n to while function assigns n to both functions display the value of n when called displays the value of n both before and after its call to if identifier n represents the same variable then shouldn t its value change to after the call to however as shown by the output the value of n remains this is because there are two distinct instances of variable n each local to the func tion assigned in and inaccessible from the other now consider the example in figure 11 in this case the functions are the same as above except that the assignment to variable n in is commented out figure 11 inaccessibility of local variables in this case we get an error indicating that variable n is not defined within this is because variable n defined in is inaccessible from in this case n is expected to be a global variable discussed next the period of time that a variable exists is called its lifetime local variables are automatically created allocated memory when a function is called and destroyed deallocated when the function more on functions terminates thus the lifetime of a local variable is equal to the duration of its function execution consequently the values of local variables are not retained from one function call to the next the concept of a local variable is an important one in programming it allows variables to be defined in a function without regard to the variable names used in other functions of the program it also allows previously written functions to be easily incorporated into a program the use of global variables on the other hand brings potential havoc to programs discussed next global variables and global scope a global variable is a variable that is defined outside of any function definition such variables are said to have global scope this is demonstrated in figure 12 figure 12 access to value of global variable variable max is defined outside and and therefore global to each as a result it is directly accessible by both functions for this reason the use of global variables is generally considered to be bad programming style although it provides a convenient way to share values among functions all functions within the scope of a global variable can access and alter it this may include functions that have no need to access the variable but none the less may unintention ally alter it another reason that the use of global variables is bad practice is related to code reuse if a function is to be reused in another program the function will not work properly if it is reliant on the existence of global variables that are nonexistent in the new program thus it is good chapter functions programming practice to design functions so all data needed for a function other than its local variables are explicitly passed as arguments and not accessed through global variables let apply it gpa calculation program the following program figure 14 computes a semester gpa and new cumulative gpa for a given student this program utilizes the following programming features tuple assignment figure 13 illustrates an example execution of the program figure 13 execution of gpa calculation program figure 14 gpa calculation program continued more on functions figure 14 gpa calculation program the program begins with the display of the program greeting on line lines get the num ber of earned credits and current cumulative gpa from the user these two variables are bundled into a tuple named on line since they are always used together bundling these variables allows them to be passed to functions as one param eter rather than as separate parameters function getgrades is called on line which gets the semester grades from the user and assigns it to variable the value returned by function getgrades is chapter functions a list of sublists in which each sublist contains the letter grade for a given course and the associ ated number of credits a b a c on line function calculategpa is called with arguments and the function returns a tuple containing the semester gpa and new cumulative gpa of the user a tuple assignment is used to unpack the two values into variables and finally these values are displayed on lines and function calculategpa is defined in lines with parameters info and a gpa is calculated as the total quality points earned for a given set of courses divided by the total number of credits the courses are worth the number of qual ity points for a given course is defined as a course grade times the number of credits the course is worth thus assuming a grade of a is worth points b worth points and grades of c d and f worth and points respectively to calculate the semester gpa for a student receiving a in two four credit courses b in two three credit courses and a c in a one credit course would be a a b b c where is the total number of credits of all courses similarly in order to calculate a new cumulative gpa the total quality points of the current cumulative gpa plus the total quality points of the new semester gpa is divided by the total number of credits the student has earned to date thus to calculate a new cumulative gpa for a current cu mulative gpa of 25 earning thirty credits and a new semester gpa as given above earning fifteen credits would be 25 47 with total earned credits thus in function calculategpa local variables pts and are initialized to zero their values for the courses provided in parameter are computed in the for loop on lines 29 this loop also calculates the semester quality points and the number of credits of the current semester assigned to local variables and respectively at lines and note that in the cal culation of the semester quality points function convertgrade is called to convert each letter grade to its corresponding numerical value finally at the end of function calculategpa local variable is assigned to the total semester quality points divided by the total semester credits similarly local variable is assigned to the total quality points to date divided by the total number of credits earned to date finally on line a tuple is returned containing both of these computed values the remaining functions defined in this program are convertgrade and getgrades function convertgrade is passed a letter grade and returns the corresponding numerical value since the ordinal value via the ord function of letters in python are sequential integers determin ing the difference between the ordinal value of a and the ordinal value of a given letter grade allows credit card calculation program the numerical value of the letter grade to be determined for example for a letter grade of a through d its numerical value is determined and returned as return ord a ord a return return ord b ord a return return ord c ord a return return ord d ord a return since there is no letter of grade e used a grade of f has to be handled separately finally function getgrades returns a list of sublists of grades and credits entered by the user as mentioned above thus local variable is initialized to an empty list on line the while loop at line 14 iterates until boolean variable is false initial ized to true in line 11 the loop continues to iterate and append another pair of grade credits to the list until the user hits the enter key when prompted for a course grade line self test questions a function call can be made anywhere within a program in which the return type of the function is appropriate true false an expression may contain more than one function call true false function calls may contain arguments that are function calls true false all value returning functions must contain at least one parameter true false every function must have at least one mutable parameter true false a local variable in python is a variable that is a defined inside of every function in a given program b local to a given program c only accessible from within the function it is defined a global variable is a variable that is defined outside of any function definition true false the use of global variables is a good way to allow different functions to access and modify the same variables true false answers true true true false false c true false computational problem solving credit card calculation program in this section we design implement and test a program that will allow us to determine the length of time needed to pay off a credit card balance as well as the total interest paid the problem the problem is to generate a table showing the decreasing balance and accumulating interest paid on a credit card account for a given credit card balance interest rate and monthly payment as shown in figure chapter functions figure 15 example execution of the credit card calculation program problem analysis the factors that determine how quickly a loan is paid off are the amount of the loan the interest rate charged and the monthly payments made for a fixed rate home mortgage the monthly payments are predetermined so that the loan is paid off within a specific number of years therefore the total interest that will be paid on the loan is made evident at the time the loan is signed for a credit card there is only a minimum payment required each month it is not always ex plicitly stated by the credit card company however how long it would take to pay off the card by making only the minimum payment the minimum payment for a credit card is dependent on the particular credit card company however it is usually around of the outstanding loan amount each month and no less than twenty dollars thus calculating this allows us to project the amount of time that it would take before the account balance becomes zero as well as the total interest paid program design meeting the program requirements no particular format is specified for how the output is to be displayed all that is required is that the user be able to enter the relevant information and that the length of time to pay off the loan and the total interest paid is displayed the user will also be given the choice of assuming the monthly payment to be the required minimum payment or a larger specified amount credit card calculation program data description all that needs to be represented in this program are numerical values for the loan amount the inter est rate and the monthly payment made there is no need to create a data structure as the table of payments can be generated as it is displayed algorithmic approach the only algorithm needed for this problem is the calculation of the required minimum payment the minimum payment is usually calculated at or of the outstanding balance with a lower limit of around therefore we will assume a worst case scenario of a minimum payment calcu lated at with a minimum payment of overall program steps the overall steps in this program design are given in figure 16 figure 16 overall steps of the credit card calculation program program implementation and testing stage developing the overall program structure we first develop and test the overall program structure given in figure 17 the program begins on line 15 with a call to function displaywelcome next the cur rent credit card balance and annual interest rate apr are input from the user lines each read as an integer value since the monthly interest rate is what will be used in the calculations the value in apr is divided by on line 21 this converts the value to a monthly interest rate as well as converting it to decimal form for example 18 as 18 the final value input from the user is the monthly payments that they wish to have the payoff calculated with they have a choice of either going with the minimum required monthly payment assumed to be for testing purposes line 28 or a specified monthly payment line 31 the credit card balance annual percentage rate and the assumed monthly payments are passed to func tion displaypayments on line to calculate and display the pay down of the balance as well as the interest paid over each month of the payoff period chapter functions figure 17 credit card calculation program stage functions displaywelcome line and displaypayments line consist only of trace statements a trace statement prints for testing purposes a message indicating that a certain point in the program has been reached trace statements are also used to display the value of certain vari ables once this part of the program is working we can focus on implementing the functions and further developing the main program section stage testing we show sample test runs of this version of the program in figure 18 from the test results we see that the appropriate values are being input and passed to func tion displaypayments so it looks like the overall structure of this stage of the program is working correctly stage generating an unformatted display of payments we next implement function displaywelcome and develop an initial implementation of function displaypayments given in figure 19 we remove the two print instructions that were included only for test purposes in stage of the program previously on lines 27 and credit card calculation program figure 18 output of first stage testing also the minimum required monthly payment is computed lines 45 48 rather than being set to function displaypayments is where most of the work is done in the program therefore we shall develop this function in stages as well at this point we develop the function to display for each month during the loan payoff the year the current balance and the total interest paid to date we delay issues of screen formatting for the alignment of numbers and only include formatting for rounding numeric values to two decimal places the while loop on line 21 iterates while balance passed as an argument to the function is greater than zero the function will keep count of the number of months lines displayed as well as the total interest paid variables and are used for this pur pose and are therefore initialized before the loop to lines 11 12 on lines 15 18 the initial in formation for the calculation is displayed within the while loop on line 22 the monthly interest paid is computed as the current balance of that month during the payoff period balance times the monthly interest rate the total interest paid is then updated on line 23 on line 24 the new balance is computed as the current balance plus the interest for the month minus the monthly payment the next step is to display these computed values since time is kept track of in terms of months the current year to be displayed is computed using integer division line adding one so that the first year is displayed as and not then on line 27 the line representing the payment for the current month is displayed formatting is used so that all numerical values are displayed with two decimal places finally variable is incremented by one for the next iteration of the loop stage testing we test this program once for a specified monthly payment amount and once for the option of minimum monthly payments the results are given in figures and 21 chapter functions figure 19 credit card calculation program stage credit card calculation program figure output of second stage testing user entered payment figure 21 output of second stage testing minimum payment chapter functions clearly there is something wrong with this version of the program the valueerror generated in figure indicates that the format specifier is an unknown format code for a string type value referring to line 18 thus this must be referring to variable but that should be a numeric value and not a string value how could it have become a string type let check if the problem also occurs when selecting the minimum payment option figure 21 in this case the program works since the problem only occurred when the user entered the monthly payment as opposed to the minimum payment option we next try to determine what dif ferences there are in the program related to the assignment of variable determine monthly payment response input use the minimum monthly payment y n if response in y y if balance else balance else input enter monthly payment when the user selects the minimum monthly payment option variable is set to integer value or of the current balance if balance is greater than otherwise its value is input from the user this variable is not redefined anywhere else in the program since the variable is not a local variable we can display its value directly from the python shell it is a string value we immediately realize that the input value for variable was not converted to an integer type and was thus left as a string type we fix this problem by re placing the line with the following int input enter monthly payment this explains why the problem did not appear in the testing of stage of the program in that ver sion variable was never formatted as a numeric value and also never used in a numerical calculation both of which would have generated an error at this point we execute a number of test cases for various initial balances interest rates and monthly payments the result is given in figure 22 checked against online loan payoff calculator tools we next move on to the final stage of program development stage formatting the displayed output in this final stage of the program input error checking is added the program is also modified to allow the user to continue to enter various monthly payments for recalculating a given balance pay off output formatting is added to make the displayed information more readable finally we cor rect the display of a negative balance at the end of the payoff schedule as appears in figure 21 the final version of the program is given in figure 23 credit card calculation program figure 22 test cases for stage of the credit card calculation program figure 23 final stage of the credit card calculation program continued chapter functions figure 23 final stage of the credit card calculation program continued credit card calculation program figure 23 final stage of the credit card calculation program the first set of changes in the program provides some input error checking we will address means of more complete error checking in chapter in lines 56 tuples and response are defined these are used to check if input from the user is an appropriate yes no response for example the while statement on line checks that the input from line is either y y n or n while response not in by checking if response is in the concatenation of tuples and for determining the specific response the tuples can be used individually line if response in similar input error checking is done on line the next set of changes allows a number of payoff schedules for an entered balance to be calculated a while statement is added at line with its condition based on the value of boolean variable calc initialized to true on line to accommodate the recalculation of payoff sched ules variables and are each reset to in function displaypayments lines 11 13 output formatting is added in function displaypayments on line 18 payoff sched ule is displayed right justified within a field of twenty on lines 19 the column headings are displayed with appropriate field widths lines display the balance payment number and in terest of each month aligned under the column headings lines 35 ensure that each year is dis played only once finally in lines 29 variable balance is set to zero if it becomes negative so that negative balances are not displayed chapter functions stage testing we give example output of this version of the program for both a payoff using the required minimum monthly payment and for a user entered monthly payment in figures 24 and 25 figure 25 depicts a portion of the output for the sake of space we run the same set of test cases used in the testing of the previous stage version of the program given in figure 26 based on these results we can assume that the program is functioning properly figure 24 output of third stage testing user entered payment figure 25 output of third stage testing minimum payment figure 26 test cases for stage of the credit card calculation program chapter functions chapter summar y general topics program routines value returning vs non value returning functions side effects of function calls parameter passing actual arguments vs formal parameters local scope and local variables global scope and global variables variable lifetime python speciﬁc programming topics defining functions in python built in functions of python value returning and non value returning functions in python tuple assignment in python mutable vs immutable arguments in python local vs global variables in python chapter exercises section function avg returns the average of three values as given in the chapter which of the following state ments each making calls to function avg are valid assume that all variables are of numeric type a result avg b result avg avg c result avg n4 d print avg e avg n3 which of the following statements each involving calls to function displaywelcome displaying a welcome message on the screen as given in the chapter are valid a print displaywelcome b displaywelcome c result displaywelcome d displaywelcome section suppose there are nine variables each holding an integer value as shown below for which the average of the largest value in each line of variables is to be computed 25 max1 25 15 35 max2 35 20 25 max3 average max1 max2 max3 25 35 30 using functions avg and max give an expression that computes the average as shown above assume that there exists a boolean function named isleapyear that determines if a given year is a leap year or not give an appropriate if statement that prints year is a leap year if the year passed is a leap year and year is not a leap year otherwise for variable year python programming exercises for the following function definition and associated function calls def somefunction main somefunction 15 a list all the formal parameters b list all the actual arguments for the following function indicate whether each function call is proper or not if improper explain why def gcd function gcd calculates the greatest common divisor of and with the requirement that be less than or equal to and and n2 are integer values a a b 20 result gcd a b b a b 20 result gcd a b c a 20 b result gcd b a d a b 20 c 30 result gcd gcd a b c e a b 20 c 30 print gcd a gcd c b python programming exercises write a python function named zerocheck that is given three integers and returns true if any of the integers is otherwise it returns false write a python function named that is passed three integers and returns true if the three in tegers are in order from smallest to largest otherwise it returns false write a python function named modcount that is given a positive integer n and a second positive inte ger m n and returns how many numbers between and n are evenly divisible by m write a python function named helloworld that displays hello world my name is name for any given name passed to the routine write a python function named printasterisks that is passed a positive integer value n and prints out a line of n asterisks if n is greater than then only asterisks should be displayed chapter functions write a python function named getcontinue that displays to the user do you want to continue y n and continues to prompt the user until either uppercase or lowercase y or n is entered returning lowercase y or n as the function value implement a python function that is passed a list of numeric values and a particular threshold value and returns the list with all values above the given threshold value set to the list should be altered as a side effect to the function call and not by function return value implement the python function described in question so that the altered list is returned as a function value rather than by side effect program modification problems temperature conversion program adding kelvin scale modify the temperature conversion program in section so that it allows the user to select tempera ture conversion to include degrees kelvin in addition to degrees fahrenheit and degrees celsius include input error checking for inappropriate temperature values note refer to questions and from chapter gpa calculation program accommodating first semester students modify the gpa calculation program in section so that it asks the student if this is their first semester if so the program should only prompt for their current semester grades and not their cumula tive gpa and total earned credits and display their semester gpa and cumulative gpa accordingly gpa calculation program allowing for plus minus grading modify the gpa calculation program in section so that it is capable of calculating a gpa for plus minus letter grades a b and so forth credit card calculation program summarized output modify the credit card calculation program in section so that the user is given the option of either displaying the balance and interest paid month by month as currently written or to simply have the total number of months and the total interest paid without the month by month details credit card calculation program adjustable minimum payment modify the credit card calculation program in section so that the user can enter the percentage from which the minimum monthly payment is calculated also modify the program so that this minimum pay ment percentage is displayed along with the other credit card related information credit card calculation program recalculation with new balance modify the credit card calculation program in section so that the program will allow the user to recalculate a new payoff schedule for a new entered balance program development problems metric conversion program develop and test a python program that allows the user to convert between the metric measurements of millimeter centimeter meter kilometer and inches feet yards and miles the program should be writ ten so that any one measurement can be converted to the other gpa projection program develop and test a python program that lets the user enter their current cumulative gpa their total cred its earned and the number of credits they are currently taking the program should then request from the program development problems user a target cumulative gpa that they wish to achieve and display the gpa of the current semester needed to achieve it tic tac toe two player program develop and test a python program that lets two players play tic tac toe let player be x and player be o devise a method for each player to indicate where they wish to place their symbol the program should terminate if either there is a winner or if the game results in a tie the tic tac toe board should be displayed after every move as shown below tic tac toe automated play develop and test a python program that plays tic tac toe against the user develop an appropriate strategy of play and implement it in your program the program should be designed to allow the user to continue to play new games until they decide to quit the program should display the total number of wins by the computer versus the player at the start of each new game in procedural programming functions are the primary building blocks of program design in object oriented programming objects are the fundamental building blocks in which functions methods are a component we first look at the use of individual software objects in this chapter and in chapter 10 look at the use of objects in object oriented design objectives after reading this chapter and completing the exercises you will be able to explain the concept of an object explain the difference between a reference and dereferenced value describe the use of object references explain the concept of memory allocation and deallocation describe automatic garbage collection explain the fundamental features of turtle graphics effectively use objects in python develop simple turtle graphics programs in python chapter contents motivation fundamental concepts software objects turtle graphics computational problem solving horse race simulation program software objects motivation an object is one of the first concepts that a baby understands during its development they understand an object as something that has a set of attributes big red ball and a related set of behaviors it rolls it bounces the idea of incorporating objects into a programming language came out of work in computer simulation given the prevalence of objects in the world it was natural to provide the corresponding notion of an object within a simulation program in the early alan kay at xerox parc palo alto research center fully evolved the notion of object oriented programming with the development of a programming language called smalltalk the language became the inspiration for the development of graphical user interfaces guis the primary means of interacting with computers today before that all interaction was through typed text in fact it was a visit to xerox parc by steve jobs of apple computers that led to the development of the first commercially successful gui based computer the apple macintosh in figure lists some of the most commonly used programming languages and whether they support procedural imperative programming object oriented programming or both in this chapter we look at the creation and use of objects in python figure common programming languages supporting procedural and or object oriented programming fundamental concepts software objects objects are the fundamental component of object oriented programming although we have not yet stated it all values in python are represented as objects this includes for example lists as well as numeric values we discuss object oriented programming in chapter 10 in this chapter we discuss what objects are and how they are used chapter objects and their use what is an object the notion of software objects derives from objects in the real world all objects have certain attri butes and behavior the attributes of a car for example include its color number of miles driven current location and so on its behaviors include driving the car changing the number of miles driven attribute and painting the car changing its color attribute for example similarly an object contains a set of attributes stored in a set of instance variables and a set of functions called methods that provide its behavior for example when sorting a list in procedural programming there are two distinct entities a sort function and a list to pass it as depicted in figure 2 figure 2 procedural programming approach in object oriented programming the sort routine would be part of the object containing the list depicted in figure figure object here is an object instance of the python built in list type all list objects contain the same set of methods thus is sorted by simply calling that object sort method sort the period is referred to as the dot operator used to select a member of a given object in this case the sort method note that no arguments are passed to sort that is because methods oper ate on the data of the object that they are part of thus the sort method does not need to be told which list to sort suppose there were another list object called containing a list of automo bile part numbers since all list objects behave the same would contain the iden tical set of methods as the data that they would operate on however would be different thus two objects of the same type differ only in the particular set of values that each holds this is depicted in figure software objects figure object in order to sort this list therefore the sort method of object is called sort the sort routine is the same as the sort routine of object in this case however the list of part numbers is sorted instead methods append insert remove count and reverse also provide additional functionality for lists as was discussed in chapter we next discuss the way that objects are represented in python 2 object references in this section we look at how objects are represented which all values in python are and the effect it has on the operations of assignment and comparison as well as parameter passing references in python in python objects are represented as a reference to an object in memory as shown in figure figure object reference a reference is a value that references or points to the location of another entity thus when a new object in python is created two entities are stored the object and a variable holding a reference to the object all access to the object is through the reference value this is depicted in figure chapter objects and their use figure object references to python values the value that a reference points to is called the dereferenced value this is the value that the vari able represents as shown in figure figure variables dereferenced values we can get the reference value of a variable that is the location in which the corresponding object is stored by use of built in function id id n id k id we see that the dereferenced values of n and k 10 is stored in the same memory location whereas the dereferenced value of 20 is stored in a different location even though n and k are each separately assigned literal value 10 they reference the same instance of 10 in memory we would expect there to be separate instances of 10 stored python is using a little cleverness here since integer values are immutable it assigned both n and k to the same instance this saves memory and reduces the number of reference loca tions that python must maintain from the programmer perspective however they can be treated as if they are separate instances software objects the assignment of references with our current understanding of references consider what happens when variable n is assigned to variable k depicted in figure 8 figure 8 the assignment of references when variable n is assigned to k it is the reference value of k that is assigned not the dereferenced value 20 as shown in figure 8 this can be determined by use of the built in id function as demonstrated below id k id k id n true id n n is k true thus to verify that two variables refer to the same object instance we can either compare the two id values by use of the comparison operator or make use of the provided is operator which performs id k id n thus both n and k reference the same instance of literal value 20 this occurred in the above example when n and k were separately assigned 20 because integers are an immutable type and python makes attempts to save memory in this case however n and k reference the same instance of 20 because assignment in python assigns reference values we must be aware of the fact therefore that when assigning variables referencing mutable values such as lists both variables reference the same list instance as well we will discuss the implication of this next finally we look at what happens when the value of one of the two variables n or k is changed as depicted in figure 9 figure 9 reassignment of reference value here variable k is assigned a reference value to a new memory location holding the value 30 the previous memory location that variable k referenced is retained since variable n is still referencing it as a result n and k point to different values and therefore are no longer equal chapter objects and their use let s tr y it from the python shell first enter the following and observe the results k 10 k 30 n k id k id k id n id n id k id n id k id n n is k n is k memory deallocation and garbage collection next we consider what happens when in addition to variable k being reassigned variable n is reas signed as well the result is depicted in figure 10 figure 10 inaccessible values after n is assigned to 40 the memory location storing integer value 20 is no longer referenced thus it can be deallocated to deallocate a memory location means to change its status from currently in use to available for reuse in python memory deallocation is automatically per formed by a process called garbage collection garbage collection is a method of automatically determining which locations in memory are no longer in use and deallocating them the garbage collection process is ongoing during the execution of a python program list assignment and copying now that we understand the use of references in python we can revisit the discussion on copying lists from chapter we know that when a variable is assigned to another variable referencing a list each variable ends up referring to the same instance of the list in memory depicted in figure 11 software objects figure 11 list assignment thus any changes to the elements of results in changes to we also learned that a copy of a list can be made as follows list list is referred to as a list constructor the result of the copying is depicted in figure 12 figure 12 copying of lists by use of the list constructor a copy of the list structure has been made therefore changes to the list elements of will not result in changes in 10 the situation is different if a list contains sublists however 10 20 30 40 list the resulting list structure after the assignment is depicted in figure 13 figure 13 shallow copy list structures chapter objects and their use we see that although copies were made of the top level list structures the elements within each list were not copied this is referred to as a shallow copy thus if a top level element of one list is reas signed for example the other list would remain unchanged as shown in figure 14 figure 14 top level reassignment of shallow copies if however a change to one of the sublists is made for example the cor responding change would be made in the other list that is would be equal to also as depicted in figure 15 figure 15 sublevel reassignment of shallow copies a deep copy operation of a list structure makes a copy of the complete structure including sub lists since immutable types cannot be altered immutable parts of the structure may not be copied such an operation can be performed with the deepcopy method of the copy module import copy copy deepcopy the result of this form of copying is given in figure 16 software objects figure 16 deep copy list structures thus the reassignment of any part top level or sublist of one list will not result in a change in the other it is up to you as the programmer to determine which form of copy is needed for lists and other mutable types such as dictionaries and sets covered in chapter 9 let s tr y it from the python shell enter the following and observe the results import copy 10 20 30 40 10 20 30 40 copy deepcopy id id id id 10 20 30 40 10 20 30 40 list list1 list2 copy deepcopy list1 id list1 id list2 list1 list1 list1 list1 list1 list2 list1 list2 chapter objects and their use self test questions all objects have a set of and 2 the operator is used to select members of a given object functions that are part of an object are called there are two values associated with every object in python the value and the value when memory locations are deallocated it means that a the memory locations are marked as unusable for the rest of the program execution b the memory locations are marked as available for reuse during the remaining program execution garbage collection is the process of automatically identifying which areas of memory can be deallocated true false indicate which of the following is true a when one variable is assigned to another holding an integer value if the second variable is assigned a new value the value of the first variable will change as well b when one variable is assigned to another holding a list of integer values if the second variable assigns a new integer value to an element in the list the list that the first variable is assigned to will be changed as well answers 1 attributes behavior 2 dot 3 methods reference dereferenced b true b 2 turtle graphics turtle graphics refers to a means of controlling a graphical entity a turtle in a graphics window with x y coordinates a turtle can be told to draw lines as it travels therefore having the ability to create various graphical designs turtle graphics was first developed for a language named logo in the for teaching children how to program remnants of logo still exist today python provides the capability of turtle graphics in the turtle python standard library mod ule there may be more than one turtle on the screen at once each turtle is represented by a distinct object thus each can be individually controlled by the methods available for turtle objects we introduce turtle graphics here for two reasons first to provide a means of better understanding objects in programming and second to have some fun 2 1 creating a turtle graphics window the first step in the use of turtle graphics is the creation of a turtle graphics window a turtle screen figure 17 shows how to create a turtle screen of a certain size with an appropriate title bar assuming that the import turtle form of import is used each of the turtle graphics methods must be called in the form turtle methodname the first method called setup 2 turtle graphics figure 17 creating a turtle graphics window creates a graphics window of the specified size in pixels in this case a window of size pixels wide by pixels high is created the center point of the window is at coordinate thus x coordinate values to the right of the center point are positive values and those to the left are nega tive values similarly y coordinate values above the center point are positive values and those below are negative values the top left top right bottom left and bottom left coordinates for a window of size are as shown in figure 18 a turtle graphics window in python is also an object therefore to set the title of this window we need the reference to this object this is done by call to method screen figure 18 python turtle graphics window of size 3 the background color of the turtle window can be changed from the default white background color this is done using method bgcolor window turtle screen window bgcolor blue see the discussion about pen color below for details on the specification of color values chapter objects and their use 2 2 the default turtle a turtle is an entity in a turtle graphics window that can be controlled in various ways like the graphics window turtles are objects a default turtle is created when the setup method is called the reference to this turtle object can be obtained by turtle getturtle a call to getturtle returns the reference to the default turtle and causes it to appear on the screen the initial position of all turtles is the center of the screen at coordinate as shown in figure 19 figure 19 the default turtle the default turtle shape is an arrowhead the size of the turtle shape was enlarged from its default size for clarity a turtle shape can be set to basic geometric shapes or even made from a provided image file shown in section 2 2 turtle graphics 2 3 fundamental turtle attributes and behavior recall that objects have both attributes and behavior turtle objects have three fundamental attri butes position heading orientation and pen attributes we discuss each of these attributes next absolute positioning method position returns a turtle current position for newly created turtles this returns the tuple a turtle position can be changed using absolute positioning by moving the turtle to a specific x y coordinate location by use of method setposition an example of this is given in figure 20 figure 20 absolute positioning of turtle the turtle is made invisible by a call to method hideturtle since newly created turtles are positioned at coordinates the square will be displayed near the middle of the turtle window to draw the square the turtle is first positioned at coordinates pixels to the right of its current position since the turtle pen is down a line will be drawn from location to loca tion the turtle is then positioned at coordinates which draws a line from the bottom right corner to the top right corner of the square positioning the turtle to coordinates draws a line from the top right corner to the top left corner finally positioning the turtle back to co ordinates draws the final line from the top left corner to the bottom left corner turtle heading and relative positioning a turtle position can also be changed through relative positioning in this case the location that a turtle moves to is determined by its second fundamental attribute its heading a newly created turtle heading is to the right at degrees a turtle with heading degrees moves up with a heading degrees moves left and with a heading degrees moves down a turtle heading can be changed by turning the turtle a given number of degrees left left or right right the forward method moves a turtle in the direction that it is currently heading an example of relative positioning is given in figure 21 chapter objects and their use figure 21 relative positioning of turtle in this example the turtle is controlled using relative positioning drawing the same square as in figure 20 above since turtles are initially positioned at coordinates with an initial head ing of 0 degrees the first step is to move the turtle forward pixels that draws the bottom line of the square the turtle is then turned left degrees and again moved forward pixels this draws the line of the right side of the square these steps continue until the turtle arrives back at the origi nal coordinates 0 0 completing the square methods left and right change a turtle heading relative to its current heading a turtle heading can also be set to a specific heading by use of method setheading set heading in addition method heading can be used to determine a turtle current heading pen attributes the pen attribute of a turtle object is related to its drawing capabilities the most fundamental of these attributes is whether the pen is currently up or down controlled by methods penup and pen down when the pen attribute value is up the turtle can be moved to another location without lines being drawn this is especially needed when drawing graphical images with disconnected segments example use of these methods is given in figure 22 in this example the turtle is hidden so that only the needed lines appear since the initial location of the turtle is at coordinate 0 0 the pen is set to up so that the position of the turtle can be set to 0 without a line being drawn as it moves this puts the turtle at the bottom of the left side of the letter the pen is then set to down and the turtle is moved to coordinate 0 drawing as it moves this therefore draws a line from the bottom of the left side to the top of the a the turtle is then moved with its pen still down to the location of the bottom of the right side of the letter coordinate 0 to cross the a the pen is again set to up and the turtle is moved to the location of the left end of the crossing line coordi nate the pen is then set to down and moved to the end of the crossing line at coordinate 90 to finish the letter 2 turtle graphics figure 22 example use of methods penup and pendown the pen size of a turtle determines the width of the lines drawn when the pen attribute is down the pensize method is used to control this pensize the width is given in pixels and is limited only by the size of the turtle screen example pen sizes are depicted in figure 23 figure 23 example turtle pen sizes the pen color can also be selected by use of the pencolor method pencolor blue the name of any common color can be used for example white red blue green yellow gray and black colors can also be specified in rgb red green blue component values these values can be specified in the range 0 if the color mode attribute of the turtle window is set as given below turtle colormode pencolor violet this provides a means for a full spectrum of colors to be displayed chapter objects and their use 2 4 additional turtle attributes in addition to the fundamental turtle attributes already discussed we provide details on other attri butes of a turtle that may be controlled this includes whether the turtle is visible or not the size both demonstrated above shape and fill color of the turtle the turtle speed and the tilt of the turtle we will discuss each of these attributes next turtle visibility as we saw a turtle visibility can be controlled by use of methods hideturtle and show turtle in which an invisible turtle can still draw on the screen there are various reasons for doing this a turtle may be made invisible while being repositioned on the screen in gaming a turtle might be made invisible when it meets its demise or maybe a given turtle needs to blink as we will see at the end of the chapter turtle size the size of a turtle shape can be controlled with methods resizemode and turtlesize as shown in figure 24 figure 24 changing the size of a turtle the first instruction sets the resize attribute of a turtle to user this allows the user programmer to change the size of the turtle by use of method turtlesize otherwise calls to turtlesize will have no effect the call to method turtlesize in the figure is passed two parameters the first is used to change the width of the shape perpendicular to its orientation and the second changes its length parallel to its orientation each value provides a factor by which the size is to be changed thus turtlesize 3 3 stretches both the width and length of the current turtle shape by a factor of 3 a third parameter can also be added that determines the thick ness of the shape outline 2 turtle graphics there are two other values that method resizemode may be set to an argument value of auto causes the size of the turtle to change with changes in the pen size whereas a value of noresize causes the turtle shape to remain the same size turtle shape there are a number of ways that a turtle shape and fill color may be defined to something other than the default shape the arrowhead and fill color black first a turtle may be assigned one of the following provided shapes arrow turtle circle square triangle and classic the default arrowhead shape as shown in figure 25 figure 25 available turtle shapes the shape and fill colors are set by use of the shape and fillcolor methods shape circle fillcolor white new shapes may be created and registered with added to the turtle screen shape dictionary one way of creating a new is shape by providing a set of coordinates denoting a polygon as shown in figure 26 figure 26 creating a new polygon turtle shape chapter objects and their use in the figure method is used to register the new turtle shape with the name mypolygon the new shape is provided by the tuple of coordinates in the second argu ment these coordinates define the polygon shown in the figure once the new shape is defined a turtle can be set to that shape by calling the shape method with the desired shape name the fillcolor method is then called to make the fill color of the polygon white with the edges remaining black it is also possible to create turtle shapes composed of various indi vidual polygons called compound shapes we refer the reader to the official online python documentation of the turtle module for details see http docs python org library turtle html module turtle the creation of this polygon may not seem too exciting but the orientation of a turtle can be changed in addition a turtle is able to stamp its shape on the screen which remains there even after the turtle is repositioned or relocated that means that we can create all sorts interesting graphic patterns by appropriately repositioning the turtle as shown in figure 27 figure 27 creating a design from a turtle using a polygon shape only a few lines of code are needed to generate this design the for loop in the figure iterates vari able angle over the complete range of degrees 0 to by increments of 10 degrees within the loop the turtle heading is set to the current angle and the stamp method is called to stamp the polygon shape at the turtle s current position by varying the shape of the polygon and the angles that the turtle is set to a wide range of such designs may be produced another way that a turtle shape can be created is by use of an image the image file used must be a gif file with file extension gif the name of the file is then registered and the shape of the turtle set to the registered name gif shape gif the final program of this chapter gives an example of the use of image shapes 2 turtle graphics turtle speed at times you may want to control the speed at which a turtle moves a turtle s speed can be set to a range of speed values from 0 to 10 with a normal speed being around to set the speed of the turtle the speed method is used speed the following speed values can be set using a descriptive rather than a numeric value 10 fast normal 3 slow 1 slowest 0 fastest thus a normal speed can also be set by speed normal when using the turtle for line drawing only the turtle will move more quickly if it is made invisible by use of the hideturtle method 2 creating multiple turtles so far we have seen examples in which there is only one turtle object the default turtle created with a turtle window however it is possible to create and control any number of turtle objects to create a new turtle the turtle method is used turtle turtle turtle turtle etc by storing turtle objects in a list any number of turtles may be maintained turtles turtles append turtle turtle turtles append turtle turtle etc an example of using multiple turtle objects is given in the following let s apply it section chapter objects and their use 2 let s apply it bouncing balls program following is a program figure 29 that displays one or more bouncing balls within a turtle screen this program utilizes the following programming features turtle module time module random module example execution of the program is given in figure 28 figure 28 execution of bouncing balls program in addition to the turtle graphics module this program makes use of the time and random python standard library modules to allow control of how long in seconds the simulation is exe cuted as indicated by the user and to generate the random motion of the bouncing balls the main section of the program begins on line with the programming greeting on lines the size of the turtle screen in pixels is hard coded into the program assigned to variables and since all references to the screen size are through these variables the desired window size can be altered by simply altering these variables 2 turtle graphics figure 29 bouncing balls program continued on lines the turtle screen is created and its reference value assigned to variable window the title of the window is assigned through a call to the title method following that the user is prompted to enter the number of seconds for the simulation as well as the number of simultaneously bouncing balls chapter objects and their use figure 29 bouncing balls program function createballs is called on line to create and return a list of turtle objects with a ball shape the function definition lines 39 50 initializes an empty list named balls and creates the requested number of balls one by one each appended to the list by use of the for loop at line each ball is created with shape circle fill color of black speed of 0 fastest speed and with pen attribute up in addition the initial heading of each turtle is set to a random angle between 1 and line 47 3 horse race simulation program back in the main program section at line the current time in seconds is obtained from a call to method time of the time module time time the current time value is stored in variable the current time is the number of seconds since the epoch which is january 1 1970 this will be discussed further in the horse racing program that follows the while loop beginning on line begins the simulation the loop iterates as long as boolean vari able terminate is false initialized to false on line the for loop at line moves each of the specified number of balls a small distance until reaching one of the four edges of the window left right top or bottom edge boolean functions atleftedge atrightedge attopedge and atbottomedge are used to determine when a ball is at an edge defined in lines 29 func tion bounceball is called to bounce the ball in the opposite direction it is heading and returns the new heading of the ball passed as the argument to that ball s setheading method finally on line a check is made to determine whether the user requested simulation time has been ex ceeded if so boolean variable terminate is set to true and the program terminates because of the call to exitonclick on line the program will properly shut down when the close button of the turtle window is clicked self test questions 1 a turtle screen is an pixel wide by pixel high graphics window true false 2 the three main attributes of a turtle object are and 3 a turtle can be moved using either or positioning 4 a turtle can only draw lines when it is not hidden true false 5 a turtle shape is limited to an arrow turtle circle square triangle or classic default shape true false what attribute of a turtle determines the size of the lines it draw a pen size b turtle size a turtle can draw in one of seven colors true false 8 a turtle can leave an imprint of its shape on the screen by use of the method 9 in order to create a new turtle object the method is called answers 1 false 2 position heading pen 3 absolute relative 4 false 5 false a false 8 stamp 9 turtle computational problem solving 3 horse race simulation program in this section we design implement and test a program that simulates a horse race chapter objects and their use 3 1 the problem the problem is to create a visualization of a horse race in which horses are moved ahead a random distance at fixed intervals until there is a winner as shown in figure 30 figure 30 example horse race simulation 3 2 problem analysis the program needs a source of random numbers for advancing the horses a random distance in the race we can use the random number generator of the python standard library module random that we used in chapter 3 in the coin change exercise example the remaining part of the problem is 3 horse race simulation program in the creation of appropriate graphics for producing a visualization of a horse race we shall make use of the turtle graphics module from the python standard library to do this 3 3 program design meeting the program requirements there are no specific requirements for this problem other than to create an appropriate simulation of a horse race therefore the requirement is essentially the generation of a horse race in which the graphics look sufficiently compelling and each horse has an equal chance of winning a given race since a specific number of horses was not specified we will design the program for ten horses in each race data description the essential information for this program is the current location of each of the ten horses in a given race each turtle is an object whose attributes include its shape and its coordinate position on the turtle screen therefore we will maintain a list of ten turtle objects with the shape attribute of a horse image for this purpose thus suitable horse images must be found or created for this purpose algorithmic approach there is no algorithm per se needed in this program other than to advance each horse a random distance at fixed time intervals until one of the horses reaches a certain point on the turtle screen the finish line overall program steps the overall steps in this program design are given in figure 31 figure 31 overall steps of the horse race simulation program 3 4 program implementation and testing stage 1 creating an initial turtle screen layout we first develop and test an initial program that lays out the positions of the starting horses on the turtle graphics screen as shown in figure figure 33 provides this first stage of the program chapter objects and their use figure output of stage 1 of the horse race simulation program at line 3 the turtle module is imported since the import form of import is used each call to a method of this module must be prefixed with the module name for example turtle setup on line 31 which sets the turtle screen size to a width of and a height of pixels the intent of this version of the program is to ensure that the turtle screen is appropriately sized and that the initial layout of horse locations is achieved therefore only the default turtle shape is used at this point in the next version we will focus on generating a set of horse images on the screen thus on line the turtle screen object is retrieved by the call to turtle sreen and its reference assigned to variable window the start location of the first lowest horse is set to an x coordinate value of and a y coordinate value of this puts the turtle screen object at the lower right corner of the screen the amount of vertical separation between the horses is assigned to variable these values were determined from knowledge of the screen coordinates in turtle graphics and a little trial and error next on line a call is made to function generatehorses at lines 9 15 this function returns a list of ten new turtle objects and assigned to variable horses function newhorse lines 5 is called by function generatehorses to create each new horse turtle object at this stage function newhorse simply creates and returns a regular turtle object in the next stage how ever it will be responsible for returning new turtle objects with an appropriate horse shape the position for each of these horses is determined by function placehorses on lines 17 23 it is passed the list of horse turtle objects the location of the first turtle and the amount of sepa ration between each established as pixels on line function placehorses therefore 3 horse race simulation program figure 33 stage 1 of the horse race simulation program contains a for loop that iterates over the list of horse objects and makes them initially hidden with their pen up lines 19 20 moves each to its starting position line 21 sets the heading of each to degrees to move left line 22 and then makes each visible line 23 finally method exiton click is called so that the program will terminate when the user clicks on the program win dow s close box in the next stage we further develop the program to include the specific shapes and images for the simulation chapter objects and their use stage 2 adding the appropriate shapes and images we next develop and test the program with additional code that adds the horse shapes images needed the resulting turtle screen is shown in figure 34 figure 35 shows this second stage of the program figure 34 output of stage 2 of the horse race simulation program in this stage of the program we add functions gethorseimages and registerhorseimages called from lines and of the main program section function gethorseimages returns a list of gif image files each image contains the same horse image each with a unique number 1 to 10 function registerhorseimages does the required registering of images in turtle graphics by calling method turtle on each function generatehorses lines 26 32 is implemented the same way as in stage 1 to return a list of horse turtle objects except that it is altered to be passed an argument containing a list of horse images thus the call to generatehorses in line is altered to pass the list of images in variable function newhorse lines 19 24 is altered as well to be passed a particular horse image for the horse that is created horse shape stage 3 animating the horses next we develop and test the program with additional code that animates the horses so that they are randomly advanced until a horse crosses the finish line the resulting turtle screen is shown in figure 36 figure 37 provides this third stage of the program 3 horse race simulation program figure 35 stage 2 of the horse simulation race program continued two new functions are added in this version of the program starthorses and display winner function starthorses lines is passed the list of horse turtle objects the location of the finish line as an x coordinate value on the turtle screen and the fundamental incre ment amount each horse is advanced by one to three times this amount the while loop for incrementally moving the horses is on line the loop iterates until a winner is found that is until the variable is true therefore is initialized to false in line variable k initialized on line 48 is used to index into the list of horse turtle objects since each horse in turn is advanced some amount during the race variable k is incremented by one modulo the number of horses in variable 10 line when k becomes equal to 21 9 it is reset to 0 for horse 1 the amount that each horse is advanced is a factor of one to three randomly determined by call to method randint 1 3 of the python standard library module random in line variable is multiplied by this factor to move the horses forward an appropriate amount chapter objects and their use figure 35 stage 2 of the horse simulation race program figure 36 output of stage 3 of the horse race simulation program 3 horse race simulation program figure 37 stage 3 of the horse race simulation program continued chapter objects and their use figure 37 stage 3 of the horse race simulation program the value of is initialized in the main program section this value can be adjusted to speed up or slow down the overall speed of the horses function displaywinner displays the winning horse number in the python shell lines 60 61 this function will be rewritten in the next stage of program development to display a winner banner image in the turtle screen thus this implementation of the function is for testing purposes only the main program section lines is the same as in the previous stage of program development except for the inclusion of the calls to functions starthorses and display winner in lines and 97 3 horse race simulation program final stage adding race banners finally we add the code for the displaying of banners at various points in the race as shown earlier in figure 30 in figure 38 is the final stage of the program this final version imports one additional module python standard library module time line 5 used to control the blink rate of the winning horse while the race progresses within the while loop at line checks for the location of the lead horse are made in two places before and after the halfway mark of the race on line if the figure 38 final stage of the horse race simulation program continued chapter objects and their use figure 38 final stage of the horse race simulation program continued x coordinate location of the lead horse is less then the early lead banner is displayed on line by a call to function displaybanner otherwise if one second has elapsed then the midrace lead banner is displayed on line the sleep method of the time module is used to control the blinking of the winning horse in function displaywinner a count down variable is set to 5 on line this will cause the winning horse to blink five times the following while loop decrements 3 horse race simulation program figure 38 final stage of the horse race simulation program continued and continues to iterate until is 0 variable show initial ized to false on line is used to alternately show and hide the turtle based on its current boolean value which is toggled back and forth between true and false each time through the loop the sleep method is called on line to cause the program execution to suspend for four tenths of a second so that the switch between the visible and invisible horse appears slowly enough to cause a blinking effect this version of displaywinner replaces the previous ver sion that simply displayed the winning horse number in the python shell window added functions getbannerimages lines 17 41 registerbannerimages lines 47 50 and displaybanner lines 90 incorporate the banner images into the program the same way that the horse images were incorporated in the previous program version function starthorses was modified to take another parameter banners containing the list of regis tered banners displayed during the race passed to it from the main program section chapter objects and their use figure 38 final stage of the horse race simulation program continued finally the default turtle created with the turtle graphics window is utilized in function displaybanners and in the main section it is used to display the various banners at the bottom of the screen to do this the turtle s shape is changed to the appropriate banner images stored in list to prevent the turtle from drawing lines when moving from the ini tial 0 0 coordinate location to where banners are displayed the default turtle is hidden and its pen attribute is set to up lines chapter exercises figure 38 final stage of the horse race simulation program chapter summar y general topics software objects methods references reference vs dereferenced values reference assignment memory allocation deallocation garbage collection shallow vs deep copy operations python speciﬁc programming topics objects and turtle graphics in python chapter exercises section 1 1 indicate exactly what the contents of and would be after each of the following set of assignments a 5 10 20 30 b 5 10 20 30 c 5 10 20 30 5 10 20 30 5 5 list 2 5 50 2 5 50 2 5 50 2 indicate which of the following set of assignments would result in automatic garbage collection in python a 5 1 2 3 b 5 hello world c 5 1 2 3 5 5 5 nice day 5 5 lst2 5 tuple1 5 4 5 3 for the set of assignments in question 1 indicate how both the id method and is operator can be used to determine if lists lst1 and lst2 are each referencing the same list instance in memory section 2 4 give a set of instructions to create a turtle window of size pixels wide and pixels high with a title of turtle graphics window 5 give a set of instructions that gets the default turtle and sets it to an actual turtle shape chapter 6 objects and their use 6 for each of the following method calls on turtle indicate in what part of the screen the turtle will be placed relative to the center of the screen a setposition 0 0 b setposition 0 c setposition 0 d setposition 0 250 for the following method calls on turtle describe the shape that will be drawn penup setposition 0 pendown setposition 100 0 setposition 100 50 setposition 50 setposition 0 8 what color line will be drawn in the following turtle colormode pencolor 0 0 pendown forward 100 9 what will be displayed by the following turtle actions pendown showturtle forward 25 penup hide_turtle forward 25 pendown showturtle the_turtle forward 25 python programming exercises give a set of instructions for controlling the turtle to draw a line from the top left corner of the screen to the bottom right corner and from the top right corner to the bottom left corner thereby making a big x on the screen there should be no other lines drawn on the screen using relative positioning give a set of instructions for controlling the turtle to draw an isosceles triangle on the screen that is a triangle with two equal length sides give a set of instructions for controlling the turtle to draw the letter w using relative positioning give a set of instructions for controlling the turtle to create three concentric circles each of different color and line width give a set of instructions that sets the turtle to an actual turtle shape and moves it from the bottom of the screen towards the top getting smaller as it moves along give a set of instructions that moves the turtle with an actual turtle shape from the bottom of the screen toward the top changing its fill color when it crosses the x axis of the grid coordinates give a set of instructions to create your own polygon shape and create an interesting design with it program modification problems give a set of instructions so that the turtle initially moves slowly around the edge of the screen then moves faster and faster as it goes around give a set of instructions to create two turtle objects each with circle shape that move to various locations of the turtle screen each stamping their circle shape of varying sizes and colors program modification problems bouncing balls with color modify the bouncing balls simulation program so that exactly three balls are created each with a different color bouncing balls with changing color modify the bouncing balls simulation program so that each time a ball hits an edge of the turtle graphics screen it changes color bouncing balls with trailing lines modify the bouncing balls simulation program so that a trail is left on the screen of each ball s path bouncing ball chase modify the bouncing balls simulation program so that there are exactly three balls generated with the first ball started in a random direction heading and the other two balls following it closely behind horse racing program multiple races and score keeping modify the horse racing program so that the user can continue to play another race without having to rerun the program also the cumulative wins of all the horses should be displayed in the shell window this allows the user to see if some horses are a more winning horse than others horse racing program handicap racing modify the horse racing program so that the user can assign a handicap to one or more horses on a scale of 1 to 5 a handicap in racing is a means of giving advantage to less competitive horses over more competitive ones if a horse is assigned a handicap of 1 it should move ahead one fifth farther than usual a handicap of 2 would increase its move by two fifths and so forth the list of handicaps should be dis played in the shell window each time before the race begins horse racing program pari mutuel betting modify the horse racing program to allow individuals to enter their name to register themselves to place bets the program should be modified so that races can be consecutively run without having to re start the program before each race bets can be placed by registered players each bet is for which horse will win the payout will be based on the rules of pari mutuel betting described below the amount of money gained or lost by each registered player should be constantly displayed in the shell example of pari mutuel betting each horse has a certain amount of money wagered on it assuming eight horses 1 2 3 4 5 6 8 30 70 12 00 00 47 00 00 40 00 thus the total pool of money on this particular wagering event is 00 following the start of the event no more wagers are accepted the event is decided and the winning outcome is determined to be outcome 4 with 55 00 wagered the payout is now calculated first the commission or take for the wagering company is deducted from the pool for example with a commission rate of 14 25 the pool is 3 1 2 0 5 this remaining amount in the pool is now distributed to those who chapter 6 objects and their use wagered on outcome 4 440 55 8 per 1 wagered this payout includes the 1 wagered plus an additional profit thus the odds of outcome 4 are 7 to 1 wikipedia contributors parimutuel betting wikipedia the free encyclopedia wikipedia the free encyclopedia may 7 2011 web may 11 2011 program development problems drunkard s walk a random walk is a trajectory taken by a sequence of random steps random walks can be used to model the travel of molecules the path that animals take when looking for food and financial fluctuations for example a specific form of random walk is called the drunkard s walk a drunken man tries to find his way home he does so by making a random choice at each street intersection of which of the four paths to take continue in the same direction go back from the direction he came turn left or turn right thus the man is traveling the same distance after each choice of direction one city block implement and test a python program using turtle graphics to display random walks select an appropriate number of pixels as the length of a city block name reversal implement and test a python program using turtle graphics to allow the user to enter their first name and have it displayed in the turtle window as a reverse mirror image battleship game visualization implement and test a python program using turtle graphics to provide a visualization for the game of battleship discussed in program development problem in chapter 4 using and defining functions as you know from the functions you have been using the effect of calling a python function is easy to understand for example when you place math sqrt a b in a program the effect is as if you had replaced that code with the return value that is produced by python math sqrt function when passed the expression a b as an argument this usage is so intuitive that we have hardly needed to comment on it if you think about what the system has to do to create this effect however you will see that it involves changing a program control flow the implications of being able to change the control flow in this way are as profound as doing so for conditionals and loops you can define functions in any python program using the def statement that specifies the function signature followed by a sequence of statements that constitute the function we will consider the details shortly but begin with a simple example that illustrates how functions affect control flow our first example pro gram harmonicf py includes a function named harmonic that takes an argument n and computes the nth harmonic number see program it also illustrates the typical structure of a python program having three components a sequence of import statements a sequence of function definitions arbitrary global code or the body of the program program has two import statements one function definition and four lines of arbitrary global code python executes the global code when we invoke the pro gram by typing python harmonicf py on the command line that global code calls the harmonic function defined earlier the implementation in harmonicf py is preferable to our original imple mentation for computing harmonic numbers program because it clearly separates the two primary tasks performed by the program calculating harmonic numbers and interacting with the user for purposes of illustration we have made the user interaction part of the program a bit more complicated than in program whenever you can clearly separate tasks within a computation you should do so next we carefully examine precisely how harmonicf py achieves this goal control flow the diagram on the next page illustrates the flow of control for the command python harmonicf py first python processes the import statements thus making all of the features defined in the sys and stdio modules available to the program next python processes the definition of the harmonic function at lines through but does not execute the function python executes a function only when it is called then python executes the first statement in the global code after the function definition the for statement which proceeds nor mally until python begins to execute the statement value harmonic arg start ing by evaluating the expression harmonic arg when arg is to do so it trans fers control to the harmonic function the flow of control passes to the code in the func import sys import stdio def harmonic n total for i in range n total i return total for i in range len sys argv i i i arg int sys argv i value harmonic arg stdio writeln value n n n tion definition python initial izes the parameter variable n to and the local vari able total to and then executes the for loop within harmonic which terminates after one iteration with total equal to then python executes the return statement at the end of the definition of harmonic causing the flow of control to jump back to the calling statement val ue harmonic arg con tinuing from where it left off but now with the expression harmonic arg replaced by thus python assigns to value and writes it to stan dard output then python it erates the loop once more and calls the harmonic function a second time with n initial flow of control for python harmonicf py ized to which results in being written the process is then repeated a third time with arg and then n equal to which results in being written finally the for loop terminates and the whole process is complete as the diagram indicates the simple code masks a rather intri cate flow of control python harmonicf py python harmonicf py 9289682539682538 787606036044348 i arg harmonic informal function call return trace one simple approach to following the con trol flow through function calls is to imagine that each function writes its name and argument when it is called and its return value just before returning with indentation added on calls and sub tracted on returns the result enhances the process of trac ing a program by writing the values of its variables which total total return value i arg harmonic total total total return value i arg harmonic total total total total total return value 083333333333333 informal trace with function call return for python harmonicf py we have been using since section an informal trace for our example is shown at right the added indentation ex poses the flow of the control and helps us check that each function has the effect that we expect generally adding calls on stdio writef to trace any program control flow in this way is a fine approach to begin to understand what it is doing if the return values match our expectations we need not trace the function code in detail saving us a substantial amount of work for the rest of this chapter your programming will be cen tered on creating and using functions so it is worthwhile to consider in more detail their basic properties and in particu lar the terminology surrounding functions following that we will study several examples of function implementations and applications basic terminology as we have been doing throughout it is useful to draw a distinction between abstract concepts and python mechanisms to implement them the python if statement implements the conditional the while statement implements the loop and so forth there are several concepts rolled up in the idea of a mathematical function and there are python constructs corresponding to each as summarized in the table at the top of the following page while you can rest assured that these formalisms have served mathematicians well for centuries and have served pro grammers well for decades we will refrain from considering in detail all of the implications of this correspondence and focus on those that will help you learn to program when we use a symbolic name in a formula that defines a mathematical func tion such as f x x the symbol x is a placeholder for some input value concept python construct description function function mapping input value argument input to function output value return value output of function formula function body function definition independent variable parameter variable symbolic placeholder for input value that will be substituted into the formula to determine the output value in python we use a parameter variable as a symbolic placeholder and we refer to a particular input value where the function is to be evaluated as an argument function definition the first line of a function definition known as its signature gives a name to the function and to each parameter variable the signa signature def function name parameter variable n function body ture consists of the keyword def the function name a sequence of zero or more parameter variable names separated by commas and enclosed in parentheses and a colon the indented statements following the signature define the function body the function local variable for i in range n total i body can consist of the kinds of statements that we discussed in chapter it also can contain a return statement which transfers control back to the point where the function was called and returns the result return return statement value anatomy of a function definition of the computation or return value the body may also define local variables which are variables that are available only inside the function in which they are defined function calls as we have seen throughout a python function call is nothing more than the function name followed by its arguments separated by commas and enclosed in parentheses in precisely the same form as is customary for mathemati cal functions as noted in section each argument can be an expression which is evaluated and the resulting value passed as input to the function when the function finishes the return value takes the place of the function call as if it were the value of anatomy of a function call a variable perhaps within an expression multiple arguments like a mathematical function a python function can have more than one parameter variable so it can be called with more than one argu ment the function signature lists the name of each parameter variable separated by commas for example the following function computes the length of the hypot enuse of a right triangle with sides of length a and b def hypot a b return math sqrt a a b b multiple functions you can define as many functions as you want in a py file the functions are independent except that they may refer to each other through calls they can appear in any order in the file def square x return x x def hypot a b return math sqrt square a square b however the definition of a function must appear before any global code that calls it that is the reason that a typical python program contains import statements function definitions and arbitrary global code in that order multiple return statements you can put return statements in a function wher ever you need them control goes back to the calling program as soon as the first return statement is reached this primality testing function is an example of a function that is natural to define using multiple return statements def isprime n if n return false i while i i n if n i return false i return true single return value a python function provides only one return value to the caller or more precisely it returns a reference to one object this policy is not as restrictive as it might seem because python data types can contain more informa tion than a single number boolean or string for example you will see later in this section that you can use arrays as return values scope the scope of a variable is the set of statements that can refer to that vari able directly the scope of a function local and parameter variables is limited to that function the scope of a variable defined in global code known as a global variable is limited to the py file containing that variable therefore global code cannot refer to either a function local or parameter variables nor can one func tion refer to either the local or parameter variables that are defined in another func tion when a function defines a local or parameter variable with the same name as a global variable such as i in program the variable name in the function refers to the local or parameter variable not the global variable a guiding principle when designing software is to define each variable so that its scope is as small as possible one of the important reasons that we use func tions is so that changes made to one part scope of n and total this code should not refer to arg or value scope of i two different variables scope of i scope of arg and value of a program will not affect an unrelated part of the program so while code in a function can refer to global variables it should not do so all communication this code cannot refer to n or total scope of local and parameter variables from a caller to a function should take place via the function parameter vari ables and all communication from a function to its caller should take place via the function return value in section we consider a technique for removing most global code thereby limiting scope and the potential for unexpected interactions default arguments a python function may designate an argument to be optional by specifying a default value for that argument if you omit an optional argument in a function call then python substitutes the default value for that argument we have already encountered a few examples of this feature for example math log x b returns the base b logarithm of x if you omit the second argument then b defaults to math e that is math log x returns the natural logarithm of x it might appear that the math module has two different logarithm functions but it actually has just one with an optional argument and a default value you can specify an optional argument with a default value in a user defined function by putting an equals sign followed by the default value after the parameter variable in the function signature you can specify more than one optional argu ment in a function signature but all of the optional arguments must follow all of the mandatory arguments for example consider the problem of computing the nth generalized har monic number of order r hn r nr for example and the generalized harmonic numbers are closely related to the riemann zeta function from number theory note that the nth generalized harmonic number of order r is equal to the nth harmonic number therefore it is appropriate to use as the default value for r if the caller omits the second argu ment we specify by writing r in the signature def harmonic n r total for i in range n total i r return total with this definition harmonic returns while both harmonic and harmonic return to the client it appears that we have two different functions one with a single argument and one with two arguments but we achieve this effect with a single implementation side effects in mathematics a function maps one or more input values to some output value in computer programming many functions fit that same model they accept one or more arguments and their only purpose is to return a value a pure function is a function that given the same arguments always return the same value without producing any observable side effects such as consuming input producing output or otherwise changing the state of the system so far in this section we have considered only pure functions however in computer programming it is also useful to define functions that do produce side effects in fact we often define functions whose only purpose is to produce side effects an explicit return statement is optional in such a func tion control returns to the caller after python executes the function last statement functions with no specified return value actually return the special value none which is usually ignored for example the stdio write function has the side effect of writing the given argument to standard output and has no specified return value similarly the following function has the side effect of drawing a triangle to standard drawing and has no specified return value def drawtriangle stddraw line stddraw line y1 stddraw line it is generally poor style to compose a function that both produces side effects and returns a value one notable exception arises in functions that read input for ex ample the stdio readint function both returns a value an integer and pro duces a side effect consuming one integer from standard input type checking in mathematics the definition of a function specifies both the do main and the range for example for the harmonic numbers the domain is the positive integers and the range is the positive real numbers in python we do not specify the types of the parameter variables or the type of the return value as long as python can apply all of the operations within a function python executes the function and returns a value if python cannot apply an operation to a given object because it is of the wrong type it raises a run time error to indicate the invalid type for example if you call the square function defined earlier with an int argument the result is an int if you call it with a float argument the result is a float however if you call it with a string argument then python raises a typeerror at run time this flexibility is a popular feature of python known as polymorphism be cause it allows us to define a single function for use with objects of different types it can also lead to unexpected errors when we call a function with arguments of un anticipated types in principle we could include code to check for such errors and we could carefully specify which types of data each function is supposed to work with like most python programmers we refrain from doing so however in this book our message is that you should always be aware of the type of your data and the functions that we consider in this book are built in line with this philosophy which admittedly clashes with python tendency toward polymorphism we will discuss this issue in some detail in section the table below summarizes our discussion by collecting together the function defi nitions that we have examined so far to check your understanding take the time to reread these examples carefully primality test def isprime n if n return false i while i i n if n i return false i return true hypotenuse of a right triangle def hypot a b return math sqrt a a b b generalized harmonic number def harmonic n r total for i in range n total i r return total draw a triangle def drawtriangle y1 stddraw line y0 y1 stddraw line y1 y2 stddraw line y2 y0 typical code for implementing functions implementing mathematical functions why not just use the python built in functions and those that are defined in the standard or extension python mod ules for example why not use the math hypot function instead of defining our own hypot function the answer to this question is that we do use such func tions when they are present because they are likely to be faster and more accurate however there is an unlimited number of functions that we may wish to use and only a finite number of functions is defined in the python standard and extension modules when you need a function that is not defined in the python standard or extension modules you need to define the function yourself as an example we consider the kind of code required for a familiar and im portant application that is of interest to many potential college students in the united states in a recent year over million students took the scholastic aptitude test sat the test consists of two major sections critical reading and mathemat ics scores range from lowest to highest on each section so overall test scores range from to many universities consider these scores when mak ing important decisions for example student athletes are required by the national collegiate athletic association ncaa and thus by many universities to have a combined score of at least out of and the minimum eligibility require ment for certain academic scholarships is out of what percentage of test takers is ineligible for athletics what percentage is eligible for the scholarships two functions from statistics enable us to compute accurate answers to these questions the standard normal gaussian probability density function is characterized by the familiar bell shaped curve and defined by the formula x e the standard normal gaussian cumulative distribution function f z is defined to be the area under the curve defined by x above the x axis and to the left of the vertical line x z these functions play an important role in science engineering and finance because they arise as accurate models throughout the natural world and be cause they are essential in understanding experimental error in particular these functions are known to accu rately describe the distribution of test scores in our ex ample as a function of the mean average value of the scores and the standard deviation square root of the average of the squares of the differences between each score and the mean which are published each year giv en the mean and the standard deviation of the test scores the percentage of students with scores less than distribution cp area is p cumulative p p cp x x z a given value z is closely approximated by the function f z f z functions to calculate and f are not available in python math module so we develop our own implementations gaussian probability functions closed form in the simplest situation we have a closed form mathematical for mula defining our function in terms of functions that are implemented in python math module this situation is the case for the math module includes func tions to compute the exponential and the square root functions and a constant value for so a function pdf corresponding to the mathematical definition is easy to implement for convenience gauss py program uses the default arguments and and actually computes x x no closed form if no formula is known we may need a more complicated algo rithm to compute function values this situation is the case for f no closed form expression exists for this function algorithms to compute function values some times follow immediately from taylor series approximations but developing reli ably accurate implementations of mathematical functions is an art and a science that needs to be addressed carefully taking advantage of the knowledge built up in mathematics over the past several centuries many different approaches have been studied for evaluating f for example a taylor series approximation to the ratio of f and turns out to be an effective basis for evaluating the function f z z z z z z this formula readily translates to the python code for the function cdf in pro gram for small respectively large z the value is extremely close to respec tively so the code directly returns respectively otherwise it uses the taylor series to add terms until the sum converges again for convenience program actually computes f z f z using the defaults and running gauss py with the appropriate arguments on the command line tells us that about of the test takers were ineligible for athletics in a year when the mean was and the standard deviation was in the same year about percent qualified for academic scholarships computing with mathematical functions of all sorts plays a central role in science and engineering in a great many applications the functions that you need are expressed in terms of the functions in python math module as we have just seen with pdf or in terms of a taylor series approximation or some other formula tion that is easy to compute as we have just seen with cdf indeed support for such computations has played a central role throughout the evolution of comput ing systems and programming languages program gaussian functions gauss py import math import sys import stdio def pdf x mu sigma x float x mu sigma return math exp x x math sqrt math pi sigma def cdf z mu sigma z float z mu sigma if z return if z return total term z i while total total term total term term z z i i return total pdf z z float sys argv mu float sys argv sigma float sys argv stdio writeln cdf z mu sigma total cumulated sum term current term this code implements the gaussian normal probability density pdf and cumulative dis tribution cdf functions which are not implemented in python math library the pdf implementation follows directly from its definition and the cdf implementation uses a tay lor series and also calls pdf see accompanying text at left and exercise note if you are referring to this code for use in another program please see gaussian py program which is designed for reuse python gauss py python gauss py using functions to organize code beyond evaluating mathematical func tions the process of calculating an output value as a function of input values is important as a general technique for organizing control flow in any computation doing so is a simple example of an extremely important principle that is a prime guiding force for any good programmer whenever you can clearly separate tasks within a computation you should do so functions are natural and universal mechanism for expressing computational tasks indeed the bird eye view of a python program that we began with in sec tion was equivalent to a function we began by thinking of a python program as a function that transforms command line arguments into an output string this view expresses itself at many different levels of computation in particular it is generally the case that you can express a long program more naturally in terms of functions instead of as a sequence of python assignment conditional and loop statements with the ability to define functions you can better organize your pro grams by defining functions within them when appropriate for example coupon py program on the facing page is an improved version of couponcollector py program that better separates the individ ual components of the computation if you study program you will identify three separate tasks given the number of coupon values n compute a random coupon value given n do the coupon collection experiment get n from the command line then compute and write the result program rearranges the code to reflect the reality that these three activities underlie the computation the first two are implemented as functions the third as global code with this organization we could change getcoupon for example we might want to draw the random numbers from a different distribution or the global code for example we might want to take multiple inputs or run multiple experiments without worrying about the effect of any of these changes on collect using functions isolates the implementation of each component of the collec tion experiment from others or encapsulates them typically programs have many independent components which magnifies the benefits of separating them into different functions we will discuss these benefits in further detail after we have seen several other examples but you certainly can appreciate that it is better to express a computation in a program by breaking it up into functions just as it is better to express an idea in an essay by breaking it up into paragraphs whenever you can clearly separate tasks within a computation you should do so program coupon collector revisited coupon py import random import sys import stdarray import stdio def getcoupon n return random randrange n def collect n iscollected stdarray n false count collectedcount while collectedcount n value getcoupon n count if not iscollected value collectedcount iscollected value true return count n int sys argv result collect n stdio writeln result n of coupon values to n iscollected i has coupon i been collected count of coupons collected collectedcount of distinct coupons collected value value of current coupon this version of program illustrates the style of encapsulating computations in functions this code has the same effect as couponcollector py but better separates the code into its three constituent pieces generating a random integer between and n running a collection experiment and managing the i o python coupon py python coupon py python coupon py passing arguments and returning values next we examine the specifics of python mechanisms for passing arguments to and returning values from func tions these mechanisms are conceptually very simple but it is worthwhile to take the time to understand them fully as the effects are actually profound understand ing argument passing and return value mechanisms is key to learning any new programming language in the case of python the concepts of immutability and aliasing play a central role call by object reference you can use parameter variables anywhere in the body of the function in the same way as you use local variables the only difference between a parameter variable and a local variable is that python initializes the parameter variable with the corresponding argument provided by the calling code we refer to this approach as call by object reference it is more commonly known as call by value where the value is always an object reference not the object value one consequence of this approach is that if a parameter variable refers to a mutable ob ject and you change that object value within a function then this also changes the object value in the calling code because it is the same object next we explore the ramifications of this approach immutability and aliasing as discussed in section arrays are mutable data types because we can change array elements by contrast a data type is immutable if it is not possible to change the value of an object of that type the other data types that we have been us ing int float str and bool are all immutable in an immutable data type operations that might seem to change a value actually result in the creation of a new object as illustrated in the simple example at right first the statement i creates an integer and assigns to i a reference to that integer then j i assigns i an object reference to j so both i and j reference the same object the integer two variables that reference the same objects are said to be aliases next j results in j referencing an object with value but it does not do so by chang ing the value of the existing integer from to in informal trace i i j i j object level trace i i j i j i j j i j deed since int objects are immutable no statement immutability of integers can change the value of that existing integer instead that statement creates a new integer adds it to the integer to create another new integer and assigns to j a reference to that integer but i still references the original note that the new integer has no reference to it in the end that is the system concern not ours the immutability of integers floats strings and booleans is a fundamental aspect of python we will consider the advantages and disadvantages of this approach in more detail in section integers floats booleans and strings as arguments the key point to remember about passing arguments to functions in python is that whenever you pass argu ments to a function the arguments and the function parameter variables become aliases in practice this is the predominant use of aliasing in python and it is im portant to understand its effects for purposes of illustration suppose that we need a function that increments an integer our discussion applies to any more compli cated function as well a programmer new to python might try this definition def inc j j and then expect to increment an integer i with the call inc i code like this would work in some program ming languages but it has no effect in python as shown in the figure at right first the statement i assigns to global variable i a reference to the integer then the statement inc i passes i an object reference to the inc function that object reference is assigned to the parameter variable j at this point i and j are aliases as before the inc function j state ment does not change the integer but rather creates a new integer and assigns a reference to that integer to j but when the inc function returns to its caller its parameter variable j goes out of scope and the vari able i still references the integer this example illustrates that in python a function cannot produce the side effect of changing the value of an integer object nothing can do so to increment vari informal trace i i inc i j after return object level trace i i inc i j i j j i after return i j able i we could use the definition aliasing in a function call def inc j j return j and call the function with the assignment statement i inc i the same holds true for any immutable type a function cannot change the value of an integer a float a boolean or a string arrays as arguments when a function takes an array as an argument it imple ments a function that operates on an arbitrary number of objects for example the following function computes the mean average of an array of floats or integers def mean a total for v in a total v return total len a we have been using arrays as arguments from the beginning of the book for ex ample by convention python collects the strings that you type after the program name in the python command into an array sys argv and implicitly calls your global code with that array of strings as the argument side effects with arrays since arrays are mutable it is often the case that the pur pose of a function that takes an array as argument is to produce a side effect such as changing the order of array elements a prototypical example of such a func tion is one that exchanges the elements at two given indices in a given array we can adapt the code that we examined at the beginning of section def exchange a i j temp a i a i a j a j temp this implementation stems naturally from the python array representation the first parameter variable in exchange is a reference to the array not to all of the array elements when you pass an array as an argument to a function you are giv ing it the opportunity to operate on that array not a copy of it a formal trace of a call on this function is shown on the facing page this diagram is worthy of careful study to check your understanding of python function call mechanism a second prototypical example of a function that takes an array argument and produces side effects is one that randomly shuffles the elements in the array using this version of the algorithm that we examined in section and the ex change function just defined def shuffle a n len a for i in range n r random randrange i n exchange a i r incidentally python standard function random shuffle does the same task as another example we will consider in section functions that sort an array rearrange its elements so that they are in order arrays as return values a function that sorts shuffles or otherwise modifies an array taken as argument does not have to return a refer ence to that array because it is changing the contents of a client array not a copy but there are many situations where it is useful for a func tion to provide an array as a return value chief among these are functions that create arrays for the purpose of returning multiple objects of the same type to a client as an example consider the following x x exchange x j i a x function which returns an array of random floats def randomarray n a stdarray n for i in range n a i random random return a later in this chapter we will be developing nu merous functions that return huge amounts of data in this way temp a i a i a j a j temp after return temp j i a x x exchanging two elements in an array the table below concludes our discussion of arrays as function arguments by high lighting some typical array procession functions mean of an array def mean a total for v in a total v return total len a dot product of two vectors of the same length def dot a b total for i in range len a total a i b i return total exchange two elements in an array def exchange a i j temp a i a i a j a j temp write a one dimensional array and its length def a stdio writeln len a for v in a stdio writeln v read a two dimensional array of floats with dimensions def m stdio readint n stdio readint a stdarray m n for i in range m for j in range n a i j stdio readfloat return a typical code for implementing functions with arrays example superposition of sound waves as discussed in section the simple audio model that we studied there needs to be embellished to create sound that resembles the sound produced by a musical instrument many different em bellishments are possible with functions we can systematically apply them to pro duce sound waves that are far more complicated than the simple sine waves that we produced in section as an illustration of the effective use of functions to solve an interesting computational problem we consider a program that has essentially the same functionality as playthattune py program but adds harmonic tones one octave above and one octave below each note to produce a more realistic sound chords and harmonics notes like concert a have a pure sound that is not very musical because the sounds that you are accustomed to hearing have many other components the sound from a guitar a c string echoes off the wooden part of the instrument the walls of the room that e a major chord a you are in and so forth you may think of such effects as modifying the basic sine wave for example most musical in struments produce harmonics the same a a 00 concert a with harmonics superposing waves to make composite sounds note in different octaves and not as loud or you might play chords multiple notes at the same time to combine multiple sounds we use superposition simply add their waves together and rescale to make sure that all values stay between and as it turns out when we su perpose sine waves of different frequencies in this way we can get arbitrarily com plicated waves indeed one of the triumphs of century mathematics was the development of the idea that any smooth periodic function can be expressed as a sum of sine and cosine waves known as a fourier series this mathematical idea corresponds to the notion that we can create a large range of sounds with musi cal instruments or our vocal cords and that all sound consists of a composition of various oscillating curves any sound corresponds to a curve and any curve corre sponds to a sound so we can create arbitrarily complex curves with superposition computing with sound waves in section 5 we saw how to represent sound waves by arrays of numbers that represent their values at the same sample points now we will use such arrays as return values and arguments to functions to pro cess such data for example the following function takes a frequency in hertz and a duration in seconds as arguments and returns a representation of a sound wave more precisely an array that contains values sampled from the specified wave at the standard 100 samples per second def tone hz duration sps n int sps duration a stdarray n for i in range n a i math sin math pi i hz sps return a the size of the array returned depends on the duration it contains about sps duration floats nearly half a million floats for seconds but we can now treat that array the value returned from tone as a single entity and compose code that processes sound waves as we will soon see in program weighted superposition since we represent sound waves by arrays of numbers that represent their values at the same sample points superposition is simple to implement we add together their sample values at each sample point to produce the combined result for greater control we also specify a relative weight for each of the two waves to be superposed with the following function def superpose a b aweight bweight c stdarray len a for i in range len a c i aweight a i bweight b i return c this code assumes that a and b are of the same length for example if we have a sound represented by an array a that we want to have three times the effect of the sound represented by an array b we would call superpose a b 75 25 the figure at the top of the next page shows the use of two calls on this function to add harmonics to a tone we superpose the harmonics then superpose the re sult with the original tone which has the effect of giving the original tone twice lo tone lo hi tone hi harmonics superpose lo hi 5 5 harmonics 5 lo 5 hi 5 982 5 693 concerta tone concerta superpose harmonics concerta 5 5 0 5 harmonics 0 5 concerta 0 5 144 0 5 0 374 0 259 adding harmonics to concert a 220 second at 100 samples second the weight of each harmonic as long as the weights are positive and sum to superpose preserves our convention of keeping the values of all waves between and program playthattunedeluxe py is an implementation that applies these concepts to produce a more realistic sound than that produced by program 5 to do so it makes use of functions to divide the computation into four parts given a frequency and duration create a pure tone given two sound waves and relative weights superpose them given a pitch and duration create a note with harmonics read and play a sequence of pitch duration pairs from standard input program play that tune revisited playthattunedeluxe py import math import stdarray import stdaudio import stdio def superpose a b aweight bweight c stdarray len a 0 0 for i in range len a c i aweight a i bweight b i return c def tone hz duration sps n int sps duration a stdarray n 0 0 for i in range n a i math sin 0 math pi i hz sps return a def note pitch duration hz 0 0 pitch 0 lo tone hz duration hi tone hz duration harmonics superpose lo hi 0 5 0 5 a tone hz duration return superpose harmonics a 0 5 0 5 while not stdio isempty pitch stdio readint duration stdio readfloat a note pitch duration stdaudio playsamples a stdaudio wait hz frequency lo lower harmonic hi upper harmonic h combined harmonics a pure tone this program reads sound samples embellishes the sounds by adding harmonics to create a more realistic tone than program 1 5 and plays the resulting sound to standard audio python playthattunedeluxe py elise txt more elise txt 125 125 125 125 5 125 125 0 25 these tasks are all amenable to implemen tation as functions which depend on one another each function is well defined and straightforward to implement all of them and stdaudio represent sound as a series of discrete values kept in an array corresponding to sampling a sound wave at 100 samples per second up to this point our use of func tions has been somewhat of a notational convenience for example the control flow in program 1 1 program 1 and program 1 is simple each function is called in just one place in the code by contrast program 1 is a convincing ex ample of the effectiveness of defining func tions to organize a computation because each function is called multiple times for example as illustrated in the figure below the function note calls the function tone three times and the function su perpose twice without functions we would need multiple copies of the code in tone and superpose with functions we can deal directly with concepts close to the application like loops functions have a simple but profound effect one sequence of statements those in the function defini tion is executed multiple times during the execution of our program once for each time the function is called in the control flow in the global code flow of control among several functions functions are important because they give us the ability to extend the python language within a program having implemented and debugged functions such as harmonic pdf cdf mean exchange shuffle isprime superpose tone and note we can use them almost as if they were built into python the flexibility to do so opens up a whole new world of programming before you were safe in thinking about a python program as a sequence of state ments now you need to think of a python program as a set of functions that can call one another the statement to statement control flow to which you have been accustomed is still present within functions but programs have a higher level con trol flow defined by function calls and returns this ability enables you to think in terms of operations called for by the application not just the operations that are built into python whenever you can clearly separate tasks within a computation you should do so the examples in this section and the programs throughout the rest of the book clearly illustrate the benefits of adhering to this maxim with functions we can divide a long sequence of statements into independent parts reuse code without having to copy it work with higher level concepts such as sound waves this point of view leads to code that is easier to understand maintain and debug compared to a long program composed solely of python assignment conditional and loop statements in the next section we discuss the idea of using functions defined in other files which again takes us to another level of programming q a q can i use the statement return in a function without specifying a value a yes technically it returns the none object which is the sole value of the type nonetype q what happens if a function has one control flow that leads to a return state ment that returns a value but another control flow that reaches the end of the func tion body a it would be poor style to define such a function because doing so would place a severe burden on the function callers the callers would need to know under which circumstances the function returns a value and under which circumstances it returns none q what happens if i compose code in the body of a function that appears after the return statement a once a return statement is reached control returns to the caller so any code in the body of a function that appears after a return statement is useless it is never executed in python it is poor style but not illegal to define such a function q what happens if i define two functions with the same name but possibly a dif ferent number of arguments in the same py file a this is known as function overloading which is embraced by many program ming languages python however is not one of those languages the second func tion definition will overwrite the first one you can often achieve the same effect by using default arguments q what happens if i define two functions with the same name in different files a that is fine for example it would be good design to have a function named pdf in gauss py that computes the gaussian probability density function and another function named pdf in cauchy py that computes the cauchy probabil ity density function in section 2 you will learn how to call functions defined in different py files q can a function change the object to which a parameter variable is bound a yes you can use a parameter variable on the left side of an assignment state ment however many python programmers consider it poor style to do so note that such an assignment statement has no effect in the client q the issue with side effects and mutable objects is complicated is it really all that important a yes properly controlling side effects is one of a programmer most important tasks in large systems taking the time to be sure that you understand the difference between passing arrays which are mutable and passing integers floats booleans and strings which are immutable will certainly be worthwhile the very same mechanisms are used for all other types of data as you will learn in chapter q how can i arrange to pass an array to a function in such a way that the function cannot change the elements in the array a there is no direct way to do so in section 3 3 you will see how to achieve the same effect by building a wrapper data type and passing an object of that type instead you will also see how to use python built in tuple data type which rep resents an immutable sequence of objects q can i use a mutable object as a default value for an optional argument a yes but it may lead to unexpected behavior python evaluates a default value only once when the function is defined not each time the function is called so if the body of a function modifies a default value subsequent function calls will use the modified value similar difficulties arise if you initialize the default value by calling an impure function for example after python executes the code fragment def append a x random random a x return a b append c append b and c are aliases for the same array of length 2 not 1 which contains one float repeated twice instead of two different floats abstract this paper presents a method to analyze the powers of a given trilinear form a special kind of algebraic construction also called a tensor and obtain upper bounds on the asymptotic complexity of matrix multiplication compared with existing approaches this method is based on convex optimization and thus has polynomial time complexity as an application we use this method to study powers of the construction given by coppersmith and winograd journal of symbolic computation and obtain the upper bound ω on the exponent of square matrix multiplication which slightly improves the best known upper bound categories and subject descriptors f analysis of algorithms and problem complexity numerical algorithms and problems computations on matrices i symbolic and algebraic manipulation algorithms algebraic algorithms analysis of algorithms general terms algorithms theory keywords algebraic complexity theory matrix multiplication introduction matrix multiplication is one of the most fundamental tasks in mathematics and computer science while the product of two n n matrices over a ﬁeld can naturally be computed in o arithmetic operations strassen showed in that o arithmetic operations are enough the discovery of this algorithm for matrix multiplication with subcubic complexity gave rise to a new area of research where the central question is to determine the value of the exponent of square matrix multiplication denoted ω and deﬁned as the permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page copyrights for components of this work owned by others than the author must be honored abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior speciﬁc permission and or a fee request permissions from permissions acm org issac july kobe japan copyright is held by the owner author publication rights licensed to acm acm http dx doi org 2608664 㻞㻥㻢 minimal value such that two n n matrices over a ﬁeld can be multiplied using o nω ε arithmetic operations for any ε it has been widely conjectured that ω and several conjectures in combinatorics and group theory if true would lead to this result however the best upper bound obtained so far is ω as we explain below coppersmith and winograd showed in that ω their approach can be described as follows a trilinear form is informally speaking a three dimensional array with coeﬃcients in a ﬁeld f for any trilinear form t one can deﬁne its border rank denoted r t which is a positive integer characterizing the number of arithmetic operations needed to compute the form for any trilinear form t and any real number ρ one can deﬁne a real number vρ t called the value of the trilinear form the theory developed by scho nhage shows that for any m and any ρ the following statement hold m r t ω ρ vρ t m here the notation t m represents the trilinear form obtained by taking the m th tensor power of t coppersmith and winograd presented a speciﬁc trilinear form t obtained by modifying a construction given earlier by strassen computed its border rank r t and introduced deep techniques to estimate the value vρ t in particular they showed how a lower bound v ρ t on vρ t can be obtained for any ρ by solving an optimization problem solving this optimization problem they obtained the upper bound ω via statement with t t and m by ﬁnding the smallest ρ such that v ρ t r t they then proceeded to study the tensor power t and showed that despite several new technical diﬃculties a similar approach can be used to reduce the computation of a lower bound v ρ t on vρ t to solving another optimization problem of several variables they discovered that v ρ t v ρ t due to the fact that the analysis of t was ﬁner thus giving a better upper bound on ω via statement with t t and m solving numerically the new optimization problem they obtained the upper bound ω in view of the improvement obtained by taking the second tensor power a natural question was to investigate higher powers of the construction t by coppersmith and winograph investigating the third power was explicitly mentioned as an open problem in more that twenty years later stothers showed that while the third power does not seem to lead to any improvement the fourth power does give an improvement see also the improvement was obtained again via statement by showing how to reduce the computation of vρ t to solving a non convex optimization problem the upper bound ω was obtained in by ﬁnding numerically a solution of this optimization problem it was later discovered that this solution was not optimal and the improved upper bound ω was given in by exhibiting a better solution of the same optimization problem independently vassilevska williams constructed a powerful and general framework to analyze recursively powers of a class of trilinear forms including the trilinear form t by coppersmith and winograd and showed how to automatically reduce for any form t in this class and any integer m the problem of obtaining lower bounds on vρ t m to solving in general non convex optimization problems the upper bound ω was obtained by applying this framework with t t and m and numerically solving this optimization problem a natural question is to determine what bounds on ω can be obtained by studying t m for m one may even hope that when m goes to inﬁnity the upper bound on ω goes to two unfortunately this question can hardly be answered by this approach since the optimization problems are highly non convex and become intractable even for modest values of m in this paper we show how to modify the framework developed in in such a way that the computation of vρ t m reduces to solving poly m instances of convex optimization problems each having poly m variables from a theoretical point a view since a solution of such convex problems can be found in polynomial time via statement we obtain an algorithm to derive an upper bound on ω from t m in time polynomial in m from a practical point of view the convex problems we obtain can also be solved eﬃciently and have several desirable properties in particular the optimality of a solution can be guaranteed by using the dual problem we use this method to analyze t and t and obtain the new upper bounds on ω described in table besides leading to an improvement for ω these results strongly suggest that studying powers higher than will give only negligible improvements our method is actually more general and can be used to eﬃciently obtain lower bounds on vρ t t for any trilinear forms t and t that have a structure similar to t this applies in particular to the case where t is trivial giving an eﬃcient i e based on convex optimization way to compute lower bounds on vρ t indeed considering possible future applications of our approach we have been attentive of stating our techniques as generally as possible algebraic complexity theory this section presents the notions of algebraic complexity needed for this work we refer to e g for more detailed treatments in this paper f denotes an arbitrary ﬁeld trilinear forms let u v and w be three positive integers and u v and w be three vector spaces over f of dimension u v and w note that while the upper bound on ω obtained for the eighth power is stated as ω in the conference version the statement has been corrected to ω in the most recent version available at the author homepage since the previous bound omitted some necessary constraints in the optimization problem our results conﬁrm the value of the latter bound and increase its precision 㻞㻥㻣 table upper bounds on ω obtained by analyzing the m th power of the construction t by coppersmith and winograd m upper bound ω ω ω ω ω ω reference ref ref ref this paper section ω given in ref this paper section this paper section respectively a trilinear form also called a tensor t on u v w is an element in u v w fu v w where denotes the tensor product if we ﬁx bases xi yj and zk of u v and w respectively then t can be written as x tijk xi yj zk t i j k for coeﬃcients tijk in f we will usually write xi yj zj simply as xi yj zk matrix multiplication of an m n matrix with entries in f by an n p matrix with entries in f corresponds to the trilinear form on fm n fn p fm p with coeﬃcients tijk if i r j t and k r t for some integers r t m n p and tijk otherwise indeed this form can be rewritten as p m x n x x x r y t z r t r t then replacing the x variables by the entries of the ﬁrst matrix and the y variables by the entries of the second matrix the coeﬃcient of z r t in the above expression represents the entry in the r th row and the t th column of the matrix product of these two matrices this trilinear form will be denoted by m n p p another important example is the form n x y z this trilinear form on fn fn fn is denoted n and corresponds to n independent scalar products given a tensor t u v w it will be convenient to denote by tc and the tensors in v w u and w u v respectively p obtained by permuting cyclicly p the coordinates of t tc ijk tijk yj zk xi and ijk tijk zk xi yj given two tensors t u v w and t u v w we can naturally deﬁne their direct sum t t which is a tensor in u u v v w w and their tensor product t t which is a tensor in u u v v w w for any integer c the tensor t t with c occurrences of t will be denoted by c t and the tensor t t with c occurrences of t will be denoted by t c let λ be an indeterminate and consider the extension f λ of f i e the set of all polynomials over f in λ let t fu v w and t fu v w be two tensors we say that t is a degeneration of t denoted t t if there exist three matrices α f λ u u β f λ v v γ f λ w w such that x tijk α xi β yj γ zj λs t λs t ijk for some tensor t f λ u v w and some nonnegative integer intuitively the fact that a tensor t is a degeneration of a tensor t means that an algorithm computing t can be converted into another algorithm computing t with essentially the same complexity the notion of degeneration can be used to deﬁne the notion of border rank of a tensor t denoted r t as follows r t min r n t r where i j and k are three ﬁnite subsets of z let us call this decomposition d we say that d is a decomposition of t if the tensor t can be written as x t i j k t i j k i j k where each t i j k is a tensor in ui vj wk the sum does not need to be direct the support of t with respect to d is deﬁned as the border rank is submultiplicative r t t r t r t for any two tensors t and t and the nonzero t i j k are called the components of t the exponent of matrix multiplication the following theorem which was obtained by scho nhage shows that good upper bounds on ω can be obtained by ﬁnding a trilinear form of small border rank that can be degenerated into a direct sum of several large matrix multiplications theorem let e and m be two positive integers let t be a tensor such that e m m m t then emω r t our results will require a generalization of theorem based on the concept of value of a tensor our presentation of this concept follows given a tensor t fu v w and a positive integer n deﬁne the set o n e m n n e m m m t tc n corresponding to all pairs e m such that the tensor t tc n can be degenerated into a direct sum of e tensors each isomorphic to m m m note that this set is ﬁnite for any real number ρ deﬁne vρ n t max emρ where the maximum is over all e m in the set of eq we now give the formal deﬁnition of the value of a tensor vρ t lim vρ n t n the limit in this deﬁnition is well deﬁned see obviously vρ t vρ t for any tensors t t such that t t by deﬁnition for any positive integers m n and p we have vρ m n p mnp ρ moreover the value is superadditive and supermultiplicative for any two tensors t and t and any ρ the inequalities vρ t t vρ t vρ t and vρ t t vρ t vρ t hold with this concept of value we can state the following slight generalization of theorem which was used implicitly in and stated explicitly in theorem let t be a tensor and ρ be a real number such that ρ if vρ t r t then ω ρ finally we will need the concept of decomposition of a tensor our presentation of this concept follows let t u v w be a tensor suppose that the vector spaces u v and w decompose as m m m ui v vj w vk u j j preliminaries and notations in this section s denotes a ﬁnite subset of z z z let s z be the three coordinate functions of s which means that α for each and all s we ﬁrst deﬁne the concept of tightness the same notion was used in definition the set s is tight if there exists an integer d such that d for all s the set s is b tight where b is a positive integer if additionally α s b for all note that if s is b tight then s we denote by f s the set of all real valued functions on s and by d s the set of all probability distributions on s i e the set of all functions f f s such that f p f note that with for each s and s pointwise addition and scalar multiplication f s forms a real vector space of dimension s given any function f f s we denote by s r s r and s r the three marginal functions of f for each and each a α s x f a f α a let ss denote the group of all permutations on s given any function f f s and any σ ss we will denote by f σ the function in f s such that f σ f σ for all s we now deﬁne the concept of invariance of a function definition for any tensor t and any ρ i i supp t i j k i j k t i j k k k 㻞㻥㻤 definition let g be a subgroup of ss a function f f s is g invariant if f σ f for all σ g we will denote by f s g the set of all g invariant realvalued functions on s and by d s g d s f s g the set of all g invariant probability distributions on s we denote by s g the vector space of all functions f f s g such that f a for all and all a α s and write χ s g dim s g we call this number χ s g the compatibility degree of s with respect to g in our applications it will be sometimes more convenient to characterize the invariance in terms of a subgroup of permutations of the three coordinates of s rather than in terms of a subgroup of permutations on s as follows let l be a subgroup of the group of permutations over we say that s is l symmetric if sσ sσ sσ s for all s and all σ l if s is l symmetric the subgroup l induces the subgroup ls πσ σ l of ss where πσ denotes the permutation in ss such that πσ sσ sσ sσ for all s we will slightly abuse notation and when s is l symmetric simply write f s l d s l s l χ s l to represent f s ls d s ls s ls and χ s ls respectively the entropy of a probability distribution p d s is x h p p log p of the lower bounds obtained ref also gives another exposition of this approach solving the optimization problem let t be a trilinear form with a decomposition that has a tight support as in the statement of theorem it will be convenient to deﬁne for any ρ the function ψt ρ d s r as s with the usual convention log using the above notations and represent the three marginal probability distributions of p for each the entropy of p is x p a log p a h p a α s it will sometimes be more convenient to represent for each the distribution p as a vector p r α s by ﬁxing an arbitrary ordering of the elements in α s we now deﬁne the concept of compatibility of two probability distributions definition two probability distributions p and q in d s are compatible if p q for each finally for any p d s we deﬁne the quantity γs p max h q h p q where the maximum is over all q d s compatible with p note that γs p is always non negative general theory in this section we describe how to analyze the value of a trilinear form that has a decomposition with tight support derivation of lower bounds on the value our main tool to analyze a trilinear form that has a decomposition with tight support is the following theorem which shows how to reduce the computation of a lower bound on its value to solving an optimization problem due to space constraints its proof is omitted from this version theorem let t be a trilinear form and d be a decomposition of t with tight support s z z z and component set t s then for any p d s and any ρ log vρ t x h p x p log vρ t γs p s this theorem can be seen as a generalized statement of the approach developed by coppersmith and winograd several similar statements already appeared in the literature a weaker statement corresponding to the simpler case where each component is isomorphic to a matrix product which removes the need for the term γs p in the lower bound can be found in the generalization to the case of arbitrary components stated in theorem was considered in and proved implicitly by considering several cases symmetric and asymmetric supports and without reference to the entropy in theorem aims at providing a concise statement unifying all these results described in terms of entropy in order to discuss the convexity 㻞㻥㻥 ψt ρ p x h p x p log vρ t s for any p d s note that this is a concave function on the convex set d s in order to optimize the lower bound on log vρ t that is obtained from theorem we would like to ﬁnd for a given value of ρ a probability distribution p d s that minimizes the expression γs p ψt ρ p this optimization problem is in general not convex due to the presence of the term γs p in this subsection we develop a method to overcome this diﬃculty and ﬁnd using theorem a lower bound on vρ t in polynomial time remember that γs p maxq h q h p where the maximum is over all q d s that are compatible with p when p is ﬁxed these conditions on q can be written as linear constraints since the entropy is a strictly concave function computing γs p is then a strictly convex optimization problem on a convex set and in particular has a unique solution q note that γs q h q h q and thus ψt ρ q is a lower bound on log vρ t the tightness of this lower bound of course depends on the initial choice of p a natural choice is to take a probability distribution p that maximizes ψt ρ p since ﬁnding such a probability distribution corresponds to solving a convex optimization problem this motivates the algorithm described in figure which we call algorithm a algorithm a input the support s z z z of the tensor t a value ρ the values vρ t for each s solve the following convex optimization problem minimize ψt ρ p subject to p d s solve the following convex optimization problem where p denotes the solution found at step minimize h q subject to q d s q compatible with p output ψt ρ q where q denotes the solution found at step figure algorithm a computing given ρ and a tensor t with a decomposition that has a tight support a lower bound on log vρ t as already mentioned the optimization problem opt has a unique solution while the solution of the optimization problem may not be unique it can actually be shown using the strict concavity of the entropy function that two solutions of must have the same marginal probability distributions since the domain of the optimization problem depends only on the marginal distributions of p the output of algorithm a does not depend on which solution p was found at step this output is thus unique and from theorem and the discussion above it gives a lower bound on log vρ t we state this conclusion in the following theorem theorem if the support of t is tight then algorithm a outputs a lower bound on log vρ t note that the output of a is not always the best lower bound on log vρ t that can be obtained from theorem this point will be discussed in the next subsection let us now discuss the time complexity of implementing the algorithm of figure the worst case running time depends on the time needed to solve the two optimization problems and at steps and let v ψt ρ q denote the output of an exact implementation of algorithm a theorem shows that v log vρ t since both and are convex and since the number of variables is upper bounded by s for any ε both problems can be solved with accuracy ε in time poly s log ε using standard methods thus for any ε we can compute in time poly s log ε a value v such that v v v ε v in particular we can use ε as a lower bound on log vρ t we ﬁnally explain how to exploit symmetries of the decomposition of t to reduce the number of variables in algorithm a these observations will enable us to slightly simplify the exposition of our results in the next sections we ﬁrst deﬁne invariance of a decomposition of a tensor tially how the powers of the construction by coppersmith and winograd were studied in previous works given any subgroup g of ss let us consider the vector space s g of dimension χ s g deﬁned in section it will be convenient to represent functions in this vector space by vectors in r s by ﬁxing an arbitrary ordering of the elements in s let r be a generating matrix of size s χ s g for s g i e the columns of r form a basis of s g since each coordinate of r s corresponds to an element of s we write rsj for s and j χ s g to represent the element in the th row and the j th column of r the approach is based on the following proposition which is similar to a characterization given in its proof is omitted from this version proposition for any p p d s g that are compatible the equality γs p h p h p holds if p satisﬁes the following two conditions i p for any s such that r contains at least one non zero entry in its row labeled by p ii s rsj log p for all j χ s g in particular applying proposition with p p shows that if conditions i and ii are satisﬁed then γs p which implies log vρ t ψt ρ p from theorem this motivates the algorithm described in figure that outputs a lower bound on log vρ t we will call it algorithm b algorithm b input solve the following optimization problem definition let t be a tensor that has a decomposition d with support s and components t s the decomposition d is g invariant if ψt ρ p σ ψt ρ p for any p d s and any σ g with a slight abuse of language we will say given a subgroup l of that d is l invariant if s is l symmetric and d is ls invariant see section for the deﬁnition of ls assume that the decomposition d of the tensor t on which we want to apply algorithm a is g invariant where g is a subgroup of ss consider the optimization problem since the value of its objective function is then unchanged under the action of any permutation σ g on p has a solution that is g invariant see e g for a discussion of symmetries in convex optimization now if p is g invariant then the unique solution of the optimization problem is g invariant as well since the value of the function h q is unchanged under the action of any permutation on q this means that if the decomposition d is g invariant then d s can be replaced by d s g at both steps and of algorithm a note that this set of probability distributions can be parametrized by dim f s g parameters instead of s parameters the support s z z z for the tensor t a value ρ the values vρ t for each s a subgroup g of ss such that the decomposition of t is g invariant minimize ψt ρ p subject to p d s g p satisﬁes cond i ii of prop output ψt ρ p where p denotes the solution found at step figure algorithm b computing given ρ and a tensor t with a decomposition that has a tight support a lower bound on log vρ t note that when χ s g algorithms a and b solve exactly the same optimization problem since in algorithm b conditions i and ii are satisﬁed for any p d s g and q p in algorithm a and thus output the same value when χ s g algorithm b usually gives better lower bounds than algorithm a but at the price of introducing χ s g highly nonconvex constraints which makes the optimization problem much harder to solve both in theory and in practice even for a modest number of variables another approach in this subsection we describe another approach to obtain lower bounds on vρ t using theorem which is essen let t and t be two trilinear forms with decompositions d and d respectively let supp t z z z and supp t 㻟㻜㻜 powers of tensors z z z denote their supports and t supp t and t supp t denote their component sets assume that both supports are tight fix ρ and assume that lower bounds on the values vρ t and vρ t are known for each supp t and each supp t in this section we describe a method inspired by and and also used in to analyze vρ t t and then show how to use it to analyze vρ t m when m is a power of two in this section we will denote z z z z the three coordinate functions of z z z consider the tensor x x t t t t supp t supp t consider the following decomposition of t t the support supp t t z z z is the set of all triples for supp t and supp t and for each a b c supp t t the associated component is x t t t t a b c where the sum is over all supp t supp t such that a b and c note that the support of this decomposition is tight if lower bounds on the value of each component are known then we can use this decomposition to obtain a lower bound on vρ t t by using algorithm a on t t which requires solving two convex optimization problems each having supp t t variables we now explain how to evaluate the value of those components t t a b c for any a b c supp t t consider the following decomposition of t t a b c the support is supp t a b c supp t and for each element in this set the corresponding component is t t a b c note that the support in this decomposition is tight and has size at most supp t the value of each component can be lower bounded as vρ t t a b c vρ t vρ t a b c from the supermultiplicativity of the value as we supposed that the lower bounds on the values of each component of t and t are known we can use algorithm a on each t t a b c to obtain a lower bound on vρ t t a b c which requires solving two convex optimization problems each having at most supp t variables let us now consider the case t t we have just shown the following result a lower bound on vρ t can be computed by solving two convex optimization problems with supp t variables and supp t convex optimization problems with at most supp t variables an important point is that this method additionally gives as described in the previous paragraphs a decomposition of t with tight support and a lower bound on vρ t a b c for each component t a b c this information can then be used to analyze the trilinear form t t t by replacing t 㻟㻜㻝 by t in the above analysis giving a decomposition of t with tight support a lower bound on vρ t and a lower bound on the value vρ t a b c of each component by iterating this approach r rtimes for any r we can anaobtain a lower lyze the trilinearrform t and in particular r bound on vρ t let us denote by the decomposition r r of t obtained by this approach its support supp t is the set of all triples r r for supp t for any a b c supp t the corresponding component is x r r t t t a b c r where the sum is over all supp t such that a b and of each c this approach also gives a decomposition dabc a b c in this decomposition the support component t r is the set of all the elements in which we denote sabc supp t such that a b c supp t for any is t r sabc r the corresponding component of t t r a b c a b c the overall number of convex optimization problems that r need to be solved in order to analyze t rby the above approach is upper bounded by r supp t while the number of variables in each optimization problem is upper r bounded by supp t in the case where supp t is b tight we can give a simple upper bound on this quantity indeed when supp t is b tight our construction guarantees that r r supp t is tight which implies that supp t r we thus obtain the following result theorem let t be a trilinear form that has a decomposition with b tight support supp t and components t fix ρ and assume that a lower bound on the value vρ t is known for each supp t then for any inr teger r a lower bound on vρ t can be computed r by solving poly b convex optimizations problems each optimization problem having poly b variables we ﬁnally present two simple lemmas that show how to exploit the symmetries of the decomposition of t to reduce the number of variables the proofs are omitted r lemma for any r and any a b c supp t is id π invariant where id dethe decomposition dabc notes the identity permutation and π is the permutation on such that sabc π a b c r for all sabc lemma let l be a subgroup of supp t is l symmetric and that assume that vρ t vρ t sσ sσ sσ for any σ l and any supp t then d r is l invariant and for any r the decomposition is l invariant as well from the discussion of section lemma enables us to reduce the number of variables when computing the lower r bound on vρ t a b c using algorithm a instead of r solving an optimization problem over d supp t a b c r we only need to consider d supp t a b c id π similarly if the conditions of lemmar are satisﬁed then instead of considering d supp t we need only to conr sider d supp t l when computing the lower bound on r vρ t using algorithm a remark the approach described in this section can be generalized to obtain lower bounds on vρ t m when m is not a power of two for instance the third power can be analyzed by studying t t with t t another possible straightforward generalization is to allow other linear dependences in the deﬁnition of the support i e deﬁning the support of t t as the set of all triples α2 for supp t and supp t where u z can be freely chosen these two generalizations nevertheless do not seem to lead to any improvement for ω when applied to existing constructions we have vρ t and vρ t q ρ from the deﬁnition of the value the other components t and t are obtained by permuting the coordinates of t while the components t and t are obtained by permuting the coordinates of t we now use theorem to obtain an upper bound on ω let p be a probability distribution in d s let us write p p p p p and p the marginal distributions of p are and since the only element in d s compatible with p is p we have γs p theorem thus implies that h h h q a5 ρ vρ t exp for any ρ evaluating this expression with q a3 a5 and ρ gives vρ t using theorem and the fact that r t q we conclude that ω this is the same upper bound as the bound obtained in section of application in this section we apply the theory developed in the previous sections to the construction t by coppersmith and winograd in order to upper bounds on ω construction let f be an arbitrary ﬁeld let q be a positive integer and consider three vector spaces u v and w of dimension q over f take a basis xq of u a basis yq of v and a basis zq of w the trilinear form t considered by coppersmith and winograd is the following trilinear form on u v w t q x yi zi xi zi xi yi zq i yq xq it was shown in that r t q consider the following decomposition of u v and w u v w where span span xq and span xq span span yq and span yq span span zq and span zq this decomposition induces a decomposition d of t with tight support s analyzing the powers using algorithm a r for any r we now consider the tensor t and analyze it using the framework and the rnotations of section the support of its decomposition is the set of all triples a b c such that a b c note that the decomposition d of t satisﬁes the conditions of lemma for the subgroup r l of which implies that is invariant thus from the discussion in section r when applying algorithm a on the trilinear form t in order to obtain a lower bound on vρ t we only need to consider probar bility distributions in d supp t this set can be r parametrized by dim f supp t parameters remember that we also need a lower bound on the value of r r each component t a b c before applying a on t using the method described in section these lower bounds are computed recursively by applying algorithm a on the of the component actually we do not decomposition dabc need to apply a when a b or c since a lower bound on the value can be found analytically in this case as stated in the following lemma see for a proof lemma for any r and any b r x b r ec q a b b vρ t e b e b e e b e b mod table presents for r the number of variables in the global optimization problem the compatibility degree and the best upper bound on ω we obtained by this approach computations have been done using the matlab software cvx for convex optimization the components associated with and are t xq q x xi yi z t q i all the programs used to perform the numerical calculations described in this section and obtain our upper bounds on ω are available as http www francoislegall com matrixmultiplication programs zip 㻟㻜㻞 the lower bound of lemma is obtained directly i e without using theorem by observing that when a b or c the component is isomorphic to the tensor of a matrix product and is actually better than the lower bound obtained by algorithm a or algorithm b r table analysis of t using algorithm a with q for r and q for r r dimension of f supp t upper bound r χ supp t ω ω ω ω ω obtained 3729372 3728672 using both algorithms a and b as mentioned in the introduction the best known upper bound on ω obtained from the fourth power of t is ω which is slightly better than what we obtained in the previous subsection using algorithm a this better bound can actually be obtained by using algorithm b instead of algorithm a when computing the lower bound on vρ t more precisely in this case the optimization problem in algorithm b asks to minimize ψt ρ p such that p d supp t and p satisﬁes two additional constraints since χ supp t these constraints are highly non convex but since their number is only two the resulting optimization problem can be solved fairly easily giving the same upper bound ω we can also use algorithm b instead of algorithm a to analyze t but solving the corresponding optimization problems in this case was delicate and required a combination of several tools we obtained lower bounds on the values of each component by solving the non convex optimization problems using the nlpsolve function in maple while the lower bound on vρ t has been obtained by solving the corresponding optimization problem with variables and non convex constraints using the fmincon function in matlab the numerical calculations give vρ t q which shows that ω while the non convex optimization problems of algorithm b seem intractable when studying higher powers of t these powers can be analyzed by applying algorithm a as in the previous subsection but using this time the lower bounds on the values of the components vρ t a b c obtained by algorithm b as a starting point this strategy can be equivalently described as using algorithm a to analyze powers of t where t t with lower bounds on the values of each component of t computed by algorithm b for q and ρ we obtain vρ t q which shows that ω we obtain this paper and to harumichi nishimura suguru tamaki and yuichi yoshida for stimulating discussions this work is supported by the grant in aid for young scientists b no of the jsps and the grant in aid for scientiﬁc research on innovative areas no of the mext for q and ρ vρ t q which shows that ω 3728639 we propose a new notion of secure multiparty computation aided by a computationally powerful but untrusted cloud server in this notion that we call on the fly multiparty computation mpc the cloud can non interactively perform arbitrary dynamically chosen computations on data belonging to arbitrary sets of users chosen on the fly all user input data and intermediate results are protected from snooping by the cloud as well as other users this extends the standard notion of fully homomorphic encryption fhe where users can only enlist the cloud help in evaluating functions on their own encrypted data in on the fly mpc each user is involved only when initially uploading his encrypted data to the cloud and in a final output decryption phase when outputs are revealed the complexity of both is independent of the function being computed and the total number of users in the system when users upload their data they need not decide in advance which function will be computed nor who they will compute with they need only retroactively approve the eventually chosen functions and on whose data the functions were evaluated this notion is qualitatively the best possible in minimizing interaction since the users interaction in the decryption stage is inevitable we show that removing it would imply generic program obfuscation and is thus impossible our contributions are two fold we show how on the fly mpc can be achieved using a new type of encryption scheme that we call multikey fhe which is capable of operating on inputs encrypted under multiple unrelated keys a ciphertext resulting from a multikey evaluation can be jointly decrypted using the secret keys of all the users involved in the computation we construct a multikey fhe scheme based on ntru a very efficient public key encryption scheme proposed permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee stoc may new york new york usa copyright acm vinod vaikuntanathan in the it was previously not known how to make ntru fully homomorphic even for a single party we view the construction of multikey fhe from ntru encryption as a main contribution of independent interest although the transformation to a fully homomorphic system deteriorates the efficiency of ntru somewhat we believe that this system is a leading candidate for a practical fhe scheme categories and subject descriptors e data data encryption general terms theory security algorithms keywords fully homomorphic encryption multiparty computation cloud computing ntru encryption introduction we are fast approaching a new digital era in which we store our data and perform our expensive computations remotely on powerful servers the cloud in popular parlance while the cloud offers numerous advantages in costs and functionality it raises grave questions of confidentiality since data stored in the cloud could be vulnerable to snooping by the cloud provider or even by other cloud clients since this data often contains sensitive information e g personal conversations medical information and organizational secrets it is prudent for the users to encrypt their data before storing it in the cloud recent advances in fully homomorphic encryption fhe make it possible to perform arbitrary computations on encrypted data thus enabling the prospect of personal computers and mobile devices as trusted but weak interfaces to a powerful but untrusted cloud on which the bulk of computing is performed fhe is only suitable in settings where the computations involve a single user since it requires inputs to be encrypted under the same key however there are many scenarios where users who have uploaded their large data stores to the cloud in encrypted form then decide to compute some joint function of their data for example they may wish the cloud to compute joint statistical information on their databases locate common files in their collections run a computational agent to reach a decision based on their pooled data without leaking anything but the final decision or generally in contexts where multiple mutually distrusting users need to pool together their data to achieve a common goal the multiparty scenario is significantly more complex and comes with a set of natural but stringent requirements first the participants involved in the computation and the function to be computed may be dynamically chosen on thefly well after the data has been encrypted and uploaded to the cloud secondly once the function is chosen we should not expect the users to be online all the time and consequently it is imperative that the cloud be able to perform the bulk of this computation on the encrypted data belonging to the participants non interactively without consulting the participants at all finally all the burden of computation should indeed be carried by the cloud the computational and communication complexity of the users should depend only on the size of the individual inputs and the output and should be independent of both the complexity of the function computed and the total number of users in the system both of which could be very large on the fly multiparty computation consider a setting with a large universe of computationallyweak users and a powerful cloud an on the fly multiparty computation protocol proceeds thus the numerous users each encrypt their data and upload them to the cloud unaware of the identity or even the number of other users in the system additional data may arrive directly to the cloud encrypted under users public keys e g as encrypted emails arriving to a cloud based mailbox the cloud decides to evaluate an arbitrary dynamically chosen function on the data of arbitrary subset of users chosen on the fly the choice may be by some users request or as a service to compute the function on the data of parties fulfilling some criterion or by a need autonomously anticipated by the cloud provider etc the cloud can perform this computation noninteractively without any further help from the users the result is still encrypted the cloud and the subset of users whose data was used in the computation interact in a decryption phase at this point the users retroactively approve the choice of function and the choice of peer users on whose data the function was evaluated and cooperate to retrieve the output crucially the computation and communication of all the users including the cloud in the decryption phase should be independent of both the complexity of the function computed and the size of the universe of parties both of which can be enormous instead the effort expended by the cloud and the users in this phase should depend only on the size of the output and the number of users who participated in the computation also crucially the users need not be online at all during the bulk of the computation they need to wake up only when it is time to decrypt the output we call this an on the fly multiparty computation or onthe fly mpc in short to signify the fact that the functions to be computed on the encrypted data and the participants in the computation are both chosen on the fly and dynamically without possibly even the knowledge of the participants protocols following this framework have additional desirable features such as the ability for users to join a computation asynchronously possible approaches and why they do not work the long line of work on secure multiparty computation mpc does not seem to help us construct on the fly mpc protocols since the computational and communication complexities of all the parties in these protocols depends polynomially on the complexity of the function being computed in contrast we are dealing with an asymmetric setting where the cloud computes a lot but the users compute very little nevertheless we will use the traditional mpc protocols to interactively compute the decryption function at the end fully homomorphic encryption fhe is appropriate in such an asymmetric setting of computing with the cloud yet traditional fhe schemes are single key in the sense that they can perform arbitrarily complex computations on inputs encrypted under the same key in our setting since the parties do not trust each other they will most certainly not want to encrypt their inputs using each other keys nevertheless gentry proposed the following way of using single key fhe schemes in order to do multiparty computation first the parties run a short mpc protocol to compute a joint public key where the matching secret key is secret shared among all the parties the parties then encrypt their inputs under the joint public key and send the ciphertexts to the cloud who then uses the fhe scheme to compute an encryption of the result finally the parties run yet another short mpc protocol to recover the result a recent work by asharov et al extends this schema and makes it efficient in terms of the concrete round communication and computational complexity this line of work does not address the dynamic and noninteractive nature of on the fly mpc in particular once a subset of parties and a function are chosen the protocols of require the parties to be online and run an interactive mpc protocol to generate a joint public key in contrast we require that once the function and a subset of parties is chosen the cloud performs the expensive computations noninteractively without help from any of the users it would also be unsatisfactory to postpone the lengthy computation of the function until the interactive decryption phase indeed we require that once the users wake up for the decryption phase the running time of all parties is independent of the complexity of the function being computed thus even the feasibility of on the fly mpc is not addressed by existing techniques our solution we present a new notion of fully homomorphic encryption fhe that we call a multikey fhe that permits computation on data encrypted under multiple unrelated keys a new construction of multikey fhe based on the ntru the works of damga rd et al are an exception to this claim however it is not clear how to build upon these results to address the dynamic and non interactive nature of on the fly mpc tion scheme originally proposed by hoffstein pipher and silverman and a new method of achieving on the fly multiparty computation for any a priori bounded number of users using a multikey fhe scheme although the number of users involved in any computation has to be bounded in our solution the total number of users in the system is arbitrary our results and techniques new notion multikey homomorphic encryption an n key fully homomorphic encryption scheme is the same as a regular fhe scheme with two changes first the homomorphic evaluation algorithm takes in polynomially many ciphertexts encrypted under at most n keys together with the corresponding evaluation keys and produces a ciphertext second in order to decrypt the resulting ciphertext one uses all the involved secret keys a multikey fhe scheme is indeed the right tool to perform on the fly mpc as shown by the following simple protocol the users encrypt their inputs using their own public keys and send the ciphertexts to the cloud the cloud then computes a dynamically chosen function on an arbitrary subset of parties using the multikey property of the fhe scheme and finally the users together run an interactive mpc protocol in order to decrypt note that the users can be offline during the bulk of the computation and they need to participate only in the final cheap interactive decryption process note also that participants in the protocol need not be aware of the entire universe of users but only those users that participate in a joint computation this simple protocol provides us security against a semi honest collusion of the cloud with an arbitrary subset of parties we then show how to achieve security against a malicious adversary using tools such as verifiable computation protocols or succinct argument systems the computation of the decryption function can itself be outsourced to the cloud in particular using the cloudassisted mpc protocol of asharov et al yields a round on the fly mpc protocol one offline round and four online rounds to perform decryption as an additional benefit in the resulting on the fly protocol the parties may communicate with the server concurrently at each stage the only disadvantage of this approach is that it requires a crs setup this does not however affect the on the fly nature of the procotol since only an apriori bound n on the number of computing parties needs to be known when creating the crs multikey fully homomorphic encryption from ntru the starting point of our main construction of multikey fhe is the ntru encryption scheme of hoffstein pipher and silverman more precisely the slightly modified version due to stehle and steinfeld ntru is one of the earliest lattice based public key encryption schemes together with the ajtai dwork cryptosystem and the goldreichgoldwasser halevi cryptosystem we first observe that ntru can be made single key fully homomorphic using the recent techniques of using some additional tricks we then show that the scheme is multikey fully homomorphic for a bounded number of users at essentially the same cost previously it was not even known whether ntru could be turned into a regular single key fully homomorphic encryption scheme this construction is one of our main contributions and we believe it to be of independent interest our construction is particularly interesting since the ntru scheme was originally proposed as an efficient public key encryption scheme meant to replace rsa and elliptic curve cryptosystems in applications where computational efficiency is at a premium for example applications that run on smart cards and embedded systems although the transformation to a fully homomorphic system deteriorates the efficiency of ntru somewhat we believe that this system is a leading candidate for a practical fhe scheme what more as we show the scheme supports homomorphic operations on encryptions under multiple keys theorem informal for every n n there is an n user multikey fully homomorphic encryption scheme under the assumption that the ntru encryption scheme described below is semantically secure and circular secure the size of the keys and ciphertexts in the scheme grow polynomially with n we briefly sketch here our variant of the ntru encryption scheme and the ideas in turning it into a multikey fully homomorphic encryption scheme the reader is referred to section and section for a detailed exposition the main differences between the original ntru scheme and our variant are threefold whereas the original ntru scheme adds a deterministic noise to the ciphertext the variant considered here adds noise chosen from a distribution with bounded support specifically a discrete gaussian distribution a modification recently introduced by stehle and steinfeld it seems that this could only improve security indeed the purpose of the stehle steinfeld work was to prove the security of ntru based on worstcase hardness assumptions on ideal lattices we do all our operations modulo xn where n is a power of as in as opposed to xn in the original ntru our parameters are more aggressive than in to support homomorphisms as a result the worst case to average case connection shown by does not carry over to our setting of parameters for security parameter κ the scheme is parametrized by a prime number q q κ and a b bounded error distribution χ over the ring r z x hxn i e χ is a distribution over polynomials whose coefficients are all at most b κ in absolute value the parameters n q and χ are public we show how to encrypt bits using the scheme all operations in the scheme take place in the ring rq r qr keygen sample bounded polynomials f g χ and set f so that f mod set the public key pk h rq and the secret key sk f r if f is not invertible over rq resample f enc pk m sample bounded polynomials e χ output the ciphertext c hs m rq the observation that ntru can be made single key fully homomorphic was made concurrently by gentry et al dec sk c let µ f c rq output µ mod as the message decryption works since fc mod q f hs m mod q gs ef f m gs ef f m mod q where the last equality is true since gs ef f m q taking this quantity mod then gives us the message m since f mod the multikey homomorphic properties of the scheme are best seen through the lens of the decryption equation as in in particular consider ciphertexts rq and rq that encrypt messages and under public keys and respectively with noise terms and a little algebraic manipulation shows that cadd and cmult are ciphertexts that encrypt the sum and product of and respectively albeit with larger error terms namely decrypting and with the joint secret key which is simply a product of the two secret keys and gives us capable of evaluating circuits of depth log n for some small constant for more details see section to turn this into a fully homomorphic encryption scheme we use the technique of modulus reduction from the work of brakerski and vaikuntanathan later refined in modulus reduction shows how to reduce the magnitude of the error while simultaneously reducing the size of the modulus this technique works transparently in the multikey setting the bottom line is that we can evaluate functions on n users as long as n log q polylog n put another way for any number n of users we get a n user multikey fhe by setting q to be a large enough function of n this gives us a leveled multikey fhe scheme finally to turn this into a full fledged multikey fhe scheme whose complexity is independent of the complexity of the function being computed we use a multikey analog of gentry bootstrapping technique our construction based on the ntru encryption scheme raises a natural question can any of the other fhe schemes be made multi key it turns out that the schemes of can be made n key fully homomorphic for a constant n or sometimes even n o log n we refer the reader to the full version for a detailed discussion of these schemes completely non interactive on the fly mpc our results raise the natural question of whether the protocols can be made completely non interactive namely the users do not ever have to talk to each other even in the decryption phase in the non interactive setting the server can always evaluate the circuit multiple times keeping some parties inputs but plugging in fake inputs of its choosing for the other parties similarly to however by drawing on the impossibility of program obfuscation we show that a non interactive protocol cannot be achieved even if we accept this as the ideal functionality thus our notion is qualitatively the best possible in terms of interaction see the full version of this paper for a formal theorem statement other related work this shows that decrypting using the joint secret key results in the sum of the two messages assuming that the error does not grow to be too large likewise we have this shows that decrypting using the joint secret key results in the product of the two messages assuming that the error does not grow to be too large extending this to circuits we observe that the effective secret key required to decrypt a ciphertext c resulting from evaluating a multivariate polynomial function on the inputs q di th of n users is n varii fi where di is the degree of the i able in the polynomial function this makes the secret key required to decrypt c dependent on the circuit evaluated which is unacceptable even for somewhat homomorphic encryption we use the relinearization technique from to transform the ciphertext into one that can be decrypted usq ing the secret key n i fi namely reduce all the exponents from di to after every operation in effect this ensures that the secret key is related to the number of users n involved in the computation and not to the function being computed with the use of relinearization one can show that the scheme is multikey somewhat homomorphic i e we associate zq with the set bq bq throughout this work the basic idea of using homomorphic encryption schemes in conjunction with threshold decryption to boost the efficiency of mpc protocols was first noticed by cramer da mgard and nielsen the idea of using a cloud to alleviate the computational efforts of parties was recently explored in the work on server aided mpc by kamara mohassel and raykova their protocols however require some of the parties to do a large amount of computation essentially proportional to the size of the function f being computed halevi lindell and pinkas recently considered the model of secure computation on the web wherein the goal is to minimize interaction between the parties while their definition requires absolutely no interaction among the participants in the protocols the participants interact with the server only they show that this notion can only be achieved for a small class of functions our goal on the other hand is to compute arbitary functions with the assistance of a cloud organization in section we formally define multikey fhe and on thefly mpc and show our construction of on the fly mpc from multikey fhe in section we show how to instantiate multikey somewhat homomorphic encryption from the ntru encryption scheme and then show how to achieve full homomorphism in section notation in the remainder of the paper we use the following notation we use κ to denote the security parameter for an integer n we use the notation n to denote the set n n for a randomized function f we write f x r to denote the unique output of f on input x with random coins r we write f x to denote a random variable for the output of f x r over uniformly random coins r for a distribution or random variable x we write x x to denote the operation of sampling a random x according to x for a set s we overload notation and use s to denote sampling from the uniform distribution over s we use y f x to denote the deterministic evaluation of f on input x with c output y for two distributions d and d denotes computational indistinguishability on the fly mpc from multikey fhe we consider the problem of a server or cloud denoted by s storing the data of u different parties pu we wish to ensure that the data of each party is kept private but also allow the server s to compute any joint function of the data of any subset v u of the parties we also wish to ensure that the server is able to do this with minimal participation from the parties in v and no interaction at all from the rest of the parties u v furthermore the communication complexity and the computation time of each party pi should be independent of the complexity of the function since we rely on the computation power of the server who will carry out the entire computation of the joint function f the computation should remain secure even if the server or any set of parties are corrupted we formalize this below for a class c of functions with at most u inputs an onthe fly multiparty protocol π for c is a protocol between u interactive turing machines pu s such that for all inputs x xu all functions f c if f is an n input function then for all ordered subsets v u such that v n the output of π in an execution where pi is given xi as input s does not receive an input and f v are chosen for the computation is y f xi i v an onthe fly multiparty protocol consists of two phases an offline phase that is performed before the function f c is chosen and an online phase that begins once f is chosen together with a subset v of inputs on which f will be evaluated all parties pu s participate in the offline phase but only the server s and parties in v participate in the online phase after the function is selected the server ignores all offline messages from non computing parties i e those in u v unlike in standard mpc we require the communication complexity of the protocol as well as the computation time of parties pu to be independent of the complexity of the function f furthermore we let the computation time of parties pi for i v depend on the party input and the output size of the f but require the computation time of parties pi for i u v to depend only on the size of the party input and be independent of the output size of f on the other hand the computation time of the server s must be linear in the circuit size of f security we prove security of an on the fly mpc protocol in the ideal real paradigm let f xi i v for v u be the function to be computed and let n v for ease of notation we assume w l o g that v n in the ideal world the computation of f is performed through a trusted functionality f that receives input xi from each party pi for i u computes y f xn ignoring all inputs xi for i v and gives y to parties pn s while parties pi for i u v do not get an output thus in the ideal world parties learn nothing more than y in the real world however this trusted functionality does not exist and so in order to compute y f xn parties pu s run a protocol π an adversary corrupting a party resp the server receives all messages directed to the corrupted party resp the server and controls the messages that it sends since the server ignores messages from parties outside v we assume w l o g that an adversary only corrupts computing parties i e parties in v and possibly the server we use idealf s x to denote the joint output of the ideal world adversary s and the outputs of the server s and the parties pn in an ideal execution with functionality f and inputs x xu similarly we use realπ a x to denote the joint output of the real world adversary a and the outputs of parties pn and server s in an execution of protocol π with inputs x xu we say that a protocol π securely realizes f if for every real world adversary a corrupting any t n parties and possibly the server there exists an ideal world adversary s with black box access to a such that for all input vectors x c idealf s x realπ a x multikey fully homomorphic encryption in this section we define multikey fully homomorphic encryption intuitively multikey fhe allows us to evaluate any circuit on ciphertexts that might be encrypted under different public keys to guarantee semantic security decryption requires all of the corresponding secret keys we introduce a parameter n which is the number of distinct keys that a scheme can tolerate we let all algorithms depend polynomially on n this is similar to the definition of leveled fhe from however we note that in our definition the algorithms depend on n but are independent of the depth of circuits that the scheme can evaluate thus we consider schemes that are leveled with respect to the number of keys n but fully homomorphic non leveled with respect to the circuits that are evaluated the construction of multikey fhe schemes that are not leveled with respect to the number of keys i e where all algorithms are independent of n remains an open problem we now define multikey fhe as follows for restricted circuit classes and for arbitrary circuits definition multikey c homomorphic encryption let c be a class of circuits a family e n keygen enc dec eval n of algorithms is a multikey c homomor phic encryption scheme family if for all integers n e n has the following properties step for i u party pi samples a key tuple pki ski eki keygen κ pk sk ek keygen for a security parameter κ outputs a public key pk a secret key sk and a public evaluation key ek c enc pk m given a public key sk and message m outputs a ciphertext c dec skn c given n secret keys ski and a ciphertext c outputs a message c eval c ct pkt ekt given a description of a boolean circuit c along with t tuples ci pki eki each comprising of a ciphertext ci a public key pki and an evaluation key eki outputs a ciphertext c we require absence of decryption failures and compactness of ciphertexts formally for every circuit c c all sequences of n key tuples j n each of which is in the support of keygen all sequences of t key tuples pki ski eki i t each of which is in j n and all plaintexts mt and ciphertexts ct such that ci is in the support of enc pki mi eval satisfies the following properties correctness let c eval c ct pkt ekt then dec c c mt and encrypts its input xi under pki ci enc pki xi it sends pki eki ci to the server s at this point a function f represented as a circuit c has been selected on inputs xi i v for some v u let n v for ease of notation assume w l o g that v n the parties proceed as follows step the server s computes c eval c cn pkn ekn and broadcasts c to parties pn step the parties pn run a secure mpc protocol πdec sh to compute dec skn c theorem let e n keygen enc dec eval n be a multikey fully homomorphic encryption scheme and let πdec sh be an n party mpc protocol for computing the decryption function dec skn c if e is semantically secure and πdec sh is secure against semi honest adversaries corrupting t n parties then the above construction is an on the fly mpc protocol secure against semi honest adversaries corrupting t parties and possibly the server s compactness let c eval c ct pkt ekt there exists a polynomial p such that c p κ n in other words the size of c is independent of t and c note however that we allow the evaluated ciphertext to depend on the number of keys n definition multikey fhe a family of encryption schemes e n keygen enc dec eval n is multikey fully homomorphic if it is multikey c homomorphic for the class c of all circuits the protocol described in section is not secure against malicious adversaries our first step in handling this type of attack is to replace our decryption protocol with one that is secure against malicious adversaries which we will denote πdec mal next we will apply general mpc techniques to the rest of our protocol steps and to ensure parties do not deviate from the protocol this requires coin flipping and zero knowledge proofs however there are two subtleties to consider first recall that in our model parties do not communicate with each other until the decryption phase in particular parties do not communicate with each other or even know about the existance of other parties during step therefore coin flipping in step is out of the question fortunately the correctness property from definition guarantees that the scheme is secure against corrupt parties that follow the protocol in step but adaptively choose their random coins this means that parties do not need to coinflip for each other random coins furthermore since the server computation throughout the protocol is deterministic the parties do not need to coin flip for the server random coins semantic security of a multikey fhe follows directly from the semantic security of the underlying encryption scheme in the presence of the evaluation key ek this is because given ek the adversary can compute eval himself note that taking n in definition and definition yield the standard definitions of c homomorphic and fully homomorphic encryption schemes the basic protocol let e n keygen enc dec eval n be a multikey fully homomorphic family of encryption schemes we construct the following on the fly mpc protocol πsh secure against semi honest adversaries we therefore only need to add zero knowledge proofs of to ensure that the parties indeed follow note that correctness still holds even if the circuit c completely ignores all ciphertexts encrypted under a public key pki or if none of the original ciphertexts were encrypted under this key in other words using superfluous keys in the decryption process does not affect its correctness as long as decryption uses at most n keys security against malicious adversaries there is a subtely here we assume a rushing adversary that is we assume that the adversary can choose his messages adaptively depending on the messages from the honest players because of this in the proof the simulator will have to provide simulated proofs for the honest parties and the protocol the intuition behind this is that correctness will guarantee that the simulator can extract the input x i for a corrupted party and therefore obtain the correct value y from the ideal functionality regardless of the coins used by the adversary second we wish to ensure that the computation time and communication complexity of the parties is small this means that the server must be able to prove that he carried out the computation of eval correctly in such a way that the parties can verify the validity of the proof in time that is much less than linear in the circuit size to solve this problem we use techniques from verifiable computation we offer several solutions each with its own benefits and drawbacks verification for small inputs we first consider the case where the ciphertexts cn are small enough to be broadcast to the n parties in v i e allowing communication complexity linear in the total input size of the participating parties in this case the server needs to convince the participating parties that c eval c cn pkn ekn i e that a deterministic circuit of size poly c κ accepts for any uniform circuit c i e computable by a poly κ time turing machine the following offer poly κ log c communcation and verification efficiency use the argument system of kilian yielding interactive round verification it relies on expensive pcps use micali cs proofs this reduces interaction to one round but assumes a random oracle it also relies on expensive pcps use the succint non interactive arguments snargs and snarks of bitansky et al or goldwasser at al these are round and hold in the standard model but require a non falsifiable assumption some variants rely on pcps pir or fhe collision resistant hash digest d is computed by a trusted party or via an mpc protocol then in step the server proves the np statement e whose digest is d and there exists a circuit c e pk cn pk ekn c eval c n this requires a succint argument system that is proof of knowledge and supports nondeterministic uniform circuits this is satisfied by micali construction of cs proofs under valiant analysis and by snarks verification for large inputs we can make communication and verification complexities depend merely polylogarithmically on the size of the relevant inputs xn in the aforementioned proofs of knowledge for nondeterministic statements the complexity depends polynomially on the size of statement being proven expressed as a nondeterministic turing machine and its input but merely polylogarithmically on the size of the witness for the statement and in particular the nondeterministic choices made by the turing machine we thus move ci from the instance into the witness to recognize the correct ci each party pi remembers the digest of ci under a collision resistant hash function family h hhk κ in the offline stage every party pi draws hash key hki computes the digest di hhki ci and sends ci hki di to the cloud each party pi remembers its own hki di but can forget the potentially long xi and ci in the online stage the server broadcasts hkn dn and proves the following np statement there exist c c n such that di hhki c i and c eval c c c n pkn ekn this is secure since whenever the server convinces the clients it actually knows such c c n which can be efficiently extracted from the server by the arguments proof of knowledge property and these extracted c i must be the ones originally sent by the parties by the collision resistance of h protocol for malicious adversaries in case that the evaluation circuit is in logspace uniform nc we have another alternative use the argument system of goldwasser et al for a round solution it relies on pir in the case of arbitrary nonuniform poly κ size circuits one can use the technique of section first in a preparatory phase the circuit c is written down and its still be able to extract from the proofs created by the adverary on behalf of corrupt players we therefore need to use simulation extractable zk proofs se zk instead of ordinary zk proofs of knowledge zk poks however in the interest of clarity we choose to present the construction above with zk poks instead of se zk for any given family of c c poly κ and thus poly κ log c poly κ but the degree of this polynomial depends on the circuit family a non falsifiable assumption is necessary for the argument system to be non interactive and secure in the standard model note that we indeed require adaptive security since the prover ie the server is free to choose the statement to be proven ie the function to be computed the protocol for fully malicious adversaries is given below let e n keygen enc dec eval n be a multikey fully homomorphic family of encryption schemes and let h hhk κ be a family of collisionresistant hash functions the following construction is an on the fly mpc protocol πmal secure against malicious adversaries step for i u party pi samples a key tuple pki ski eki keygen ri and encrypts its input xi under pki ci enc pki xi si it also samples a hash key hki and computes the digest of the ciphertext di hhki ci it computes zero knowledge proofs of knowledge πigen πienc showing it computed these steps correctly the proofs πigen πienc attest to these relations preliminaries for the ntru instantiation rgen pki eki ski ri pki ski eki keygen ri renc pki hki di xi si ci ci enc pki xi si and di hhki ci party pi sends the tuple pki eki ci hki di πigen πienc to the server s the server verifies all proofs πigen πienc i u from this point forward party pi can forget its input xi and the ciphertext ci it need only remember the hash key hki and digest di a function f represented as a circuit c is now selected on inputs xi i v for some v u let n v for ease of notation we assume w l o g that v n step the server s computes c eval c cn pkn ekn we work over rings r z x hφ x i and rq r qr for some degree n integer polynomial φ x z x and a prime integer q z note that rq zq x hφ x i i e the ring of degree n polynomials modulo φ x with coefficients in zq addition in these rings is done component wise in their coefficients thus their additive group is isomorphic to zn and zn q respectively multiplication is simply polynomial multiplication modulo φ x and also q in the case of the ring rq thus an element in r or rq can be viewed as a degree n polynomial over z or zq we represent such an element the vector of its n coefficients each in the using range for an element a x x an xn r we let kak max ai denote its norm in this work we set φ x xn to be the nth cyclotomic polynomial where n is a power of two we use distributions over the ring z x hxn to show the homomorphic properties of the scheme the only property of these distributions we use is the magnitude of the polynomials coefficients hence we define a b bounded distribution to be a distribution over r where the norm of a sample is bounded and a short argument ϕ proving that definition b bounded polynomial a polynomial e r is called b bounded if kek b c c n t di hhki c i and c eval c c c n pkn ekn it broadcasts c ϕ to parties pn together with pki eki hki di πigen πienc i n step the parties pn verify the argument ϕ and all proofs πigen πienc i n the parties run an mpc protocol πdec mal to compute dec skn c theorem let e n keygen enc dec eval n be a multikey fully homomorphic encryption scheme and let πdec mal be an n party mpc protocol for computing the decryption function dec skn c let h hhk κ be a family of collision resistant hash functions if e is semantically secure and πdec mal is secure against malicious adversaries corrupting t n parties then the above construction is an on the fly mpc protocol secure against malicious adversaries corrupting t parties and possibly the server s definition b bounded distribution a distribution ensemble χn n n supported over r is called bbounded for b b n if for all e in the suport of χn we have kek b in other words a b bounded distribution over r outputs a b bounded polynomial we present some elementary facts about the gaussian distribution and multiplication over the ring z x hxn the first fact shows that the discrete gaussian distribution over zn with standard deviation r denoted by dzn r outputs a r n bounded polynomial with high probability this allows us to define a truncated gaussian distribution that is r n bounded and statistically close to the discrete gaussian the second says that multiplication in the ring z x hxn increases the norm of the constituent elements only by a small amount lemma see theorem let n n for any real number r ω log n we have pr x r n n x dzn r multikey somewhat homomorphic encryption based on ntru we show how to construct a multikey somewhat homomorphic encryption scheme based on the ntru encryption system first proposed by hoffstein pipher and silverman more precisely we rely on a variant of the ntru scheme proposed by stehle and steinfeld in section we first review definitions and facts from the literature that we use extensively in section we describe the encryption scheme in section we discuss its security and in section show that it is multikey somewhat homomorphic define the truncated discrete gaussian distribution to be one that samples a polynomial according to the discrete gaussian dzn r and repeat the sampling if the polynomial is not r n bounded by lemma this distribution is statistically close to the discrete gaussian lemma see let n n let φ x xn and let r z x hφ x i for any t r t mod φ x n t t mod φ x n t lemma yields the following corollary corollary let n n let φ x xn and r z x hφ x i let χ be a b bounded distribution over the q ring r and let sk χ then ki si is nk b k bounded the ring lwe assumption we describe a special case of the ring learning with errors assumption of lyubaskevsky peikert and regev which we will call rlwe the rlwe assumption is analogous to the standard learning with errors lwe assumption first defined by regev generalizing the learning parity with noise assumption of blum et al the rlweφ q χ assumption is that for a random ring element rq given any polynomial number of samples of the form ai bi ai ei rq where ai is uniformly random in rq and ei is drawn from the error distribution χ the bi are computationally indistinguishable from uniform in rq we use the hermite normal form of the assumption as in where the secret is sampled from the noise distribution χ rather than being uniform in rq this presentation is more useful for the purposes of this paper and it turns out to be equivalent to the original one up to obtaining one additional sample definition the rlwe assumption hermite normal form for all κ n let φ x φκ x z x be a polynomial of degree n n κ let q q κ z be a prime integer let the ring r z x hφ x i and rq r qr and let χ denote a distribution over the ring r the decisional ring lwe assumption rlweφ q χ states that for any poly κ it holds that c ai ai ei i ai ui i the worst case to average case connection we state a worst case to average case reduction from the shortest vector problem on ideal lattices to the rlwe problem for our setting of parameters the reduction stated below is a special case of the results of theorem a special case of let φn x xn be the nth cyclotomic polynomial where n is a power of two let r ω log n be a real number and let q mod be a prime integer let r z x hxn then there is a randomized reduction from log n q r approximate r svp to rlweφ q χ where χ dzn r is the discrete gaussian distribution the reduction runs in time poly n q we describe the ntru encryption scheme with the modifications proposed by stehle and steinfeld for a security parameter κ the scheme is parametrized by the following quantities an integer n n κ a prime number q q κ a degree n polynomial φ x φκ x a b κ bounded error distribution χ χ κ over the ring r z x hφ x i the parameters n q φ x and χ are public and we assume that given κ there are polynomial time algorithms that output n q and φ x and sample from the error distribution χ the message space is m and all operations on ciphertexts are carried out in the ring rq i e modulo q and φ x keygen sample polynomials f g χ and set f so that f mod if f is not invertible in rq resample f set where is sampled from the noise distribution χ ai are uniform in rq the error polynomials ei are sampled from the error distribution χ and finally the ring elements ui are uniformly random over rq pk h rq rlweφ q χ we use to denote the assumption when the number of samples is fixed where all operations are done modulo q and φ x c dec sk c let µ f c rq output µ mod where ai ei and ui are as in definition we set φ x to be the nth cyclotomic polynomial this implies that φ x xn the error distribution χ is the truncated discrete gaussian distribution dzn r for standard deviation r a sample from this distribution is a r n bounded polynomial e r sk f r c hs m rq ai ai ei i ai ui i as already stated above we will rely of the following specific choices of the polynomial φ x and the error distribution χ for security parameter κ and a dimension parameter n n κ which is a power of two enc pk m sample polynomials e χ output the ciphertext fact the rlwe φ q χ assumption implies that choice of parameters the scheme it is easily seen that this scheme is correct as long as there is no wrap around modulo q to decrypt a ciphertext c we compute in rq f c f hs e f m e f m if there is no wrap around modulo q then f c mod e f m mod f m mod m one possible setting which ensures that there is no wraparound modulo q is to set φ x xn to see why this helps notice that since the coefficients of g f e are all bounded by by corollary we know that the in fact the coefficients of g and e are bounded by b and that of f is bounded by coefficients of gs mod xn and f e mod xn are both bounded by n thus the coefficients of f c are bounded by b q in other words as long as we set q a fresh ciphertext generated by enc is guaranteed to decrypt correctly from here on we refer to µ f c rq as the error in a ciphertext c using the same security proof as in we can base the security of the scheme in section on the dspr assumption and the rlwe assumption with the choice of parameters stated below this yields security under the dspr assumption and the hardness of approximating shortest vec tors on ideal lattices to within a factor of which is believed to be hard for the rest of our exposition we will use φ x xn as our modulus polynomial lemma let n be a power of two let φ x xn let q for and χ dzn r with r poly n then the modified ntru encryption scheme described in section is secure under the dsprφ q χ assumption and the worst case hardness of approximating shortest vectors on ideal lattices over the ring r z x hφ x i to within a ω n factor of security we base the security of the encryption scheme in section on two assumptions the rlwe assumption described in section as well as a new assumption that we call the decisional small polynomial ratio dspr assumption towards this end we define the following problem multikey homomorphism a polynomial h g f where f and g are sampled from the distribution χ conditioned on f being invertible over rq r qr and let and be two different public secret key pairs and let and be two ciphertexts encrypted under public keys and respectively we show how to compute ciphertexts that decrypt to the sum and the product of and in particular we show that the ciphertexts cmult and cadd can be decrypted to the product and the sum of and respectively using the secret key to see this note that as long as there is no wrap around modulo q a polynomial h sampled uniformly at random over rq mod definition decisional small polynomial ratio dsprφ q χ problem let φ x z x be a polynomial of degree n let q z be a prime integer and let χ denote a distribution over the ring r z x hφ x i the decisional small polynomial ratio problem dsprφ q χ is to distinguish between the following two distributions stehle and steinfeld have shown that the dsprφ q χ problem is hard even for unbounded adversaries namely the two distributions above are statistically close if n is a power of two φ x xn is the nth cyclotomic poly nomial and χ is the discrete gaussian dzn r for r q poly n this allowed them to prove semantic security for the modified ntru scheme described in section under the rlweφ q χ assumption alone the security proof follows by a hybrid argument in two steps the hardness of dsprφ q χ allows us to change the public key h f to h for a uniformly sampled once this is done we can use rlweφ q χ to change the challenge ciphertext c hs m to c u m where u is uniformly sampled from rq in this final hybrid the advantage of the adversary is exactly since c is uniform over rq independent of the message m unfortunately their result holds only if r q poly n which is too large to permit even a single homomorphic multiplication to support homomorphic operations we need to use a much smaller value of r for which it is easy to see that the dsprφ q χ assumption does not hold in a statistical sense any more this makes it necessary for us to assume that the decisional small polynomial ratios problem is hard for our choice of parameters which we refer to as the dsprφ q χ assumption mod mod and mod s1 2e1 2e1 mod mod since mod and f2 mod in other words the joint secret key f2 can be used to decrypt cadd and cmult we can extend this argument to any combination of operations with ciphertexts encrypted under multiple public keys of course the error in the ciphertexts will grow with the number of operations performed as with all known fully homomorphic encryption schemes thus correctness of decryption will only hold for a limited number of operations we can show that the scheme can correctly evaluate circuits of depth roughly log n when q and b poly n however an astute reader would have observed by now that in order to successfully decrypt a ciphertext that was the result of a homomorphic evaluation we must know the circuit that was evaluated for example to decrypt we need to multiply it by f2 whereas to decrypt we need to multiply by f22 this is obviously unsatisfactory furthermore consider what happens when we add or multiply two ciphertexts c that are themselves a result of homomorphic evaluation suppose for example that c and where ci is encrypted under hi for i we know c can be decrypted using f2 and can be decrypted using f2 thus we know that eval c ct pkt ekt we show how to evaluate a t input circuit c to this end we show how to homomorphically add and multiply two elements in given two ciphertexts we assume that we also have a set of distinct public keys associated with each ciphertext we will denote these lists by respectively the public key set of a fresh encryption is simply the set pk containing the public key under which it was encrypted f2 c m f2 for some messages m and and error terms e and following the discussion above we can see that c can be decrypted using the key f2 in general given a ciphertext c encrypted under a set of keys k and encrypted under a set of keys k their sum can be decrypted using the product of the keys in k k the absolute magnitude of the entries in this product grows exponentially with the number of keys in k k analogously in the context of homomorphic multiplication we need f22 to decrypt c this hints at the fact that the size of the joint secret key needed to decrypt an evaluated ciphertext grows exponentially with the degree of the evaluated circuit and not just with the number of parties involved as in the case of addition indeed after d multiplications the joint secret key needed to decrypt will be the product of d polynomials and the magnitude of the coefficients in this product will be exponential in d it is beneficial especially for our constructions in section that we eliminate the exponential dependence of the magnitude of the coefficients of the joint secret key on the degree d we remark that we will not succeed in eliminating the exponential dependence on n the number of users indeed this is the reason why our solution will eventually assume an a priori upper bound on n this motivates our use of relinearization a technique first introduced by brakerski and vaikuntanathan informally we will introduce a public evaluation key ek that will be output by the keygen algorithm every time we multiply ciphertexts that share a key fi we will use the evaluation key to ensure that we only need fi and not to decrypt the new ciphertext this ensures two things given two ciphertexts and with corresponding public key sets and output the ciphertext cadd rq as an encryption of the sum of the underlying messages output the set kadd as its corresponding public key set given two ciphertexts and with corresponding public key sets and compute ciphertext e rq and let pkir if let cmult e otherwise for j r and τ blog qc define e cj τ so that blog qc e cj we present below our modified scheme as well as the eval algorithm keygen sample f g χ and set f so that f mod if f is not invertible in rq resample f set pk h rq sk f r e cj τ τ is the binary representation of e cj parse ekij γij γij blog qc and let blog qc e cj x e cj τ γij τ τ at the end of the iteration let cmult e cr first it ensures that to decrypt a ciphertext c we only need to multiply it by one copy of each secret key making decryption independent of the circuit used to produce c second it ensures that the size of the joint secret key needed to decrypt depends only on the number of keys n and not on the circuit c that was evaluated x in either case output cmult as an encryption of the product of the underlying messages and output the set kmult as its corresponding public key set we first show that the scheme works correctly as advertised lemma if q for and χ is a bbounded distribution for b poly n then the modified ntru encryption scheme described above is multikey homomorphic for n o nδ keys and circuits of depth d δ log n log log n o enc pk m sample e χ output the ciphertext c hs m rq proof the main step in the proof of correctness is to show that a homomorphic operation increases the error from η to at most η b poly n o n putting this together with the fact that successful decryption requires the error to be smaller than q gives us the statement of the lemma we first compute how much the error in a ciphertext grows with a homomorphic multiplication let c and be ciphertexts that decrypt to m and under two sets of secret dec skn c parse ski fi for i n let µ fn c rq output µ mod that is we assume each set contains distinct public keys but the intersection of any two sets might not be empty for all τ blog qc sample sτ eτ χ and compute γτ hsτ f rq set ek γblog qc rqdlog qe keys k and q k respectively q let fk resp fk denote the product i k fi resp i k fi then we have has norm at most nb by corollary thus the error is at most errorj n nb blog qc b errorj fk c m log q nb ηη j log q nb fk ηη j log q nb where m η and η letting cmult we have fk fk cmult m thus the error in the ciphertext cmult is at most ηη and it decrypts to the product of the two messages as long as the error is not too large we now move to analyzing the additional error introduced by relinearization let k k ir then the joint decryption key fk fk contains the term the goal of relinearization is to convert cmult into a ciphertext that decrypts to the same message under the secret key y fk fk fj j k k which replaces the term with the term fir let fk fk and let fj fj fij for j r thus fr is a simple product of the secret keys fi without any quadratic terms we show that the ciphertext e cj decrypts to the message and has error at most ηη j log q nb the base case i e j follows from equation since e cmult in general we have fj e cj fij fj fij fij e cj fj fij fij e cj blog xqc fj fij e cj τ fij γij τ τ now fij γij τ gij sij τ fij eij τ where eτ substituting back into equation we get fj e cj fj fij blog xqc thus the final error after a multiplication and relinearization is at most ηη log q nb ηη b n o n as claimed for the setting of q for a circuit of depth d and an initial error the error grows to at most d b n after homomorphic evaluation this results in correct decryption as long as d log log q log log n log n o in particular for n o nδ keys and q we get d δ log n log log n o the main difference between the scheme in section and the one in this section in terms of security is in the evaluation key ek the evaluation key contains the elements γτ hsτ f which can be thought of as pseudo encryptions of multiples of the secret key f under the corresponding public key h we point out that these are not true encryptions of the message f since f is not a binary polynomial whereas our scheme is only equipped to correctly encrypt polynomials m x hφ x i the security of the scheme then relies on a circular security assumption which states that semantic security of the scheme is maintained given the evaluation key as constructed above we remark that this assumption can be avoided at the cost of obtaining a leveled homomorphic encryption scheme where the size of the evaluation key grows with the depth of the circuits that the scheme supports in section we show how to convert this somewhat homomorphic scheme into a multi key fully homomorphic one in the same section we also discuss any additional assumptions we need to make to maintain security o n e cj τ fij γij τ from somewhat to fully homomorphic encryption τ blog qc fj fi j x we use gentry bootstrapping theorem to convert a multikey somewhat homomorphic scheme into a multikey fully homomorphic one unfortunately as we will see we cannot apply the bootstrapping theorem directly to the somewhat homomorphic encryption scheme from section we therefore turn to modulus reduction a technique introduced by to convert our somewhat homomorphic scheme into a bootstrappable scheme we first describe the bootstrapping theorem and then present the modulus reduction technique and how we can apply it in our case e cj τ eτ τ blog qc fj x e cj τ τ blog qc x fj fi e c e j τ τ j τ fj e cj since fj e cj is exactly plus an even error by the inductive assumption this shows that e cj decrypts to as well under the secret key fj it remains to compute the error in the ciphertext e cj since fj fi is a product of at most small polynomials it j bootstrapping we remind the reader of the definition of a bootstrappable encryption scheme and present gentry bootstrapping theorem that states that a bootstrappable scheme can be converted into a fully homomorphic one we modify the theorem and the corresponding definitions from their original presentation to generalize it to the multikey setting taking n yields the theorem and the definitions from definition bootstrappable scheme let e e n keygen enc dec eval n be a family of multikey c homomorphic encryption schemes and let fadd and fmult be the the augmented decryption functions of the scheme defined as fadd skn dec skn xor dec skn fmult skn dec skn and dec skn then e is bootstrappable if fadd fmult c namely the scheme can homomorphically evaluate fadd and fmult modulus reduction is a noise management technique that provides an exponential gain on the depth of the circuits that can be evaluated while keeping the depth of the decryption circuit unchanged informally if ddec is the depth of the decryption circuit of the multikey scheme described in section then we modify the scheme so that its decryption circuit is unchanged but the scheme can now evaluate circuits of depth ddec hence the new scheme can evaluate its own decryption circuit making it bootstrappable details follow we let q denote the corresponding element in rq ie reducing modulo φ x and q as in then if h f is a key pair and c is a ciphertext under public key h f c q corresponds to the noise in c recall that for decryption to be successful we need f c q to be equal to f c over the integers however each homomorphic operation increases this noise modulus reduction allows us to keep the noise magnitude small by simply scaling the ciphertext after each operation the key idea is to exploit the difference in how the noise affects security and homomorphisms we first define the notion of weak circular security and then describe our generalization of gentry bootstrapping theorem definition weak circular security a public key encryption scheme gen enc dec is weakly circular secure if it is ind cpa secure even for an adversary with auxiliary information containing encryptions of all secret key bits enc pk sk i i namely no polynomial time adversary can distinguish an encryption of from an encryption of even given the additional information we now state a generalization of gentry bootstrapping theorem to the multi key setting theorem multikey bootstrapping theorem let e be a bootstrappable family of multikey homomorphic schemes that is also weakly circular secure then there is a multikey fully homomorphic family e of encryption schemes the idea behind multi key bootstrapping is that given a public evaluation key that consists of encryptions of all bits of all secret keys αj i enc pkj skj i we can evaluate circuits of any depth by periodically refreshing the evaluated ciphertext c simply evaluate the decryption circuit dec skn c homomorphically using the evaluation key αj i enc pkj skj i the result is a ciphertext c that encrypts the same plaintext as c and can again be decrypted using skn but has much smaller noise thus after this refreshing step we can continue evaluating homomorphically for a few more levels before the noise grows again and we need to apply the refreshing procedure once more unfortunately the somewhat homomorphic scheme that we described in section is not bootstrappable recall that we can only evaluate circuits of depth less than log n where however the shallowest implementation of the decryption circuit we are aware of has depth c log n log n for some constant c we therefore turn to modulus reduction which will allow us to convert our somewhat homomorphic encryption scheme into a bootstrappable scheme modulus reduction the growth of noise upon homomorphic multiplication is governed by the magnitude of the noise security is governed by the ratio between the magnitude of the noise and the modulus q this suggests a method of reducing the magnitude of the noise and the modulus by roughly the same factor thus preserving security while at the same time making homomorphic multiplications easier in particular modulus reduction gives us a way to transform a ciphertext c rq into a different ciphertext rp where p is smaller than q while preserving correctness for q joint secret key f n i fi f c p f q mod the transformation from c to involves simply scaling by p q and rounding appropriately lemma let p and q be two odd moduli and let c rq define to be the polynomial in rp closest to p q c such that c mod then for any f with k f c q k q q p kf we have f p f c q mod and f p p q k f c q k kf k where kf k and kf are the and norms of f respectively the beauty of lemma is that if we know the depth d of the circuit we want to evaluate in our case d ddec the depth of the augmented decryption circuit then we can construct a ladder of decreasing moduli qd and perform modulus reduction after each operation so that at level i all ciphertexts reside in rqi and the magnitude of the noise at each level is small in particular this is true at level d so that once the evaluation is complete it is possible to decrypt the resulting ciphertext without decryption errors bootstrappable scheme we change the scheme so that it uses modulus reduction during the evaluation keygen will now sample a ladder of decreasing moduli qddec we will choose the distribution χ in order to guarantee that any sample is b bounded where b qddec the modified scheme is as below keygen for every i ddec sample g i u i χ and set f i i so that f i mod if f i is not invertible in rqi resample u i let h i i f i rqi and set pk given two ciphertexts rqi with corresponding public key sets compute ciphertext e rqi and let pkjr for r and τ blog qi c define e c τ so that blog qi c sk f ddec rqddec finally reduce the modulus let cadd be the integer vector closest to qi qi e cr such that cadd e cr mod output cadd rqi as an encryption of the sum of the underlying messages output the set kadd as its corresponding public key set e c for all i ddec and τ blog qi c sample i i sτ eτ χ and compute x e c τ τ is the binary representation of e c parse i i ekj γj τ ζj τ i ddec τ blog qi c i τ i γτ i h i i rqi τ f if pkj let i τ i rqi ζτ i h i i τ f blog qi c set ek x e c γτ i ζτ i i ddec τ blog qi c i e c ζj τ rq i τ otherwise let enc pk m sample e χ output the ciphertext c hs m dec skn c assume w l o g that c rqddec parse ski fi for i n let µ fn c rqddec output µ mod eval c ct pkt ekt we show how to evaluate a t input circuit c we assume w l o g that the circuit c is leveled in that it is composed of alternating xor and and levels we show how to homomorphically add and multiply two elements in given two ciphertexts we assume that we also have a set of distinct public keys associated with each ciphertext we will denote these lists by respectively the public key set of a fresh encryption is simply the set pk containing the public key under which it was encrypted given two ciphertexts rqi with corresponding public key sets compute c rqi and let pkjr for r and τ blog qi c define e c τ so that blog qi c x e c i e c γj τ rqi τ finally reduce the modulus let cmult be the integer vector closest to qi qi e cr such that cmult e cr mod output cmult rqi as an encryption of the product of the underlying messages and output the set kmult as its corresponding public key set we can now show the following lemma whose proof is deferred to the full version lemma if q for and χ is a bbounded distribution for b poly n then the modified ntru encryption scheme described above is multikey homomorphic for n keys and circuits of depth d as long as n d o n log n multikey fully homomorphic encryption to turn this into a fully homomorphic encryption scheme we use the multi key bootstrapping theorem theorem but first we show an upper bound on the depth of the decryption circuit blog qi c e c x e c τ lemma the decryption circuit for the scheme above for n keys can be implemented as a polynomial size arithmetic circuit over gf of depth o log n log log q log n τ is the binary representation of e c parse i proof the decryption circuit for a ciphertext encrypted under n keys can be written as i ekj γj τ ζj τ i ddec τ blog qi c let fn c c blog qi c e c x i e c γj τ rqi n y fi i τ that is we assume each set contains distinct public keys but the intersection of any two sets might not be empty multiplying two polynomials over rq can be done using a polynomial size boolean circuit of depth o log log q log n see e g lemma for a proof using a binary tree of polynomial multiplications the decryption operation above can then be done in depth o log n log log q log n as claimed this means that the modified scheme is bootstrappable and therefore applying the bootstrapping theorem gives us theorem for every n n let b poly n χ to be a b bounded distribution and q n log n log n then there exists a multikey fully homomorphic encryption scheme for n keys secure under the dsprφ q χ and rlweφ q χ assumptions proof to apply theorem we require that the depth of the decryption circuit is smaller than the depth of the circuits that the scheme can evaluate that is log n log log q log n c log q n log n for some universal constant c which holds for the parameter settings in the statement of the theorem in particular this scheme maintains security as long as q δ for some δ thus supporting as many as n δ n logo n users finally we remark that bootstrapping and therefore assuming weak circular security can be avoided at the cost of obtaining a leveled homomorphic encryption scheme where the size of the evaluation key grows with the depth of the circuits that the scheme supports abstract we develop an automated approach for designing matrix multiplication algorithms based on constructions similar to the coppersmith winograd construction using this approach we obtain a new improved bound on the matrix multiplication exponent ω categories and subject descriptors f analysis of algorithms and problem complexity nonnumerical algorithms and problems general terms algorithms theory keywords matrix multiplication introduction the product of two matrices is one of the most basic operations in mathematics and computer science many other essential matrix operations can be eﬃciently reduced to it such as gaussian elimination lup decomposition the determinant or the inverse of a matrix matrix multiplication is also used as a subroutine in many computational problems that on the face of it have nothing to do with matrices as a small sample illustrating the variety of applications there are faster algorithms relying on matrix multiplication for graph transitive closure see e g context free grammar parsing and even learning juntas until the late it was believed that computing the product c of two n n matrices requires essentially a cubic number of operations as the fastest algorithm known was the naive algorithm which indeed runs in o time in strassen excited the research community by giving the ﬁrst subcubic time algorithm for matrix multiplication running in o time this amazing discovery spawned a long line of research which gradually reduced the matrix multiplication exponent ω over time in pan showed ω the following year bini et al introduced the notion of border rank and obtained ω scho nhage generalized this notion in proved his τ theorem also called the asymptotic sum inequality and showed that ω in the same paper combining his work with ideas by pan he also showed ω the following year romani found that ω the ﬁrst result to break was by coppersmith and winograd who obtained ω in strassen introduced his laser method which allowed for an entirely new attack on the matrix multiplication problem he also decreased the bound to ω three years later coppersmith and winograd combined strassen technique with a novel form of analysis based on large sets avoiding arithmetic progressions and obtained the famous bound of ω which has remained unchanged for more than twenty years in cohn and umans introduced a new grouptheoretic framework for designing and analyzing matrix multiplication algorithms in together with kleinberg and szegedy they obtained several novel matrix multiplication algorithms using the new framework however they were not able to beat many researchers believe that the true value of ω is in fact both coppersmith and winograd and cohn et al presented conjectures which if true would imply ω recently alon shpilka and umans showed that both the coppersmith winograd conjecture and one of the cohn et al conjectures contradict a variant of the widely believed sunﬂower conjecture of erdo and rado nevertheless it could be that at least the remaining cohn et al conjecture could lead to a proof that ω the coppersmith winograd algorithm in this paper we revisit the coppersmith winograd cw approach we give a very brief summary of the approach here we will give a more detailed account in later sections one ﬁrst constructs an algorithm a which given q length vectors x and y for constant q computes q values of the form zk i j tijk xi yj say with tijk using a smaller number of products than would naively be necessary the values zk do not necessarily have to correspond to entries from a matrix product then one considers the algorithm an obtained by applying a to vectors x y of length qn recursively n times as follows one splits x and y into permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee stoc may new york new york usa copyright acm q subvectors of length qn then one runs a on x and y treating them as vectors of length q with entries that are vectors of length qn when the product of two entries is needed one uses an to compute it this algorithm an is called the nth tensor power of a its running time is essentially o rn if r is the number of multiplications performed by a the goal of the approach is to show that for very large n one can set enough variables xi yj zk to so that running an on the resulting vectors x and y actually computes a matrix product that is as n grows some subvectors x of x and y of y can be thought to represent square matrices and when an is run on x and y a subvector of z is actually the matrix product of x and y if an can be used to multiply m m matrices in o rn time then this implies that ω logm rn so that the larger m is the better the bound on ω coppersmith and winograd introduced techniques which when combined with previous techniques by scho nhage and strassen allowed them to eﬀectively choose which variables to set to so that one can compute very large matrix products using an part of their techniques rely on partitioning the index triples i j k q n into groups and analyzing how similar each group g computation zkg i j i j k g tijk xi yj k is to a matrix product the similarity measure used is called the value of the group depending on the underlying algorithm a the partitioning into groups varies and can aﬀect the ﬁnal bound on ω coppersmith and winograd analyzed a particular algorithm a which resulted in ω then they noticed that if one uses as the basic algorithm the base case instead one can obtain the better bound ω they left as an open problem what happens if one uses as the basic algorithm instead rithms satisfying the requirement we hope that some of the new group theoretic techniques can help in this regard why wasn t an improvement on cw found in the after all the cw paper explicitly posed the analysis of the third tensor power as an open problem the answer to this question is twofold firstly several people have attempted to analyze the third tensor power from personal communication with umans kleinberg and coppersmith as the author found out from personal experience analyzing the third tensor power reveals to be very disappointing in fact no improvement whatsoever can be found this ﬁnding led some to believe that may be the ﬁnal answer at least for the cw algorithm the second issue is that with each new tensor power the number of new values that need to be analyzed grows quadratically for the eighth tensor power for instance separate analyses are required prior to our work each of these analyses would require a separate application of the cw techniques it would have required an enormous amount of patience to analyze larger tensor powers and since the third tensor power does not give any improvement the prospects looked bleak stothers work our contribution we give a new way to more tightly analyze the techniques behind the coppersmith winograd cw approach we demonstrate the eﬀectiveness of our new analysis by showing that the tensor power of the cw algorithm in fact gives ω it is likely that higher tensor powers can give tighter estimates and this could be the subject of future work there are two main theorems behind our approach the ﬁrst theorem takes any tensor power an of a basic algorithm a picks a particular group partitioning for an and derives an eﬃcient procedure computing formulas for the values of these groups the second theorem assumes that one knows the values for an and derives an eﬃcient procedure which outputs a nonlinear constraint program on o variables the solution of which gives a bound on ω we then apply the procedures given by the theorems to the second fourth and eighth tensor powers of the coppersmithwinograd algorithm obtaining improved bounds with each new tensor power similar to our proofs apply to any starting algorithm that satisﬁes a simple uniformity requirement which we formalize later the upshot of our approach is that now any such algorithm and its higher tensor powers can be analyzed entirely by computer in fact our analysis of the tensor power of the cw algorithm is done this way the burden is now entirely oﬄoaded to constructing base algo we were recently made aware of the thesis work of a stothers in which he claims an improvement to ω stothers argues that ω by analyzing the tensor power of the coppersmith winograd construction before learning of stothers result we were only able to prove a bound of the form ω as our analysis of some of the values was not tight after seeing a shortcut that stothers employs in his analysis of the values for the tensor power we will point out this shortcut in the main text we were able to tighten the analysis of the values fully automate our approach and improve our bound to ω there are several diﬀerences between our approach and stothers the ﬁrst is relatively minor the cw approach requires the use of some hash functions ours are diﬀerent and simpler than stothers the main diﬀerence is that because of the generality of our analysis we do not need to fully analyze all groups of each tensor power construction instead we can just apply our formulas in a mechanical way stothers on the other hand did a completely separate analysis of each group finally stothers approach only works for tensor powers up to starting with the th tensor power the values of some of the groups begin to depend on more variables and a more careful analysis is needed incidentally we also obtain a better bound from the tensor power ω however this is an artifact of our optimization software as we end up solving essentially the same constraint program preliminaries use notation n n and we the following n n a ai i k k we deﬁne ω to be the inﬁmum over the set of all reals r such that n n matrix multiplication over q can be computed in nr additions and multiplications for some natural number n however the cw approach and our extensions work over any ring a three term arithmetic progression is a sequence of three integers a b c so that b a c b or equivalently a c an arithmetic progression is nontrivial if a b c the following is a theorem by behrend improving on salem and spencer the subset a computed by the theorem is called a salem spencer set now let consider the case k first assume that the bi are sorted so that bi bi since i ai we obtain k n k n n bi bi bi ai i k i a i i k i i k where bi bi j bj we will prove the claim for ai n i bi i k and the lemma will follow for the b as well hence we can i assume that i bi suppose that we have proven the claim for k this means that in particular k n n k n n k bj bj n k aj n j j j theorem there exists an absolute constant c such that for every n exp c2 one can construct in poly n time a subset a n with no three term arithmetic progressions and a n exp c log n the following lemma is needed in our analysis incidentally a similar lemma appears in in a slightly diﬀerent context and the quantity is maximized for aj n n n bj j bj for all j n n n k now consider an b b by our base j j n case we get that this is maximized and is at least kj bj n n for the setting hence we will get k n k k n bj bj n k aj n j k j j lemma let k be a constant and n be suﬃciently large let bi be ﬁxed for i k let ai for i k be variables such that ai and i ai then the quantity k an n b i ai n i k i i is maximized for the choices ai bi kj bj for all i k and for these choices it is at least k n bj n k for the setting and for j aj n n n bj j bj implies aj bj and hence aj bj we have proven the lemma j proof we will prove the lemma by induction on k suppose that k and consider n n an n a n x y x y an y an an a brief summary of the techniques used in bilinear matrix multiplication algorithms a full exposition of the techniques can be found in the book by bu rgisser clausen and shokrollahi the lecture notes by bla ser are also a nice read bilinear algorithms and trilinear forms where x y n when x y the function f a an x y an of a is concave for a hence its maximum is achieved when f a a consider f a it is n an n a x y an we can take the logarithm to obtain ln f a ln n n a ln x y ln an ln n a f a grows exactly when a ln x y ln an n ln n a n does taking stirling approximation we obtain matrix multiplication is an example of a trilinear form n n matrix multiplication for instance can be written as xik ykj zij i j n k n which corresponds to the equalities zij k n xik ykj for all i j n in general a trilinear form has the form i j k tijk xi yj zk where i j k are indices in some range and tijk are the coeﬃcients which deﬁne the trilinear form tijk is also called a tensor the trilinear form for the product of a k m by an m n matrix is denoted by k m n strassen algorithm for matrix multiplication is an example of a bilinear algorithm which computes a trilinear form a bilinear algorithm is equivalent to a representation of a trilinear form of the following form r tijk xi yj zk αλ i xi βλ j yj γλ k zk a ln x y ln an n ln n a n a ln x y a ln a a ln a ln n o log n n since n is large the o log n n term is negligible thus we are interested in when g a a ln x y a ln a a ln a is maximized because of concavity for a and x y the function is maximized when g a a i e when ln x y ln a ln a ln x y ln a a hence a a x y and so a x x y furthermore since the maximum is attained for this value of get that for each t n we have that n a t we n n t n t n n t an n a x x y y and since t t x y t an x y n we obtain that for a x x y n xan y n a x y n n an i j k λ i j k given the above representation the algorithm is then to ﬁrst compute the r products pλ i αλ i xi j βλ j yj and then for each k to compute zk λ γλ k pλ the minimum number of products r in a bilinear construction is called the rank of the trilinear form or its tensor it is known that the rank of matrix multiplication is and hence strassen bilinear algorithm is optimal for the product of matrices a basic property of the rank r of matrix multiplication is that r k m n r k n m r m k n r m n k r n m k r n k m this property holds in fact for any tensor and the tensors obtained by permuting the roles of the x y and z variables any algorithm for n n matrix multiplication can be applied recursively k times to obtain a bilinear algorithm for nk nk matrices for any integer k furthermore one can obtain a bilinear algorithm for by splitting the matrix into blocks of size and the matrix into blocks of size the one can apply a bilinear algorithm for on the matrix with block entries and an algorithm for to multiply the blocks this composition multiplies the ranks and hence r r r m2 because of this r r k and if n r n n n n n hence ω logn r n n n in general if one has a bound r k m n r then one can symmetrize and obtain a bound on r kmn kmn kmn and hence ω logkmn r the above composition of two matrix product trilinear forms to form a new trilinear form is called the tensor product for two generic trilinear of the two forms forms i j k tijk xi yj zk and i j k t ijk xi yj zk their tensor product is the trilinear form tijk t i j k x i i y j j z k k i i j j k k theorem scho nhage τ theorem if for some r q r qi ki mi ni r then let τ be deﬁned as q τ i ki mi ni r then ω coppersmith and winograd s algorithm λ q xi i q yi i q zi i λ qλ xq yq zq a lot of interesting work ensued after strassen discovery bini et al showed that one can extend the form of a bilinear construction to allow the coeﬃcients αλ i βλ j and γλ k to be linear functions of the integer powers of an indeterminate speciﬁcally one considers approximate bilinear algorithms of the form r αλ i xi βλ j yj γλ k zk j the minimum number of products of such a bilinear algorithm is called the border rank r of a trilinear form or its tensor border rank is a stronger notion of rank and it allows for better bounds on ω most of the properties of rank also extend to border rank so that if r k m n r then ω logkmn r for instance bini et al used their construction above to obtain a border rank of for the product of a by a matrix and by symmetry an upper bound of the border rank of for the product of two matrices this gave the new bound of ω scho nhage generalized bini et al approach and proved his τ theorem also known as the asymptotic sum inequality up until his paper all constructions used in designing matrix multiplication alrgorithms explicitly computed a single matrix product trilinear form scho nhage theorem allowed a whole new family of contructions in particular he showed that constructions that are direct sums of rectangular matrix products suﬃce to give a bound on ω i i j k i we recall coppersmith and winograd cw construction q λ λxi λyi λzi i e the new form has variables that are indexed by pairs if indices and the coordinate tensors are multiplied the direct sum of two trilinear forms is just their sum but where the variable sets that they use are disjoint that is the direct sum of i j k tijk xi yj zk and t x y z is a new trilinear form with the set of varii j k i j k ijk ables i j k tijk t ijk yj1 λ q xi yi xi zi yi zi zq yq xq o λ i the construction computes a particular symmetric trilinear form the indices of the variables are either q or some integer in q we deﬁne p i k tijk xi yj zk o if i if i q if i q the important property of the cw construction is that for any triple xi yj zk in the trilinear form p i p j p k in general the cw approach applies to any construction for which we can deﬁne an integer function p on the indices so that there exists a number p so that for every xi yj zk in the trilinear form computed by the construction p i p j p k p we call such constructions p p uniform i j k where the o term hides triples which have coeﬃcients that depend on positive powers of as an example bini et al gave the following construction for three entries of the product of matrices in terms of bilinear products definition let p be a function from n to n let p n a trilinear form i j k n tijk xi yj zk is p p uniform if whenever tijk p i p j p k p a construction computing a p p uniform trilinear form is also called p p uniform z12 o x12 x22 y21 z11 z21 x11 y11 z11 z12 any tensor power of a p p uniform construction is p p uniform for some p and p there are many ways to deﬁne p and p in terms of p and p for the k th tensor power the variable indices xindex yindex zindex are length k sequences of the original indices xindex xindex k yindex yindex k and zindex zindex k then for instance one can pick p to be an arbitrary linear combination so that p xindex k i ai xindex i and similarly k k p yindex and p zindex i ai yindex i i ai zindex i clearly then p p i ai and the k th tensor power construction is p p uniform in this paper we will focus on the case where ai for all i k so that p index k i index i and p p k similar results can be obtained for other choices of p the cw approach proceeds roughly as follows suppose we are given a p p uniform construction and we wish to derive a bound on ω from it the approach only works when the range of p is at least let c be the trilinear form computed by the construction and let r be the number of bilinear products performed if the trilinear form happens to be a direct sum of diﬀerent matrix products then one can just apply the scho nhage τ theorem to obtain a bound on ω in terms of r and the dimensions of the small matrix products however typically the triples in the trilinear form c cannot be partitioned into matrix products on disjoint sets of variables the ﬁrst cw idea is to partition the triples of c into groups which look like matrix products but may share variables then the idea is to apply procedures to remove the shared variables by carefully setting variables to in the end one obtains a smaller but not much smaller number of independent matrix products and can use scho nhage τ theorem two procedures are used to remove the shared variables the ﬁrst one deﬁnes a random hash function h mapping variables to integers so that there is a large set s such that for any triple xi yj zk with h xi h yj h zk s one actually has h xi h yj h zk then one can set all variables mapped outside of s to and be guaranteed that the triples are partitioned into groups according to what element of s they were mapped to and moreover the groups do not share any variables since s is large and h maps variables independently there is a setting of the random bits of h so that a lot of triples at least the expectation are mapped into s and are hence preserved by this partitioning step the construction of s uses the salem spencer theorem and h is a cleverly constructed linear function after this ﬁrst step the remaining nonzero triples have been partitioned into groups according to what element of s they were mapped to and the groups do not share any variables the second step removes shared variables within each group this is accomplished by a greedy procedure that guarantees that a constant fraction of the triples remain more details can be found in the next section when applied to the cw construction above the above procedures gave the bound ω the next idea that coppersmith and winograd had was to extend the τ theorem to theorem below using the notion of value vτ the intuition is that vτ assigns a weight to a trilinear form t according to how close an algorithm computing t is to an o matrix product algorithm suppose that for some n the n th tensor power of t can be reduced to qi ki mi ni by substitution of variables then as in we introduce the constraint n q τ vτ t ki mi ni i furthermore if π is the cyclic permutation of the x y and z variables in t then we also have vτ t vτ t πa π t vτ t vτ πt vτ π t we can give a formal deﬁnition of vτ t as follows consider all positive integers n and all possible ways σ to zeroout variables in the n th tensor power of t so that one ob q σ σ σ σ tains a direct sum of matrix products i ki mi ni then we can deﬁne n q σ σ σ σ τ vτ t lim supn σ ki mi ni i we can argue that for any permutation of the x y z variables π and any n there is a corresponding permutation of the zeroed out variables σ that gives the same under the permutation π direct sum of matrix products hence vτ t vτ πt and since t can be replaced with πt and π with π we must have vτ t vτ πt thus also satisfying the inequality vτ t vτ t vτ πt vτ π t it is clear that values are superadditive and supermultiplicative so that vτ vτ vτ and vτ vτ vτ with this notion of value as a function of τ we can state an extended τ theorem implicit in theorem let t be a trilinear form such that t qi ti and the ti are independent copies of the same trilinear form t if there is an algorithm that computes t by performing at most r multiplications for r q then ω for τ given by qvτ t r theorem has the following eﬀect on the cw approach instead of partitioning the trilinear form into matrix product pieces one could partition it into diﬀerent types of pieces provided that their value is easy to analyze a natural way to partition the trilinear form c is to group all triples xi yj zk for which i j k are mapped by p to the same integer p i p j p k this partitioning is particularly good for the cw construction and its tensor powers in the full version we show for instance that the trilinear form which consists of the triples mapped to j k for any j k is always a matrix product of the form q for some q using this extra ingredient coppersmith and winograd were able to analyze the second tensor power of their construction and to improve the estimate to the current best bound ω in the following section we show how with a few extra ingredients one can algorithmically analyze an arbitrary tensor power of any p p uniform construction amusingly the algorithms involve the solution of linear systems indicating that faster matrix multiplication algorithms can help improve the search for faster matrix multiplication algorithms tensor powers of trilinear forms can be deﬁned analogously to how we deﬁned tensor powers of an algorithm computing them k i p index i and that the k tensor power is p p k uniform now we can represent c as a sum of trilinear forms x i y j z k where x i y j z k only contains those triples in c xxindex yyindex zzindex for which p maps xindex to i yindex to j and zindex to k that is if c ijk tijk xi yj zk then x i y j z k i j k p i i p j j tijk xi yj zk we refer to i j k as blocks following the cw analysis we will later compute the value vijk as a function of τ for each trilinear form x i y j z k separately the trilinear forms x i y j z k can share vari ables for instance x i y j z k and x i y j z k share the x variables mapped to block i we use the cw tools to zero out some variables until the remaining trilinear forms no longer share variables and moreover a nontrivial number of the same trilinear form remain so that one can obtain a good estimate on τ and hence ω we outline the approach in what follows take the n th tensor power c n of c for large n we will eventually let n go to now the indices of the variables of c are n length sequences of k length sequences the blocks of c n are n length sequences of blocks of c we will pick rational values ai for every block i of c so that i ai then we will set to zero all x y z variables of c n the indices of which map to blocks which do not have exactly n ai positions of block i for every i for large enough n n ai is an integer j k we will consider for each triple of blocks of c n i the trilinear subform of c n x i y j z k where as before c n is the sum of these trilinear forms consider values aijk for all valid block triples i j k of c which satisfy ai aij p k i j aji p k i j a p k i j ji for each i j k p k i j determine the value vijk of the trilinear form i j p i i p j j tijk xi yj zk as a nondecreasing function of τ deﬁne variables aijk and a ijk for i j k p k i j form the linear system for all i ai j a ijk where a ijk a sort ijk determine the rank of the linear system and if necessary pick enough variables a ijk to place in s and treat as constants so the system has full rank solve for the variables outside of s in terms of the ai and the variables in s compute the derivatives pi j k ijk form and solve the program below to obtain ω minimize τ subject to a perm ijk a ijk rk i j k aijk aijk ijk perm ijk aijk vijk i a ai i a ijk aijk for all i j k i j k perm ijk a ijk for all a ijk s a ijk a s p a pi j k ijk i j k i j k ijk i j k p a s p a i j k i j k ijk i j k i j k ijk and is setting aijk a ijk unless one j aijk j a ijk for all i figure the procedure to analyze the k tensor power j analyzing arbitrary tensor powers of uniform constructions let k be an integer let p be an integer function with range size at least we will show how to analyze the ktensor power of any p p uniform construction by proving the following theorem j j the values aijk will correspond to the number of index positions pos such that any trilinear form x i y j z k of c n we have that i pos i j pos j k pos k the aijk need to satisfy the following additional two constraints ai i theorem given a p p uniform construction and lower bounds for the values for its k tensor power the procedure in figure outputs a constraint program the solution τ of which implies ω we begin the proof consider the the k tensor power of a particular p p uniform construction call the trilinear form computed by the construction c let r be the bound on the border rank of the original construction then rk is a bound on the border rank of c the variables in c have indices which are k length sequences of the original indices let the index of an x variable be denoted by xindex the index of a y variable by yindex and the index of a z variable by zindex then for every triple xxindex yyindex zzindex in the trilinear form and any particular position pos in the index sequences p xindex pos p yindex pos p zindex pos p recall that we deﬁned the extension p of p for the k tensor power as p index and pk aijk i j k i ai i we note that although the second constraint is explicitly stated in it actually automatically holds as a consequence of constraint and the deﬁnition of aijk since iai iai jaj kak i i i aij p k i j i j j i j k j aij p k i j i j p k i j aij p k i j i j pk i j k a p k j k j k k j aij p k i j p k thus the only constraint that needs to be satisﬁed by the aijk is i j k aijk recall that rin i s denotes ri ri n ri where the n n ai aijk i n ai n aijk j let us show how to process the triples so that they no longer share variables pick m to be a prime which is θ ℵ let s be a salemspencer set of size roughly m o as in the salem spencer theorem the o term will go to when we let n go to inﬁnity in the following we ll let s m ε and in the end we ll let ε go to similar to this is possible since our ﬁnal inequality will depend on m ε n which goes to as n goes to and ε goes to choose random numbers wn in m deﬁne the hash functions which for an index sequence i map the variable indices to integers just as in n wpos i pos n thus the salem spencer set s has allowed us to do some partitioning of the triples let us analyze how many triples remain since m is prime and due to our choice of functions the x y and z blocks are independent and are hashed uniformly to m if the i and j blocks of a triple x i y j z k are mapped to the same value so is the k block the expected fraction of triples which remain is hence m ε m m which is m ε this holds for the triples that satisfy our choice of partition aijk the trilinear forms corresponding to block triples mapped to the same value in s can still share variables we do some pruning in order to remove shared blocks similar to with a minor change for each s process the triples hashing to separately we ﬁrst process the triples that obey our choice of aijk until they do not share any variables after that we also process the remaining triples zeroing them out if necessary this is slightly diﬀerent from greedily build a list l of independent triples as follows j k if i is among suppose we process a triple with blocks i the x blocks in another triple in l then set to all y vari similarly if i is not shared but j or k ables with block j is then set all x variables with block i to if no blocks are shared add the triple to l j k we ﬁnd suppose that when we process a triple i with a triple i j k in l that it shares a block say i suppose that we then eliminate all variables sharing block and thus eliminate u new triples for some u then we j eliminate at least pairs of triples which share a block u the pairs of the eliminated triples that share block j and the pair i j k and i j k which share i since u we eliminate at least as many pairs as triples the expected number of unordered pairs of triples sharing an x or y or z block and for which at least one triple obeys our choice of aijk is n n ℵ ℵ ℵ ℵ ℵ n ai n ai m ε m ε wpos i pos n ℵ ℵ ℵ n ai thus at most this many triples obeying our choice of aijk have been eliminated hence the expected number of such triples remaining after the pruning is mod m pos by i mod m set to all variables with blocks mapping to outside s j k in the remaining trilinfor any triple with blocks i by j k hence ear form we have that bx i the hashes of the blocks form an arithmetic progression of length since s contains no nontrivial arithmetic progressions we get that for any nonzero triple by j bz k bx i s where the sum ranges over the values aijk which satisfy the above constraint this is since the number of nonzero blocks is nn and the number of block triples which con ai n ai for every tain a particular x block is exactly i n a ijk j partition of ai into aijk p k i j j for k n ai let ℵ aijk i n aijk the current number of n j nonzero block triples is ℵ n ai our goal will be to process the remaining nonzero triples by zeroing out variables sharing the same block until the remaining trilinear forms corresponding to diﬀerent block triples do not share variables furthermore we would like for the remaining nonzero trilinear forms to be essentially the same so that we can use theorem this would be the case if we ﬁx for each i a partition aijk n j of ai n suppose that each remaining triple x i y j z k has exactly aijk n positions pos such that i pos i j pos j k pos k then all remaining triples would be isomor aijk n phic and each would have value at least i j vijk by supermultiplicativity suppose that we have ﬁxed a particular choice of the aijk we will later show how to pick a choice which maximizes our bound on ω the number of small trilinear forms corresponding to dif ferent block triples of c n is ℵ nn where ai n ai ℵ n aijk j i bx i p k wpos i pos pos indices i s are the elements of s when s is implicit we only write rni by our choice of which variables to set to we get that the number of c n block triples which still have nonzero trilinear forms is n bz i mod m m pos ε n ℵ ℵ m ℵ n ai n ℵ cm ε n ai the values vijk are nondecreasing functions of τ where τ ω the inequality above gives an upper bound on τ and hence on ω for some constant c depending on how large we pick m to be in terms of ℵ we can pick values for the variables wi in the hash functions which we deﬁned so that at least this many triples remain picking these values determines our algorithm we have that n ai max ℵ aijk n aijk j i poly n max aijk i computing a ijk and aijk here we show how to compute the values a ijk forming ℵmax and aijk which maximize our bound on ω j aijk the only restriction on aijk is that ai pick a ijk we j a ijk and so if we know how to can let aijk vary subject to the constraints j aijk j a ijk hence we will focus on computing a ijk recall that the setting of the variables aijk which a ijknis ai maximizes i n a for ﬁxed ai ijk j because of our symmetric choice of the ai the above is maximized for a ijk a sort ijk where sort ijk is the permutation of i j k which sorts them in lexicographic order let perm i j k be the number of distinct permutations of i j k the constraint satisﬁed by the aijk becomes ai perm i j k aijk n ai n aijk j n ai max aijk i n a ijk j we will approximate ℵ by ℵmax we have obtained ℵ n ω n ai ℵmax poly n m ε trilinear forms that share any variables and each of do not aijk n which has value i j vijk n n ai if we were to set ℵ ℵmax we would get ω poly n m ε i trilinear forms instead we use this setting in our analyses though a better analysis may be possible if you allow ℵ to vary we will see later that the best choice of aijk sets aijk asort ijk for each i j k where sort ijk is the permutation of ijk sorting them in lexicographic order so that i j k since tensor rank is invariant under permutations of the roles of the x y and z variables we also have that vijk vsort ijk for all i j k let perm i j k be the number of distinct permutations of i j k that is if i j k perm i j k if i j k perm i j k and ﬁnally if i j k i perm i j k recall that r was the bound on the border rank of c given by the construction then by theorem we get the inequality r kn n ℵ n ai ℵmax poly n m ε i j k vijk τ perm ijk n aijk be the choices which achieve ℵmax so that ℵmax a nijk let ai i n a ijk j then by taking stirling approximation we get that a a ijk ijk ℵ ℵmax n aijk a ijk ijk the constraint above together with a ijk a sort ijk are the only constraints in the original cw paper however it turns out that more constraints are necessary for k the equalities ai a form a system of linear ijk j equations involving the variables a ijk and the ﬁxed values ai if this system had full rank then the a ijk values for a ijk a sort ijk would be determined from the ai n ai and a further and hence ℵ would be exactly i n a ijk j maximization step would not be necessary this is exactly the case for k in this is also why in setting aijk a ijk was necessary however the system of equations may not have full rank because of this let us pick a minimum set s of variables a i j k so that viewing these variables as constaints would make the system have full rank then all variables a ijk s would be determined as linear functions depending on the ai and the variables in s consider the function g of ai and the variables in s deﬁned as n ai g n a ijk a ijk s n a ijk a ijk s i g is only a function of a ijk s for ﬁxed ai i we want to know for what values of the variables of s g is maximized g is maximized when ij a ijk n is minimized which in turn is minimized exactly when f ij ln n a ijk is minimized where k p k i j using stirling approximation ln n n ln n n o ln n we get that f is roughly equal to taking the n th root taking n to go to and ε to go to and using stirling approximation we obtain the following inequality a a ijk perm ijk v perm ijk aijk ijk rk ijk a ijk i aaijk i ai i j k n a ijk ln a ijk a ijk a ijk ln n o log n a ijk n ij if we set aijk a ijk we get the simpler inequality a rk vijk perm ijk aijk ai i i j k i j k n ln n n i ij a ijk ln aijk a ijk o log n a ijk n since ij a ijk i ai as n goes to for any ﬁxed setting of the a ijk variables the o log n n term vanishes and f is roughly n ln n n ij a ijk ln a ijk which is what we use in our application of the theorem as it reduces the number of variables and does not seem to change the ﬁnal bound on ω by much a ijk hence to minimize f we need to minimize f ij a ijk ln a ijk a ijk we want to know for what values of a ijk f is minimized since f is convex for positive aijk it is actually minimized when a f for every a ijk s recall that the variijk ables not in s are linear combinations of those in s taking the derivatives we obtain for each a ijk in s a f ln a i j k i j k a ijk a ijk deﬁne variables αijk and xi yj zk for all good triples i j k form the linear system consisting of xi j αijk yj i αijk and zk i αijk determine the rank of the system it i is exactly j k because of the fact that i xi j yj k zk if the system does not have full rank then pick enough variables αijk to treat as constants place them in a set δ i j k we can write this out as a i j k a i j k a ijk solve the system for the variables outside of δ in terms of the ones in δ and xi yj zk now we have αijk αijk xi yj xk y δ i j k let wi j k vi j k vi i j j k k compute for every since each variable a i j k in the above equality for a ijk is a linear combination of variables in s we have that the a j k is actually a constant and exponent pi j k ijk a i ijk so we get a system of polynomial equality constraints which deﬁne the variables in s in terms of the variables outside of s for any a ijk s we get a ijk α ijk x wijk i j k ny α ijk y wijk and i j k a i j k pi j k ijk nz a i j k s p i j k ijk nx α ijk z wijk i j k a i j k pi j k ijk compute for every variable y δ a i j k s p i j k ijk ny given values for the variables not in s we can use to get valid values for the variables in s and hence also for the ai for that choice of the ai g is maximized for exactly the variable settings we have picked now all we have to do is ﬁnd the correct values for the variables outside of s and for a ijk given the constraints ai j a ijk we cannot pick arbitrary values for the variables outside of s they need to satisfy the following constraints the obtained ai satisfy i ai and αijk y wi j k i j k compute for each αijk its setting αijk δ as a function of the y δ when x nx i nxi y ny j nyj and z nz k nzk then set vijk nx ny nz ny y y δ subject to the constraints on y δ given by the variables in s obtained from equation are nonnegative y for all y δ αijk δ for every αijk s in summary we obtain the procedure to analyze the k tensor power shown in figure find the setting of the y δ that maximizes the bound ﬁxed guess for τ this is a linear on vijk for any program maximize y δ y log ny subject to the above linear constraints or alternatively let vijk be a function of y δ and add the above two constraints to the ﬁnal program in figure computing ω analyzing the smaller tensors consider the trilinear form consisting only of the variables from the k tensor power of c with blocks i j k where i j k p k in this section we will prove the following theorem figure the procedure for computing vijk for arbitrary tensor powers theorem given a p p uniform construction c using the procedure in figure one can compute lower bounds on the values vijk for any tensor power of c the k tensor power requires o applications of the procedure suppose that we have analyzed the values for some powers k and k k of the trilinear form from the construction with k k we will show how to inductively analyze the values for the k power using the values for these smaller powers the theorem will follow by noting that the number of values for the k power is o and that one can use recursion to ﬁrst compute the values for the k and k powers and then combining them to obtain the values for the k power consider the k tensor power of the trilinear form c it we could have instead written f ij a ijk ln a ijk and minimized f and the equalities we would have obtained would have been exactly the same since the system of equations includes the equation ij a ijk and although ln aijk ijk f a is ij a ijk a ij a a ln a aijk the in the brackets would be canceled out if a p k a p k ln a p k then ij i j a ijk a a p k a i j k ln a p k a i j i j a can actually be viewed as the tensor product of the k and k k tensor powers of c recall that the indices of the variables of the k tensor power of c are k length sequences of indices of the variables of c also recall that if p was the function which maps the indices of c to blocks then we deﬁne pk to be a function which maps the k power indices to blocks as pk index pos p index pos an index of a variable in the k tensor power of c can also be viewed as a pair l m such that l is an index of a variable in the k tensor power of c and m is an index of a variable in the k k tensor power of c hence we get that pk l m pk l pk k m for any i j k which form a valid block triple of the k tensor power we consider the trilinear form ti j k consisting of all triples xi yj zk of the k tensor power of the construction for which pk i i pk j j pk k k ti j k consists of the trilinear forms ti j k ti i j j k k for all i j k that form a valid block triple for the k power and such that i i j j k k form a valid block triple for the k k power call such blocks i j k good then tijk of x y or z blocks is n n n γ n yj n zk n xi the number of block triples which contain a particular x y or z block is ℵ i n xi n αijk j j n yj n αijk i k n zk n αijk i hence the number of triples is γ ℵ set m θ ℵ to be a large enough prime greater than ℵ create a salem spencer set s of size roughly m ε pick random values w3n in m the blocks for x y or z variables of the new big trilinear form are sequences of length the ﬁrst n positions of a sequence contain pairs i i i the second n contain pairs j j j and the last n contain pairs k k k we can thus represent the block sequences i of the k tensor power as where is a sequence of length of blocks of the k power of c and is a sequence of length of blocks of the k k power of c the ﬁrst n are x blocks the second n are y blocks and the third n are z blocks for a particular block sequence i we deﬁne the hash functions that depend only on ti j k ti i j j k k good ijk the sum above is a regular sum not a disjoint sum so the trilinear forms in it may share indices the above decomposition of tijk was ﬁrst observed by stothers when considering the special cases of the and tensor powers this is the shortcut that allowed us to simplify our analysis let qijk tijk ti i j j k k by supermultiplicativity the value wijk of qijk satisﬁes wijk vijk vi i j j k k if the trilinear forms qijk didn t share variables then we would immediately obtain a lower bound on the value vijk as ijk vijk vi i j j k k however the trilinear forms qijk may share variables and we ll apply the techniques from the previous section to remove the dependencies to analyze the value vijk of ti j k we ﬁrst take the n th tensor power of ti j k the n th tensor power of tk i j and the n th tensor power of tj k i and then tensor multiply these altogether by the deﬁnition of value vi j k is at least the th root of the value of the new trilinear form here is how we process the n th tensor power of ti j k the powers of tk i j and tj k i are processed similarly we pick values x i for each block i of the k tensor power of c so that i xi set to all x variables except those that have exactly xi n positions of their index which are mapped to i i i by pk pk k for all i n the number of nonzero x blocks is n x i i similarly pick values yj for the y variables with j yj and retain only those with yj index positions mapped to j j j similarly pick values zk for the z variables with k zk and retain only those with zk index positions mapped to k k k n the number of nonzero y blocks is n y the number j j n of nonzero z blocks is n zk k for i j k p k i j which are valid blocks of the k tensor power of c with good i j k let αijk be variables such that xi j αijk yj i αijk and zk i αijk after taking the tensor product of what is remaining of the n th tensor powers of ti j k tk i j and tj k i the number bx i wpos pos mod m pos by i wpos pos mod m pos bz i p k wpos pos mod m pos we then set to all variables that do not have blocks hashing to elements of s again any surviving triple has all variables blocks mapped to the same element of s the expected fraction of triples remaining is m ε m m ε as before we do the pruning of the triples mapped to each element of s separately the expected number of unordered pairs of triples sharing an x y or z block is γℵ ℵ m γℵ c m for large constant c and the number of remaining block triples over all elements of s is ω γℵ m ε ω γ m ε recall that γ is the number of blocks and γℵ was the original number of triples analogously to we will let ε go to and so the expected number of remaining triples is roughly γ hence we can pick a setting of the wi variables so that roughly γ triples remain we have obtained about γ independent trilinear forms each of which has value at least vi j k vi i j j k k αijk i j k this follows since values are supermultiplicative the ﬁnal inequality becomes vi j k n n n vi j k vi i j j k k αijk n yj n zk n xi i j k recall that we have equalities xi j αijk yj i αijk and zk i αijk if we ﬁx xi yj zk over all i j k this forms a linear system the linear system does not necessarily have full rank and so we pick a minimum set δ of variables αijk so that if they are treated as constants the linear system has full rank and the variables outside of δ can be written as linear combinations of variables in δ and of xi yj zk now we have that for every αijk αijk y αijk y exploiting the symmetry in the full version we show how to modify the procedure in figure conclusion y δ xi yj zk i j k δ we use the linear function obtained where for all αijk from the linear system αijk let δijk y δ y y let wi j k vi j k vi i j j k k then i αijk wi j k wijk deﬁne nx α ijk xi x i i wijk α ijk yj y j α k wijk ijk zk z k δ ijk wi j k α ijk x i j k wijk for any set nx αijk y nx nx αijk z deﬁne similarly ny i j k wijk nz i j k wijk ny nz and nz nz setting ny ny consider the right hand side of our inequality for vijk αijk n n n wi j k n yj n zk n xi acknowledgments i j k nx ny n n nx ny n yj n xi the author is grateful to satish rao for encouraging her to explore the matrix multiplication problem more thoroughly and to ryan williams for his unconditional support the author would also like to thank franc ois le gall who alerted her to stothers work suggested the use of nlopt and pointed out that the feasible solution obtained by stothers for his tensor power constraint program is not optimal and that one can obtain ω with a diﬀerent setting of the parameters the author was supported by nsf grants ccf and ccf at uc berkeley and by nsf grants and iis and an afosr muri grant at stanford university ijk n z y δ y α y n nz wi j k n zk i j k y by lemma the above is maximized for x nx ny and nz for all and for these settings for in z n x stance nn is essentially nx n poly n nx xi and hence after taking the th root and letting n go to we obtain vi j k nx we applied our procedures to the second third fourth and eighth tensor powers by applying our procedures to the second tensor power of the cw construction we obtained the same constraint program that coppersmith and winograd obtained thus obtaining ω applying them to the fourth tensor power we obtained the same constraint program as in stothers thesis however we were able to obtain an improved solution ω by using better optimization procedures applying the procedures to the eighth tensor power was done entirely by computer using a combination of maple and c and the nonlinear optimization software nlopt we are not certain whether the optimization software we used to solve the resulting constraint program found the optimum solution however we found a feasible solution which shows that ω it is possible that a better bound can be found for the eighth tensor power and we believe that higher tensor powers of the cw construction should improve the bound on ω further nevertheless it seems that in order to approach ω we need a new basic construction it is very possible that a combination of the group theoretic approach of and our techniques can lead to further improvements a multi objective ant colony system algorithm for virtual machine placement in cloud computing yongqiang gao a haibing guan a zhengwei qi a yang hou b liang liu c a shanghai key laboratory of scalable computing and systems department of computer science and engineering shanghai jiao tong university shanghai china um sjtu joint institute shanghai jiao tong university shanghai china c ibm research china beijing china b a r t i c l e i n f o article history received december received in revised form april accepted february available online march keywords multi objective optimization ant colony optimization virtual machine placement cloud computing a b t r a c t virtual machine placement is a process of mapping virtual machines to physical machines the optimal placement is important for improving power eﬃciency and resource utilization in a cloud computing environment in this paper we propose a multi objective ant colony system algorithm for the virtual machine placement problem the goal is to eﬃciently obtain a set of non dominated solutions the pareto set that simultaneously minimize total resource wastage and power consumption the proposed algorithm is tested with some instances from the literature its solution performance is compared to that of an existing multi objective genetic algorithm and two single objective algorithms a well known binpacking algorithm and a max min ant system mmas algorithm the results show that the proposed algorithm is more eﬃcient and effective than the methods we compared it to elsevier inc all rights reserved introduction in recent year cloud computing has become a popular computing paradigm for hosting and delivering services over the internet there are three major types of cloud computing infrastructure as a service iaas platform as a service paas and software as a service saas the adoption and deployment of cloud computing platforms have many attractive beneﬁts such as reliability quality of service and robustness to the consumer the cloud appears to be inﬁnite and the consumer can purchase as much or as little computing power as they need from a provider perspective the key issue is to maximize proﬁts by minimizing the operational costs in this regard power management in cloud data centers is becoming a crucial issue since it dominates the operational costs moreover power consumption in large scale computer systems like clouds also raises many other serious issues including carbon dioxide and system reliability the emergence of cloud computing has made a tremendous impact on the information technology it industry over the past few years where large companies such as amazon google salesforce ibm microsoft and oracle have begun to establish new data centers for hosting cloud computing applications in various locations around the world to provide redundancy and ensure reliability in case of site failures there are a number of key technologies that make cloud computing possible one of the most important is virtualization virtualization provides a promising approach through which hardware resources on one or more machines can be divided through partial or complete machine simulation time sharing hardware and software partitioning into multiple execution environments each of which can act as a complete system virtualization enables dynamic sharing of physical resources corresponding author fax e mail addresses gaoyongqiang sjtu edu cn y gao hbguan sjtu edu cn h guan qizhwei sjtu edu cn z qi sjtu edu cn y hou liuliang cn ibm com l liu see front matter elsevier inc all rights reserved http dx doi org j jcss y gao et al journal of computer and system sciences in cloud computing environments allowing multiple applications to run in different performance isolated platforms called virtual machines vms in a single physical server this technology also enables on demand or utility computing a just intime resource provisioning model in which computing resources such as cpu memory and disk space are made available to applications only as needed and not allocated statically based on the peak workload demand through virtualization a cloud provider can ensure the quality of service qos delivered to the users while achieving a high server utilization and energy eﬃciency virtual machine placement is a process of mapping virtual machines to physical machines as virtualization is a core technology of cloud computing the problem of virtual machine vm placement has become a hot topic recently this vm placement is an important approach for improving power eﬃciency and resource utilization in cloud infrastructures several research works addressed the importance of placing vms appropriately vogels quoted the beneﬁt of packing vms eﬃciently in server consolidation the proxy placement and object placement replacement for transparent data replication bear some resemblance to the issues we face since they all attempt to exploit the ﬂexibility available in determining proper placement the following are some of the approaches that have been used to solve the virtual machine placement problem linear programming a traditional analytical approach is linear programming for example chaisiri et al presented a nice algorithm for optimal placement of virtual machines on physical machines the goal is that the number of used nodes is minimum they provided approaches based on linear and quadratic programming in and the authors described linear programming formulations of server consolidation problems they also designed extension constraints for allocating virtual machines to a speciﬁc set of physical servers that contain some unique attribute restricting the number of virtual machines in a single physical server ensuring that some virtual machines are assigned to different physical servers and limiting the total number of migrations in addition they developed an lp relaxation based heuristic for minimizing the cost of solving the linear programming problem genetic algorithm another approach to this problem is to use a genetic algorithm in the authors proposed a genetic algorithm based approach namely gaba to adaptively self reconﬁgure the vms in cloud data centers consisting of heterogeneous nodes gaba can eﬃciently decide the optimal physical locations of vms according to time varying requirements and the dynamic environmental conditions in the vm placement problem is formulated as a multi objective optimization problem of simultaneously minimizing total resource wastage power consumption and thermal dissipation costs a modiﬁed genetic algorithm with fuzzy multi objective evaluation was proposed for eﬃciently searching the large solution space and conveniently combining possibly conﬂicting objectives constraint programming constraint programming methods have also been applied for vm placement in various environments van et al proposed a resource management framework combining a utility based dynamic virtual machine provisioning manager and a dynamic vm placement manager the vm provisioning and placement problems were expressed as two constraint satisfaction problems in the authors proposed the entropy resource manager for homogeneous clusters which performs dynamic consolidation based on constraint programming and takes into account both the problem of allocating the vms to the available nodes and the problem of how to migrate the vms to these nodes bin packing the problem of vm placement in a data center is often formulated as a variant of the vector bin packing problem which is an np hard optimization problem various heuristics have been proposed for this problem for example the pmapper system tackled power cost tradeoffs under a ﬁxed performance constraint by minimizing migration costs while packing vms in a small number of machines the packing algorithm is an extension of the ﬁrst ﬁt decreasing ffd heuristic moreover feller et al proposed a single objective algorithm based on the mmas metaheuristic to minimize the number of physical machines required to support the current load the majority of the studies on virtual machine placement focus on a single criterion however many real world problems require taking multiple criteria into account for this reason recent research tends to look at the multiple objective situation therefore in this paper the problem of vm placement is formulated as a multi objective combinatorial optimization problem aiming to simultaneously optimize total resource wastage and power consumption a modiﬁed version of the ant colony system acs algorithm is proposed and designed to deal effectively with the potential large solution space for large scale data centers to the best of our knowledge this study is the ﬁrst application of the acs metaheuristic to a multi objective virtual machine placement problem where both power consumption and resource wastage should be minimized the performance of the proposed algorithm is compared to that of a multi objective genetic algorithm and two single objective algorithms a well known bin packing algorithm and an mmas algorithm computational experiments on benchmark problems are carried out the results show that the proposed algorithm can compete eﬃciently with other promising approaches to the problem the remainder of this paper is organized as follows in section we provide an overview of both ant colony optimization and evolutionary multi objective optimization and present a simple procedure to perform vm placement in a virtualized cloud environment section formulates the virtual machine placement problem in section the proposed algorithm is presented the computational results on benchmark problems are given in section we conclude in section y gao et al journal of computer and system sciences fig an example of vm placement in a virtualized environment background an example of vm placement in a virtualized cloud environment for example let us consider the situation depicted in fig we have seven servers each of which has a quad core processor which is capable of executing four vms the system is currently hosting seven virtualized application labeled application to application a simple process for vm placement is as follows for each server compute application resource requirement using server resource usage statistics over a period of time e g several weeks choose a target server with compatible virtualization software comparable cpu types similar network connectivity and usage of shared storage place the ﬁrst virtual machine on the ﬁrst server in step place the second virtual machine on the same server if it can satisfy the resource requirements if not add a new physical machine and place the vm on this new machine continue this step until each of the vms has been placed on a physical machine adding a new physical machine when required the set of resulting hosts at the end of step comprises the consolidated server farm finally the number of servers required in the cluster is reduced from down to ant colony optimization ant colony optimization aco is a metaheuristic inspired by the observation of real ant colonies and based upon their collective foraging behavior ants are social insects and live in colonies their behavior is governed by the goal of colony survival when searching for food ants frequently travel between their nest and food sources at the beginning ants explore the area surrounding their nest in a random manner while moving ants deposit special substances called pheromones along their paths ants can smell pheromones when choosing their way they tend to choose in probability paths marked by strong pheromone concentrations as soon as an ant ﬁnds a food source it evaluates the quantity and the quality of the food and carries some of it back to the nest during the return trip the quantity of pheromones that an ant leaves on the ground may depend on the quantity and quality of the food the pheromone trails will guide other ants to the food source the indirect communication between the ants via pheromone trails enables them to ﬁnd the shortest paths between their nest and food sources ant colony optimization has been successfully applied to solve numerous optimization problems such as the traveling salesman problem the ﬂow shop scheduling problem and the quadratic assignment problem besides its original domain of combinatorial optimization aco is also now used to solve continuous optimization problems some extensions of aco algorithms have been proposed in the literature such as acs ant system as and mmas recently there have also been a number of studies extending aco to the ﬁeld of multi objective optimization these algorithms mainly differ with respect to the three following points y gao et al journal of computer and system sciences pheromone update when updating pheromone trails one has to decide on which of the constructed solutions to lay pheromones there are usually two strategies to update the pheromone trails a ﬁrst strategy is to select the iteration best or best so far solutions to update the pheromone matrices with respect to each objective a second strategy is to collect and store the non dominated solutions in an external set only the solutions in the non dominated set are allowed to update the pheromones deﬁnition of pheromone and heuristic information at each step of the construction of a solution a candidate is chosen relative to a transition probability which depends on two factors a pheromone factor and a heuristic factor there are two approaches to deﬁne the pheromone heuristic information using one or multiple matrices when only one matrix is utilized the pheromone information associated with each objective is combined to reduce the multiple objectives into a single one if multiple matrices are used usually each matrix corresponds to one objective with respect to the pheromone information each matrix may contain different values depending on the implementation strategy applied the same applies to the heuristic information pheromone and heuristic aggregation whenever multiple matrices are used one must use some form of aggregation procedure to aggregate the pheromone heuristic matrices there are three common strategies for this the weighted sum where matrices are aggregated by a weighted sum the weighted product where matrices are aggregated by a weighted product and random where at each step a random objective is selected to be optimized whenever weights are used for aggregating multiple matrices two strategies can be applied for setting the weights used at each iteration of the algorithm a dynamically where each ant may be assigned a different weight from the other ants at each iteration b ﬁxed where we can assign to all ants the same weight and each objective has the same importance during the entire algorithm run evolutionary multi objective optimization multi objective evolutionary algorithms moeas are stochastic optimization methods which usually use a populationbased approach to ﬁnd pareto optimal solutions the majority of existing moeas use the concept of dominance during selection therefore we focus here on the class of dominance based moeas only the formal deﬁnition of the dominance concept is as follows let us consider without loss of generality a multi objective minimization problem with m parameters decision variables and n objectives minimize f x f xm f n xm where x xm x f f fn y where x is called the decision parameter vector x parameter space f objective vector and y objective space here we consider the term solution as a decision vector and the term point as the corresponding objective vector a solution x is said to dominate the other solution x if the both following conditions are true see the solution x is not worse than x in any objective the solution x is strictly better than x in at least one objective all points which are not dominated by any other point are called the non dominated points usually the non dominated points together constitute a front in the objective space and are often visualized to represent a non domination front the points lying on the non domination front by deﬁnition do not get dominated by any other point in the objective space hence they are pareto optimal points together they make up the pareto optimal front and the corresponding variable vectors are called pareto optimal solutions the above concept can also be extended to ﬁnd a non dominated solution set let us consider a set of n solutions each having m m objective function values in our work the following procedure is used to ﬁnd the non dominated solution set begin with i for all j i compare solutions x i and x j for domination using the above two conditions for all m objectives if for any j x i is dominated by x j mark x i as dominated increment i by one and go to step if all solutions that is when i n is reached in the set are considered go to step else increment i by one and go to step step all solutions that are not marked dominated are non dominated solutions step step step step problem statement and formulation in a cloud environment we have a pool of server nodes with applications running on them suppose that the cluster is fully virtualized and all the applications are running on vms the problem of vm placement across a pool of server nodes y gao et al journal of computer and system sciences is related to the multidimensional vector packing problems dimensions in the packing problem are resource utilizations in our work we use two dimensions to characterize a vm and a server node cpu and memory we do not consider the disk size dimension because we assume that network attached storage nas is used as main storage across the cluster if two vms are running on the same server the cpu utilization of the server is estimated as the sum of the cpu utilizations of the two vms this is the case with memory resources for example let be a pair of the cpu and memory requests of a vm and be that of another vm then the utilizations of a server accommodating the two vms are estimated at i e the sum of the vectors to prevent cpu and memory usage of a server from reaching we have to impose an upper bound on resource utilization of a single server with some threshold value the main idea behind this is that utilization can cause severe performance degradation and vm live migration technology consumes some amount of cpu processing capability on the migrating node resource wastage modeling the remaining resources available on each server may vary greatly with different vm placement solutions to fully utilize multidimensional resources the following equation is used to calculate the potential cost of wasted resources p wj l j lmj ε p u j um j p where w j denotes the resource wastage of the j th server u j and u m represent the normalized cpu and memory resource j p lj and l m represent the normalized remaining cpu and memory usage i e the ratio of used resource to total resource j resource ε is a very small positive real number and its value is set to be the key idea behind the above equation is to make effective use of the resources in all dimensions and balance the resources left on each server along different dimensions power consumption modeling recent studies show that the power consumption of servers can be accurately described by a linear relationship between the power consumption and cpu utilization this linear relationship is also conﬁrmed by our proﬁling conducted on a dell server in order to save energy servers are turned off when they are idle hence their idle power is not part of the total energy consumption finally we deﬁned the power consumption of the j th server as a function of the cpu utilization as shown in eq pj busy p j p p idle u j p idle u cj j j otherwise busy where p idle and p j are the average power values when the j th server is idle and fully utilized respectively in our j simulation experiments the values have been ﬁxed to and watt according to the measurements performed on a dell server optimization formulation next we formalize the vm placement optimization problem suppose that we are given n vms applications i i that are to be placed on m servers j j for simplicity we assume that none of the vms requires more resource than can be provided by a single server let r p i be cpu demand of each vm t p j be the threshold of cpu utilization associated with each server r m i be the memory demand of each vm and t m j be the threshold of memory utilization associated with each server we use two binary variables xi j and y j the binary variable xi j indicates if vm i is assigned to server j and the binary variable y j indicates whether server j is in use or not our objective is to simultaneously minimize the power consumption and the resource wastage the placement problem can therefore be formulated as minimize m pj m j minimize m j j wj yj m j busy pj p idle j n xi j r p i p idle j i t p j ni xi j r p i t m j ni xi j r mi ε n n yj i x i j r p i i x i j r m i subject to m j xi j i i y gao et al journal of computer and system sciences n r p i xi j t p j y j j j r mi xi j t m j y j j j j j and i i i n i y j x i j constraint assigns a vm i to only one of the servers constraints and model the capacity constraint of the server constraint deﬁnes the domain of the variables of the problem given a set of n virtual machines and a set of m physical machines there are a total of mn possible vm placement solutions it is therefore typically impractical to make a complete enumeration of all possible solutions to ﬁnd the best solutions the following shows how to apply an aco algorithm to eﬃciently search for good solutions in large solution spaces the description of the proposed multi objective ant colony system algorithm the algorithm proposed to solve the problems formulated in section is mainly based on an acs a feasible and complete solution of the formulated multi objective vm placement problem is considered as a permutation of vm assignment the terms host and server will be used interchangeably in this paper an assignment of a vm to a host is called a movement and represented by vm host the pseudocode of the proposed multi objective ant colony system algorithm vmpacs is depicted in fig this algorithm works as follows in an initialization phase the parameters are initialized and all the pheromone trails are set to in the iterative part each ant receives all vm requests introduces a physical server and starts assigning vms to hosts this is achieved by the use of a pseudo random proportional rule which describes the desirability for an ant to choose a particular vm as the next one to pack into its current host this rule is based on the information about the current pheromone concentration on the movement and a heuristic which guides the ants towards choosing the most promising vms a local pheromone update is performed once an artiﬁcial ant has built a movement after all ants have constructed their solutions a global update is performed with each solution of the current pareto set deﬁnition of the pheromone trail and the heuristic information similarly to the general implementation of aco algorithms vmpacs starts with a pheromone trails matrix and a heuristic information matrix the quality of an aco implementation depends greatly on the deﬁnition of the meaning of the pheromone trail it is crucial to choose a deﬁnition which conforms to the feature of the problem one may consider two different pheromone structures one that associates a pheromone trail with every movement vm host or one that associates a pheromone trail with every pair of vms in this paper the ﬁrst way of laying pheromone trails is used i e the pheromone trail τi j will be deﬁned as the favorability of packing vm i into host j in the initialization phase initial pheromone level is calculated by n p s w s where n is the number of vms s is the solution generated by the ffd heuristic and w s is the resource wastage of the solution s p s is the normalized power consumption of the solution s and its value is calculated according to the following equation p m p j p max j j where p max is the peak power consumption of server j j apart from pheromone trails another important factor in an aco application is the choice of a good heuristic which will be used in combination with the pheromone information to build solutions it guides the probabilistic solution construction of ants with problem speciﬁc knowledge the heuristic information is denoted by ηi j this information indicates the desirability of assigning vm i to host j in order to accurately assess the desirability of each move the heuristic information is dynamically computed according to the current state of the ant since the heuristic information is calculated for all movements in all ants it may signiﬁcantly affect the eﬃciency of the algorithm to overcome such diﬃculties it therefore should be computed in an eﬃcient manner the proposed method to calculate the heuristic information considers the partial contribution of each move to the objective function value let pl be a list composed of all the servers when constructing a solution every ant starts with the set of all vms to be placed and the list pl arranged in randomly order it initially assigns vms one by one to the ﬁrst host in the list pl then assigns to the second host and so on till all vms are assigned therefore while calculating the value of ηi j the permutation of vm assignments from the host to host j is known the partial contribution of assigning vm i to host j for the ﬁrst objective function can therefore be calculated as follows ηi j ε j max v p v p v y gao et al journal of computer and system sciences fig the vmpacs algorithm similarly to the ﬁrst objective function the partial contribution of assigning vm i to host j for the second one can be calculated as follows ηi j ε j v wv there are several ways to combine desirability in multi objective problem to ﬁnd the total desirability of each movement in this paper we propose the following formula to calculate the total desirability of assigning vm i to host j ηi j ηi j ηi j y gao et al journal of computer and system sciences constructing a solution in the process of making assignments the ant k selects a vm i as the next one to pack into its current host j according to the following pseudo random proportional rule i arg maxu ωk j α τu j α ηu j q otherwise where α is a parameter that allow a user to control the relative importance of pheromone trail and q is a random number uniformly distributed in if q is greater than this process is called exploration otherwise it is called exploitation is a ﬁxed parameter determined by the relative importance of exploitation of accumulated knowledge about the problem versus exploration of new movements n ωk j is the set of vms that qualify n for inclusion in the current host j m that is ωk j i n u xiu u xu j r p u r p i t p j u xu j r m u r m i t m j ηi j is deﬁned in eq above the pheromone value τi j is given in eq below is a random variable selected according to the following random proportional rule probability distribution which is the probability that ant k chooses to assign vm i to host j pki j α τi j α ηi j u ωk j α τu j α ηu j i ωk j otherwise there are two reasons for adopting the above method to calculate the selection probability the ﬁrst is the simplicity of the approach proposed in as only one control parameter i e α is used to map the relative importance of quantity of pheromone and the desirability of each movement the second reason is the computational eﬃciency of this method as multiplication operations are used instead of exponentiations pheromone trail update another vital component of vmpacs is the update of pheromone trails the pheromone trail value can either increase as ants deposit pheromone or decrease due to pheromone evaporation the deposit of new pheromone is based on the fact that the information contained in some good solutions should be indicated by pheromone trails and the movement included in these good solutions will be biased by other ants constructing subsequent solutions however pheromone evaporation also implements a useful form of forgetting it avoids a too rapid convergence of the algorithm toward a suboptimal region therefore favoring the exploration of new areas of the search space it is a kind of diversiﬁcation strategy in our proposed algorithm the pheromone updating process includes two steps a local pheromone update and a global pheromone update while constructing an assignment of vm i to host j an ant decreases the pheromone trail level between vm i and host j by applying the following local updating rule τi j t ρl τi j t ρl where is the initial pheromone level and ρl ρl is the local pheromone evaporating parameter the global updating rule is applied after all ants have ﬁnished building a solution since all non dominated or pareto solutions are considered as optimal or best solutions for a multi objective optimization problem we suppose that all nondominated solutions have the same and highest quality and all dominated solutions must be omitted therefore the global update is performed for each solution s of the current pareto set by applying the following rule τi j t ρ g τi j t ρg λ p s w s where λ na t ni in eq ρ g ρ g is the pheromone evaporation parameter of global updating the global non dominated solutions that form the pareto set are stored in an external set if a solution in the current iteration is not dominated by any other solutions in the current iteration or the external set of non dominated solutions this solution is added to the external set and the quantity of pheromone in all movements which constructed it will be increased then all solutions dominated by the added one are eliminated from the external set in eq na is the number of ants and ni represents the number of iterations that solution s has resided in the external set λ is an adaptive coeﬃcient used to control how a solution in the external set contributes to pheromone information over time this global updating rule tries to increasing the learning of ants y gao et al journal of computer and system sciences computational results in this section we use some simulation experiments to evaluate the proposed algorithm with respect to performance and scalability the performance of the proposed ant algorithm is compared to that of a multi objective grouping genetic algorithm mgga proposed in a single objective aco saco algorithm proposed in and a single objective ffd algorithm proposed in the programs for the proposed algorithm mgga algorithm and ffd heuristic were coded in the java language and ran on an intel pentium dual core processor with ghz cpu and gb ram the settings for various parameters in vmpacs have a direct effect on the algorithm performance appropriate parameter values were determined on the basis of preliminary experiments the ﬁnal parameter settings were determined to be na m α ρl ρ g and in the case of the mgga algorithm the population size is the initial population was generated randomly the crossover rate is and the mutation rate is the maximum number of generations for each search process is with the above conﬁguration we randomly generated problem instances the instances were a demand set of cpu and memory utilizations for vms the number of servers was set to the number of vms in order to support the worst vm placement scenario in which only one vm is assigned per server for simplicity we simulated homogeneous server environments but the proposed approach can be used for the case of heterogeneous servers after the vm placement algorithm was ﬁnished if there were several non dominated solutions a solution belonging to the set of non dominated solutions was randomly chosen every test was repeated with runs for each instance and the average results over independent runs are reported we introduced the linear correlations of cpu and memory utilizations into the instances and used the method proposed in to generate random sequences of cpu and memory utilizations in the experiments that had several correlations the detailed algorithm is as follows for i to n do r p i rand p r m i rand r m r rand if r p r p i r p r p r p i r p then rm i rm i rm end if end for where rand a is a function that returns a random uniformly distributed number of double type in the range a r p represents the reference cpu utilization and r m represents the reference memory utilization and the probability p is a reference value we can control the correlations of cpu and memory utilizations to some extent by varying probability p two kinds of the reference values and ﬁve probabilities were used in the experiments we set both r p and r m to and then the distributions of cpu and memory utilizations are in the range when r p r m and when r p r m for r p r m we set p to and and then the average correlation coeﬃcients became and for each set of instances these correlation coeﬃcients correspond to strong negative weak negative no weak positive and strong positive correlations we similarly set p for r p r m the correlation coeﬃcients were then 374 and we set the thresholds of both utilizations to t p i t m i throughout the experiments comparison of vmpacs with mgga to evaluate the effectiveness of the proposal vm placement algorithm its performance is compared to that of the mgga algorithm which is used as the benchmark because it is an effective and eﬃcient method used by to solve multiobjective vm placement problems we computed two measures overall non dominated vector generation onvg and spacing sp for each algorithm onvg and sp can be calculated as onvg y known c sp y known c y known c d di i m where y known denotes the calculated pareto front c denotes cardinality di min j i k f m j f m i j y known c f is the objection function m is the number of objectives and d is the mean of all di the higher the value of the onvg the better for understanding pareto front details a good solution set should have a value close to for the sp metric table shows onvg and sp under vmpacs and mgga the column corr indicates the correlation coeﬃcients for the cpu y gao et al journal of computer and system sciences table onvg and sp performance comparison of vmpacs and mgga reference value corr algorithm onvg sp r p r m mgga vmpacs mgga vmpacs 072 mgga vmpacs 371 mgga vmpacs mgga vmpacs 755 mgga vmpacs 57 374 mgga vmpacs 80 mgga vmpacs 398 mgga vmpacs 76 mgga vmpacs r p r m fig power consumption and resource wastage of vmpacs and mgga in the case of r p r m a and r p r m b for interpretation of the references to color in this ﬁgure legend the reader is referred to the web version of this article and memory utilizations fig compares the total resource wastage and power consumption for each of the algorithms under consideration from the results we can clearly see that the vmpacs algorithm outperforms mgga the reason is that vm placement under vmpacs combines the partial solution information under construction and the feed information of y gao et al journal of computer and system sciences table power consumption and resource wastage of vmpacs and two single objective algorithms reference value corr algorithm power w wastage r p r m ffd saco vmpacs 38 86 348 ffd saco vmpacs 80 57 072 ffd saco vmpacs 371 ffd saco vmpacs 755 ffd saco vmpacs 755 ffd saco vmpacs 56 374 ffd saco vmpacs 42 ffd saco vmpacs 92 57 85 398 ffd saco vmpacs 95 48 751 ffd saco vmpacs 37 52 r p r m the reserved time of a non dominated solution in the external set and simultaneously incorporates continuous updating of pheromone therefore it can ﬁnd more appropriate vm placement and achieve better performance comparison of vmpacs with two single objective approaches in this set of experiments we compared the proposed approach with two single objective algorithms an ffd algorithm and a saco approach saco is a modiﬁed mmas algorithm for vm placement ffd considers vms in a decreasing order of utilization of a certain resource and places each vm into the ﬁrst host that has enough resource remaining table compares the total resource wastage and power consumption for each of the algorithms under consideration from the table we can see that ffd yields the highest power consumption and resource wastage because it tends to use a larger number of servers compared with other algorithms vmpacs produces the lowest power consumption and resource wastage because it is able to search the solution space more eﬃciently and globally so that it can ﬁnd the solutions with a smaller number of used servers and high resource utilization compared with ffd and saco the power consumption and resource wastage of saco are between those of the other two the reason is that saco can ﬁnd solutions with a smaller number of used servers compared with ffd and a larger number of used servers compared with vmpacs scalability of vmpacs in this subsection we provide experimental results about whether the proposed algorithm is scalable to larger data centers and more vm requests in the experiment we ﬁx p and change the number of vm requests from to and set both r p and r m to and then fig shows the result of the experiment conducted for instances with r p r m and r p r m we can see from the graph that in the case of r p r m it takes seconds to calculate a new placement of vms but the running time increases to seconds when we increase the number of vms to however in the case of r p r m we can see that it increases faster than for r p r m it takes 36 and seconds to calculate a new placement for and vms with r p r m the reason for two different results with the same number of vms is that the number of servers used to contain vms differs in each case on average vms per server in the case of r p r m and two vms in the case of r p r m this experiment y gao et al journal of computer and system sciences 1242 fig vmpacs algorithm running time relative to number of vm requests shows that our algorithm takes less than minutes to solve a diﬃcult placement problem with up to vms therefore we can say that our algorithm is suitable for large data centers conclusion with the increasing prevalence of large scale cloud computing environments how to eﬃciently place vms into available computing servers has become an essential research problem in this paper we propose a multi objective ant colony system algorithm for the virtual machine placement problem the goal is to eﬃciently obtain a set of non dominated solutions that simultaneously minimizes total resource wastage and power consumption the proposed algorithm is tested with some instances from the literature its solution performance is compared to that of an existing multi objective grouping genetic algorithm the results demonstrate that our algorithm is competitive we also compare our algorithm with two singleobjective approaches the comparison shows that vmpacs is superior to those algorithms finally the scalability of the proposed algorithm is veriﬁed by means of several experiments hidden markov models hmms are one of the most fundamental and widely used statistical tools for modeling discrete time series in general learning hmms from data is computationally hard under cryptographic assumptions and practitioners typically resort to search heuristics which suffer from the usual local optima issues we prove that under a natural separation condition bounds on the smallest singular value of the hmm parameters there is an eﬃcient and provably correct algorithm for learning hmms the sample complexity of the algorithm does not explicitly depend on the number of distinct discrete observations it implicitly depends on this quantity through spectral properties of the underlying hmm this makes the algorithm particularly applicable to settings with a large number of observations such as those in natural language processing where the space of observation is sometimes the words in a language the algorithm is also simple employing only a singular value decomposition and matrix multiplications elsevier inc all rights reserved introduction hidden markov models hmms are the workhorse statistical model for discrete time series with widely diverse applications including automatic speech recognition natural language processing nlp and genomic sequence modeling in this model a discrete hidden state evolves according to some markovian dynamics and observations at a particular time depend only on the hidden state at that time the learning problem is to estimate the model only with observation samples from the underlying distribution thus far the predominant learning algorithms have been local search heuristics such as the baum welch em algorithm it is not surprising that practical algorithms have resorted to heuristics as the general learning problem has been shown to be hard under cryptographic assumptions fortunately the hardness results are for hmms that seem divorced from those that we are likely to encounter in practical applications the situation is in many ways analogous to learning mixture distributions with samples from the underlying distribution there the general problem is also believed to be hard however much recent progress has been made when certain separation assumptions are made with respect to the component mixture distributions e g roughly speaking these separation assumptions imply that with high probability given a point sampled from the distribution one can determine the mixture component that generated the point in fact there is a prevalent sentiment that we are often only interested in clustering when such a separation condition holds much of the theoretical work here has focused on how small this separation can be and still permit an eﬃcient algorithm to recover the model we present a simple and eﬃcient algorithm for learning hmms under a certain natural separation condition we provide two results for learning the ﬁrst is that we can approximate the joint distribution over observation sequences of corresponding author e mail addresses djhsu rci rutgers edu d hsu skakade wharton upenn edu s m kakade tongz rci rutgers edu t zhang see front matter doi j jcss elsevier inc all rights reserved d hsu et al journal of computer and system sciences length t here the quality of approximation is measured by total variation distance as t increases the approximation quality degrades polynomially our second result is on approximating the conditional distribution over a future observation conditioned on some history of observations we show that this error is asymptotically bounded i e for any t conditioned on the observations prior to time t the error in predicting the t th outcome is controlled our algorithm can be thought of as improperly learning an hmm in that we do not explicitly recover the transition and observation models however our model does maintain a hidden state representation which is closely in fact linearly related to the hmm and can be used for interpreting the hidden state the separation condition we require is a spectral condition on both the observation matrix and the transition matrix roughly speaking we require that the observation distributions arising from distinct hidden states be distinct which we formalize by singular value conditions on the observation matrix this requirement can be thought of as being weaker than the separation condition for clustering in that the observation distributions can overlap quite a bit given one observation we do not necessarily have the information to determine which hidden state it was generated from unlike in the clustering literature we also have a spectral condition on the correlation between adjacent observations we believe both of these conditions to be quite reasonable in many practical applications furthermore given our analysis extensions to our algorithm which relax these assumptions should be possible the algorithm we present has both polynomial sample and computational complexity computationally the algorithm is quite simple at its core is a singular value decomposition svd of a correlation matrix between past and future observations this svd can be viewed as a canonical correlation analysis cca between past and future observations the sample complexity results we present do not explicitly depend on the number of distinct observations rather they implicitly depend on this number through spectral properties of the hmm this makes the algorithm particularly applicable to settings with a large number of observations such as those in nlp where the space of observations is sometimes the words in a language related work there are two ideas closely related to this work the ﬁrst comes from the subspace identiﬁcation literature in control theory the second idea is that rather than explicitly modeling the hidden states we can represent the probabilities of sequences of observations as products of matrix observation operators an idea which dates back to the literature on multiplicity automata the subspace identiﬁcation methods used in control theory use spectral approaches to discover the relationship between hidden states and the observations in this literature the relationship is discovered for linear dynamical systems such as kalman ﬁlters the basic idea is that the relationship between observations and hidden states can often be discovered by spectral svd methods correlating the past and future observations in particular such methods often do a cca between the past and future observations however algorithms presented in the literature cannot be directly used to learn hmms because they assume additive noise models with noise distributions independent of the underlying states and such models are not suitable for hmms an exception is in our setting we use this idea of performing a cca between past and future observations to uncover information about the observation process this is done through an svd on a correlation matrix between past and future observations the state independent additive noise condition is avoided through the second idea the second idea is that we can represent the probability of sequences as products of matrix operators as in the literature on multiplicity automata see for discussion of this relationship this idea was re used in both the observable operator model of jaeger and the predictive state representations of littman et al both of which are closely related and both of which can model hmms in fact the former work by jaeger provides a non iterative algorithm for learning hmms with an asymptotic analysis however this algorithm assumed knowing a set of characteristic events which is a rather strong assumption that effectively reveals some relationship between the hidden states and observations in our algorithm this problem is avoided through the ﬁrst idea some of the techniques in the work in for tracking belief states in an hmm are used here as discussed earlier we provide a result showing how the model conditional distributions over observations conditioned on a history do not asymptotically diverge this result was proven in when an approximate model is already known roughly speaking the reason this error does not diverge is that the previous observations are always revealing information about the next observation so with some appropriate contraction property we would not expect our errors to diverge our work borrows from this contraction analysis among recent efforts in various communities 33 the only previous eﬃcient algorithm shown to pac learn hmms in a setting similar to ours is due to their algorithm for hmms is a specialization of a more general method for learning phylogenetic trees from leaf observations while both this algorithm and ours rely on the same rank condition and compute similar statistics they differ in two signiﬁcant regards first mossel and roch were not concerned with large observation spaces and thus their algorithm assumes the state and observation spaces to have the same dimension in addition mossel and roch take the more ambitious approach of learning the observation and transition matrices explicitly which unfortunately results in a less sample eﬃcient algorithm that injects noise to artiﬁcially spread apart the d hsu et al journal of computer and system sciences eigenspectrum of a probability matrix our algorithm avoids recovering the observation and transition matrix explicitly and instead uses subspace identiﬁcation to learn an alternative representation preliminaries hidden markov models the hmm deﬁnes a probability distribution over sequences of hidden states ht and observations xt we write the set of hidden states as m m and set of observations as n n where m n let t rm m be the state transition probability matrix with t i j pr ht i ht j o rn m be the observation prob rm be the initial state distribution with π i pr i the conditional ability matrix with o i j pr xt i ht j and π independence properties that an hmm satisﬁes are conditioned on the previous hidden state the current hidden state is sampled independently of all other events in the history and conditioned on the current hidden state the current observation is sampled independently from all other events in the history these conditional independence properties of the hmm imply that t and o fully characterize the probability distribution of any sequence of states and observations a useful way of computing the probability of sequences is in terms of observation operators an idea which dates back to the literature on multiplicity automata see the following lemma is straightforward to verify see lemma for x n deﬁne a x t diag o x o x m for any t m pr xt a xt a π our algorithm learns a representation that is based on this observable operator view of hmms notation m is the all ones vector in rm we denote by t the sequence xt as already used in lemma the vector and by xt its reverse xt when we use a sequence as a subscript we mean the product of quantities indexed by a m we will use h t the sequence elements so for example the probability calculation in lemma can be written xt π to denote a probability vector a distribution over hidden states with the arrow distinguishing it from the random hidden state variable ht additional notation used in the theorem statements and proofs is listed in table assumptions we assume the hmm obeys the following condition element wise and o and t are rank m condition hmm rank condition π the rank condition rules out the problematic case in which some state i has an output distribution equal to a convex combination mixture of some other states output distributions such a case could cause a learner to confuse state i with a mixture of these other states as mentioned before the general task of learning hmms even the speciﬁc goal of simply accurately modeling the distribution probabilities is hard under cryptographic assumptions the rank condition is a natural way to exclude the malicious instances created by the hardness reduction the rank condition on o can be relaxed through a simple modiﬁcation of our algorithm that looks at multiple observation symbols simultaneously to form the probability estimation tables for example if two hidden states have identical observation probability in o but different transition probabilities in t then they may be differentiated by using two consecutive observations although our analysis can be applied in this case with minimal modiﬁcations for clarity we only state our results for an algorithm that estimates probability tables with rows and columns corresponding to single observations learning model our learning model is similar to those of for pac learning discrete probability distributions we assume we can sample observation sequences from an hmm in particular we assume each sequence is generated starting from the in appendix c we discuss the key step in and also show how to use their technique in conjunction with our algorithm to recover the hmm observation and transition matrices our algorithm does not rely on this extra step we believe it to be generally unstable but it can be taken if desired d hsu et al journal of computer and system sciences same initial state distribution e g the stationary distribution of the markov chain speciﬁed by t this setting is valid for practical applications including speech recognition natural language processing and dna sequence modeling where multiple independent sequences are available for simplicity this paper only analyzes an algorithm that uses the initial few observations of each sequence and ignores the rest we do this to avoid using concentration bounds with complicated mixing conditions for markov chains in our sample complexity calculation as these conditions are not essential to the main ideas we present in practice however one should use the full sequences to form the probability estimation tables required by our algorithm in such scenarios a single long sequence is suﬃcient for learning and the effective sample size can be simply discounted by the mixing rate of the underlying markov chain our goal is to derive accurate estimators for the cumulative joint distribution pr t and the conditional distribution pr xt t for any sequence length t for the conditional distribution we obtain an approximation that does not depend on t while for the joint distribution the approximation quality degrades gracefully with t observable representations of hidden markov models a typical strategy for learning hmms is to estimate the observation and transition probabilities for each hidden state say by maximizing the likelihood of a sample however since the hidden states are not directly observed by the learner one often resorts to heuristics e g em that alternate between imputing the hidden states and selecting parameters o and t that maximize the likelihood of the sample and current state estimates such heuristics can suffer from local optima issues and require careful initialization e g an accurate guess of the hidden states to avoid failure however under condition hmms admit an eﬃciently learnable parameterization that depends only on observable quantities because such quantities can be estimated from data learning this representation avoids any guesswork about the hidden states and thus allows for algorithms with strong guarantees of success this parameterization is natural in the context of observable operator models but here we emphasize its connection to subspace identiﬁcation deﬁnition our hmm representation is deﬁned in terms of the following vector and matrix quantities p i pr i p i j pr i j p x i j pr i x j x n where p rn is a vector and p rn n and the p x rn n are matrices these are the marginal probabilities of observation singletons pairs and triples the representation further depends on a matrix u rn m that obeys the following condition condition invertibility condition u o is invertible in other words u deﬁnes an m dimensional subspace that preserves the state dynamics this will become evident in the next few lemmas a natural choice for u is given by the thin svd of p as the next lemma exhibits and that o and t have column rank m then rank p m moreover if u is the matrix of left singular lemma assume π vectors of p corresponding to non zero singular values then range u range o so u rn m obeys condition proof using the conditional independence properties of the hmm entries of the matrix p can be factored as p i j m m pr i j k k m m o j o ik t k π k o and thus range p ot diag π imply that t diag π o has so p range o the assumptions on o t and π linearly independent rows and that p has m non zero singular values therefore o o p t diag π where x denotes the moore penrose pseudo inverse of a matrix x which in turn implies range o range p thus rank p rank o m and also range u range p range o d hsu et al journal of computer and system sciences our algorithm is motivated by lemma in that we compute the svd of an empirical estimate of p to discover a u that satisﬁes condition we also note that this choice for u can be thought of as a surrogate for the observation matrix o see remark now given such a matrix u we can ﬁnally deﬁne the observable representation u p b p u b p b x u p x u p x n basic properties b b b n is suﬃcient to the following lemma shows that the observable representation parameterized by b compute the probabilities of any sequence of observations lemma observable hmm representation assume the hmm obeys condition and that u rn m obeys condition then u o π b u o b m b x u o a x u o x n b x b t n xt n pr t b t in addition to joint probabilities we can compute conditional probabilities using the observable representation we do so through normalized conditional internal states that depend on a history of observations we should emphasize that these states are not in fact probability distributions over hidden states though the following lemma shows that they are linearly related as per lemma the initial state is u o π b generally for any t given observations t with pr t we deﬁne the internal state as t b t t b b xt b b x b t b t because the denominator is b b u o u o π the case t is consistent with the general deﬁnition of b m the following result shows how these internal states can be used to compute conditional probabilities π pr xt i t lemma conditional internal states assume the conditions in lemma then for any time t recursive update of states if pr t then t b t b xt b b x b t b t relation to hidden states t u o h t t b t t i pr ht i t is the conditional probability of the hidden state at time t given the observations t where h conditional observation probabilities b x b t pr xt t b t remark if u is the matrix of left singular vectors of p corresponding to non zero singular values then u acts much like the observation probability matrix o in the following sense t given a conditional state b t given a conditional hidden state h t i pr xt i t u b t i pr xt i t o h to see this note that u u is the projection operator to range u since range u range o lemma we have t u u o h t o h t u u o o so u b d hsu et al journal of computer and system sciences proofs for the second claim we write p in the following proof of lemma the ﬁrst claim is immediate from the fact p o π unusual but easily veriﬁed form m o m p t diag π u o o m u o t diag π u o u p o t and the condition on u so the matrix u p has linearly independent rows by the assumptions on π p u p u o u p u p u o b m m to prove the third claim we ﬁrst express p x in terms of a x o o ax u o p x o a x t diag π o o ax u o u o t diag π u p again using the fact that u p has full row rank b x u p x u p u o ax u o u p u p u o a x u o the probability calculation in the fourth claim is now readily seen as a telescoping product that reduces to the product in lemma proof of lemma the ﬁrst claim is a simple induction the second and third claims are also proved by induction as π pr for and b u o π and also b follows the base case is clear from lemma since h b b a π the inductive step t b t b xt b b x b t b t t b xt u o h inductive hypothesis pr xt t u o a xt h t lemma pr xt t pr ht xt t u o pr xt t pr ht t pr xt t u o pr xt t u o ht t and b x b t a x h t pr xt t b t m t again using lemma spectral learning of hidden markov models algorithm the representation in the previous section suggests the algorithm detailed in fig which simply uses random samples to estimate the model parameters note that in practice knowing m is not essential because the method presented here tolerates models that are not exactly hmms and the parameter m may be tuned using cross validation as we discussed earlier the requirement for independent samples is only for the convenience of our sample complexity analysis the model returned by learnhmm m n can be used as follows to predict the probability of a sequence xt b pr b xt b b given an observation xt the internal state update is b t b xt b t b b xt b t d hsu et al journal of computer and system sciences algorithm learnhmm m n inputs m number of states n sample size returns hmm model parameterized by b b b x x n independently sample n observation triples from the hmm to form empirical estimates p p p x x n of p p p x x n compute the svd of p and let u be the matrix of left singular vectors corresponding to the m largest singular values compute model parameters a b u p b b p u p c b x u p x u p x n fig hmm learning algorithm to predict the conditional probability of xt given t xt t b b xt b t pr x b b x b t aside from the random sampling the running time of the learning algorithm is dominated by the svd computation of an n n matrix the time required for computing joint probability calculations is o for length t sequences same as if one used the ordinary hmm parameters o and t for conditional probabilities we require some extra work proportional to n to compute the normalization factor however our analysis shows that this normalization factor is always close to see lemma so it can be safely omitted in many applications note that the algorithm does not explicitly ensure that the predicted probabilities lie in the range this is a dreaded problem that has been faced by other methods for learning and using general operator models and a number of heuristic for coping with the problem have been proposed and may be applicable here see for some recent developments we brieﬂy mention that in the case of joint probability prediction clipping the predictions to the interval can only increase the l accuracy and that the kl accuracy guarantee explicitly requires the predicted probabilities to be non zero main results we now present our main results the ﬁrst result is a guarantee on the accuracy of our joint probability estimates for observation sequences the second result concerns the accuracy of conditional probability estimates a much more delicate quantity to bound due to conditioning on unlikely events we also remark that if the probability distribution is only approximately modeled as an hmm then our results degrade gracefully based on this approximation quality joint probability accuracy let σm m denote the m th largest singular value of a matrix m our sample complexity bound will depend polynomially on σm p and σm o also deﬁne k min pr j s n s n k j s and let ε min k k ε in other words ε is the minimum number of observations that account for about of the total probability mass clearly ε n but it can often be much smaller in real applications for example in many practical applications the frequencies of observation symbols observe a power law called zipf law of the form f k k where f k is the frequency of the k th most frequently observed symbol if then k o and ε o ε becomes independent of the number of observations n this means that for such problems our analysis below leads to a sample complexity bound for the cumulative distribution pr t that can be independent of n this is useful in domains with large n such as natural language processing theorem there exists a constant c such that the following holds pick any σm o σm p m assume the hmm obeys condition and n c m σm o σm p m σm o σm p log η η and t and let d hsu et al journal of computer and system sciences with probability at least η the model returned by the algorithm learnhmm m n satisﬁes pr xt pr xt xt the main challenge in proving theorem is understanding how the estimation errors accumulate in the algorithm probability calculation this would have been less problematic if we had estimates of the usual hmm parameters t and o the fully observable representation forces us to deal with more cumbersome matrix and vector products conditional probability accuracy xt xt intuitively we in this section we analyze the accuracy of our conditional probability predictions pr might hope that these predictive distributions do not become arbitrarily bad over time as t the reason is that while estimation errors propagate into long term probability predictions as evident in theorem the history of observations constantly provides feedback about the underlying hidden state and this information is incorporated using bayes rule implicitly via our internal state updates this intuition was conﬁrmed by eyal et al who showed that if one has an approximate model of t and o for the hmm then under certain conditions the conditional prediction does not diverge this condition is the positivity of the value of observation γ deﬁned as γ inf v v o v note that γ σm o n so it is guaranteed to be positive by condition however γ can be much larger than what this crude lower bound suggests h rm then o h h γ h h to interpret this quantity γ consider any two distributions over hidden states h as the true hidden state distribution and h as the estimated hidden state distribution this inequality gives a regarding h lower bound on the error of the estimated observation distributions under o in other words the observation process on average reveal errors in our hidden state estimation the work of eyal et al uses this as a contraction property to show how prediction errors due to using an approximate model do not diverge in our setting this is more diﬃcult as we do not explicitly estimate o nor do we explicitly maintain distributions over hidden states we also need the following assumption which we discuss further following the theorem statement condition stochasticity condition for all observations x and all states i and j a x i j α theorem there exists a constant c such that the following holds pick any η and let σm o σm p m assume the hmm obeys conditions and and n c m α log α α γ m σm o σm p m σm o σm p log η with probability at least η then the model returned by learnhmm m n satisﬁes for any time t kl pr xt xt pr xt xt t ln pr xt t xt t pr to justify our choice of error measure note that the problem of bounding the errors of conditional probabilities is complicated by the issue of that over the long run we may have to condition on a very low probability event thus we need to control the relative accuracy of our predictions this makes the kl divergence a natural choice for the error measure unfortunately because our hmm conditions are more naturally interpreted in terms of spectral and normed quantities we end up switching back and forth between kl and l errors via pinsker style inequalities as in it is not clear to us if a signiﬁcantly better guarantee could be obtained with a pure l error analysis nor is it clear how to do such an analysis the analysis in which assumed that approximations to t and o were provided dealt with this problem of dividing by zero during a bayes rule update by explicitly modifying the approximate model so that it never assigns the probability of any event to be zero since if this event occurred then the conditional probability is no longer deﬁned in our setting condition ensures that true model never assigns the probability of any event to be zero we can relax this condition somewhat so that we need not quantify over all observations though we do not discuss this here we should also remark that while our sample complexity bound is signiﬁcantly larger than in theorem we are also bounding the more stringent kl error measure on conditional distributions d hsu et al journal of computer and system sciences 1480 table summary of notation m n ε o t ax p p p x p p p x x u b b x b number of states and observations number of signiﬁcant observations hmm parameters marginal probabilities empirical marginal probabilities sampling errors section matrix of m left singular vectors of p true observable parameters using u section b b x b δ x estimated observable parameters using u parameter errors section x x section m th largest singular value of matrix m σm m t b t b true and estimated states section t h t g t h a x γ α h section u o b t u o b t h t m t u o b x u o section inf o v v min a x i j learning distributions close to hmms our l error guarantee for predicting joint probabilities still holds if the sample used to estimate p p p x come from a probability distribution pr that is merely close to an hmm speciﬁcally all we need is that there exists some t max and some m state hmm with distribution prhmm such that prhmm satisﬁes condition hmm rank condition hmm pr x t hmm t for all t t max t pr t hmm σm p is the resulting error of our learned model pr pr t pr prhmm t pr t hmm t t t t for all t t max the second term is now bounded as in theorem with spectral parameters corresponding to prhmm subsequent work following the initial publication of this work siddiqi boots and gordon have proposed various extensions to the learnhmm algorithm and its analysis first they show that the model parameterization used by our algorithm in fact captures the class of hmms with rank m transition matrices which is more general than the class of hmms with m hidden states second they propose extensions for using longer sequences in the parameter estimation and also for handling real valued observations these extensions prove to be useful in both synthetic experiments and an application to tracking with video data a recent work of song boots siddiqi gordon and smola provides a kernelization of our model parameterization in the context of hilbert space embeddings of conditional probability distributions and extends various aspects of the learnhmm algorithm and analysis to this setting this extension is also shown to be advantageous in a number of applications proofs throughout this section we assume the hmm obeys condition table summarizes the notation that will be used throughout the analysis in this section estimation errors deﬁne the following sampling error quantities p p p p x p x p x d hsu et al journal of computer and system sciences 1480 the following lemma bounds these errors with high probability as a function of the number of observation samples used to form the estimates lemma if the algorithm independently samples n observation triples from the hmm then with probability at least η n ln n η ln η n x ln x min η k n k x n n max x ln n η k n k ln n η n where k is deﬁned in proof see appendix a the rest of the analysis estimates how the sampling errors affect the accuracies of the model parameters which in turn affect the prediction quality we need some results from matrix perturbation theory which are given in appendix b let u rn m be matrix of left singular vectors of p the ﬁrst lemma implies that if p is suﬃciently close to p i e is small enough then the difference between projecting to range u and to range u is small in particular u o will be invertible and be nearly as well conditioned as u o lemma suppose ε σm p for some ε let ε σm p then with the gaining popularity of remote storage e g in the cloud we consider the setting where a small protected local machine wishes to access data on a large untrusted remote machine this setting was introduced in the ram model in the context of software protection by goldreich and ostrovsky a secure oblivious ram simulation allows for a client with small e g constant size protected memory to hide not only the data but also the sequence of locations it accesses both reads and writes in the unprotected memory of size n our main results are as follows we analyze several schemes from the literature observing a repeated design flaw that leaks information on the memory access pattern for some of these schemes the leakage is actually non negligible while for others it is negligible on the positive side we present a new secure oblivious ram scheme extending a recent scheme by goodrich and mitzenmacher our scheme uses only o local memory and its amortized overhead is o n log log n outperforming the previously best o n overhead among schemes where the client only uses o additional local memory computer science department technion supported by isf grant and bsf grant e mail eyalk cs technion ac il stealth software technologies inc e mail steve stealthsoftwareinc com department of computer science and department of mathematics ucla this material is based upon work supported in part by nsf grants 1118126 and us israel bsf grant grants from okawa foundation ibm lockheed martin corporation and the defense advanced research projects agency through the u s office of naval research under contract the views expressed are those of the author and do not reflect the official policy or position of the department of defense or the u s government e mail rafail cs ucla edu rafail ostrovsky we also present a transformation of our scheme above whose amortized overhead is o n log log n into a scheme with worst case overhead of o n log log n keywords oblivious ram cuckoo hashing secure computation introduction consider the following problem a small protected cpu wants to run a program that requires access to unprotected ram without revealing anything but the running time and the size of memory used by the program encryption can be employed to hide the contents of the memory cells accessed by the program but the sequence of read and write locations made to the ram may leak information as well the notion of oblivious ram was introduced by goldreich as a means to hide both the contents and the so called access pattern of memory probes this can be viewed as casting into the ram model the work on oblivious simulation of turing machines the oblivious ram model also fits the setting of a client that stores its data in a remote storage server and has various other applications see e g in previous work the initial results on oblivious ram were presented by goldreich and featured two solutions a square root solution with low round complexity and a recursive square root solution in the square root solution the time overhead for executing the program increases by a factor of o n while in the recursive solution the time overhead in creases by o log n log log n factor where n is the number of memory locations used by the original program ostrovsky proposed a different oblivious simulation for which the time requirement is increased only by a factor of o min n t where t is the program running time ostrovsky simulation algorithm is referred to as the hierarchical algorithm because of the nature of the data structure used by the ram the high level description of ostrovsky hierarchical solution can be stated as follows there is a sequence of buffers whose size grow at a geometric rate and copyright siam unauthorized reproduction of this article is prohibited smaller buffers are reshuffled into larger ones as they fill up in the original work of the buffers were standard hash tables with sufficiently large buckets and it employed a technique known as oblivious shuffling for the reshuffles a slightly different somewhat simpler reshuffling methods was proposed by goldreich and ostrovsky though the asymptotic behavior of reshuffles in and in is the same in all of the above solutions to protect the data contents a private key encryption and authentication scheme was used and each time elements were read and written back to the insecure ram they would be re encrypted and authenticated the efficiency of oblivious ram is measured by three main parameters the amount of local client storage the amount of remote server storage and the amortized overhead of reading or writing an element goldreich used sub linear local storage while all used a constant amount of local storage ostrovsky hierarchical solution used o n log n remote storage and offered two different ways to perform oblivious shuffling that led to either o n access overhead with a small hidden constant or o n access overhead with a large hidden constant subsequent works improved upon ostrovsky hierarchical solution williams and sion introduced a method to perform oblivious sorting by using o n local memory to reduce the access overhead down to o n but with super linear remote storage subsequently williams et al made use of bloom filters to further reduce the overhead to o log n log log n and the remote storage to o n but as we shall see this efficiency improvement allows an adversary to obtain information about the access pattern recent works of pinkas and reinman goodrich and mitzenmacher and goodrich et al investigated the usage of cuckoo hashing to further improve parameters we will show that cuckoo hashing may potentially lead to a severe reduction in security in particular we explicitly construct an adversary that breaks the security of the scheme of however when used correctly cuckoo hashing can result in secure schemes as seen in in a scheme was constructed with constant local storage o n remote storage and o n access overhead in a scheme using cuckoo hashing was given with similar parameters as well as an alternative scheme that used o nε local storage with o log n overhead which has comparable parameters to recently boneh mazieres and popa extended the notion of oblivious ram and described a new scheme is a journal version that merged results of with low round complexity analogous to goldreich square root solution of low round complexity this new scheme introduces a block size b so that the remote storage has size roughly bn storing n elements the local memory size is an adjustable p parameter for which two possible values b n and bn log bn were suggested and the access overhead with these memory q sizes is o bn log bn and o n log bn respectively n the key feature is that the amortized round complexity is logarithmic or constant we give a more detailed review of the most relevant schemes in section another interesting question is that of obtaining schemes with worst case rather than amortized overhead this question was recently studied by goodrich et al and shi et al goodrich et al constructed two schemes one of which used o n client memory to achieve o log n worst case overhead the other used constant client memory and showed how to extend the square root solution of to achieve o n n worst case overhead shi et al constructed a scheme with o n worst case overhead with constant client memory we consider the worstcase overhead measure for our construction in section the works of ajtai and damga rd meldgaard and nielsen provide a solution to oblivious ram with information theoretic security in a restricted model where adversary is not allowed to read memory contents these solutions result in poly logarithmic access overhead and operate in a slightly different model which only considers hiding the access pattern and not the data itself to compare these to existing schemes in the standard model of we can encrypt the data with no additional asymptotic overhead to hide the data as well although it is the case that these converted schemes have worse performance than existing schemes in the standard model the comparison is unfair as these schemes were created as a trade off between hiding the data and achieving information theoretic hiding of the access pattern we also mention the related work of boneh kushilevitz ostrovsky and skeith with amortized improvements in that describes a pir writing scheme in a public key setting the scheme allows for private storage and retrieval of public key encrypted data but the key difference is that while the communication complexity of the scheme is sub linear specif ically o n log n and the round complexity is small the server must perform a linear amount of work on the database pir writing allows users without the private key to write into the database with hiding the read write pattern but the model is more taxing than oblivious ram schemes that only require a sub linear amount of computation for the database copyright siam unauthorized reproduction of this article is prohibited our contribution in this paper we point out a flaw regarding the way hash overflows are dealt with in previous schemes the idea is as follows after the end of a reshuffle the server knows that the data stored at a level can be inserted into the hash table at that level without an overflow while this simple observation might not seem very useful it could actually reveal a significant amount of information for a concrete example when using cuckoo hashing it is known that no three elements a b c have the property that a b c and a b c otherwise they could not have been inserted into the table however during subsequent probes x y z to that hash table for elements x y z which are actually not in the table with some noticeable probability we may have x y z and x y z in such a case the server immediately knows that x y z cannot all live inside that table this can be used to distinguish two sequences of memory accesses one in which only elements living at this hash table are read and one which reads elements that do not live at this level if the table is of constant size this lead to a distinguisher with constant success probability we and independently discovered a flaw in that is due to this issue in section we demonstrate this flaw by constructing an actual adversary that can distinguish between two read write sequences with constant probability believing that there was a deeper cause to this issue we further investigated how hash overflows affected other oblivious ram schemes we uncover previously undiscovered flaws that we believe to be the root of the problem and present these in section we summarize the impact of the hash overflow issue on other existing schemes there is a flaw in the proof of lemma in however in this case the flaw has only a negligible impact on the relevant probabilities and so the overall security of the scheme remains intact same holds for claim a similar flaw appears also in the proof of theorem of in this case however the impact is significant and the scheme turns out to be insecure the schemes in also rehash their tables if a bucket overflows the security proofs in these papers note that this only happens with negligible probability the scheme in does not explicitly specify a hash function because it uses a bloom filter to determine if an element is in a table before querying it however if the bloom filter returns a false positive with an inverse polynomial probability and a standard hash algorithm such as bucket hashing is used then there exists an adversary with a nonnegligible chance of breaking the scheme for a careful analysis of this scheme see one of the schemes in uses constant local memory and o log n sized stashes for cuckoo hashing the probability of overflow in this case was shown to be negligible so the security is not impacted two other variants one with constant local memory and one with o n local memory use a constant size stash for cuckoo hashing the probability of overflow for these hash tables are an inverse polynomial where the degree is proportional to the size of the stash and results in a non negligible advantage for a distinguisher we emphasize that we refer to just to give another illustration of the hashing issue indeed in a newer version of that work constant sized stashes are no longer used and so there are no security problems we then ask the question is there a secure oblivious ram that has amortized access overhead o n with constant local memory the work of offers a solution with access overhead o log n log log n using o n local storage but based on the accurate analysis of and our newly discovered flaws regarding hash overflows the distinguishing advantage of an adversary may be non negligible the recent results of show that it is possible to achieve a secure oblivious ram with o log n access overhead using o nε local storage we answer the above question in the affirmative by presenting a scheme with o n log log n access overhead while maintaining optimal o local storage and optimal o n remote storage to accomplish this we observe that the access overhead consists of two types scan overhead and reshuffle overhead in several known schemes there is an imbalance between these two quantities and by carefully balancing the size of each level we can asymptotically reduce the overall access overhead the starting point for our scheme is the secure scheme of that uses o local memory a stash of size o log n and has o n access overhead we apply two modifications to it we reduce the scan overhead and we balance the cost of a scan and a reshuffle by increasing the sizes of the levels but reducing the overall number of levels our construction also uses a mechanism that may be seen as a version of the shared stash idea from namely one stash that is common to all buffers and accumulates a limited number of elements that cannot be stored in the buffers due to unlucky choice of hash functions except that copyright siam unauthorized reproduction of this article is prohibited in our case this stash is virtual and its elements are actually re inserted into the data structure our scheme is described in section some of the previous work on which we rely is described in section finally because the overhead is amortized processing some queries may take much longer than others although these times are known in advance and do not reveal any information it could have a negative impact on practical applications that require each query to be performed quickly the question of achieving worstcase overhead for oblivious ram was recently considered by goodrich et al and shi et al we focus on the case where the client has constant memory in it is shown how to extend the squareroot solution of so that it achieves o n n worst case overhead in section we describe a technique where we increase the memory size at each level by a constant factor and we schedule the operations in a way that spreads out the amortization as a warmup we first show how this technique can be applied to the original hierarchical solution to obtain a scheme with o n worst case overhead then we describe the necessary additional modifications for our new scheme to achieve an oblivious ram scheme with o n log log n worst case overhead using constant client memory without increasing the asymptotic server memory preliminaries ram model the random access machine ram model consists of a cpu and a memory storage of size n the cpu executes a program that can access the memory by using read r and write r val operations where r n is an index to a memory location the access pattern of the program is the sequence of reads and writes it makes to the memory including both the data and the memory locations in an actual implementation of a ram the cpu is a protected local device possibly probabilistic that internally has a small number of registers performs basic arithmetic operations and has access to an external device that simulates the memory e g a physical memory device we say that a simulation is a secure oblivious ram if for any two access patterns in ideal ram the corresponding access patterns in the simulation are computationally indistinguishable hash functions throughout this paper as well as in prior works a hash function is considered to be a random oracle or a pseudo random function family fk the domain of this hash function is typically taken to be n along with some dummy values and the range of these hash functions are indices of hashtables of size at most o n the key property that we need from these hash functions are that they behave like independent random functions rather than any particular cryptographic feature such as resistance to finding pre images these hash functions are used for the hash tables standard hashing with buckets or cuckoo hashing as described next in the hierarchical solution cuckoo hashing cuckoo hashing is a hash table data structure introduced by pagh and rodler the salient property of cuckoo hashing is that an element x can only live in one of two locations the scheme uses two hash functions and and x can only reside in x or x when an element is inserted it is always inserted in the first location kicking out any previous occupant the kicked out element is then moved to its other location possibly kicking out another element this process continues until no element is kicked out or it runs too long in which case new hash functions are chosen and the entire table is rehashed it was shown that if the number of elements is at most a constant fraction say of the table size then the probability of requiring a rehash is small enough so that an insert takes o amortized time subsequent works such as have shown how to de amortize cuckoo hashing an important variant of cuckoo hashing is cuckoo hashing with a stash introduced by kirsch et al a stash of size is added to the cuckoo hash table to deal with overflows the work shows that if a constant size stash is used then inserting n elements into a cuckoo hash table of θ n size will succeed with all but o n probability goodrich and mitzenmacher showed that if the size of the table is m ω n and the stash size is o log n then the probability of success in inserting m elements into the table is all but negligible in n we note that although in general cuckoo hashtables are dynamic for oblivious ram we only use them in a static manner i e all the elements to be inserted into the table are always known ahead of time bloom filters a bloom filter is a datastructure to determine set membership it consists of an array of bit flags all initially set to zeros along with a set of hash functions hn when element x is being inserted compute x hn x and set the bit flags corresponding to those locations to when we want to test whether or not some element is present we compute hn and check if all bit flags are set to this method of testing can possibly lead to false positives as we shall see in section this can lead to insecurity in oblivious ram schemes that employ bloom filters copyright siam unauthorized reproduction of this article is prohibited overview of previous schemes in this section we give an overview of some of the previous schemes specifically this serves as a starting point for our scheme in section for full details we refer the reader to the original papers starting from most schemes use ostrovsky hierarchical data structure that consists of a sequence of buffers bk bk bl of geometrically increasing sizes i e bi is of size typically k o i e the first buffer is of constant size and l log n i e the last buffer may contain all n elements where n is the total number of memory locations each buffer is maintained as a hash table of one of two types the first type is where bi is a standard hash table associated with a secret hash function hi in this case originated from each entry j is actually a bucket of size b containing all the elements mapped by hi to j the second type is where bi is a table of cuckoohashing associated with a secret pair of hash functions hi hi in this case originated from no bucket is needed it should be noted that with a certain small probability the chosen hash functions do not allow for legal storage of the elements into the data structure in such a case the schemes either fail if this event is sufficiently rare or simply re choose the problematic hashfunctions as we shall see below those buffers are static data structures and so all the elements to be stored in them are known in advance in section below we show that this failure event actually leaks information that can be utilized by an attacker a cuckoo hashtable with stash of carefully chosen parameters takes care of these bad events finally it is also possible to use an hybrid scheme where for buffers bk bk we use standard hashing while for bk bl we use cuckoo hashing with stash reading an element when reading an element from the hierarchical data structure via a read r operation we wish to hide the identity of the buffer from which we read which yields information on the sequence of operations hence we actually read from each of the at most l buffers specifically we start by reading the top smallest buffer bk in its entirety then for each i if buffer bi uses standard hashing we compute j hi r and read the entire j th bucket b elements of that buffer this is how it was done in the original hierarchical solution of if buffer bi uses cuckoo each entry of these buffers is of the form id x where id n dummy n indicating which of the original n memory locations is stored here or that this is a dummy element whose sole purpose is to help in hiding the access pattern and x is a value taken from an appropriate domain f and encrypted under a secret key hashing then we read the pair of elements hi r and hi r moreover if a stash is employed we read the entire stash corresponding to bi if element r exists in more than one buffer bi then the insertion method see below guarantees that the smallest such i for which bi contains the element r has the most updated value so once element r is found we search upon dummy locations by using dummy t instead of r where t is a local counter indicating the number of queries performed already from subsequent buffers in addition at the end of this process we re insert element r into the top buffer of the data structure see below this together with the re shuffling procedure described below guarantees that when executing future read r operations with the same r independent locations will be read from each buffer finally we remark that even if element r was not in any buffer before the read r operation it will be inserted into the top buffer insertion elements are always inserted into the top buffer bk this is done both as a result of a read operation as described above and in response to an update of entry r in memory when performing a write r val operation now every insertions buffer bi is considered full and its content is moved into the larger buffer bi more precisely we do the following after α insertions one insertion per each read and write operation where α is divisible by but not by we move all the elements from buffers bk bi there are less than such elements into buffer bi at such time step bi itself is empty for this we pick fresh random hashfunctions for bi a function hi in the case of standard hashing and functions hi and hi in the case of cuckoo hashing finally to destroy the correlation between the new locations of the elements in bi and their old locations we use oblivious hashing the re shuffling method guarantees the following property which is crucial in the security analysis of all those schemes if memory location r is accessed more than once by either a read or a write operation we never look for it in the same buffer bi with the same hash function in other words in the time until picking a fresh hash function for bi an item r should not be allowed to come back to bi and of course if r is found in a previously accessed buffer then in bi we search for a random element instead a no duplicates variant next we describe a variant of the above schemes in which the datastructure never contains more than one copy of any b may not necessarily be full but to prevent the possibility i of distinguishing for example the extreme cases one where all the insertions are distinct and the other case where all the insertions are identical we need to act the same in all cases copyright siam unauthorized reproduction of this article is prohibited element i e for every r n there is a unique element of the form r value in the data structure this idea comes from and is used there as an optional modification for efficiency in our scheme in section we rely on this variant to obtain some simplifications in this variant when performing a read r operation for reading an element r we proceed as above except that when finding r value in some buffer bi we remove it from bi and keep just the new version of r value that is inserted into the top this is done by writing into each location that we read in each buffer in most places we just write the same pair id value that we just read re encrypted to hide this fact and only in the unique place that we found the element r we write a dummy value instead when performing a write r value operation we start by performing a read operation with identifier r where as above if we find a pair r it is removed except that rather than re inserting the read value r into the top buffer we insert the new value r value the reshuffling mechanism works as before in fact re shuffling is somewhat simpler when there are no duplicates as we do not need to worry about possible conflicts a completely random pattern which is one that may not be consistent with a non overflowing hash table if this event occurs with inverse polynomial probability an adversary can distinguish between these two access patterns with a non negligible advantage we now take a look at how this affects existing schemes bucket hashing bucket hashing inherently has a chance of failure on insert due to a bucket overflowing and this probability depends on the size of the buckets in the hash table the works of rely on bucket hashing as the primary data structure in the hierarchy in these schemes it is suggested that in the case of failure new hash functions are selected and all the elements are re hashed into a new table cuckoo hashing in ordinary cuckoo hashing the probability of failure is also inverse polynomial the most obvious way an insert can fail is if the new element hashes to the exact same two locations as two previous elements since elements cannot fit into two locations in such a case new hash functions are selected and all the elements are re hashed into a new table similar to the case of bucket hashing based on this fact we give explicit examples of distinguishers for the scheme of and we point out that the cuckoo hashing collisions remain problematic even if a small constant sized stash is used such as in one of the schemes in distinguisher example for the sake of consistency it is assumed that we may only read elements that have previously been inserted the examples provided will abide by this rule we let numbers denote elements inserted letters denote dummy elements as prescribed by the scheme in and denote don t care elements as prescribed by their scheme in the following examples it does not matter how long the query sequences are and we only state the initial subsequence necessary to make the distinguishing go through we write write rn as shorthand for writing to locations through rn with arbitrary values written the construction in allows the server to know where the empty slots of the cuckoo hash table are as a warm up example demonstrating how this can leak information we show that this actually allows the server to distinguish between the two sequences write read and write read with probability after the sequence of initial writes the buffers will look like this flaws in previous schemes in this section we point out a flaw that occurs in many previous schemes and comes from the re choosing of hash functions in the case of insertion failure we note that the details of this section are not needed for following our own construction in section if the event of overflow occurs with non negligible probability the act of re choosing new hash functions can reveal significant information about the data that is stored inside a particular level the act of choosing new hash functions appears in several previous works such as we take a deeper look into the ways information can be leaked due to these hash functions to succinctly summarize the problem we describe a general method for distinguishing access patterns based on hash overflows consider two access sequences and both and start off by inserting enough elements to trigger re shuffles so that only levels i and i are non empty and contain disjoint distinct elements continues with reads of distinct elements in level i and continues with reads of distinct elements in level i in both sequences the server can observe the sequence of probes caused by the hash functions in the level empty former case because all elements live within level i the sequence induces a pattern that is consistent with a non level a b c d stored in a cuckoo hashtable of size overflowing hash table in the latter case if the hash function is random the sequence of probes will induce level e f g h i j k l stored copyright siam unauthorized reproduction of this article is prohibited in a cuckoo table of size in the operation read will scan the entire level then two random hash locations on level then the places where hashes to on level in the operation read will read the entire level then the places where hashes to on level then the places where dummy e hashes to on level thus if the two locations that hashes to on level are empty then the distinguisher outputs since we are using random hash functions there is a chance each of hitting an empty spot giving us a distinguishing advantage next we give an example that addresses our main concern that has to deal with the collision free property of a cuckoo hash table the trick to distinguishing is that reading three elements that exist in a level will never collide since they must occupy distinct locations but accessing three random elements gives a small probability of them all hashing in to the same two addresses in the hash table in this example the server can distinguish between the two sequences write read and write read with a advantage after the sequence of initial writes the buffers will look like this level empty level empty level a b c d e f g h stored in a table of size level i j k l m n o p q r s t u v w x stored in a table of size in the operation read will read all of level two random locations in level two random locations in level and the two places it hashes to in level it will be found and written back to level the operation read will read all of level two random locations in level two random locations in level and the two places it hashes to in level it will be found and written back to level a reshuffle from level down to level now occurs the hash functions at level are now repicked to be new random functions this does not affect level the operation read will read all of level two random locations in level two random locations in level and the two places it hashes to in level it will be found and written back to level in the operation read will read all of level two random locations in level the two places it hashes to in level and the two places dummy i hashes to in level it will be found and written back to level the operation read will read all of level two random locations in level the two places it hashes to in level and the two places dummy j hashes to in level it will be found and written back to level a reshuffle from level down to level now occurs the hash functions at level are now re picked to be new random functions this does not affect level the operation read will read all of level two random locations in level the two places it hashes to in level and the two places dummy k hashes to in level it will be found and written back to level in the two places that the sequence of reads probe in level will never be the exact same two places every time this is because the hash pattern must induce a valid cuckoo pattern or hash graph however in because the locations are random there is a probability that the two locations probed by the second read will match the two locations probed by the first read and similarly another probability for the last read this is an example of how the triple hit problem stated in the introduction can be seen in practice the subtle issue can be found in the simulation proof of the simulated probe sequence only probes things that are either a real element in level i or a dummy element in level i however in a real execution of the protocol one might probe something that exists in a lower level or does not exist at all in the database in the worst case as in the example above none of the elements probed in between reshuffles of level i exist in level i this is as if one were to build a brand new cuckoo hash table with the given hash functions which is known to fail with probability o n thus there exists a list of instructions such that with probability o n in the real execution the sequence of probes on level i as observed by the server induces an invalid cuckoo hash graph the simulator described in will never generate a sequence of probes that induces an invalid cuckoo hash graph because their simulated probes are only to elements in the cuckoo hash table impact on existing schemes we also examined prior schemes and have found that some suffer from similar flaws as mentioned above namely if a hash table overflows and new hash functions are chosen this could potentially leak information that would allow an adversary to distinguish between two sequences of reads and writes in standard hash tables are used with buckets of size o log n in lemma of the proof states that the only values hashed at level i between reshuffles will be ones corresponding to either elements living inside level i or dummy elements this is not copyright siam unauthorized reproduction of this article is prohibited true because if an element x is sitting inside level i or deeper then we still must hash x at level i and access the corresponding bucket after some number of queries if the hashes result in a bucket being accessed more times than the total capacity of the bucket it will be immediately revealed that at least one of these previous queries was not found at level i it is still true that all the elements being hashed at level i will be unique so under the condition that these hashes do not overflow a bucket the accesses made are oblivious a similar flaw can be found in theorem of and claim of in in the case of overflow the simulation is aborted in it is suggested that in the case of an overflow which happens with negligible chance a new hash function is chosen in these three works aborting or choosing new hash functions results in an information leakage because the server knows that the data stored in a level is consistent with the hash function however the abort or rehash event occurs with only a negligible chance so the security claims remain intact this is not the case with where in fact this flaw leads to a distinguisher the concept of choosing a new hash function is repeated in the subsequent work of the security of this scheme is not impacted as in fact this overflow only happens with negligible probability see lemma below in it is shown that a cuckoo hash table with constant size stash which was used in has a probability of overflow of o ms where is the size of the stash and m is the size of a level this amount can be made to be smaller than any fixed polynomial by increasing the size of though it is still not negligible it is shown in that a cuckoo hashtable with logarithmic size stash will overflow with only negligible probability assuming that the table is of size ω n and hence suitable for use for larger levels in the updated version constant sized stashes are no longer used thus resulting in schemes that only have a negligible chance of overflow a problem related to hash overflow occurs in the work of where a bloom filter is used to check whether or not an element is in the hash table before searching for it this appears to mitigate the problems above but the issue is that a bloom filter has some probability for generating false positives if the bloom filter generates enough false positives on elements not present in the hash table the problem appears again in order to make the probability negligible the size of the bloom filter must be super logarithmic thereby increasing the overall storage and overhead our scheme in this section we present our oblivious ram scheme where we both fix the leakage of information and obtain improved efficiency the high level ideas behind our scheme are as follows we eliminate the stash by reinserting its elements back into the data structure and we balance between the cost of reads and the cost of writes technically we use as a starting point the constant local memory scheme of whose top buffer bk is sufficiently large and the first cuckoo hash level bk is also sufficiently large these choices make the probability of bad events negligible next we provide a detailed description of our scheme our starting point is the hybrid scheme with no duplicates as described in section we choose k log log n and k log log n which means that standard hashing is used for the top o log log n buffers whose sizes are between log n and n the bucket size for these buffers is set to b log n the choice of parameters is made to deal with attacks of the type described in section for the larger buffers we employ cuckoo hashing we use the version with stash of size log n as in the first modification we make is that rather than actually keeping the stash at each level at the end of the re shuffling stage as in there will be a single stash of size the concept of a shared stash was introduced in subsequently used in in the construction of oblivious ram using o n local storage in our case we have constant local storage so instead of keeping an explicit shared stash we virtualize the stash by reinserting it into the top buffer note that at the end of re shuffling the top buffer is empty and we make exactly insertions independent of the actual content of the stash the second ingredient of our solution refers to balancing the cuckoo hashing levels at level k i we actually have t log n buffers rather than just one b b t and each of them is of size ti i in particular this means that a buffer at each of these levels is larger by a factor of than the buffers at the previous level and so in each re shuffling one such buffer can accommodate the elements of all buffers in all previous levels in order to keep notation consistent we define b b to be the only buffer for levels between k and k we may refer to b j as a subbuffer to emphasize that it is one of a set of buffers at the same level the operations on the modified data structure are now implemented as follows when reading an element r via a read r operation we look this is similar to the notion of a shared stash introduced by see below for an explanation of the difference copyright siam unauthorized reproduction of this article is prohibited for it at each cuckoo hashing level in all t sub buffers in reverse order starting from b t down to b the top levels that do not use cuckoo hashing are read exactly as before when we perform a re shuffling into level for the first time we do it into sub buffer b in the next time we will use b and so on only after using all t subbuffers the next re shuffling will be done into level having t sub buffers at each levels allows us to reduce the total number of levels l to l o log n log log n a detailed description of our algorithm for performing read and write operations appears in appendix a we refer the reader to for a detailed description of the unmodified version of re shuffling complexity analysis we first analyze the physical size and the capacity of each level by which we mean the number of elements it may contain before we consider it to be full the top level bk is treated as an array and has size and capacity equal to log n the buffers using standard hash with buckets occupy levels k through k the capacity of each one is twice that of the level before it and thus for buffer bk i it is equal to log n the size however is bk i n due to the fact that we have buckets of size o log n for the cuckoo hash tables the capacj ity of a single sub buffer bk at level k is n and it has size log n the total size and capacity at level k is log n times larger due to the fact that we keep log n of these buffers for subsequent levels we have subbuffers of size bk i i n for i l k and capacities of half that amount because we wish to store a total of n elements in the largest level l we must have l k n θ n solving for l we have l o log n log log n hence the total number of elements stored remotely is k k x i log n l k x at each level we read from each of the t buffers and so the total cost for a read is o n log log n dominated by the cuckoo hashing buffers on the other hand reshuffling is done less frequently since for each level we wait until all t sub buffers are utilized before reshuffling and so the amortized cost of re shuffling is also o n log log n in detail we separate the buffers into three categories the top buffer bk the standard hash buffers bk bk and the cuckoo hash buffers bk bl during a read the entire top buffer is scanned twice which requires log n operations when scanning the buffers the standard hash buffers each have buckets of size b which results in b k k o log n log log n total elements read finally in the cuckoo hash buffers two elements are read and then written from each buffer giving a total of l k log n o log n log n log log n elements although the cuckoo hash tables have stashes the stashes have already been moved to the top level and possibly reshuffled down and do not need to be read recall that a reshuffle step can be done with o m log m accesses where m is the number of elements involved in the shuffle this includes the re insertion of the stash whose size is o log n o m to the top buffer after every reshuffle note that due to the growth of the levels the number of elements in levels k i is at most twice the number of elements contained in a sub buffer bij also the act of inserting a single element at the top buffer can be viewed as also inserting phantom elements at every level and these phantom elements are identified during a reshuffle by this observation it takes o bij log n o bij steps for bi to become full thus the amortized cost of reshuffling is i n o n o n l x o bi log bi i i k o bi o log log n which is o n k k x in addition we locally store only a constant number o i log log n of elements typically or during reading and o i elements also suffice for reshuffling l k x next we analyze the overhead of our scheme the o i log log n idea is that each read now costs more compared to the i basic hybrid scheme but we gain by reducing the cost of re shuffling i e the sort namely the number of since k k is o log log n and l k is levels is now o log n log log n since between cuckoo o log n log log n this gives hashing levels the size grows by an o log n factor but o log log n o log log n this is important for obtaining security in particular in the o log n log log n log log n proof of lemma the distinction between size and capacity is important when o n log log n using cuckoo hashing whose performance is guaranteed only for buffers whose capacity is not more than say of their size total amortized overhead for reshuffling copyright siam unauthorized reproduction of this article is prohibited me i i mi e i i security to argue the security of the scheme we begin with a lemma that is common to previous schemes lemma except for the top level no index id will be searched upon twice in any sub buffer bij before this sub buffer is reshuffled i e elements are shuffled into or out of it in other words all searches to bij between reshuffles are from unique indices which may collide when hashed proof suppose index id is searched upon i e id is the value to be hashed for sub buffer bij either id is an actual index or it is a dummy index in the case of id being a dummy index the lemma immediately follows from the fact that dummy indices are of the form dummy t where t is a query counter and subsequent queries will have ever increasing values of t either the hierarchy is designed with a query bound t t or we can add polynomially many new levels at the bottom of the hierarchy on the other hand if id is an actual index whether or not it is found in the ram is of no consequence it will be inserted into the top level bk all subsequent queries upon id will result in it being found in a younger buffer or sub buffer causing dummy locations to be searched upon in the remaining buffers which includes bij the only way id can be present in bij or a deeper buffer again is if id is shuffled into bij or the entirety of bi is full and all bij are shuffled out to a bigger level either way bij will have been reshuffled and a new hash function or functions will be chosen for it next we show using that the probability that our hash tables ever overflow is negligible lemma assuming the hash functions are perfectly random a reshuffle into sub buffer bij succeeds with all but negligible probability proof first observe that a reshuffle into sub buffer bij involves at most m elements being shuffled into a hash table of capacity m for the lower levels bk bk there will be m buckets each holding up to b log n elements and for sub buffers of bk bl there will be cuckoo hash tables of size each and a stash of size log n in the case of standard hashing we consider the probability that more than b elements end up in any bucket when we throw m balls into m bins the probability that there are exactly i elements in any bucket can be bounded as m i m i pr bucket has i elements m i mi summing over all i b m we get e b pr bin has at least i balls taking b e b the union bound over all m bins the probability b m that any bin overflows is bounded by eb e b which is negligible in our case where b log n and log n m n it is shown in appendix c that for cuckoo hash tables of size m with a stash of size as long as m ω n and o log n the probability of overflow is m ω which is negligible in n these restrictions on m hold for all sub buffers of levels bk b l theorem assume one way functions exist then there exists a ppt simulator s such that for all t and for all read write sequences of length t the distribution of s t is computationally indistinguishable from the distribution of executions of our balanced oram protocol on any request sequence of length t proof we describe a simulator that generates a view of the server that is computationally indistinguishable from that of a real execution on any sequence of accesses first by the pseudorandomness of the hash functions we can simulate these using random functions with domain equal to n along with all possible dummy values we can also simulate encrypted data elements by encryptions of zero by the semantic security of the underlying encryption scheme we observe that both a read r and a write r val operations will begin with the same reading step followed by the same insertion step causing exactly one new element to be inserted at the top level buffer bk accesses to bk will always be oblivious as bk is always fully scanned we show how to simulate the view of the server for these operations after we investigate what reshuffling does we now turn our attention to what happens in sub buffer bij first observe that when elements are shuffled into bij brand new hash functions are chosen thereby breaking any possible previous dependencies in addition by the oblivious hashing of the server does not know the locations of the elements in bij be it in a standard hash table cuckoo hash table or the stash therefore to simulate the server view of bij we only need to consider what happens between two reshuffles let r indicate the set of elements that was inserted into bij and possibly its stash let be another arbitrary set of r elements and let h or h2 denote copyright siam unauthorized reproduction of this article is prohibited the hash function used for bij during this reshuffle recall that because we are treating hash functions as truly random functions we know that h is sampled uniformly from the set of random functions consistent with r i e inserting the elements of r into the hashtable does not cause an overflow let h denote this distribution and let denote the uniform distribution of random functions that are consistent with by lemma the statistical distance between h and u the uniform distribution of random functions is negligible as is the statistical distance between and u during a sequence of queries to bij all elements accessed will be unique by lemma the server only sees a sequence of probes of distinct elements from some sequence of elements s let s be an arbitrary set of s distinct elements and consider the following distributions h s h h h s h u h s h u h s h the statistical distance between each neighboring pair of distributions is negligible and therefore the distance between the first and last is negligible as the last distribution is for arbitrary and s we can simulate the server view by choosing an arbitrary sequence of operations has o n log log n worst case overhead per query while increasing the work and the size of the storage required by the server by a constant factor warm up worst case construction as a warm up we first describe a method to convert the goldreich ostrovsky oblivious ram scheme so that it will have o n overhead in the worst case rather than o n amortized overhead converting our own scheme will proceed in a similar manner though with additional ingredients see section below recall that the scheme of can simply be viewed as the lower levels of our construction where no cuckoo hashing is occurring and no sub buffers are being used that is each level i o log n is a standard bucketed hash table bi with buckets of o log n elements each also recall that in the scheme of they do not wipe out an element when it is found but merely copy it to the top buffer when two identical memory addresses are being shuffled into the same level the older duplicate is removed we make the following modifications to the scheme of triple each level each level i will now have three buffers hash tables bi and these three buffers are identical in size and structure to the original buffers they will be marked in a rotating fashion as active inactive and output searching for an element on level i will involve searching first the active and then the inactive buffer from amortized overhead to worst case overhead in our construction above the client may need to perform a large amount of work during some reshuffle operations up to ω n work when handling the very lowest level although the overall amortized overhead add working memory each level i will now have per query is only o n log log n for certain permanently allocated working memory necessary applications we may want to guarantee that every to perform a shuffle into level i although the query even those that induce a reshuffle has a result of the shuffle will eventually be written from small bounded amount of overhead such a guarantee the working memory to the output buffer we define was recently considered by goodrich et al and it as a separate entity from the buffers because shi et al shi et al give a construction of the working memory may in fact be larger than an oblivious ram with o log n worst case overhead a buffer recall that in this working memory with constant client memory goodrich et al is of size log n for level i which is only a constructed two schemes one of which used o n client constant times larger than bi thus the overall memory to achieve o log n worst case overhead the asymptotic storage complexity is not affected by other used constant client memory and showed how to permanent working memory being allocated extend square root solution of to achieve the o n log n worst case overhead this extension made smearing the reshuffle when the active buffer in use of doubling up the buffer and labeling each as a level i is full i e when it is the time for it to be current or previous and switching between the two shuffled into the next level it is not immediately in this section we describe a method of tripling each shuffled as this may take more time than we can buffer and switching between them in order to spread afford for satisfying the desired worst case guaranout the amount of work required for the reshuffling tee but instead it is marked inactive the inwe show how to convert the scheme of to have active buffer on level i should by now be wiped worst case o n query overhead and then go on to clean and will be labeled active instead of imshow how to extend our construction into a scheme that mediately performing the entire shuffling for every copyright siam unauthorized reproduction of this article is prohibited subsequent query the client performs a fixed number of steps κ o n of the oblivious sorting algorithm by this the newly labeled inactive buffer on level i call it ni and the active buffer in level i call it ai are slowly over κ queries copied into the working memory of level i call it wi where they are shuffled together to clarify over the course of these κ queries the following steps are performed the elements from ni and ai are copied into wi the oblivious sort is performed on wi while the buffers ni and ai remain available to be searched upon the oblivious sort finishes by coping the result from wi to the output buffer at level i upon completion of the shuffle after κ queries for level i the output buffer is now marked as the active buffer for level i the inactive buffer from level i is wiped clean and labeled active and the active buffer from level i is wiped clean and labeled output it is important to note that κ the number of steps of the oblivious sort we perform per client query is fixed and independent of the data also note that the oblivious sorting algorithm occurs entirely within the working memory the newly inactive buffer at level i and the active buffer at level i remain intact and are available to be searched upon indeed the only activity in these two buffers are from client query lookups and data being copied into working memory only until the oblivious sorting is finished do we clear the contents of ni and ai to see why the modification works we need to show that shuffles do not overlap each other the only danger is wiping out an inactive buffer that is not yet finished shuffling however we observe that this does not cause a problem because the shuffle at level i involves shuffling o log n elements which requires o log n log log n steps which is o n since i log n since we perform κ o n steps per query the shuffle will be over once queries are processed at level i i e exactly the rate at which level i fills up how much overhead do these modifications incur on top of the regular query overhead which takes o n time we perform o n additional steps per level on the shuffle and there are at most log n levels that need to be shuffled thus giving a total of o n overhead per query we have thus transformed the o n amortized overhead of the original scheme into o n worst case overhead worst case construction for main scheme we now sketch the modifications needed to make our scheme have o n log log n worst case overhead we begin by modifying our scheme so that it does not cascade when shuffling i e when need to shuffle level i only level i is shuffled into the next level i rather than shuffling all levels above i into level i in addition the scheme is modified such that it does not wipe out an element upon finding it instead duplicates are only removed when they are being shuffled into the same sub buffer note that the latter modification causes an unexpected issue that did not occur in the scheme of namely consider the following scenario an element is not wiped out and ends up in the stash of a very big level i l a newer copy of the same element is present in a middle level somewhere because we re insert the stash into the hierarchy the old version of the element will appear at a higher level than the newer version thus causing a conflict to handle this issue we add a tag to each element that indicates the lowest level and sub buffer it has appeared at as an element is shuffled into level i sub buffer j the tag is updated via the rule tag max tag i j based on the order in which we search levels and sub buffers i e i j j if i or i and j j which is not the usual lexicographical ordering when an element is stashed the tag is not updated so as to keep track of the true age of this element when searching for an element v if it is found we keep track of the tag but continue searching for v up to a point let tag be the minimum one of all tags of v found so far before we search a level and sub buffer we first check that it is less than tag if so then we search for v otherwise we stop and search the remaining levels and sub buffers for random elements the value that is output is the v corresponding to the minimal tag to ensure that our main construction will have a worst case guarantee we begin by making the modifications as described above to our sub buffers except that now the client performs fewer steps o log n for the lower levels i k l of the shuffling algorithm however we now need to deal with another ingredient where our scheme differs from the goldreichostrovsky scheme there may be up to l k o log n log log n active stashes sitting at the lower levels that need to be re inserted to the top dealing with all these insertions immediately will violate the desired worst case bound to handle this we add a stash backlog at the copyright siam unauthorized reproduction of this article is prohibited very top level that can hold up to o n log log n elements i e all the elements in all the active stashes for each level i k l now when the client wants to execute a query the client scans the entire stash backlog in addition to the topmost level after each query is handled the client removes one element from the stash backlog and inserts it into the main hierarchy it is important to note that the backlog will never overflow because it will be emptied within o n log log n queries whereas it will only be filled when an active stash is being shuffled into which happens at intervals of at least o n steps to see why this scheme remains secure we need to justify why lemma applies here as well consider any configuration where some value v may appear several times in the hierarchy with different tags as a thought experiment we call each sub buffer v searched if v was searched for in this sub buffer between shuffles of that sub buffer we must make sure that we never search for v in a v searched sub buffer we also call a sub buffer v safe if the minimal v tag is smaller than or equal to the index of that sub buffer again the ordering is the search order not lexicographical the point is that v will not be searched upon in any v safe sub buffer due to our new search rules as long as all v searched buffers are v safe then we will never search for v twice in a sub buffer we argue that this holds by induction initially all sub buffers are labeled with neither we show that when v is read or written a copy of v is being shuffled into a new level or v is stashed with an old tag back to the top the invariant that all v searched sub buffers are also v safe remains true in case when v is read or written it will be re inserted at the top with a fresh tag which will be the minimal tag thus all levels and sub buffers in the hierarchy will now be v safe and many of them will also now be vsearched in case if the copy of v being shuffled is not the one with the smallest tag the minimal tag remains the same and thus the set of v safe sub buffers does not change if the v being shuffled is the one with the minimal tag the tag will be updated to the new level and sub buffer thus possibly reducing the number of v safe sub buffers however the only sub buffers that will be affected are the ones that were on the same level as the v that was shuffled since the updated tag is only one level deeper therefore these sub buffers must have also been shuffled out of and emptied in the same shuffle that moved v which means that they are no longer vsearched if they previously were finally if after a shuffle a copy of v ends up in the stash as in case then because the tag is not updated the minimal tag for v must remain the same and therefore the set of v safe sub buffers remains unchanged in all three cases the vsearched sub buffers remain v safe sub buffers finishing the induction we now observe the total worst case overhead in the oblivious ram scheme incurred by these modifications from our analysis in section the client takes o n log log n time to perform a read or write query the client now also perform up to o log n shuffle steps for each of the o log n log log n levels and read in the stash backlog both of which take o n log log n time thus overall the worst case overhead for our modified scheme is o n log log n finally we consider the amount of server storage we triple each sub buffer in our construction as well as add a new stash backlog of size o n log log n furthermore as in the amount of permanently allocated working memory per level is on the order of the size of a sub buffer for that level therefore the overall storage complexity in our worst case oblivious ram is only a constant factor worse than our amortized oblivious ram we consider the sparse fourier transform problem given a complex vector x of length n and a parameter k estimate the k largest in magnitude coefficients of the fourier transform of x the problem is of key interest in several areas including signal processing audio image video compression and learning theory we propose a new algorithm for this problem the algorithm leverages techniques from digital signal processing notably gaussian and dolph chebyshev filters unlike the typical approach to this problem our algorithm is not iterative that is instead of estimating large coefficients subtracting them and recursing on the reminder it identifies and estimates the k largest coefficients in one shot in a manner akin to sketching streaming algorithms the resulting algorithm is structurally simpler than its predecessors as a consequence we are able to extend considerably the range of sparsity k for which the algorithm is faster than fft both in theory and practice introduction the fast fourier transform fft is one of the most fundamental numerical algorithms it computes the discrete fourier transform dft of an n dimensional signal in o n log n time the algorithm plays a central role in several application areas including signal processing and audio image video compression it is also a fundamental subroutine in integer multiplication and encoding decoding of error correcting codes any algorithm for computing the exact dft must take time at least proportional to its output size which is θ n in many applications however most of the fourier coefficients of a signal are small or equal to zero i e the output of the dft is approximately sparse for example a typical block in a video frame has on average non negligible coefficients i e of the coefficients are negligible images and audio data are equally sparse this sparsity provides the rationale underlying compression schemes such as mpeg and jpeg other applications of sparse fourier analysis include computational learning theory analysis of boolean functions o multiscale analysis compressed sensing similarity search in databases spectrum sensing for wideband channels and datacenter monitoring when the output of the dft is sparse or approximately sparse one can hope for an output sensitive algorithm whose runtime depends on k the number of computed large coefficients formally given a complex vector x whose fourier transform is x we require the algorithm to output an approximation x to x that satisfies the following guarantee kx x c min k sparse y kx where c is some approximation factor and the minimization is over k sparse signals note that the best k sparse approximation can be obtained by setting all the algorithm in this paper has a somewhat stronger guarantee see results for more details copyright siam unauthorized reproduction of this article is prohibited but the largest k coefficients of x to such a vector can be represented using only o k numbers thus if k is small the output of the algorithm can be expressed succinctly and one can hope for an algorithm whose runtime is sublinear in the signal size n the first such sublinear algorithm for the hadamard transform was presented in shortly after several sublinear algorithms for the fourier transform over the complex field were discovered ggi these algorithms have a runtime that is polynomial in k and log n the exponents of the polynomials however are typically large the fastest among these algorithms have a runtime of the form o k logc n as in ggi or the form o k logc n as in for some constant c in practice the exponents in the runtime of these algorithms and their complex structures limit their applicability to only very sparse signals in particular the more recent algorithms were implemented and evaluated empirically against fftw an efficient implementation of the fft with a runtime o n log n the results show that the algorithm in is competitive with fftw for n and k the algorithms in ggi require an even sparser signal i e larger n and smaller k to be competitive with fftw results in this paper we propose a new sublinear algorithm for sparse fourier transform over the complex field the key feature of our algorithm is its simplicity the algorithm has a simple structure which leads to efficient runtime with low big oh constant specifically for the typical case of n a power of our algorithm has the runtime of p o log n nk log n thus the algorithm is faster than fftw for k up to o n log n in contrast earlier algorithms required asymptotically smaller bounds on k this asymptotic improvement is also reflected in empirical runtimes for example for n our algorithm outperforms fftw for k up to about which is an order of magnitude higher than previously achieved the estimations provided by our algorithm satisfy the so called guarantee specifically let y be the minimizer of kx for a precision parameter δ no and a constant our randomized see for a streamlined exposition of some of the algorithms assuming all inputs are represented using o log n bits algorithm outputs x such that kx x kx k with probability n the additive term that depends on δ appears in all past algorithms ggi although typically with the exception of it is eliminated by assuming that all coordinates are integers in the range no no in this paper we keep the dependence on δ explicit the guarantee of equation is stronger than the guarantee of equation in particular the guarantee with a constant approximation factor c implies the guarantee with a constant approximation factor c if one sets all but the k largest entries in x to furthermore instead of bounding only the collective error the guarantee ensures that every fourier coefficient is well approximated techniques we start with an overview of the techniques used in prior works then describe our contribution in that context at a high level sparse fourier algorithms work by binning the fourier coefficients into a small number of buckets since the signal is sparse in the frequency domain each bucket is to have only one large coefficient which can then be located to find its position and estimated to find its value for the algorithm to be sublinear the binning has to be done in sublinear time to achieve this goal these algorithms bin the fourier coefficient using an n dimensional filter vector g that is concentrated both in time and frequency i e g is zero except at a small number of time coordinates and its fourier transform g is negligible except at a small fraction about k of the frequency coordinates the pass region depending on the choice of the filter g past algorithms can be classified as iteration based or interpolation based iteration based algorithms use a filter that has a significant mass outside its pass region ggi for example the papers ggi set g to the box car function in which case g is the dirichlet kernel whose tail decays in an inverse linear fashion since the tail decays slowly the fourier coefficients binned to a particular bucket leak into other buckets on the other hand the paper estimates the convolution in time domain via random sampling which also leads to a large estimation error to reduce these errors and obtain the guarantee these algorithms have to perform multiple iterations where this fact was implicit in for an explicit statement and proof see remarks after theorem one can randomize the positions of the frequencies by sampling the signal in time domain appropriately ggi see section part b for the description copyright siam unauthorized reproduction of this article is prohibited each iteration estimates the largest fourier coefficient the one least impacted by leakage and subtracts its contribution to the time signal the iterative update of the time signal causes a large increase in runtime the algorithms in ggi perform this update by going through o k iterations each of which updates at least o k time samples resulting in an o k term in the runtime the algorithm introduced a bulk sampling algorithm that amortizes this process but it requires solving instances of a non uniform fourier transform which is expensive in practice interpolation based algorithms are less common and limited to the design in this approach uses a leakage free filter g to avoid the need for iteration specifically the algorithm in iwe10a uses for g a filter in which gi iff i mod n p and gi otherwise the fourier transform of this filter is a spike train with period p this filter does not leak it is equal to on p fraction of coordinates and is zero elsewhere unfortunately however such a filter requires that p divides n moreover the algorithm needs several different values of p since in general one cannot assume that n is divisible by all numbers p the algorithm treats the signal as a continuous function and interpolates it at the required points interpolation introduces additional complexity and increases the exponents in the runtime our approach the key feature of our algorithm is the use of a different type of filter in the simplest case we use a filter obtained by convolving a gaussian function with a box car function a more efficient filter can be obtained by replacing the gaussian function with a dolph chebyshev function see fig for an illustration because of this new filter our algorithm does not need to either iterate or interpolate specifically the frequency response of our filter g is nearly flat inside the pass region and has an exponential tail outside it this means that leakage from frequencies in other buckets is negligible and hence our algorithm need not iterate also filtering can be performed using the existing input samples xi and hence our algorithm need not interpolate the signal at new points avoiding both iteration and interpolation is the key feature that makes our algorithm efficient further once a large coefficient is isolated in a bucket one needs to identify its frequency in contrast to past work which typically uses binary search for this task we adopt an idea from and tailor it to our problem specifically we simply select the set of large bins which are likely to contain large coefficients and directly estimate all frequencies in those bins to balance the cost of the bin selection and estimation steps we make the number of bins somewhat larger than the typical value of o k specifically we use b which leads to the stated runtime nk notation transform related notations for an input signal x cn its fourier spectrum is denoted by x we will use x y to denote the convolution of x and y and x y to denote the coordinate wise product of x and n y recall that xd y x y we define ω to be e a primitive nth root of unity here i but in the rest of the paper i is an index indices related notations all operations on indices in this paper are taken modulo n therefore we might refer to an n dimensional vector as having coordinates n or n n interchangeably finally refers to the set of indices whereas supp x refers to the support of vector x i e the set of non zero coordinates in this paper we assume that n is an integer power of basics a window functions in digital signal processing one defines window functions in the following manner definition we define a δ w standard window function to be a symmetric vector f rn with supp f w w such that f f i for all i n n and f i δ for all i n n claim for any and δ there exists an δ o log δ standard window function proof this is a well known fact for example for any and δ one can obtain a standard window p by taking a gaussian with standard deviation θ log δ and truncating it at w o log δ the dolph chebyshev window function also has the claimed property but with minimal big oh constant in particular half the constant of the truncated gaussian the above definition shows that a standard window function acts like a filter allowing us to focus on a subset of the fourier coefficients ideally however we would like the pass region of our filter to be as flat as possible although it is plausible that one could combine our filters with the binary search technique of and achieve an algorithm with a o k logc n runtime our preliminary analysis indicates that the resulting algorithm would be slower intuitively observe that for n and k the values of nk and k n are quite close to each other copyright siam unauthorized reproduction of this article is prohibited definition we define a δ w flat window lemma if j n is a power of two and σ is function to be a symmetric vector f rn with a uniformly random odd number in n then pr σj supp f w w such that f i δ δ c c n for all i n n and f i δ for all i n n proof if j for some odd a then the orbit of σj is a flat window function like the one in fig can be ob for all odd there are thus round c tained from a standard window function by convolving possible values in c c out of n such it with a box car window function i e an interval elements in the orbit for a chance of at most n specifically we have the following note that for simplicity we will only analyze our claim for any and δ with there exists algorithm when n is a power of two for general n n an δ o log δ flat window function the analog of lemma would lose an n ϕ n o o log log n factor where ϕ is euler totient function note that in our applications we have δ n this will correspondingly increase the running time of and thus the window lengths w of the flat the algorithm on general n window function and the standard window function are claim allows us to change the set of coeffithe same up to a constant factor cients binned to a bucket by changing the permutation proof let f and let f be an lemma bounds the probability of non zero coeffiδ w standard window function with minimal cients falling into the same bucket f n c subsampled fft suppose we have a vector n w o we can assume log δ x cn and a parameter b dividing n and would like because n n otherwise so log δ n to compute y i x i n b for i b o log nδ let be the sum of f n adjacent claim y is the b dimensional fourier transform pn b copies of f normalized to that is we define of yi j xi bj p f n therefore y can be computed in o supp x j f n f i j ˆ b log b time f i pf n f j j f n proof so by the shift theorem in the time domain b n x n b x x f n ij n b x xbj a ω i bj a n b x ω x j i n b j fa fa ω a j j j f n since f i for i f n the normalization factor pf n j f n f j is at least for each i n n the sum on top contains all the terms from the sum on bottom the other n terms in the top sum have magnitude at most δ n δ f n so i n δ f n δ for i n however i f nδ f n δ thus f is an δ w flat window function with the correct w b x n b x a as desired j xbj a ω ian b b x ya ω ian b y i a algorithm a key element of our algorithm is the inner loop which finds and estimates each large coefficient with constant probability in we describe the inner loop and in we show how to use it to construct the full b permutation of spectra following algorithm we can permute the fourier spectrum as inner loop let b be a parameter that dilows by permuting the time domain vides n to be determined later let g be a claim define the transform pσ τ such that given b δ w flat window function for some δ and an n dimensional vector x an integer σ that is invertible w o b log n we will have δ nc so one can δ mod n and an integer τ n pσ τ x i xσi τ then think of it as negligibly small τ i p there are two versions of the inner loop location σ τ x σi x i ω loops and estimation loops location loops are given pn aj proof for all a p a parameter d and output a set i n of dkn b σ τ x a j xσj τ ω pn a j τ σ x aσ ω τ aσ coordinates that contains each large coefficient with j xj ω copyright siam unauthorized reproduction of this article is prohibited g linear scale b linear scale g a b g log scale b log scale g 05 0001 1e 1e 1e 1e c d figure an example flat window function for n this is the sum of 31 adjacent dolph chebyshev window functions giving a 06 133 flat window function although our proof b in a linear only guarantees a tolerance δ the actual tolerance is better the top row shows g and g scale and the bottom row shows the same with a log scale copyright siam unauthorized reproduction of this article is prohibited good probability estimation loops are given a set we have that i n and estimate xi such that each coordinate is n x estimated well with good probability y σi oσ i p x g σ τ t σi t oσ i t choose a random σ τ n with σ odd n x define y g pσ τ x so yi gi xσi τ then pσ τ x σj g σ i j oσ i supp y supp g w j compute z i y i n b for i b by claim pdw be this is the dft of zi j yi jb x p x g σ τ σj σ i j oσ i define the hash function hσ n b j t by hσ i round σib n and the offset oσ n n n by oσ i σi x hσ i n b pσ τ x σj g σ i j oσ i location loops let j contain the dk coordinates j t of maximum magnitude in z output i i x x n hσ i j which has size dkn b p x p x δ σ τ σj σ τ σj estimation loops for i i estimate x i as j t j t τi x i z hσ i ω g oσ i by claim we can compute z in o w b log b o b log nδ time location loops thus take o b log nδ dkn b time and estimation loops take o b log nδ i time fig illustrates the inner loop for estimation loops we get the following guarantee lemma let s be the support of the largest k coefficients of x and x s contain the rest then for any pr x x i σ τ k kx s kx o k b in the last step the left term follows from g a δ for all a the right term is because for j t σ i j oσ i σ i j oσ i b n n b and g a δ for a n b by claim this becomes x τ j y σi oσ i δ x j ω kx j t p define v j t x j ω τ j as a choice over τ v is the energy of a random fourier coefficient of the vector x t so we can bound the expectation over τ e v kx t proof by the definitions τ now for each coordinate j i prσ j t b by lemma thus prσ s t b so with probability b over σ we have x z hσ i ω τ i g oσ i y σi oσ i ω τ i g oσ i consider the case that x is zero everywhere but at i so supp p σ τ x σi then y is the convolution of g and pσ τ x and g is symmetric so y σi oσ i g p σ τ x σi oσ i g oσ i pσ τ xσi g oσ i x i ω τ i kx t x t s let e be the event that this occurs so e is with probability b and otherwise we have e ev e e kx t e e x t s e x t s σ τ σ σ σ which shows that x x i in this case but x x i is furthermore we know linear in x so in general we can assume x i and x bound x e ev e x t s x j pr σ i j b b σ τ σ σ since x i z hσ i g oσ i z hσ i δ j s y σi oσ i δ it is sufficient to bound y σi oσ i kx s define t j n σ i j b b b copyright siam unauthorized reproduction of this article is prohibited convolved signal magnitude magnitude permuted signal π π π a magnitude magnitude regions estimated large π b samples actually computed π p σ τ x y g pσ τ x p σ τ x π π π y g pσ τ x chosen region p σ τ x z sample cutoff z c d figure example inner loop of the algorithm on sparse input this run has parameters n k g being the 06 133 flat window function in fig and selecting the top of b samples in part a the algorithm begins with time domain access to pσ τ x given by pσ τ x i xσi τ which permutes the spectrum of x by permuting the samples in the time domain in part b the algorithm computes the time domain signal y g pσ τ x the spectrum of y pictured is large around the large coordinates of pσ τ x the algorithm then computes z which is the rate b subsampling of y as pictured in part c during estimation loops the algorithm estimates x i based on the value of the nearest coordinate in z namely z hσ i during location loops part d the algorithm chooses j the top dk here coordinates of z and selects the elements of n that are closest to those coordinates the shaded region of the picture it outputs the set i of preimages of those elements in this example the two coordinates on the left landed too close in the permutation and form a hash collision as a result the algorithm misses the second from the left coordinate in its output our guarantee is that each large coordinate has a low probability of being missed if we select the top o k samples copyright siam unauthorized reproduction of this article is prohibited by lemma therefore by markov inequality and and by lemma a union bound we have for any c that kn e v o c b pr v kx s pr e ev c e ev σ τ σ τ σ τ b n o d thus by markov inequality pr v dk b k or c b n pr u v dk k o hence b d since the rhs is only meaningful for d we have n k therefore dk b c δ kx s kx b k c b pr y σi oσ i σ τ pr u v dk n o b d replacing c with b and using x x i and thus y σi oσ i δ gives pr j b z j δ e kd o d δ kx s δ kx pr x x i σ τ δ δ hence a union bound over this and the probability that k x x i e gives b k pr i i o which implies the result b d furthermore since g oσ i δ δ z hσ i is a good estimate for x i the division is mainly useful for fixing the phase therefore in location loops we get the following guarantee q lemma define e k kx s 3δ kx to be the error tolerated in lemma then for any i n with x i pr i i o as desired outer loop our algorithm is parameterized by and δ it runs l o log n qiterations of the inner loop with parameters b o nk log n δ and d o as well as δ for r l run a location inner loop to get ir for each i i il let si r i ir let i i i si l for r l run an estimation inner loop on i to get x ri for i i estimate x median x ri i i where we take the median in real and imaginary coordinates separately k b d k proof with probability at least o b x x i e in this case z hσ i δ e thus it is sufficient to show that with probability at least o d there are at most dk locations j b with z j δ e since each z j corresponds to n b locations in n we will show that with probability at least o d there are at most dkn b locations lemma the algorithm runs in q j n with x δ e nk log n δ log n o let u j n x j e and v j n x x j e therefore j n x u v proof to analyze this algorithm note that since δ we have x l x i si ir ldkn b j x δ e u v i time r we also know u k kx s e2 note k that b is chosen in order to minimize the running time for the purpose of correctness it suffices that b ck for some constant c we will use this fact later in the experimental section copyright siam unauthorized reproduction of this article is prohibited or i b therefore the running time of both the location and estimation inner loops is o b log nδ dkn b computing i and computing the medians both take linear time namely o ldkn b thus the total running time is o lb log nδ ldkn b plugging q nk in b o log n δ and d o this running q time is o nk log n δ log n we require b ω k however for k n log n δ this would cause the run time to be larger but in this case the predicted run time is ω n log n already so the standard fft is faster and we can fall back on it all i i have x and x i with probability with total probability we have kx x k kx s 48δ kx k rescaling and δ gives our theorem experimental evaluation implementation we implement our algorithm in c using the standard template library and refer to this implementation as sfft which stands for sparse fft we have two versions sfft which theorem running the algorithm with parameters implements the algorithm as in and sfft which adds a heuristic to improve the runtime δ gives x satisfying sfft the idea of the heuristic is to apply the filter used in to restrict the locations of the kx x k kx s δ kx k large coefficients the heuristic is parameterized by an m dividing n during a preprocessing stage it does the with q probability n and running time following log n o nk log n δ choose τ n uniformly at random proof define for i m set yi xi n m τ compute y r output t m containing the largest kx s 3δ kx e k elements of y lemma says that in each location iteration r for observe that y pn m x τ i m j thus i m j i ω j any i with x i x e y i x j k r τ pr i i o i j mod m b d thus e si and each iteration is an independent trial so by a chernoff bound the chance that si l is at most l therefore by a union bound with probability at least i i for all i with x i next lemma says that for each estimation iteration r and index i pr x ri x i e o k b therefore with probability ω l x ri x i e in at least of the iterations since real x is the median of the real x ri there must exist two r with x ri x i e but one real x ri above real x and one below hence one of these r has real x x i real x ri x i e and similarly for the imaginary axis then x x i max real x x i imag x x i by a union bound over i with probability at least we have x x i for all i i since this means that the filter is very efficient in that it has no leakage at all also it is simple to compute unfortunately it cannot be randomized using pσ τ after permuting by σ any two colliding elements j and j i e such that j j mod m continue to collide nevertheless if x j is large then j mod m is likely to lie in t at least heuristically on random input sfft completes the algorithm assuming that all large coefficients j have j mod m in t that is in the main algorithm of we then restrict our sets ir to contain only coordinates i with i mod m t we expect that ir m dkn b rather than the previous dkn b this means that our heuristic will improve the runtime of the inner loops from o b log n δ dkn b k to o b log n δ m dkn b m dk at the cost of o m log m preprocessing note that on worst case input sfft may give incorrect output with high probability for example if xi when i is a multiple of n m and otherwise then y with probability m n and the algorithm will output over supp x however in practice the algorithm works for sufficiently random x copyright siam unauthorized reproduction of this article is prohibited claim as a heuristic approximation sfft experimental setup the test signals are generruns in o k n log n δ log n as long as k ated in a manner similar to that in for the run n log n δ time experiments k frequencies are selected uniformly at random from n and assigned a magnitude of and justification first we will show that the heuristic a uniformly random phase the rest are set to zero improves the inner loop running time to o b log n δ for the tolerance to noise experiments the test signals k are generated as before but they are combined with adm dkn b m dk then optimize the parameters m and b ditive white gaussian noise whose variance varies deheuristically one would expect each of the ir to be a pending on the desired snr each point in the graphs t m factor smaller than if we did not require the elements is the average over runs with different instances of to lie in t modulo m hence we expect each of the ir test signals and different instances of noise in all ex k periments the parameters of sfft sfft and and i to have size t m dkn b o m dkn b then in each location loop rather than spending o dkn b aafft are chosen so that the average error k time to list our output we spend o m dkn b time in the absence of noise is between and per plus the time required to figure out where to start listing non zero frequency finally all experiments are run on coordinates from each of the dk chosen elements j of z a dual core intel ghz cpu running ubuntu linux we do this by sorting j and σi i t mod m then 04 with a cache size of kb and gb of ram runtime vs signal size in this experiment scanning through the elements it takes o m dk time to sort o dk elements in m so the total runtime of we fix the sparsity parameter k and report the k dkn b m dk runtime of the compared algorithms for different each location loop is o b log n δ m the estimation loops are even faster since they benefit signal sizes n we plot in fig a the mean maximum and minimum runtimes for sfft from i being smaller but avoid the m dk penalty the full algorithm does o m log m preprocessing sfft aafft fftw and fftw opt and runs the inner loop l o log n times with over runs the relative runtimes of aafft and d o therefore given parameters b and fftw are consistent with those reported in m the algorithm takes o m log m b log nδ log n fig k kn k as expected fig a shows that the runtimes m b log n m log n log n time optimizing over of sfft and fftw and their variants are approxib we take mately linear in the log scale however the slope of the r line for sfft is less than the slope for fftw which is a n k o m log n k log n δ log n log n result of sfft sub linear runtime further the figure m shows that for signal sizes n both sfft time then optimizing over m this becomes and sfft are faster than both variants of fftw at recovering the exact non zero coefficients on the k other hand the runtime of aafft is proportional o k n log n δ log n log n to polylog n and thus it appears almost constant as we increase the signal size for n i e time if k n log n δ the first term dominates aafft eventually beats the runtime of sfft factor smaller note that this is an n log n δ k and is only times slower than sfft overall than the running time of sfft for a large range of signal sizes from about to sfft has the fastest runtime numerical results we evaluate the perforruntime vs number of non zero frequenmance of sfft and sfft and compare them cies in this experiment we fix the signal size to n against two baselines fftw fj which is i e and evaluate the run time of sfft the fastest public implementation for computing the vs the number of non zero frequencies k for each value dft and has a runtime of o n log n and aafft of k the experiment is repeated times fig b which is the prior sublinear algorithm with lustrates the mean maximum and minimum runtimes the fastest empirical runtime for completeness for the compared algorithms we compare against two variants of fftw basic and fig b shows that sfft and sfft have a optimized the optimized version requires preprocessfaster runtime than basic fftw for k up to and ing during which the algorithm is tuned to a particular machine hardware in contrast our current for the values of k and n that are close to the ones considered tations of sfft variants do not perform hardware spe in we use the parameters therein for other ranges we cific optimizations follow the guidelines in the aafft documentation copyright siam unauthorized reproduction of this article is prohibited run time vs signal size k run time vs sparsity n sfft sfft fftw fftw opt run time sec run time sec aafft sfft sfft fftw fftw opt aafft 219 223 number of non zero frequencies k signal size n a b figure a runtime vs signal size the figure shows that for a large range of signal sizes n 226 sfft is faster than fftw and the state of the art sublinear algorithm b runtime vs sparsity parameter the figure shows that sfft significantly extends the range of applications for which sparse approximation of dft is practical and beats the runtime of fftw for values of k order of magnitude larger than those achieved by past work respectively when compared to the optimized fftw the crossing values become and thus sfft crossing values are around n in comparison aafft is faster than fftw variants for k between and further the relative runtimes of aafft and fftw are close to those reported in fig finally fftw has a runtime of o n log n which is independent of the number of non zero frequencies k as can be seen in fig b thus as the sparsity of the signal decreases i e k increases fftw eventually becomes faster than sfft and aafft nonetheless the results show that in comparison with the fastest prior sublinear algorithm sfft significantly extends the range of applications for which sparse approximation of dft is practical robustness to noise last we would like to check that sfft reduced runtime does not come at the expense of reducing robustness to noise thus we compare the performance of sfft and sfft against aafft for different levels of white gaussian noise specifically we fix the n and k and experiment with different signal snrs we change the snr by changing the variance of the gaussian noise for each noise variance we run multiple experiments by regenerating new instances of the signal and noise vectors for each run we compute the error metric per as the average error between the output the snr is defined as sn r log n dimensional noise vector x z where z is an approximation x restricted to its k largest entries and the best k sparse approximation of x referred to as y average error x x y i k i n fig plots the average error per non zero frequency for sfft sfft and aafft the figure shows that all three algorithms are stable under noise further sfft variants appear to be more robust to noise than aafft seminal papers in theoretical cs papadimitriou et al and machine learning hofmann probabilistic latent semantic analysis suggested that documents arise as a convex combination of i e distribution on a small number of topic vectors where each topic vector is a distribution on words i e a vector of word frequencies each convex combination of topics thus is itself a distribution on words and the document is assumed to be generated by drawing n independent samples from it subsequent work makes speciﬁc choices for the distribution used to generate topic combinations the wellknown latent dirichlet allocation lda model of blei et al hypothesizes a dirichlet distribution see section iv thus the topic modeling problem consists of ﬁtting a good topic model to the document corpus the prevailing approach in machine learning is to use local search e g or other heuristics in an attempt to ﬁnd a maximum likelihood ﬁt to the above model for example ﬁtting to a corpus of newspaper articles may reveal ﬁfty topic vectors corresponding to say politics sports weather entertainment etc and a particular article could be explained as a combination of the topics politics sports and entertainment unfortunately and not surprisingly the maximum likelihood estimation is n p hard see section v and consequently when using this paradigm it seems necessary to rely on unproven heuristics even though these have well known limitations e g getting stuck in a local minima 31 the work of papadimitriou et al which also formalized the topic modeling problem and a long line of subsequent work have attempted to give provable guarantees for the problem of learning the model parameters assuming the data is actually generated from it this is in contrast to a maximum likelihood approach which asks to ﬁnd the closest ﬁt model for arbitrary data the principal algorithmic problem is the following see section i a for more details meta problem in topic modeling there is an unknown topic matrix a with nonnegative entries that is dimension n r and a stochastically generated unknown matrix w that is dimension r m each column of aw is viewed as a probability distribution on rows and for each column we are given n n i i d samples from the associated distribution abstract topic modeling is an approach used for automatic comprehension and classiﬁcation of data in a variety of settings and perhaps the canonical application is in uncovering thematic structure in a corpus of documents a number of foundational works both in machine learning and in theory have suggested a probabilistic model for documents whereby documents arise as a convex combination of i e distribution on a small number of topic vectors each topic vector being a distribution on words i e a vector of word frequencies similar models have since been used in a variety of application areas the latent dirichlet allocation or lda model of blei et al is especially popular theoretical studies of topic modeling focus on learning the model parameters assuming the data is actually generated from it existing approaches for the most part rely on singular value decomposition svd and consequently have one of two limitations these works need to either assume that each document contains only one topic or else can only recover the span of the topic vectors instead of the topic vectors themselves this paper formally justiﬁes nonnegative matrix factorization nmf as a main tool in this context which is an analog of svd where all vectors are nonnegative using this tool we give the ﬁrst polynomial time algorithm for learning topic models without the above two limitations the algorithm uses a fairly mild assumption about the underlying topic matrix called separability which is usually found to hold in real life data perhaps the most attractive feature of our algorithm is that it generalizes to yet more realistic models that incorporate topictopic correlations such as the correlated topic model ctm and the pachinko allocation model pam we hope that this paper will motivate further theoretical results that use nmf as a replacement for svd just as nmf has come to replace svd in many applications i i ntroduction developing tools for automatic comprehension and classiﬁcation of data web pages newspaper articles images genetic sequences user ratings is a holy grail of machine learning topic modeling is an approach that has proved successful in all of the aforementioned settings though for concreteness here we will focus on uncovering thematic structure of a corpus of documents see e g in order to learn structure one has to posit the existence of structure and in topic models one assumes a generative model for a collection of documents speciﬁcally each document is represented as a vector of word frequencies research supported by the nsf grants ccf and ccf research supported in part by nsf grant no dms and by an nsf computing and innovation fellowship ieee doi 1109 focs 49 ankur moitra institute for advanced study moitra ias edu goal reconstruct a and parameters of the generating distribution for w the challenging aspect of this problem is that we wish to recover nonnegative matrices a w with small innerdimension r the general problem of ﬁnding nonnegative factors a w of speciﬁed dimensions when given the matrix aw or a close approximation is called the nonnegative matrix factorization nmf problem see and for a longer history and it is np hard lacking a tool to solve such problems theoretical work has generally relied on the singular value decomposition svd which given the matrix aw will instead ﬁnd factors u v with both positive and negative entries svd has the feel of tool clustering and its application in this setting seems to require assuming that each document has only one topic in papadimitriou et al this is called the pure documents case and is solved under strong additional assumptions about the topic matrix a see also and the recent work of anandkumar et al which completely solves this case using the method of moments alternatively other papers use svd to recover the span of the columns of a i e the topic vectors which sufﬁces for some applications such as computing the inner product of two document vectors in the space spanned by the topics as a measure of their similarity these limitations of existing approaches either restricting to one topic per document or else learning only the span of the topics instead of the topics themselves are quite serious in practice documents are much more faithfully described as a distribution on topics and indeed for a wide range of applications one needs the actual topics and not just their span such as when browsing a collection of documents without a particular query phrase in mind or when tracking how topics evolve over time see for a survey of various applications here we consider what we believe to be a much weaker assumption separability indeed this property has already been identiﬁed as a natural one in the machine learning community and has been empirically observed to hold in topic matrices ﬁtted to various types of data separability requires that each topic has some near perfect indicator word a word that we call the anchor word for this topic that appears with reasonable probability in that topic but with negligible probability in all other topics e g could be an anchor word for the topic personal ﬁnance we give a formal deﬁnition in section i a this property is particularly natural in the context of topic modeling where the number of distinct words dictionary size is very large compared to the number of topics in a typical application it is common to have a dictionary size in the thousands or tens of thousands but the number of topics is usually somewhere in the range from ﬁfty to a hundred note that separability does not mean that an anchor word always occurs in fact a typical document may be very likely to contain no anchor words instead separability says that when an anchor word does occur this is a strong indicator that the corresponding topic is in the mixture used to generate the document recently we gave a polynomial time algorithm to solve nmf under the condition that the topic matrix a is separable the intuition that underlies this algorithm is that the set of anchor words can be thought of as extreme points in a geometric sense of the dictionary this condition can be used to identify all of the anchor words and then also the nonnegative factors ideas from this algorithm are a key ingredient in our present paper but our focus is on the question question what if we are not given the true matrix aw but are instead given a few samples say a hundred samples from the distribution represented by each column the main technical challenge in adapting our earlier nmf algorithm is that each document vector is a very poor approximation to the corresponding column of aw it is too noisy in any reasonable measure of noise nevertheless the core insights of our nmf algorithm still apply note that it is impossible to learn the matrix w to within arbitrary accuracy indeed this is information theoretically impossible even if we knew the topic matrix a as well as the distribution from which the columns of w are generated so we cannot in general give an estimator that converges to the true matrix w and yet we can give an estimator that converges to the true topic matrix a for an overview of our algorithm see the ﬁrst paragraph of section iii we hope that this application of our nmf algorithm is just a starting point and other theoretical results can use nmf as a replacement for svd just as nmf has come to replace svd in several applied settings in addition the geometric problems that underly nmf are not yet fully understood and there are many interesting theoretical challenges that remain practical issues the estimates of runtimes throughout the paper are possibly too pessimistic as mentioned in the conclusions section simple variations of the algorithms in this paper run very fast much more so than existing software for topic models a our results here we formally deﬁne the topic modeling learning problem which we informally introduced above there is an unknown topic matrix a which is of dimension n r i e n is the dictionary size and each column of a is a distribution on n there is an unknown r m matrix w each of whose columns is itself a distribution i e a convex combination on r the columns of w are i i d samples from a distribution t which belongs to a known family e g dirichlet distributions but whose parameters are unknown thus each column of aw being a convex combination of distributions is itself a distribution on n and the algorithm input consists of n i i d samples for each column of aw here n is the document size and is assumed to be a constant for simplicity our algorithm can be easily adapted to work when the documents have different sizes the algorithm running time will necessarily depend upon various model parameters since distinguishing a very small parameter from zero imposes a lower bound on the number of samples needed the ﬁrst such parameter is a quantitative version of separability which was presented above as a natural assumption in context of topic modeling dirichlet as happens in the popular latent dirichlet allocation lda model this is done in section iv a by computing a lower bound on the γ in terms of the parameter for the dirichlet distribution which allows us with some other ideas see section iv b to recover the parameters of t from the co variance matrix r t recently the basic lda model has been reﬁned to allow correlation among different topics which is more realistic see for example the correlated topic model ctm and the pachinko allocation model pam perhaps the most attractive aspect of our algorithm is that it extends to these models as well we can learn the topic matrix even though we cannot always identify t in real data there are always topics that are closely correlated or very anticorrelated and we believe that this extra generality is the reason our algorithm returns high quality topics on real data comparison with related works i we rely crucially on separability but prior works assume a single topic per document which can be thought of as a stronger separability assumption about w instead of a ii after posting a draft of this paper a subsequent paper by anandkumar et al gave an algorithm to recover parameters of an lda model without requiring a to be separable these results are incomparable since we require separability but can allow topic correlations we believe that allowing topic correlations is crucial when working with real data and have found empirical evidence that supports this conclusion iii we remark that some prior approaches learn the span of a instead of a require large document sizes on the order of the number of words in the dictionary by contrast we can work with documents of length deﬁnition i p separable topic matrix an n r matrix a is p separable if for each i there is some row π i of a that has a single nonzero entry which is in the ith column and it is at least p the next parameter measures the lowest probability with which a topic occurs in the distribution that generates columns of w deﬁnition i topic imbalance the topic imbalance of the model is the ratio between the largest and smallest expected entries in a column of w in other words a e xi maxi j r e x where x rr is a random weighting of j topics chosen from the distribution finally we require that topics stay identiﬁable despite sampling induced noise to formalize this we deﬁne a matrix that will be important throughout this paper deﬁnition i topic topic covariance matrix r t if t is the distribution that generates the columns of w then r t is deﬁned as an r r matrix whose i j th entry is e xi xj where xr is a vector chosen from t ii t ools for n oisy n onnegative m atrix factorization let γ be a lower bound on the condition number of the matrix r t this is deﬁned in section ii but for a r r matrix it is within a factor of r of the smallest singular value our algorithm will work for any γ but the number of documents we require will depend polynomially on γ a various condition numbers central to our arguments will be various notions of matrices being far from being low rank the most interesting one for our purposes was introduced by kleinberg and sandler in the context of collaborative ﬁltering and can be thought of as an analogue to the smallest singular value of a matrix theorem i main there is a polynomial time algorithm that learns the parameters of a topic model if the number of documents is at least log n r6 log r r4 m max o o γ n deﬁnition ii condition number if a matrix b has nonnegative entries and all rows sum to one then its condition number γ b is deﬁned as where the three parameters a p γ are as deﬁned above the algorithm learns the topic term matrix a up to additive error moreover when the number of documents is also larger the algorithm can learn the topic topic than o log r r covariance matrix r t up to additive error γ b min xb x if b does not have row sums of one then γ b is equal to γ db where d is the diagonal matrix such that db has row sums of one as noted earlier we are able to recover the topic matrix even though we do not always recover the parameters of the column distribution t in some special cases we can also recover the parameters of t e g when this distribution is for example if the rows of b have disjoint support then γ b and in general the quantity γ b can be thought of a measure of how close two distributions on disjoint sets of rows can be note that if x is an n dimensional real vector x x n x and hence if σmin b is the smallest singular value of b we have again by claim ii the matrix is γp robustly simplicial b noisy nmf under separability σmin b γ b mσmin b n a key ingredient is an approximate nmf algorithm from which can recover an approximate nonnegative matrix factorization m aw when the distance between each row of m and the corresponding row in aw is small we emphasize that this is not enough for our purposes since the term by document matrix m will have a substantial amount of noise when compared to its expectation precisely because the number of words in a document n is much smaller than the dictionary size n rather we will apply to the gram matrix m m t the algorithms given in the following theorem and its improvement in the subsequent theorem the above notions of condition number will be most relevant in the context of the topic topic covariance matrix r t we shall always use γ to denote the condition number of r t the deﬁnition of condition number will be preserved even when we estimate the topic topic covariance matrix using random samples lemma ii when m log r with high probability the matrix r m w w t is entry wise close to r t with error further when γ where a is topic imbalance the matrix r has condition number at least γ theorem ii robust nmf algorithm suppose m aw where w and m are normalized to have rows sum up to a is separable and w is γ robustly simplicial let time algorithm that given o γ there is a polynomial m such that for all rows m i m i ﬁnds a w i such that w w i γ further every row proof since e wi wit r t the ﬁrst part of the lemma follows by a chernoff bound and a union bound the second part follows because r t has condition number γ and for unit vector v the vector vr can change by at most ar r in norm the extra factor ar comes from the normalization of rows of r in our previous work on nonnegative matrix factorization we deﬁned a different measure of distance from singular which is essential to the polynomial time algorithm for nmf w i in w is a row in m the corresponding row in m can be represented as o γ w i o γ w i here w i is a vector in the convex hull of other rows in w with unit length in norm in this paper we have an incomparable goal than in our goal is not to recover estimates to the anchor words that are close in norm but rather to recover almost anchor words word whose row in a has almost all its weight on a single coordinate hence we will be able to achieve better bounds by treating this problem directly and we give a substitute for the above theorem the proof of the theorem can be found in the full version deﬁnition ii β robustly simplicial if each column of a matrix a has unit norm then we say it is β robustly simplicial if no column in a has distance smaller than β to the convex hull of the remaining columns in a the following claim clariﬁes the interrelationships of these latter condition numbers theorem ii suppose m aw where w and m are normalized to have rows sum up to a is separable and w is γ robustly simplicial when γ 100 there is a polynomial time algorithm that given m such that for all rows m i m i ﬁnds r row almost anchor words in m the i th almost anchor word corresponds to a row in m that can be represented as o γ w i o γ w i here w i is a vector in the convex hull of other rows in w with unit length in norm claim ii i if a is p separable then at has condition number at least p ii if at has all row sums equal to then a is β robustly simplicial for β γ at we shall see that the condition number for product of matrices is at least the product of the condition numbers the main application of this composition is to show that the matrix r t at or the empirical version rat is at least ω γp robustly simplicial the following lemma will play a crucial role in analyzing our main algorithm iii a lgorithm for l earning a t opic m odel p roof of t heorem i lemma ii composition lemma if b and c are matrices with condition number γ b γ and γ c β then γ bc is at least βγ speciﬁcally when a is p separable the matrix r t at is at least γp robustly simplicial first it is important to understand why separability helps in nonnegative matrix factorization and the exact role played by the anchor words suppose the nmf algorithm is given a matrix ab if a is p separable then this means that a contains a diagonal matrix up to row permutations thus a scaled copy of each row of b is present as a row in ab in fact if we knew the anchor words of a then by looking proof for any vector x we have xbc γ c xb γ c γ b x for the matrix r t at by claim ii we know the matrix at has condition number at least p hence γ r t at is at least γp and at the corresponding rows of ab we could read off the corresponding row of b up to scaling and use these in turn to recover all of a thus the anchor words constitute the key that unlocks the factorization and indeed the main step of our earlier nmf algorithm was a geometric procedure to identify the anchor words when one is given a noisy version of ab the analogous notion is almost anchor words which correspond to rows of ab that are very close to rows of b see theorem ii next we sketch how to apply these insights to learning topic models let m denote the given term by document matrix each of whose columns describes the empirical word frequencies in the documents it is obtained from sampling aw and thus is an extremely noisy approximation to aw our algorithm starts by forming the gram matrix m m t which can be thought of as an empirical word word covariance matrix in fact as the number of documents increases t tends to a limit q m e aw w t a implying mmm t q ar t a see lemma iii imagine that we are given the exact matrix q instead of a noisy approximation notice that q is a product of three nonnegative matrices the ﬁrst of which is p separable and the last is the transpose of the ﬁrst nmf at ﬁrst ﬂance seems too weak to help ﬁnd such factorizations however if we think of q as a product of two nonnegative matrices a and r t at then our nmf algorithm can at least identify the anchor words of a as noted above these sufﬁce to recover r t at and then using the anchor words of a again all of a as well see section iii a for details the complication is that we are not given q but merely a good approximation to it now our nmf algorithm allows us to recover almost anchor words of a and the crux of the proof is section iii b showing that these sufﬁce to recover provably good estimates to a and w w t this uses mostly bounds from matrix perturbation theory and interrelationships of condition numbers mentioned in section ii for simplicity we assume the following condition on the topic model which we will see in section iii d can be assumed without loss of generality the number of words n is at most please see algorithm main algorithm for description of the algorithm note that r is our shorthand for m wwt which as noted converges to r t as the number of documents increases proof the lemma is straight forward from figure and the procedure by figure we can ﬁnd the exact value of drat and drd in the matrix q step of recover computes dr by computing drat the two vectors are equal because a is the topic term matrix and its columns sum up to in particular at in step since r is invertible by lemma ii d is a diagonal matrix with entries at least p the matrix drd is also invertible therefore there is a unique solution z drd dr d also d z and hence ddiag z i finally using the fact that ddiag z i the output in step is just dr drat at and the output in step is equal to r a recover r and a with anchor words b recover r and a with almost anchor words we ﬁrst describe how the recovery procedure works in an idealized setting algorthm r ecover with t rue a nchor w ords when we are given the exact value of arat and a set of anchor words one for each topic we can permute the rows of a so that the anchor words are exactly the ﬁrst r words therefore at d u t where d is a diagonal matrix note that d is not necessarily the identity matrix nor even a scaled copy of the identity what if we are not given the exact anchor words but are given words that are close to anchor words in general we cannot hope to recover the true anchor words but nevertheless a good approximation to these will be enough to recover r and a when we restrict a to the rows corresponding to almost anchor words the submatrix will not be diagonal however d d w u wt u drd drut urd urut r figure the matrix q matrix but we do know that the diagonal entries are at least p we apply the same permutation to the rows and columns of q as illustrated in figure the submatrix formed by the ﬁrst r rows and r columns is exactly drd similarly the submatrix consisting of the ﬁrst r rows is exactly drat we can use these two matrices to compute r and a in this idealized setting and we will use the same basic strategy in the general case but need only be more careful about how the errors accumulate in our algorithm our algorithm has exact knowledge of the matrices drd and drat and so the main task is to recover the diagonal matrix d given d we can then compute a and r for the dirichlet allocation we can also compute its parameters i e the α so that r α r the key idea to this algorithm is that the row sums of dr and drat are the same and we can use the row sums of dr to set up a system of linear constraints on the diagonal entries of d lemma iii when the matrix q is exactly equal to arat and we know the set of anchor words r ecover with t rue a nchor w ords outputs a and r correctly algorithm m ain a lgorithm output r and a query the oracle for m documents where log r log r r4 log n r6 o o m max o p γ n split the words of each document into two halves and let m m be the term by document matrix with ﬁrst and second half of words respectively compute word by word matrix q n m m t apply the robust nmf algorithm of theorem ii to q which returns r words that are almost the anchor words of a use these r words as input to r ecover with a lmost a nchor w ords to compute r m w w t and a algorithm r ecover with t rue a nchor w ords input r anchor words output r and a permute the rows and columns of q so that the anchor words appear in the ﬁrst r rows and columns compute drat which is equal to dr solve for z drd z dr output at drddiag z drat output r diag z drddiag z be the largest absolute value of any entry of b bmax is achieved we max bi consider the entry i where bmax know bmax bi zb i j zi j bj bmax thus bmax now all the entries in z b are within in absolute value and we know that b z b hence all the entries of b are in the range as desired lemma iii let e i z and i j zi j then the columns of e i have norm at most it will be close to a diagonal in the sense that the submatrix will be a diagonal matrix d multiplied by e where e is entry wise close to the identity matrix and the diagonal entries of d are at least ω p here we analyze the same procedure as above and show that it still recovers a and r approximately even when given almost anchor words instead of true anchor words for clarity we state the procedure again in algorithm r ecover with a lmost a nchor w ords the guarantees at each step are different than before but the implementation of the procedure is the same notice that here we permute the rows of a and hence the rows and columns of q so that the almost anchor words returned by theorem ii appear ﬁrst and the submatrix a on these rows is equal to de here we still assume that the matrix q is exactly equal to arat and hence the ﬁrst r rows of q form the submatrix derat and the ﬁrst r rows and columns are dere t d the complication here is that diag z is not necessarily equal to d since the matrix e is not necessarily the identity however we can show that diag z is close to d if e is suitably close to the identity matrix i e given good enough proxies for the anchor words we can bound the error of the above recovery procedure we write e i z intuitively when z has only small entries e should behave like the identity matrix in particular e should have only small off diagonal entries we make this precise through the following lemmas lemma iii let e i z and i j zi j then e is a vector with entries in the range proof without loss of generality we can consider just the ﬁrst column of e i which is equal to e i e where e is the indicator vector that is one on the ﬁrst coordinate and zero elsewhere the approach is similar to that in lemma iii let b e i e left multiply by e i z and we obtain b z b z e hence b z b e let bmax be the largest absolute value of entries of b bmax max bi let i be the entry in which bmax is achieved then bmax bi z b i z e i bmax therefore bmax further the b z e z b now we are ready to show that the procedure r ecover with a lmost a nchor w ords succeeds when given almost anchor words lemma iii when the matrix q is exactly equal to arat the matrix a restricted to almost anchor words is de where e i has norm when viewed as a vector procedure r ecover with a lmost a nchor w ords outputs a such that each column of a has error at most the matrix r has additive error zr whose norm when viewed as a vector is at most proof e is clearly invertible because the spectral norm of z is at most let b e since e i z we multiply e on both sides to get b z b let bmax algorithm r ecover with a lmost a nchor w ords input r almost anchor words output r and a permute the rows and columns of q so that the almost anchor words appear in the ﬁrst r rows and columns compute derat which is equal to der solve for z dere t d z der output at dere t ddiag z derat output r diag z dere t ddiag z proof since q is exactly arat our algorithm is given derat and dere t d with no error in step since d e and r are all invertible we have of order and can safely be bounded by for suitably small finally we consider the general case in which there is additive noise in step we are not given arat exactly we are given q which is close to arat by lemma iii we will bound the accumulation of this last type of error suppose in step of recover we obtain derat u and dere t d v and furthermore the entries of u and u have absolute value at most and the matrix v has norm when viewed as a vector z dere t d der d e t ideally we would want diag z d and indeed ddiag z diag e t from lemma iii the vector e t has entries in the range thus each entry of diag z is within a multiplicative factor from the corresponding entry in d consider the output in step since d e r are invertible the ﬁrst output is dere t ddiag z derat ddiag z e t a t our goal is to bound the error of the columns of the output compared to the corresponding columns of a notice that it is sufﬁcient to show that the j th row of ddiag z e t is close in distance to the indicator vector e j t the main idea of the proof is to write dere t d v as der e t v d in this way the error v can be translated to an error v on e and lemma iii can be applied proof we shall follow the proof of lemma iii first can express the error term v instead as v der v d this is always possible because all of d e r are invertible moreover the norm of v when viewed as a vector is at most because this norm will grow by a factor of at most p when multiplied by d a factor of at most when multiplied by e and at most ra γ r when multiplied by r the bound of γ r comes from lemma ii we lose an extra ra because r may not have rows sum up to hence dere t d v der e t v d and the additive error for dere t d can be transformed into error in e and we will be able to apply the analysis in lemma iii similarly we can express the error term u as u deru entries of u have absolute value at most r the right hand side of the equation in step is equal to der u so the error is at most per entry following the proof of lemma iii we know diag z d has diagonal entries within now we consider the output the output for at is equal to der e t v ddiag z der at u which is ddiag z e t v at u here we know e t v i has norm at most o ra per row ddiag z is a diagonal matrix with entries in o ra entries of u has absolute value o r following the proof of lemma iii the ﬁnal entry wise error claim iii for each j e j t ddiag z e t e j t proof again without loss of generality we can consider just the ﬁrst row from lemma iii e t e t has distance at most to e t ddiag z has entries in the range and so e t ddiag z e t e t lemma iii if are sufﬁciently small recover outputs a such that each entry of a has additive error at most o ra r γ also the matrix r has additive error zr whose norm when viewed as a vector is at most o ra r γ e t ddiag z e t e t e t e t e t e t the last term can be bounded by consider the ﬁrst term on the right hand side the vector e t ddiag z e t has one non zero entry the ﬁrst one whose absolute value is at most hence from lemma iii the ﬁrst term can be bounded by and this implies the claim the ﬁrst row of ddiag z e t at is z t a where z is a vector with norm at most so every column of a is recovered with error at most consider the second output of the algorithm the output is diag z dere t ddiag z and we can write diag z d i and e i the leading error are r r and hence the norm of the leading error term when treated as a vector is at most and other terms are of a is roughly the sum of these three errors and is bounded by o ra r γ notice that lemma iii gives bound for norm of rows which is stronger here we switched to entry wise error because the entries of u are bounded while the norm of u might be large similarly the output of r is equal to diag z dere t d v diag z again we write diag z d i and e i the extra term diag z v diag z is small because the entries of z are at most to p otherwise diag z d won t be close to identity the error can be bounded by o ra r γ multivariate probability distributions the support of the dirichlet distribution is the unit simplex whose dimension is the same as the dimension of α let α be a r dimensional vector then for a vector θ rr in the r dimensional simplex its probability density is given by γ r αi r αi p r θ α r i where γ is the gamma i θi i γ αi function in particular when all of the αi are equal to one the dirichlet distribution is just the uniform distribution on the probability simplex the expectation and variance of θi are easy to compute r given the parameters α we denote α i αi then the ratio αi should be interpreted as the size of the i th variable θi and controls whether the distribution is concentrated in the interior when is large or near the boundary when is small the ﬁrst two moments of the αi dirichlet distribution αi αj are e θi when i j e θi θj αi αi when i j suppose the dirichlet distribution has max αi min αi a and the sum of parameters is we give an algorithm that computes close estimates to the vector of parameters α given a sufﬁciently close estimate to the co variance matrix r t theorem iv combining this with theorem i we obtain the following corollary now in order to prove our main theorem we just need to show that when number of documents is large enough the matrix q is close to the arat and plug the error bounds into lemma iii we state the convergence of q below and defer the details to the full version c error bounds for q here we state the error bound for matrix q whose proof we defer to the full version log n n t t lemma iii when m with high probability all aw w a have absolute value at most entries of q m q further the norm of rows of q are also q close to the norm of the corresponding row in m aw w t at theorem iv there is an algorithm that learns the topic matrix a with high probability up to an additive error of from at most log n a6 r8 m max o n log r r4 o d reducing dictionary size so far we have assumed that the number of distinct words is not too large here we give a simple gadget to demonstrate that this is true without loss of generality lemma iii the general case can be reduced to an instance in which there are at most words all of which with at most one exception occur with probability at least documents sampled from the lda model and runs in time polynomial in n m furthermore we recover the parameters of the dirichlet distribution to within an additive the proof is straightforward and the idea is to collect all words that occur infrequently and merge all of these words into a aggregate word that we will call the runoff word we defer the proof to the full version our main goal in this section is to bound the condition number of the dirichlet distribution see section iv a and using this we show how to recover the parameters of the distribution from its covariance matrix section iv b iv t he d irichlet s ubcase here we demonstrate that the parameters of a dirichlet distribution can be robustly recovered from just the covariance matrix r t hence an immediate corollary is that our main learning algorithm can recover both the topic matrix a and the distribution that generates columns of w in a latent dirichlet allocation lda model provided that a is separable we believe that this algorithm may be of practical use and provides the ﬁrst alternative to local search and unproven approximation procedures for this inference problem the dirichlet distribution is parametrized by a vector α of positive reals and is a natural family of continuous a condition number of a dirichlet distribution there is a well known meta principle that if a matrix w is chosen by picking its columns independently from a fairly diffuse distribution then it will be far from low rank however our analysis will require us to prove an explicit lower bound on γ r t we now prove such a bound when the columns of w are chosen from a dirichlet distribution with parameter vector α we note that it is easy to establish such bounds for other types of distributions as well recall that we deﬁned r t in section i and here we will abuse notation and throughout this section we will denote by r α algorithm d irichlet r input r output α vector of parameters set α r let i be the row with smallest norm let u ri i and v αi u v set u v v output α α matrix a that has the largest probability of generating the observed documents when the columns of w are generated by a uniform dirichlet distribution the matrix r t where t is the dirichlet distribution with parameter α r let i αi the mean variance and co variance for a dirichlet distribution are well known from which we α αj when i j and observe that r α i j is equal to αi is equal to αi αi surprisingly this appears to be the ﬁrst proof that computing the mle estimate in a topic model is indeed computationally hard although its hardness is certainly to be expected on a related note sontag and roy recently proved that given the topic matrix and a document computing the maximum a posteriori map estimate for the distribution on topics that generated this document is n p hard here we will establish that tm mle is n p hard via a reduction from the min bisection problem in minbisection the input is a graph with n vertices n is an even integer and the goal is to partition the vertices into two equal sized sets of n vertices each so as to minimize the number of edges crossing the cut when i j lemma iv the condition number of r α is at least proof as the entries r α i j is αi αj when i j i and α when i j after normalization r α is just the matrix d α i where is outer product and i is the identity matrix let x be a vector such that x and d x achieves the minimum in γ r α and let i i xi and let j i be the complement we can assume without loss of generality that i i xi i j xi otherwise just take xi α x the ﬁrst x instead the product d x is term is a nonnegative vector and hence for each i i d x i this implies that d x i i xi theorem v there is a polynomial time reduction from min bisection to tm mle r we defer the proof to the full version we remark that the canonical solutions in our reduction are all separable and hence this reduction applies even when the topic matrix a is known and required to be separable so even in the case of a separable topic matrix it is n p hard to compute the mle yet here we have given an efﬁcient estimator that converges to the true separable topic matrix a when the data is actually generated according to the lda model b recovering the parameters of a dirichlet distribution when the covariance matrix r α is recovered with error r in norm when viewed as a vector we can use algorithm d irichlet to compute the vector α theorem iv when the covariance matrix r α is recovered with error r in norm when viewed as a vector the procedure d irichlet r learns the parameter of the dirichlet distribution with error at most o ar r vi c onclusions though the goal of the paper is design of an algorithm with theoretical guarantees the actual algorithm turns out to be practical a straightforward implementation using a more efﬁcient lp free subroutine to ﬁnd anchor words but no other tuning runs much faster than state of the art software for topic models and gives results of comparable quality for example on the uci bag of words dataset with new york times articles we ﬁt topics in only minutes on a dataset with articles with a vocabulary of size whereas mallet takes several hours a detailed study of its performance is underway and will be reported soon the separability assumption seems benign on such datasets in fact our machine learning colleagues suggest that real life topic matrices satisfy even stronger separability assumptions e g the presence of many anchor words per topic instead of a single one leveraging this promising suggestion is an open problem proof the αi all have error at most r the value u is r and the value v is αi r since v ar we know the error for u v is at most r finally αi we need to bound the denominator αi since r thus the ﬁnal error is at most α0 r v m aximum l ikelihood e stimation is h ard here we observe that computing the maximum likelihood estimate mle of the parameters of a topic model is n p hard we call this problem the topic model maximum likelihoood estimation tm mle problem deﬁnition v tm mle given m documents and a target of r topics the tm mle problem asks to compute the topic computer science division uc berkeley berkeley ca sanjamg berkeley edu this author was at ucla when this work was done and was supported in part from nsf grants and a xerox faculty research award a google faculty research award an equipment grant from intel and an okawa foundation research grant ibm research hawthorne ny craigbgentry gmail com shaih alum mit edu these authors were supported by the intelligence advanced research projects activity iarpa via department of interior national business center doi nbc contract number the u s government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation thereon department of computer science yale university new haven ct mariana cs columbia edu this author was at ibm research when this work was done she was supported in part from nsf grants department of computer science ucla los angeles ca sahai cs ucla edu this author was supported in part from a darpa arl safeware award nsf frontier award nsf grants and a xerox faculty research award a google faculty research award an equipment grant from intel and an okawa foundation research grant this material is based upon work supported by the defense advanced research projects agency through the arl under contract c university of texas at austin austin tx bwaters cs utexas edu this author is supported by nsf cns and cns cns darpa google faculty research award the alfred p sloan fellowship microsoft faculty fellowship and packard foundation fellowship copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption introduction in this work we study two long standing open feasibility questions in cryptography secure program obfuscation and functional encryption obfuscation roughly speaking program obfuscation aims to make a computer program unintelligible while preserving its functionality the formal study of program obfuscation was initiated by hada and barak et al bgi bgi hada observed that a super strong notion of obfuscation requiring that the obfuscated code does not leak anything beyond what can be learned given black box oracle access to the underlying function cannot be met unless the obfuscated function is learnable barak et al deﬁned a slightly weaker but still very strong notion of virtual black box vbb obfuscation roughly requiring that the obfuscated circuit does not leak any predicate of the obfuscated function beyond what can be learned given black box oracle access to that function unfortunately they showed that this notion too is impossible to achieve for general programs in a very strong sense they showed that there exist unobfuscatable functions a family of functions fs such that given any circuit that implements fs an eﬃcient procedure can extract the secret however any eﬃcient adversary given only black box access to fs cannot guess even a single bit of with nonnegligible advantage indeed under widely believed intractability assumptions such unobfuscatable function families exist with very low circuit complexity i e in so one cannot hope for general purpose obfuscators achieving a natural simulation based deﬁnition of obfuscation even for very low complexity classes faced with this impossibility result barak et al bgi bgi suggested another notion of program obfuscation called indistinguishability obfuscation an indistinguishability obfuscator io for a class of circuits c guarantees that given two equivalent circuits and from the class such that the two distribution of obfuscations io and io should be computationally indistinguishable we note that if the circuit class c has eﬃciently computable canonical forms speciﬁcally a form that can be eﬃciently obtained given any implementation of that circuit then the computation of that canonical form would already be an indistinguishability obfuscator bgi since their introduction in it has been an open problem to construct indistinguishability obfuscators for any natural circuit class that is not known to admit eﬃciently computable canonical forms in particular the central open question regarding indistinguishability obfuscation has been the following do there exist indistinguishability obfuscators for all polynomial size circuits it is important to note that unlike simulation based deﬁnitions of obfuscation it is not immediately clear how useful indistinguishability obfuscators would be perhaps the strongest philosophical justiﬁcation for indistinguishability obfuscators comes from the work of goldwasser and rothblum who showed that eﬃciently computable indistinguishability obfuscators achieve the notion of best possible obfuscation informally a best possible obfuscator guarantees that its output hides as much about the input circuit as any other implementation of the same circuit of a certain size nevertheless very few applications of indistinguishability obfuscators were described in the literature in fact the only application that we are aware of is to removing software watermarks bgi the main contributions of this work are to construct indistinguishability obfuscators for all circuits and show how to use indistinguishability obfuscators to solve the central open problem in the area of functional encryption functional encryption in functional encryption ciphertexts encrypt inputs x and keys are issued for strings y the striking feature of this system is that using the copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters key sky to decrypt a ciphertext ctx enc x yields the value f x y but does not reveal anything else about x where f is some public universal circuit ﬁxed in advance furthermore any arbitrary collusion of key holders relative to many strings yi does not yield any more information about x beyond what is naturally revealed by the functionality i e f x yi for all i both simulation based and indistinguishabilitybased deﬁnitions of fe have been considered in the literature here we will restrict the discussion to indistinguishability based deﬁnition and elaborate on this in section functional encryption started with the notion of attribute based encryption abe and evolved over time the term functional encryption ﬁrst appeared in and formal deﬁnitions and constructions were subsequently given in o earlier inﬂuences include identity based encryption and searching on encrypted data the general notion of functional encryption is stronger than abe in abe encrypted messages consist of a secret payload m and a public attribute string x where the secret key sky permits the recovery of the payload m if and only if f x y while both abe and fe require full collusion resilience the critical diﬀerence between abe and general fe is that abe does not ensure the secrecy of the attribute string x recent breakthrough works ggh show how to achieve abe for arbitrary circuits f but these constructions do not provide any secrecy for the x input in contrast to abe existing fe schemes prior to the current work are much more limited in power with the state of the art roughly limited to the inner product construction of katz sahai and waters and its adaptation to lattice based cryptography by agrawal mandell freeman and vaikuntanathan in these systems ciphertexts and keys are associated with vectors x y and on decryption we only learn if the dot product x y is zero while there are interesting applications of this basic functionality it is a far cry from the goal of realizing functional encryption for general functions or even a large general class of circuits a separate line of interesting works has realized constructions that achieve only limited collusion notions of security and also several impossibility results for strong simulation based security notions we discuss these in section below the central open question regarding functional encryption has been the following does there exist a functional encryption scheme supporting all polynomial size circuits our results our ﬁrst result is a candidate construction of an indistinguishability obfuscator io for all polynomial size log depth circuits i e the security of our construction relies on a new algebraic hardness assumption we provide evidence for the plausibility of our assumption by proving it to be true in a speciﬁc generic model that seems to capture most natural attacks on this type of construction our construction and assumption are staged in a framework that we call multilinear jigsaw puzzles multilinear jigsaw puzzles are a simpliﬁed variant of multilinear maps in which only the party who generated the system parameters can encode elements in the exponent of the diﬀerent groups these multilinear jigsaw puzzles can be constructed by eliminating some components from the ggh asymmetric approximate multilinear maps most notably we eliminate the elements of the ggh setup that give rise to the most potent cryptanalytic attacks including the weak discrete log attack this is essential for the security of our construction we can also instantiate multilinear jigsaw puzzles by eliminating some parts from the recent work of over the integers we note that multilin copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption ear jigsaw puzzles are just a simplifying framework for presenting our construction and essentially the same as previous multilinear maps constructions after constructing indistinguishability obfuscators for from multilinear jigsaw puzzles we use these indistinguishability obfuscators in a black box manner to obtain the following results using indistinguishability obfuscator for together with any leveled fully homomorphic encryption fhe scheme with decryption in e g we show how to obtain an indistinguishability obfuscator for all polynomial size circuits the essential new idea here is to use a two key encryption technique similar in spirit to naor yung using indistinguishability obfuscator for polynomial size circuits together with injective one way functions public key encryption and a novel variant of sahai simulation sound noninteractive zero knowledge proofs we show how to obtain functional encryption schemes supporting all polynomialsize circuits our construction achieves selective indistinguishability security which can be ampliﬁed to full indistinguishability security using complexity leveraging finally using the recent transformation of de caro et al cij one can obtain meaningful simulation based security for functional encryption for all circuits our construction furthermore achieves succinct ciphertexts the size of the ciphertexts in our scheme does not depend on potential secret key circuit sizes or even its depth nor do they depend on the parameter sizes needed for multilinear jigsaw puzzles we emphasize that these results use our indistinguishability obfuscator for as a black box and do not need to make further use of multilinear jigsaw puzzles we are able to obtain these results through a novel use of indistinguishability obfuscators by making an intuitive connection between the use of indistinguishability obfuscation and witness indistinguishable proofs there is more on this below multilinear jigsaw puzzles for our construction we use a variant of multilinear maps that oﬀers only a strict subset of the functionality and we term this variant multilinear jigsaw puzzles these are similar to the asymmetric version of the ggh multilinear encoding schemes except that in our setting only the party that generated the system parameters is able to encode elements below is an intuitive description of multilinear jigsaw puzzles using multiplicative notation for groups a more formal treatment can be found in section although technically inaccurate it is useful to think of an instance of a multilinear jigsaw puzzle scheme as including a multilinear map system over groups of prime order p e gk gt the system parameters allows a user to perform group and multilinear operations but the parameters need not include canonical generators for the diﬀerent groups gi gi and gt gt a valid multilinear form over such system is anything that can be computed using the group operation in the separate groups and the multilinear map for example for k with variables xi yi zi and wi gt the expression e e is a valid form in the multilinear jigsaw puzzle we view group elements as the puzzle pieces the intuitive analogy to jigsaw puzzles is that these group elements can only be combined copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters in very structured ways like jigsaw puzzle pieces diﬀerent puzzle pieces either ﬁt together or if they do not ﬁt then they cannot be combined in any meaningful way we view a valid multilinear form in these elements as a suggested solution to this jigsaw puzzle a valid multilinear form suggests ways to interlock the pieces together a solution is correct if evaluating the multilinear form on the given elements yields the unit in the target group gt thus a multilinear jigsaw puzzle scheme consists of just two algorithms the jigsaw generator that generates the system parameters and some group elements and the jigsaw veriﬁer that checks whether a given solution is correct for these elements in more detail the jigsaw generator outputs some system parameters prms and k i i nonempty sets of elements si xni gi for i k t the jigsaw veriﬁer takes as input prms sk st π where π is a valid multilinear form in these elements and it outputs yes if π evaluates to the unit element in gt on the given elements and no otherwise note that the multilinear jigsaw puzzle scheme itself does not specify which particular multilinear form the jigsaw veriﬁer should apply this choice will typically come from the application the hardness assumptions that we make typically say that two output distributions of the jigsaw generator are computationally indistinguishable for instance an asymmetric analogue of the bddh assumption for k could be that gt gtabc is indistinguishable from gt gtr where gt are generators of and gt respectively that satisfy e gt indistinguishability obfuscation for we now sketch some elements of our candidate construction of indistinguishability obfuscator for using multilinear jigsaw puzzles a more detailed high level overview of our construction can be found in section and the construction itself is described in section it is known that any circuit c can be transformed into a polynomiallength oblivious matrix branching program that is it can be transformed into a collection of k square matrices and another k square matrices the computation of the original circuit on an input x of length can then be reduced to a matrix product as follows where f k is a public function independent of c that is ﬁxed in advance c x iﬀ k x mi f i i i one such transformation which is suited for our purposes is using barrington celebrated theorem other transformations are also possible and barrington theorem was avoided for building obfuscation ﬁrst in in the context of ﬁnding more eﬃcient constructions of obfuscation similarly in blr alternative transformations avoiding barrington theorem were used for the purpose of achieving greater eﬃciency our starting intuition is that the branching program evaluation is very well suited for the setting of multilinear maps the entries of the matrices and can be given in the group gi for instance if the entry of is α we can encode this as giα then the multilinear maps will allow us to compute the ﬁnal product in gt we remark that multilinear maps allow us to multiply k matrices together not just k elements however simply giving these matrices even when encoded in the exponent is insecure for example an attacker in the description above can multiply diﬀerent ma copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption trices together disregarding function f instead we apply a combination of kilian matrix product randomization technique together with novel multiplicative randomization techniques that we develop this yields a construction of an indistinguishability obfuscator for and a corresponding assumption we justify our assumption by showing that it holds in a generic matrix model how to use indistinguishability obfuscation now that we have constructed an indistinguishability obfuscator we are faced with the question what good is an indistinguishability obfuscator the deﬁnition of indistinguishability obfuscation does not make clear what if anything an indistinguishability obfuscator actually hides about a circuit in particular if the circuit being obfuscated was already in an obvious canonical form then we know that the indistinguishability obfuscator would not need to hide anything we observe here an analogy to witness indistinguishable proofs if a statement being proved only has a unique witness then a witness indistinguishable proof does not need to hide the witness the way witness indistinguishability can be used is by explicitly constructing statements that can have multiple witnesses similarly we will use indistinguishability obfuscation by constructing circuits that inherently have multiple equivalent forms we use this analogy to build our main application of functional encryption warmup witness encryption for np as a warmup to see how indistinguishability obfuscation can be applied we start by considering a cryptographic notion which implicitly already considers circuits that inherently have multiple equivalent forms recall the notion of witness encryption for np recently introduced in given an np language l a witness encryption scheme for l is an encryption scheme that takes as input an instance x and a message bit b and outputs a ciphertext c if x l and w is a valid witness for x then a decryptor can use w to decrypt c and recover b however if x l then an encryption of should be computationally indistinguishable from an encryption of we now observe that indistinguishability obfuscation for implies witness encryption for an np complete language such as l satisfiability consider the point ﬁlter function function fx b w deﬁned as follows if w is a valid witness for x then fx b w b if w is an invalid witness for x then fx b w note that for satisfiability it is immediate that fx b is in now consider an indistinguishability obfuscation of fx b this obfuscated circuit is a valid witness encryption for x b correctness of decryption is immediate to see why secrecy holds note that if x l then both fx and fx are constant functions that always output thus by the deﬁnition of indistinguishability obfuscation the obfuscations of fx and fx are computationally indistinguishable indistinguishability obfuscation for all polynomial size circuits next we show how to use indistinguishability obfuscation for and leveled fhe with decryption in to obtain indistinguishability obfuscation for all polynomialsize circuits the main idea is to adapt the two key paradigm to work using indistinguishability obfuscators instead of witness indistinguishable proofs this construction is described in section to obfuscate a circuit c we choose and publish two fhe keys and we also give out encryptions of the circuit c under both fhe public keys yielding ciphertexts and finally we give out an indistinguishability obfuscation of a certain circuit that we will describe below the evaluator who holds an input x is told to use the fhe evaluation algorithm with x and both ciphertexts and to yield encryptions of c x under both copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters and let us call these encryptions and respectively the evaluator also keeps track of all the intermediate bit values encountered during evaluation which can be seen as a proof π that it used the same x to perform the evaluation on both and the evaluator then feeds x π into the obfuscated circuit io this circuit ﬁrst checks the proof π to make sure that and were correctly computed note that this can be done easily in since the evaluator has included the transcript of the homomorphic evaluation computation as part of the proof so the circuit merely needs to check that each appropriate triple of intermediate bit values respects the and or or nand operations that were used during the evaluation process if the proof checks out then the circuit decrypts using and outputs this decryption which should be c x the key insight is that there is another circuit that is equivalent to which simply decrypts using instead because of the proof π that must be provided both of these circuits always behave identically on all inputs furthermore when using we note that is never used anywhere and therefore the semantic security of the fhe scheme using is maintained even given thus by alternatively applying the semantic security of the fhe scheme and switching back and forth between and using the indistinguishability obfuscation property we can prove that the obfuscation of any two equivalent circuits c and c are computationally indistinguishable functional encryption for all circuits we next sketch our main application of building functional encryption for all circuits this construction is described in section the starting point for our solution is to pick a standard public key encryption key pair pk sk in the setup phase of the functional encryption an encryption of a value x will simply be an encryption of x using the public key pk the secret key skc corresponding to a circuit c will be an obfuscation of a program that uses sk to decrypt x and then computes and output c x while this solution would work if our obfuscator achieved the virtual black box obfuscation deﬁnition there is no reason to believe that an indistinguishability obfuscator would necessarily hide sk the indistinguishability security deﬁnition for functional encryption requires the adversary to declare two inputs and with the promise that all secret keys skc that she will ask for will satisfy c c then the security deﬁnition requires that the adversary will not be able to distinguish an encryption of from an encryption of a natural ﬁrst step would be to have two public keys and and require the encryption of x to consist of encryptions of x under both keys however in this case unlike above the receiver cannot generate a proof on his own that both ciphertexts encrypt the same message to provide to the obfuscated decryption circuit thus we must require the encryptor to generate such a proof which must hide x a natural solution is to have the encryptor generate a noninteractive zero knowledge nizk proof that the ciphertexts both encrypt the same input the obfuscated circuit would ﬁrst check this proof and if it checks out it would use one of the two secret keys or for decryption and evaluation conﬁdent that the output would be the same no matter which secret key it uses when we are proving that the adversary cannot distinguish an encryption of from an encryption of we could move to an intermediate hybrid experiment where the nizk proof is simulated and the two ciphertexts are actually to x0 and respectively here we encounter the key new problem that we must tackle when simulating a nizk proof we need to change the common reference string and in all known nizk copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption systems with respect to a simulated common reference string valid proofs of false statements exist however the notion of indistinguishability obfuscation requires that circuits be equivalent for all inputs even inputs to the circuit that contain valid proofs for false statements we address this problem by introducing the notion of statistically simulation sound nizk which requires that except with respect to one particular statement to be simulated all other valid proofs that exist must be only for true statements statistical simulation soundness can be achieved in a standard way using witness indistinguishable proofs once we have it then a similar two key alternation argument to the one given above allows us to establish the security of our functional encryption scheme note that the one false statement that can be proved is such that the it does not eﬀect the equivalence of the two circuits we note that full collusion resistance follows from a standard hybrid argument using the indistinguishability obfuscation deﬁnition for each secret key given out one at a time once we achieve indistinguishability security for functional encryption this can be upgraded to natural simulation based deﬁnitions of security using the work of de caro et al cij furthermore we note that our construction enjoys the property that ciphertexts are succinct indeed their size depends only on the publickey encryption scheme and nizk being used and does not depend on the underlying details of the obfuscation mechanism in any way and therefore it does not depend on the parameters needed for our multilinear jigsaw puzzles we also place no a priori bound on the circuit size of the circuits used for secret key generation ciphertext succinctness and applications one thing we emphasize is that ciphertexts in our scheme are very succinct in that their size depends only upon the message size and security parameter in fact if the right combination of public key encryption and nizk is used the ciphertext size and encryption time can be considered small in a practical sense prior solutions for the weaker primitives of abe for circuits ggh and single use functional encryption gkp achieved ciphertexts sizes and encryption times that were proportional to the maximum depth of the function to be evaluated it should be noted that these solutions rely on weaker assumptions following parno raykova and vaikuntanathan we get a delegatable computation scheme for which the client work is independent of the circuit size following goldwasser et al gkp we get a new reusable yao garbled circuit construction for which the work to reuse a garbled circuit is proportional to the input size and independent of the size and depth of the circuit finally we get an eﬃcient private linear broadcast encryption scheme as described by boneh sahai and waters which they show implies a secure traitor tracing system this ﬁnal application requires collusion resistance an application of io to restricted use software we have already given evidence that indistinguishability obfuscation has signiﬁcant applications to cryptography here we discuss an application of indistinguishability obfuscation to a question that does not deal with encryption or authentication but rather restricted use software software developers will often want to release a demo or restricted use version of their software that limits the features that are available in a full version in some cases because this statement must be ﬁxed in advance the resulting security proof is for selective security selective security can be upgraded to full security using a standard complexity leveraging argument by assuming subexponential security of the underlying cryptographic primitives that we use copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters a commercial software developer will do this to demonstrate their product in other cases the developer will want to make multiple tiers of a product with diﬀerent price points in other domains the software might be given to a partner that is only partially trusted and the developer only wants to release the features needed for the task ideally a developer could create a downgraded version of software simply by starting with the full version and then turning oﬀ certain features at the interface level requiring minimal additional eﬀort however if this is all that is done it could be easy for an attacker to bypass these controls and gain access to the full version or the code behind it the other alternative is for a software development team to carefully excise all unused functionality from the core of the software removing functionality can become a very time consuming task that could itself lead to the introduction of software bugs in addition in many applications it might be unclear what code can and cannot remain for a restricted use version one immediate solution is for a developer to restrict the use at the interface level and then release an obfuscated version of the program for this application indistinguishability obfuscation suﬃces since by deﬁnition a version restricted in the interface is indistinguishable from an obfuscated program with equivalent behavior that has its smarts removed at the start stronger notions of obfuscation in this work we give new constructions for secure obfuscation while indistinguishability obfuscation is the technical focus of our work we note that we know of no actual attacks on our obfuscation scheme as a virtual black box obfuscator except for the attacks that arise on speciﬁc circuit classes that are known to be unobfuscatable this leads to the intriguing possibility that our construction or future alternative constructions could achieve virtual black box obfuscation for a large class of circuits that somehow exclude the problematic examples that have been identiﬁed indeed as observed by goldwasser and rothblum indistinguishability obfuscation must yield virtual black box obfuscation if such a virtual black box obfuscation is possible for the function being obfuscated in particular while much study still needs to be done we currently have no reason to belive that our obfuscation mechanism does not suﬃce for such applications as the following secure patching of software if a new malware vulnerability is found in software there is a risk that releasing a software patch will allow attackers to become aware of a vulnerability before the patch has a chance to fully circulate among users obfuscation oﬀers a solution concept an initial patch can be released in obfuscated form and then transitioned to a more eﬃcient unobfuscated patch once large scale adoption has occurred for the initial patch here the assumption would be that the obfuscated patch would hide where the vulnerability in the software was at least as well as the original vulnerable software did intellectual property protection if a new algorithm is developed for solving a problem and there is a worry that available legal protections will not suﬃce for preventing reverse engineering of the algorithm then obfuscation oﬀers a natural solution here the general required security property seems very close to virtual black box obfuscation but if only certain parts of the algorithm are novel weaker yet suﬃcient obfuscation security guarantees may be formalizable in some contexts future directions in obfuscation our work opens up several new possibilities in obfuscation below we discuss some of them copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption the power of indistinguishability obfuscation while a virtual black box obfuscator immediately results in several turnkey applications our work shows that indistinguishability obfuscation combined with adroit use of other cryptographic primitives can give rise to powerful functionalities seeing that indistinguishability obfuscation can be much more powerful than it initially appears an important line of research is to see how far we can push the limits of what it gives us indistinguishability obfuscation from weaker assumptions the argument of security for our candidate indistinguishability obfuscator clearly has a significant heuristic component in future research we can hope to gain better understanding of these heuristics and eventually achieve constructions under more standard assumptions a major open problem is to obtain an unconditional proof for our scheme or a variant of it in a less idealized generic model such as the plain generic multilinear map model as opposed to the generic colored matrix model that we use the eventual goal of this line of research may be to get an indistinguishability obfuscator provably secure under the learning with errors lwe assumption with the aid of complexity leveraging perhaps using techniques similar to those developed in the context of fhe studying the possibility of virtual black box obfuscation as mentioned earlier we know of no actual attacks on our obfuscation scheme as a virtual blackbox obfuscator except for attacks that arise on speciﬁc circuit classes that are known to be unobfuscatable a clear line of research is to better understand the potential for our construction to serve as a virtual black box obfuscator by identifying amenable function classes one thrust is cryptanalysis namely ﬁnding nongeneric attacks when using our scheme perhaps even against natural functionalities a second direction is to attempt to gain some evidence of the viability of virtual black box security of our construction in some heuristic model such as the generic group model of course such a study would not be limited to our construction and we expect that other interesting candidates or variants of our construction would emerge in the near future improving the practical eﬃciency while our current obfuscation construction runs in polynomial time it is likely too ineﬃcient for practical problems an important objective is to improve the eﬃciency for use in practical applications reasoning about obfuscation of learnable programs in practice a ﬁnal important task is to begin to close the gap between the theoretical deﬁnitions for obfuscation and informal expectations that often arise in practice large software development projects can consume millions of dollars and hundreds of person years a company that invested such an eﬀort might reasonably expect that a good obfuscator would provide some real and tangible protection for example the company might hope that releasing an obfuscated version of their software would not be any worse than providing remote access to a server running the software one issue is that despite the substantial investment the obfuscated program may be learnable in that its behavior can be completely determined by a large polynomial number of input queries if this is the case then simply giving out the program in the clear would technically count as valid obfuscation from the cryptographic deﬁnition even though this strategy would be very disappointing in a real sense copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters we would like to be able to bridge this gap between theory and practice and ﬁgure out ways to rigorously deﬁne and argue about meaningful obfuscation notions for learnable programs getting a better understanding of this gap appears critical for using obfuscation in noncryptographic applications we emphasize that an important initial step is to simply understand this problem better other related work some recent work has focused on a collusionbounded form of functional encryption gkp where security is only ensured so long as an attacker does not acquire more than some predetermined number of keys this concept is very similar to the manner in which one time signatures relax the general notion of signatures in these settings collusion bounded fe has been achieved for general circuits and in the case of single key fe this has been done with quite succinct ciphertexts gkp certain collusionbounded functional encryption schemes with special properties suﬃce for several interesting applications such as delegated computation and most notably reusable garbled circuits gkp however in the general use scenario envisioned for fe unbounded collusion resistance is essential as a single user or a collection of users would be holding multiple secret keys this is the focus of our work recent work demonstrated that certain strong simulation based formulations of the goal of functional encryption are impossible to meet our work focuses on realizing the indistinguishability based deﬁnition of fe but recent work has shown that indistinguishability security for general circuits can be used to achieve some form of meaningful simulation based security as well cij in some cases technically constructing obfuscation involves performing computation in a way such that intermediate state remain hidden one natural idea for doing this is to perform computation in the exponent in some cryptographic group this was previously considered as part of unpublished and unsuccessful attempts by several groups e g and however none of these works suggested a plausible structure where such an encoding in the exponent could be done we elaborate on these ideas and why they are insuﬃcient for realizing obfuscation in section two follow up works have proposed obfuscation constructions that use directly circuits rather than bps these constructions require composite order multilinear maps preliminaries in this section we will start by giving our deﬁnition of indistinguishability obfuscation io and the notions of multilinear jigsaw puzzles and barrington branching program that will be needed in the realization of our obfuscation candidate indistinguishability obfuscators definition indistinguishability obfuscator io a uniform ppt machine io is called an indistinguishability obfuscator for a circuit class cλ if the following conditions are satisﬁed for all security parameters λ n for all c cλ for all inputs x we have that pr c x c x c io λ c for any not necessarily uniform ppt distinguisher d there exists a negligible function α such that the following holds for all security parameters copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption λ n for all pairs of circuits cλ we have that if x x for all inputs x then pr d io λ pr d io λ α λ definition indistinguishability obfuscator for n c a uniform ppt machine io is called an indistinguishability obfuscator for n c if for all constants c n the following holds let cλ be the class of circuits of depth at most c log λ and size at most λ then io c is an indistinguishability obfuscator for the class cλ definition indistinguishability obfuscator for p poly a uniform ppt machine io is called an indistinguishability obfuscator for p poly if the following holds let cλ be the class of circuits of size at most λ then io is an indistinguishability obfuscator for the class cλ we also deﬁne a variant of the indistinguishability obfuscator deﬁnition that is suitable for use in uniform security proofs this deﬁnition easily follows from the basic deﬁnition above with respect to nonuniform adversaries definition family indistinguishability obfuscator io a uniform ppt machine io is called an family indistinguishability obfuscator for a circuit class cλ if the following conditions are satisﬁed for all security parameters λ n for all c cλ for all inputs x we have that pr c x c x c io λ c for any not necessarily uniform ppt adversaries samp d there exists a negligible function α such that the following holds if pr for all x x x σ samp α λ then we have pr d σ io λ σ samp pr d σ io λ σ samp α λ observe that against nonuniform adversaries every indistinguishability obfuscator for a circuit class c must also be a family indistinguishability obfuscator for the circuit class c this is because we can nonuniformly ﬁx the coins of samp to produce a concrete σ where and are functionally equivalent and σ yields nonnegligible advantage for the distinguisher d σ multilinear jigsaw puzzles in this section we formalize our multilinear jigsaw puzzle framework the description of the framework does not refer to any speciﬁc implementation of multilinear maps as such we do not require any familiarity with the ggh multilinear encodings for this section however readers familiar with the ggh framework should think of multilinear jigsaw puzzles as oﬀering a strict subset of the functionality of asymmetric ggh multilinear encoding schemes the main diﬀerence is that only the party who generated the system parameters can encode elements this simpliﬁed framework will suﬃce for our work but perhaps not for other applications using this simpliﬁed variant oﬀers several advantages most importantly as opposed to we copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters do not provide nontrivial encodings of and in the public parameters which avoids the weak discrete logarithm attack from for more information on the general ggh framework we refer the reader to note also that our multilinear puzzles framework can alternatively be implemented using a subset of the recent work on approximate multilinear maps over the integers in the multilinear jigsaw puzzle framework there are two entities the jigsaw generator and the jigsaw veriﬁer informally the jigsaw generator takes as input a description of the plaintext elements that need to be encoded and outputs jigsaw puzzle pieces which are encoding of those plaintext elements we call them jigsaw puzzle pieces because they can only be meaningfully combined in very restricted ways i e according to group operations and the multilinear map very roughly the jigsaw veriﬁer takes as input the jigsaw puzzle pieces and a speciﬁc multilinear form for combining these pieces the output of the jigsaw veriﬁer is if it successfully arranges the jigsaw puzzle to produce a suitable encoding of we ﬁrst formalize the manner in which the plaintext elements are speciﬁed intuitively we would like these elements to be given to the jigsaw generator as input but in our setting it is the generator itself that chooses the plaintext space zp from which these plaintext elements are taken so we cannot give it the elements as input instead the generator is given a jigsaw speciﬁer which is roughly an algorithm that takes p as input and produces as output the elements from zp that the generator should encode one helpful fact for reading the deﬁnition below is that in asymmetric k multilinear encoding schemes there are encoding levels that correspond to subsets of the index set k k hence the jigsaw speciﬁer would output not only the plaintext elements but also the levels relative to which they need to be encoded definition jigsaw speciﬁer a jigsaw speciﬁer is a tuple k a where k z are parameters and a is a probabilistic circuit with the following behavior on input a prime number p a outputs the prime p and an ordered set of pairs s a where each ai zp and each si k for a polynomial α we say that a family of jigsaw speciﬁers kλ λ aλ λ z is an α bounded jigsaw speciﬁer family if kλ λ aλ α λ we next formalize the notion of a multilinear form this is a purely syntactic object corresponding to an arithmetic circuit that we want to compute so below we deﬁne forms with gates for addition negation and subtraction we also include below special ignore gates that allow us to have more potential inputs than the number of real inputs to the circuit and then we can choose only a subset of them for the actual computation definition multilinear form a multilinear form is a tuple f k π f where k z are parameters and π is a circuit with input wires made out of binary addition gates binary multiplication gates unary negation gates and unary ignore gates f is an assignment of an index set i k to every wire of π a multilinear form must satisfy the following constraints a for every gate or gate all the inputs and outputs of that gate are assigned the same set i b for every gate its two inputs are assigned disjoint sets k and its outputs are assigned the union set c the out degree of all gates is zero and d the output wire is assigned the set k a form f k π f is a bounded for some a z if the size of the circuit π is at most a copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption we note that what makes this form multilinear is the requirement that the sets corresponding to the input of a multiplication gate be disjoint this means that any index i k can be added only once on any path from input to output which implies that the output must be a multilinear in the inputs multilinear forms are meant to be evaluated on the output from the jigsaw speciﬁer as deﬁned next definition multilinear evaluation let x p s a be the output of the jigsaw speciﬁer k a with each ai zp and each si k we say that a multilinear form f k π f is compatible with x if k k and the input wires of π are assigned the sets s if f is compatible with x then the evaluation of f on x denoted f x is the output of the circuit π on the input a we say that the multilinear evaluation f x succeeds if the ﬁnal output is with these deﬁnitions we can now formally deﬁne multilinear jigsaw puzzles definition multilinear jigsaw puzzle a multilinear jigsaw puzzle scheme consists of two ppt algorithms mj p jgen jver as follows jigsaw generator the generator is a ppt algorithm that takes as input a jigsaw speciﬁer k a and the security parameter λ and it outputs a number p a private output x the output of a jigsaw speciﬁer and a public output puzzle p x puzzle jgen k a for ease of notation if we want to refer only to the public output with respect to a jigsaw speciﬁer k a we sometimes abuse notation and write puzzlea to denote just the public output in the experiment above jigsaw veriﬁer the veriﬁer jver is a deterministic polynomial time algorithm that takes as input the public output of a jigsaw generator puzzle and a multilinear form f k π f it outputs either accept or reject for a particular generator output p x puzzle and a form f compatible with x we say that the veriﬁer jver is correct relative to p puzzle f x if either f x and jver puzzle f or f x and jver puzzle f we require perfect correctness for the scheme namely the veriﬁer must be correct on all forms for any λ z and jigsaw speciﬁer k a any output p x puzzle in the support of jgen k a and any form f k π f compatible with x the veriﬁer jver must be correct relative to p puzzle f x security in the multilinear jigsaw puzzle framework the above formalization and guarantees speak to the correctness of the multilinear jigsaw puzzle framework for capturing security we ﬁrst consider what intuitively we want the multilinear jigsaw puzzle framework to capture we want it to be the case that the only way to distinguish between two diﬀerent jigsaw puzzles puzzlea and puzzlea is for there to be a particular multilinear form f that succeeds with noticeably diﬀerent probabilities when evaluated on a vs a thus we will consider distributions over puzzles puzzlea and puzzlea where we do not believe that any multilinear form distinguishes between these puzzles and our assumptions will state that these puzzles are computationally indistinguishable from each other thus more formally hardness assumptions in the multilinear jigsaw puzzle framework will require that for two diﬀerent polynomial size families of jigsaw speciﬁers kλ λ aλ λ z and kλ λ a λ λ z the public output of the jigsaw generator on kλ λ aλ will be computationally indistinguishable from the public output of the jigsaw generator on kλ λ a λ in appendix a we describe how to implement the multilinear jigsaw puzzle framework using a strict subset of copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters branching programs for our n c indistinguishability obfuscation scheme we use oblivious matrix branching programs as the underlying computational model this is similar to the notion of branching programs used in barrington theorem with the permutations encoded by permutation matrices namely in this model a branching program consists of a sequence of steps where in each step we examine one input bit and depending on its value we choose one of two permutations we then multiply all these permutations and the output of the computation is one if the resulting permutation is the identity slightly more generally we can specify any pair of permutations to represent the outputs and barrington theorem requires only permutations in and hence in the deﬁnition below we restrict ourselves to only this case also below let w denote the set w definition oblivious matrix branching program let be two distinct permutation matrices an oblivious matrix branching program of length n for bit inputs is a sequence n bp inp i ai ai i where the ai b are permutation matrices in and inp i is the input bit position examined in step i the function computed by this oblivious matrix branching program is n if i ai xinp i n def fbp x if i ai xinp i undef otherwise in the rest of the paper a branching program will be assumed to refer to an oblivious matrix branching program theorem see there exist two distinct cycle permutation matrices where can be assumed to the identity matrix such that for any depth d fan in boolean circuit c there exists an oblivious matrix branching program of length at most that computes the same function as the circuit c intuition indistinguishability obfuscation candidate for to help explain the role of the diﬀerent components in our cryptosystem we describe below a trajectory that begins with kilian protocol for oblivious circuit evaluation and modify it until we end up with our obfuscation candidate for circuits starting point barrington and kilian barrington theorem has played a major role in cryptography bogg 88 in a nutshell barrington showed how to transform an arbitrary fan in depth d boolean circuit c into a width permutation branching program bp of length n this program is deﬁned by a sequence bp inp i ai ai ni where the function inp n describes what input bit is examined in the ith step and the ai b are permutations in which we can view as permutation matrices on any particular n bit input x the evaluation of bp consists of computing the product matrix p i ai xinp i outputting one if p and zero otherwise let us recall kilian protocol in which the two players alice and bob evaluate an circuit on their joint input let c x y denote that circuit with alice input x and bob input y and x y invoking barrington theorem we copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption get a length n branching program bp that computes the same function as c alice begins the protocol by choosing n random invertible matrices ri ni over zp computing their inverses and then setting a i b ri ai b ri for all i n b call the new program with a i b along with rn the randomized branching program rbp it is clear that rbp evaluates the same function as the original bp denoting the joint bit input by χ x y alice sends to bob the matrices corresponding to her input x a i χinp i i n inp i x along with rn next they use an oblivious transfer protocol by which bob learns the matrices corresponding to his input y a i χinp i i n inp i x now bob can put the matrices in the proper order to compute the product p i n a i χinp i rn and hence c x y alice learns nothing about bob input and kilian shows that bob does not learn anything about alice input since the randomization hides alice input perfectly scheme number one kilian protocol as is starting our journey from kilian to io we think of alice as the obfuscator and bob as the evaluator the randomized branching program rbp for a universal circuit c x y in its existence is guaranteed by theorem is generated by alice the x input corresponds to the speciﬁc circuit being obfuscated and y the input on which it is evaluated the obfuscation consists of the matrices corresponding to alice input x a i χinp i i n inp i x along with rn and all the matrixes for the inputs corresponding to y namely a i b i n b inp i x bob during evaluation chooses the matrices corresponding to his input y putting together with the matrices corresponding to input x and evaluates the program outputting c x y unsurprisingly this scheme is broken informally speaking we next describe three diﬀerent kinds of attacks against this basic scheme partial evaluation attacks a partial evaluation attack occurs when the adversary computes partial products a i χinp i a i χinp i a j χinp j and a i χ inp i a i χ inp i a j χ inp j corresponding to two inputs χ x y and χ x y and compares these against each other note that the outer randomization matrices ri and rj will be the same this would allow the adversary to learn whether two partial computations in the branching program yield the same value potentially revealing information about the input x mixed input attacks a mixed input attack occurs when the adversary performs the matrix product involved in a correct computation but doesn t respect the inp function that is for diﬀerent steps in the bp that consider the same input bit yi it uses some matrices that correspond to yi and others that correspond to yi again this could yield information about x beyond just the ﬁnal output c x y attacks that do not respect algebraic structure or are not multilinear these attacks must either fail to respect the algebraic structure of the matrices over zp or compute nonmultilinear algebraic functions over these matrices informally speaking we prove in a particular idealized generic model that all multilinear attacks over matrices must fall into one of the two attack categories above kilian uses random permutation matrices for the r but that aspect of the protocol is not i relevant to our discussion for example ishai pointed out to us that computing matrix inverses a function of algebraic degree p can lead to some nontrivial attacks on this scheme copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters dealing with other attacks using multilinear jigsaw puzzles we begin by addressing the possibility of nonmultilinear attacks i e attacks that do things other than evaluating multilinear forms on the given elements this is precisely where multilinear jigsaw puzzles come in as they give us a way to encode the elements in the exponent making it hard to meaningfully manipulate them other than via the prescribed operations in our case the jigsaw generator will be the obfuscator and will generate a puzzle system with multilinearity parameter n where n is the length of the branching program to be obfuscated recall that the jigsaw generator needs access to a jigsaw speciﬁer which will correspond to the plaintext matrices as above in particular the jigsaw speciﬁer outputs a pair for each entry in each of the matrices a i b each entry of the ith matrices a i b is given relative to the singleton index set i n the jigsaw veriﬁer will be used when evaluating the program in order to decide if the resulting matrix is the identity or not we note that obfuscation by encoding kilian construction in the exponent in diﬀerent algebraic settings was considered in the past as part of unpublished and unsuccessful attempts by several groups e g and however none of these works suggested a plausible structure where such an encoding in the exponent could be done moreover it is clear that encoding in the exponent is not enough by itself to get a secure scheme in particular it does not address partial evaluation or mixed input attacks which we consider below dealing with partial evaluation attacks bookends and higher dimensional matrices note that encoding the matrices in the multilinear jigsaw puzzle framework does nothing to prevent partial evaluation and mixed input attacks as those attacks arise even when the adversary respects the matrix product structure to address the partial evaluation attack we would like to add special bookend components at the start and end of the computation without which no partial computation can be compared to any other to do this ﬁrst we embed the matrices ai b inside higher dimension matrices of the form observe that these random scalars indeed thwart mixed evaluation attacks for every input bit yj if we choose all the αi and all the α i for inp i j then the product matches and we could get δ similarly if we choose all the αi and α i however any attempt to mix and match these matrices will result in two diﬀerent random products for the primary and dummy programs which cancel out only if the two evaluation result in the same random number note that this happens only with a negligible probability additional safeguards in the construction above we choose to set the number m of random dimensions that are added to the matrices to more than the multilinearity parameter of the system namely set m n we note that we do not have any concrete reason why even setting m is insecure but we increase m as an additional safeguard against unanticipated attacks by doing so we increase the level of randomness present in both the di b matrices and the individual ri matrices to be well beyond the maximum multilinearity supported by the multilinear jigsaw puzzle in particular this means that written as formal polynomials each entry of ri is a polynomial with a degree exceeding the maximum multilinearity of the system intuitively this gives us more conﬁdence in our assumption because it seems quite implausible that an adversary that is essentially limited to computations that are close to multilinear forms would be able to detangle such high degree computations involving so many degrees of freedom in terms of the randomness used however again we are not aware of any attacks on our scheme even without this additional safeguard indistinguishability obfuscation candidate for in this section we describe our indistinguishability obfuscation candidate for we begin by presenting a notion of randomized branching programs somewhat similar to kilian and then build upon this construction using multilinear jigsaw puzzles to get our indistinguishability obfuscation candidate copyright by siam unauthorized reproduction of this article is prohibited indistiguishability obfuscation and func encryption downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php notation consider a length n branching program over input variables bp inp i ai ai i n inp i ai b where inp i is the position of the input bit that is examined in step i we extend this notation naturally to a set of steps s n namely inp s inp i i s conversely for bit position j we denote by ij the steps in the branching program bp that examine the jth input bit ij i n inp i j and let ij ij j j for j randomized branching programs like kilian we express the steps of the branching program in terms of matrices and randomize the ith matrix by enveloping it by random matrices ri and ri however in our setting we also need some additional garbling techniques let zp be the ring over which we randomize the branching program and let m we generate a randomized branching program as follows αi α i α i i n in sample random and independent scalars αi zp subject to the constraint that i ij αi i ij α i and i ij αi i ij αi for all j for every i n compute two block diagonal matrices di di where the diagonal entries are chosen independently at random and the bottom right are the scaled aj b and two more di where the diagonal entries are matrices di random and the bottom right are the scaled identity di b b di b αi b i αi b ai b choose vectors and t and and t of dimension as follows the entries m in the and vectors are set to zero and entries m are chosen independently at random in the t and t vectors the entries m are chosen independent at random and m are set to zero choose two pairs of random vectors and t and and t such that t t the last ﬁve entries in are set to and the last ﬁve entries in t are set to t the last ﬁve entries in are set to and the last ﬁve entries in t are set to t m m m m t t t m m m m t t t sample n random full rank matrices over zp rn and rn and compute their inverses the randomized branching program over zp is the following rn dp bp sr t rn t r t r n t r i n b d i b d i b ri di b r i i n b i di b ri copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters we remark that this randomized program consists in essence of two parallel programs one embeds the original branching program bp with all the ai b and the other embeds a dummy program of the same length consisting only of identity matrices so it computes the constant function in our construction we use the dummy program for the purpose of equality test the original program outputs on a given input only when it agrees with the dummy program on that input garbled branching programs and hardness assumptions below we describe a transformation for garbling bps with ﬁxed inputs and formally deﬁne our hardness assumption that says that this garbling transformation has good obfuscation properties we use multilinear jigsaw puzzles as speciﬁed in section let bp be a lengthn branching program computing some function f we apply our multilinear jigsaw puzzle framework to the jigsaw speciﬁer that on input p randomizes the branching program over zp and outputs a tuple corresponding to each element in rn d p bp as above speciﬁcally it provides each element of the step i matrices at the level of the singleton index set i each element of the vectors at the level of the singleton index set and each element of the vectors t t at the level of the singleton index set n we denote the randomized and encoded program which is the public output of the jigsaw generator by rn dp bp encode t encode n t prms encode t encode n t i n b encode i d i b d i b encode i d i b i n b d i b the corresponding private output of the jigsaw generator i e plaintext is p rn d p bp above encodes a denotes an encoding of a at level s generated by jigsaw generator note that this information is provided to it by the jigsaw speciﬁer for every input χ to the original branching program we can choose the corresponding matrices from both the primary and the dummy programs and test whether they yield the same result using only the allowed multilinearity in more detail for every input χ we consider the multilinear form fχ which is deﬁned as d i χinp i t d i χinp i t mod p fχ rn d p bp i i we observe that a if bp χ then fχ rn d p bp with probability and b if bp χ then fχ rn d p bp except with negligible probability we know that bp χ and therefore the two terms in the polynomial fχ rn d p bp above do not cancel out but the polynomial might itself evaluate to however this happens only with a negligible probability since all the variables in the polynomial are assigned random values given rn d p bp and χ we can use the jigsaw veriﬁer to check if fχ rn d p bp thereby learning with high probability the output of bp χ hardness assumptions roughly we assume that if for two diﬀerent ways of ﬁxing some inputs to the branching program result in the same function on the remaining nonﬁxed inputs then it is infeasible to decide which of the two sets of ﬁxed inputs is used in a given garbled program input ﬁxing given rn d p bp as above and a partial assignment for the input bits σ j for j the parameter ﬁxing procedure just removes all the copyright by siam unauthorized reproduction of this article is prohibited indistiguishability obfuscation and func encryption downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php matrices d i b d i b that are not consistent with that partial assignment σ i e where i ij and b σ inp i thus we have garble rn d p bp j σ prms t t d i b i ij b σ inp i d i b i ij b σ inp i d i ij b d i b i ij b i b we think of a garbled program relative to the partial assignment j σ as a representation of a function speciﬁcally the function implemented by the branching program with the input restricted according to j σ if the underlying program is computing a function f and we have a partial assignment j σ then we denote the resulting function by f σ definition functionally equivalent assignments fix a function f and consider the two partial assignments over the same input variables j and j we say that these assignments are functionally equivalent relative to f if f f assumption equivalent program indistinguishability for any length n branching program bp computing a function f and any two partial assignments on the same variables j and j if these assignments are functionally equivalent relative to f then the following two ensembles of garbled program distributions are computationally indistinguishable garble rn d p bp j λ n c garble rn d p bp j λ n where the garbling is done using a multilinear jigsaw puzzles scheme and the length of p is an appropriate polynomial in λ n our candidate theorem under the equivalent program indistinguishability assumption assumption there exists an io for circuits proof fix a constant γ and for every value of the security parameter λ let cλ be the class of circuits of depth γ log λ and size at most λ let uλ be a poly sized logdepth universal circuit for this circuit class where uλ c m c m for all c cλ and m n furthermore all circuits c cλ can be encoded as an λ bit string as input to u let u bpλ c m be the universal branching program which was obtained by applying barrington theorem to a universal circuit uλ c m its existence is guaranteed by theorem denote by ic the steps in the program u bpλ that examine the input bits from the c input and for each particular circuit c denote by ic σc the partial assignment that ﬁxes the bits of that circuit in the input of u bpλ the obfuscator on input circuit c simply applies our garbling method from section to u bpλ with parameter ﬁxing ic σc namely we have io λ c garble rn d p u bpλ ic σc functionality and polynomial slowdown are obvious note that for any two circuits c2 that are of the same size and compute the same function we have that u bp u bp c2 next assumption directly implies that for suﬃciently copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters large λ the distributions io λ garble rn d p u bpλ ic and io λ c2 garble rn d p u bpλ ic are computationally indistinguishable amplifying to poly sized circuit indistinguishability obfuscation in this section we show how to achieve poly sized circuit indistinguishability obfuscation from an indistinguishability obfuscator for circuits in in addition to io our construction makes use of two primitives perfectly sound noninteractive witness indistinguishable proofs and leveled homomorphic encryption we also make use of the fact that any poly time circuit computation can be veriﬁed by a low depth circuit in background material for these primitives can be found in appendix b we let he keygen he enc he dec he eval be a leveled homomorphic encryption scheme furthermore we assume the decryption algorithm he dec can be realized by a family of circuits in and that the system has perfect correctness our construction consider a family of circuit classes cλ for λ n where the input size n and the maximum circuit size are polynomial functions of λ let uλ be a poly sized universal circuit family for these circuit classes where uλ c m c m for all c cλ and m n furthermore all circuits c cλ can be encoded as an λ a polynomial functions of λ bit string as input to u we note that this may require some padding since circuits in the family are not required to be all of the same size we show how to build an io for such a circuit class given an indistinguishability obfuscator for circuits in our construction is described by an obfuscation algorithm and an evaluation algorithm obfuscate c cλ the obfuscate algorithm takes the security parameter λ and a circuit c and computes he keygen and he keygen are generated if we are using a leveled he scheme the number of levels should be set to be the depth of uλ encrypt he enc c and he enc c here we assume that c is encoded in a canonical form as an bit string for use by the universal circuit uλ generate an n c obfuscation for program skhe as p skhe see figure the obfuscation components are outputed as σ p evaluate σ p m the evaluate algorithm takes in the obfuscation output σ and program input m and computes the following compute he eval uλ m and he eval uλ m technically we could make do with just a single obfuscation algorithm that outputs a circuit description as is the convention given in section however for the exposition of this construction we have the obfuscation algorithm output cryptographic material that is used by the evaluation algorithm the circuit u m is the universal circuit with m hardwired in as an input this in hardwired λ circuit takes in an bit circuit description c as its input and evaluates to u c m copyright by siam unauthorized reproduction of this article is prohibited indistiguishability obfuscation and func encryption downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php given input m φ skhe proceeds as follows check if φ is a valid low depth proof for the np statement he eval uλ m he eval uλ m if the check fails output otherwise output he dec fig given input m φ skhe proceeds as follows check if φ is a valid low depth proof for the np statement he eval uλ m he eval uλ m if the check fails output otherwise output he dec sk2he fig compute a low depth proof φ that and were computed correctly background material for low depth proofs can be found in appendix b we note that this statement for which proof is being given is in p run p m φ and output the result correctness to verify correctness we ﬁrst check that the size of the circuits evaluating skhe are in step of the program is in since it involves veriﬁcation of a low depth proof as described in appendix b the second step of the program is also in since we use an he scheme with decryption in since both steps are in the entire circuit is in the correctness of he implies that the evaluated ciphertext will be an encryption of the value u c m c m next the correctness of the implies that p implements the same function as p these two facts together imply the correctness of the obfuscation scheme proof of security we prove that for all cλ such that they are functionally equivalent there can be no poly time indistinguishability attacker a that wins the security game from deﬁnition with nonnegligible advantage we organize our proof into a sequence of hybrids in the ﬁrst hybrid the challenger obfuscates we then gradually change the obfuscation in multiple hybrid steps into an obfuscation of we show that each successive hybrid experiment is indistinguishable from the last thus showing our obfuscator to have indistinguishability security the proof hybrid steps themselves primarily weave back and forth between changing the underlying ciphertexts and the programs that are used in a two key proof type manner the changes program that we will use is described in figure sequence of hybrids this hybrid corresponds to a honest execution of the indistinguishability obfuscation game where is obfuscated this is the same as hybrid except we set he enc he enc now and encrypt diﬀerent circuits copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters we still generate he enc and he enc as in now p is created as p skhe we now generate he enc and he enc the obfuscated program p is still created as p skhe we still generate he enc and he enc as in now p is created as p skhe note that this hybrid corresponds to an honest execution of the indistinguishability obfuscation game where is obfuscated proofs of hybrid arguments claim if our he scheme is ind cpa secure then no poly time attacker can distinguish with nonnegligible probability between and proof we show that if there is a poly time attacker a that has a nonnegligible diﬀerence in advantage between and then there is a poly time algorithm b that breaks the ind cpa security of our pke scheme b on input begins by running a b generates by itself and gets from the ind cpa challenger next it gives the ind cpa challenger and gets back a ciphertext g it sets g b then sets he enc and p is created as p skhe note that only the ﬁrst secret key is needed to create p if the ind cpa challenger used the ﬁrst message then we are exactly in hybrid if it chose the second message then we are in therefore if an attacker can distinguish between the two hybrids with nonnegligible advantage it will break the ind cpa property of the he scheme claim if the family io deﬁnition holds for our indistinguishability obfuscator then no poly time attacker can distinguish between and proof given an attacker a that can distinguish between and we will describe constructions of a sampler samp and a distinguisher d our samp will be such that with overwhelming probability it yields circuits and which are functionally equivalent where σ samp however the distinguisher d will have a nonnegligible chance of distinguishing between obfuscations of and even when it is given σ samp ﬁrst generates the two he private keys itself keeping both secret keys it then creates he enc and he enc next it outputs the circuits skhe and skhe as and and sets σ our distinguisher d works as follows given an obfuscation p of either or and σ it executes a with input p pk2he d outputs the output of a given that the output of is equivalent to the output of on all inputs we have that both programs skhe and skhe g2 will have the same output on all inputs both inputs will halt and output if the check does not pass if the check does pass this means that and are encryptions of the same message this is due to the perfect correctness of he scheme with the fact that u m u m for all m since both of the programs give the same output on all inputs we are in a valid instance of the assumption if the challenger used the ﬁrst circuit skhe g2 in generation of p then we are exactly in hybrid if it chose the second circuit p2 skhe g2 then we are in therefore if an attacker can distinguish between the two hybrids with nonnegligible advantage it will break the indistinguishability security of the obfuscator copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption claim if our he scheme is ind cpa secure then no poly time attacker can distinguish with nonnegligible probability between and proof the proof is analogous to the proof of claim claim if the family io deﬁnition holds for our indistinguishability obfuscator then no poly time attacker can distinguish between and proof the proof is analogous to the proof of claim theorem under the assumptions listed above our io for poly sized circuits is secure in the indistinguishability game deﬁned in deﬁnition proof our claims show a succession of ﬁve hybrids where no poly time attacker can distinguish one from the next with nonnegligible advantage the ﬁrst hybrid corresponds to obfuscating circuit and the last hybrid corresponds to obfuscating the circuit in the indistinguishability security game security in the indistinguishability game follows functional encryption our syntax for functional encryption roughly follows in the line of boneh sahai waters except we specialize our notation for the case where the private key is a function f and the ciphertext input is a message m this is without loss of generality when f can be any poly sized circuit and thus includes a universal circuit for security we use the indistinguishability notion which was the ﬁrst one considered for functional encryption as well as predicate encryption indistinguishability is a basic notion of security which may be enough for some applications and furthermore de caro et al cij show how one can transform a system with indistinguishability security into one with meaningful strong simulation security in particular the resulting scheme is simulation secure for a bounded number of encryption and nonadaptive key queries but an unbounded number of adaptive key queries definition functional encryption a functional encryption scheme for a class of functions f f λ over message space m mλ consists of four algorithms f e setup keygen encrypt decrypt setup a polynomial time randomized algorithm that takes the unitary representation of the security parameter λ and outputs public parameters pp and a master secret key msk keygen msk f a polynomial time randomized algorithm that takes as input the master secret key msk and a description of function f f and outputs a corresponding secret key skf encrypt pp m a polynomial time randomized algorithm that takes the public parameters pp and a message m m and outputs a ciphertext ct decrypt skf ct a polynomial time algorithm that takes a secret key skf and ciphertext encrypting message m m and outputs f m a functional encryption scheme for f is correct if there exists a negligible function α such that for all f fλ and all messages m mλ pr pp msk setup decrypt keygen msk f encrypt pp m f m α λ indistinguishability security for functional encryption we describe indistinguishability security as a multiphased game between an attacker a and a challenger setup the challengers runs pp msk setup and gives pp to a query a adaptively submits queries f in fλ and is given skf keygen msk f this step can be repeated any polynomial number of times by the attacker copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters challenge a submits two messages mλ such that f f for all f queried in the key query phase the challenger samples a random bit b and gives a the value encrypt pp mb query a continues to issue key queries as before subject to the same restriction that any f queried must satisfy f f guess a eventually outputs a bit b in the advantage of an algorithm a in this game is adva pr b b definition a functional encryption scheme is indistinguishability secure if for all poly time a the function adva λ is negligible we also consider a weaker selective variant of security for indistinguishability the selective game is equivalent to the adaptive one with the exception that the attacker must declare the challenge messages in an init phase that occurs before it sees the public parameters definition a functional encryption scheme is selectively indistinguishability secure if for all poly time a the advantage of a is negligible in the selective indistinguishability security game remarks and transformations we make a few remarks about the above deﬁnitions first we observe that any system that is selectively secure can be argued to be adaptively secure e g as noted in if we are willing to utilize complexity leveraging and lose a factor of m in the security reduction our construction in the next section will use the selective notion second our deﬁnition considers a single challenge ciphertext however using the usual hybrid methods it can be shown that any system indistinguishability secure under this deﬁnition is secure under a multiple message deﬁnition such as that used in cij statistical simulation sound non interactive zero knowledge proofs in this section we deﬁne statistical simulation sound noninteractive zero knowledge proofs which will be needed in our construction in the next subsection let r be an eﬃciently computable binary relation for pairs x w r we call x the statement and w the witness let l be the language consisting of statements in r a noninteractive proof system for a relation r consists of a common reference string generation algorithm k a prover p and a veriﬁer v we require that they all be probabilistic polynomial time algorithms i e we are looking at eﬃcient prover proofs the common reference string generation algorithm produces a common reference string σ of length ω λ the prover takes as input σ x w and produces a proof π the veriﬁer takes as input σ x π and outputs if the proof is acceptable and otherwise we call k p v a noninteractive proof system for r if it has the completeness and statistical soundness properties described below perfect completeness a proof system is complete if an honest prover with a valid witness can convince an honest veriﬁer formally we require that for all x w r for all σ k and π p σ x w we have that v σ x π statistical soundness a proof system is sound if it is infeasible to convince an honest veriﬁer when the statement is false for all even unbounded adversaries a we have pr σ k x π a σ v σ x π x l negl λ computational zero knowledge a proof system is computational zero knowledge if the proofs do not reveal any information about the witnesses to a copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption bounded adversary we say a noninteractive proof k p v is computational zeroknowledge if there exists a polynomial time simulator sim which returns a simulated common reference string together a simulated proof for all nonuniform polynomial time adversaries a we have for all x w r pr σ k π p σ x w a x σ π pr σ π sim x a x σ π typically in the zero knowledge literature for example as in the simulated crs is required to be independent of the statement for which the simulated proof is provided this requirement is not essential for us and we only deﬁne the weaker variant as above statistical simulation soundness sss a proof system is said to be statistically simulation sound if it is infeasible to convince an honest veriﬁer of a false statement even when the adversary itself is provided with a simulated proof for all statements x we have l negl λ pr σ π sim x x π x x v σ x π x we note that this new notion of soundness requires that except with respect to one particular statement to be simulated all other valid proofs that exist must be only for true statements this is diﬀerence from sahai simulation sound noninteractive zero knowledge proofs sahai proof system required that a computationally bounded attacker even with access to simulated proofs cannot come up with a proof for false statements however such proofs for false statements might still exist in our case such proofs do not exist we stress that our new proof system is not stronger than the sahai proof system in particular in our case the simulator can generate simulated proof for only one statement ﬁxed in advance and even a bounded attacker might be able to generate fake proofs for this statement realizing statistical simulation soundness constructions of noninteractive zeroknowledge nizk proof system are well known now we will describe how a nizk proof system can be used to realize a sss nizk proof system formally using a nizk proof system k p v and a noninteractive commitment scheme com we will construct a nizk proof k p v that is also statistically simulation sound let be an upper bound on the length of the statements proven and let represent a special statement that is never proven k generate σ k and c com r where r are the random coins and output the common random string as σ σ c p σ σ c x w generate the proof π as p σ x w r where x is the following statement w r such that x w r c com x r v σ x π the veriﬁer v outputs v σ x π where x is the statement as in correctness the correctness of the scheme k p v follows directly from the correctness of k p v zero knowledge we will start by describing our simulator sim assuming the simulator sim for k p v copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters sim x generates the common random string σ σ c where σ k and c com x r sim generates the proof for x from equation using the randomness r more speciﬁcally it just outputs p σ x w r the zero knowledge property of our simulator follows from the following simple hybrid argument this hybrid corresponds to the honest generation of the proof we start generating c as a commitment to x rather than indistinguishability between hybrids and follows from the computational hiding property of the commitment scheme we start using the randomness used in generation of the commitment c for generating the proof rather than the real witness in other words we use the simulation strategy indistinguishability between hybrids and follows from the zeroknowledge property of k p v statistical simulation soundness sss follows directly from the statistical soundness of the k p v proof system and the fact that the only false statement for which an acceptable proof exists is x because the commitment c is perfectly binding functional encryption from indistinguishability obfuscation we now give our construction and proof of security of a functional encryption system from an indistinguishability obfuscator io in addition to an io our construction uses public key encryption and statistically simulation sound noninteractive zero knowledge proofs sss nizks we let setupp ke encryptp ke evalp ke decryptp ke be the algorithms comprising our perfectly correct encryption scheme our sss nizk system will consist of algorithms setupn izk proven izk verifyn izk and has a simulator sim we build a functional encryption system for messages of length n n λ for messages of length n and security parameter λ the ciphertexts of our pke scheme will be of length λ n the construction is as follows setupf e the setupf e algorithm takes the security parameter λ and computes the following generate ke ke setupp ke and ke ke setupp ke set crs setupn izk it sets the public parameters and master secret key as pp ke ke crs and msk ke keygenf e msk f output an obfuscation f ke crs for the program f skp ke crs deﬁned in figure we output the secret key skf as the obfuscated program encryptf e pp m n output c π where encryptp ke ke m and encryptp ke ke m and π is a nizk proof of decryptf e skf c π the decryption algorithm runs the obfuscated program skf on input π and outputs the answer correctness correctness follows immediately from the correctness of the io pke system sss nizk and the description of the program template we describe the two program classes in the ﬁgures below copyright by siam unauthorized reproduction of this article is prohibited indistiguishability obfuscation and func encryption downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php given input π f sk p ke crs proceeds as follows check that π is valid nizk proof using the verifynizk algorithm and crs for the np statement m encryptp ke ke m encryptp ke ke m if any checks fail output otherwise output f decryptp ke ke fig proof of security we now prove that no poly time attacker can break our system if our underlying assumptions hold we prove selective security for the functional encryption indistinguishability game let q q λ be the maximum number of private key queries a makes for simplicity we assume that an algorithm will always make exactly q queries we denote fi for i q to be the ith function queried in the indistinguishability game by the rules of the game fi is constrained to be equal to fi where and are the messages declared by a in the init phase we organize our proof into a sequence of hybrids in the ﬁrst hybrid the challenger encrypts we then gradually change the encryption in multiple hybrid steps via a two key type proof into an encryption of we show that each successive hybrid experiment is indistinguishable from the last thus showing our obfuscator to have indistinguishability security sequence of hybrids this hybrid corresponds to the honest execution of the selective indistinguishability game given in deﬁnitions and where the challenger encrypts in the challenge ciphertext this hybrid is similar to with the exception of how the crs which is part of the public key and the proof π which is a part of the challenge ciphertext are generated in particular in this hybrid e e which are parts of the the challenge ciphertext are encrypted ﬁrst next simulated crs and π are generated based on e e as follows crs π sim m e encryptp ke ke m e encryptp ke ke m note that in the selective security game the challenge ciphertext can be given out simultaneously with the public parameters this hybrid is identical to the last hybrid with the exception that the challenge ciphertext is generated as e encryptp ke ke and e encryptp ke ke the nizk is still simulated i for i q in this set of hybrids we change the form of the secret keys used to decrypt the ciphertext in i the ﬁrst i private keys requested will result in private keys generated as obfuscations of the program fi skp ke crs deﬁned in figure the remaining i to q private keys are generated using the program fi skp ke crs as in we observe that is equivalent to copyright by siam unauthorized reproduction of this article is prohibited garg gentry halevi raykova sahai and waters downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php given input π f sk p ke crs proceeds as follows check that π is valid nizk proof using the verifynizk algorithm and crs for the np statement m encryptp ke ke m encryptp ke ke m if any checks fail output otherwise output f decryptp ke ke fig this hybrid is identical to the hybrid q with the exception that the challenge ciphertext is generated as e encryptp ke ke and e encryptp ke ke the nizk is still simulated and the keys are all created from f skp ke crs i for i q the challenge ciphertext and crs is formed the same as in in this set of hybrids we change the form of the secret keys used to decrypt the ciphertext in i the ﬁrst i private keys requested will result in private keys generated as obfuscations of the program fi skp ke crs the remaining i to q private keys are generated using the program fi skp ke crs as in we observe that is equivalent to this hybrid is the same as q with the exception that the crs is generated from an honest run of the setupn izk algorithm and that the nizk proof component π of the challenge ciphertext is generated from the witness this corresponds to the security game when message is encrypted for the challenge ciphertext proofs of hybrid arguments claim if our sss nizk system is computationally zero knowledge then no poly time attacker can distinguish with nonnegligible probability between and proof we show that if there is a poly time attacker a that can distinguish between and with nonnegligible advantage then there is a poly time algorithm b that breaks the zero knowledge security of our nizk scheme b begins by running a and receiving b generates both public keys itself keeping the ﬁrst secret key and encrypts e encryptp ke ke and e encryptp ke ke it then submits to the challenger the statement x m e encryptp ke ke m e encryptp ke ke m as well as the witness it receives back crs π it uses crs crs to make the public parameters and sets the challenge ciphertext as e e π π the attacker a then makes q private key queries to b all queries for a function f are answered by obfuscation program instances of f skp ke crs for appropriate function with ke hard wired inside it if the zero knowledge challenger used the honest setup algorithm and prover to generate crs π then we are exactly in hybrid if it simulated the proof then we are in hybrid therefore if an attacker can distinguish between the two copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption hybrids with nonnegligible advantage it will break the zero knowledge property of the nizk scheme claim if our pke system is ind cpa secure then no poly time attacker can distinguish with nonnegligible probability between and proof we show that if there is a poly time attacker a that can distinguish between and with nonnegligible advantage then there is a poly time algorithm b that breaks the ind cpa security of our pke scheme b begins by running a and receiving b ﬁrst generates the ﬁrst public key itself keeping the secret key and encrypts e encryptp ke ke it then receives a public key from the challenger and designates it as ke next it submits to the challenger and receives back e it sets e e finally it uses the simulation algorithm to get crs π sim m e encryptp ke ke m e encryptp ke ke m r2 the parameters and challenge ciphertext are now given out as pp ke ke crs and e e π the attacker a then makes q private key queries to b all queries for a function f are answered by obfuscation program instances of f skp ke crs for appropriate function with skp ke hard wired inside it if the ind cpa challenger gave an encryption of then we are exactly in hybrid if it gave e as an encryption of then we are in hybrid therefore if an attacker can distinguish between the two hybrids with nonnegligible advantage it will break the ind cpa property of the pke scheme claim if the family io deﬁnition holds for our indistinguishability obfuscator and the sss nizk has the statistical simulation soundness property then no poly time attacker can distinguish between i and i for i q proof we show that if there is a poly time attacker a that can distinguish between i and i then we will describe constructions of a sampler samp and a distinguisher d our samp will be such that with overwhelming probability it yields circuits and which are functionally equivalent where σ samp however the distinguisher d will have a nonnegligible chance of distinguishing between obfuscations of and even when it is given σ samp begins by running a and receiving it generates both public keys keeping both secret keys and encrypts e encryptp ke ke and e encryptp ke ke r2 it uses the simulation algorithm to get crs π sim m r2 e encryptp ke ke m e encryptp ke ke m r2 the parameters and challenge ciphertext are now given to a as pp ke ke crs and e e π the attacker a then makes q private key queries for j i the jth private key is created as an obfuscation of the program fj skp ke crs given the i private key query fi samp outputs fi skp ke crs and c1 p4 fi skp ke crs it additionally outputs σ as the transcript of its internal state and the interaction with a copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters our distinguisher d on input the obfuscated circuit c of or c1 and σ proceeds as follows d gives c as the i private key to a for j i the jth private key is created as an obfuscation of the program p3 fj skp ke crs using information from σ we now check that we are using a valid instance of the family io deﬁnition in showing that both programs have the same output on each input we break these down by cases on the input we ﬁrst consider inputs π where are valid encryptions of the same message and π is a proof of this that passes verifyn izk for these inputs both circuits will proceed to the second step where they decrypt the same message m no matter what key they use and compute the same function fi thus the output is the same on all inputs of this class the second sets of inputs we consider are where the veriﬁcation check of in step does not pass in this case both circuits output finally we can consider cases where the veriﬁcation check passes but are not valid encryptions of the same message due to the statistical simulation soundness property of the sss nizk this can only happen if e and e2 e in this case decrypting e gives and decrypting e however the ﬁrst circuit outputs fi which must be equal to fi by the rules of the game which is the output of the second circuit since the two circuits have the same output on all inputs this constitutes a valid instance of the assumption if the family io challenger chose the ﬁrst program then we are exactly in hybrid i if it chose the second then we are in i therefore if an attacker can distinguish between the two hybrids with nonnegligible advantage it will break the family io deﬁnition claim if our pke system is ind cpa secure then no poly time attacker can distinguish with nonnegligible probability between q and the proof of this claim follows in a directly analogous way to that of claim claim if the family io deﬁnition holds for our indistinguishability obfuscator and the sss nizk has the statistical simulation soundness property then no poly time attacker can distinguish between i and i for i q the proof of this claim follows in a directly analogous way to that of claim claim if our sss nizk system is computationally zero knowledge then no poly time attacker can distinguish with nonnegligible probability between q and the proof of this claim follows in a directly analogous way to that of claim theorem under the assumptions listed above our fe system is selectively secure in the indistinguishability game from deﬁnitions and proof our claims show a succession of a polynomial number of hybrid experiments where no poly time attacker can distinguish one from the next with nonnegligible advantage the ﬁrst hybrid corresponds to encrypting a message in the selective functional encryption indistinguishability game and the last hybrid corresponds to encrypting the message security in the indistinguishability game follows appendix a multilinear jigsaw puzzle implementation details one instantiation of multilinear jigsaw puzzles is obtained by taking the ggh multilinear encoding schemes and eliminating those aspects that are no longer needed namely the encodings of in the public parameters we note that we can also use the recent multilinear maps over the integers of coron lepoint and tibouchi as a basis for our multilinear jigsaw puzzles we provide the ggh based instantiation here our implementation of the multilinear jigsaw puzzle framework has a determin copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption istic jigsaw veriﬁer and a bad jigsaw puzzle without correctness can be eﬃciently recognized and resampled so as to provide perfect correctness jigsaw generator on input k λ π the jigsaw generator proceeds as follows instance generator the jigsaw generator ﬁrst chooses a large random prime q which will be the overall modulus used she will use a dimension parameter m which will be an appropriate large enough power of two further work will be done in the ring r z x x m and the ring rq zq x x m r qr the size of these parameters will be discussed at the end of this section the ﬁrst sequence of operations that the jigsaw generator performs are to set up the approximate multilinear maps she begins by choosing several secret polynomials first she chooses a small random polynomial g r such that r g is a large prime p here small refers to each coeﬃcient in g being a small integer we will describe exactly what small should mean when we later discuss parameter selection this polynomial g must also satisfy a technical condition ensuring that g when viewed as residing in q x x m is suﬃciently small we omit the proofs that these conditions can be satisﬁed with noticeable probability by a random choice of small g these are found in then she chooses k random polynomials zk rq these polynomials are chosen uniformly and therefore are very unlikely to be small the jigsaw generator will identify the plaintext space with zp in other words in the analogy to true multilinear maps the underlying groups will be of prime order p furthermore each level of the multilinear map will be associated with a subset of the set k k again to make the analogy complete we will think of having groups gs for every subset s k and there will be a bilinear map from gs gs gs s for every disjoint pair of sets s s k encoding next the jigsaw generator is ready to create the puzzle pieces which are encodings of various elements of zp at diﬀerent levels to create an encoding encodes a of an element a zp at the level s k the puzzle maker does the following first she reduces a modulo g to create a small polynomial a a mod g in r then she computes in rq the following a e g encodes a i s zi where e is chosen so that a e g is discrete gaussian centered at the origin and so that the resulting e is small once again we will discuss exactly what small means here when we discuss settings of parameters we think of e as being a crucial noise term for achieving security we refer the reader to for details on a based more sophisticated sampling procedure the various puzzle pieces generated by the jigsaw generator will be given to the jigsaw veriﬁer zero test generation the ﬁnal step of the jigsaw generator is to create a zero testing parameter that will allow the jigsaw veriﬁer to know when he has solved the puzzle by creating a nontrivial encoding of at the highest level k this will be used to allow the jigsaw veriﬁer to check if two elements at the highest level are equal to each other by subtracting and then using the zero test copyright by siam unauthorized reproduction of this article is prohibited garg gentry halevi raykova sahai and waters downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php the jigsaw generator now creates the zero testing element by performing the following computation in rq h pzt k i zi g here h is a random mid size polynomial chosen from a discrete gaussian in r whose coeﬃcients are of size roughly q intuitively since an encoding of at level k is e g i zi by multiplying it by pzt one is left with h e which will be small by testing if this product is small the puzzle solver will know whether it has successfully created an encoding of at the ﬁnal level the proof of this fact is omitted and can be found in when she is done she sends prms pzt and the various puzzle pieces a k a encodings she has created to the jigsaw veriﬁer the jigsaw generator plays no further role jigsaw veriﬁer the veriﬁer is given the system parameters prms k m q p pzt a valid form π and inputs for π u2 in un if the output wire is not associated with k then the veriﬁer outputs if it is associated with k then the veriﬁer just evaluates π on these inputs using rq operations and then uses the zero test parameter to check if the output is an encoding of zero namely every gate in the form π is replaced by the operations in rq respectively applied to the input elements to this gate explained below denote the element at the output wire by u the properties of the encoding scheme ensure that it is of the form u a i k zi rq where a q and a g if and only if the underlying computation in zp yields a zero the veriﬁer multiplies the output encoding by the zero test parameter to get w u pzt rq and output if w q and otherwise now we give more details on the the algorithm used by the jigsaw veriﬁer addition at same level given two encodings α encodes a and β encodes b at the same level s observe the following computation over rq α β a b e g i s zi we will choose parameters so that any polynomial number of additions will keep the term e from growing too large thus we see that addition yields another valid encoding at the same level multiplication jumping levels given two encodings α encodes a and β encodet b at disjoint level sets s t k observe the following computation over rq α β a b e g i s t zi we will choose parameters so that the term e will not grow too large as long as the level sets s and t are disjoint thus we see that multiplication yields another valid encoding at a diﬀerent level copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption the encodings are designed so that only these operations are meaningful other operations such as addition across diﬀerent levels or division of encodings will produce junk similarly multiplication so that the ﬁnal level contains any element with multiplicity more than one will produce an encoding that essentially cannot be distinguished from a random see for further details finally the jigsaw veriﬁer will yield some concrete encoded value u at the highest level k at which point it will then invoke the zero testing procedure zero testing the procedure iszero prms pzt u just multiplies v u pzt in rq and tests if v is small enough e g if it has canonical embedding of euclidean norm smaller than q if the zero test passes the jigsaw veriﬁer outputs otherwise it outputs setting parameters if we think of the dimensionality m as being an eﬀective security parameter then to achieve the level of security and functionality that we need we can set k mδ and q of size approximately m where δ and are suitably chosen constants δ the deﬁnition of small for the size of g δ and the noise term e above will be that each coeﬃcient should be smaller than m appendix b additional background in this section we ﬁrst give background on two cryptographic primitives used in section noninteractive and perfectly binding commitments leveled homomorphic encryption in addition we describe how any np proofs that are veriﬁable in by a family of polynomial sized ciruits can be transformed into a proof that is veriﬁable by a family of circuits belonging to b noninteractive and perfectly binding commitments we let com denote the commitment function of a noninteractive commitment scheme com takes as input a bit b and randomness r λ and outputs a commitment c com b r commitment schemes must satisfy two properties hiding and binding computational hiding hiding means that no computationally bounded adversary can distinguish as to which bit is locked in the commitment let a be any nonuniform adversary running in time poly λ we say that the commitment scheme is computationally hiding if b negl λ pr b b c com b r b a c perfectly binding intuitively speaking binding requires that no even unbounded adversary can open the commitment in two diﬀerent ways here we deﬁne the strongest variant known as perfectly binding formally we require that there do not exist values such that com com for simplicity of exposition in what follows we will assume that random coins are an implicit input to the commitment function we will also naturally extend the commitment function com to commit to strings instead of just bits for simplicity of exposition in the presentation of our results in this manuscript we use a noninteractive perfectly binding string commitment which can be based on any to one way function it is easy to see that all the constructions in this work can in fact be built using naor round commitment scheme which can be based on any one way function b leveled homomorphic encryption here we give the deﬁnition of a leveled homomorphic encryption scheme following a homomorphic encryption scheme he is a tuple of ppt algorithms he keygen he enc he dec he eval the copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters message space rm of he is some ring and our computational model will be arithmetic circuits over this ring with addition and multiplication gates he keygen takes the security parameter and a depth parameter on the depth l of the computation desired and outputs a secret key sk and a public key pk he enc takes the public key pk a message μ and outputs a ciphertext c that encrypts μ he dec takes the secret key sk and a ciphertext c and outputs a message μ he eval takes the public key pk an arithmetic circuit f of depth less than l over rm and ciphertexts c where is the number of inputs to f and outputs a ciphertext cf definition we say that a homomorphic encryption has perfect correctness for a circuit family f if for all f f and for all μ rm it holds that if sk pk were properly generated by he keygen with security parameter λ and if ci he encpk μi for all i and cf he evalpk f c then pr he decsk cf f μ we say that the scheme compactly evaluates the family if in addition the running time of the decryption circuit only depends on λ and not on its input we use standard semantic security security under chosen plaintext attack as our security notion definition a homomorphic scheme is secure if any polynomial time adversary that ﬁrst gets a properly generated pk and then speciﬁes rm and ﬁnally gets he encpk μb for a random b cannot guess the value of b with probability negl λ gentry also showed that semantically secure leveled homomorphic encryption schemes exists assuming the hardness of certain lattice problems follow up scheme schemes have been constructed based on the approximate gcd problem the learning with errors lwe problem bgv12 the ring learning with errors rlwe problem and the ntru problem in all such schemes that we know of dec is in b low depth proofs let r be an eﬃciently computable binary relation for pairs x w r we call x the statement and w the witness let l be the language consisting of statements in r a noninteractive proof with perfect completeness and perfect soundness for a relation r consists of an eﬃcient prover p and a veriﬁer v such that the following holds perfect completeness a proof system is perfectly complete if an honest prover with a valid witness can always convince an honest veriﬁer for all x w r we have pr π p x w v x π perfect soundness a proof system is perfectly sound if it is infeasible to convince an honest veriﬁer when the statement is false for all x l and we have π such that v x π furthermore we say that a noninteractive proof is low depth if the veriﬁer v can be implemented in we note that low depth proofs are useful even for statements in p we stress that unlike the proof systems in section this proof system does not need a common random string we next sketch a very simple construction of a low depth noninteractive proof the prover p executes the np veriﬁcation circuit on the witness and generates the proof as the sequential concatenation in some speciﬁed order of the bit values assigned to the individual wires of the circuit the veriﬁer v proceeds by checking copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption consistency of the values assigned to the internal wires of the circuit for each gate in particular for each gate in the np veriﬁcation circuit the veriﬁer checks if the wire vales provided in the proof represent a correct evaluation of the gate since the veriﬁcation corresponding to each gate can be done independent of every other gate and in constant depth we have that v itself is in appendix c security of our assumption in this section we provide evidence that that our hardness assumption assumption does not fall prey to a class of simple attacks below we deﬁne a generic model that we term the generic colored matrix model capturing attacks where the adversary only multiplies matrices in the correct order and prove that assumption holds unconditionally in this model later in section c we discuss why we think that the generic colored matrix model captures the most natural attacks against our scheme c generic colored matrix model roughly the generic colored matrix model considers attacks in which the adversary is provided with matrices in some speciﬁc order and is only allowed to add multiply them in ways that respect this order and to check for equality the order is speciﬁed by assigning each matrix a left color and a right color permitting only addition of matrices of matching colors and only multiplication of matrices where the right color of one matches the left color of the other in more detail a matrix in this model is associated with left and right colors lc rc and an opaque handle h the colors are just arbitrary strings and below we use simple indexes for the handles i e the ﬁrst matrix that the adversary sees has handle the second has handle etc the adversary in this model is given it is the prime number p that deﬁnes the ﬁeld zp and for every matrix a zm n p given the tuple m lc n rc h but not the matrix a itself the adversary is interacting with a stateful representation oracle that keeps track of the correspondence between matrices and their handles and let the adversary perform generic computation on these matrices given the prime p the representation oracle chooses an initial set of colored matrices and assigns to them the handles inserts into its database the records ai mi lci ni rci i i sends to the adversary the prime p and representations mi lci ni rci i i without the matrices themselves and then processes queries from the adversary as follows addition when the adversary makes a query add h h the representation oracle looks up in the database the records corresponding to the two handles h h if such records exist let us denote them by a m lc n rc h and a m lc n rc h if they have the same dimensions and colors m lc m lc and n rc n rc then the representation oracle computes their modular sum a a a mod p if the database already contains the matrix a with dimensions and colors m lc n rc then the oracle returns the handle of that matrix otherwise the oracle assigns to this matrix the next available handle index h inserts into the database a record for a m lc n rc h and returns h to the adversary multiplication when the adversary makes a query mult h h the representation oracle looks up in the database the records corresponding to the two handles h h if such records exist let us denote them by a m lc n rc h and a m lc n rc h if they have matching dimensions and colors namely n rc m lc then the representation oracle computes their modular product a a a zm n if the database already contains the matrix a with dimenp sions and colors m lc n rc then the oracle returns the handle of that matrix otherwise the oracle assigns to this matrix the next available handle index h in copyright by siam unauthorized reproduction of this article is prohibited garg gentry halevi raykova sahai and waters downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php serts into the database a record for a m lc n rc h and returns h to the adversary c equivalent program indistinguishability in the colored matrix model we now proceed to show that assumption when ported to the generic colored matrix model holds unconditionally speciﬁcally an adversary that gets t handles from the representation oracle has at most o p advantage in distinguishing any two equivalent programs recall the structure of the randomized branching programs in our construction as explained in rn dp bp sr t rn t i n b d i b ri di b r i r t r n t r i n b d i b r i di b i our analysis in this generic model examines only adversary strategies that cancel out the randomizing matrices ri ri which means that these matrices must be multiplied in order hence in our model we assign to the matrices d i b left color i and right color i and similarly the matrices d i b are assigned the distinct left color i and right color i the row vectors and both are assigned the left color and they are assigned the right colors and respectively similarly the column vectors t t are assigned left colors n and n respectively and they are both assigned the right color n the representation oracle in our model on input a prime p branching program bp and a partial input assignment j σ chooses the matrices and vectors as above using the procedure described in section then discards the matrices that are incompatible with the partial assignment j σ and interacts with the adversary using the remaining matrices denoting n 2m the initial database of the representation oracle consists of the following matrices and their representation db p bp j σ lc n rc h lc n rc h n lc n rc n h t n lc n rc n h t d n lc i n rc i h i i b σ inp i j i b d n lc i n rc i h i i b j i b d i b n lc i n rc i h i ij b σ inp i d i b n lc i n rc i h i ij b where the handles are just all the indexes db the adversary is given as input the representation without the matrices themselves and then it interacts with the representation oracle as above we denote the total number of handles that the representation oracle sends to the adversary by t these include the initial db handles that are given to the adversary as input and all the handles that are sent in reply to the adversary queries the view of the adversary a in this interaction is denoted by viewa p bp j σ t staged in this model our hardness assumption can now be stated as the following theorem theorem for any prime p branching program bp and two functionallyequivalent partial input assignments j σ and j σ and any generic colored matrixmodel adversary a that receives at most t handles from the representation oracle the statistical distance between the views viewa p bp j σ t and viewa p bp j σ t is bounded below o nt p note that within the generic colored matrix model the above claim is unconditional copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption c security proof to prove theorem we describe an information theoretic simulation strategy that can simulate the queries or in other words the view of the adversary without knowledge of σ but just with oracle access to the function bp σ since the simulation relies just on bp σ which are identical over the two choices of σ σ the simulation in the two settings must be identical and since it is a good simulation in both cases the theorem follows we note that the our simulation strategy will be ineﬃcient and as a consequence we do not get virtual back box security noncommutative formal polynomials we introduce some useful notation for describing our simulation strategy the simulation strategy must keep track of the computation that the adversary is performing over the representations that are given to it and it does that using noncommutative formal polynomials ncfps a noncommutative formal polynomial over the set of variables v vt is of the form i ci viti where ci zp is a constant and viti is the ith monomial and ti denotes the number of variables in the monomial speciﬁcally the iti ones the addition subtraction and multiplication of any two ncfps is also a ncfp let t t di b di b denote the variables corresponding to the representations that are given to the adversary at initialization and then any handle that the adversary gets during the interaction must correspond to a ncfp in these variables simulation the simulator gets as input the prime p the length n of the branching program bp and the set j of ﬁxed input bits but not the actual input assignment σ or σ and it has access to the function bp σ bp σ the simulator maintains a database p of ncfps in the variables above each stored in canonical sum of monomials form where the monomials are in lexicographic order along with their representations initially the database consists of only the single variable terms and the adversary is given the corresponding corresponding to t t di b di b representation thereafter the simulator interacts with the adversary playing the role of the representation oracle as follows addition when the adversary makes a query add h h the simulator checks its database to ﬁnd the records corresponding to these two handles if these records exist we denote them by p lc m n rc h and p lc m n rc h where p p are ncfps if both records exist and they satisfy lc m lc m n rc n rc then the simulator computes the ncfp p p p with scalar arithmetic modulo p otherwise the simulator returns the simulator next checks if a record for p lc m n rc h exists in the database and otherwise it inserts such record to the database using the next available index for h finally the simulator returns h to the adversary multiplication when the adversary makes a query mult h h the representation oracle looks up in the database the records corresponding to the two handles h h if such records exist let us denote them by p m lc n rc h and p m lc n rc h if they have matching dimensions and colors namely n rc m lc then the representation oracle computes their product p p p if they do not exist or are not matching then the simulator returns if lc and rc n which means that this is a matrix corresponding to products that includes the bookend vectors then the simulator performs an additional step to ensure consistency note that because of the color consistency re quirements all of the monomials in p must be of the form either c ni d i bi t n t for some bits b2 bn but cannot have a mix of primed or c i d i b i and nonprimed variables moreover for all i ij we must have bi σ inp i since copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters for these steps we only give out one matrix in the primal program and one in the dummy program corresponding to the bit σ inp i considering the sequence of bits b b2 bn we say that this it is consistent with the program bp if there exists an input χ such that bi χinp i for all i moreover in this case we sometimes say that b is consistent with χ in other words if we have two steps i i with inp i inp i but bi bi then we say that b is inconsistent with the program bp and otherwise it is consistent the same terminology is extended to the monomials a monomial c ni d i bi t or n t is consistent with χ if b2 bn is consistent with χ and c i d i b i inconsistent otherwise if lc and rc n then the simulator goes over all the monomials n in p if it ﬁnds any monomial that is a made up of nonprimed variables c i d i bi t and b is consistent with some χ such that bp χ then the simulator replaces n t if there is already a that monomial by its primed counterpart c i d i b i monomial for these primed variables then it adds the two constants note that the simulator can check the two conditions a and b above since it knows the branching program bp and it has access to an oracle for bp σ denote the resulting ncfp after all the substitutions if any by p if lc or rc n then no substitutions are made and we have p p if the database does not already have a record for p lc m n rc h then the simulator inserts one using the next available handle index h the simulator returns h to the adversary intuition note that a consistent monomial made out of nonprimed variables corresponds to the evaluation of the primary program on some input χ and the counterpart monomial of primed variables corresponds to the evaluation of the dummy program on the same input by construction if bp χ then these two evaluations yield the same element of zp even though as ncfps they correspond to diﬀerent terms in the simulation we therefore identify these two terms and only keep one of them the choice to keep the dummy term is arbitrary we just as well could have kept the primal term instead we note that simulator above uses superpolynomial time and space but this is irrelevant since the statement we are proving is an unconditional probability statement c proof that the simulation works now we need to argue that the view of the adversary a when interacting with the simulation strategy above is close to its view when interacting with the representation oracle regardless of whether the execution uses j σ or j σ it is clear by construction that whenever the simulator returns an existing handle from its database i e the latest query returns the same ncfp as a previous one then also the representation oracle must return the same existing handle since also the actual matrix must be the same in this case it is left to show that when the simulator returns a new handle corresponding to a new ncfp then with high probability also the representation oracle would have returned a new handle corresponding to a new actual matrix modulo p for the arguments below recall again that the coloring constraints restrict what monomials we could have in the ncfps in p speciﬁcally the variables can only appear in a speciﬁc order in particular can only be followed by or similarly for each i n di and di can only be followed by di or di finally dn and dn can only be followed by t similar constraints are also true for the primed variables this implies that primed and nonprimed variables cannot be copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption multiplied and they can only be added for terms that include the bookends and t or and t we partition the argument into two cases one for queries that return a matrix with colors lc rc n and the other for all the other queries we begin with the latter case ncfps with lc or rc n this case corresponds roughly to the partial evaluation attacks from section consider a query made by the adversary that results in a matrix with left color lc i e i or i for i or right color rc n i e rc j or rc j for j n below we provide the details for the nonprimed case lc and rc n treatment of the other cases is analogous recall that we consider here a query that results in a new formal polynomial diﬀerent than the formal polynomials in all previous queries we denote the previous ncfps that the simulator has in its database before that query by p2 pr and the new ncfp generated by the query is denoted by p our goal is to show that with high probability over the choices of the representation oracle the actual matrix that result is also diﬀerent from the matrices in all the previous queries this would imply that in both the simulation and the real execution the adversary will get a new handle in reply to this query for the case lc i and rc j n the coloring constraints imply that all the variables in p must be nonprimed and also that p does not include the variables or t thus p describes an n n matrix of the form c ri drj where d is mostly diagonal except in the bottom quadrant and the r are invertible matrices fix any one of the previous ncfps pk and we have two subcases p and pk are colored diﬀerently this is the easy case by deﬁnition of the model the handle that is returned to the adversary for this query in both the simulation and the real execution is diﬀerent than what was returned for pk p and pk have the same left and right colors hence also pk is of the form ck ri dk rj note that the scalars c ck zp are completely determined by the queries of the adversary and are independent of the random choices of the representation oracle in the real execution and the matrices ri and rj are the same for p and pk it follows that the actual matrices in the real execution are equal if and only if c d ck dk mod p note now that the matrices c d and ck dk can themselves be expressed as formal multilinear polynomials over zp in the variables di b and these polynomials have degree at most n and must diﬀer as formal polynomials since p and pk diﬀer moreover since the di b are all diagonal except the bottom right quadrant then the top left entries in c d and ck dk can be expressed as the same formal polynomials in the top left entries of the di b since in our construction all these top left entries are chosen independently at random in zp we conclude that the top left entry in the matrix c d ck dk mod p is obtained as the evaluation of a nonzero multilinear polynomial of degree at most n in variables that are chosen uniformly in zp the schwartz zippel lemma implies that this entry is nonzero except with probability bounded below n p hence the probability that the actual matrices corresponding to p pk are equal is also bounded below n p all the other subcases are treated mostly the same the nonprimed case lc and rc n is identical to above except that here we consider the ﬁrst entry in the column vectors c dt and ck dk t the nonprimed case lc and rc n is similar but we consider the m entry in the row vectors c sd and ck sdk copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters note that the ﬁrst entry is identically zero by construction finally treatment of the primed variables is identical to their nonprimed counterparts ncfps with lc and rc n this case corresponds roughly to the mixed input attacks from section in this case the formal polynomial p correspond n d i bi t to a matrix and all the terms in p are either of the form x c i n or x c i d i bi t for a scalar c zp and bits bi recall that the structure of and t cancels out all the random diagonal entries in except the bottom right quadrant hence the actual the d i b and d i b matrix corresponding to every such term therefore depends only on the permutation matrix in the bottom quadrant the random scalars αi bi or α i bi and the t or t speciﬁcally a term of primed variables corresponds to vectors ρ where ρ t and a term of nonprimed variables corresponds α i n i bi to i n αi bi i n ai bi t below we refer to the set of steps ij in the program bp which are the steps that examine the input bit χj as the jth block of steps a monomial x as above is consistent with the jth block if for all i ij we have bi or for all i ij we have bi otherwise if for some steps i ij we have bi and for others bi we call this monomial inconsistent with the jth block clearly a monomial is consistent with some input χ if and only if it is consistent with all the blocks j at random subject to the connext recall that the αi b and α i b were chosen straint that for every j and b we have i ij αi b i ij α i b below it will be convenient to consider the following procedure for choosing these scalars denoting k ij and ij ik for b we draw random scalars γg b zp for g and then for h k set αih b b b and α ih b b b with index arithmetic modulo note that b b it is easy to see that this process indeed generates the uniform distribution subject to the constraint above with this procedure for every concrete choice of the vectors t and t we can express the scalar corresponding to each ncfp monomial x as a formal monomial in the γ below we prove that for any two distinct ncfps p p it holds with high probability over the choice of the vectors that the corresponding formal polynomials in the γ are also be distinct applying the schwartz zippel lemma again we conclude that the scalars that correspond to p pk are diﬀerent with high probability a key lemma that we use is the following lemma straddling lemma fix an integer k and consider drawing random scalars γj zq for j and for i k setting αi and α i with index arithmetic modulo note that also ﬁx two subsets i i k with i k and then we have that i i αi and i i αi are distinct formal multilinear monomials in the variables proof it is clear that the two products are indeed multilinear monomials in the γ it remains to show that they are distinct two case arise either i i or i i if i i then since i is a nonempty set strictly contained in k there must exists an index j i such that j mod k i i this implies that i i αi contains but i i αi does not exists j such that either j i i or j i i in the ﬁrst case if i i there we get but i i αi does not and in the second that i i αi contains γ case i i αi contains but i i αi does not armed with lemma we can now ﬁnally present our analysis let p p be two ﬁxed distinct ncfps in the database of the simulator at the end of the simulation copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption both with colors lc rc n and we prove that the scalar corresponding to δ p p in the real execution of the representation oracle is nonzero except with probability at most p since δ is a nonzero ncfp then let x be a monomial in δ we analyze two cases where x is either a consistent monomial or an inconsistent one inconsistent monomials for an inconsistent monomial assume that it is made n up of nonprimed variables x c i d i bi t the case for primed variables is symmetric as argued above the scalar corresponding to this term can also be described as c αi bi ψt c bi bi ψt i n i n for some permutation matrix ψ for every concrete choice of ψt this gives a single monomial in the γ with coeﬃcient which is nonzero except with probability o p we claim that no other monomial in the ncfp δ corresponds to the same product of γ it is easy to see that no other monomial of nonprimed variables in δ can correspond to the same γ since every other nonprimed monomial must correspond to some b i bi for some i n and thus include αi b i b i b i rather than αi bi bi bi to see that also no primed monomial can correspond to the same product we use lemma above since x is an inconsistent terms it is in particular inconsistent with some block j this means that the product of d contains both d i for some i ij and d i for some other i ij hence the corresponding product of the α contains some of the αi for i ij but not all of them we can therefore apply lemma using the set of steps where we use αi as our nonempty proper subset i the lemma then tells us that no product of the α i b can yield the same product of γ which is what we needed to show since no other term in δ has the same monomial in the γ it means in particular that the standard formal polynomial in the γ must be nonzero whenever ψt since nothing can cancel out this monomial consistent monomials assuming that δ consists only of consistent monomials let x be one of these monomials and let χ be the input that x is consistent with observe that if bp χ then it means that x is made up of primed variables since nonprimed monomials that are consistent with accepted inputs are replaced by the simulator with their primed counterparts in this case we again argue that that there is no other term in δ that corresponds to the same product of γ as x since every other primed or nonprimed monomial must have some αi bi in at least one step where x has αi bi in other words every other monomial is consistent with a diﬀerent input and hence the product of the αi b values for that input will be diﬀerent it remains to examine the case where x is consistent with an input χ such that bp χ in this case δ can perhaps contain another term corresponding to the t and the same product of γ i e one primed monomial x c ni d i χ inp i n one nonprimed x c i d i χinp i t we assume without loss of generality that both terms exist with at least one of c c nonzero denoting bi χinp i the unique term in δ with the product of all the γ corresponding to x x is therefore γ bi γ bi c t c π t i n copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php garg gentry halevi raykova sahai and waters where π is the resultant nonidentity permutation matrix when the output of bp is zero since π is a permutation matrix diﬀerent from the identity and since at least one of c c is nonzero the expression c t c π t is nonzero except with probability o p to be precise for every permutation π i and every c c zp not both zero we have p pr c t c π t p where the probability is taken over the choice of the vectors t t such that t t the bounds follow from the facts that once t have been chosen the values t can be chosen in a way such that the expression evaluates to with probability p and that if either c or c is zero then the expression is with probability p taking union bound gives the above inequality we again conclude that except with small probability over the choice of the vectors the formal polynomial in the γ must be nonzero completing the proof we have shown that for every nonzero ncfp δ it holds with probability at least o p over the choice of the vectors t and t that the corresponding formal polynomial in the γ s is also nonzero since the latter formal polynomial has degree is follows from the schwartz zippel lemma that the actual scalar corresponding to δ is nonzero with probability at least o n p we thus conclude that for any pair of distinct ncfps that the simulator has in its database the representation oracle will have distinct matrices except with probability no more than o n p as there are t such ncfps the probability that any pair of them causes a collision is at most o n p o t n p and as long as there are no collisions the view of the adversary in the simulated execution is identical to its view in the real one this competes the proof of theorem c justification for the model we now describe our intuition for why attacks under the generic colored matrix model constitute the most natural class of attacks against our obfuscation scheme our results in this model should be viewed as ensuring that our scheme cannot be broken by linear algebra we note however that at present we are not aware of any plausible attack on the scheme that does not ﬁt in this model except of course lattice reduction attacks on the underlying ggh encoding scheme recall that in our scheme we give out ggh type encoding of the elements of the matrices d i b and vectors s t which are computed as s d i b ri di b ri and t rn t for randomly chosen ri s the elements of d i b are encoded relative to a ggh denominator zi the vectors s t are encoded relative to z0 zn respectively and the zero test parameter is given relative to a product of all the zi s regarding plausible attacks on the scheme we note the following points using the zero test parameter to cancel out the zi s requires an element which is encoded relative to the product of all of them and obtaining such an element from the given public parameters boils down essentially to computing a multilinear function with each term having exactly one element encoded relative to each zi note that the presence of noise in the numerator seems to eﬀectively destroy the utility of performing any higher degree computations such as building a term which is degree two in each zi in the denominator and then attempting to compute a square root trying to mix and match elements from diﬀerent matrices vectors or to multiply these matrices out of order makes it seemingly impossible to eliminate the random ri s copyright by siam unauthorized reproduction of this article is prohibited downloaded to redistribution subject to siam license or copyright see http www siam org journals ojsa php indistiguishability obfuscation and func encryption at the same time it seems that ggh speciﬁc attacks such as the weak discrete logarithm attack from are not applicable to our scheme since this scheme does not provide encoding of known constants such as zero or one that are needed in such attacks although a nontrivial encoding of zero relative to the product of all the zi s is obtained as part of the computation it is not clear how an attacker can compute a lower level encoding of zero relative to just part of the zi s as required those attacks given these diﬃculties in devising attacks that do not respect the matrix structure or their order we therefore devised the generic colored matrix model to isolate and analyze the attacks that do respect this structure a large body of work has been devoted to the study of fast randomized approximation algorithms for problems in numerical linear algebra several well studied problems in this area include least squares regression low rank approximation and approximate computation of leverage scores these problems have many applications in data mining azar et al recommendation systems drineas et al information retrieval papadimitriou et al web search achlioptas et al kleinberg clustering drineas et al mcsherry and learning mixtures of distributions kannan et al achlioptas and mcsherry the use of randomization and approximation allows one to solve these problems much faster than with deterministic methods for example in the overconstrained least squares regression problem we are given an n d matrix a of rank r as input n d together with an n column vector b the goal is to output a vector x so that with high probability ax b ε minx ax b for some small enough ε the minimizing vector x can be expressed in terms of the moore penrose pseudoinverse a of a namely x a b the pseudoinverse is discussed in definition see section for some notation and background if a has full column rank this simplifies to x a a a b this minimizer can be computed deterministically in o time but with randomization and approximation this problem can be solved in o nd log d poly dε time sarlo s drineas et al which is much faster for d n and not too small here poly dε denotes a low degree polynomial in dε the generalization of this problem to p regression is to output a vector x so that with high probability ax b p ε minx ax b p this can be solved exactly using convex programming though with randomization and approximation it is possible to achieve o nd log n poly dε time clarkson et al for any constant p p another example is low rank approximation here we are given an n n matrix which can be generalized to n d and an input parameter k the goal is to find an n n matrix a of rank at most k for which a a f ε a ak f where n n for an n n matrix b b i j bi j is the squared frobenius norm and ak argminrank b k a b f here ak can be computed deterministically using the singular value decomposition in o time however using randomization and approximation this problem can be solved in o nnz a k ε k log k n poly k ε time sarlo s clarkson and woodruff where nnz a denotes the number of nonzero entries of a the problem can also be solved using randomization and approximation in o log n n poly k ε time sarlo s which may be faster than the former for dense matrices and large k another problem that we consider is approximating the leverage scores given an n d matrix a with n d one can write a u v in its singular value decomposition where the columns of u are the left singular vectors is a diagonal matrix and the columns of v are the right singular vectors although u has orthonormal columns not much can be immediately said about the squared lengths ui of its rows these values are known as the leverage scores which measure the extent to which the singular vectors of a are correlated with the standard basis the leverage scores are basis independent since they are equal to the diagonal elements of the projection matrix onto the span of the columns of a see drineas et al for background on leverage scores as well as a list of applications the leverage scores will also play a crucial role in our work as we shall see the goal of approximating the leverage scores is to simultaneously for each i n output a constant factor approximation to ui using randomization this can be solved in o nd log n log d log n time drineas et al journal of the acm vol no article publication date january low rank approximation and regression in input sparsity time there are also solutions for these problems based on sampling they either get a weaker additive error frieze et al papadimitriou et al achlioptas and mcsherry drineas et al drineas and mahoney rudelson and vershynin deshpande et al or they get a bounded relative error but are slow deshpande and vempala drineas et al many of the latter algorithms were improved independently by deshpande and vempala and sarlo s as well as in follow up work drineas et al nguyen et al magen and zouzias there are also solutions based on iterative and conjugate gradient methods see for example trefethen and bau or zouzias and freris as recent examples these methods repeatedly compute matrix vector products ax for various vectors x in the most common setting such products require nnz a time thus the work per iteration of these methods is nnz a and the number of iterations n that are performed depends on the desired accuracy spectral properties of a numerical stability issues and other concerns and can be large a recent survey suggests that n is typically k for krylov methods such as arnoldi and lanczos iterations to approximate the k leading singular vectors halko et al one can also use some of these techniques together for example by first obtaining a preconditioner using the johnson lindenstrauss jl transform johnson and lindenstrauss then running an iterative method while these results illustrate the power of randomization and approximation their main drawback is that they are not optimal for example for regression we could hope for o nnz a poly d ε time ideally while the o nd log d poly d ε time algorithm for least squares regression is almost optimal for dense matrices if nnz a nd say nnz a o n as commonly occurs this could be much worse than an o nnz a poly d ε time algorithm similarly for low rank approximation the best known algorithms that are condition independent run in o nnz a k ε k log k n poly k ε time while we could hope for o nnz a poly k ε time results we resolve these gaps by achieving algorithms for least squares regression low rank approximation and approximate leverage scores whose time complexities have a leading order term that is o nnz a sometimes up to a log factor with constant factors that are independent of any numerical properties of a our results are as follows least squares regression we present several algorithms for an n d matrix a with rank r and given ε one has a runtime bound of o nnz a log n ε r r r log ε stated in theorem note the logarithmic dependence on ε a variation of this algorithm has an o nnz a log ε d log ε runtime another has a runtime of o nnz a o ε stated in theorem note that the dependence on nnz a is linear here o f f log o f we also give an algorithm for generalized multiple response regression where min x ax b f is found for b rn d in time o nnz a log n r r d ε rd r r log n see theorem we also note improved results for constrained regression section low rank approximation we achieve a runtime of o nnz a n poly k log n ε to find an orthonormal l w rn k and diagonal d rk k matrix with a ldw f within ε of the error of the best rank k approximation more specifically theorem gives a time bound of o nnz a o ε ε journal of the acm vol no article publication date january k l clarkson and d p woodruff note that ldw is not the truncated singular value decomposition but has the same structure we use different notation than the usual u v as a reminder of this distinction approximate leverage scores for any fixed constant ε we simultaneously ε approximate all n leverage scores in o nnz a log n r r r log n time this can be generalized to subconstant ε to achieve o nnz a log n poly r ε time however in the applications we are aware of such as coresets for regression dasgupta et al ε is typically constant in the applications of this a general ε can be achieved by oversampling drineas et al dasgupta et al p regression for p we achieve a runtime of o nnz a log n poly rε in theorem as an immediate corollary of our results and a recent connection between and p regression given in clarkson et al for p the nnz a log n term can be improved to nnz a as stated earlier techniques all of our results are achieved by improving the time complexity of computing what is known as a subspace embedding for a given n d matrix a call s rn rt a subspace embedding matrix for a if for all x rd s ax ε ax that is s embeds the column space colspace a ax x rd into rt while approximately preserving the norms of all vectors in that subspace the subspace embedding problem is to find such an embedding matrix obliviously that is to design a distribution π over linear maps s rn rt such that for any fixed n d matrix a if we choose s π then with large probability s is an embedding matrix for a the goal is to minimize t as a function of n d and ε while also allowing the matrix matrix product s a to be computed quickly a closely related construction easily derived from a subspace embedding is an affine embedding involving an additional matrix b rn d such that ax b f s ax b f d d for all x r see section these affine embeddings are used for our low rank approximation results and immediately imply approximation algorithms for constrained regression when embedding matrix s is a fast johnson lindenstrauss transform ailon and chazelle one can set t o d and achieve o nd log t time for d γ for any constant γ one can also take s to be a subsampled randomized hadamard transform srht see e g lemma of boutsidis and gittens and set t o ε log d d log n to achieve o nd log t time these were the fastest known subspace embeddings achieving any value of t not depending polynomially on n our main result improves this to achieve t poly d ε for matrices s for which s a can be computed in nnz a time given our new subspace embedding we plug it into known methods of solving the linear algebra problems presented earlier given a subspace embedding as a black box in fact our subspace embedding is nothing other than the countsketch matrix in the data stream literature charikar et al see also thorup and zhang this matrix was also studied by dasgupta et al formally s has a single randomly chosen nonzero entry sh j j in each column j for a random mapping h n t here for i n h i is chosen independently uniformly from t with probability sh j j and with probability sh j j while such matrices s have been studied before the surprising fact is that they actually provide subspace embeddings the usual way of proving that a random s π is a subspace embedding is to show that for any fixed vector y rd pr sy ε y exp d one then puts a net see e g arora et al on journal of the acm vol no article publication date january low rank approximation and regression in input sparsity time the unit vectors in the column space colspace a and argues by a union bound that sy ε y for all net points y this then implies for a net that is sufficiently fine and using the linearity of the mapping that sy ε y for all vectors y colspace a we stress that our choice of matrices s does not preserve the norms of an arbitrary set of exp d vectors with high probability thus this approach cannot work for our choice of matrices s we instead critically use that these exp d vectors all come from a d dimensional subspace i e colspace a and therefore have a very special structure the structural fact we use is that for any α there is a fixed set h of size d α that depends only on the subspace such that for any unit vector y colspace a h contains the indices of all coordinates of y larger than α in magnitude it is useful to think of α as about d the key property here is that the set h is independent of y or in other words only a small set of coordinates could ever be large as we range over all unit vectors in the subspace the set h selects exactly the set of large leverage scores of the columns space colspace a given this observation by setting t k h for a large enough constant k recall that h is the cardinality of h we have that with probability k there are no two distinct j j with j j h for which h j h j that is we avoid the birthday paradox and the coordinates in h are perfectly hashed with large probability call this event e which we condition on given a unit vector y in the subspace we can write it as y h y l where y h consists of y with the coordinates in n h replaced with while y l consists of y with the coordinates in h replaced with we seek to bound sy sy h sy l sy h sy l since e occurs we have the isometry sy h y h now y l α thus we can apply theorem of dasgupta et al which shows that for mappings of our form if the input vector has small infinity norm then s preserves the norm of the vector up to an additive o ε factor with high probability here it suffices to set α poly d ε finally we can bound sy h sy l as follows define g n h to be the set of coordinates j for which h j h j for a coordinate j h that is those coordinates in n h that collide with an element of h then sy h sy l sy h sy l where y l is a vector that agrees with y l on coordinates j g and is on the remaining coordinates by cauchy schwarz this is at most sy h sy l we have already argued that sy h y h for unit vectors y moreover we can again apply theorem of dasgupta et al to bound sy l since conditioned on the coordinates of y l hashing to the set of items that the coordinates of y h hash to they are otherwise random thus we again have a mapping of our form with a smaller t and applied to a smaller n applied to a vector with a small infinity norm therefore sy l l o ε y with high probability finally by bernstein bounds since the coordinates of y l are small and t is sufficiently large y l ε with high probability hence conditioned on event e sy ε y with probability exp d and we can complete the argument by union bounding over a sufficiently fine net we note that an inspiration for this work comes from work on estimating norms in a data stream with efficient update time by designing separate data structures for the heavy and light components of a vector nelson and woodruff kane et al a key concept here is to characterize the heaviness of coordinates in a vector space in terms of its leverage scores optimizing the additive term the approach outlined early already illustrates the main idea behind our subspace embedding providing the first known subspace embedding that can be implemented in nnz a time this is sufficient to achieve our numerical linear algebra results in time o nnz a poly d ε for regression journal of the acm vol no article publication date january k l clarkson and d p woodruff and o nnz a n poly k log n ε for low rank approximation however for some applications d k or ε may also be large thus it is important to achieve a small degree in the additive poly d ε and n poly k log n ε factors the number of rows of the matrix s is t poly d ε and the simplest analysis described earlier would give roughly t d ε we now show how to optimize this the first idea for bringing this down is that the analysis of dasgupta et al can itself be tightened by applying it on vectors coming from a subspace instead of on a set of arbitrary vectors this involves observing that in the analysis of dasgupta et al if on input vector y and for every i t j h j i is small then the remainder since our of the analysis of dasgupta et al does not require that y be small vectors come from a subspace it suffices to show that for every i t j h j i u j is small where u j is the jth leverage score of a therefore we do not need to perform this analysis for each y but can condition on a single event this effectively allows us to increase α in the earlier outline thereby reducing the size of h as well as the size of t since we have t h in fact we instead follow a simpler and slightly tighter analysis of kane and nelson based on the hanson wright inequality another idea is that the estimation of y h the contribution from the heavy coordinates is inefficient since it requires a perfect hashing of the coordinates which can be optimized to reduce the additive term to ε polylog d ε in the worst case there are d leverage scores of value about of value about of value about and so on while the top d leverage scores need to be perfectly hashed e g if a contains the d d identity matrix as a submatrix it is not necessary that the leverage scores of smaller value yet still larger than d be perfectly hashed allowing a small number of collisions is okay provided that all vectors in the subspace have a small norm on these collisions which just corresponds to the spectral norm of a submatrix of a this gives an additive term of ε polylog d ε instead of o ε this refinement is discussed in section there is yet another way to optimize the additive term to roughly log n which is useful in its own right since the error probability of the mapping can now be made very low namely poly n this low error probability bound is needed for our application to p regression see section by standard balls and bins analyses if we have o log n bins and balls then with high probability each bin will contain o log n balls we thus make t roughly o log n and think of having o log n bins in each bin i o log n heavy coordinates j will satisfy h j i then we apply a separate jl transform on the coordinates that hash to each bin i this jl transform maps a vector z rn to an o log n dimensional vector z for which z ε z with probability at least poly n since there are only o log n heavy coordinates mapping to a given bin we can put a net on all vectors on such coordinates of size only poly n we can do this for each of the o log n bins and take a union bound it follows that the norm of the vector of coordinates that hash to each bin is preserved and so the entire vector y h of heavy coordinates has its norm preserved by a result of kane and nelson the jl transform can be implemented in o log n ε time giving total time o nnz a log n ε which reduces t to roughly o log n we also note that for applications such as least squares regression it suffices to set ε to be a constant in the subspace embedding since we can use an approach in drineas et al and dasgupta et al which given constant factor approximations to all of the leverage scores can then achieve a ε approximation to least squares regression by slightly oversampling rows of the adjoined matrix a b proportional to its leverage scores and solving the induced subproblem this results in a better dependence on ε journal of the acm vol no article publication date january low rank approximation and regression in input sparsity time we can also compose our subspace embedding with a fast jl transform ailon and chazelle to further reduce t to the optimal value of about d since s a already has small dimensions applying a fast jl transform is now efficient finally we can use a recent result of cheung et al to replace most dependencies on d in our runtimes for regression with a dependence on the rank r of a which may be smaller note that when a matrix a is input that has leverage scores that are roughly equal to each other then the set h of heavy coordinates is empty such a leverage score condition is assumed for example in the analysis of matrix completion algorithms for such matrices the sketching dimension can be made ε log d ε slightly improving our ε polylog d ε dimension presented earlier recent related work in the first version of our technical report on these ideas july the additive poly k d ε terms were not optimized in the second version the additive terms were more refined and results on p regression for general p were given but the analysis of sparse embeddings in section was absent in the third version we refined the dependence still further with the partitioning in section recently a number of authors have told us of follow up work all building on our initial technical report miller and peng showed that regression can be done with the additive term sharpened to subcubic dependence on d and with linear dependence on nnz a more fundamentally they showed that a subspace embedding can be found in o nnz a dω α ε time to dimension o α log d nnz a d ε here ω is the exponent for asymptotically fast matrix multiplication and α is an arbitrary constant some constant factors here are increasing in α nelson and nguyen obtained similar results for regression and showed that sparse embeddings can embed into dimension o in o nnz a time this considerably improved on our dimension bound for that runtime at that point our second version although our current bound is within polylog d ε of their result they also showed a dimension bound of o α for α with work o f α nnz a ε for a particular function of α their analysis techniques are quite different from ours both of these papers use fast matrix multiplication to achieve subcubic dependence on d in applications our cubic term involves a jl transform which may have favorable properties in practice regarding subspace embeddings to dimensions near linear in d note that by computing leverage scores and then sampling based on those scores we can obtain subspace embeddings to o dε log d dimensions in o nnz a log n o r time this may be incomparable to the results just mentioned for which the runtimes increase as α possibly significantly paul et al implemented our subspace embeddings and found that in the techtc matrices a collection of sparse matrices of document term data with an average of to rows and columns our subspace embeddings as used for the projection step in their svm classifier are about times faster than the fast jl transform while maintaining the same classification accuracy despite this large improvement in the time for projecting the data further research is needed for svm classification as the jl transform empirically possesses additional properties important for svm that make it faster to classify the projected data even though the time to project the data using our method is faster meng and mahoney improved on the first version of our additive terms for subspace embeddings and showed that these ideas can also be applied to p regression for p our work on this in section achieves p and was done journal of the acm vol no article publication date january k l clarkson and d p woodruff independently we note that our algorithms for p regression require constructions of embeddings that are successful with high probability as we obtain for generalized embeddings thus some of the constructions in miller and peng and nelson and nguyen as well as our nongeneralized embeddings will not yield such p results after the conference publication of this article several additional papers on the topic have appeared in the last year woodruff and zhang show how to obtain o nnz a time embeddings for all p norms p into the norm with embedding dimension max p poly d rows and poly d distortion this ultimately results in o nnz a log n poly d ε time for the p regression problems as we achieve but the space complexity of the algorithm is lower due to the lower embedding dimension which is the first work to obtain a dimension smaller than n poly d for p while the follow up works meng and mahoney nelson and nguyen have simpler proofs of our subspace embedding results we believe that our original technique of separating coordinates into heavy and light is still valuable as it is currently the only known way of obtaining o nnz a time subspace embeddings for p meng and mahoney woodruff and zhang for p it also captures the intuition on the structure of the subspace vectors that was used to originally discover these results and provides an embedding of any set of exp d vectors that have a fixed small set of heavy coordinates not only those that come from a subspace avron et al show how to use our sparse embedding matrices s together with additional vandermonde like assumptions on the structure of a to achieve the fastest known algorithms for polynomial fitting yang et al have further extended their aforementioned work meng and mahoney to obtain the first o nnz a time algorithms for quantile regression nelson and nguyen have studied the problem of obtaining trade offs on the sparsity required of subspace embeddings versus the embedding dimension avron et al show how to combine a fast multiplication algorithm of pagh for applying our subspace embeddings to the polynomial kernel expansion k a of an underlying matrix a in time that only depends on o nnz a rather than on nnz k a recently the second author has written a survey on this topic woodruff which discusses a number of the works covered earlier in this article and other more recent works outline we formally define our sparse embedding construction in section we then introduce basic notation and definitions in section and present a basic analysis in section a more refined analysis is given in section in section generalized embeddings with high probability guarantees are discussed in these sections we generally follow the framework presented earlier splitting coordinates of column space vectors into sets of large and small ones analyzing each such set separately then bringing these analyses together shifting to applications we discuss leverage score approximation in section and regression in section including the use of leverage scores and the algorithmic machinery used to estimate them considering affine embeddings in section constrained regression in section and iterative methods in section our low rank approximation algorithms are given in section in which we use constructions and analysis based on leverage scores and regression in section we apply generalized sparse embeddings to p regression in section we give some preliminary experimental results journal of the acm vol no article publication date january low rank approximation and regression in input sparsity time sparse embeddings definition sparse embeddings for a parameter t we define a random linear map d rn rt as follows h n t is a random map so that for each i n h i t for t t with probability t t n is a t n binary matrix with h i i and all remaining entries d is an n n random diagonal matrix with each diagonal entry independently chosen to be or with equal probability we will refer to a matrix of the form d as a sparse embedding matrix notation assumptions background general notation definition o we use the standard asymptotic notation o f x f x log o f x for a function f of x we let poly x denote x o as x definition indicator for an event p let p denote when p holds and otherwise definition set cardinality for a finite set h we let h denote the number of elements of h definition for real values a b c we use a b c to denote the condition b c a b c and extend this with the distribution of multiplication over thus for e a e b c if and only if e b c a e b c matrix notation assumptions and background throughout this article a rn d is an n d matrix with n d we assume that a has no rows or columns containing only zeros so that the number of nonzero entries of a is at least n we let r denote the rank of a definition n ai aj nnz for integer n let n n let ai denote the ith row of a and a j denote its jth column let nnz a denote the number of nonzero entries of a definition norms for a vector y rn and p the p norm y p i n yi p p we may omit the subscript for the euclidean norm let a f de note its frobenius norm i n j d ai j let a supx rd ax x denote the spectral norm of a definition colspace tr let colspace a denote the column space of a colspace a ax x rd i e the range of a for square a the trace tr a i n ai i definition rank svd pseudoinverse best rank k the rank of a rank a is the dimension of its column space it is known that rank a rank a as shown by eckart and young golub and van loan for any a rn d of rank r there is u rn r v rd r and rk r called the singular value decomposition svd of a such that a u v with u and v having orthonormal columns and a diagonal matrix with positive entries σi i i the singular values of a this is the so called thin or economical svd alternatively u and v are square with rn d and having some diagonal entries that are zero the columns of the svd are permuted journal of the acm vol no article publication date january k l clarkson and d p woodruff so that the moore penrose pseudoinverse of a denoted a is v u where is the diagonal matrix with diagonal entries σi i r the best rank k approximation to a is the matrix ak argminrank y k y a f we may also write a k for this matrix it is known that a k u k v where k is equal to in its top k entries and zero thereafter we will need the upper bound part of khintchine s inequality using a constant factor following from haagerup s bound haagerup lemma khintchine for integers n p and x rn let ai n be random with each ai independent and equal to and with equal probability then e a x p p c p x where c p o p as p bernstein s inequality the following variation of bernstein s will be helpful lemma for l t and independent random variables xi t with v i var xi if v lt then pr xi e xi lt exp l i i proof here bernstein s inequality says that for yi xi e xi so that e var xi v and yi t log pr yi z v zt i by the quadratic formula the latter is no more than l when lt z lt which holds for z lt and v lt notation for matrix a definition u u s s let u rn r have columns that form an orthonormal basis for the column space colspace a the u of the svd a u v could be such a basis for example for un the rows of u let ui ui throughout we let s min i ui t and s max i s j i u j it will be convenient to regard the rows of a and u to be rearranged so that the ui are in nonincreasing order thus is largest of course this order is unknown and unused by our algorithms definition ya b s s for y rn and a b n let ya b denote the vector with ith coordinate equal to yi when i a b and zero otherwise analysis handling vectors with small entries we begin the analysis by considering ys n for fixed unit vectors y colspace a since y there must be a unit vector x so that y u x thus by cauchy schwartz see wikipedia entry on bernstein s inequalities probability theory journal of the acm vol no article publication date january low rank approximation and regression in input sparsity time yi ui x ui this implies that ys n us we extend this to all unit vectors in subsequent sections we show an embedding property for ys n stated in lemma to prove the lemma we need the following standard balls and bins analysis similar to lemma of dasgupta et al lemma for δh t t and s min i ui t let eh be the event that w max ui j t i h j i s where w t log t δh r t if t us n t log t δh then pr eh δh proof we will apply lemma to prove that the bound holds for fixed j t with failure probability δh t then apply a union bound let xi denote the random variable ui h i j i s we have that xi t e x i s ui t r t and v i s e xi i s ui t us n t applying lemma with l log t δh gives pr xi t log t δh r t exp log t δh δh t i when us n t lt or t us n lt we will use the following theorem from hanson and wright recall that tr b i n bi i for b rn n theorem hanson and wright let z rn be a vector of i i d random values for any symmetric b rn n and e z bz tr b c q where q max b f b and c is a universal constant lemma for w as in lemma suppose that the event eh holds then for unit vector y colspace a and any w with failure probability δ l e dys n ys n kl w log δ l where kl is an absolute constant proof we will use theorem to prove a bound on the th moment of dy for large note that dy can be written as z bz where z has entries from the diagonal of d and b rn n has bii yi yi h i h i here tr b ys n our analysis uses some ideas from the proofs for lemmas and of kane and nelson journal of the acm vol no article publication date january k l clarkson and d p woodruff since by assumption event eh of lemma occurs and for unit y colspace a ui for all i we have for j t that i h j i s w thus yi yi h i h i b i i s i s i h h i i s w i n w for b observe that for given j t z j r with z j i yi h i j i s is an eigenvector of b with eigenvalue z j and the set of such eigenvectors spans the column space of b it follows that w b max z j n j i h j i s putting this and equation into the q of theorem we have that q max b f b max w w w where we used w by a markov bound applied to z bz tr b with log δ l pr dys n ys n ec w e δ l handling vectors with large entries a small number of entries can be handled directly lemma for given s let eb denote the event that h i h i for all i i s then δ b pr e b t given event e b we have that for any y s s proof since pr h i h i t the probability that some such i i has h i h i is at most t the last claim follows by a union bound handling all vectors we have seen that d preserves the norms for vectors with small entries lemma and large entries lemma before proving a general bound we need to prove a bound on the cross terms lemma for w as in lemma suppose that the event eh and e b hold then for unit vector y colspace a with failure probability at most δc s d dys n kc w log δc for an absolute constant kc proof with the event e b for each i s there is at most one i s with h i h i let zi yi di i and zi otherwise we have the following for integer p using journal of the acm vol no article publication date january low rank approximation and regression in input sparsity time khintchine s inequality lemma e d dys n s p p p p e yi dii zi i s cp i s cp i s i h i i s c p w where c p o p and the last inequality uses the assumption that eh holds and i s yi putting p log δc and applying the markov inequality we have that pr s d dys n ec p w exp p δc therefore with failure probability at most δc we have that s d dys n kc w log δc for an absolute constant kc lemma suppose that the events eh and e b hold and w is as in lemma then for δ y there is an absolute constant ky such that if w ky log δ y then for unit vector y colspace a with failure probability δ y dy ε y when δ y proof assuming eh and e b we apply lemmas and and have with failure probability at most δ l δc dy y s s dys n ys n s d dys n dys n ys n 2y1 s d dys n kl w log δ l w log δc ky kl kc for the given w putting δ l δc δ y and assuming δ y thus ky kl kc suffices lemma suppose that δsub l is an r dimensional subspace of rn and b r rk is a linear map if for any fixed x l bx ε x with probability at least δsub then there is a constant ksub for which with probability at least r δsub ksub for all x l bx ε x n proof we will need the following standard lemmas for making a net argument let sr be the unit sphere in rr and let e be the set of points in sr defined by γ e w w zr w r where zr is the r dimensional integer lattice and γ is a parameter journal of the acm vol no article publication date january k l clarkson and d p woodruff fact lemma of arora et al e ecr for c fact lemma of arora et al for any r r matrix j if for every u v e ε we have that u jv ε then for every unit vector w we have that w jw γ let u rn r be such that the columns are orthonormal and the column space equals l let ir be the r r identity matrix define j u t bt bu ir consider the set e in fact and fact then for any x y e we have by the statement of the lemma that with probability at least 3δsub bu x ε u x bu y ε u y and bu x y ε u x y ε u x u y u x u y since u x and u y it follows that x jy ε by fact for γ and sufficiently large ksub we have by a union bound that with probability at least r δsub ksub x jy ε for every x y e thus with this probability by fact t w jw ε for every unit vector w which by definition of j means that for all y l by ε y the following is our main theorem in this section theorem there is t o r r such that with probability at least 10 d is a subspace embedding matrix for a that is for all y colspace a dy ε y the embedding d can be applied in o nnz a time for s min i ui t where t is a parameter in r log r it suffices if t max r t proof for suitable t t and s with failure probability at most δh δ b events eh and e b both hold conditioned on this and assuming that w is sufficiently small as in lemma we have with failure probability δ y for any fixed y colspace a that r dy ε y thus by lemma with failure probability δh δ b δ y ksub r dy y for all y colspace a we need δh δ b δ y ksub 10 and the parameter conditions of lemmas lemma and lemma holding listing these conditions r δh δ b δ y ksub 10 where δ b can be set to be t us t t us n log t δh t ln δ y w corresponding to the condition w of lemma since we set δ y δ l e w t log t δh r t ky log δ y r we put δ y ksub 30 δh 30 and require that t 30 for the last condition it suffices that t o r log t and t r the last condition implies the fourth condition for small enough constant ε also since us n i s i s ui t rt the bound for t implies that t o r log t suffices for condition thus when the leverage scores are such that s is small t can be o r log r since i ui r s r t suffices thus t o r t o r r suffices for the conditions of the theorem partitioning leverage scores in this section we further optimize the low order additive poly r term arising in the analysis in the previous section by refining the analysis for large leverage scores those larger than a threshold t in r log r we partition the scores into groups that are equal up to a constant factor and analyze the error resulting from the relatively journal of the acm vol no article publication date january low rank approximation and regression in input sparsity time small number of collisions that may occur also using the leverage scores to bound the error in what follows we have not optimized the poly log r ε factors let q t o log r ε where we assume that t is a power of we partition the leverage scores ui with ui t into groups g j j q where g j i j ui j n ui r and β j t we have for all j that let β j j and n j g j since i n j r β j r t we may also use g j to refer to the collection of rows of u with leverage scores in g j for given hash function h and corresponding let g j g j denote the collision indices of g j those i g j such that h i h i for some i g j let kj g j first we bound the spectral norm of a submatrix of the orthogonal basis u of colspace a where the submatrix comprises rows of g j the spectral norm consider a matrix b rn j r with b and each row of b has squared euclidean norm at least β j and at most j for some j q we want to bound the spectral norm of the matrix b whose rows comprise those rows of u in the collision set g j we let t r q7 be the number of hash buckets this can be chosen so that t r t n j nj the expected number of collisions in the t buckets is e g j t let d j be the event that the number kj g j of such collisions in the t buckets is at most t q let d j d j by a markov and a union bound pr d we will assume that d occurs while each row in b has some independent probability of participating in a collision we first analyze a sampling scheme with replacement we generate independent random matrices h m for m j for a parameter j kj by picking i n j uniformly at random and letting h m bi bi note that e h m b b nj lemma fix j q assume event d for kj there is j kj q so that with failure probability at most r r t n j t h m o q m j to prove this lemma we will use a special case of the version of matrix bernstein inequalities described by recht fact paraphrase of theorem recht let be an integer parameter for m let hm rr r be independent symmetric zero mean random matrices suppose journal of the acm vol no article publication date january k l clarkson and d p woodruff that ρm e hm hm and m maxm hm then for any τ τ τ log h log pr m m m ρm mτ proof of lemma we apply fact with j kj q and hm h m e h m so that e hm hm ρm b b b b bb b i i i n j i n j 2β j nj nj also m hm 2β j by equation nj n j applying fact with these bounds for ρm and m we have that τ log hm τ log pr m j m j ρm mτ log log τ j nj nj τ j τ o r t o q nj τ for n j t q setting if n j t q then kj by the assumption that event d holds τ r t gives a probability bound of r using n j t q we therefore have that with probability at least r j o r t b b h m n j m j o r t n j t where we use that b and use again j n j o n j t the lemma follows we can now prove the following lemma lemma with probability o for all leverage score groups g j and for u an orthonormal basis of colspace a the submatrix b j of u consisting of rows in g j that is those in g j that collide in a hash bucket with another row in g j under has squared spectral norm o r t n j t journal of the acm vol no article publication date january low rank approximation and regression in input sparsity time 17 proof with probability condition d holds condition on this event so that for j with n j t q the number of collisions kj is zero and the conclusion holds vacuously now consider a j q for which n j t q q since n j t from equation we have that j kj q t o q n j o q j when sampling with replacement the expected number of distinct items is j j nj j o e2 nj nj by a standard application of azuma s inequality using that j q is sufficiently large we have that the number of distinct items is at least j with probability at least r by a union bound with probability o for all j q using n j t q r at least j distinct items are sampled when sampling j items with replacement from g j since j kj o q it follows that at least kj distinct items are sampled from each g j by lemma for a fixed j q we have that the submatrix of u consisting of the j sampled rows in g j has squared spectral norm o r t n j t with probability at least r note that m j h m is the square of the spectral norm of the submatrix of u consisting of the j sampled rows from g j since the probability of this event is at least r for a fixed j q we can conclude that it holds for all j q simultaneously with probability o finally using that the spectral norm of a submatrix of a matrix is at most that of the matrix we have that for each j the squared spectral norm of a submatrix of kj random distinct rows among the j sampled rows of g j from u is at most o r t n j t within group errors let l j rn denote the set of vectors y so that yi for i not in the collision set g j and there is some unit y colspace a such that yi yi for i g j note that the error for such vectors is the same as that for the corresponding set of vectors with zeros outside of g j in this section we prove the following theorem theorem there is an absolute constant c for which for any parameters p and for sparse embedding dimension t o p r ε log r ε for all unit y colspace a j q sy j c with failure probability at most o log r where y j denotes the member of l j derived from y proof for y l j the error in estimating y by using y d dy contributed by collisions among coordinates yi for i g j is yi yi dii di i κj t t i i h t g j and we need a bound on this quantity that holds with high probability by a standard balls and bins analysis every bucket has o log t o q collisions with high probability since n j t from equation we assume this event the squared euclidean norm of the vector of all yi that appear in the summands that is with i g j is at most o r t n j t by lemma thus the squared journal of the acm vol no article publication date january k l clarkson and d p woodruff euclidean norm of the vector comprising all summands in equation is at most γj t t i i h t g j t t i h t g j o q β j o β j r t n j t o β j r t r t using equation by khintchine s inequality lemma for p p p e κj o p γ j o p β j r t r t therefore κ j is less than the last quantity with failure probability at most p putting p k j min r kj with failure probability at most kj for any fixed vector y l j the squared error in estimating y using the sketch of y is at most o k j β j r t r t assuming the event d from the earlier section we have that k j min r q q t we have that k j t t and r k j β j r t r t min β j r r t βjt using equation putting these bounds on the terms together the squared error is o r t or for t so that the error is o q since the dimension of l j is bounded by k j it follows from the net argument of lemma that for all y l j sy y o q thus the total error for unit y colspace a is o handling the cross terms to complete the optimization we must also handle the error due to cross terms let be an arbitrary parameter for j j q let the event e j j be n n that the number of bins containing both an item in g j and in g j is at most j let e j j e j j the event that no pair of groups has too many intergroup collisions lemma pr e proof fix a j j q then the expected number of bins containing an n nn n item in both g j and in g j is at most t tj tj j t j thus by a markov bound the n n number of bins containing an item in both g j and g j is at most j with probability at least the lemma follows by a union bound over the choices of j j in the remainder of the analysis we set t p r ε q7 for a parameter p q let f be the event that no bin contains more than cq elements of i g j where c is an absolute constant lemma pr f r journal of the acm vol no article publication date january low rank approximation and regression in input sparsity time 19 q q q proof observe that i g j i n j r i j by standard ballsand bins analysis with the given t with p with probability at least r no bin contains more than cq elements for a constant c lemma condition on events e and f occurring consider any unit vector y ax j in the column space of a consider any j j q define the vector y j yi yi for j i g j and yi otherwise then sy j sy j o proof since e occurs the number of bins containing both an item in g j and g j is at most n j n j call this set of bins s moreover since f occurs for each bin i s there are at most cq elements from g j in the bin and at most cq elements from g j in the bin thus for any s d we have using n j β j r for all j that sy j sy j n j n j cq cq β j β j tδ1 q2 the following is our main theorem concerning cross terms in this section theorem there is an absolute constant c for which for any parameters p and for sparse embedding dimension t o p r ε r the event y ax with y sy j sy j j j q occurs with failure probability at most r where y j y j are as defined in lemma proof the theorem follows at once by combining lemma lemma and lemma putting it together putting the bounds for within group and cross term errors together and replacing the use of lemma in the proof of theorem 10 we have the following theorem theorem there is an absolute constant c for which for any parameters p and for sparse embedding dimension t o p r ε r ε for all unit y colspace a sy c with failure probability at most o log r generalized sparse embedding matrices as discussed in the introduction we can use small jl transforms within each hash bucket to obtain the following theorem where the term in the runtime dependent on nnz a is more expensive but the quality bounds hold with high probability such generalized sparse embedding matrices s satisfy the following theorem theorem for given δ with probability at least δ for t o rε log r εδ r log εδ a generalized sparse embedding matrix s given in section is an embedding matrix for a that is for all y colspace a sy ε y s can be applied to a in o nnz a log r δ time in this section we introduce the version of jl transforms that we will use give our construction in section then as before analyze vectors with small entries with large entries and the cross terms we then conclude the section with the proof of theorem journal of the acm vol no article publication date january k l clarkson and d p woodruff johnson lindenstrauss transforms we start with a theorem of kane and nelson restated here in our notation we also present a simple corollary that we need concerning very low dimensional subspaces let ε a ε log r ε and v ε let b rn rva be defined as follows we view b meaning we stack the rows on top of each as the concatenation other of matrices a d1 a a da each i di being a linear map from rn to rv which is an independently chosen sparse embedding matrix of section with associated hash function hi n v theorem kane and nelson for any δ kn ε there are a ε log δ kn and v ε for which for any fixed x rn a randomly chosen b of the form above satisfies bx ε x with probability at least δ kn corollary let δ suppose that l is an o log r εδ dimensional subspace of rn let csubkn be any constant then for any ε there are a ε log r εδ and v ε such that with failure probability at most ε rδ csubkn by ε y for all y l proof we use theorem together with lemma for the latter we need that for any fixed y l by ε y with probability at least δsub by theorem we have this for δsub δε r c kn for an arbitrarily large constant c kn thus by lemma there is a constant ksub so that with probability at least ksub o log r εδ δε r c kn δε r csubkn for all y l by ε y here we use that c kn can be made arbitrarily large independent of ksub the construction we now define a generalized sparse embedding matrix s let a rn d with rank r let a ε log r εδ and v ε be such that theorem and corollary apply with parameters a and v for a sufficiently large constant csubkn further let q ct rε r log δε where ct is a sufficiently large absolute constant and let t avq let h n q q be a random hash function for i q define ai h i note that i ai n we choose independent matrices b b q with each b i as in theorem with parameters a and v here b i is a va ai matrix finally let p be an n n permutation matrix which when applied to a matrix a maps the rows of a in the set h to the set of rows maps the rows of a in the set h to the set of rows and for a general i q maps the set of rows of a in the set h i to the set of rows a2 ai a2 ai the map s is defined to be the product of a block diagonal matrix and the matrix p b b s p b q lemma s a can be computed in o nnz a log r εδ ε time proof as p is a permutation matrix p a can be computed in o nnz a time and has the same number of nonzero entries of a for each nonzero entry of p a we journal of the acm vol no article publication date january low rank approximation and regression in input sparsity time multiply it by b i for some i which takes o a o log r εδ ε time thus the total time to compute s a is o nnz a log r εδ ε analysis we adapt the analysis given for sparse embedding matrices to generalized sparse embedding matrices again let u rn r have columns that form an orthonormal basis for the column space colspace a let un be the rows of u and let ui ui for δ we set the parameter t o r ct q log t δ log r εδ r log εδ where ct is a sufficiently large absolute constant vectors with small entries let s min i ui t and for y colspace a of at most unit norm let y ys n since ui this implies that y t since p is a permutation matrix we have that py t in this case we can reduce the analysis to that of a sparse embedding matrix observe i i i that the matrix b i rva ai is the concatenation of matrices i d1 a da where i i each j d j rv ai is a sparse embedding matrix now fix a value j a and consider the block diagonal matrix n j rqv ai nj j dj p j dj q q j dj lemma n j is a random sparse embedding matrix with qv t a rows and n columns proof n j has a single nonzero entry in each column and the value of this nonzero entry is random in therefore it remains to show that the distribution of locations of the nonzero entries of n j is the same as that in a sparse embedding matrix this follows from the distribution of the values aq and the definition of p j lemma let δ for j a let eh be the event eh of lemma applied to matrix n j with δh δ a and w t log qv δh r qv ct q suppose j that j a eh holds this event has probability at least δ then there is an absolute constant kl such that with failure probability at most δ l sys n ys n kl w log a δ l proof we apply lemma with n j the sparse embedding matrix d and qv the number of rows of n j taking on the role of t in lemma so that the parameter w t log qv δh r qv as in the lemma statement since t avq qv δh t δ thus w r ct q r qv ct q since us n rt it suffices for lemma if qv is at least t log t δh q or v journal of the acm vol no article publication date january k l clarkson and d p woodruff j with δh δ a by a union bound j a eh occurs with failure probability δ as claimed we have for given n j that with failure probability δ l a n j ys n ys n kl w log a δ l applying a union bound and using n j ys n a a sys n j the result follows vectors with large entries again let s min i ui t since have that i ui r we s r t ct q log t δ the following is a standard nonweighted balls and bins analysis lemma suppose that the previously defined constant ct is sufficiently large let enw be the event that h i s ct log r εδ for all i q then pr enw δ r proof for any given i q e h i s s q ct log t δ o log r δ thus by a chernoff bound for a constant ct pr h i s ct log r εδ e log r εδ δ rq the lemma now follows by a union bound over all i q lemma assume that enw holds let es be the event that for all y colspace a s ε s then pr es δ r proof for i q let li be the at most ct log r εδ dimensional subspace that is the restriction of the column space colspace a to coordinates j with h j i and j s by corollary for any fixed i with probability at least δε r csubkn for all y li sy ε y by a union bound and sufficiently large csubkn this holds for all i q with probability at least q δε r csubkn δ r this condition implies es since s can be expressed as i q y i where each y i li and letting b i denote the va rows of s corresponding to entries from b i s b i y i i q ε y i i q ε s a rescaling to ε completes the proof cross terms now consider any unit vector y in colspace a and write it as s ys n we seek to bound s sys n for notational convenience define the journal of the acm vol no article publication date january low rank approximation and regression in input sparsity time block diagonal matrix n j to be the matrix j dj j dj n j q q j dj p then s a aj n j note that since the set of nonzero rows of n j and n j are disjoint for j j a n j s n j ys n s sys n a j a n j s n j ys n a 10 j where by lemma each n j is a sparse embedding matrix with qv t a rows and n columns j lemma for w as in lemma and assuming events aj eh enw and es there is absolute constant kc such that with failure probability δc s sys n kc w log a δc proof we generalize lemma slightly to bound each summand n j s n j ys n for a given j and for each i s let zm j yi di i i h j m i s journal of the acm vol no article publication date january k l clarkson and d p woodruff where h j is the hash function for j p we have for integer p using khintchine s inequality lemma p e n j s n j ys n p p p j e yi dii zh j i i s cp zh2 j i c p i s where v j m h j s zm i h j m i s c pw v j m h j s m and c p p p o p and the last inequality j eh holds putting p log a δc and applying the markov uses the assumption that inequality we have for all j a that pr n j s n j ys n ec p w v j a exp p δc moreover j a v j s which under es is at most ε s ε therefore with failure probability at most δc we have that s sys n kc w log a δc for an absolute constant kc putting it all together we are ready to prove theorem which we restate here for convenience theorem 10 restatement of theorem for given δ with probability at least δ for t o rε log r εδ r log εδ a generalized sparse embedding matrix s given in section is an embedding matrix for a that is for all y colspace a sy ε y s can be applied to a in o nnz a log r δ time proof of theorem note that t avq o ε log r εδ ε ct rε r log εδ j yielding the bound claimed from lemma 5 event j a eh occurs with failure probability at most δ from lemmas 5 and 5 the joint occurrence of enw and es holds with failure probability at most r δ given these events from lemmas 5 and 5 we have with failure probability at most δ l δc that sy y s y1 s sys n ys n s sys n ε y1 s kl w log a δ l w log a δc where w ct q r setting δc δ l δ ksub where ksub is from lemma and recalling that a o ε log r εδ we have that w log a δ l o r log εδ log a δ l ct ct q ct r log εδ journal of the acm vol no article publication date january low rank approximation and regression in input sparsity time 25 for absolute constant ct using lemma we have that with failure probability at r r most δ δ ksub ksub sy y ε ε2 ct kl ε for suitable choice of ct adjusting δ by a constant factor gives the result approximating leverage scores let a rn d with rank r let u rn r be an orthonormal basis for colspace a in drineas et al it was shown how to obtain a ε approximation ui to the leverage score ui for all i n for a constant ε in time o nd log n o log n log d in this section we improve the runtime of this task as follows we state the runtime for constant ε though for general ε the runtime would be o nnz a log n poly rε log n theorem for any constant ε there is an algorithm which with probability at least outputs a vector u u n so that for all i n ui ε ui the runtime is o nnz a log n r r r log n the success probability can be amplified by independent repetition and taking the coordinate wise median of the vectors u across the repetitions proof we first run the algorithm of theorem and theorem of cheung et al the first theorem gives an algorithm that outputs the rank r of a while the second theorem gives an algorithm that also outputs the indices ir of linearly independent columns of a the algorithm takes o nnz a log d o r time and succeeds with probability at least o log d thus in what follows we can assume that a has full rank we follow the same procedure as algorithm in drineas et al using our improved subspace embedding the proof of drineas et al proceeds by choosing a subspace embedding computing a then computing a change of basis matrix r so that ar has orthonormal columns the analysis there then shows that the row norms ar i are equal to ui ε to obtain these row norms quickly an r o log n johnson lindenstrauss matrix is sampled one first computes r followed by a r using a fast johnson lindenstrauss transform one can compute a in o nr log n time has o r log nlog r rows one can compute the r r matrix r in o r log n log r time by computing a qr factorization computing r can be done in o r log n time and computing a r can be done in o nnz a log n time our only change to this procedure is to use a different matrix which is the composition of our subspace embedding matrix s of theorem 5 with parameter t o r log r together with a fast johnson lindenstrauss transform f that is we set f s here f is an o r r t matrix see section of drineas et al for an instantiation of f then s a can be computed in o nnz a log r time by lemma 5 moreover f s a can be computed in o t r log r o r log2 r time one can then compute the matrix r above in o r 3 log2 r time by computing a qr factorization of f s a then one can compute r in o r log n time and computing a r can be done in o nnz a log n time thus the total time is o nnz a log n r 3 log2 r r log n time note that by theorem 5 with probability at least 5 sy ε y for all y colspace a and by lemma 3 of drineas et al with probability at least 10 f sy ε sy for all y colspace a therefore f s ax ε ax for all x rd with probability at least 10 there is also a small n probability journal of the acm vol no article publication date january k l clarkson and d p woodruff of failure that ar i ε ar i for some value of i thus the overall success probability is at least 3 the rest of the correctness proof is identical to the analysis in drineas et al least squares regression let a rn d and b rn be a matrix and vector for the regression problem minx ax b we assume that n d again let r be the rank of a we show that with probability at least 3 we can find an x for which ax b ε min ax b x we will give several different algorithms first we give an algorithm showing that the dependence on nnz a can be linear next we shift to the generalized case with multiple right hand sides and after some analytical preliminaries give an algorithm based on sampling using leverage scores finally we discuss affine embeddings constrained regression and iterative methods theorem the regression problem can be solved up to a ε factor with probability at least 3 in o nnz a o ε d ε time proof by theorem 3 10 applied to the column space colspace a b where a b is a adjoined with the vector b it suffices to compute da and db and output argminx dax db we use the fact that d r and apply theorem 4 with t o ε d ε the theorem implies that with probability at least 10 all vectors y in the space spanned by the columns of a and b have their norms preserved up to a ε factor note that da and db can be computed in o nnz a time now we have a regression problem with d o ε d ε rows and d columns using the fast johnson lindenstrauss transform this can be solved in o d d log d ε ε log d time see theorem 12 of sarlo s the success probability is at least 9 10 this is o ε log8 d ε time our remaining algorithms will be stated for generalized regression generalized regression and affine embeddings the regression problem can be slightly generalized to min ax b f x where x and b are matrices rather than vectors this problem also called multipleresponse regression is important in the analysis of our low rank approximation algorithms and is also of independent interest moreover while an analysis involving the embedding of a b is not significantly different than for an embedding involving a alone this is not true for a b different techniques must be considered this section provides the theorems needed for analyzing algorithms for generalized regression as well as a general result for affine embeddings preliminaries we collect a few standard lemmas and facts in this section there is a form of sketching matrix that relies on sampling based on leverage scores it will be convenient to define it using sampling with replacement for given sketching dimension t for m t let s rt n have sm zm tpzm where pi ui and zm i with probability pi journal of the acm vol no article publication date january low rank approximation and regression in input sparsity time 27 the following fact is due to rudelson but has since seen many proofs and follows readily from noncommutative bernstein inequalities recht which are very similar to matrix bernstein inequalities zouzias fact leverage score embeddings for rank r a rn d with row leverage scores ui there is t o rε log r such that leverage score sketching matrix s rt n is an embedding matrix for a lemma 3 approximate matrix multiplication for a and b matrices with n rows where ahas n columns and given there is t so that for a t n generalized sparse embedding matrix s or t n fast jl matrix or t log nd n subsampled randomized hadamard matrix or leverage score sketching matrix for a under the condition that a has orthonormal columns pr a s sb a b f a b δ for any fixed δ proof for a generalized sparse embedding matrix as in section 5 with parameter v first suppose that v so that s is the embedding matrix of section 5 let x a s sb ab then xi j a i s sb j a i b j where a i is the ith column of a and b j is the jth column of b thorup and zhang have shown that e xi j and var xi j o t a i b j consequently e xi j var xi j o t a i b j from which for an appropriate t the lemma follows by chebyshev s inequality for v xi j vt i t v x i j see equation 10 so that var xi j v var x i j a i b j a i b j t i t t and similarly the lemma follows for the sparse embedding matrices the result for fast jl matrices was shown by sarlo s and for subsampled hadamard by drineas et al proof of lemma 5 the claim also follows from norm preserving properties of these transforms see kane and nelson for leverage score sampling first note that zm i a s sb a b ai bi t pi i n m t we have that e a s sb a b and using the independence of the zm the second moment of a s sb a b f is the expectation of tr a s sb a b a s sb a b tr bi ai ai bi i i n m t which is zm i zm i pi pi tr bi ai ai bi b aa b pi m t i n journal of the acm vol no article publication date january k l clarkson and d p woodruff or using the cyclic property of the trace the fact that pi ai a and the fact that tr b aa b a b f a b ai bi tr b aa b a b t pi t i n thus the lemma follows for large enough t in o ε by chebyshev s inequality fact 4 given n d matrix a of rank k γ for γ and an m n fast jl matrix with m k is a subspace embedding for a with failure probability at most δ for any fixed δ and requires o nd log n time to apply to a a similar fact holds for subsampled hadamard transforms fact 5 pythagorean theorem if c and d matrices with the same number of rows and columns then c d implies that c d c d fact normal equations given n d matrix c and n d matrix d consider the problem min c x d x rd d the solution to this problem is x c d where c is the moore penrose inverse of c definition moreover c c x d thus if c is any vector in the column space of c then c c x d using fact 5 for any x c x d c x x c x d 3 generalized regression conditions the main theorem in this section is the following it could be regarded as a generalization of lemma of drineas et al theorem suppose that a and b are matrices with n rows and a has rank at most r suppose that s is a t n matrix and the event occurs that s satisfies lemma 3 with error parameter r and that s is a subspace embedding for a with error parameter then if y is the solution to min s ay b min ay b 12 y and y is the solution to y then ay b f ay b f before proving theorem we will need the following lemma lemma for s a b y and y as in theorem assume that a has orthonormal columns then a y y f ε b ay f proof the proof is in the appendix proof of theorem let a have the thin svd a u v since u is a basis for colspace a there are x and x so that s u x b s ay b and u x b ay b journal of the acm vol no article publication date january low rank approximation and regression in input sparsity time 29 therefore u x b f ε u x b f implies the theorem we can assume without loss of generality that a has orthonormal columns with this assumption and using the pythagorean theorem fact 5 with the normal equations fact then lemma ay b f ay b a y y f ay b ay b 4ε ay b and taking square roots and adjusting ε by a constant factor completes the proof 4 generalized regression algorithm our main algorithm for regression is given in the proof of the following theorem theorem 9 given a rn d of rank r and b rn d the regression problem miny ay b f can be solved up to ε relative error with probability at least 3 in time o nnz a log n r rε rd r log2 r d ε log n and obtaining a coreset of size o r ε log r proof we estimate the leverage scores of a to relative error using the algorithm of theorem which has the side effect of finding r independent columns of a so that we can assume that d r if u is a basis for colspace a then for any x there is a y so that u x ay and vice versa so that conditions satisfied by u x are satisfied by ay that is we can and will hereafter assume that a has r orthonormal columns when considering products ay we construct a leverage score sketching matrix s for a with t o r ε r log r so that lemma 3 is satisfied for error parameter at most ε r with this t s will also be an ε embedding matrix with ε using lemma these conditions and theorem imply that the solution y to miny s ay b f has ay b f ε min ay b f y the runtime is that for computing the leverage scores plus the time needed for finding y which can be done by computing a qr factorization of s aand then computing r q sb which requires r 3 ε log r r ε log r d r 3 d and the cost bound follows 5 affine embeddings we also use affine embeddings for which a stronger condition than theorem is satisfied theorem 10 suppose that aand b are matrices with n rows and ahas rank at most r suppose that s is a t n matrix and the event occurs that s satisfies lemma 3 with error parameter ε r and that s is a subspace embedding for a with error parameter ε let x be the solution of min x ax b f and b ax b for all x of appropriate shape s ax b s b f ax b b f journal of the acm vol no article publication date january 30 k l clarkson and d p woodruff for ε thus s is an affine embedding with relative error up to an additive constant i e a weak embedding if also s b f ε b f then s ax b ax b and s is a affine embedding note that even when only the weaker first statement holds the sketch still can be used for optimization since adding a constant to the objective function of an optimization does not change the solution proof if u is a basis for colspace a then for any x there is a y so that u x ay and vice versa so that conditions satisfied by u x are satisfied by ay we can and will hereafter assume that a has r orthonormal columns using the fact that w tr w w for any w the embedding property the fact that a f r and the matrix product approximation condition of lemma 3 s ax b s b f s a x x s ax b s b f s a x x tr x x a s s b a x x ε a x x x x f b f the normal equations fact imply that ax b a x x b f and using the observation that a b a2 b2 for a b r s ax b s b f ax b b f ε a x x x x f b f ε a x x f b f a x x b f ax b and the first statement of the theorem follows when s b f ε b f the second statement follows since then s ax b 2ε ax b ε b f 3ε ax b using b f ax b f for all x to apply this theorem to sparse embeddings we will need the following lemma lemma let a be an n d matrix let s rt n be a randomly chosen sparse embedding matrix for an appropriate t ε then with probability at least 9 10 s a ε a proof see the appendix lemma 12 let a be an n d matrix let s rt n be a sampled randomized hadamard transform srht matrix for an appropriate t ε log n then with probability at least 9 10 s a ε a journal of the acm vol no article publication date january low rank approximation and regression in input sparsity time 31 proof see the appendix theorem 13 let a and b be matrices with n rows and a has rank at most r the following conditions hold with fixed nonzero probability if s is a t n srht matrix there is t o ε 2 log2 n log r r log n 2 such that s is an ε affine embedding for a and b if s is a t n sparse embedding there is t o ε 2 log7 r ε such that s is an ε affine embedding if s is a t n leverage score sampling matrix there is t o ε log r such that s is a weak ε affine embedding if the row norms of b are available a modified leverage score sampler is an ε embedding here b is as in theorem 10 note that none of the dimensions t depend on the number of columns of b proof to apply theorem 10 we need each given sketching matrix to satisfy conditions on multiplicative error subspace embedding and preservation of b f as in that theorem we can assume without loss of generality that a has r orthonormal columns regarding the multiplicative error bound of r lemma 3 tells us that srht achieves this bound for t o log n 2 ε and the other two need t o ε 2 r regarding subspace embedding as noted in the introduction an srht matrix achieves this for t o ε 2 log r r log n 2 a sparse embedding requires t o ε 2 log7 r ε as in theorem 4 9 and leverage score samplers need t o ε log r as mentioned in fact 2 regarding preservation of the norm of b lemma 12 gives the claim for srht matrices and lemma 11 gives the claim for sparse embeddings where the a of those lemmas is b thus the conditions are satisfied for theorem 10 to yield the claims for srht and for sparse embeddings and for the weak condition for leverage score samplers we give only a terse version of the argument for the last statement of the theorem 2 when the squared row norms bi b i f of b are available a sampler that picks row 2 that row with tpi will yield i with probability pi min tbi b f and scales a matrix whose frobenius norm will be t b f with high probability if the leverage score sampler picks rows with probability qi create a new sampler that picks rows with probability pi pi qi 2 and scales by tpi the resulting sampler will satisfy the norm preserving property for b and also satisfy the same properties as the leverage score sampler up to a constant factor the resulting sampler is thus an o ε affine embedding affine embeddings and constrained regression from condition 13 an affine embedding can be used to reduce the work needed to achieve small error in regression problems even when there are constraints on x we consider the constraint x that the entries of x are nonnegative the problem min x ax b for b rn n and a rn d arises among other places as a subroutine in finding a nonnegative approximate factorization of b for an affine embedding s min s ax b ε min ax b x x yielding an immediate reduction yielding a solution with relative error ε just solve the sketched version of the problem journal of the acm vol no article publication date january 32 k l clarkson and d p woodruff from theorem 13 suitable sketching matrices for constrained regression include a sparse embedding an srht matrix or a leverage score sampler the last may not need the condition of preserving the norm of b if a high accuracy solver is used for the sketched solution or if otherwise the additive constant is not an obstacle for that solver since it is immediate that affine embeddings can be composed to obtain an affine embedding with a constant factor loss the most efficient approach might be to use a sketch that first applies a sparse embedding and then applies an srht matrix resulting in a sketched problem with o ε log r ε 2 rows and where computing the sketch takes o nnz a nnz b o ε 2 d d time for b rn d when r is unknown the upper bound r d can be used for low rank approximation discussed in section we require x to satisfy a rank condition the same techniques apply iterative methods for regression a classical approach to finding min x ax b f is to solve the normal equations fact 6 a ax a b via gaussian elimination for a rn r and b rn d this re quires o min r nnz b d nnz a to form a b o r nnz a to form a a and o r 3 r 2 d to solve the resulting linear systems another method is to factor a qw where q has orthonormal columns and w is upper triangulation this typically trades a slowdown for a higher quality solution another approach to regression is to apply an iterative method from the general class of krylov cg like methods to a preconditioned version of the problem in such methods an estimate x m of a solution is maintained for iterations m using data obtained from previous iterations the convergence of these methods depends on the condition number κ a a supx x ax 2 inf x x ax 2 from the input matrix a classical result luenberger and ye via meng et al or theorem 10 2 6 golub and van loan is that m 2 a x m x κ a a 2 2 κ a a a x x thus the runtime of cg like methods such as cgnr golub and van loan depends on the unknown condition number the runtime per iteration is the time needed to compute matrix vector products ax and a v plus o n d for vector arithmetic or o nnz a preconditioning reduces the number of iterations needed for a given accuracy suppose that for nonsingular matrix r the condition number κ r a ar is small then a cg like method applied to ar would converge quickly moreover for iterate y m that has error α m ary m b small the corresponding x ry m would have ax b α m the runtime per iteration would have an additional o for computing products involving r consider the matrix r obtained for leverage score approximation in section 6 where a subspace embedding matrix is applied to a and r is computed so that ar has orthonormal columns since is a subspace embedding matrix to constant accuracy for all unit x rd arx 2 arx 2 2 it follows that the condition number κ r a ar 2 2 journal of the acm vol no 6 article publication date january low rank approximation and regression in input sparsity time 33 that is ar is very well conditioned plugging this bound into equation after m 2 iterations ar x m x is at most times its starting value thus starting with a solution x with relative error at most and applying log ε iterations of a cg like method with e the relative error is reduced to ε and the work is o nnz a r 2 log ε where we assume that d has been reduced to r as in the leverage computation plus the work to find r we have the following theorem theorem 14 the 2 regression problem can be solved up to a ε factor with probability at least 2 3 in o nnz a log n ε r 3 log2 r r 2 log ε time note that only the matrix r from the leverage score computation is needed not the leverage scores thus the nnz a term in the runtime need not have a log n factor however since reducing a to r columns requires that factor the resulting runtime without that factor is o nnz a log ε log2 d log ε and depends on d the matrix ar is so well conditioned that a simple iterative improvement scheme has the same runtime up to a constant factor again start with a solution x 0 with relative error at most and for m 0 let x m x m r a b arx m then using the normal equations ar x m x ar x m r a b arx m x ar arr a ar x m x u 3 v x m x where ar u v is the svd of ar for all unit x rd arx 2 ε0 2 thus we have that all singular values σi of ar are ε0 and the diagonal entries of 3 are all at most σi 0 2 σi 3 0 for 0 thus ar x m x ar x m x and by choosing 0 2 say o log ε iterations suffice for this scheme also to attain ε relative error this scheme can be readily extended to generalized multiple response regression using the iteration x m x m r a b arx m the initialization cost then includes that of computing a b which is o min r nnz b d nnz a where again b n d r the product a a used implicitly per iteration could be computed in o r nnz a and then applied per iteration in time d r 2 or applied each iteration in time d nnz a that is this method is never much worse than cg like methods but comparable in runtime when d r when d r it is a little worse in asymptotic runtime than solving the normal equations low rank approximation this section gives algorithms for low rank approximation understood using generalized regression analysis as in earlier work suchas sarlo s and clarkson and woodruff let k a a k f where a k denotes the best rank k approximation to a we seek low rank matrices whose distance to a is within ε of k we give an algorithm for finding such matrices and prove the following theorem journal of the acm vol no 6 article publication date january 34 k l clarkson and d p woodruff theorem for a rn n there is an algorithm that with failure probability 10 finds matrices l w rn k with orthonormal columns and diagonal d rk k so that a ldw f ε k the algorithm runs in time o nnz a o ε 4 ε 5 we will apply embedding matrices composed of products of such matrices thus we need to check that this operation preserves the properties that we need fact 8 2 if s rt n approximates matrix products and is a subspace embedding with error and failure probability δ s and rt t approximates matrix products with error and failure probability δ then s approximates matrix products with error o and failure probability at most δ s δ proof this follows from two applications of lemma 3 together with the observation that s ax ax for basis vectors x implies that s a f a f fact 8 3 if s rt n is a subspace embedding with error and failure probability δ s and rt t is a subspace embedding with error and failure probability δ then s is a subspace embedding with error o and failure probability at most δ s δ the following lemma implies a regression algorithm that is linear in nnz a but has a worse dependence in its additive term lemma 8 4 let a rn d of rank r b rn d and c d d for r rt n a sparse embedding matrix rt t a sampled randomized hadamard matrix there is t o r 2 log7 r rε and t o rε log r ε such that for r r x argmin x r ax b f has ax b f ε min x ax b f the operator r can be applied in o nnz a nnz b tc log t time we are now ready to describe the algorithm promised by theorem 8 1 compute ar and an orthonormal basis u for colspace ar where r is as in lemma 8 4 with r k 2 compute su and s a for s the product of a v v srht matrix with a v n sparse embedding where v ε 4 k2 log7 k ε and v ε 3 k log2 k ε instead of this affine embedding construction an alternative might use leverage score sampling where even the weaker claim of theorem 13 would be enough v 3 compute the svd of su u u s a k where again z k denotes the best 4 compute the svd u dw of v rank k approximation to matrix z 5 return l u u d and w proof of theorem 8 1 runtime computing ar in the first step takes o nnz a o nk k ε 1 time then o n k ε 2 to compute the n o ε 1 k log k ε matrix u computing su and s a requires o nnz a o ε 4 time computing the svd of the o kε 3 o kε 1 matrix su requires o k3 ε 5 computing u s a requires o ε 4 time computing the svd of the o kε 1 n matrix of the next step requires o ε 2 time as does computing u u correctness apply lemma 8 4 with a of that lemma mapping to a k and b mapping to a taking transposes this implies that with small fixed failure probability y ar ak r has y ak a f 1 min y ak a f 1 k y journal of the acm vol no 6 article publication date january low rank approximation and regression in input sparsity time 35 thus min x rank x k ar x a f ar ak r ak a f 1 k since u is a basis for colspace ar 1 ε min x rank x k u x a f 1 ε min x rank x k ar x a f with the given construction of s theorem 7 13 applies twice with ar taking the role of a and a taking the role of b so that s is an ε affine embedding after adjusting constants it follows that for x argmin x rank x k s u x a f u x a f 1 ε 1 ε min u x a f min ar x a f x rank x k x rank x k 1 ε 2 k using equation from lemma 4 3 of clarkson and woodruff the solution to min x rank x k u x s a f is x u s a k where this denotes the best rank k approximation to u s a it follows x is a solution to min x rank x k s u x a f moreover the rank k that x v matrix u x ldw has ldw a f 1 ε 2 k and l d and w have the properties promised 9 p regression for any 1 p let a rn d and b rn be a matrix and vector for the regression problem minx ax b p we assume that n d let r be the rank of a we show that with probability at least 2 3 we can quickly find an x for which ax b p 1 ε min ax b p x here p is any constant in 1 this theorem is an immediate corollary of theorem 5 1 and the construction given in section 3 2 of clarkson et al which shows how to solve p regression given a subspace embedding for 2 as a black box we review the construction of clarkson et al later for completeness a key concept for these methods is that of the α β p well conditioned basis for colspace a for p 2 an orthonormal basis is well conditioned the definition uses the entrywise norm which for a is a i j ai j p 1 p definition 9 1 well conditioned basis for the p norm an n d matrix u is an α β p well conditioned basis for the column space of a if using m x x p 1 u α and 2 for all x rd x q β u x p where 1 p 1 q 1 for ease of notation we will just say that u is a well conditioned basis for a if α β do p where p is understood from context as in the proof of theorem 6 1 in o nnz a log d o r 3 time we can replace the input matrix a with a new matrix with the same column space of a and full column rank where r is rank of a we therefore assume that a has full rank in what follows journal of the acm vol no 6 article publication date january 36 k l clarkson and d p woodruff let w r 6 log n r log n and assume that w n split a into n w matrices an w each w r so that ai is the submatrix of a indexed by the ith block of w rows we invoke theorem 5 1 with the parameters n w r ε 1 2 and δ 1 100n choosing a generalized sparse embedding matrix s with t o r log n r log n rows theorem 5 1 has the guarantee that for each fixed i s ai is a subspace embedding with probability at least 1 δ it follows by a union bound that with probability at least 1 1 for all i n w s ai is a subspace embedding we condition on this event occurring consider the matrix f rnt w n which is a block diagonal matrix comprising n w blocks along the diagonal each block is the t w matrix s given earlier s s f s we will need the following theorem theorem 9 2 theorem 5 of dasgupta et al restated let a be an n r matrix and let p 1 then there exists an α β p well conditioned basis for the column space of a such that if p 2 then α r 1 2 1 p and β 1 if p 2 then α r 1 2 and β 1 and if p 2 then α r 1 2 1 p and β r 1 2 1 p an r r change of basis matrix u for which a u is a well conditioned basis can be computed in o nr 5 log n time we use the following algorithm condition a given a matrix a rn r 1 compute f a 2 apply theorem 9 2 to f a to obtain an r r change of basis matrix u so that fau is an α β p well conditioned basis of the column space of matrix f a 3 output au rγ p where γ p p 1 2 for p 2 and γ p 1 2 1 p for p 2 the following lemma is the analog of that in clarkson et al proved for the fast johnson lindenstauss transform however the proof in clarkson et al only used that the fast johnson lindenstrauss transform is a subspace embedding we state it here with our new parameters and give the analogous proof in the appendix for completeness lemma 9 3 with probability at least 1 1 the output au rγ p of condition a is guaranteed to be a basis that is α β tw 1 p 1 2 p well conditioned that is an α β poly max r log n p well conditioned basis the time to compute u is o nnz a log n poly rε 1 the following text is from clarkson et al we state it here for completeness a well conditioned basis can be used to solve p regression problems via an algorithm based on sampling the rows of a with probabilities proportional to the norms of the rows of the corresponding well conditioned basis this entails using for speed a second projection 2 applied to au on the right to estimate the row norms where 2 can be an o r o log n matrix of i i d normal random variables which is the same as is done in drineas et al this allows fast estimation of the 2 norms of the rows of au however we need the p norms of those rows which we thus know up to a factor of r 1 2 1 p we use these norm estimates in the sampling algorithm of dasgupta et al as discussed for the runtime bound of that article theorem 7 this algorithm samples a number of rows proportional to r αβ p when an journal of the acm vol no 6 article publication date january low rank approximation and regression in input sparsity time 37 fig 1 a 1 pareto curve of error as a function of the size of r α β p well conditioned basis is available this factor together with a sample complexity increase of r p 1 2 1 p r p 2 1 needed to compensate for error due to using 2 gives a sample complexity increase for our algorithm over that of dasgupta et al of a factor of r p 2 1 r p 1 tw p 2 1 max r log n o p while the leading term in the complexity for n r is reduced from o nr 5 log n to o nnz a log n observe that if r log n then poly rε 1 log n is less than n log n which is o nnz a log n thus the overall time complexity is o nnz a log n poly rε 1 we adjust theorem 4 1 of dasgupta et al and obtain the following theorem 9 4 given 0 1 a constant p 1 a rn d and b rn there is a sampling algorithm for p regression that constructs a coreset specified by a diagonal sampling matrix d and a solution vector x rd that minimizes the weighted regression objective d ax b p the solution x satisfies with probability at least 1 2 the relative error bound that ax b p 1 ax b p for all x rd further with probability 1 o 1 the entire algorithm to construct x runs in time o nnz a log n poly rε 1 10 preliminary experiments some preliminary experiments show that a low rank approximation technique that is a simplified version of these algorithms is promising and in practice may perform much better than the general bounds of our results journal of the acm vol no 6 article publication date january 38 k l clarkson and d p woodruff fig 2 a 1 pareto curve of cond su 1 as a function of the size of s relative to r here we apply the algorithm of theorem 8 1 except that we skip the randomized hadamard and simply use a sparse embedding r and leverage score sampling we compare the frobenius error of the resulting ldw with that of the best rank k approximation in our experiments the matrices tested are n d the resulting low rank approximation was tested for tr the number of columns of r taking values of the form 1 6z 0 5 for integer z 1 while tr d 5 the number ts of rows of s was chosen such that the condition number of su was at most 1 2 since u has orthogonal columns its condition number is 1 thus a large enough leverage score sample will have this property for such tr and ts we took the ratio re of the frobenius norm of the error to the frobenius norm of the error of the best rank k approximation where k took values of the form 1 6 j for integers j 0 with k tr 2 the resulting points k tr re 1 were generated for all test matrices for three independent trials resulting in a set of points p the test matrices are from the university of florida sparse matrix collection essentially most of those with at most nonzero entries and with n up to about there were matrices tested from subcollections of matrices each such subcollection representing a particular application area the curve in figure 1 represents the results of these tests in which for a particular point x y on the curve at most one percent of points t kr re 1 p gave a result where k tr x but re 1 y figure 2 shows a similar curve for the points tr ts cond su 1 thus the necessary ratio tr ts so that cond su 1 2 as for the results in figure 1 need be no smaller than about 1 110 journal of the acm vol no 6 article publication date january the lemma now follows by chebyshev s inequality for appropriate t ε 2 proof of lemma 7 12 lemma of boutsidis and gittens shows that s a f 1 ε a f with arbitrarily low failure probability the other direction follows from a similar argument briefly the expectation of s a is a by construction and lemma 11 of boutsidis and gittens implies that with arbitrarily small failure probability all rows of s a will have a squared norm at most β αt a where α is a value in o log n assuming that this bound holds it follows from hoeffding s inequality that the probability that s a a ε a is at most 2 exp 2 ε a 2 tβ 2 or 2 exp t α 2 so that t ε 2 log n 2 suffices to make the failure probability at most 1 10 proof of lemma 9 3 this is almost exactly the same as in clarkson et al we simply adjust notation and parameters applying theorem 5 1 we have that with probability at least 1 1 for all x rr if we consider y ax and write p well conditioned basis the time to comthus au rγ p is an α β tw pute f a is o nnz a log n by theorem 5 1 note that f a is an nt w n matrix which is o n r 5 r thus the time to compute u from f a is o n r 5 r 5 log n o nnz a log n since nnz a n 