chapter the editing pane introduction figure overview of the editing pane shows the argouml window with the editing pane high lighted figure overview of the editing pane this is where all the diagrams are drawn in earlier versions of argouml this pane went under a variety of names you may encounter drawing pane diagram pane or multi editor pane in other docu mentation that is still being updated the pane has a tool bar at the top and a single tab labeled as diagram at the bottom which has no function in the version of argouml the main area shows the currently selected diagram of which the name is shown in the window title bar mouse behavior in the editing pane behavior of the mouse in general and the naming of the buttons is covered in the chapter on the overall user interface see chapter introduction button click in the tool bar of the editing pane button click is used to select a tool for creating a new model ele ment and adding it to the diagram see double clicking for creating multiple model elements for most tools adding a new model element to the diagram is achieved by moving the mouse into the editing area and clicking again in the main editing area button click is used to select an individual model element many model elements e g actor class show special handles when selected and the mouse hovers over them these are called selection action buttons see section selection action buttons they appear at the sides top and bottom and indicate a relationship type clicking on a selection action but ton creates a new related model element with the relation of the type that was indicated if the shift key is pressed when hovering the mouse over a selected model element sometimes different handles are shown which stand for different relation types where button click has been used to bring up a context sensitive pop up menu see below button click is used to select the menu entry required the pop up menu will be removed by any button click outside of the menu area there are various more detailed effects which are discussed under the descriptions of the various tools see section the tool bar button double click when used on the tool bar with a tool to add a model element the selected model element will be added multiple times to the drawing area once for each further button click until the tool is again selected or another tool chosen when used within the drawing area on a model element that has sub components double click will se lect the sub component for editing creating it if necessary for example double clicking over an operation compartment of a class will select the operation or cre ate one if there is none yet a special use is with package model elements on the class diagram a double click on a package will navigate to the class diagram associated with a package the first created if there is more than one or will offer to create one for you if there is none see figure the dialog for adding a new class dia gram figure the dialog for adding a new class diagram button motion where the model element being added is some form of connector its termination point is shown with button up over the terminating model element button click may be used in the space between model elements to create articulation points in the connector this is particularly useful where connectors must loopback on themselves over graphical model elements button motion will move the model element to a new position graphical model elements that are selected show handles at the corners or ends and these can be used for re sizing some model elements e g actor class show special handles called selection action buttons see section selection action buttons at the sides top and bottom which can be dragged to form types of relationship with other model elements where the model element is some form of connector between other items button motion other than at a handle will cause a new handle to be created allowing the connector to be articulated at that point this only works when the connecting line is not straight angled such new handles can be removed by moving them to the end of the connector there are various more detailed effects which are discussed under the descriptions of the various tools see section the tool bar shift and ctrl modifiers with button where multiple selections are to be made the ctrl key or shift key is used with button to add un selected model elements to the current selection where a model element is already selected it is re moved from the current selection clicking button while the alt key or the alt gr key is pressed invokes the broom tool which causes the selected model elements and any others swept up with them to be moved with the broom tool see section layout tools button actions when used over model elements in the the editing pane this will display a context dependent pop up menu menu entries are highlighted but not selected and sub menus exposed by subsequent mouse mo tion without any buttons menu entry selection is with button or button see section pop up menus for details of the specific pop up menus in case multiple elements are selected the pop up menu only appears if all the items are of the same kind in this case the functions apply to all selected elements button double click this has no effect other than that of button single click button motion this is used to select items in a context sensitive menu popped up by use of button click middle button motion pressing the middle mouse button down anywhere in the diagram allows to scroll the canvas in all dir ections with middle button motion keyboard behavior in the editing pane many keyboard shortcuts can be used when the editing pane is active mainly to change the selection or to move across the model elements nudging a model element you can nudge a fig by selecting an element and using arrow keys keeping the shift or the alt key pressed will produce a wider movement moving across the model elements you can select the model element nearer to the selected one by using the arrow keys while clicking the right button of the mouse you can also select the next model element using the tab key or the previous using the ctrl tab keys the tool bar the toolbar at the top of the editing pane provides the main functions of the pane the default tool is the select tool in general button click on any tool selects a tool for one use before reverting to the default tool and button double click selects a tool for repeated use the tools fall into four categories layout tools provide assistance in laying out model elements on the diagram annotation tools used to annotate model elements on the diagram drawing tools used to add general graphic objects to diagrams diagram specific tools used to add uml model elements specific to a particular diagram type to the diagram some of the tools that are generally not all used so often are combined in a dropdown to take less space on the toolbar see e g figure the drawing tools selector press the symbol at the right of the tool to pop it open these drop down tools remember their last used tool persistently this means that when argouml starts they show the last tool that was activated the previous time argouml was run layout tools the following two tools are provided in all diagrams in this category select this tool provides for general selection of model elements on the diagram button click will select a model element ctrl with button can be used to select or deselect multiple model elements button motion will move selected items or add and move a new control point on a link button motion on a selected component control point will stretch that component shape broom button motion with this tool provide a broom which will sweep all model elements along this is a very shortcut way of lining things up the broom can also be invoked by using alt or alt gr with button motion when the select tool is in use the broom is discussed at length in its own chapter see section the broom tip additional control of model element layout is provided through the arrange menu see section the arrange menu annotation tools the annotation tool comment is used to add a comment to a selected uml model element caution unlike most other tools you use the select tool to select a model element and then but ton click on comment to create the comment if no element is selected when the com ment tool is clicked then the comment is created and put at the left top corner the comment is created alongside the selected model element empty by default the text can be selec ted with button double click and edited from the keyboard the uml standard allows comments to be attached to any model element you can link any comment to aditional elements using the commentlink tool drawing tools these are a series of tools for providing graphical additions to diagrams although they are not uml model elements the uml standard provides for such decoration to improve the readability of diagrams tip these drawing tools provide a useful way to partially support some of the uml features such as general purpose notes that are missing from the current release of argouml eight tools are provided all grouped into one drop down widget see figure the drawing tools selector button click on the diagram will place an instance of the graphical item of the same size as the last one placed the size can be controlled by button motion during placement one side or end of the element will be at button down the other side or end at button up in general after they are placed on the diagram graphical elements can be dragged with the select tool and button and re sized by button motion on the handles after they have been selected figure the drawing tools selector rectangle provides a rectangle rounded rectangle provides a rectangle with rounded corners there is no control over the degree of rounding circle provides a circle line provides a line text provides a text box the text is entered by selecting the box and typing text is centered horizontally and after typing the box will shrink to the size of the text however it can be re sized by dragging on the corners polygon provides a polygon the points of the polygon are selected by button click and the polygon closed with button double click which will link the final point to the first point spline provide an open spline the control points of the spline are selected with button and the last point selected with button double click ink provide a polyline the points are provided by button motion use case diagram specific tools several tools are provided specific to uml model elements on use case diagrams the detailed proper ties of these model elements are described in the section on use case diagram model elements see chapter use case diagram model element reference actor add an actor to the diagram for convenience when the mouse is over a selected actor it displays two handles to left and right which may be dragged to form association relationships use case add a use case to the diagram for convenience when the mouse is over a selec ted use case it displays two handles to left and right which may be dragged to form association rela tionships and two handles top and bottom which may be dragged to form generalization and special ization relationships respectively association add an association between two model elements selected using button mo tion from the first model element to the second there are types of association offered here see figure the association tool selector association aggregation and composi tion and all these three can be bidirectional or unidirectional figure the association tool selector dependency add a dependency between two model elements selected using button motion from the dependent model element generalization add a generalization between two model elements selected using button motion from the child to the parent extend add an extend relationship between two model elements selected using button mo tion from the extended to the extending use case include add an include relationship between two model elements selected using button motion from the including to the included use case add extension point add an extension point to a selected use case the extension point is given the default name newep and location loc where the extension point compartment is dis played the extension point may be edited by button double click and using the keyboard or by se lecting with button click after the use case has been selected and using the property tab other wise it may be edited through its property tab selected through the property tab of the owning use case note this tool is grayed out except when a use case is selected class diagram specific tools several tools are provided specific to uml model elements on class diagrams the detailed properties of these model elements are described in the section on class diagram model elements see chapter class diagram model element reference package add a package to the diagram class add a class to the diagram for convenience when the mouse is over a selected class it displays two handles to left and right which may be clicked or dragged to form association relation ships or composition in case shift has been pressed and two handles top and bottom which may be dragged or clicked to form generalization and specialization relationships respectively association add an association between two model elements selected using button mo tion from the first model element to the second there are types of association offered here bi directional or unidirectional aggregation add an aggregation between two model elements selected using button mo tion from the first model element to the second there are types of aggregation offered here bi directional or unidirectional composition add an composition between two model elements selected using button mo tion from the first model element to the second there are types of composition offered here bidirectional or unidirectional association end add another end to an already existing association using button from the association middle to a class or vice versa this is the way to create so calld n ary associations generalization add a generalization between two model elements selected using button from the child to the parent interface add an interface to the diagram for convenience when the mouse is over a se lected interface it displays a handle at the bottom which may be dragged to form a realization rela tionship the target being the realizing class realization add a realization between a class and an interface selected using button mo tion from the realizing class to the realized interface dependency add a dependency between two model elements selected using button motion from the dependent model element there are also special types of dependency offered here permission and usage a permission is created by default with stereotype import and is used to import elements from one package into another attribute add a new attribute to the currently selected class the attribute is given the de fault name newattr of type int and may be edited by button double click and using the key board or by selecting with button click after the class has been selected and using the property tab note this tool is grayed out except when a class is selected operation add a new operation to the currently selected class or interface the operation is given the default name newoperation with no arguments and return type void and may be ed ited by button double click and using the keyboard or by selecting with button click after the class has been selected and using the property tab note this tool is grayed out except when a class or interface is selected association class add a new association class between two model elements selected us ing button motion from the first model element to the second datatype add a datatype to the diagram for convenience when the mouse is over a selected datatype it displays handles at the top and at the bottom which may be clicked or dragged to form a generalization relationship the target being another datatype there are other elements available here enumeration and stereotype these two have similar handles except the one at the top of a stereotype when clicked it creates a metaclass connected by a dependency marked with stereotype this eases the creation of stereotype declaration diagrams see the literature on the subject sequence diagram specific tools seven tools are provided specific to uml model elements on sequence diagrams the detailed proper ties of these model elements are described in the section on sequence diagram model elements see chapter sequence diagram model element reference classifierrole add a classifierrole to the diagram message with call action add a call message between two classifierroles selected using button motion from the originating classifierrole to the receiving classifierrole message with return action add a return message between two classifierroles se lected using button motion from the originating classifierrole to the receiving classifierrole message with create action add a create message between two classifierroles se lected using button motion from the originating classifierrole to the receiving classifierrole message with destroy action add a destroy message between two classifierroles selected using button motion from the originating classifierrole to the receiving classifierrole add vertical space to diagram add vertical space to a diagram by moving all mes sages below this down click the mouse at the point where you want the space to be added and drag down the screen vertically the distance which matches the height of the space you d like to have ad ded remove vertical space in diagram remove vertical space from diagram and move all elements below up vertically click and drag the mouse vertically over the space that you want deleted collaboration diagram specific tools three tools are provided specific to uml model elements on collaboration diagrams the detailed prop erties of these model elements are described in the section on collaboration diagram model elements see chapter collaboration diagram model element reference classifier role add a classifier role to the diagram association role add an association role between two classifier roles selected using button motion from the originating classifier role to the receiving classifier role there are types of association roles offered here see figure the association tool selector associ ation aggregation and composition and all these three can be bidirectional or unidirectional generalization add a generalization between two model elements selected using button from the child to the parent dependency add a dependency between two model elements selected using button motion from the dependent model element add message add a message to the selected association role note this tool is grayed out except when an association role is selected statechart diagram specific tools eleven tools are provided specific to uml model elements on statechart diagrams the detailed proper ties of these model elements are described in the section on statechart diagram model elements see chapter statechart diagram model element reference simple state add a simple state to the diagram composite state add a composite state to the diagram all model elements that are sub sequently placed on the diagram on top of the composite state will form part of that composite state transition add a transition between two states selected using button motion from the originating state to the receiving state synch state add a synchstate to the diagram submachine state add a submachinestate to the diagram stub state add a stubstate to the diagram initial add an initial pseudostate to the diagram caution there is nothing to stop you adding more than one initial state to a diagram or compos ite state however to do so is meaningless and one of the critics will complain final state add a final state to the diagram junction add a junction pseudostate to the diagram caution a well formed junction should have at least one incoming transition and at least one outgoing argouml does not enforce this but an argouml critic will complain about any junction that does not follow this rule choice add a choice pseudostate to the diagram caution a well formed choice should have at least one incoming transition and at least one out going argouml does not enforce this but an argouml critic will complain about any choice that does not follow this rule fork add a fork pseudostate to the diagram caution a well formed fork should have exactly one incoming transition and two or more out going argouml does not enforce this but an argouml critic will complain about any fork that does not follow this rule join add a join pseudostate to the diagram caution a well formed join should have exactly one outgoing transition and two or more in coming argouml does not enforce this but an argouml critic will complain about any join that does not follow this rule shallow history add a shallow history pseudostate to the diagram deep history add a deep history pseudostate to the diagram call event add a call event as trigger to a transition there are types of events offered here call event change event signal event and time event guard add a guard to a transition call action add a call action i e the effect to a transition there are types of actions offered here call action create action destroy action return action send action terminate action uninterpreted action and action sequence activity diagram specific tools seven tools are provided specific to uml model elements on activity diagrams the detailed properties of these model elements are described in the section on activity diagram model elements see chapter activity diagram model element reference action state add an action state to the diagram transition add a transition between two action states selected using button motion from the originating action state to the receiving action state initial add an initial pseudostate to the diagram caution there is nothing to stop you adding more than one initial state to a diagram however to do so is meaningless and one of the critics will complain final state add a final state to the diagram junction add a junction decision pseudostate to the diagram caution a well formed junction should have one incoming transition and two or more outgoing argouml does not enforce this but an argouml critic will complain about any junc tion that does not follow this rule fork add a fork pseudostate to the diagram caution a well formed fork should have one incoming transition and two or more outgoing argouml does not enforce this but an argouml critic will complain about any fork that does not follow this rule join add a join pseudostate to the diagram caution a well formed join should have one outgoing transition and two or more incoming argouml does not enforce this but an argouml critic will complain about any join that does not follow this rule callstate add a callstate to the diagram a call state is an action state that calls a single op eration hence the name of the operation being called is put in the symbol along with the name of the classifier that hosts the operation in parentheses under it objectflowstate add a objectflowstate to the diagram an objectflowstate is an object that is input to or output from an action deployment diagram specific tools ten tools are provided specific to uml model elements on deployment diagrams the detailed proper ties of these model elements are described in the section on deployment diagram model elements see chapter deployment diagram model element reference note remember that argouml deployment diagrams are also used for component diagrams node add a node to the diagram for convenience when the mouse is over a selected node it displays four handles to left right top and bottom which may be dragged to form association rela tionships node instance add a node instance to the diagram for convenience when the mouse is over a selected node instance it displays four handles to left right top and bottom which may be dragged to form link relationships component add a component to the diagram for convenience when the mouse is over a se lected component it displays four handles to left right top and bottom which may be dragged to form dependency relationships component instance add a component instance to the diagram for convenience when the mouse is over a selected component instance it displays four handles to left right top and bot tom which may be dragged to form dependency relationships generalization add a generalization between two model elements selected using button from the child to the parent realization add a realization between a class and an interface selected using button mo tion from the realizing class to the realized interface dependency add a dependency between two model elements selected using button motion from the dependent model element association add an association between two model elements node component class or interface selected using button motion from the first model element to the second model ele ment there are types of association offered here see figure the association tool selector association aggregation and composition and all these three can be bidirec tional or unidirectional caution the constraint that associations between classes and interfaces must not be navigable from the interface still applies on deployment diagrams object add an object to the diagram for convenience when the mouse is over a selected ob ject it displays four handles to left right top and bottom which may be dragged to form link rela tionships link add a link between two model elements node instance component instance or object selected using button motion the broom argouml broom alignment tool is specialized to support the needs of designers in achieving the kind of alignment used in uml diagrams it is common for designers to roughly align objects as they are cre ated or by using simple movement commands the broom is an easy way to precisely align objects that are already roughly aligned furthermore the broom distribution options are suited to the needs of uml designers making related objects appear evenly spaced packing objects to save diagram space and spreading objects out to make room for new objects the broom also makes it easy to change from horizontal to vertical alignment or from left alignment to right alignment the t shaped icon in argouml diagram toolbar invokes the broom alignment tool when the mouse button is pressed while in broom mode the designer initial mouse movement orients the broom to face in one of four directions north south east or west after that mouse drag events cause the broom to advance in the chosen direction withdraw or grow in a lateral direction like a real world push broom the broom tool pushes diagram elements that come in contact with it this has the effect of align ing objects along the face of the broom and provides immediate visual feedback see the figure below unlike a real world broom moving backwards allows diagram elements to return to their original posi tion growing the broom makes it possible to align objects that are not near each other when the mouse button is released the broom disappears and the moved objects are selected to make it easy to manipu late them further figure the broom if the designer presses the space bar while using the broom objects on the face of the broom are distrib uted i e spaced evenly argouml broom supports three distribution modes objects can be spaced evenly across the space that they use objects can be packed together with only a small gap between them or objects can be distributed evenly over the entire length of the broom face repeatedly pressing the space bar cycles among these three distribution modes and displays a brief message indicating the operation just performed space evenly pack tightly spread out and original if the designer presses the enter key while using the broom the broom turns red instead of the normal blue and objects are not picked up by the broom when moving forward it works like lifting up the broom pressing enter again returns to the normal mode pressing the tab key works exactly like the enter key selection action buttons when the user selects a model element in a uml diagram several handles are drawn on it to indicate that it is selected and to provide user interface affordances to resize the node argouml also displays some selection action buttons around the selected model element see the figure below for some ex amples of the handles and selection action buttons the two figures for a class differ because for cre ating the second one the shift key has been depressed figure some examples of selection action buttons selection action buttons offer common operations on the selected object for example a class node has a button at o clock for adding a superclass one at o clock for adding a subclass and buttons at o clock and o clock for adding associations these buttons support a click or drag interaction a single click creates a new related class at a default position relative to the original class and creates a generalization or association a drag from the button to an existing class creates only the generalization or association and a drag to an empty space in the diagram creates a new class at the mouse position and the generalization or association argouml provides some automated layout support so that click ing the subclass button will position the new classes so that they do not overlap selection action buttons are transparent they have a visibly recognizable rectangular shape and size and they contain an icon that is the same as the icon used for the corresponding type of design element on the standard toolbar however these icons are unfilled line drawings with many transparent pixels this allows selection action buttons to be overlaid onto the drawing area without overly obscuring the diagram itself also the buttons are only drawn when the mouse is over the selected model element if any part of the diagram is obscured the mouse can simply be moved away to get a clearer view of the diagram clarifiers a key feature of argouml are the critics which run in parallel with the main argouml tool when they find a problem they typically raise a to do item and also highlight the problem on the editing pane the graphical techniques used for highlighting are called clarifiers note icon displayed at the top left of a model element indicates a critic of that model ele ment moving the mouse over the icon will pop up the critic headline colored wavy line used for critics specific to sub components of graphical model elements for example to underline attributes with a problem within a class solid colored line not seen in ordinary editing but used when a to do item is highlighted from the to do pane see chapter the to do pane by button double click the solid line is used to show all the model elements affected by the critic for example all stimuli that are out of order the drawing grid the editing pane is provided with a background grid which can be set in various styles or turned off al together through the menu see section adjust grid whatever grid is actually displayed placement of items on the diagram is always controlled by the set ting for grid snap which ranges from to pixels see section adjust snap the diagram tab at the bottom of the editing pane is a small tab labeled as as diagram the concept is that a uml diagram can be displayed in a number of ways for example as a graphical diagram or as a table each representation would have its own tab and be selected by button click on the tab earlier versions of argouml did implement a tabular representation but the current release only sup ports a diagram representation so this tab does not have any function pop up menus within the editing pane button click over a model element will bring up a pop up menu with a vari able number of main entries many with a sub menu critiques this sub menu gives list of all the critics that have triggered for this model element selection of a menu entry causes that entry to be highlighted in the to do pane and its detailed explanation to be placed in the todoitem tab of the details pane a solid colored line indicates the offending element ordering this menu controls the ordering of overlapping model elements on the diagram it is equivalent to the reorder sub menu of the arrange menu see section reorder there are four entries forward the selected model elements are moved one step forward in the ordering hierarchy with respect to other model elements they overlap backward the selected model elements are moved one step back in the ordering hierarchy with respect to other model elements they overlap to front the selected model elements are moved to the front of any other model elements they overlap to back the selected model elements are moved to the back of any other model elements they overlap add this sub menu only appears for model elements that can have notes attached class interface object state pseudostate or have operations or attributes added class interface there are at most three entries new attribute only appears where the selected model element is a class creates a new at tribute on the model element new operation only appears where the selected model element is a class or interface cre ates a new operation on the model element new comment attaches a new comment to the selected model element add all relations only appears where the selected model element is a class or interface makes all relations visible that exist in the model and that are connected to the selected model ele ment remove all relations only appears where the selected model element is a class or interface removes all connected relations from the diagram without removing them from the model show this sub menu only appears with certain model elements it is completely context dependent there are many possible entries depending on the selected model element and its state hide extension point compartment only appears when the extension point compart ment of a use case is displayed hides the compartment show extension point compartment only appears when the extension point compart ment of a use case is hidden displays the compartment hide all compartments only appears when both attribute and operation compartments are displayed on a class or object hides both compartments show all compartments only appears when both attribute and operation compartments are hidden on a class or object displays both compartments hide attribute compartment only appears when the attribute compartment of a class or object is displayed hides the compartment show attribute compartment only appears when the attribute compartment of a class or object is hidden displays the compartment hide operation compartment only appears when the operation compartment of a class or object is displayed hides the compartment show operation compartment only appears when the operation compartment of a class or object is hidden displays the compartment hide enumeration literal compartment only appears when the enumeration literal compartment of an enumeration is displayed hides the compartment show enumeration literal compartment only appears when the enumeration literal compartment of an enumeration is hidden displays the compartment show all edges only appears on a class displays all associations to shown model elements that are not shown yet this is the same function as the add to diagram on the asociation in the ex plorer context menu currently hide all edges only appears on a class hides all associations this is the same function as remove from diagram on all the associations of this class hide stereotype only appears when the stereotype of a package is displayed hides the ste reotype show stereotype only appears when the stereotype of a package is hidden displays the ste reotype hide visibility only appears when the visibility of a package is displayed hides the visibil ity show visibility only appears when the visibility of a package is hidden displays the visibil ity modifiers this sub menu only appears with class interface package and use case model elements it is used to set or clear the values of the various modifiers available abstract set for an abstract model element leaf set for a final model element i e one with no sub model elements root set for a root model element i e one with no super model elements active set for a model element with dynamic behavior note this really ought to be set automatically for model elements with state machines or activity diagrams multiplicity this sub menu only appears with association model elements when clicking at one end of the associ ation it is used to control the multiplicity at the end of the association nearest the mouse click point there are only four entries a sub set of the range of multiplicities that are available through the property sheet of a association end see section association end aggregation this sub menu only appears with association model elements when clicking at one end of the associ ation it is used to control the aggregation at the end of the association nearest the mouse click point there are three entries none remove any aggregation aggregate make this end a shared aggregation loosely known as an aggregation composite make this end a composite aggregation loosely known as a composition caution uml requires that an end with a composition relationship must have a multiplicity of the default navigability this sub menu only appears with association model elements when clicking at one end of the associ ation it is used to control the navigability of the association there are three entries bidirectional make the association navigable in both directions to make the association navigable only from to in other words can reference but not the other way round to make the association navigable only from to in other words can reference but not the other way round note uml does permit an association to be non navigable in both directions argouml will al low this but you will have to set each of the association ends navigation property reached from the property tab of the association and the diagram does not show any arrows in this case this is considered bad design practice it will trigger a critic in argouml so is only of theoretical interest note uml does not permit navigability from an interface to a class argouml does not prevent this notation notation is the textual representation on the diagram of a modelelement or its properties notation languages argouml supports showing notation in different languages by default all text is shown in uml nota tion but the menus contain an item to select between java and uml with plugin modules it is even possible to select other languages such as c figure a class in uml notation shows a class in uml notation while figure a class in java notation shows the same class in java notation figure a class in uml notation figure a class in java notation notation editing on the diagram most text shown on a diagram may be edited by double clicking button on the text this causes a edit box to be shown with the previous text selected ready for amending also the status bar of argouml i e the small area at the bottom of the argouml window shows an help text that indicates the syntax of the text to be entered text entry can be concluded by pressing or for single line fields by pressing the enter key additionally editing can be concluded by clicking somewhere in the diagram outside the edit area editing notation on the diagram is a very powerful way to enter a lot of model information in a very compact way it is e g possible to create an operation its stereotype all parameters and their types and operation properties visibility concurrency all at once by typing order customerid int items list void sequential an association e g between two classes is showing many texts close to its middle and ends so it de serves some extra explanation figure a couple of associations with adornments shows two as sociations to clarify the following figure a couple of associations with adornments the association on the right shows that invisible fields where text can be entered become visible once the modelelement is selected the fields are indicated by blue rectangles double click on them with mouse button to start editing the visibility the or is shown together with the association end name but it is not shown for an unnamed association end likewise the multiplicity is not shown if it is unless the setting show multiplicities in the menu file properties is checked the example figure does not demonstrate this but stereotypes of an association are shown on the dia gram but are not editable and stereotypes of association ends are shown together with the association end name notation parsing to be written chapter the details pane introduction figure overview of the details pane shows the argouml window with the details pane high lighted figure overview of the details pane for any model element within the system this pane is where all its associated data is viewed and entered the pane has a series of tabs at the top which are selected by button click the body of a tab is a menu of items to be checked selected or entered specific to the particular tab selected of these the properties tab is by far the most complex with a different presentation for each mod el element within the system the detailed descriptions of the properties tab for each model element are the subject of separate chapters covering the model elements that may appear on the various diagrams see chapter top level model element reference through chapter deployment diagram model element reference to do item tab this tab provides control over the various to do items created by the user or raised automatically by the argouml critics discussed in more detail in the section on the critique menu see section the critique menu figure example of the to do item tab on the properties pane shows a typical pane the to do item is selected with button in the to do pane see chapter the to do pane or by using the critiques context sensitive pop up menu on the editing pane figure example of the to do item tab on the properties pane customization of the critics behaviour is possible through the browse critics menu see sec tion browse critics the body of the tab describes the problem found by the critic and outlines how it can be fixed to the left are four buttons new to do item this launches a dialog box see figure dialog box for new to do item which allows you to create your own to do item with its own headline which ap pears in the to do pane priority for the to do pane reference url and detailed description for fur ther information figure dialog box for new to do item resolve item this pops up a dialog allowing the user to resolve the selected to do item see figure dialog box for resolve item this is an important dialog because it allows you to deal with to do items in ways other than the recommendation of the to do item which is the whole point of their being advisory this dialog box is intended to be used for the following reasons deleting todo items that were manu ally created preventing a single critic to trigger on a single object and dismissing categories of todo items by lowering design concerns or design goals figure dialog box for resolve item at the top are three radio buttons of which by default the last is selected labeled it is not relevant to my goals it is not of concern at the moment and reas on given below if you choose the third of these you should enter a reason in the main text box tip if you wish to resolve a to do item that is generated by a critic by following its re commendations just make the recommended changes and the to do item will disappear of its own accord there is no need to use this dialog warning the version of argouml implementation is incomplete the reason given is not stored when the project is saved and there is no way to retrieve todo items that were resolved so it is not usefull to give a reason at all when a todo item generated by a critic is resolved then there is no way to undo this unless by re creating the object that triggered the critic snooze critic this suspends the activity of the critic that generated the current to do item the to do item and all others generated by the critic will disappear from the to do pane the critic will wake up after a period of time initially this period is minutes but it doubles on each successive application of the snooze button the critic can be awakened explicitly through the critique browse critics menu see section browse critics tip some common critics can fire the whole time as you build a big diagram some users find it useful to snooze these critics until the diagram has been completed wizards some of the more common critics have a wizard available to help in fixing the problem the wizard comprises a series of pages one or more in the todo item tab that step you through the changes start the wizard by clicking the next button figure example of a wizard the wizard is driven through the first three buttons at the bottom of the todo item tab back this will take you back to the previous step in the wizard grayed out if this is the first step next this will take you back to the next step in the wizard grayed out if this is the last step finish this will commit the changes you have made through the wizard in previous steps and or use the defaults for all next steps note not all to do items have wizards if there is no wizard all three buttons will remain grayed out the argouml wizards are non modal i e once started you may select other todo items or do some other actions and all the while the wizard will remeber where it was so if you return to the todo item the wizard will indicate the same step it was on when you left it the help button there is one remaining button at the bottom of the to do item tab labeled help this will fire up a browser to a url with further help properties tab through this tab the properties of model elements selected in the explorer or editing pane may be set the properties of an model element may be displayed in one of the following ways selection of the model element in the explorer or editing panes followed by selection of the prop erties tab in the details pane or navigation buttons cause different model elements to be selected i e the go up button on the properties tab the navigate back and navigate forward buttons in the main tool bar and the various menu items under edit select figure a typical properties tab on the details pane shows a typical properties tab for a model element in argouml in this case a class figure a typical properties tab on the details pane at the top left is the icon and name of the type of model element i e the uml metaclass not the actual name of this particular model element in this example the property tab is for a class to the right of this is a toolbar of icons relevant to this property tab the first one is always navigation go up the last is always delete to delete the selected model element from the model the ones in between depend on the model element the remainder of the tab comprises fields laid out in two or three columns each field has a label to its left the fields may be text boxes text areas drop down selectors radio boxes and check boxes in most but not all cases the values can be changed in the case of text boxes this is sometimes by just typing the required value however for many text boxes and text areas data entry is via a context sensitive pop up menu using button click which offers options to add a new entry delete an entry or move entries up and down in text areas with multiple entries the first field is almost always a text field name where the name of the specific model element can be entered the remaining fields vary depending on the model element selected the detailed property sheets for all argouml model elements are discussed in separate chapters for each of the diagram types use case diagram chapter use case diagram model element reference class diagram chapter class diagram model element reference sequence diagram chapter sequence diagram model element reference statechart diagram chapter statechart diagram model element reference collaboration diagram chapter collaboration diagram model element reference activity diagram chapter activity diagram model element reference deployment dia gram chapter deployment diagram model element reference property sheets for model ele ments that are common to all diagram types have their own chapter chapter top level model ele ment reference caution argouml will always try to squeeze all fields on to the property sheet if the size of the property tab is too small it may become unusable the solution is to either enlarge the property tab by enlarging the main window or by moving the dividers to left and top documentation tab within the uml standard all model elements are children of the element metaclass the ele ment metaclass defines a tagged value documentation for comment description or explanation of the element to which it is attached since this tagged value applies to every model element it is given its own tab in the details pane rather than being part of the tagged values tab figure a typical documentation tab on the details pane shows a typical documentation tab for a model element in argouml figure a typical documentation tab on the details pane as you can see many more fields have been added to the documentation field alone the other fields similarly store their information under tagged values author version since deprecated see the fields on this tab are the same for all model elements since uml comments are a kind of documentation they are also shown on this tab with name and body author a text box for the author of the documentation version a text box for the version of the documentation since a text box to show how long the documentation has been valid deprecated a check box to indicate whether this model element is deprecated i e planned for removal in future versions of the design model see pointers to documentation outside the system documentation literal text of any documentation comment name the names of all comments attached to the modelelement body the bodies of all comments attached to this modelelement tip argouml is not primarily a documentation system for model elements that require heavy documentation notably use cases the use of the see field to point to external documents is more practical presentation tab this tab provides some limited control over the graphical representation of model elements in the dia gram in the editing pane model elements that do not have any specific direct graphical representation on the screen beyond their textual description do not have style tabs of their own for example the style sheet of an operation on a class will be downlighted style sheets vary a little from model element to model element but figure a typical present ation tab on the details pane shows a typical style tab for a model element in argouml in this case a class figure a typical presentation tab on the details pane there may be further fields in some cases e g for a package but most fields are common to many mod el elements path this checkbox allow to display or hide the path in front of the name of the modelelement it is shown in uml notation with seperators e g the argouml main class would be shown as org argouml application main attributes this checkbox allows to hide or show the attributes compartment of a class operation this checkbox allows to hide or show the operations compartment of a class or inter face stereotype this checkbox allows to reveal or hide the stereotypes of a package shown above the name visibility this checkbox allows to hide the visibility of a package the visibility is shown in uml notation as or extension points this checkbox allows to reveal or hide the extensions points compartment of a usecase bounds this defines the corners of the bounding box for a model element it comprises four numbers separated by commas these four numbers are respectively i the x coordinate of the upper left corner of the box ii the y coordinate of the upper left corner of the box iii the width of the box and iv the height of the box all units are pixels on the editing pane this field has no effect on model elements that link other model elements associations general izations etc since their position is constrained by their connectedness in this case the field is down lighted fill this drop down selector specifies the fill color for model elements it is not present for line model elements selecting no fill makes the model element transparant selecting custom allows to create other colors then the ones listed it causes the color selector dialog box to appear see figure the custom fill line color dialog box line this drop down selector specifies the line color for model elements selecting no fill makes the model element transparant selecting custom allows to create other colors then the ones listed it causes the color selector dialog box to appear see figure the custom fill line color dialog box figure the custom fill line color dialog box figure the custom fill line color dialog box figure the custom fill line color dialog box source tab this tab shows the source code that will be generated for this model element in the selected language argouml generates the code e g for classes and interfaces the code shown here may be saved in the indicated files with the aid of the functions in the generation menu figure the source tab of a class any code you add will be lost that is not the intention of argouml use an ide instead the dropdown at the right allows selection of the output file this function is not very useful for lan guages that generate all code for a class within one file but serves its purpose for e g c where a h and cpp file are generated see the figure below figure a c example constraints tab constraints are one of the extension mechanisms provided for uml argouml is equipped with a powerful constraint editor based on the object constraint language ocl defined in the uml standard caution the ocl editor implementation for argouml doesn t support ocl constraints for elements other than classes and features this is something of a general restriction of ocl although the uml specification claims that there may be a constraint for every model element the ocl specification only defines classes interfaces and operations as allowable contexts it is not before ocl that a more general definition of allowable contexts is introduced the key issue is that for each context definition you need to define what is the contextual classifier i e the classifier that will be associated with the self keyword the creators of the ocl specification claim that this is not an issue for the ocl specification but rather for uml or some integration task force conversely it seems that the uml specification people seem to expect this to be defined in the ocl specification which is why we did a first step in that direction in ocl so to cut a long story short it appeared that the simplest solution for argouml at the mo ment would be to enable the ocl property panel only for those model elements for which there actually exists a definition of the contextualclassifier in ocl these are above class interface and feature the standard pre defines a small number of constraints for example the xor constraint over a set of as sociations indicating that only one may be manifest for any particular instance the standard also envisages a number of circumstances where general purpose constraints may be use ful to specify invariants on classes and types in the class model to specify type invariants for stereotypes to describe pre conditions and post conditions on operations and methods to describe guards as a navigation language and to specify constraints on operations figure a typical constraints tab on the details pane shows a typical constraint tab for a model element in argouml in this case a class figure a typical constraints tab on the details pane along the top of the tab are a series of icons new constraint this creates a new constraint and launches the constraint editor in the constraints tab for that new constraint see section the constraint editor the new constraint is created with a context declaration for the currently selected model element warning it seems logical that when a new constraint is created it needs to be edited but argouml fails to start the ocl editor upon creation you have to do this by primo selecting the new constraint first secundo rename it and tertio press the edit constraint button it is essental for successfully creating a constraint to follow these steps accurately create select rename edit the step to rename is necessary because the validity check will refuse the constraint if its name differs from the name mentioned in the constraint text for the same reason renaming a constraint afterwards is impossible delete constraint the constraint currently selected in the constraint name box see below is deleted caution in of argouml this button is not downlighted when it is not functional i e when no constraint is selected edit constraint this launches the constraint editor in the constraints tab see sec tion the constraint editor the editor is invoked on the constraint currently selected in the constraint name box caution in of argouml this button is not downlighted when it is not functional i e when no constraint is selected configure constraint editor this a dialog to configure options in the constraint editor see figure dialog box for configuring constraints figure dialog box for configuring constraints the dialog box has a check box for the following option check type conformance of ocl constraints ocl is strictly typed at the early stages of design it may be helpful to disable type checking rather than follow through all the de tailed specification needed to get type consistency at the bottom are two buttons labeled ok to accept the option changes and cancel to discard the changes the main body of the constraints tab comprises two boxes a smaller to the left and a larger one to the right the two are separated by two small arrow buttons which control the size of the boxes shrink left button click on this icon shrinks the box on the left its effect may be reversed by use of the shrink right button see below shrink right button click on this icon shrinks the box on the right its effect may be re versed by use of the shrink left button see above finer control can be achieved by using button motion to drag the dividing bar to left and right the box on the left is titled constraint name and lists all the constraints if any so far defined for the selected model element a constraint may be selected by button click the box on the right is labeled preview and contains the text of the constraint this box only shows some contents if a constraint is selected where a constraint is too large for the box a scroll bar is provided to the right the constraint editor this is invoked through the use of the edit constraint button on the main constraints tab the constraint editor takes up the whole tab see figure dialog box for configuring constraints figure dialog box for configuring constraints along the top of the tab are a series of icons cancel edit constraint this exits the constraint editor without saving any changes and returns to the main constraints tab check ocl syntax this button invokes a full syntax check of the ocl written in the edit or if the syntax is valid the constraint is saved and control returns to the main constraints tab if the syntax is not valid a dialog box explains the problem warning whether type checking is included should be configurable with the configure constraint editor button see below but argouml does always check and refuses to accept any constraint with the slightest error configure constraint editor this a dialog to configure options in the constraint editor it is also available in the main constraints tab and is discussed in detail there see sec tion constraints tab to the right of the toolbar is a check box labeled syntax assistant unchecked by default which will enable the syntax assistant in the constraint editor if the syntax assistant is enabled six drop down menus are provided in a row immediately below the toolbar these provide standard templates for ocl that when selected will be inserted into the con straint being edited the syntax assistant can be made floating in a separate window by button motion on the small divider area to the left of the row of drop down menus general general ocl constructors entries inv inserts an invariant pre inserts a pre condition post inserts a post condition self inserts a self reference pre inserts a refer ence to a value at the start of an operation and result inserts a reference to a previous result basic operators relational operators and parentheses entries and numbers arithmetic operators and functions entries mod div abs max min round and floor strings string functions entries concat size tolower toupper and substring booleans logical functions entries or and xor not implies and if then else collections operators and functions on collections bags sets and sequences the large num ber of functions are organized into sub groups general functions that apply to all types of collection entries collection insert a new collection set insert a a new set bag insert a new bag sequence insert a new sequence size count isempty notempty includes includesall iterate exists forall collect select reject union intersection including excluding and sum sets operators and functions that apply only to sets entries set difference and symmet ricdifference sequences functions that apply to sequences entries first last at append pre pend and subsequence the remainder of the tab comprises a writable text area containing the text to be edited the mouse but tons have their standard behavior within an editable text area see section general mouse behavi or in argouml in addition cut copy and paste operations may be invoked through the keyboard shortcuts ctrl x ctrl c and ctrl v respectively stereotype tab this tab shows the available and applied stereotypes for the currently selected modelelement it consists of panels and buttons the buttons allow to move the stereotypes from one list to the other figure an example of a stereotype tab for a class in the lists between the baseclass of the stereotypes is shown e g in the figure above the thread classifier stereotype may be applied to all types of classifiers such as class usecase tagged values tab tagged values are another extension mechanism provided by uml the user can define name value pairs to be associated with model elements which define properties of that model element the names are known as tags uml pre defines a number of tags that are useful for many of its model elements note the tag documentation is defined for the top uml metaclass element and is so available to all model elements in argouml documentation values are provided through the documentation tab rather than by using the tagged values tab the tagged values tab in argouml comprises a two column table with a combo box on the left to select the tagdefinition and an editable box on the right for the associated value there is always at least one empty row available for any new tag the button at the top of this tab allows creation of a new tagdefinition after clicking this button go to the properties tab first to set the name of the new tagdefinition the mouse buttons have their standard behavior within the editable value area see section gener al mouse behavior in argouml in addition when in the value field cut copy and paste operations may be invoked through the keyboard shortcuts ctrl x ctrl c and ctrl v respectively checklist tab conducting design reviews and inspections is one of the most effective ways of detecting errors during software development a design review typically consists of a small number of designers implementers or other project stakeholders holding a meeting to review a software development artifact many devel opment organizations have developed checklists of common design problems for use in design review meetings recent research indicated that reviewers inspecting code without meeting makeing use of these checklists are just as effective as design review meetings hence a checklist feature has been added to argouml that is much in the spirit of design review checklists however argouml checklists are integrated into the design tool user interface and the design task a software designer using argouml can see a review checklist for any design element the checklist tab presents a list of check off items that is appropriate to the currently selected design element for ex ample when a class is selected in a design diagram the checklist tab shows items that prompt critical thinking about classes see the figure below designers may check off items as they consider them checked items are kept in the list to show what has already been considered while unchecked items prompt the designer to consider new design issues argouml supplies many different checklists with many possible items figure an example of a checklist for a class caution in the release of argouml this tab is not completely implemented e g the checks are not saved chapter the to do pane introduction figure overview of the to do pane shows the argouml window with the to do pane high lighted figure overview of the to do pane this pane provides access to the advice that comes from the critics processes running within argouml a selector box at the top allows a choice of how the data is presented a button allows the display of the hierarchy to be changed and there is an indicator of the number of to do items identified more information on critics can be found in the discussion of the critique menu see section the critique menu mouse behavior in the to do pane behavior of the mouse in general and the naming of the buttons is covered in the chapter on the overall user interface see chapter introduction button click this action is generally used to select an item for subsequent operations within the hierarchical display elements which have sub hierarchies may be indicated by when the hierarchy is hidden and when the hierarchy is open when these icons are displayed the display of the hierarchy is toggled by button click on these icons button click over the headline of any to do item will cause its details to be shown in the to do item tab of the details pane that tab is automatically selected if it is not currently visible button double click when applied to the folder icon alongside a hierarchy category this will cause the display of that hier archy to be toggled when applied to a headline button double click will show the diagram for the model element to which the to do item applies in the editing pane and select the model element on the diagram using an appro priate clarifier the model element may be highlighted underlined with a wavy line or surrounded by a colored box as appropriate button actions there are no button functions in the to do pane button double click there are no button functions in the to do pane presentation selection at the top of the pane is a drop down selector controlling how the to do items are presented the to do items may be presented in six different ways this setting is not stored persistently i e it is on its default vallue when argouml is started by priority this is the default setting the to do items are organized into three hierarchies by priority high medium and low the priority associated with the to do items generated by a par ticular critic may be altered through the critique browse critics menu see sec tion browse critics by decision the to do items are organized into hierarchies by design issue uncategor ized class selection behavior naming storage inheritance containment planned extensions state machines design patterns relationships in stantiation modularity expected usage methods code generation and stereotypes the details of the critics in each category are discussed in section design issues by goal argouml has a concept that critics may be grouped according to the user goals they af fect this presentation groups the to do items into hierarchies by goal caution in the current release of argouml there is only one goal unspecified and all to do items will appear under this heading by offender the to do items are organized into a hierarchy according to the model element that caused the problem todo items that were manually created with the new todo item button i e not by a critic are not listed here by poster the to do items are organized into a hierarchy according to which critic generated the to do item the class name of the critic is listed instead of just its headline name since the former is guaranteed to be a unique name by knowledge type argouml has the concept that a critic reflects a deficiency in a category of knowledge this presentation option groups the critics according to their knowledge category designer correctness completeness consistency syntax semantics op timization presentational organizational experiencial and tool the former category designer contains the manually entered todo items item count to the right of the flat hierarchical button is a count of the number of to do items currently found it will be highlighted in yellow when the number of to do items grows above todo items and red when above chapter the critics introduction the key feature that distinguishes argouml from other uml case tools is its use of concepts from cognitive psychology the theory behind this is well described in jason robbins phd dissertation ht tp argouml tigris org docs http argouml tigris org docs critics are one of the main ways in which these ideas are implemented running in the background they offer advice to the designer which may be accepted or ignored a key point is that they do not impose a decision on the designer note the critics are asynchronous processes that run in parallel with the main argouml tool changes typically take a second or two to propagate as the critics wake up terminology the critics are background processes which evaluate the current model according to various good design criteria there is one critic for every design criterion the output of a critic is a critique a statement about some aspect of the model that does not appear to follow good design practice finally a critique will generally suggest how the bad design issue it has identified can be rectified by raising a to do item design issues argouml categorizes critics according the the design issue they address some critics may be in more than one category at present there are such categories within this manual the descriptions of critics are grouped in sections by design issue uncategorized these are critics that do not fit into any other category argouml has no critics in this category maybe some will be added in later versions class selection these are critics concerning how classes are chosen and used argouml has the following critics in this category wrap datatype datatypes are not full classes within uml they can only have enumeration literals as values and only support query operations that is operations that do not change the datatype state datatypes cannot be associated with classes unless the datatype is part of a composite black dia mond aggregation such an association relects the tight binding of a collection of datatype instances to a class instance in effect such a datatype is an attribute of the class with multiplicity good ooa d depends on careful choices about which entities to represent as full objects and which to represent as attributes of objects there are two options to fix this problem replace the datatype with a full class or change the association aggregation to composite relationship at the datatype end reduce classes in namespace namespace suggestion to improve understandability by having fewer classes in one namespace if one namespace such as the model a package or a class has too many classes it may become very difficult for humans to understand defining an understandable set of namespaces is an important part of your design the wizard of this critic allows setting of the treshold i e the maximum number of classes allowed be fore this critic fires caution this number is not stored persistently and there is no way to reduce it after it has been set higher except by creating more classes until the critic fires again restarting argouml re sets this number to its default clean up diagram suggestion that the diagram could be improved by moving model elements that are overlapping naming these are critics concerning the naming of model elements the current version of argouml has critics in this category resolve association name conflict suggestion that two association names in the same namespace have the same name this is not permit ted in uml revise attribute names to avoid conflict suggestion that two attribute names of a class have the same name this is not permitted in uml note the problem may be caused by inheritance of an attribute through a generalization rela tionship change names or signatures in a model element two operations in model element have the same signature this means their name is the same and the list of parameters has the same type where there are conflicting signatures correct code cannot be generated for mainstream oo languages it also leads to very unclear semantics of the design in comparing signatures this critic considers the name the list of in out and in out parameter types in order and only if these all match in both type and order will the signatures be considered as the same this follows the line of java c in ignoring the return parameters for the signature this may be unsat isfactory for some functional oo languages note some purists would argue that the comparison should really differentiate between in out and in out parameters however no practical programming language can do this when resolving an overloaded method invocation so this critic lumps them all together duplicate end role names for an association the specified association has two or more ends roles with the same name one of the well formedness rules in uml for associations is that all end role names must be unique this ensures that there can be unambiguous reference to the ends of the association to fix this manually select the association and change the names of one or more of the offending ends roles using the button pop up menu or the property sheet role name conflicts with member a suggestions that good design avoids role names for associations that clash with attributes or opera tions of the source class roles may be realized in the code as attributes or operations causing code gen eration problems choose a name classes and interfaces the class or interface concerned has been given no name it will appear in the model as unnamed suggestion that good design requires that all interfaces and classes are named name conflict in a namespace the namespace e g package concerned contains a class or interface specified that has the same name as another in the same namespace which is bad design and will prevent valid code generation choose a unique name for a model element classes and interfaces suggestion that the class or interface specified has the same name as another in the namespace which is bad design and will prevent valid code generation choose a name attributes the attribute concerned has been given no name it will appear in the model as unnamed attrib ute suggestion that good design requires that all attributes are named choose a name operations the operation concerned has been given no name it will appear in the model as unnamed opera tion suggestion that good design requires that all operations are named choose a name states the state concerned has been given no name it will appear in the model as unnamed state suggestion that good design requires that all states are named choose a unique name for a state related model element suggestion that the state specified has the same name as another in the current statechart diagram which is bad design and will prevent valid code generation revise name to avoid confusion two names in the same namespace have very similar names differing only by one character sugges tion this could potentially lead to confusion caution this critic can be particularly annoying since at times it is useful and good design to have a series of model elements etc it is important to remember that critics offer guidance and are not always correct argouml lets you dismiss the resulting to do items through the to do pane see chapter the to do pane choose a legal name all model element names in argouml must use only letters digits and underscore characters this crit ic suggests an entity has not met this requirement change a model element to a non reserved word suggestion that this model element name is the same as a reserved word in uml or within one char acter of one which is not permitted choose a better operation name suggestion that an operation has not followed the naming convention that operation names begin with lower case letters caution following the java and c convention most designers give their constructors the same name as the class which begins with an upper case character in argouml this will trig ger this critic unless the constructor is stereotyped create it is important to remember that critics offer guidance and are not always correct argouml lets you dismiss the resulting to do items through the to do pane see chapter the to do pane choose a better attribute name suggestion that an attribute has not followed the naming convention that attribute names begin with lower case letters capitalize class name suggestion that a class has not followed the naming convention that classes begin with upper case let ters note although not triggering this critic the same convention should apply to interfaces revise package name suggestion that a package has not followed the naming convention of using lower case letters with peri ods used to indicated sub packages storage critics concerning attributes of classes the current version of argouml has the following critics in this category revise attribute names to avoid conflict this critic is discussed under an earlier design issues category see section revise attribute names to avoid conflict add instance variables to a class suggestion that no instance variables have been specified for the given class such classes may be cre ated to specify static attributes and methods but by convention should then be given the stereotype utility add a constructor to a class you have not yet defined a constructor for class class constructors initialize new instances such that their attributes have valid values this class probably needs a constructor because not all of its attributes have initial values defining good constructors is key to establishing class invariants and class invariants are a powerful aid in writing solid code to fix this add a constructor manually by clicking on class in the explorer and adding an operation us ing the context sensitive pop up menu in the property tab or select class where it appears on a class dia gram and use the add operation tool in the uml standard a constructor is an operation with the stereotype create although not strictly standard argouml will also accept create as a stereotype for constructors by convention in java and c a constructor has the same name as the class is not static and returns no value argouml will also accept any operation that follows these conventions as a constructor even if it is not stereotyped create caution operators are created in argouml with a default return parameter named return you will need to remove this parameter to meet the java c convention reduce attributes on a class suggestion that the class has too many attributes for a good design and is at risk of becoming a design bottleneck the wizard of this critic allows setting of the treshold i e the maximum number of attributes allowed before this critic fires caution this number is not stored persistently and there is no way to reduce it after it has been set higher except by creating more attributes until the critic fires again restarting argouml resets this number to its default planned extensions critics concerning interfaces and subclasses note it is not clear why this category has the name planned extensions the current version of argouml has three critics in this category operations in interfaces must be public suggestion that there is no point in having non public operations in interfaces since they must be visible to be realized by a class interfaces may only have operations suggestion that an interfaces has attributes defined the uml standard defines interfaces to have opera tions caution argouml does not allow you to add attributes to interfaces so this should never occur in the argouml model it might trigger if a project has been loaded with xmi created by an other tool remove reference to specific subclass suggestion that in a good design a class should not reference its subclasses directly through attributes operations or associations state machines critics concerning state machines argouml has the following critics in this category reduce transitions on state suggestion given state is involved in so many transitions it may be a maintenance bottleneck the wizard of this critic allows setting of the treshold i e the maximum number of transitions allowed before this critic fires caution this number is not stored persistently and there is no way to reduce it after it has been set higher except by creating more transition until the critic fires again restarting argouml resets this number to its default reduce states in machine machine suggestion that the given state machine has so many states as to be confusing and should be simplified perhaps by breaking into several machines or using a hierarchy the wizard of this critic allows setting of the treshold i e the maximum number of states allowed be fore this critic fires caution this number is not stored persistently and there is no way to reduce it after it has been set higher except by creating more states until the critic fires again restarting argouml re sets this number to its default add transitions to state suggestion that the given state requires both incoming and outgoing transitions add incoming transitions to model element suggestion that the given state requires incoming transitions add outgoing transitions from model element suggestion that the given state requires outgoing transitions remove extra initial states suggestion that there is more than one initial state in the state machine or composite state which is not permitted in uml place an initial state suggestion that there is no initial state in the state machine or composite state add trigger or guard to transition suggestion that a transition is missing either a trigger or guard one at least of which is required for it to be taken change join transitions suggestion that the join pseudostate has an invalid number of transitions normally there should be one outgoing and two or more incoming change fork transitions suggestion that the fork pseudostate has an invalid number of transitions normally there should be one incoming and two or more outgoing add choice junction transitions suggestion that the branch choice or junction pseudostate has an invalid number of transitions nor mally there should be at least one incoming transition and at least one outgoing transition add guard to transition suggestion that the transition requires a guard caution it is not clear that this is a valid critic it is perfectly acceptable to have a transition without a guard the transition is always taken when the trigger is invoked clean up diagram this critic is discussed under an earlier design issues category see section clean up diagram make edge more visible suggestion that an edge model element such as an association or abstraction is so short it may be missed move the connected model elements apart to make the edge more visible composite association end with multiplicity an instance may not belong by composition to more than one composite instance you must change the multiplicity at the composite end of the association to either or for your model to make sense remember that composition is the stronger aggregation kind and aggregation is the weaker the prob lem can be compared to a model where a finger can be an integral part of several hands at the same time this is the second well formedness rule on associationend in uml design patterns critics concerning design pattern usage in argouml these relate to the use of patterns as described by the so called gang of four argouml also uses this category for critics associated with deployment and sequence diagrams the current version of argouml has the following critics in this category consider using singleton pattern for class the class has no non static attributes nor any associations that are navigable away from instances of this class this means that every instance of this class will be identical to every other instance since there will be nothing about the instances that can differentiate them under these circumstances you should consider making explicit that you have exactly one instance of this class by using the singleton pattern using the singleton pattern can save time and memory space within argouml this can be done by using the singleton stereotype on this class if it is not your intent to have a single instance you should define instance variables i e non static at tributes and or outgoing associations that will represent differences bewteen instances having specified class as a singleton you need to define the class so there can only be a single instance this will complete the information representation part of your design to achieve this you need to do the following you must define a static attribute a class variable holding the instance this must therefore have class as its type you must have only private constructors so that new instances cannot be made by other code the creation of the single instance could be through a suitable helper operation which invokes this private constructor just once you must have at least one constructor to override the default constructor so that the default con structor is not used to create multiple instances for the definition of a constructor under the uml standard and extensions to that definition accep ted by argouml see section add a constructor to a class singleton stereotype violated in class this class is marked with the singleton stereotype but it does not satisfy the constraints imposed on singletons argouml will also accept singleton stereotype as defining a singleton a singleton class can have at most one instance this means that the class must meet the design criteria for a singleton see section consider using singleton pattern for class whenever you mark a class with a stereotype the class should satisfy all constraints of the stereotype this is an important part of making a self consistent and understangle design using the singleton pat tern can save time and memory space if you no longer want this class to be a singleton remove the singleton stereotype by clicking on the class and selecting the blank selection on the stereotype drop down within the properties tab to apply the singleton pattern you should follow the directions in section consider using singleton pattern for class nodes normally have no enclosers a suggestion that nodes should not be drawn inside other model elements on the deployment diagram since they represent an autonomous physical object nodeinstances normally have no enclosers a suggestion that node instances should not be drawn inside other model elements on the deployment diagram since they represent an autonomous physical object components normally are inside nodes a suggestion that components represent the logical entities within physical nodes and so should be drawn within a node where nodes are shown on the deployment diagram componentinstances normally are inside nodes a suggestion that component instances represent the logical entities within physical nodes and so should be drawn within a node instance where node instances are shown on the deployment diagram classes normally are inside components a suggestion that classes as model elements making up components should be drawn within compon ents on the deployment diagram interfaces normally are inside components a suggestion that interfaces as model elements making up components should be drawn within com ponents on the deployment diagram objects normally are inside components a suggestion that objects as instances of model elements making up components should be drawn within components or component instances on the deployment diagram linkends have not the same locations a suggestion that a link e g association connecting objects on a deployment diagram has one end in a component and the other in a component instance since objects can be in either this makes no sense set classifier deployment diagram suggestion that there is an instance object without an associated classifier class datatype on a de ployment diagram missing return actions suggestion that a sequence diagram has a send or call action without a corresponding return action missing call send action suggestion that a sequence diagram has a return action but no preceding call or send action no stimuli on these links suggestion that a sequence diagram has a link connecting objects without an associated stimulus without which the link is meaningless warning triggering this critic indicates a serious problem since argouml provides no mechanism for creating a link without a stimulus it probably indicates that the diagram was created by loading a corrupt project with an xmi file describing a link without a stimulus possibly created by a tool other than argouml set classifier sequence diagram suggestion that there is an object without an associated classifier class datatype on a sequence dia gram wrong position of these stimuli suggestion that the initiation of send call return message exchanges in a sequence diagram does not properly initiate from left to right relationships critics concerning associations in argouml the current version of argouml has the following critics in this category circular association suggestion that an association class has a role that refers back directly to itself which is not permitted warning this critic is meaningless in the version of argouml which does not support asso ciation classes make association navigable suggestion that the association referred to is not navigable in either direction this is permitted in the uml standard but has no obvious meaning in any practical design remove navigation from interface via association associations involving an interface can be not be navigable in the direction from the interface this is because interfaces contain only operation declarations and cannot hold pointers to other objects this part of the design should be changed before you can generate code from this design if you do gen erate code before fixing this problem the code will not match the design to fix this select the association and use the properties tab to select in turn each association end that is not connected to the interface uncheck navigable for each of these ends the association should then appear with a stick arrowhead pointed towards the interface when an association between a class and interface is created in argouml it is by default navigable only from the class to the interface however argouml does not prevent to change the navigability af terwards into a wrong situation which will cause this critic to be triggered add associations to model element suggestion that the specified model element actor use case or class has no associations connecting it to other model elements this is required for the model element to be useful in a design remove reference to specific subclass this critic is discussed under an earlier design issues category see section remove reference to specific subclass reduce associations on model element suggestion that the given model element actor use case class or interface has so many associations it may be a maintenance bottleneck the wizard of this critic allows setting of the treshold i e the maximum number of associations al lowed before this critic fires caution this number is not stored persistently and there is no way to reduce it after it has been set higher except by creating more associations until the critic fires again restarting argouml resets this number to its default make edge more visible this critic is discussed under an earlier design issues category see section make edge more visible instantiation critics concerning instantiation of classifiers in argouml the current version of argouml has no critics in this category modularity critics concerning modular development in argouml the current version of argouml has the following critics in this category 11 classifier not in namespace of its association one of the well formedness rules in uml for associations is that all the classifiers attached to the ends of the association should belong to the same namespace as the association if this were not the case there would be no naming by which each end could refer to all the others this critic is triggered when an association does not meet this criterion the solution is to delete the as sociation and recreate it on a diagram whose namespace includes those of all the attached classifiers caution in the current implementation of argouml this critic does not handle hierarchical namespaces as a consequence it will trigger for associations where the immediate namespaces of the attached classifiers is different even though they are part of the same namespace hierarchy 11 add elements to package package suggestion that the specified package has no content good design suggests packages are created to put things in note this will always trigger when you first create a package since you cannot create one that is not empty expected usage critics concerning generally accepted good practice in argouml the current version of argouml has one critic in this category clean up diagram this critic is discussed under an earlier design issues category see section clean up diagram methods critics concerning operations in argouml the current version of argouml has the following critics in this category change names or signatures in model ele ment this critic is discussed under an earlier design issues category see section change names or signatures in a model element class must be abstract suggestion that a class that inherits or defines abstract operations must be marked abstract add operations to class suggestion that the specified class has no operations defined this is required for the class to be useful in a design reduce operations on model element suggestion that the model element class or interface has too many operations for a good design and is at risk of becoming a design bottleneck the wizard of this critic allows setting of the treshold i e the maximum number of operations allowed before this critic fires caution this number is not stored persistently and there is no way to reduce it after it has been set higher except by creating more operations until the critic fires again restarting argouml resets this number to its default code generation critics concerning code generation in argouml the current version of argouml has one critic in this category change multiple inheritance to interfaces suggestion that a class has multiple generalizations which is permitted by uml but cannot be gener ated into java code because java does not support multiple inheritance stereotypes critics concerning stereotypes in argouml the current version of argouml has no critics in this category inheritance critics concerning generalization and specialization in argouml the current version of argouml has the following critics in this category revise attribute names to avoid conflict this critic is discussed under an earlier design issues category see section revise attribute names to avoid conflict remove class circular inheritance suggestion that a class inherits from itself through a chain of generalizations which is not permitted caution this critic is marked inactive by default in the current release of argouml the only one so marked it will not trigger unless made active class must be abstract this critic is discussed under an earlier design issues category see section class must be ab stract remove final keyword or remove subclasses suggestion that a class that is final has specializations which is not permitted in uml illegal generalization suggestion that there is a generalization between model elements of different uml metaclasses which is not permitted caution it is not clear that such a generalization can be created within argouml it probably indic ates that the diagram was created by loading a corrupt project with an xmi file describing such a generalization possibly created by a tool other than argouml remove unneeded realizes from class suggestion that the specified class has a realization relationship both directly and indirectly to the same interface by realization from two interfaces one of which is a generalization of the other for example good design deprecates such duplication define concrete sub class suggestion that a class is abstract with no concrete subclasses and so can never be realized define class to implement interface suggestion that the interface referred to has no influence on the running system since it is never imple mented by a class change multiple inheritance to interfaces this critic is discussed under an earlier design issues category see section change multiple inheritance to interfaces make edge more visible this critic is discussed under an earlier design issues category see section make edge more visible containment critics concerning containment in argouml that is where one model element forms a component part of another the current version of argouml has the following critics in this category remove circular composition suggestion that there is a series of composition relationships associations with black diamonds that form a cycle which is not permitted duplicate parameter name suggestion that a parameter list to an operation or event has two or more parameters with the same name which is not permitted two aggregate ends roles in binary associ ation only one end role of a binary association can be aggregate or composite this a well formedness rule of the uml standard aggregation and composition are used to indicate whole part relationships and by definition the part end cannot be aggregate to fix this identify the part end of the association and use the critic wizard the next button or manually set its aggregation to none using the button pop up menu or the property sheet composition more correctly called composite aggregation is used where there is a whole part relation ship that is one to one or one to many and the lifetime of the part is inextricably tied to the lifetime of the whole instances of the whole will have responsibility for creating and destroying instances of the as sociated part this also means that a class can only be a part in one composite aggregation an example of a composite aggregation might be a database of cars and their wheels this is a one to four relationship and the database entry for a wheel is associated with its car when the car ceases to exist in the database so do its wheels aggregation more correctly called shared aggregation is used where there is a whole part relationship that does not meet the criteria for a composite aggregation an example might be a database of uni versity courses and the students that attend them there is a whole part relationship between courses and students however there is no lifetime relationship between students and course a student continues to exist even after a course is finished and the relationship is many to many aggregate end role in way or more asso ciation three way or more associations can not have aggregate ends roles this a well formedness rule of the uml standard aggregation and composition are used to indicate whole part relationships and by definition can only apply to binary associations between model elements to fix this manually select the association and set the aggregation of each of its ends roles to none using the button pop up menu or the property sheet wrap datatype this critic is discussed under an earlier design issues category see section wrap datatype part model reference chapter top level model element reference introduction this chapter describes each model element that can be created within argouml the chapter covers top level general model elements the following chapters see chapter use case diagram model element reference through chapter deployment diagram model element reference cover each of the argouml diagrams there is a close relationship between this material and the properties tab of the details pane see sec tion properties tab that section covers properties in general in this chapter they are linked to specific model elements the model the model is the top level model element within argouml in the uml meta model it is a sub class of package in many respects within argouml it behaves similarly to a package see section pack age note argouml is restricted to one model within the tool standard data types classes and packages are loaded the default see chapter profiles as sub packages of the model these sub packages are not initially present in the model but are added to the model when used model details tabs the details tabs that are active for the model are as follows todoitem standard tab properties see section model property toolbar and section property fields for the mod el below documentation standard tab see section documentation tab stereotype standard tab this contains a a list of the stereotypes applied to this model and a list of available stereotypes that may be applied to the model tagged values standard tab in the uml meta model model has the following standard tagged values defined derived from the superclass modelelement values true meaning the class is redundant it can be formally derived from other elements or false meaning it cannot derived models have their value in analysis to introduce useful names or concepts and in design to avoid re computation model property toolbar go up navigate up through the composition structure of the model since the model is the top package nothing can happen and this button is allways downlighted new package this creates a new package see section package within the model which appears on no diagram navigating immediately to the properties tab for that package tip while it can make sense to create packages of the model this way it is usually a lot clearer to create them within diagrams where you want them new datatype this creates a new datatype see section datatype within the model which appears on no diagram navigating immediately to the properties tab for that datatype new enumeration this creates a new enumeration see section enumeration within the model which ap pears on no diagram navigating immediately to the properties tab for that enumeration new stereotype this creates a new stereotype see section stereotype within the model navigating imme diately to the properties tab for that stereotype delete this tool is always downlighted since it is meaningless to delete the model property fields for the model name text box the name of the model the name of a model like all packages is by convention all lower case note the default name supplied to a new model by argouml untitledmodel is thus erroneous and guarantees that argouml always starts up with at least one problem being reported by the design critics stereotype drop down selector model is provided by default with the uml standard stereotypes for model systemmodel and metamodel and package facade framework stub stereotyping models is a useful thing although it is of limited value in argouml where you have only a single model navigate stereotype icon if a stereotype has been selected this will navigate to the stereotype property panel see section stereotype namespace text box records the namespace for the model this is the package hierarchy however since the model is at the top of the hierarchy in argouml this box is always empty visibility radio box with entries public private protected and package records the visibility for the model since argouml only permits one model this has no meaning ful use modifiers check box with entries abstract leaf and root abstract is used to declare that this model cannot be instantiated but must always be special ized the meaning of abstract applied to a model is not that clear it might mean that the model contains interfaces or abstract classes without realizations since argouml only permits one model this is not a meaningful box to check leaf indicates that this model can have no further subpackages while root indicates it is the top level model within argouml root only meaningfully applies to the model since all packages sit within the model in the absence of the toplevel stereotype this could be used to emphasize that the model is at the top level generalizations text area lists any model that generalizes this model note since there is only one model in argouml there is no sensible specialization or gen eralization that could be created specializations text box lists any specialized model i e for which this model is a generalization note since there is only one model in argouml there is no sensible specialization or gen eralization that could be created owned elements text area a listing of the top level packages classes interfaces datatypes actors use cases asso ciations generalizations and stereotypes within the model button double click on any of the model elements yields navigating to that model element datatype datatypes can be thought of as simple classes they have no attributes and any operations on them must have no side effects a useful analogy is primitive datatypes in a language like java the integer stands on its own it has no inner structure there are operations for example addition on the integers but when i perform the result is a new number and are unchanged by the exercise within uml datatype is a sub class of the classifier metaclass it embraces the predefined primitive types byte char double float int long and short the predefined enumeration boolean and user defined enumeration types note also void is implemented as a datatype within argouml within argouml new datatypes may be created using the new datatype button on the property tabs of the model and packages in which case the new datatype is restricted in scope to the package as well as the properties tab for datatype datatypes can also be created with the tool in the diagram toolbar of a class diagram the uml standard allows user defined datatypes to be placed on class diagrams to define their in heritence structure this is also possible in argouml it is represented on the diagram by a box with two compartments of which the top one is marked with datatype and contains the name the lower one contains operations datatype details tabs the details tabs that are active for datatypes are as follows todoitem standard tab properties see section datatype property toolbar and section property fields for data type below documentation standard tab see section documentation tab source standard tab unused one would expect a class declaration for the new datatype to support code generation tagged values standard tab in the uml metamodel datatype has the following standard tagged values defined persistence from the superclass classifier values transitory indicating state is destroyed when an instance is destroyed or persistent marking state is preserved when an instance is destroyed tip since user defined datatypes are enumerations they have no state to preserve and the value of this tagged value is irrelevant semantics from the superclass classifier the value is a specification of the se mantics of the datatype derived from the superclass modelelement values true meaning the class is redund ant it can be formally derived from other elements or false meaning it cannot tip while formally available a derived datatype does not have an obvious value and so datatypes should always be marked with derived false datatype property toolbar go up navigate up through the package structure new datatype this creates a new datatype see section class within the same package as the current datatype tip while it can make sense to create datatypes this way it can be clearer to create them within the package or model where you want them new enumeration this creates a new enumeration see section enumeration in the same package as the datatype navigating immediately to the properties tab for that enumeration new operation this creates a new operation within the datatype navigating immediately to the properties tab for that operation new stereotype this creates a new stereotype see section stereotype within the same package as the datatype navigating immediately to the properties tab for that stereotype delete this deletes the datatype from the model property fields for datatype name text box the name of the datatype the primitive datatypes all have lower case names but there is no formal convention note the default name supplied for a newly created datatype is the empty string data types with empty string names will appear with the name unnamed datatype in the explorer namespace drop down selector with navigate button allows changing the namespace for the datatype this is the package hierarchy modifiers check box with entries abstract leaf and root abstract is used to declare that this datatype cannot be instantiated but must always be spe cialized note argouml provides no mechanism for specializing datatypes so this check box is of little use leaf indicates that this datatype can have no further sub types while root indicates it is a top level datatype tip you can define the specialization of datatypes in a class diagram by drawing gen eralizations between them visibility radio box with entries public private protected and package records the visibility for the datatype client dependencies text area lists any elements that depend on this datatype caution it is not clear that dependencies between datatypes makes much sense supplier dependencies text area lists any elements that this datatype depends on caution it is not clear that dependencies between datatypes makes much sense generalizations text area lists any datatype that generalizes this datatype specializations text box lists any specialized datatype i e for which this datatype is a generalization operations text area lists all the operations defined on this datatype button double click navigates to the selected operation button click brings up a pop up menu with two entries move up only available where there are two or more operations and the operation selected is not at the top it is moved up one move down only available where there are two or more operations listed and the operation selected is not at the bottom it is moved down one see section operation for details of operations caution argouml treats all operations as equivalent any operations created here will use the same mechanism as operations for classes remember that operations on datatypes must have no side effects they are read only this means the query modifier must be checked for all operations enumeration an enumeration is a primitive datatype that can have a fixed short list of values it has no attributes and any operations on them must have no side effects a useful analogy is the primitive datatype boolean in a language like java the boolean stands on its own it has no inner structure there are operations for example logical xor on the booleans but when i perform true xor true the result is a new boolean and the original booleans true are unchanged by the exercise within uml enumeration is a sub class of the datatype metaclass the big difference with other datatypes is that an enumeration has enumerationliterals e g the enumeration boolean is defined as having enumerationliterals true and false within argouml new enumerations may be created using the new enumeration button on the property tabs of the model and packages in which case the new enumeration is restricted in scope to the package as well as the properties tab for datatype and enumeration enumerations can also be created with the tool in the diagram toolbar of a class diagram the uml standard allows user defined enumerations to be placed on class diagrams to define their inheritence structure this is also possible in argouml it is represented on the diagram by a box with three compartments of which the top one is marked with enumeration and contains the name the middle compartment shows the enumeration literals the lower one contains operations enumeration details tabs the details tabs that are active for enumerations are as follows todoitem standard tab properties see section enumeration property toolbar and section property fields for enumeration below documentation standard tab see section documentation tab presentation standard tab source standard tab stereotype standard tab the uml metamodel has the following stereotypes defined by default for a classifier which also apply to an enumeration metaclass from the superclass classifier powertype from the superclass classifier process from the superclass classifier thread from the superclass classifier utility from the superclass classifier tagged values standard tab in the uml metamodel enumeration has no standard tagged values defined enumeration property toolbar go up navigate up through the composition structure new datatype this creates a new datatype see section class within the same package as the current enumeration new enumeration this creates a new enumeration within the same namespace as the current enumeration navigating immediately to the properties tab for new enumeration new enumeration literal this creates a new enumeration literal within the enumeration navigating immediately to the prop erties tab for that literal new operation this creates a new operation within the enumeration navigating immediately to the properties tab for that operation new stereotype this creates a new stereotype see section stereotype within the same package as the enumeration navigating immediately to the properties tab for that stereotype delete from model this deletes the datatype from the model property fields for enumeration name text box the name of the enumeration the primitive enumerations all have lower case names but there is no formal convention note the default name supplied for a newly created datatype is the empty string enu merations with empty string names will appear with the name unnamed enumer ation in the explorer namespace drop down selector with navigation button allows changing the namespace for the enumeration this is the composition hierarchy modifiers check box with entries abstract leaf and root abstract is used to declare that this enumeration cannot be instantiated but must always be specialized leaf indicates that this enumeration can have no further sub types while root indicates it is a top level enumeration visibility radio box with entries public private protected and package records the visibility for the enumeration client dependencies text area lists any elements that depend on this enumeration button double click navigates to the selected modelelement button click brings up a pop up menu with following entry add this brings up a dialog box that allows to create dependencies from other modelele ments supplier dependencies text area lists any elements that this enumeration depends on button double click navigates to the selected modelelement button click brings up a pop up menu with the following entry add this brings up a dialog box that allows to create dependencies to other modelele ments generalizations text area lists any enumeration that generalizes this enumeration specializations text box lists any specialized enumerations i e for which this enumeration is a generalization operations text area lists all the operations defined on this enumeration button double click navigates to the selected operation button click brings up a pop up menu with two entries move up only available where there are two or more operations and the operation selected is not at the top it is moved up one move down only available where there are two or more operations listed and the operation selected is not at the bottom it is moved down one see section operation for details of operations caution argouml treats all operations as equivalent any operations created here will use the same mechanism as operations for classes remember that operations on enumerations must have no side effects they are read only this means the query modifier must be checked for all operations literals text area lists all the enumeration literals defined for this enumeration button double click nav igates to the selected literal button click brings up a pop up menu with two entries move up only available where there are two or more literals and the literal selected is not at the top it is moved up one move down only available where there are two or more literals listed and the literal selected is not at the bottom it is moved down one 5 enumeration literal an enumeration literal is one of the predefined values of an enumeration stereotype stereotypes are the main extension mechanism of uml providing a way to derive specializations of the standard metaclasses stereotype is a sub class of generalizableelement in the uml metamodel stereotypes are supplemented by constraints and tagged values new stereotypes are added from the property tab of almost any model element properties of existing stereotypes can be reached by selecting the property tab for any model element with that stereotype and using the navigate button within the property tab stereotype details tabs the details tabs that are active for stereotypes are as follows todoitem standard tab properties see section stereotype property toolbar and section property fields for ste reotype below documentation standard tab see section documentation tab stereotype standard tab warning here you can set stereotypes of stereotypes not a very usefull thing to do tagged values standard tab in the uml metamodel stereotype has the following standard tagged values defined derived from the superclass modelelement values true meaning the class is redund ant it can be formally derived from other elements or false meaning it cannot note this indicates any element with this stereotype has the derived tag set accord ingly caution tagged values for a stereotype are rather different to those for elements in the uml core architecture in that they apply to all model elements to which the stereotype is applied not just the stereotype itself stereotype property toolbar go up navigate up through the package structure of the model add stereotype this creates a new stereotype see section stereotype within the model which appears on no diagram navigating immediately to the properties tab for that stereotype new tag definition this creates a new tag definition see section tag definition within the model which ap pears on no diagram navigating immediately to the properties tab for that tagdefinition delete this deletes the stereotype from the model property fields for stereotype name text box the name of the stereotype there is no convention for naming stereotypes beyond start ing them with a lower case letter even the standard uml stereotypes vary between all lower case e g metamodel bumpy caps e g systemmodel and space separated e g object model note argouml does not enforce any naming convention for stereotypes base class drop down selector any stereotype must be derived from one of the metaclasses in the uml metamodel or the model element classes that derive from them the stereotype will then be avail able to model elements that derive from that same metaclass or that model element namespace drop down selector with navigation button records the namespace for the stereotype this is the package hierarchy modifiers check box with entries abstract leaf and root abstract is used to declare that model elements that use this stereotype cannot be instanti ated but must always be specialized leaf indicates that model elements that use this stereotype can have no further sub types while root indicates it is a top level model element caution remember that these modifiers apply to the model elements using the stereotype not just the stereotype warning argouml neither imposes nor checks that model elements using a stereotype adopt the stereotype modifiers visibility radio box with entries public private protected and package records the visibility for the stereotype generalizations text area lists any stereotype that generalizes this stereotype specializations text area lists any specialized stereotype i e for which this stereotype is a generalization tag definitions text area lists any tag definitions that are defined for this stereotype extended elements text area lists all modelelements that are stereotyped by this stereotype tag definition to be written diagram the uml standard specifies eight principal diagrams all of which are supported by argouml use case diagram used to capture and analyse the requirements for any ooa d project see chapter use case diagram model element reference for details of the argouml use case dia gram and the model elements it supports class diagram this diagram captures the static structure of the system being designed showing the classes interfaces and datatypes and how they are related variants of this diagram are used to show package structures within a system the package diagram and the relationships between particular instances the object diagram the argouml class diagram provides support for class and package diagrams see chapter class diagram model element reference for details of the model elements it supports the object diagram is suported on the deployment diagram behavior diagrams there are four such diagrams or strictly speaking five since the use case dia gram is a type of behavior diagram which show the dynamic behavior of the system at all levels statechart diagram used to show the dynamic behavior of a single object class instance this diagram is of particular use in systems using complex communication protocols such as in tele communications see chapter statechart diagram model element reference for details of the argouml statechart diagram and the model elements it supports activity diagram used to show the dynamic behavior of groups of objects class instance this diagram is an alternative to the statechart diagram and is better suited to systems with a great deal of user interaction see chapter activity diagram model element reference for details of the argouml activity diagram and the model elements it supports interaction diagrams there are two diagrams in this category used to show the dynamic inter action between objects class instances in the system sequence diagram shows the interactions typically messages or procedure calls between instances of classes objects and actors against a timeline particularly useful where the tim ing relationships between interactions are important see chapter sequence diagram model element reference for details of the argouml sequence diagram and the model ele ments it supports collaboration diagram shows the interactions typically messages or procedure calls between instances of classes objects and actors against the structural relationships between those instances particularly suitable where it is useful to relate interactions to the static struc ture of the system see chapter collaboration diagram model element reference for de tails of the argouml collaboration diagram and the model elements it supports implementation diagrams uml defines two implementation diagrams to show the relationship between the software components that make up a system the component diagram and the relation ship between the software and the hardware on which it is deployed at run time the deployment dia gram the argouml deployment diagram provides support for both component and deployment diagrams and additionally for object diagrams see chapter deployment diagram model element refer ence for details of the diagram and the model elements it supports diagrams are created using the create drop down menu see section the create menu or with the tools on the toolbar see section create operations or with the pop up menus in the ex plorer note argouml uses its deployment diagram to create the uml component deployment and object diagrams caution statechart and activity diagrams are associated with a particular class or operation or the latter also with a package and can only be created when this modelelement has been se lected warning in argouml version the uml object diagram as a variant of the class diagram is not directly supported however it is possible to create object diagrams within the argouml deployment diagram diagram details tabs the details tabs that are active for diagrams are as follows todoitem standard tab properties see section property fields for diagram below diagram property toolbar go up navigate up through the package structure of the model delete this deletes the diagram from the model as a consequence in case of a statechart diagram or an activity diagram all contained elements are deleted too property fields for diagram name the name of the diagram there are no conventions for naming diagrams by default argouml uses the space separated diagram name and a sequence number thus use case diagram tip this name is used to generate a filename when activating the save graphics menu item home model the home model of the diagram is not something defined in the uml specification the home model is the modelelement represented by the diagram hence its type depends on the type of dia gram e g it is the namespace represented by a class diagram or the statemachine in case of a state chart diagram chapter use case diagram model element reference introduction this chapter describes each model element that can be created within a use case diagram note that some sub model elements of model elements on the diagram may not actually themselves appear on the diagram there is a close relationship between this material and the properties tab of the details pane see sec tion properties tab that section covers properties in general in this chapter they are linked to specific model elements figure typical model elements on a use case diagram shows a use case diagram with all typic al model elements displayed figure typical model elements on a use case diagram argouml limitations concerning use case dia grams use case diagrams are now well supported within argouml there still are some minor limitations though especially with extension points note earlier versions of argouml and earlier implemented extend and include relation ships by using a stereotyped dependency relationship although such diagrams will show correctly on the diagram they will not link correctly to the use cases and should be re placed by proper extend and include relationships using the current system actor an actor represents any external entity human or machine that interacts with the system providing in put receiving output or both within the uml metamodel actor is a sub class of classifier the actor is represented by a stick man figure on the diagram see figure typical model ele ments on a use case diagram actor details tabs the details tabs that are active for actors are as follows todoitem standard tab properties see section actor property toolbar and section property fields for actor be low documentation standard tab see section documentation tab presentation standard tab the fill color is used for the stick man head source standard tab usually no code is provided for an actor since it is external to the system stereotype standard tab tagged values standard tab in the uml metamodel actor has the following standard tagged values defined persistence from the superclass classifier values transitory indicating state is destroyed when an instance is destroyed or persistent marking state is preserved when an instance is destroyed tip actors sit outside the system and so their internal behavior is of little concern and this tagged value is best ignored semantics from the superclass classifier the value is a specification of the se mantics of the actor derived from the superclass modelelement values true meaning the actor is redund ant it can be formally derived from other elements or false meaning it cannot note derived actors have limited value since they sit outside the system being de signed they may have their value in analysis to introduce useful names or con cepts checklist standard tab for a classifier actor property toolbar go up navigate up through the package structure of the model add actor this creates a new actor within the model but not within the diagram navigating immediately to the properties tab for that actor tip this method of creating a new actor may be confusing it is much better to create an actor on the diagram new reception this creates a new reception within the model but not within the diagram navigating immediately to the properties tab for that rception tip a reception is a declaration that the actor handles a signal but the actual handling is specified by a state machine delete this deletes the selected actor from the model warning this is a deletion from the model not just the diagram to delete an actor from the dia gram but keep it within the model use the main menu remove from diagram or press the delete key property fields for actor name text box the name of the actor the diagram shows this name below the stick man figure since an actor is a classifier it would be conventional to capitalize the first letter and initial letters of any component words e g remotesensor note argouml does not enforce any naming convention for actors namespace text box with navigation button records the namespace for the actor this is the package hier archy modifiers check box with entries abstract leaf and root abstract is used to declare that this actor cannot be instantiated but must always be special ized caution while actors can be specialized and generalized it is not clear that an abstract act or has any meaning perhaps it might be used to indicate an actor that does not it self interact with a use case but whose children do leaf indicates that this actor can have no further children while root indicates it is a top level actor with no parent generalizations text area lists any actor that generalizes this actor button double click navigates to the generalization and opens its property tab specializations text box lists any specialized actor i e for which this actor is a generalization the specialized actors can communicate with the same use case instances as this actor button double click navigates to the generalization and opens its property tab association ends text area lists any association ends of associations connected to this actor button double click navigates to the selected entry use case a use case represents a complete meaningful chunk of activity by the system in relation to its external users actors human or machine it represents the primary route through which requirements are cap tured for the system under construction within the uml metamodel use case is a sub class of classifier the use case icon is an oval see figure typical model elements on a use case diagram it may be split in two with the lower compartment showing extension points caution by default argouml does not show the extension point compartment it may be revealed by the context sensitive show menu using button click or from the presentation tab use case details tabs the details tabs that are active for use cases are as follows todoitem standard tab properties see section use case property toolbar and section property fields for use case below documentation standard tab see section documentation tab presentation standard tab the fill color is used for the use case oval the display extension points check box is used to control whether an extension point compartment is displayed source standard tab it would not be usual to provide any code for a use case since it is primarily a vehicle for capturing requirements about the system under construction not creating the solution stereotype standard tab tagged values standard tab in the uml metamodel usecase has the following standard tagged values defined persistence from the superclass classifier values transitory indicating state is destroyed when an instance is destroyed or persistent marking state is preserved when an instance is destroyed tip in general the instantiation of use cases is not a major aspect of any design method they are mostly concerned with requirements capture for most ooa d method ologies this tag can safely be ignored semantics from the superclass classifier the value is a specification of the se mantics of the use case derived from the superclass modelelement values true meaning the use case is re dundant it can be formally derived from other elements or false meaning it cannot note derived use cases still have their value in analysis to introduce useful names or concepts checklist standard tab for a classifier use case property toolbar go up navigate up through the package structure of the model new use case this creates a new use case within the model but not within the diagram and shows immediately the properties tab for that use case tip this method of creating a new use case can be confusing it is much better to create a new use case on the diagram of your choice new extension point this creates a new use extension point within the namespace of the current use case with the cur rent use case as its associated use case navigating immediately to the properties tab for that exten sion point new attribute this creates a new attribute within the current use case navigating immediately to the properties tab for that attribute new operation this creates a new operation within the current use case navigating immediately to the properties tab for that operation new reception this creates a new reception within the current use case navigating immediately to the properties tab for that reception new stereotype this creates a new stereotype within the current use case navigating immediately to the properties tab for that stereotype delete this deletes the selected use case from the model warning this is a deletion from the model not just the diagram to delete a use case from the diagram but keep it within the model use the main menu remove from diagram or press the delete key property fields for use case name text box the name of the use case since a use case is a classifier it would be conventional to capitalize the first letter and initial letters of any component words e g remotesensor the name is shown inside the oval representation of the use case on the diagram note argouml does not enforce any naming convention for use cases namespace text box with navigation button records the namespace for the use case this is the package hier archy modifiers check box with entries abstract leaf and root abstract is used to declare that this actor cannot be instantiated but must always be special ized leaf indicates that this use case can have no further children while root indicates it is a top level use case with no parent client dependencies text area lists the depending ends of the relationship i e the end that makes use of the other end button double click navigates to the dependency and opens its property tab button click shows a pop up menu with one entry add that opens a dialog box where you can add and remove depending modelelements supplier dependencies text area lists the supplying ends of the relationship i e the end supplying what is needed by the other end button double click navigates to the dependency and opens its property tab button click shows a pop up menu with one entry add that opens a dialog box where you can add and remove dependent modelelements generalizations text area lists use cases which are generalizations of this one will be set whenever a generaliza tion is created on the from this use case button double click on a generalization will navigate to that generalization specializations text box lists any specialized use case i e for which this use case is a generalization button double click navigates to the generalization and opens its property tab extends text box lists any class that is extended by this use case where an extends relationship has been created button double click will navigate to that relation ship includes text box lists any use case that this use case includes where an include relationship has been created button double click will navigate to that relation ship attributes text area lists all the attributes see section attribute defined for this use case button double click navigates to the selected attribute button gives a pop up menu with two entries which allow reordering the attributes move up only available where there are two or more attributes listed and the attribute selec ted is not at the top it moves the attribute up one position move down only available where there are two or more attributes listed and the attribute se lected is not at the bottom it moves the attribute down one position association ends text box lists any association ends see section association of associations connected to this use case button double click navigates to the selected entry operations text area lists all the operations see section operation defined on this use case button click navigates to the selected operation button gives a pop up menu with two entries which al low reordering the operations move up only available where there are two or more operations listed and the operation se lected is not at the top it moves the operation up one position move down only available where there are two or more operations listed and the operation selected is not at the bottom it moves the operation down one position extension points text box if this use case is or can be extended this field lists the extension points for the use case note extension points are listed by their location point rather than their name where an extension point has been created see below button double click will navigate to that relationship button gives a pop up menu with the following entries new add a new extension point and navigate to it making this use case the owning use case of the extension point move up only available where there are two or more extension points listed and the exten sion point selected is not at the top it moves the extension point up one position move down only available where there are two or more extension points listed and the exten sion point selected is not at the bottom it moves the extension point down one position extension point an extension point describes a point in a use case where an extending use case may provide additional behavior examples for a travel agent sales system might be the use case for paying for a ticket which has an ex tension point in the specification of the payment extending use cases may then extend at this point to pay by cash credit card etc within the uml metamodel extension point is a sub class of modelelement a use case may display an extension point compartment see section use case for details in which extension points are shown with the following syntax name location extension point details tabs the details tabs that are active for extension points are as follows todoitem standard tab properties see section extension point property toolbar and section property fields for extension point below documentation standard tab see section documentation tab stereotype standard tab extensionpoints do not have any stereotypes defined by default tagged values standard tab in the uml metamodel extensionpoint has the following standard tagged val ues defined derived from the superclass modelelement values true meaning the extension point is redundant it can be formally derived from other elements or false meaning it cannot note it is not clear how derived extension points could have any value in analysis extension point property toolbar go up navigate up to the use case which owns this extension point new extension point this creates a new extension point below the selected extension point navigating immediately to the properties tab of the newly created extension point new stereotype this creates a new stereotype see section stereotype for the selected extension point navigating immediately to the properties tab for that stereotype delete this deletes the selected extension point from the model property fields for extension point name text box the name of the extension point tip it is quite common to leave extension points unnamed in use case analysis since they are always listed within use cases and extend relationships by their location note argouml does not enforce any naming convention for extension points location text box a description of the location of this extension point within the owning use case tip extension points are always listed within use cases and extend relationships by their location typically this will be the number name of the paragraph in the specification base use case text box shows the base use case within which this extension point is defined button double click will navigate to the use case extend text box lists all use cases which extend the base use case through this extension point where an extending use case exists button double click will navigate to that relationship 5 association an association on a use case diagram represents a relationship between an actor and a use case showing that actor involvement in the use case the invocation of the use case will involve some significant change perceived by the actor associations are described fully under class diagrams see section association association end association ends are described under class diagrams see section association end dependency dependencies are described under class diagrams see section dependency caution dependency has little use in use case diagrams it is provided because earlier versions of argouml used it incorrectly to implement include and extends relationships generalization generalization is a relationship between two use cases or two actors where a is a generalization of b it means a describes more general behavior and b a more specific version of that behavior examples for a travel agent sales system might be the use case for making a booking as a generalization of the use case for making a flight booking and a salesman actor being a generalization of a supervisor actor since supervisors can also act as salesmen but not vice versa generalization is analogous to class inheritance within oo programming note it is easy to confuse extends relationships between use cases with generalization however extends is about augmenting a use case behavior at a specific point generalization is about specializing the behavior throughout the use case within the uml metamodel generalization is a sub class of relationship generalization is represented as an arrow with white filled head from the specialized use case or actor to the generalized use case or actor see figure typical model elements on a use case diagram generalization details tabs the details tabs that are active for associations are as follows todoitem standard tab properties see section generalization property toolbar and section property fields for generalization below documentation standard tab see section documentation tab presentation standard tab note the values in the bounds field of the generalization are not editable since they are determined by the properties of the endpoints of the line stereotype standard tab tagged values standard tab in the uml metamodel generalization has the following standard tagged val ues defined derived from the superclass modelelement values true meaning the generalization is redundant it can be formally derived from other elements or false meaning it cannot note derived generalizations still have their value in analysis to introduce useful names or concepts and in design to avoid re computation generalization property toolbar go up navigate up through the package structure of the model for a generalization this will be the pack age containing the generalization new stereotype this creates a new stereotype see section stereotype for the selected generalization nav igating immediately to the properties tab for that generalization delete this deletes the selected generalization from the model warning this is a deletion from the model not just the diagram to delete a generalization from the diagram but keep it within the model use the main menu remove from dia gram or press the delete key property fields for generalization name text box the name of the generalization textfeld der name der generalisierung tip it is quite common to leave generalizations unnamed in use case analysis note argouml does not enforce any naming convention for associations note there is no representation of the name of a generalization on the diagram discriminator diskriminator text box the name of a discriminator for the specialization uml allows grouping of specializ ations into a number of sets on the basis of this value tip the empty string is a valid entry and the default for this field the discriminator is only of practical use in cases of multiple inheritance a class diagram example is shown in figure example use of a discriminator with generalization here each type of user should inherit from two sorts of user one distinguishing between local or remote user which can be identified by one discriminator and one indicating their function as a user identified by a different discriminator there is little point in using this within a use case diagram namespace text box with navigation button records the namespace for the generalization this is the package hierarchy parent text box shows the use case or actor that is the parent in this relationship i e the more general end of the relationship button double click on this entry will navigate to that use case or actor child text box shows the use case or actor that is the child in this relationship i e the more specific end of the relationship button double click on this entry will navigate to that use case or actor powertype drop down selector providing access to all standard uml types provided by argouml and all new classes created within the current model this is the type of the child entity of the generalization tip this can be ignored for use case analysis the only sensible value to put in would be the child use case type as a classifier this appears in the drop down list figure example use of a discriminator with generalization extend extend is a relationship between two use cases where a extends b it means a describes some addi tional behavior that is executed conditionally under exceptional circumstances at some point during the normal behavior of b in some respects extend is like generalization however the key difference is that the extended use case defines extension points see section extension point which are the only places where its be havior may be extended the extending use case must define at which of these extension points it adds behavior this makes the use of extend more tightly controlled than general extension and it is thus preferred wherever possible examples for a travel agent sales system might be the use case for paying for a ticket which has an ex tension point in the specification of the payment extending use cases may then extend at this point to pay by cash credit card etc within the uml metamodel extend is a sub class of relationship an extend relationship is represented as a dotted link with an open arrow head and a label extend if a condition is defined it is shown under the extend label see figure typical model ele ments on a use case diagram extend details tabs the details tabs that are active for extend relationships are as follows note there is no source tab since there is no source code that could be generated for an extend relationship todoitem standard tab properties see section extend property toolbar and section property fields for extend below documentation standard tab see section documentation tab presentation standard tab note the values in the bounds field of the extend are not editable since they are determ ined by the properties of the endpoints of the line stereotype standard tab tagged values standard tab in the uml metamodel extend has the following standard tagged values defined derived from the superclass modelelement values true meaning the extend relation ship is redundant it can be formally derived from other elements or false meaning it cannot note derived extend relationships could have their value in analysis to introduce useful names or concepts extend property toolbar go up navigate up through the package structure of the model for a extend this will be the package con taining the extend new extension point this creates a new use case extension point within the namespace of the current extend relationship with the current extend relationship as its first extending relationship tip while it is perfectly valid to create extension points from an extend relationship the created extension point will have no associated use case it can subsequently be set up it would be more usual to instead create the extension point within a use case and sub sequently link to it from an extend relationship see section property fields for extend below new stereotype this creates a new stereotype see section stereotype for the selected extent relationship navigating immediately to the properties tab for that stereotype delete this deletes the selected extend relationship from the model warning this is a deletion from the model not just the diagram to delete a extend from the diagram but keep it within the model use the main menu remove from diagram or press the delete key property fields for extend name text box the name of the extend relationship tip it is quite common to leave extends unnamed in use case analysis note argouml does not enforce any naming convention for extend relationships namespace text box records the namespace for the extend relationship this is the package hierarchy button double click on the entry will navigate to the package defining this namespace or the model for the top level namespace base use case text box shows the use case that is being extended by this extend relationship button double click on this entry will navigate to the base use case extension text box show the use case that is doing the extending through this extend relationship button double click on this entry will navigate to the extension use case extension points text box lists the extension points of the base use case where the extension will be applied if the condition holds note if the condition is fulfilled the sequence obeyed by the use case instance is extended to include the sequence of the extending use case the different parts of the extending use case are inserted at the locations defined by the sequence of extension points in the relationship one part at each referenced extension point note that the condition is only evaluated once at the first referenced extension point and if it is fulfilled all of the extending use case is inserted in the original sequence hence the sequence of the extension points is irrelevant except for the position of the first one since that one determines where the condition is evaluated where an extension point has been created button double click will navigate to that relationship button gives a pop up menu with the following entries add the ad remove extensionpoints window opens in this window it is possible to build a list of extension points new add a new extension point in the list and navigate to it the current extend relationship is added as the first in list of extending relationships of the new extension point move up only available where there are two or more extension points listed and the exten sion point selected is not at the top it moves the extension point up one position move down only available where there are two or more extension points listed and the exten sion point selected is not at the bottom it moves the extension point down one position condition text area multi line textual description of any condition attached to the extend relationship the text entered here is shown on the diagram include include is a relationship between two use cases where a includes b it means b described behavior that is to be included in the description of the behavior of a at some point defined internally by a examples for a travel agent sales system might be the use case for booking travel which includes use cases for booking flights and taking payment within the uml metamodel include is a sub class of relationship an include relationship is represented as a dotted link with an open arrow head and a label include see figure typical model elements on a use case diagram include details tabs the details tabs that are active for include relationships are as follows note there is no source tab since there is no source code that could be generated for an include relationship todoitem standard tab properties see section include property toolbar and section property fields for in clude below documentation standard tab see section documentation tab presentation standard tab note the values in the bounds field of the include relationships are not editable since they are determined by the properties of the endpoints of the line tagged values standard tab in the uml metamodel include has the following standard tagged values defined derived from the superclass modelelement values true meaning the include rela tionship is redundant it can be formally derived from other elements or false meaning it can not note derived include relationships could have their value in analysis to introduce useful names or concepts include property toolbar go up navigate up through the package structure of the model for a include this will be the package con taining the include new stereotype this creates a new stereotype see section stereotype for the selected include relation ship navigating immediately to the properties tab for that stereotype delete this deletes the selected include relationship from the model warning this is a deletion from the model not just the diagram to delete a include from the diagram but keep it within the model use the main menu remove from diagram or press the delete key 10 property fields for include name text box the name of the include relationship tip it is quite common to leave include relationships unnamed in use case analysis note argouml does not enforce any naming convention for include relationships namespace text box records the namespace for the include this is the package hierarchy button click on the entry will navigate to the package defining this namespace or the model for the top level namespace base use case drop down selector records the use case that is doing the including in this include relationship button click on this entry will give a drop down menu of all available use cases which may be se lected by button click included use case drop down selector records the use case that is being included by this include relationship button click on this entry will give a drop down menu of all available use cases and an empty entry which may be selected by button click chapter class diagram model element reference introduction this chapter describes each model element that can be created within a class diagram note that some sub model elements of model elements on the diagram may not actually themselves appear on the dia gram class diagrams are used for only one of the uml static structure diagrams the class diagram itself ob ject diagrams are represented on the argouml deployment diagram in addition argouml uses the class diagram to show model structure through the use of packages there is a close relationship between this material and the properties tab of the details pane see sec tion properties tab that section covers properties in general in this chapter they are linked to specific model elements figure possible model elements on a class diagram shows a class diagram with all possible model elements displayed figure possible model elements on a class diagram figure possible model elements on a package diagram shows a package diagram with all pos sible model elements displayed figure possible model elements on a package diagram figure possible model elements on a datatype diagram shows a datatype diagram with a data type and an enumeration displayed figure possible model elements on a datatype diagram figure possible model elements on a stereotype definition diagram shows a stereotype defini tion diagram with all possible model elements displayed figure possible model elements on a stereotype definition diagram limitations concerning class diagrams in argouml various limitations exist in of argouml for stereotype definition diagrams e g the implementa tion does not allow stereotype compartments to be shown on stereotype definition diagrams another variant of the class diagram within the uml standard is the object diagram there is currently no support for objects or links within argouml class diagrams instead the argouml deployment dia gram does have both objects and links and can be used to draw object diagrams package the package is the main organizational model element within argouml in the uml metamodel it is a sub class of both namespace and generalizableelement note argouml also implements the uml model model element as a sub class of package but not the subsystem model element argouml also implements some less common aspects of uml model management in particular the re lationship uml defines as generalization and the sub class dependency permission for use between packages package details tabs the details tabs that are active for packages are as follows todoitem standard tab standard register properties see section package property toolbar and section property fields for package below documentation standard tab see section 4 documentation tab presentation standard tab the editable bounds field defines the bounding box for the package on the diagram stereotype standard tab tagged values standard tab in the uml metamodel package has the following standard tagged values defined derived from the superclass modelelement values true meaning the package is re dundant it can be formally derived from other elements or false meaning it cannot note derived packages still have their value in analysis to introduce useful names or concepts and in design to avoid re computation package property toolbar go up navigate up through the package structure new package this creates a new package within the package which appears on no diagram navigating immedi ately to the properties tab for that package new datatype this creates a new datatype see section 3 datatype for the selected package navigating immediately to the properties tab for that datatype new enumeration this creates a new enumeration see section 4 enumeration for the selected package nav igating immediately to the properties tab for that enumeration new stereotype this creates a new stereotype see section stereotype for the selected package navigating immediately to the properties tab for that stereotype new tag definition this creates a new tag definition see section tag definition within the package which appears on no diagram navigating immediately to the properties tab for that tagdefinition delete package deletes the package from the model warning this is a deletion from the model not just the diagram to delete a package from the diagram but keep it within the model use the main menu remove from diagram or press the delete key 3 property fields for package name text box the name of the package the name of a package like all packages is by convention all lower case not containing any punctuation marks note by default a new package has no name defined the package will appear with the name unnamed package in the explorer namespace drop down selector records the namespace for the package this is the package hierarchy visibility radio box with four entries public private protected and package indicates whether the package is visible outside the package modifiers check box with entries abstract leaf and root abstract is used to declare that this package cannot be instantiated but must always be spe cialized tip the meaning of abstract applied to a package if not that clear it might mean that the package contains interfaces or abstract classes without realizations this is probably better handled through stereotyping of the package for example facade leaf indicates that this package can have no further subpackages root indicates that it is the top level package tip within argouml root only meaningfully applies to the model since all pack ages sit within the model this could be used to emphasize that the model is at the top level generalizations text area lists any package that generalizes this package button double click navigates to the generalization and opens its property tab specializations text box lists any specialized package i e for which this package is a generalization button double click navigates to the generalization and opens its property tab owned elements text area a listing of all the packages classes interfaces datatypes actors use cases associations generalizations stereotypes etc within the package button double click on any item listed here navigates to that model element imported elements text area a listing of all imported elements i e elements that are owned by a different package but are explicitely made visible in this package button double click on any item listed here navigates to that model element button gives a pop up menu with the following entries add the add remove imported elements window opens in this window it is possible to build a list of imported elements remove removes the import 3 datatype datatypes are not specific to packages or class diagrams and are discussed within the chapter on top level model elements see section 3 datatype 4 enumeration enumeration are not specific to packages or class diagrams and are discussed within the chapter on top level model elements see section 4 enumeration 5 stereotype stereotypes are not specific to packages or class diagrams and are discussed within the chapter on top level model elements see section stereotype class the class is the dominant model element on a class diagram in the uml metamodel it is a sub class of classifier and generalizableelement a class is represented on a class diagram as a rectangle with three compartments the top compartment displays the class name and stereotypes the second compartment any attributes and the third any oper ations these last two compartments may optionally be hidden class details tabs the details tabs that are active for classes are as follows todoitem standard tab properties see section class property toolbar and section 3 property fields for class be low documentation standard tab see section 4 documentation tab presentation standard tab the tick boxes attributes and operations allow the attributes and operations compartments to be shown the default or hidden this is a setting valid for only the current dia gram that shows the class the editable bounds field defines the bounding box for the package on the diagram source standard tab this contains a template for the class declaration and declarations of associated classes constraints standard tab there are no standard constraints defined for class within the uml metamodel stereotypes standard tab tagged values standard tab in the uml metamodel class has the following standard tagged values defined persistence from the superclass classifier values transitory indicating state is destroyed when an instance is destroyed or persistent marking state is preserved when an instance is destroyed semantics from the superclass classifier the value is a specification of the se mantics of the class derived from the superclass modelelement values true meaning the class is redund ant it can be formally derived from other elements or false meaning it cannot note derived classes still have their value in analysis to introduce useful names or con cepts and in design to avoid re computation note the uml element metaclass from which all other model elements are derived in cludes the tagged element documentation which is handled by the documentation tab under argouml checklist standard tab for a classifier class property toolbar go up navigate up through the package structure new attribute this creates a new attribute see section attribute within the class navigating immedi ately to the properties tab for that attribute new operation this creates a new operation see section operation within the class navigating immedi ately to the properties tab for that operation new reception this creates a new reception navigating immediately to the properties tab for that reception new inner class this creates a new inner class which appears on no diagram within the class this belongs to the class and is restricted to the namespace of the class it exactly models the java concept of inner class as an inner class it needs no attributes or operations since it shares those of its owner note inner class is not a separate concept in uml this is a convenient shorthand for creat ing a class that is restricted to the namespace of its owning class new class this creates a new class which appears on no diagram within the same namespace as the current class new stereotype this creates a new stereotype see section 6 stereotype for the selected class navigating immediately to the properties tab for that stereotype delete this deletes the class from the model warning this is a deletion from the model not just the diagram to delete a class from the dia gram but keep it within the model use the main menu remove from diagram or press the delete key 6 3 property fields for class name text box the name of the class the name of a class has a leading capital letter with words separ ated by bumpy caps note the argouml critics will complain about class names that do not have an initial cap ital namespace drop down selector with navigation button records and allows setting of the namespace for the class this is the package hierarchy button click on the entry will move the class to the selected namespace modifiers check box with entries abstract leaf root and active abstract is used to declare that this class cannot be instantiated but must always be sub classed the name of an abstract class is displayed in italics on the diagram caution if a class has any abstract operations then it should be declared abstract argouml will not enforce this leaf indicates that this class cannot be further subclassed while root indicates it can have no superclass it is possible for a class to be both abstract and leaf since its static operations may still be referenced active indicates that this class exhibits dynamic behavior and is thus associated with a state or activity diagram visibility radio box with four entries public private protected and package indicates whether the class is visible outside the namespace client dependencies text area lists the depending ends of the relationship i e the end that makes use of the other end button double click navigates to the dependency and opens its property tab button click shows a pop up menu with one entry add that opens a dialog box where you can add and remove depending modelelements supplier dependencies text area lists the supplying ends of the relationship i e the end supplying what is needed by the other end button double click navigates to the dependency and opens its property tab button click shows a pop up menu with one entry add that opens a dialog box where you can add and remove dependent modelelements generalizations text area lists any class that generalizes this class button double click navigates to the generalization and opens its property tab specializations text box lists any specialized class i e for which this class is a generalization button double click navigates to the generalization and opens its property tab attributes text area lists all the attributes see section attribute defined for this class button double click navigates to the selected attribute button gives a pop up menu with two entries which allow reordering the attributes move up only available where there are two or more attributes listed and the attribute selec ted is not at the top it moves the attribute up one position move down only available where there are two or more attributes listed and the attribute se lected is not at the bottom it moves the attribute down one position association ends text box lists any association ends see section association of associations connected to this class button double click navigates to the selected entry operations text area lists all the operations see section operation defined on this class button click navigates to the selected operation button gives a pop up menu with two entries which al low reordering the operations move up only available where there are two or more operations listed and the operation se lected is not at the top it moves the operation up one position move down only available where there are two or more operations listed and the operation selected is not at the bottom it moves the operation down one position owned elements text area a listing of model elements contained within the classes namespace this is where any inner class see section 6 class property toolbar will appear button double click on any of the model elements navigates to that model element tip most namespace hierarchies should be managed through the package mechanism namespace hierarchies through classes are best restricted to inner classes conceivable datatypes signals and interfaces could also appear here but actors and use cases would seem of no value attribute attribute is a named slot within a class or other classifier describing a range of values that may be held by instances of the class in the uml metamodel it is a sub class of structuralfeature which is itself a sub class of feature an attribute is represented in the diagram on a single line within the attribute compartment of the class its syntax is as follows visibility attributename type initialvalue visibility is or corresponding to public protected private or package visibility re spectively attributename is the actual name of the attribute being declared type is the type uml datatype class or interface declared for the attribute initialvalue is any initial value to be given to the attribute when an instance of the class is created this may be overridden by any constructor operation in addition any attribute declared static will have its whole entry underlined on the diagram attribute details tabs the details tabs that are active for attributes are as follows todoitem standard tab properties see section 7 attribute property toolbar and section 7 3 property fields for attrib ute below documentation standard tab see section 4 documentation tab constraints standard tab there are no standard constraints defined for attribute within the uml metamod el stereotype standard tab tagged values standard tab in the uml metamodel attribute has the following standard tagged values defined transient volatile this is an argouml extension to the uml 4 standard to indicate that this attrib ute is realized in some volatile form for example it will be a memory mapped control register note the uml element metaclass from which all other model elements are derived in cludes the tagged element documentation which is handled by the documentation tab under argouml checklist standard tab for a attribute 7 attribute property toolbar go up navigate up through the package structure go to previous navigate to the previous attribute of the class that owns them this button is downlighted if the cur rent attribute is the first one go to next navigate to the next attribute of the class that owns them this button is downlighted if the current attribute is the last one new attribute this creates a new attribute within the owning class of the current attribute navigating immediately to the properties tab for that attribute tip this is a very convenient way to add a number of attributes one after the other to a class new datatype this creates a new datatype see section 3 datatype for the selected attribute navigating immediately to the properties tab for that datatype new enumeration this creates a new enumeration see section 4 enumeration for the package that owns the class navigating immediately to the properties tab for that enumeration new stereotype this creates a new stereotype see section 6 stereotype for the selected attribute navigat ing immediately to the properties tab for that stereotype delete this deletes the attribute from the model warning this is a deletion from the model not just the diagram if desired the whole attribute compartment can be hidden on the diagram using the style tab see section 7 attribute property toolbar or the button pop up menu for the class on the dia gram 7 3 property fields for attribute name text box the name of the attribute the name of a attribute has a leading lower case letter with words separated by bumpy caps note the argouml critics will complain about attribute names that do not have an initial lower case letter owner text box records the class which contains this attribute button double click on the entry will navigate to the class multiplicity editable drop down selector with checkmark the default value is that there is one instance of this attribute for each instance of the class i e it is a scalar the drop down provides a number of commonly used specifications for non scalar attributes when the checkmark is unchecked then the multiplicity remains undefind in the model and the drop down selector is downlighted note argouml presents a number of predefined ranges for multiplicity for easy access the user may also enter any user defined range that follows the uml syntax such as 3 7 10 the value is equivalent to the default exactly one scalar instance the selec tion indicates an optional scalar attribute visibility radio box with entries public private protected and package public the attribute is available to any model element that can see the owning class private the attribute is available only to the owning class and any inner classes protected the attribute is available only to the owning class or model elements that are subclasses of the owning class package the attribute is available only to model elements contained in the same package changeability radio box with entries addonly changeable and frozen addonly meaningful only if the multiplicity is not fixed to a single value additional values may be added to the set of values but once created a value may not be removed or altered changeable there are no restrictions of modification frozen also named immutable the value of the attribute may not change during the life time of the owner class the value must be set at object creation and may never change after that this implies that there is usually an argument for this value in a constructor and that there is no operation that updates this value modifiers check box for static if unchecked the defaults then the attribute has instance scope if checked then the attribute is static i e it has class scope static attributes are indicated on the diagram by underlining type drop down selector with navigation button the type of this attribute this can be any uml clas sifier although in practice only class datatype or interface make any sense pressing the navigation button will navigate to the property panel for the currently selected type see section 6 class section 3 datatype and section interface note a type must be declared it can be void by default argouml supplies int as the type initial value text box with compartments this allows you to set an initial value for the attribute if desired this is optional the drop down menu provides access to the common values 2 and null the left hand side of this field contains the body of the expression that forms the initial value the right hand side defines the language in which the expression is written hovering the mouse pointer over these fields reveals a tooltip body or language to help re member which is which caution any constructor operation may ignore this initial value operation an operation is a service that can be requested from an object to effect behavior in the uml metamodel it is a sub class of behavioralfeature which is itself a sub class of feature in the diagram an operation is represented on a single line within the operation compartment of the class its syntax is as follows visibility name parameter list return type expression property string you can edit this line directly in the diagram by double clicking on it all elements are optional and if left unspecified the old values will be preserved a stereotype can be given between any two elements in the line in the format stereotype the following properties are recognized to have special meaning abstract concurrency concurrent guarded leaf query root and sequential the visibility is or corresponding to public protected private visibility or pack age visibility respectively static and final optionally appear if the operation has those modifiers any operation declared static will have its whole entry underlined on the diagram there may be zero or more entries in the parameter list separated by commas every entry is a pair of the form name type the return type expression is the type uml datatype class or interface of the result returned finally the whole entry is shown in italics if the operation is declared abstract operation details tabs the details tabs that are active for operations are as follows todoitem standard tab properties see section 2 operation property toolbar and section 8 3 property fields for opera tion below documentation standard tab see section 4 documentation tab presentation standard tab the bounds field does allow editing but the changes have no effect source standard tab this contains a declaration for the operation constraints standard tab there are no standard constraints defined for operation within the uml metamod el tagged values standard tab in the uml metamodel operation has the following standard tagged values defined semantics the value is a specification of the semantics of the operation derived from the superclass modelelement values true meaning the operation is re dundant it can be formally derived from other elements or false meaning it cannot note derived operations still have their value in analysis to introduce useful names or concepts and in design to avoid re computation note the uml element metaclass from which all other model elements are derived in cludes the tagged element documentation which is handled by the documentation tab under argouml checklist standard tab for an operation 8 2 operation property toolbar go up navigate up through the package structure new operation this creates a new operation within the owning class of the current operation navigating immedi ately to the properties tab for that operation tip this is a very convenient way to add a number of operations one after the other to a class new parameter this creates a new parameter for the operation navigating immediately to the properties tab for that parameter new raised signal this creates a new raised signal for the operation navigating immediately to the properties tab for that raised signal new datatype this creates a new datatype see section 3 datatype in the namespace of the owner of the operation navigating immediately to the properties tab for that datatype new stereotype this creates a new stereotype see section 6 stereotype for the selected operation navigat ing immediately to the properties tab for that stereotype delete this deletes the operation from the model warning this is a deletion from the model not just the diagram if desired the whole operation compartment can be hidden on the diagram using the presentation tab see sec tion 8 2 operation property toolbar or the button 2 pop up menu for the class on the diagram 8 3 property fields for operation name text box the name of the operation the name of an operation has a leading lower case letter with words separated by bumpy caps note the argouml critics will complain about operation names that do not have an initial lower case letter tip if you wish to follow the java convention of constructors having the same name as the class you will violate this rule silence the critic by setting the stereotype create for the constructor operation stereotype drop down selector there are two uml standard stereotypes for operation from the parent metaclass behavioralfeature create and destroy tip you should use create as the stereotype for constructors and destroy for de structors which are called finalize methods under java navigate stereotype icon if a stereotype has been selected clicking button will navigate to the stereotype prop erty panel see section 5 stereotype owner text box records the class which contains this operation button double click on the entry will navigate to the class visibility radio box with entries public private protected and package public the operation is available to any model element that can see the owning class private the operation is available only to the owning class and any inner classes protected the operation is available only to the owning class or model elements that are subclasses of the owning class package the operation is available only model elements contained in the same package modifiers check box with entries abstract leaf root query and static abstract this operation has no implementation with this class the implementation must be provided by a subclass important any class with an abstract operation must itself be declared abstract leaf the implementation of this operation must not be overridden by any subclass root the declaration of this operation must not override a declaration of the operation from a superclass query this indicates that the operation must have no side effects i e it must not change the state of the system it can only return a value caution operations for user defined datatypes must always check this modifier static there is only one instance of this operation associated with the class as opposed to one for each instance of the class this is the ownerscope attribute of a feature metaclass within uml any operation declared static is shown underlined on the class diagram concurrency radio box with entries guarded sequential and concurrent guarded multiple calls from concurrent threads may occur simultaneously to one instance on any guarded operation but only one is allowed to commence the others are blocked until the performance of the first operation is complete caution it is up to the system designer to ensure that deadlock cannot occur it is the re sponsibility of the operation to implement the blocking behavior as opposed to the system sequential only one call to an instance of the class with the operation may be outstanding at any one time there is no protection and no guarantee of behavior if the system violates this rule concurrent multiple calls to one instance may execute at the same time the operation is re sponsible for ensuring correct behavior this must be managed even if there are other sequential or synchronized guarded operations executing at the time parameter text area with entries for all the parameters of the operation see section 9 parameter a new operation is always created with one new parameter return to define the return type of the operation button double click on any of the parameters navigates to that parameter button 2 click brings up a pop up menu with two entries move up only available where there are two or more parameters and the parameter selected is not at the top it is moved up one position move down only available where there are two or more parameters listed and the parameter selected is not at the bottom it is moved down one position raised signals text area with entries for all the signals see section 10 signal that can be raised by the op eration caution argouml at present has limited support for signals in particular they are not linked to signal events that could drive state machines button double click on any of the signals navigates to that parameter 9 parameter a parameter is a variable that can be passed in the uml metamodel it is a sub class of modelele ment a parameter is represented within the operation declaration in the operation compartment of a class as follows name type name is the name of the parameter type is the type uml datatype class or interface of the parameter the exception is any parameter representing a return value whose type only is shown at the end of the operation declaration 9 parameter details tabs the details tabs that are active for parameters are as follows todoitem standard tab properties see section 9 2 parameter property toolbar and section 9 3 property fields for para meter below documentation standard tab see section 4 documentation tab source standard tab this contains a declaration for the parameter tagged values standard tab in the uml metamodel parameter has the following standard tagged values defined derived from the superclass modelelement values true meaning the parameter is re dundant it can be formally derived from other elements or false meaning it cannot caution a derived parameter is a meaningless concept note the uml element metaclass from which all other model elements are derived in cludes the tagged element documentation which is handled by the documentation tab under argouml 9 2 parameter property toolbar go up navigate up through the package structure new parameter this creates a new parameter for the for the same operation as the current parameter navigating im mediately to the properties tab for that parameter tip this is a convenient way to add a series of parameters for the same operation new datatype this creates a new datatype see section 3 datatype in the namespace of the owner of the operation of the parameter navigating immediately to the properties tab for that datatype new stereotype this creates a new stereotype see section 6 stereotype for the selected parameter navigat ing immediately to the properties tab for that stereotype delete this deletes the parameter from the model warning this is a deletion from the model not just the diagram if desired the whole operation compartment can be hidden on the diagram using the presentation tab or the button 2 pop up menu for the class on the diagram 9 3 property fields for parameter name text box the name of the parameter by convention the name of a parameter has a leading lower case letter with words separated by bumpy caps note the argouml critics do not complain about parameter names that do not have an ini tial lower case letter stereotype drop down selector there are no uml standard stereotypes for parameter navigate stereotype icon if a stereotype has been selected this will navigate to the stereotype property panel see section 6 stereotype owner text box records the operation which contains this parameter button double click on the entry will navigate to the operation type drop down selector the type of this parameter this can be any uml classifier although in practice only class datatype or interface make any sense note a type must be declared it can be void but this only makes sense for a return para meter by default argouml supplies int as the type the first time a parameter is created and thereafter the type of the most recently created parameter default value text box with drop down this allows you to set an initial value for the parameter if desired this is optional the drop down menu provides access to the common values 2 and null caution this only makes sense for out or return parameters kind radio box with entries out in out return and in out the parameter is used only to pass values back from the operation in out the parameter is used both to pass values in and to pass results back out of the opera tion note this is the default for any new parameter return the parameter is a return result from the call note there is nothing to stop you declaring more than one return parameter some pro gramming languages support this concept tip the name of the return parameter does not appear on the diagram but it is con venient to give it an appropriate name such as the default return to identify it in the list of parameters on the operation property tab in the parameter is used only to pass values in to the operation 10 signal a signal is a specification of an asynchronous stimulus communicated between instances in the uml metamodel it is a sub class of classifier within argouml signals are not fully handled their value is when they are received as signal events driving the asynchronous behavior of state machines and when associated with send actions in state ma chines and messages for collaboration diagrams tip in general there is limited value at present in defining signals within argouml it may prove more useful to define signals as classes with a user defined stereotype of signal as suggested in the uml 4 standard this allows any dependency relation ships between signals to be shown 10 signal details tabs the details tabs that are active for signals are as follows todoitem standard tab properties see section 10 2 signal property toolbar and section 10 3 property fields for signal below documentation standard tab see section 4 documentation tab source standard tab there is nothing generated for a signal tagged values standard tab in the uml metamodel signal has the following standard tagged values defined persistence from the superclass classifier values transitory indicating state is destroyed when an instance is destroyed or persistent marking state is preserved when an instance is destroyed semantics from the superclass classifier the value is a specification of the se mantics of the signal derived from the superclass modelelement values true meaning the signal is re dundant it can be formally derived from other elements or false meaning it cannot note derived signals still have their value in analysis to introduce useful names or con cepts and in design to avoid re computation note the uml element metaclass from which all other model elements are derived in cludes the tagged element documentation which is handled by the documentation tab under argouml 10 2 signal property toolbar go up navigate up through the package structure new signal this creates a new signal navigating immediately to the properties tab for that signal caution the signal is not associated with the same operation as the original signal so this will have to be done afterwards new stereotype this creates a new stereotype see section 6 stereotype for the selected signal navigating immediately to the properties tab for that stereotype delete this deletes the signal from the model warning this is a deletion from the model 10 3 property fields for signal name text box the name of the signal from their similarity to classes by convention the name of a sig nal has a leading upper case letter with words separated by bumpy caps note the argouml critics do not complain about signal names that do not have an initial upper case letter stereotype drop down selector signal is provided by default with the uml standard stereotypes for its parent in the uml meta model classifier metaclass powertype process thread and utility navigate stereotype icon if a stereotype has been selected this will navigate to the stereotype property panel see section 6 stereotype namespace drop down selector records and allows changing the namespace for the signal this is the package hierarchy of the signal contexts text area lists all the contexts defined for this signal button double click navigates to the selec ted context button 2 click brings up a pop up menu with one entry add add a new context this opens the add remove contexts dialog box see figure below which allows choosing between all possible operations and adding them to the selected list figure 5 the add remove context dialog box 11 reception to be written a reception is association an association on a class diagram represents a relationship between classes or between a class and an interface on a usecase diagram an association binds an actor to a usecase within the uml metamodel association is a sub class of both relationship and general izableelement the association is represented as a solid line connecting actor and usecase or class or interface see fig ure possible model elements on a class diagram the name of the association and any stereo type appear above the line argouml is not restricted to binary associations see section three way and greater associ ations and association classes for more on this associations are permitted between interfaces and classes but uml 3 specifies they must only be navigable toward the interface in other words the interface cannot see the class argouml will draw such associations with the appropriate navigation associations are often not named when their meaning is obvious from the context note argouml provides no specific way of showing the direction of the association as de scribed in the uml 4 standard the naming should attempt to make this clear the association contains at least two ends which may be navigated to via the association property sheet see section association end for more information three way and greater associations and asso ciation classes uml 3 provides for n ary associations and associations that are governed by a third associative class both are supported by argouml n ary associations are created by drawing with the association tool from an existing association to a third class the current implementation of argouml does not allow the inverse drawing from a class towards an existing association is not possible association classes are drawn exactly like a normal association i e between two classes but with a dif ferent dedicated tool from the diagram toolbar 2 association details tabs the details tabs that are active for associations are as follows todoitem standard tab properties see section 3 association property toolbar and section 4 property fields for association below documentation standard tab see section 4 documentation tab presentation standard tab note the values for the bounds of the association have no meaning since they are determ ined by the location of the connected items changing them has no effect on the dia gram source standard tab you would not expect to generate any code for an association and any code entered here is ignored it will have disappeared when you come back to the association tagged values standard tab in the uml metamodel association has the following standard tagged values defined persistence values transitory indicating state is destroyed when an instance is des troyed or persistent marking state is preserved when an instance is destroyed derived from the superclass modelelement values true meaning the association is redundant it can be formally derived from other elements or false meaning it cannot note derived associations still have their value in analysis to introduce useful names or concepts and in design to avoid re computation note the uml element metaclass from which all other model elements are derived in cludes the tagged element documentation which is handled by the documentation tab under argouml 12 3 association property toolbar go up navigate up through the package structure of the model for an association this will be the package containing the association new stereotype this creates a new stereotype see section 6 stereotype for the selected association navig ating immediately to the properties tab for that stereotype delete this deletes the selected association from the model warning this is a deletion from the model not just the diagram to delete an association from the diagram but keep it within the model use the main menu remove from dia gram or press the delete key 12 4 property fields for association name text box the name of the association by convention association names start with a lower case let ter with bumpy caps used to indicate words within the name thus saleshandling note argouml does not enforce any naming convention for associations tip although the design critics will advise otherwise it is perfectly normal not to name associations on a class diagram since the relationship is often obvious from the classes or class and interface name stereotype drop down selector association is provided by default with the uml standard stereotype for asso ciation implicit stereotyping can be useful when creating associations in the problem domain requirements cap ture and solution domain analysis as well as for processes based on patterns the stereotype is shown between and below the name of the association on the diagram navigate stereotype icon if a stereotype has been selected this will navigate to the stereotype property panel see section 6 stereotype namespace drop down selector records and allows changing the namespace for the association this is the package hierarchy connections text area lists the ends of this association an association can have two or more ends for more on association ends see section association end the names of the association ends are listed unless the association end has no name the case when it is first created in which case unnamed associationend is shown note the only representation of association ends on a diagram is that their name appears at the relevant end of the corresponding association button 1 double click on an association end will navigate to that end association roles text area to be written links text area to be written association end two or more association ends are associated with each association see section 5 association within the uml metamodel associationend is a sub class of modelelement the association end has no direct access on any diagram for binary associations the ends of an n ary association may be selected by clicking on the line in the diagram the stereotype name and multiplicity are shown at the relevant end of the parent association see figure 1 typical model elements on a use case diagram where shared or composite aggregation is selected for one association end the op posite end is shown as a solid diamond composite aggregation or hollow diamond shared aggrega tion tip although you can change attributes of association ends when creating a use case model this is often not necessary many of the properties of an association end relate to its use in class diagrams and are of limited relevance to use cases the most useful attributes to con sider altering are the name used as the role name and the multiplicity note argouml does not currently support showing qualifiers on the diagram as described in the uml 1 3 standard 1 association end details tabs the details tabs that are active for associations are as follows todoitem standard tab properties see section 2 association end property toolbar and section 3 property fields for association end below documentation standard tab see section 4 documentation tab presentation standard tab source standard tab this tab contains a declaration for the association end as an instance of the model ele ment to which it is connected tagged values standard tab in the uml metamodel associationend has the following standard tagged val ues defined derived from the superclass modelelement values true meaning the association end is redundant it can be formally derived from other elements or false meaning it cannot tip derived association ends still have their value in analysis to introduce useful names or concepts and in design to avoid re computation however the tag only makes sense for an association end if it is also applied to the parent association note the uml element metaclass from which all other model elements are derived in cludes the tagged element documentation which is handled by the documentation tab under argouml 2 association end property toolbar go up navigate up to the association to which this end belongs go opposite this navigates to the other end of the association new qualifier this creates a new qualifier for the selected association end navigating immediately to the proper ties tab for that qualifier warning qualifiers are only partly supported in argouml hence activating this button creates a qualifier in the model which is not shown on the diagram also the proper ties panel for a qualifier equals that of a regular attribute new stereotype this creates a new stereotype see section 6 stereotype for the selected association end navigating immediately to the properties tab for that stereotype delete this deletes the selected association end from the model note this button is downlighted for binary associations since an association needs at least two ends only for n ary associations this button is accessable and deletes just one end from the association 3 property fields for association end name text box the name of the association end which provides a role name for this end of the associ ation this role name can be used for navigation and in an implementation context provides a name by which the source end of an association can reference the target end note argouml does not enforce any naming convention for association ends stereotype drop down selector association end is provided by default with the uml standard stereotypes for associationend association global local parameter self navigate stereotype icon if a stereotype has been selected this will navigate to the stereotype property panel see section 6 stereotype association text box records the parent association for this association end button 1 double click on this entry will navigate to that association type drop down selector providing access to all standard uml types provided by argouml and all new classes created within the current model this is the type of the entity attached to this end of the association tip by default argouml will select the class of the model element to which the linkend is connected however an association can be moved to another class by selecting an other entry here multiplicity drop down menu with edit box the value can be chosen from the drop down box or a new one can be edited in the text box records the multiplicity of this association end with respect to the other end i e how many instances of this end may be associated with an instance of the other end the multiplicity is shown on the diagram at that end of the association modifiers there are 3 modifiers navigable ordered and static all 3 are checkboxes navigable indicates that this end can be navigated to from the other end note the uml 1 4 standard provides a number of options for how navigation is dis played on an association end argouml uses option 3 which means that arrow heads are shown at the end of an association when navigation is enabled at only one end to indicate the direction in which navigation is possible this means that the default with both ends navigable has no arrows ordered when placed on one end specifies whether the set of links from the other instance to this instance is ordered the ordering must be determined and maintained by operations that add links it represents additional information not inherent in the objects or links themselves possibilities for the checkbox are unchecked the links form a set with no inherent ordering checked a set of ordered links can be scanned in order static to be written specification list designates zero or more classifiers that specify the operations that may be applied to an in stance accessed by the associationend across the association these determine the minimum inter face that must be realized by the actual classifier attached to the end to support the intent of the as sociation may be an interface or another classifier the type of classifier is indicated by an icon button 1 double click navigates to the selected classifier button 2 click brings a pop up menu with one entry add add a new specification classifier this opens the add remove specifications dialog box see figure below which allows choosing between all possible classifiers and adding or remov ing them to the selected list figure 6 the add remove specifications dialog box qualifiers text box records the qualifiers for this association end button 1 double click on this entry will navigate to that qualifier button 2 click will show a popup menu containing two items move up and move down which allow reordering the qualifiers aggregation radio box with three entries composite none and aggregate indicates whether the relation ship with the far end represents some type of loose whole part relationship aggregation or tight whole part relationship composite shared aggregation is shown by a hollow diamond at the whole end of the association composite aggregation is shown by a solid diamond note you may not have aggregation at both ends of an association argouml does not en force this constraint the whole end of a composite aggregation should have a multiplicity of one argouml does not enforce this constraint changeability radio box with three entries add only changeable and frozen indicates whether instances of this end of the association end may be i created but not deleted after the target instance is cre ated ii created and deleted by the source after the target instance is created or iii not created or deleted by the source after the target instance is created visibility radio box with four entries public private protected and package indicates whether navigation to this end may be by i any classifier ii only by the source classifier or iii only the source classifier and its children dependency dependency is a relationship between two model elements showing that one depends on the other within the uml metamodel dependency is a sub class of relationship dependency is represented as a dashed line with an open arrow head from the depending model element to that which it is dependent upon 1 dependency details tabs the details tabs that are active for dependencies are as follows todoitem standard tab properties see section 2 dependency property toolbar and section 3 property fields for dependency below documentation standard tab see section 4 documentation tab presentation standard tab note the values in the bounds field of the dependency are not editable since they are de termined by the properties of the endpoints of the line tagged values standard tab in the uml metamodel dependency has no tagged values of its own but through superclasses has the following standard tagged values defined derived from the superclass modelelement values true meaning the dependency re lationship is redundant it can be formally derived from other elements or false meaning it cannot note derived dependencies still have their value in analysis to introduce useful names or concepts 14 2 dependency property toolbar go up navigate up through the package structure of the model for a dependency this will be the package containing the dependency new stereotype this creates a new stereotype see section 6 stereotype for the selected dependency navig ating immediately to the properties tab for that stereotype delete this deletes the selected dependency from the model warning this is a deletion from the model not just the diagram to delete a dependency from the diagram but keep it within the model use the main menu remove from dia gram or press the delete key 14 3 property fields for dependency name text box the name of the dependency tip it is quite common to leave dependencies unnamed note argouml does not enforce any naming convention for associations note there is no representation of the name of a dependency on the diagram stereotype drop down selector dependency has no standard stereotypes of its own under uml 1 3 and so argouml does not provide any the stereotype is shown between and above or across the gen eralization navigate stereotype icon if a stereotype has been selected this will navigate to the stereotype property panel see section 6 stereotype namespace text box records the namespace for the dependency this is the package hierarchy suppliers text area lists the end of the relationship that is supplying what is needed by the other end button 1 double click on a supplier will navigate to that element clients text area lists the depending ends of the relationship i e the end that makes use of the other end button 1 double click on a client will navigate to that element generalization generalization is described under use case diagrams see section 8 generalization note within the context of classes generalization and specialization are the uml terms describ ing class inheritance interface an interface is a set of operations characterizing the behavior of an element it can be usefully thought of as an abstract class with no attributes and no non abstract operations in the uml metamodel it is a sub class of classifier and through that generalizableelement an interface is represented on a class diagram as a rectangle with two horizontal compartments the top compartment displays the interface name and above it interface and the second any operations just like a class the operations compartment can be hidden 1 interface details tabs the details tabs that are active for interfaces are as follows todoitem standard tab properties see section 2 interface property toolbar and section 3 property fields for inter face below documentation standard tab see section 4 documentation tab presentation standard tab the tick box display operations allows the operation compartment to be shown the default or hidden this is a setting valid for only the current diagram the bounds field defines the bounding box for the package on the diagram source standard tab this contains a template for the interface declaration and declarations of associated in terfaces tagged values standard tab in the uml metamodel interface has the following standard tagged values defined persistence from the superclass classifier values transitory indicating state is destroyed when an instance is destroyed or persistent marking state is preserved when an instance is destroyed warning since interfaces are by definition abstract they can have no instance and so this tagged value must refer to the properties of the realizing class semantics from the superclass classifier the value is a specification of the se mantics of the interface derived from the superclass modelelement values true meaning the interface is re dundant it can be formally derived from other elements or false meaning it cannot note derived interfaces still have their value in analysis to introduce useful names or concepts and in design to avoid re computation note the uml element metaclass from which all other model elements are derived in cludes the tagged element documentation which is handled by the documentation tab under argouml checklist standard tab for an interface 16 2 interface property toolbar go up navigate up through the package structure new operation this creates a new operation see section 8 operation within the interface navigating im mediately to the properties tab for that operation new reception this creates a new reception navigating immediately to the properties tab for that reception new interface this creates a new interface in the same namespace as the selected interface navigating immedi ately to the properties tab for the new interface new stereotype this creates a new stereotype see section 16 6 stereotype for the selected interface navigat ing immediately to the properties tab for that stereotype delete this deletes the interface from the model warning this is a deletion from the model not just the diagram to delete an interface from the diagram but keep it within the model use the main menu remove from diagram or press the delete key course description studies the development of information systems in organizations the development life cycle of information systems is used as a framework for studying the management of systems development and the evaluation of opportunities for improving information systems within organizations textbook information required text experiencing mis updated second canadian edition kroenke ed students may use or updated edition mymislab not required comes free in updated text pkg if you purchased a used text an access code can be purchased from pearson for the text price policies missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the application must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses http www arts usask ca students transition tips php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the course outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations section of the calendar university of saskatchewan calendar exams grades grading system department policy on academic honesty students are expected to be academically honest in all of their scholarly work including course assignments and examinations academic honesty is defined and described in the department of computer science statement on academic honesty http www cs usask ca content academichonesty academichonesty jsp and the university of saskatchewan academic honesty website http www usask ca honesty the student academic affairs committee treats all cases according to the university policy and has the right to apply strict academic penalties see http www usask ca honesty php student evaluation grading scheme short assignments in class labs projects project use project management software to plan and develop resource requirements for a workflow document management system project implement the system from project using sharepoint this depends on the availability of sharepoint midterm examination tuesday october final examination required coursework a student must write both exams midterm and final to pass the course failure to average on both exams will result in an automatic failure in the course in certain rare circumstances a student who due to a reasonable error or unusual circumstance misses the midterm examination may with written permission of the instructor undertake a makeup examination or have the midterm marks moved to the final examination in such a case the student must achieve a average in the final examination to pass the course attendance expectations you are expected to attend all lectures and labs notes on the lectures or labs will not necessarily be made available by the instructor so if you do miss a class it is up to you to catch up with the help of one or more of your peers posted lecture outlines while some lecture outlines will be posted in advance of some lectures these notes are expected to be viewed as an aide to the student in following the lecture often material introducted at lecture will not be specifically mentioned in posted notes students are responsible at examination for all materials covered in the course introduced at lecture or in assigned labs and or readings important please read all students must be properly registered in order to attend lectures and receive credit for this course failure to write the final exam will result in failure of the course failure to complete required course work without advance permission may result in failure of the course course description advanced introduction to concepts and structures used to develop guis in software focusing on building user interfaces covers the fundamentals of gui toolkits including input widgets layout events modelview controller architectures and two dimensional graphics on the web the moodle page for the course https moodle cs usask ca course view php id will be the main location for all materials related to prerequisite cmpt brief learning objectives when finished this course you will be able to build graphical user interfaces in several development environments develop interactive systems that use widgets layout managers event handlers and graphics build systems using the model view controller pattern implement common interaction techniques such as selection drag and drop and undo course meeting information lecture tuesdays and thursdays 20am thorv lab tuesdays 20pm spinks student evaluation grading scheme assignments midterm exam project final exam assignments approximately one every two weeks in january and february midterm in class february tentative project start in february due at end of term textbook required olsen d r building interactive systems principles for human computer interaction course technology boston ma isbn isbn available in the u of s bookstore electronic versions also available recommended the u of s library has several electronic reference books for the different development environments java fx and android additional reference materials will be made available on the course website course topics intro to ui development and interactive systems gui architecture the layered model mvc and the interaction cycle paradigms for specifying uis programmatic and declarative widgets layout and events building basic uis in different environments java fx and android graphics canvases points lines shapes and transformations model view controller pattern and implementation threading for guis basic interactions select scroll drag and drop cut copy paste undo advanced interaction e g ink gestures zoom pan other topics as determined by class interest policies late assignments no late assignments will be accepted or marked without medical reason and no extensions will be given missed assignments missed assignments will be given a grade of missed examinations students who miss an exam should contact the instructor as soon as possible if it is known in advance that an exam will be missed the instructor should be contacted before the exam a student who is absent from a final examination due to medical compassionate or other valid reasons may apply to the college of arts and science undergraduate student office for a deferred exam application must be made within three business days of the missed examination and be accompanied by supporting documents http artsandscience usask ca undergraduate advising strategies php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions past the final examination date for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the class which factors in the incomplete coursework as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the student has a passing percentile grade but the instructor has indicated in the course outline that failure to complete the required coursework will result in failure in the course a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised assigned final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed a student can pass a course on the basis of work completed in the course provided that any incomplete course work has not been deemed mandatory by the instructor in the course outline and or by college regulations for achieving a passing grade http policies usask ca policies academic affairs academic courses php for policies governing examinations and grading students are referred to the assessment of students section of the university policy academic courses class delivery examinations and assessment of student learning academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals section of the university secretary website and avoid any behavior that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss course description this course is one of the courses in the software engineering track in user interface design the designers are faced with many questions regarding appearance content modalities and functionalities of the interface in this course user interface patterns which are essentially reusable software components are introduced as the cornerstone in a usercentric design methodology iconic visual gestural tactile audio and unconventional user interfaces are discussed systematically class projects enable the students to apply the user interface patterns to the design of novel multimedia interfaces for diversified applications prerequisites introduction to software engineering with passing grade of c or better or consent of the instructor course organization week basic concepts of interface computercomputer interface messages and protocols humancomputer interface week software components interaction standards composition containers week uml uniformed modelling language as a basis for patterns you will need to download argouml open source software whose official website is http argouml tigris org and there is also a wellwritten extensive argo uml user manual week theory of patterns reusable software components week introduction to user interface patterns week a survey of user interface patterns search continuous filter continuous highlight data views overview beside detail expand in context fisheye storages rule storage data storage placeholder temporary storage selecting and manipulating objects double list editable table pile of items master and instances time calendar strip schedule hierarchies and sets tree groups and items save and undo autosave global undo object specific undo deleted data storage web applications shopping cart reservation interface design methodology http people cs pitt edu chang html hypermedia navigational context news landmark information on demand slow intelligence systems enumerate propagate eliminate adapt concentrate week term project to design multimedia user interface concurrent with classes week iconic user interface week visual user interface math expressions sketches week gestural user interface american sign language week audio user interface week tactile user interface touch interface braille reader unconventional user interfaces brain wave etc week case study of interface for smarter healthcare systems week class project presentations course requirements detailed class notes similar to those available on class web pages for introduction to software engineering www cs pitt edu chang html will be provided the class notes are based upon research papers and articles on user interface design patterns componentbased software engineering and multimedia user interfaces there will be a term project which amounts to a substantial multiweek homework carried out by a team the project teams are required to give project presentations students will form twoperson project teams in week sixteen each project team will make a project presentation during the class periods term projects the term projects will be centered around the use of kinect tmor other interface modalities to design multimedia user interfaces for the stage set of a science fiction play class notes detailed class notes will be provided grading policy the grade scaling will be computed based on the performance of the undergraduate students only graduate students who may also take this course will then be graded on this undergraduate scale unless noted otherwise the work in this course is to be done independently discussions with other students on the assignments or term projects should be limited to understanding the statement of problems and discussing programming techniques homeworks midterm exam final exam term project late policy a late assignment will receive a deduction of point per day any assignment that is more than days late will be accepted only under special circumstances with the instructor determining the penalty in a fair manner interface design methodology http people cs pitt edu chang html students with disability if a student has a disability for which the student may request an accommodation the student is encouraged to contact both the course instructor and the office of disability resources and services william pitt union as early as possible in the term user interface patterns components for user interfaces von udo arend user productivity sap ag german version this article was published in sap info online sap ag november reusable components known as user interface patterns considerably simplify the development of consistent and ergonomic user interfaces the first part of the following article describes the pattern approach used at sap the remainder of the article shows how the patterns are derived from generic user tasks and then mapped on the user interface developers are faced by a great many questions when designing the user interface or ui for a corporate software application should they use icons where should the header data and the detail data be positioned how do users search for objects or data how should new rows be inserted into a table which working areas should make up a view finding the right answers to all these questions is not easy this is why sap uses predefined floor plans that are populated by reusable generic user interface patterns when developing user interfaces as part of a user centric design process user interface first developers design business applications and configure their user interface using predefined components they are assisted in this task by appropriate development tools the pattern approach at sap reusable components or patterns are not new concepts in software development and user interface design according to the definition of design experts douglas van duyne james landay and jason hong they provide insights into design problems by defining the specifics of a particular issue and its solution in a compact form a well known example would be the shopping cart used by online shops where users can collect all the items they want to buy before placing the order figure the ui pattern concept the pattern approach at sap is characterized by two special features firstly it separates the description of a task that the user executes from its representation on the user interface this separation has the advantage that the floor plans and components can be structured both according to the prevailing design paradigm and also in line with the specific features of the terminal such as a large monitor on a pc or a small monitor on a pda secondly the approach only works if generic user interface solutions are found for generally applicable or generic tasks after all the intention is to be able to use one component for a large number of applications both tasks and the elements of the user interface can be mapped in a nested decomposition hierarchy that is simple elements can be combined to produce more complex ones the search for a work object is one example of a generic task it is irrelevant whether the object is an order a delivery an invoice or a material the general assumption is that if generic tasks exhibit similar properties for a specific number of business processes then a standard solution can be found on the user interface for instance a button represents a classical solution for the generic trigger function problem the sap approach is about moving away from the level of these elementary actions and solutions such as entering text or clicking a button and identifying more complex tasks to achieve this elementary components or user interface controls such as the button are assembled into more complex components the pattern elements a pattern element may be a toolbar that combines several controls for triggering functions pattern elements themselves can be combined into even more complex components the user interface patterns a user interface pattern fulfils one or more generic tasks such as searching for and selecting a business object a pattern such as this may consist of a search area various function buttons in a tool bar and a collection area for the hit list standardized floor plans for applications during application development user interface patterns are combined into application floor plans and are arranged on the screen the application floor plans can be standardized for identical application types business activities such as editing a customer order one part of the people centric mysap customer relationship management mysap crm solution which marks the beginning of the deployment of user interface patterns at sap consists of just two floor plans and uses only two different user interface patterns this provides for a highly consistent solution however it is only possible to do with a small number of floor plans in this application area because the applications have a similar structure naturally more floor plans and user interface patterns may be used to make the application more suited to the needs of the user sap bases its development of user interfaces on karen holtzblatt contextual design method which takes the users working practices as its starting point developers and ui designers use this method to create the model of an interaction structure user environment model that is the structure of a business process from the user point of view the interaction structure consists of discrete areas known as the focus areas each of which describes a user activity such as entering invoice line items a focus area is defined by its purpose for the user by other areas to which the user can navigate and by functions available to the user according to holtzblatt the user interface should represent a focus area as a coherent whole on the screen that allows users to concentrate on it and complete their tasks there focus areas are normally derived for one specific business process and are only valid for this however the sap approach requires similar focus areas to be defined from different business processes only then can reusable ui components be built that can then be configured for each specific application scenario one result of this is a consistent user interface structure of different applications that have a similar composition the approach of configuring user interfaces with user interface patterns only works if there are a small number of generic tasks which can describe very many different business activities the following sections use examples to demonstrate what generic tasks may look like and how they can be used to define reusable components design method figure searching for and selecting a work object before users can perform an activity such as editing an invoice they need to select the corresponding business process users can then search for a work object or define one directly the work object can be selected on the user interface by means of a solution with various drop down boxes and a button for example the show drop down box enables a predefined query to be selected the get drop down box enables a key field to be selected instead and also enables a value to be entered in the corresponding input field the advanced button activates a window in which an extended search query can be launched figure possible work objects in a results list orienting identifying editing the possible work objects that were identified by the query can be grouped in a particular area of the user interface the user can thus decide how to proceed that is whether to select one or more work objects the user requires information about the current work object such as its status and navigation options for orientation purposes the user interface must enable the user to identify the work object clearly a work object most important data can be made available in a form in a table in a tree or displayed graphically figure sample object structure for an invoice transparent structures sap selected an activity such as recording an invoice as the unit of measure for the definition of an application the assumption is that each business activity consists of an easily describable underlying structure that a user can make a mental picture of this structure is made up of concepts objects of differing degrees of abstraction each object may be of the type categorial or enumerating the header data of an invoice for instance consists of different information that can be represented as a form categorial the invoice line items on the other hand are enumerating and can be easily represented in a table during the re engineering of a large number of business activities sap established that the specific object structure is often part of a general object structure figure floor plan for generic tasks of the editing objects type the focus areas of a business process can be combined into a floor plan each room of the floor plan defines a place on the user interface such as horizontal top vertical left or bottom right further each story of the floor plan describes a screen change in the context of what has been said above an underlying generic task can be identified for every focus area that describes a specific task for its user centric mysap crm solution for instance sap defined an application floor plan that is suitable for the edit objects task other floor plans are required for other tasks figure mapping of generic tasks to ui patterns mapping generic tasks to the user interface the floor plan of the generic tasks can be mapped to a floor plan of user interface patterns which have been programmed in full the following applies at least one generic task is mapped to a user interface pattern this component is assigned on the screen in accordance with a predefined floor plan the component contains all functions data and links that a user needs for his or her current task user interface patterns enable developers to configure interfaces instead of programming them all the developer has to do is decide which functions and fields should appear on the user interface for each pattern to do so the developer selects items from proposal lists provided by the development tool the actual view seen on screen is generated for instance a form view for categorial data that groups and arranges the corresponding screen elements such as fields radio buttons or check boxes in two dimensions can be generated by layout algorithms positive outcome sap is happy with the outcome of the first project that used user interface patterns the following results can be noted the selected level of abstraction proved suitable for deriving functionally powerful and general components the number of identifiable generic functions remains transparent generic functions can be mapped to generic user interface patterns a precise description of the functions is necessary here composition into floor plans is comparatively simple in doing so the way in which data is to be exchanged between user interface patterns must be established in the case of master detail relationships between two object data patterns the selection of a different row in the superordinate pattern naturally influences the representation in the subordinate pattern configuration of the user interface as opposed to programming it can be implemented technically and makes developers work far easier predefined user interface components provide three key advantages firstly they provide a simple means of creating highly consistent user interfaces secondly the usability of individual components can be optimized retrospectively and thirdly a high gain in productivity is made during development as the interfaces merely need to be configured the different types of business activities pose challenges as they demand a compromise between suitability for the task and consistency similarly development requests for functional enhancements may over time complicate the clear structure of a user interface pattern experience will show which floor plans are most suited to which application types people cs pitt edu chang sowall htm http people cs pitt edu chang sowall htm statement of work group we are going to use the kinect to emulate the shadow of the hero and snakeman during their first battle using full body tracking the shadow for the hero will look like a normal human while the snakeman shadow will look snakelike we are going to also work on the possibility of using or more kinects at the same time to extend the lateral range of the sensor and allow the actors more flexibility in their movements across the stage statement of work group bradlee krupa and chaitanya sathi group for our semester project for cs1635 we plan to design a system for tracking the motion of a thrown object the object will be thrown from realspace and be translated into virtual space while maintaining trajectory and speed the motion tracking will be done with the kinect software milestone group samantha kuhn zachary parker a statement of work scenario second fight between our hero and the snakeman act the hero tries to activate a safety valve on the wall screen while the snakeman is doing its best to prevent that from happening with visual effects in this project a software component will be developed which will use a microsoft kinect to control the obfuscation of a displayed image through gestures of the hands and or body this will be applied to the above scenario in which a safety valve must be hidden by motioning toward the screen smoke etc can be generated to hide the displayed object b conceptual pattern problem the safety valve must be obscured by using gestures to generate smoke context the hero is trying to activate a safety valve on the wall while the snakeman is trying to prevent that from happening solution by using gestures by the snakeman smoke can be generated interactively in accordance with the snakeman movement by rendering smoke on top of the safety valve the device can be obscured in order to keep the hero from finding and reaching it c design pattern people cs pitt edu chang sowall htm http people cs pitt edu chang sowall htm statement of work group brandon spires and lawrence west cs group we are interested in the two scenes which depict our hero fighting on a rooftop of the burning city however our group will focus on developing scenario six namely the second fight between our hero and the snakemen in this scene the snakemen will be onscreen while the actor plays the hero in the foreground as the snakemen advance towards the hero he will repel then by using various attacks these different attacks will register different onscreen reactions possibly in the snakemen themselves or in the scenery the scenario is ended once the hero reaches the safety valve statement of work group kinect gesture control for robots author yang hu qizhang dai introduction we want to develop a system that using kinect as an input device to recognize human body movements then sends specific command to remote device through sis server idea our idea is that not like the others using something like a game pad to control the robot we can use our natural body movement to achieve it the architect will look like this kinect component responsible to recognize and classify human body movements then generate standard control message send the message to the sis server device component connect to the sis server and wait for the message from the kinect component different device component will react differently according to different message sequence device remote device will be something like a tiny robot car it can people cs pitt edu chang sowall htm http people cs pitt edu chang sowall htm move forward turn right turn left stop gesture in order to control the device easily we are going to design an algorithm that can tell the difference between these gestures below raise both hands up means start the device left hand to the left means turn left right hand to the right means turn right both hands down means stop the car message format sample message for left gesture xml version standalone yes author yang hu email pitt edu msg head msgid msgid description gesture command description head body item key gesture key value left value item body msg sow for andrew powell and cj mcallister people cs pitt edu chang sowall htm http people cs pitt edu chang sowall htm we would like to do something in our project that involves sound to change the state of the machine to perform different actions onscreen i was thinking this would go well with scenario snakeman while trying to stop the hero could have some dialog that would set the state so that the valve could be turned statement of work group group will do sound playback we envision controlling sound output with two hands where hand depth and height will control such factors as volume and pitch gestures will select between different sound types similar to an electronic keyboard sound presets other controls and gestures may be created as the project develops statement of work group the primary objective of the project will be to try and implement human motions that will control google tv over the kinect we shall implement recognition of a set of simple gestures for this control the gestures will activate and deactivate kinect control as the user desires along with controlling television operation furthermore we will use depth perception via the kinect to determine the interactions possibly using a reference point area on the user body we plan to start with power and channel surfing control possibly expanding to other areas if time allows a testbed for intelligent interface design in what follows we describe a testbed for componentbased software development the specific application example is a personal healthcare system in general the testbed can be used for intelligent interface design application example the personal healthcare system consists of the following universal interface the universial interface is for testing the system the user can enter a message on the left panel and observe an output message on the right panel in fact any message sent from one component to another component will be displyed on the right panel the actual message sent has the following format name value name value i e name and value pairs with three dollar signs serving as delimiters sis server the sis server processes messages the special messages create kill and activate will enalbe the sis server to create kill and activate a component other messages will be routed to the receipient components by the sis server input processor the input processor is a predesigned componet which inputs data stream generated by the health sensors and outputs data streams specific for the various vital signals such as blood pressure blood sugar pulse oximeter oxygen saturation ekg and so on uploader the uploader is a predesigned component which inputs data streams and uploads them via the internet to a shared database so that patients physicians and other authorized personnel can access patient information other components these components include the gui and various monitors including bloodpressure monitor bloodsugar monitor ekg monitor monitor generalhealth monitor and so on if class projects are for personal healthcare these are the components to be developed by class projects the universal interface the sis server the inputprocessor and the uploader constitute the component infrastructure the other components are built around this component infrastructure people cs pitt edu chang sequencesis htm http people cs pitt edu chang sequencesis htm test bed installation the complete sis test bed can be downloaded from the zip file once downloaded just follow the the general setup procedure to install the test bed if you are working on kinectrelated projects follow the siskinect setup procedure to install the test bed you are now ready to develop and test your plugin components preliminary condition universalinterface sisserver and healthsensors either preexist or have already been instantiated activated gui inputprocessor bloodpressure monitor and uploader will be instantiated activated in case some of these components are not available it is still ok you can use the universalinterface to simulate messages from these components messages to these components will be ignored by the sis server but still displayed on the right panel of the universalinterface for the class projects for example you can simulate messages from inputprocessor therefore you don t need to have access to the healthsensors to develop and test your plugin components initialization universalinterface sends msg bbb create to sisserver to instantiate activate gui universalinterface sends msg bbb create to sisserver to instantiate activate inputprocessor universalinterface sends msg abb create to sisserver to instantiate activate bloodpressure monitor universalinterface sends msg abb create to sisserver to instantiate activate monitor universalinterface sends msg abb create to sisserver to instantiate activate ekg monitor universalinterface sends msg bbb create to sisserver to instantiate activate uploader a scenario gui interacts with the user and sends msg user profile containing username age sex weight height and medical conditions to uploader and all monitors healthsensors sends msg sensor data input to inputprocessor which parses the data and extracts vital signals inputprocessor accepts msg sensor data input and produces msg or however msg may not be a formatted message inputprocessor sends msg blood pressure reading to bloodpressure monitor which checks for abnormality there could be a knowledge base but initially we just check if high blood pressure reading is over to generate an alert bloodpressure monitor accepts msg and msg and produces msg bloodpressure monitor sends msg blood pressure alert to gui which displays the vital signals gui accepts msg 38 and produces msg bloodpressure monitor sends msg blood pressure alert to uploader which uploads patient username and medical conditions to remote database uploader accepts msg and uploads it the uploading may not be a formatted message people cs pitt edu chang sequencesis htm http people cs pitt edu chang sequencesis htm there are other messages and other components for heart rate level ekg signals for irregular heart beats etc but it is the same scenario only more complicated the gui may also need to display the combined vitals design considerations a monitor component and the associated knowledge base component can be considered as one coarse grained business component the refined monitor component and the associated knowledge base component are the fine grained components a component is brought to life by the create component message msg id or if there is a knowledge base when a component first comes to life it should send a connect to server message msg id to the sisserver sisserver will return an ack message msg id to the said component the activate component message msg id and deactivate component message msg id will activate deactivate the said component finally the kill component message msg id will destroy it component specification we need to specify at least the input messages output messages the variables attributes and the methods for the components therefore the class diagrams should be prepared for a complete specification we can use the following uml diagrams the usecase diagram specifies the major usecases or components the sequence diagram describes the interactions among the classes and actors based upon messages the state diagram describes the state transitions for the major component special attention should be paid to the interface kinectrelated projects for kinectrelated projects there is a kinect server that provides that url of prestored kinect test sequences upon request this kinect server is just another component therefore before testing the kinectserver a createkinectserver message message type should be sent to the sisserver see testbed kinect xml initxml createkinectserver xml to test the kinect server we can implement a component to send a request message to the kinect server to request a stored kinect action sequence this component should receive a response message containing the ibm many eyes tool ibm many eyes tool is an online tool that makes use of the cognos application without having to setup a cognos environment to do so google public data explorer ibm many eyes tool relies on the availability of data sets there is a project from google that provides different data sets which can be used for building demos google public data explorer to keep the demos to a manageable environment it is suggested to make use of the above two links examples of visualization and visual analytics there is a screen shots page showing different screen shots of cognos dashboards which can be helpful to illustrate visualization and visual analytics you can click on each image to expand interactive fighting dancing scenarios all scenarios should be recorded as a video clip of no longer than seconds the deliverables include the recorded video clip both as a video file and on youtube the source code the user actor manual or readme file with the implementation procedure included as an appendix of the user actor manual scenario boring boxing by a bored boxer act the actor would draw the character meaning bored then point at a gate and draw a heart to create the same character and then draw a gate and point at the heart to create the same etc a demo to draw a chinese character meaning bored using gestures same demo but in faster speed scenario mailbox act the actor would open a mailbox in mid air and point at objects inside to open the letter scenario dance of the totalhistorians version i act scene the kinect program to track multiple fullbody motions can be used to create mimic the dancers the totalhistorians on the screen albeit with more gimmicks scenario dance of the totalhistorians version ii act scene the symbol of the totalhistory is a circle or wheel of fortune the totalhistorians will interact with a circle on the screen to create various visual effects scenario first fight between our hero and the snakeman act scene the hero uses the wheel of fortune to escape with the snakeman chasing from historical period to historical period scenario second fight between our hero and the snakeman act the hero tries to activate a safety valve on the wall screen while the snakeman is doing its best to prevent that from happening with visual effects scenario vicious snakeman version i tbd the snakeman uses a gesture to throw a physical object perhaps a block or a robot car into the virtual world scenario vicious snakeman version ii tbd interactive fighting dancing scenarios http people cs pitt edu chang fallencity scenarios html to be determined scenario fate teller intermission the audience can go to the kiosk during intermission and use fist finger to determine whether the story has a happy ending or sad ending homework fall http mips lrdc pitt edu homework from fall contents the art of virtual keyboarding resources what to submit screen recording submission instructions create a wiki page for this assignment create upload screencast video add link to your finished assignment links to finished assignments the art of virtual keyboarding due date september homework http mips lrdc pitt edu courses restricted pdf resources file source code zip source code of the baseline keyboard file keyboard jar compiled keyboard what to submit upload a demo video in the form of screen recordings of your finished program to the homework assignment wiki page please use voice overs and or onscreen captions to explain your application implemented you can use either cam studio https sourceforge net projects camstudio windows free or ishowu hd http store shinywhitebox com store file php p mac free evaluation to record the video email a brief report no more than pages in total that describes your solutions to both the instructor and the ta make it clear the design decisions you made and the reasons pros and cons of those decisions use figures when necessary make sure your descriptions are accurate and succinct email a copy of the compressed source code of your project to the ta please also include a readme file to describe how to build your application please also include a web link to a compressed copy of prebuilt ready to run application in the readme file please make sure the total size of your source code package is no more than mb please contact the instructor and the ta if you couldn t make the source of your project smaller than the size limit screen recording homework fall http mips lrdc pitt edu windows you can use the free cam studio http camstudio org os x you can use a free trial version of ishowu hd http store shinywhitebox com store file php p or create a screen recording with quicktime x submission instructions you will submit the demo video part of your assignment on this wiki create a wiki page for this assignment begin by creating a new wiki page for this assignment go to your user page that you created when you made your account you can get to it by typing the following url into your browser http mips lrdc pitt edu index php user replace firstname and lastname with either you or your homework partner real first and last names this will take you to the page you created for yourself when you created your wiki account if you have trouble accessing this page please check that you created your wiki account properly edit your user page to add a link to a new wiki page for this assignment the wiki syntax should look like this firstnamelastname homework again replace firstname and lastname with your name look at my user page for an example then click on the link and enter the information about your assignment you should upload the files described below and describe any extra functionality you implemented and want us to review create upload screencast video what your screencast should contain narrated walkthrough of the interface including all implemented interactions be concise your video shouldn t be longer than seconds be prepared to do multiple takes plan and or write out a script first your file should be in wmv avi or mov format and no larger than rename the wmv mov file to wmv e g wmv upload the wmv file to the page you just created create a new file link like this media wmv save the page then click on the file link you just created to upload the wmv file add link to your finished assignment one you are finished editing the page add a link to it at the bottom of the page with your full name as the link text the wiki syntax will look like this firstnamelastname firstname lastname hit the edit button for the last section to see how i created the link for my name homework fall http mips lrdc pitt edu homework from fall contents getting physical and creating internet of things iot with arduino and galileo what to submit some inspirational ideas from previous students submission instructions create a wiki page for this assignment create upload demo video add link to your finished assignment links to finished assignments getting physical and creating internet of things iot with arduino and galileo due date october homework http mips lrdc pitt edu courses restricted pdf what to submit upload some small video clips no more than seconds each for assignments no more than minute for no more than minutes for of your completed system to the homework assignment wiki page please use voice overs and or onscreen captions to explain your system no need to use a camcorder if you don t have one the video recording function of your digital camera or camera phone will be fine email a brief report no more than pages in total that describes your solutions to both the instructor and the ta describe challenges you met and lessons learned during the implementation a couple of sentences will be okay for assignments use figures when necessary make sure your descriptions are accurate and concise email a copy of the compressed source code of your project to the ta please also include a readme file to describe how to build your application please also include a web link to a compressed copy of prebuilt ready to run application on the pc side in the readme file please make sure the total size of your source code package is no more than mb please contact the instructor and the ta if you couldn t make the source of your project smaller than the size limit some inspirational ideas from previous students media smartcoffeecoaster wmv by david krebs lindsey bieda and alexander conrad media drivingsimulator mov by abedul haque phillip walker and charmgil hong media microwavescheduler mov by lingjia deng xiangmin fan and xianwei zhang 3 3 homework fall http mips lrdc pitt edu 3 submission instructions you will submit the demo video part of your assignment on this wiki create a wiki page for this assignment begin by creating a new wiki page for this assignment go to your user page that you created when you made your account you can get to it by typing the following url into your browser http mips lrdc pitt edu index php user replace firstname and lastname with either you or your homework partner real first and last names this will take you to the page you created for yourself when you created your wiki account if you have trouble accessing this page please check that you created your wiki account properly edit your user page to add a link to a new wiki page for this assignment the wiki syntax should look like this firstnamelastname homework again replace firstname and lastname with your name look at my user page for an example then click on the link and enter the information about your assignment you should upload the files described below and describe any extra functionality you implemented and want us to review create upload demo video what your demo video should contain narrated walkthrough of the interface including all implemented interactions be concise be prepared to do multiple takes plan and or write out a script first your files should be in wmv avi or mov format and no larger than rename the wmv mov file to wmv e g wmv upload the wmv file to the page you just created create a new file link like this media wmv save the page then click on the file link you just created to upload the wmv file add link to your finished assignment one you are finished editing the page add a link to it at the bottom of the page with your full name as the link text the wiki syntax will look like this firstnamelastname firstname lastname hit the edit button for the last section to see how i created the link for my name how to read a research paper later in the semester we will talk about how to write a research paper to begin the course however we consider how to read a research paper this discussion presupposes that you have a good reason to carefully read a research paper for example the fact that i assign a paper is probably a good reason for you to read it you may also need to carefully read a paper if you are asked to review it or if it is relevant to your own research we might also later discuss how to skim a paper so that you can decide whether a paper is worth a careful reading when you read a research paper your goal is to understand the scientific contributions the authors are making this is not an easy task it may require going over the paper several times expect to spend several hours to read a paper here are some initial guidelines for how to read a paper ead critically reading a research paper must be a critical process you should not assume that the authors are always correct instead be suspicious critical reading involves asking appropriate questions if the authors attempt to solve a problem are they solving the right problem are there simple solutions the authors do not seem to have considered what are the limitations of the solution including limitations the authors might not have noticed or clearly admitted are the assumptions the authors make reasonable is the logic of the paper clear and justifiable given the assumptions or is there a flaw in the reasoning if the authors present data did they gather the right data to substantiate their argument and did they appear to gather it in the correct manner did they interpret the data in a reasonable manner would other data be more compelling ead creatively reading a paper critically is easy in that it is always easier to tear something down than to build it up reading creatively involves harder more positive thinking what are the good ideas in this paper do these ideas have other applications or extensions that the authors might not have thought of can they be generalized further are there possible improvements that might make important practical differences if you were going to start doing research from this paper what would be the next thing you would do ake notes as you read the paper many people cover the margins of their copies of papers with notes use whatever style you prefer if you have questions or criticisms write them down so you do not forget them underline key points the authors make mark the data that is most important or that appears questionable such efforts help the first time you read a paper and pay big dividends when you have to re read a paper after several months would be easier if more research papers were well written but again we will discuss writing later on fter the first read through try to summarize the paper in one or two sentences almost all good research papers try to provide an answer a specific question sometimes the question is a natural one that people specifically set out to answer sometimes a good idea just ends up answering a worthwhile question if you can succinctly describe a paper you have probably recognized the question the authors started with with and the answer they provide once you have focused on the main idea you can go back and try to outline the paper to gain insight into more specific details indeed if summarizing the paper in one or two sentences is easy go back and try to deepen your outline by summarizing the three or four most important subpoints of the main idea f possible compare the paper to other works summarizing the paper is one way to try to determine the scientific contribution of a paper but to really guage the scientific merit you must compare the paper to other works in the area are the ideas really novel or have they appeared before of course we do not expect you to be experts and know the areas ahead of time in this class it is worth mentioning that scientific contributions can take on many forms some papers offer new ideas others implement ideas and show how they work others bring previous ideas together and unite them under a novel framework knowing other work in the area can help you to determine which sort of contribution a paper is actually making for this class i will often ask you to provide a short one page review of a paper although this may sound like a simple assignment i expect that it will take a significant amount of time especially in the beginning remember i am expecting it to take several hours just to read the paper keeping the above in mind as you read the paper should make the process easier your one page review should include the following one or two sentence summary of the paper deeper more extensive outline of the main points of the paper including for example assumptions made arguments presented data analyzed and conclusions drawn ny limitations or extensions you see for the ideas in the paper our opinion of the paper primarily the quality of the ideas and its potential impact in this course you will complete a halfsemesterlong class project this project will be completed in groups of two please contact the instructor if you want to work independently at a high level successful projects will raise an important research question and plan and execute a methodology for answering that question often this methodology will include building and evaluating a prototype system but hacking is not strictly necessary all projects require a study obviously a much more thorough study will be expected of projects that do not involve system building the goal of the project abstract draft described below is to help you scope your work appropriately a draft of your project proposal is due on october course staff will provide feedback on the draft to assist in the preparation of a polished version due on october both need to be submitted via email attachments to the instructor and the ta topics the draft project abstract should cover the following topics research question what are you trying to answer state this as clearly as possible in one sentence hypothesis what do you think the answer to your question is and why method how will you explore your hypothesis and why is that the right approach this should include the design of your study grounding this in methodologies that other researchers have used e g by drawing from the class readings is a good idea there are three major points you should hit here study design what are you going to do evaluation how will you know you succeeded what will you measure how will you measure it ecological validity why does your study answer your research question why does your evaluation address your hypothesis study recruitment plan how will you get participants for your study for pilot studies we suggest you recruit from within the class trading participation with other groups is a great way to learn about what others are doing for larger studies e g for those not building a system you need a clear recruitment plan biggest risk what the riskiest component of your project may not be able to get the hardware you need robustly implementing the algorithm may take too long the difference between conditions may not be measurable for the draft we expect you to cover all topics in paragraphsbe concise but concrete in your descriptions for the final version you ll want to go into greater depth approximately paragraphs for each issue with the exception of the research question which should still be be one precise sentence 3 3 project proposal fall http mips lrdc pitt edu we encourage you to iterate multiple times on this abstract while there is only one formally defined point for receiving feedback from course staff you should seek out more informal feedback as you work on this email us at any point if you d like us to take a look at your current submission or come to office hours if you d like to discuss in person you are free to change directions after submitting your draft but the sooner you nail down a direction the better your project is likely to be submission instructions one submission is needed for each project group please send your proposal as an email attachment to the course description this course examines the use of game design techniques for use in playful and serious computer applications and interfaces the emphasis will be on including game elements into interfaces and applications to provide playful experiences to motivate behaviour to connect people or to crowdsource work and evaluating the efficacy and experience of gameful systems course objective course components and the evaluation criteria are designed to reflect the learning objectives of the course the objectives of the course are as follows that students read and critique the seminal and recent research on game motivation game design gameful design gamification serious and persuasive games and evaluating player experience lead a seminar discussion on the topic of the week discuss the advantages and drawbacks of the varying game science readings measure game efficacy and player experience via subjective and objective measures using statistical tools and models implement a gamified system gameful interface or game for research purposes present their system through a project report project presentation and project video similar to how computer game research is presented in the community 3 course enrollment the course will be limited to students so that the discussion aspect of the seminars is supported textbook and lecture notes there are no required textbooks for this course all required readings for the course will be posted in pdf format on the course website or handed out in class the heavy emphasis on discussion means that class attendance is essential to success in this course course website the course website is hosted using moodle and accessible though the cs website http www cs usask ca classes course announcements regarding assignments as well as other information will be communicated to the class via moodle the student is responsible for reading this website regularly lecture topics please see the schedule for a list of topics and dates the following topics may be covered but are subject to change foundations of game science the motivational pull of games player types and personalization serious games o crowdsourcing work though games o motivating behaviour though games o persuasive games o the science of gamification social play o connecting people through play o balancing play o matchmaking o competition gameful design in non game contexts the post blap world of games evaluating player experience computing facilities the project for this course could be completed using a variety of languages all tools can either be downloaded or are available on the computers in the spinks labs or in the hci lab student evaluation assignments reading critiques discussion seminars project assignments a series of small assignments will be completed throughout the course to teach the practical skills need for the final project assignments will be available via the course website and will be graded by the course instructor whether assignments are completed in groups or individually will be specified in the assignment instructions absolutely no late assignments will be accepted for credit absolutely no extensions will be provided for assignment due dates reading critiques readings will be used as tool to reinforce concepts learned in class and there will be weekly reading assignments over the course all reading assignments must be completed individually unless otherwise stated weekly readings will consist of two or three papers depending on paper length i expect you to carefully read and reflect on the papers the critiques are not intended to be summaries or in depth reviews but are a tool to help guide your reading of the papers critique format each critique should follow this format two to three sentences describing the paper o point out what you think is the main contribution do not simply copy from the abstract two to four paragraphs of critique o potential topics for consideration include what idea or innovation enabled this research is there more to be had from that idea or innovation what new questions or research agendas are suggested by this research what would you have done differently with approximately the same resources available to the original authors what would you have done differently with twice or half the resources available to the original authors how might this research have informed some other research you ve seen how does this research relate or compare to other research you have seen what are the limitations of the research o note that these topics are only suggestions please also explore other topics and ideas o focus both on what was done well in the research and what could have been improved list of 3 questions o these questions can be things which you would like clarified or aspects of the paper that you did not understand if the paper was self explanatory the questions might instead be related to things you would like to ask the authors for example you may want to ask them to justify their choice of some treatment of the data or why they chose a specific game imagine that you are listening to the conference presentation of the paper and consider what some good questions regarding the work might be each critique should be approximately a page in length somewhere in the range of words but this isn t a firm requirement your grades will be based on your demonstrated understanding of the content the depth of your insight and the intellectual effort made improvement over the semester could be considered when calculating the final grade keeping the critiques short is an intentional choice i m not interested in you filling up pages with filler short and focused opinions and demonstration of understanding is key grading critiques grades will be given for each critique on the following scale excellent you exceeded the expectations of the assignment showing original thought and interesting insights great you demonstrated an understanding of the paper and made some solid points good you completed the critique but failed to show much original thought or insight ok you summarized the paper without giving your own thoughts or opinions poor you failed to meet expectations absolutely no late reading assignments will be accepted for credit absolutely no extensions will be provided for reading assignment due dates the grading scheme does map well to a through f the default grade will be good you will have to earn the greats and excellent seminar discussions students will be required to lead multiple seminar discussions based on the weekly readings quality of leadership of the discussion and preparedness for the discussion will contribute to the discussion component of the final grade the course instructor will assign the week of your seminar discussion leadership class participation class time will be used for content presentation examples case studies design exercises and group interaction the visual nature of the course content combined with the interactive nature of the content presentation means that class attendance is essential to success in this course all in class activities are improved when there is sufficient class participation as such the discussion seminar portion of the final grade will depend on class participation and will be assigned by the instructor project this course requires completion of a project which has several marked deliverables throughout the term the goal is to provide students with experience in research in computer games there are multiple stages to the project each with a milestone and deliverable please consult the course schedule early and often for timing of components and for details on the deliverables course description the course will cover a variety of topics related to the emerging area of social computing and participative web it will discuss theories technologies and human issues of web 0 how people network online what communities they form why they participate and contribute and how to design infrastructures for successful online communities the course will have three interwoven components analytical which discusses how people act and participate in different kinds of communities based on observational studies the theoretical component will focus on analyzing the interactions in online communities from different perspectives physics and dynamics of complex emergent systems behavioral economics social psychology and sociology using the insights gained from theory the practical component of the course will focus on how to design and use environments to successfully support online communities and participative web applications prerequisites web programming class industrial experience in web programming textbook information recommended texts steven johnson emergence scribner ny albert laszlo barabasi linked plume james surowiecki wisdom of crowds random house dan ariely predictably irrational harper collins charlene li josh bernoff groundswell forrester research joshua porter designing for the social web new riders berkley ca malcolm gladwell the tipping point seth godin tribes penguin amy jo kim community building on the web secret strategies for successful online communities pdf available for purchase online http www peachpit com store product aspx isbn lecture topics social computing and participative web web 0 list of topics communities o sharing content trading and playing o discussions and socialization forums bulletin boards chat im o publishing blogs and wikis wikipedia social networking o coding open source movement theories o analytic theories metcalfe law network effects scale free networks o economic theories wisdom of crowds games reciprocation and norms o behavioural theories social psychology sociology organizational science technologies o search for common meaning personal information management tagging community ontologies semantic web o recommender systems community awareness visualization o trust and reputation mechanisms social network analysis o motivating participation incentive mechanisms o building communities and putting them to work policies missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the application must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term 2 and full year courses http www arts usask ca students transition tips php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the course outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations section of the calendar course description the course will cover a variety of topics related to the emerging area of social computing and participative web it will discuss theories technologies and human issues of web how people network online what communities they form why they participate and contribute and how to design infrastructures for successful online communities the course will have three interwoven components analytical which discusses how people act and participate in different kinds of communities based on observational studies the theoretical component will focus on analyzing the interactions in online communities from different perspectives physics and dynamics of complex emergent systems behavioral economics social psychology and sociology using the insights gained from theory the practical component of the course will focus on how to design and use environments to successfully support online communities and participative web applications prerequisites web programming class industrial experience in web programming textbook information recommended texts steven johnson emergence scribner ny albert laszlo barabasi linked plume james surowiecki wisdom of crowds random house dan ariely predictably irrational harper collins charlene li josh bernoff groundswell forrester research joshua porter designing for the social web new riders berkley ca malcolm gladwell the tipping point seth godin tribes penguin amy jo kim community building on the web secret strategies for successful online communities pdf available for purchase online http www peachpit com store product aspx isbn lecture topics social computing and participative web web list of topics communities o sharing content trading and playing o discussions and socialization forums bulletin boards chat im o publishing blogs and wikis wikipedia social networking o coding open source movement theories o analytic theories metcalfe law network effects scale free networks o economic theories wisdom of crowds games reciprocation and norms o behavioural theories social psychology sociology organizational science harvesting participation o search for meaning and recommendation personal information management tagging o community ontologies semantic web recommender systems trust and reputation mechanisms o motivating participation incentive mechanisms community awareness visualization o building communities and putting them to work policies missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the application must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses http www arts usask ca students transition tips php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the course outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations section of the calendar university of saskatchewan calendar exams grades grading system department policy on academic honesty students are expected to be academically honest in all of their scholarly work including course assignments and examinations academic honesty is defined and described in the department of computer science statement on academic honesty http www cs usask ca content academichonesty academichonesty jsp and the university of saskatchewan academic honesty website http www usask ca honesty the student academic affairs committee treats all cases according to the university policy and has the right to apply strict academic penalties welcome to the university of saskatchewan department of computer science cmpt usability engineering web site for class notes and readings can be accessed by registered students via moodle cs usask ca usability engineering ue is a structured approach to developing usable user interface designs ue helps integrate human computer interaction hci requirements and design approaches within development projects managed by software engineering se methodologies this course presents a requirements engineering re approach to usability engineering requirements engineering a sub discipline of software engineering focuses on applying development processes and documenting information items that support these development processes in order to engineer large scale software projects and to provide information that readily supports further evolutionary development of these projects while re is most needed for large scale projects its processes and information items can also be scaled down to smaller developments this approach is being taken because it can be applied to all types and sizes of developments this class focuses on the needs of users and their tasks which are at the heart of all systems development it demonstrates how re can be applied to engineering usable systems while it does not deal with technical issues such as program or database design and construction it provides clear linkages to those se activities which also fit into the same overall life cycle in addition to providing students with an advanced understanding of ue and re this class introduces them to a variety of significant international standards in the fields of software engineering and of ergonomics please contact prof jim carter carter cs usask ca if you are interested in further information about this class class news history of usability related definitions posted sept course syllabus cmpt catalog description usability engineering ue is a structured approach to developing usable user interface designs ue helps integrate human computer interaction hci requirements and design approaches within development projects managed by software engineering se methodologies this course presents a requirements engineering re approach to usability engineering by providing in depth coverage of putting usability first prerequisites cmpt prerequisite cmpt or permission of instructor cmpt prerequisite graduate student standing course objectives a student successfully completing this course shall be able to identify opportunities for improving the usabillity and accessibility of existing and proposed systems to apply the components of the definitions of usability and accessibility to evaluating and developing interactive systems to apply usability methods in evaluating and developing interactive systems to apply principles and other forms of ergonomic and user interface guidance to evaluating and developing interactive systems to identify and analyze the various components of the overall context of use of an interactive system to develop usability and accessibility specifications that can be used in evaluating and developing interactive systems to identify techniques and technologies that can satisfy usability and accessibility specifications student evaluation cmpt cmpt assignment oct assignment assignment oct assignment term project term project final exam class participation note it is important to complete assignments and the term project on time students having difficulties in meeting the due dates are encouraged to discuss these difficulties with prof carter as soon as possible attendance expectations it is expected that students will attend and participate in all class sessions students who are unable to attend a particular class are requested to advise the instructor by e mail at least minutes prior to the class final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course text recommended reading the main text is a manuscript of the book usability centered development by dr jim carter free access to all chapters of the text will be provided to students registered in the class via the moodle system lecture schedule all dates and topics subject to change dates topics sept course introduction sept sept introduction to usability centered development usability accessibility and related concepts sept sept ch continued usability methods sept sept principles and other sources of guidance discuss assignment applying principles which is due noon tuesday oct possibilities and scenarios oct oct identifying tasks identifying users oct oct identifying content assignment applying principles due noon tuesday oct discuss assignment user testing which is due noon tuesday oct identifying environments oct oct needs assessment discuss term project proposal due noon thursday oct basic task characteristics oct oct additional task characteristics assignment due noon tuesday oct user interaction capabilities oct oct user cognitive and affective capabilities group characteristics project proposal due noon thursday oct nov nov content issues specifying needs requirements and recommendations nov nov interaction navigation design principles for the presentation of information nov nov interface design implementation issues nov nov usability engineering student project presentations dec no class today incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations section of the calendar university of saskatchewan calendar academic courses policy academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals section of the university secretary website and avoid any behavior that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca honesty studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of complaints and appeals http www usask ca honesty studentnon pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca undergrad honesty php for more information on what academic integrity means for students see the student conduct appeals section of the university secretary website at http www usask ca pdf pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to register with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports students must follow dss policy and procedures for more information check http www students usask ca disability or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss welcome to the university of saskatchewan department of computer science cmpt accessible computing web site for this course will investigate the analysis and design of accessibility issues and features related to computing applications while the focus is on computing accessibility this course also provides a comprehensive background to accessibility that can also be applied within other domains this course material has been significantly revised to make use of information in the new iso iec guide on accessibility especially the accessibility goals principles that it discusses class news updated july welcome computer accessibility is extreme hci human computer interaction is about designing usable systems iso guidance on software accessibility defines accessibility as usability of a product service environment or facility by people with the widest range of capabilities and it notes that the concept of accessibility addresses the full range of user capabilities and is not limited to users who are formally recognized as having a disability thus good accessibility serves all of us accessibility is the new frontier of developing usable systems cmpt accessible computing is in the forefront of this movement assets the acm conference that focuses on accessibility published a paper techniques to assist in developing accessibility engineers about our course in before any other accessible computing courses had appeared in north america accessibility involves the ultimate in multi media visual auditory tactile and media shifting to communicate using media that the user is capable of using therefore accessibility involves finding new and alternate ways of interacting with different users within a single application this leads us to consider new ways of using existing technologies and new technologies for existing problems please contact prof jim carter carter cs usask ca for further information learning objectives a student successfully completing this course shall be able to identify opportunities for improving the accessibility of existing and proposed systems to apply and combine various approaches to developing accessible systems to understand the needs of persons with disabilities and to be able to recognize and make use of their abilities to understand and apply the concepts of user preferences individualization and assistive technologies prerequisites credit units of cmpt courses at the level or above or a prerequisite waver from the instructor text recommended reading students enrolled in this class will be provided via moodle with free electronic access to a new accessibility text currently under development lecture discussions the class will meet every m w f afternoon from to in thorvaldson spinks students are expected to attend all class sessions and are requested to inform the instructor via e mail if they will be unable to attend any particular session the role of the lecture sessions is to present important material to the students and to engage all class participants in a discussion of this material questions and discussions are highly encouraged students are responsible for all material covered in the class lecture sessions assignments project note there are no exams the expectations of your class work take this into account this means that the critiques and project play a very significant role in your grade assignments a number of interactive assignments will be used to acquaint students with a range of accessibility issues and a range to techniques for dealing with these issues it is expected that the weekly assgnments will take approximately hr each critiques students will be expected to become familiar with the assigned readings prior to the class in which they will be discussed students will be assigned to prepare critiques of particular readings it is expected that weekly critiques will take approximately hrs each project a major project will require students to investigate a related topic in greater detail than is covered in the class students will be required to make a short accessible presentation about their project the project is divided into phases to provide students with feedback on their progress marking assignments critiques of assigned readings term project project proposal due oct noon project analysis and design report due nov noon project evaluation report due nov noon project presentation week of dec during class time revised project due dec pm course outline all dates and topics subject to change basic concepts introduction sept course overview accessibility involves each of us identifying our own needs who needs accessibility approaches to accessibility sept defining accessibility and usability access accessibility usability approaches to accessibility medical models of disability social models of disability universal accessibility iso iec guide accessibility principles goals user characteristics design strategies critique ch widest range of users due noon friday sept assignment before and after considering accessibility due noon tues sept user focused principles widest range of users sept user characteristics and abilities diverse users diverse vs entitled users diverse contexts of use traditional it contexts mobile contexts internet of things contexts related user accessibility needs and solutions critique ch user expectations due noon friday sept assignment automatic accessibility evaluations due noon tues sept user expectations sept sources of expectations user personal experiences commonly accepted conventions standards and regulations creating new expectations consistency and metaphors intuitiveness vs training related user accessibility needs and solutions critique ch individualization due noon friday sept assignment cultural and linguistic issues due noon tues sept individualization sept oct individualization basics customization and adaptation profiles settings features sharing individualization settings the common accessibility profile cap cloud for all related user accessibility needs and solutions critique ch approachability due noon friday oct project proposal due noon monday oct assignment os based accessibility settings and services due noon tues oct interaction focused principles approachability oct barriers partial and complete physical barriers psychological barriers other barriers e g socio economic navigation possibilities removing barriers alternate means of approaching reassurance and support related user accessibility needs and solutions critique ch perceivability due noon friday oct assignment using a screen reader due noon tues oct perceivability oct human perception perception vs understanding sensory modalities the diversity of devices modalities and media multi modal and multi media modality shifting and loading selecting modalities and media related user accessibility needs and solutions critique ch understandability due noon friday oct assignment colour shifting and shading due noon tues oct understandability oct understanding understanding cognitive aspects affective aspects personality based aspects designing content wcag principles of the presentation of information dealing with complexity related user accessibility needs and solutions critique ch controllability due noon friday oct assignment cognitive issues due noon tues oct controllability oct human control issues means and modalities of control strength and stamina speed and precision designing interactions dialog interaction principles control actions and feedback designing controls related user accessibility needs and solutions critique ch usability due noon friday oct project analysis and design due noon monday nov assignment using voice recognition due noon tues nov task focused principles usability nov effectiveness correctness completeness appropriateness efficiency time used human effort finances and materials satisfaction attitudes emotional effects physiological effects related user accessibility needs and solutions critique ch error tolerance due noon friday nov assignment creating alternative text due noon tues nov error tolerance nov error avoidance risks slips and errors active vs passive guidance user testing error recovery undo redo and related aids related user accessibility needs and solutions critique ch equitable use and compatibility due noon friday nov project evaluation due noon monday nov assignment using head and eye tracking due noon tues nov system focused principles equitable use and compatibility nov equitable use identical manner of use equivalent manner of use compatibility assistive technologies designing ats related user accessibility needs and solutions assignment using single switch input due noon tues dec student project presentations presentations in class dec revised project due pm friday dec policies in this class late assignments ciritques and projects late assignments critiques and parts of the project will automatically receive marks unless the student receives an extention from the instructor students receiving extentions may at the discreetion of the instructor have their mark reduced by up to for each day that the assignment critique or part of the project is late students with a sufficiently serious reason for being late with an assignment that is acceptable to the instructor may be allowed an extention it is important to contact the instructor as soon as possible to request any extention the instructor will not accept notes from student health services as support for the student reason for being late with an assignment missed assignments critiques and parts of the project missed assignments and critiques will receive a mark of missed examinations there are no examinations in this course general policies incomplete course work and final grades when a student has not completed the required course work by the time of submission of the final grades they may be granted an extension to permit completion of an assignment the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade if an extension is granted and the required assignment is submitted within the allotted time the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations section of the calendar university of saskatchewan calendar academic courses policy academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals section of the university secretary website and avoid any behavior that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of complaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca undergrad honesty php it is nice to have an awareness of accessibility needs and solutions but it is far more important to put this into action that is the basis of a new course at the university of saskatchewan cmpt accessible computing focuses on producing accessibility engineers it helps students from diverse backgrounds with a wide range of motivations but with at least a reasonable background in computing science to discover their own approach to integrating accessibility concerns within the development of mainstream computing applications and systems while the course presents students with a comprehensive survey of user needs and accessibility issues it recognizes the importance of developing an engineering approach to the application of accessibility principles and technologies to particular development activities for actual products and systems this engineering approach incorporates consideration of the importance of using methodologies and methods to ensure that user requirements are met a methodology is a high level structure of individual methods that guides development throughout the life cycle of a product or system from the initial identification of a need for a new or improved product or system through its development and use to its final replacement or retirement methods specify the means of combining and applying research and experience to one or more life cycle activities this course situates accessibility engineering within the family of engineering practices already developed within information technology systems engineering deals with general methodologies and methods for use in the development of all types of information technology systems software engineering deals with general methodologies and methods for use in the development of all types of software systems usability engineering deals with specific methodologies and methods for use in the development of user interfaces to all types of systems since users generally interact with a combination of hardware and software it needs to deal with both systems and software engineering accessibility engineering deals with specific methodologies and methods for use in the development of all types of accessible systems iso and iso define accessibility as usability of a product service environment or facility by people with the widest range of capabilities this ties the emerging field of accessibility engineering to usability engineering and through it to software and systems engineering the participants the january version of cmpt involved one instructor one teaching assistant and nine students the instructor is a professor of computer science and the head of the usability engineering research lab userlab at the university of saskatchewan he is the co developer of the universal access reference model the editor or iso iec tr guidelines for the design of icons and symbols accessible to all users including the elderly and persons with disabilities and the co editor of iso accessibility guidelines for information communication technology ict equipment and services and of iso iec information technology framework for specifying a common access profile cap of needs and capabilities of users systems and their environments the teaching assistant is a graduate student in computer science and a member of userlab who took a previous version of the course three years before his participation in that previous version of the class led to the development of the universal access reference model the teaching assistant is the editor of iso iec which is based on his master thesis research he is also hard of hearing in addition to leading the lab exercises the accessibility demonstration experiences he participated in the class sessions along with the other students the course was made available in january as a special topics course to both senior undergraduate and graduate students it has since been approved as a regular offering of the computer science department for undergraduate and graduate students the prerequisites have remained relatively open focusing on an advanced standing in computer science rather than on particular courses in january four graduate students and five undergraduate students took the course reasons for taking the course varied considerably between students one student was blind and one student had a brother with cerebral palsy four students recently came from countries where english was not their first language i e from china and from finland while the majority of students were interested in human computer interaction or usability engineering one undergraduate student chose it because she had a lack of proficiency in technical english and one undergraduate student chose the course as an alternative to taking an advanced algorithms class the treatment the methodology the course starts from the perspective that we all have many disabilities by accepting disabilities as normal to the human condition we make the provision of accessible systems equally normal thus it is recognized from the start that accessible design is good design and that it should be the norm rather than the exception the course also makes use of the universal access reference model to recognize that identifying and removing barriers to communications is more important than assigning the blame for these barriers the course topics follow an engineering life cycle approach building knowledge and understanding systematically they follow a simplified life cycle approach from problem identification through analysis to design and construction rather than relegating evaluation and testing to a single stage late in the life cycle the course includes evaluation throughout the life cycle the course takes an active learner centered involvement approach in the class sessions the lab exercises and the project by focusing more on student activities than on traditional lectures it should be easier for other instructors to reproduce successfully the emphasis of class sessions is on students critiquing and discussing a variety of research papers and international standards student critiques go beyond recognizing existing knowledge requiring them to identify important challenges problems with what the papers suggest and opportunities omissions that the papers failed to include prior to the first class of the week students are expected to read the main paper for the week and to submit a critique containing five challenges and or opportunities the instructor then uses the best critique items as the basis for the week discussions the use of critiques is discussed further in the following section additional papers are provided to reinforce topics to be discussed for the week the lab component provides students with a range of first hand experiences via the set of userlab accessibility demonstration experiences ades each ade introduces an accessibility issue set of barriers to the abilities of individuals to access various forms of information technology or option strategies and technologies for meeting the needs of users with specific partial or full disabilities combinations of disabilities or barriers resulting from the user environment each ade is intentionally kept short enough to leave students wanting to know more most ades also provided suggestions for further readings and research that students could use to satisfy their desire to know more and or to help create the basis of a class project these ades are further discussed later in this paper the projects expect students to research and apply some aspect of accessibility engineering at a level more advanced than that covered by the main portion of this class the requirements and methods of class projects are discussed later in this paper critiques and their discussions critiques are very important for the class since they form the basis of most course discussions each critique deals with the primary reading for that week critiques are not book reports or summaries of a paper creating good critiques requires students to thoroughly understand the paper being critiqued to identify the important concepts in the paper and most importantly to go beyond what it contains critiques involve identifying and discussing at least five major challenges and or opportunities critique items arising from the paper challenges identify portions of a paper where significant improvements e g further discussion and or consideration of an alternate viewpoint should be made opportunities identify omissions from a paper where significant additions e g of topics not discussed but that should be included in a discussion of this area should be made each critique item is formatted to identify the challenge or opportunity it is expected that identification involves o a meaningful self descriptive name for the challenge opportunity o a brief elaboration discussion of what is involved and o information about the location in the paper where the challenge opportunity occurs explain the significance of the proposed addition or improvement students are expected to determine the significance and specify its affect on the success of applying this information in performing accessibility engineering it is important for critiques to go considerably beyond just dealing with editorial issues such as grammar spelling or any changes that could be made by just adding a few words it is expected that the discussion of significance involves a good reason why this challenge or opportunity should be discussed in class o challenges can be significant if they pose risks to the technical information in understanding of the readings or if the student has strong grounds to disagree with major points in the reading o opportunities can be significant if they involve omissions that need to be explained for someone to be able to understand the readings suggest what should be done about this challenge or opportunity this should be the starting point for a discussion in class it is expected that suggestions include a summary of what the student thinks about or needs to know about this challenge or opportunity students are encouraged to provide particular references to support their suggestions critiques are e mailed to the instructor the night before the first class of the week each critique item is marked out of a maximum two points and the marks of the five best if more than the minimum five are submitted are recorded as the mark for the critique out of a maximum of ten points thus students are encouraged to include more than the required minimum of five items the following criteria are used in marking individual critique items points for missing or irrelevant discussions point for incomplete or weak discussions points for very good to excellent discussions in addition to this marking all point critique items are evaluated to see whether they are suitable as the basis for class discussion the suitable critique items are considered items a record of the obtained by each student is kept as a partial indication of the student contributions to the class once marking is completed the instructor consolidates all of the critique items for use in class discussions the actual method of using these items evolves throughout the course helping students to deepen their involvement in the discussions and with the materials being discussed as the course progresses less and less detailed information is used to start the discussions in early weeks the complete text of critique items is presented to the students in class and volunteers are asked to comment usually the most comments come from the author of the critique item after a few weeks only the suggestions from the critique items are presented to the class and students are expected to take turns e g based on their seating arrangements discussing items in the set so that all students participate in the discussion this has the added benefit of providing models of good critique items to other students by the middle of the course students are only provided the meaningful self descriptive names of critique items written on individual cards that they randomly draw turn taking is now based on a sequence number that is indicated on each card towards the end of the course the instructor replaces individual critique items with questions that help consolidate ideas from individual critique items the emphasis of discussions of critique items is on developing informed critical thinking skills relating to accessibility engineering within the students therefore the amount of comments made by the instructor and teaching assistant should be limited to introducing major discussion points missed by the students the instructor role is to focus more on integrating student points and on steering the discussion than on lecturing to the points for the primary reading each week also has one or more secondary readings that can supplement the reading assigned to be critiqued ideally the students should read and consider these readings however from a practical standpoint it is recognized that most if not all of their reading time for the week will focus on the primary reading thus it remains to the instructor to provide a short summary of any main points in these secondary readings that have not been covered depending on the extent of discussions on the critique items there may be little time to present this summary the weekly topics week introduction to accessibility issues the goal of week was to recognize that we all have disabilities in some aspects of our life and that these disabilities may hinder accessibility to certain aspects of life achieving this goal involved identifying our own disabilities and needs accepting the disabilities and needs of others and developing an initial approach to accessibility we specifically avoided any attempt to limit prioritize or evaluate the set of disabilities claimed by people in this way we recognized that all of us have some disabilities and that what really matters is to identify barriers we have that prevent full access to some aspect of life due to designs that require abilities that we do not possess by expanding our set of disabilities far beyond those that are recognized in approaches such as us section we were able to have all participants recognize that providing accessibility is more than just an accommodation it is a fundamental expectation of appropriate design in this way we also avoided providing artificial boundaries on accessibility opening up the course to investigate all aspects that could contribute to greater accessibility we also avoided any tendency to argue my disability is more important than yours week universal accessibility the goal of week was to go beyond consideration of the needs of individuals and recognize the needs and implications of achieving universal accessibility since we had already achieved consensus on the importance of universal accessibility we focused on some of the major issues identified in student critiques of stephanidis paper on from user interfaces for all to an information society for all these included considerations of how dialogue abstraction and design patterns can be used to separate between communication needs and media specific implementations of communications an analysis issue tradeoffs between platform independent solutions and platform specific solutions a design issue the potential for using information about user abilities preferences and needs in customizing interaction a design and the extensive range of user testing needed to ensure accessibility to the widest possible range of users an evaluation issue by containing a range of issues relating to various activities in a systems development life cycle this discussion was used as a general introduction to the need for considering analysis design and evaluation in accessibility engineering the class then discussed a critique prepared by the instructor on keates paper on pragmatic research issues confronting hci practitioners when designing for universal access this discussion both served to consider another introductory point of view and to provide students with a concrete example of the level and style of critiquing that was expected of them week sensory limitations week started detailed treatment of analysis related issues the goal of this week was to gain an understanding of the major types of disabilities that are typically considered within the scope of accessibility concerns we investigated the needs of individuals with visual auditory and physical disabilities by discussing one research paper and a summary of user accessibility needs considerations coming from student critiques of jacko conceptual framework for individuals with disabilities included moving away from medical issues to abilities and disabilities differences between models profiles and individual needs matching abilities to tasks conflicting needs in multi user environments and the need for development methods to satisfy differing user needs and capabilities a discussion of iso iec swg accessibility user needs summary that originally focused on supporting the development of accessibility standards led to recognizing that there is an extremely large number of user needs to consider to achieve universal accessibility and even comprehensive sets of user needs such as the one discussed may miss some user needs and thus the analysis for a project should be sure to investigate the actual needs of its intended users not just adopt a predefined set of needs week methodologies for providing accessibility part i week started investigations of how accessibility considerations can be integrated within various types of systems software development methodologies it focused on an example of a process oriented methodology and on the use of principles in providing high level guidance to development and evaluation considerations coming from student critiques of the canadian accessibility domain architecture included the difference between providing accessibility as part of design and needing to provide accommodations to overcome design limitations the importance of operating systems providing accessibility features that can be used consistently by multiple application programs the difference between providing multiple formats and providing flexible formats and going beyond presenting content accessibly to make all navigation and interaction accessible the strengths and weaknesses of using principles to guide development were also discussed week methodologies for providing accessibility part ii week continued investigations of ways of integrating accessibility within systems software development methodologies it focused on examples of model based and forms based methodologies considerations coming from student critiques of the universal access reference model included the effect of the environment context of use on accessibility the use of shared context in helping to make communications accessible the effect of interactions between interaction channels including multiple channels that make use of the same modality on resulting accessibility and the difference between a user preferences and a user needs discussion of a forms based approach recognized that extensive documentation does not guarantee that resulting systems will be accessible only that a lot of time will be spent on creating the documentation week accessibility standards week considered international standards dealing with the accessibility of information and communication technology due to both the timing of its development and the involvement of the instructor as its co editor students had the unique opportunity to create informative notes and examples for inclusion in iso a high level accessibility standard applying to information and communication technology critiques resulted in twenty four notes and ten examples being added as well as five significant rewordings to existing guidelines in the standard the class then compared the detailed guidance on software accessibility contained in iso with the more general guidance in iso and discussed how both standards might be applied to various development activities because of the unique situation this year it is expected future years will concentrate more on the application of standards than on the improvement of them week cultural linguistic adaptability week dealt with requirements strategies and implementations relating to cultural and linguistic accessibility cla while many people may feel that cla is beyond the bounds of traditional accessibility language is considered an accessibility issue in canada considerations coming from student critiques of a set of cla guidelines included the difference between using guidelines for evaluation and those for design the wide range of cultures including ethnic professional age related the sharing of contextual information with users from other cultures the need to deal with cultural limitations and differences in the use of various symbols the use of metadata to help interpret symbols and the range of issues involved in translating text properly the class then considered how to include cross cultural accessibility within development week analyzing and evaluating accessibility week dealt with techniques and technologies for evaluating accessibility related issues student critiques focused on the common accessibility profile cap approach to identifying media related accessibility issues they focused on various detailed technical issues and limitations of caps a lively discussion was held noting how the cap standard actually addressed many of the issues and limitations they identified in the conference paper that they read and critiqued the class then discussed the strengths and limitations of automatic accessibility tools based on their ade experience with using a web accessibility evaluation tool and the second reading for the week week accessibility features of specific technologies week dealt with various web accessibility initiative guidelines 38 and with the use of metadata registries to support the standardization of the meaning of various common types of content discussions resulting from student critiques of the web content accessibility guidelines included the uses and abuses of scoping of conformance the uses and abuses of baselines in evaluating accessibility techniques and limitations to making text understandable readable and translatable techniques for expanding abbreviations and automatically substituting for difficult or unknown figures of speech and techniques for error avoidance minimization and correction the discussion of metadata considered both the inclusion of metadata and alternate data within designed content and the identification of suitable metadata or alternate data from content that does not have its own or sufficient metadata or alternate data week assistive technologies week dealt with various issues relating to finding and using assistive technologies discussions resulting from student critiques of the at it compatibility guidelines included possibilities for application program interface api specifications that would be reserved for the use of assistive technologies the importance of using standard features in operating systems and issues that arise when devices share access to resources this discussion was supplemented by a consideration of new work that is moving towards solutions for these issues after briefly considering the role and the range of assistive technologies 33 the last class was spent with a guest presenter demonstrating some state of the art assistive technologies week adaptive technologies week dealt with various strategies of individualization including customization and automatic adaptation discussions resulting from student critiques of stephanidis paper on supporting interface adaptation included managing the conflicting needs of consistency and adaptation including avoiding confusing the user when adapting limitations and concerns with portability and storage of user profiles including on line profiles and using profiles on publically available systems possibilities and limitations with the system learning about the user including detecting where a user needs assistance and where adaptations may be helpful and limiting instances where the system interferes with the user including mistaking the user tasks or pressuring the user to speed up further discussion dealt with the role of modeling in designing individualizations week advanced research topics week combined considerations of a variety of cognitive issues both for developers and users the primary reading focused on the role of context in unifying designs for all the other two readings dealt with some of the issues in trying to support persons with cognitive disabilities discussions resulting from student critiques of stary paper on a structured contextual approach to design for all included how to evaluate our development processes to ensure we keep improving how the structuring of tasks can affect the design and the resulting accessibility how to manage contradictions that come up during development how to deal with current planned and unplanned contexts of use and the role of optimizing in development discussions regarding developing systems for persons with cognitive disabilities focused on the wide range of different cognitive disabilities and the lack of a clear structure of user needs comparable to the user needs identified for persons with physical disabilities one of the major concerns discussed how to make the required different simplicity tactics for different users accessible to the particular users who need them week project presentations the university term is weeks in canada the goal of week is to determine the students understanding of their own project and to expose other students to aspects of accessibility that are beyond those already covered in the course week consisted of minute presentations about their projects by each of the students this included minutes for handling questions from other students and from the instructional team lab experiences the ades mentioned above made up the lab experiences for this course these ades are designed to be accessible to a wide range of users and are intended to help students understand the needs and expectations of users with disabilities the ades cover a wide range of issues and options in accessible computing five ades were designed for this course another seven are currently under development they explore web accessibility testing for accessible design using a screen reader issues specific to users with cognitive disabilities and the built in accessibility settings and services available with modern operating systems although some of these individual exercises are based on materials or exercises available via the web these source materials can be hard to find and are not otherwise available as a comprehensive set that represents the widest possible range of accessibility issues and options further while different exercises from different sources may not be consistently presented the ades are consistently formatted and follow a standard structure the ades were developed so that they could be completed within a one hour tutorial and to fit the need to rapidly develop student understanding of the issues surrounding accessible computing beyond a theoretical understanding the activities were intended to provide tutorial information as well as hands on activities to the extent possible the ades were intended to be platform independent to allow students to complete them either in the computer facilities provided for the course or on their own computers upon completion the ades required students to submit a report on their experience for marking purposes due to limitations in the computing facilities provided for the course the activity on screen readers used voiceover version the screen reader available with the macintosh tiger operating system likewise the activity on built in accessibility settings and services was specifically designed for exploration of the windows xp operating system future development of these ades and changes to computing facilities is expected to make these activities more platform independent comments from students were solicited during and after completing each ade these evaluations of the existing set of ades helped in revising the common structure in revising the specific contents of existing ades and in identifying and developing new ades for use in future courses the ades are designed to follow a common structure of four major sectons the first major section of every ade is an introduction to the topic usually this introduction consists of a multimedia presentation primarily intended to motivate the students and to open their mind to the topic area of the ade the introductions also outline the goals of the ade and may include a brief presentation to relate the ade to topics within the class syllabus the second major section is the interactive activity ia ias are used to engage students through first hand experiences with some representative aspects of the accessibility issue or option being explored there is no attempt to teach the students all they should know about the topic rather the set of activities are intended to encourage students to explore the topic more fully on their own both through available technology and literature resources most ades have multiple ias the ade provides unifying guidance on using individual activities whether they come from other sources or were built in house by userlab a key consideration in the selection and design of each ia is the amount of interaction involvement that it provides the student various techniques are used for creating interaction in different ias including simulations using actual tools and interactive dialogues the third major section is a reflection activity students are asked to reflect on their experiences and to consider how they can apply them to the design of accessible computing specific questions about each ia focus on what was learned and how their experience might be applied to improving design accessibility students may also be asked to specifically reflect on how they felt during the activity the last section of every ade provides recommendations for further activities that students can do on their own as well as a list of other resources that can provide further information and may also form the basis of readings to support course projects while it is hoped that students will choose to follow up selected activities and references there are no particular expectations placed on the students the ades are publicly available at http userlab usask ca ade projects projects provide students with an opportunity to either complete an accessibility research project or to apply accessibility engineering to some particular development project project ideas can come from class discussions or from individual student suggestions that are discussed with the instructor undergraduate projects can be done with or without developing software as long as they contained significant application of accessibility to some problem area some of the initial suggestions for undergraduate projects included doing a professional quality accessibility evaluation of an organization web site analyzing and designing how to include accessibility within other software usability engineering methods and developing and evaluating accessibility methods for particular problems graduate projects go beyond undergraduate ones by requiring the development of some useful software to demonstrate or apply the aspect of accessibility that was researched by the student some initial suggestions for graduate level projects included creating additional ades and creating tools to assist individual users undergraduate students were also allowed to do partial versions of graduate project topics the project involves three formal stages a proposal research and development of the project and a presentation of results to the class students are strongly encouraged to discuss possible project topics with the instructor as part of developing their proposals proposals require a title a justification of the uniqueness and significance of the proposed work an analysis of relevant background materials a planned methodology and a description of the type of deliverables that will result from the project once a proposal is accepted by the instructor it becomes a learning contract that forms the basis for evaluating the student project developing good quality deliverables ensures the student of a good mark additions to or deletions from the set of agreed upon deliverables will have a major impact on the student mark the results the course produced a variety of results that can be used to evaluate its success quality of critique items each week a sufficient number of excellent items were obtained to require all of the available class time in their discussion due to the large number of excellent items the instructor was able to become more and more selective of items deserving of a as the course progressed in the first week students averaged excellent items per critique eic this rose slightly to an overall average of eic over the duration of the course two weeks where the average went below eic had particularly short and narrowly focused primary readings three weeks achieved averages above eic providing expert advice students in the course were offered the rare opportunity to provide advice to international groups of accessibility experts the iso iec special working group accessibility swg a user needs summary uns is a major computer accessibility document that has been developed by a large group of accessibility experts over the last few years in september version 0 was issued recognizing that it has achieved a significant level of stability and completeness however swg a also recognized that there might be room for further additions after discussing the uns in class students were given the opportunity to contribute to its evolution the instructor was asked by iso iec user interfaces sc of which he is an expert member to find a way to include cultural and linguistic issues into the uns during the week that the class was discussing this very issue he sent an e mail request for cultural and linguistic user needs to the students and received six suggestions after discussing them in class he forwarded five reworded suggestions to iso iec which then refined and forwarded all five to the swg a for possible inclusion at their april meeting swg a accepted all five with revisions recognizing that four of them were completely new and that the fifth was a new need to be added to an existing category 3 the projects students in this course produced a range of high quality projects dealing with a variety of accessibility issues and options three of these projects have formed the basis for new ades in the areas of cultural and linguistic issues secondary encoding and assisting vision one project has produced a tool to help blind database developers create entity relationship diagrams that can be used by sighted developers three of these projects have led to further research resulting in conference papers the students this course has significantly influenced some of the students three of the nine students are now interested in graduate work in the area of computer accessibility the project to help blind computer developers is currently being expanded into a software engineering tool for uml diagrams as the basis for one student master thesis two of the undergraduate students are now considering becoming graduate students to further their understanding of this field conclusions we have found that teaching accessibility engineering goes beyond providing students with an awareness of accessibility issues and helps prepare them to make real contributions to improving the accessibility of information systems by having students critique a selection of suitable papers they learn how to both use and question the information in those papers by following an engineering life cycle students acquire the skills and organization to successfully apply this learning to research and development activities the results of our course surpassed our expectations and help validate accessibility engineering as a discipline whose time has come the university of saskatchewan saskatoon canada department of computer science cmpt human computer interaction course outline 2014 2015 term course description fundamental theory and practice in the design implementation and evaluation of human computer interfaces course objectives course components and the evaluation criteria are designed to reflect the learning objectives of the course the objectives of the course are as follows that students read and critique the seminal work on human computer interaction practice the skills of ideation low fidelity prototyping and medium fidelity prototyping evaluate the usability of a system through techniques that do not involve the end user e g heuristic evaluation and those that do involve an end user e g controlled user studies iterate on an interface using the results of the evaluation implement an interface using the skills of ideation prototyping and evaluation present and communicate their interface and evaluation through a project report project presentation and project video similar to how human computer interaction research and case studies are presented in the community textbook and lecture notes there are no required textbooks for this course although some books are recommended as a reference all required readings for the course will be posted in pdf format on the course website or handed out in class partial lecture notes will be provided online via the course website however lecture notes are not a substitute for attending class class time will be used for content presentation examples case studies design exercises and group interaction the visual nature of the course content combined with the interactive nature of the content presentation means that class attendance is essential to success in this course recommended textbook rogers y sharp h preece j interaction design beyond human computer interaction edition john wiley sons isbn isbn 6 course website the course website is available via moodle course announcements regarding assignments and examinations as well as other information may and will be communicated to the class via this website the student is responsible for reading this website regularly lecture topics please see the course schedule for a list of topics and lecture dates the following topics may be covered but are subject to change introduction design principles o pathological designs o the psychology of everyday things o guis and wimps o design thinking o ideation prototyping o low fidelity paper o medium fidelity e g flash tcl tk o high fidelity ui toolkits evaluating with and without the user o usability inspection o observational evaluation o surveys interviews and focus groups o quantitative evaluation human processing o information processing o motor processing o visual attention graphic design principles o c r a p layout o colour o typography applications o ubiquitous computing o affective computing o computer supported collaboration o games o tangible interfaces in addition examples and case studies of the aforementioned techniques will be discussed from different domains and applications including facebook online dating sites computer games blogs and productivity software advanced topics may cover issues not described here class schedule lectures tuesday and thursday biol there are no tutorials for computing facilities the programming assignments for this course will be in a variety of languages everything needed will be available on computers in the spinks labs student evaluation all students will have the following weighting of course components determine their final grade midterm exam assignments project class participation final exam note that graduate students have a different weighting midterm exam assignments project class participation 5 final exam important regulations all students must be properly registered in order to attend lectures and receive credit for this course failure to write the final exam will result in failure of this course to obtain a passing grade in this course the weighted average of the student midterm test and final exam grades must be at least to be eligible to write the final examination the student must have a standing of at least in all other course work weighted average of all assignments and midterm 11 examination regulations a student who misses the midterm test due to illness must contact their instructor by email on the day of the missed test explaining the reason for their absence the student must subsequently provide appropriate medical documentation to the course instructor at which time the instructor and the student shall discuss how the missed exam will be made up a student who cannot attend a midterm test for religious reasons or due to a conflict with another class or examination must inform the instructor at least two weeks prior to the test date so that alternative arrangements can be made a student who misses the final examination for any reason has a conflict with another final examination or cannot attend the final examination for religious reasons must follow the appropriate procedures outlined in the university of saskatchewan calendar assignments assignments will be used as tool to reinforce concepts learned in class all assignments must be completed individually unless otherwise stated assignment due dates there will be a number of small assignments over the course assigned as the course material dictates students will be given a week to complete any assignment that is expected to take more than an hour to complete smaller assignments may be due the following class submission of assignments submission of all assignments will occur via moodle 3 late assignment policy absolutely no late assignments will be accepted 4 assignment extensions absolutely no extensions will be provided for assignment due dates project this course requires completion of a single team project which has several marked deliverables throughout the term the goal is to provide students with practical experience in negotiating the iterative stages of the user interface design cycle and in conducting user centered design students will participate in the same team throughout the course graduate students will have the option to complete the course project individually for the project students will either 1 analyze and then specify design prototype and evaluate an improvement to an existing user interface that is demonstrably flawed or specify design prototype and evaluate an innovative and novel interface or 3 conduct a small research project in the area of human computer interaction there are five stages to the team project each with a milestone and deliverable more detail for each component will become available in time please consult the course schedule early and often for timing of components 1 project components 1 proposal students will propose an interface that could be improved or created based on a preliminary analysis ideation low fi prototyping and evaluation without users students will create a design using low fidelity prototyping techniques and conduct a low cost usability test of the prototype 3 mid fi prototyping and evaluation design students will create a detailed evaluation design and build a testable prototype using the medium fidelity method of choice 4 evaluation and recommendations students will carry out the evaluation analyze the results discuss findings and make recommendations in a project report 5 presentation students will deliver a presentation to the class which will include a video and or demo component 2 project grading scheme each project component will be graded and given a weight of the total project grade of grade if option c research project is chosen these weights can be adjusted 1 proposal 2 ideation low fidelity prototype and evaluation without users 3 medium fidelity prototype in class demo and evaluation plan 4 evaluation and recommendations final report 5 class presentation 3 team peer evaluation group work is beneficial for the learning experience but has the drawback that some members of a group may not carry their weight in terms of group participation to mitigate this factor students will perform peer evaluations of their project group members these evaluations will be used to scale the project grade 4 submission of project submission instructions for projects will be given in the descriptions of individual project components programming components will be submitted using moodle 5 late project policy absolutely no late project components will be accepted for grading as the project components build upon each other feedback will be provided on late projects but the grade for the late component will be zero 13 6 project extensions absolutely no extensions will be given for project components course description this course examines the use of game design techniques for use in playful and serious computer applications and interfaces the emphasis will be on including game elements into interfaces and applications to provide playful experiences to motivate behaviour to connect people or to crowdsource work and evaluating the efficacy and experience of gameful systems course objective course components and the evaluation criteria are designed to reflect the learning objectives of the course the objectives of the course are as follows that students read and critique the seminal and recent research on game motivation game design gameful design gamification serious and persuasive games and evaluating player experience lead a seminar discussion on the topic of the week discuss the advantages and drawbacks of the varying game science readings measure game efficacy and player experience via subjective and objective measures using statistical tools and models implement a gamified system gameful interface or game for research purposes present their system through a project report project presentation and project video similar to how computer game research is presented in the community course enrollment the course will be limited to students so that the discussion aspect of the seminars is supported textbook and lecture notes there are no required textbooks for this course all required readings for the course will be posted in pdf format on the course website or handed out in class the heavy emphasis on discussion means that class attendance is essential to success in this course course website the course website is hosted using moodle and accessible though the cs website http www cs usask ca classes course announcements regarding assignments as well as other information will be communicated to the class via moodle the student is responsible for reading this website regularly lecture topics please see the schedule for a list of topics and dates the following topics may be covered but are subject to change foundations of game science the motivational pull of games player types and personalization serious games o crowdsourcing work though games o motivating behaviour though games o persuasive games o the science of gamification social play o connecting people through play o balancing play o matchmaking o competition gameful design in non game contexts the post blap world of games evaluating player experience computing facilities the project for this course could be completed using a variety of languages all tools can either be downloaded or are available on the computers in the spinks labs or in the hci lab student evaluation assignments reading critiques discussion seminars project assignments a series of small assignments will be completed throughout the course to teach the practical skills need for the final project assignments will be available via the course website and will be graded by the course instructor whether assignments are completed in groups or individually will be specified in the assignment instructions absolutely no late assignments will be accepted for credit absolutely no extensions will be provided for assignment due dates reading critiques readings will be used as tool to reinforce concepts learned in class and there will be weekly reading assignments over the course all reading assignments must be completed individually unless otherwise stated weekly readings will consist of two or three papers depending on paper length i expect you to carefully read and reflect on the papers the critiques are not intended to be summaries or in depth reviews but are a tool to help guide your reading of the papers critique format each critique should follow this format two to three sentences describing the paper o point out what you think is the main contribution do not simply copy from the abstract two to four paragraphs of critique o potential topics for consideration include what idea or innovation enabled this research is there more to be had from that idea or innovation what new questions or research agendas are suggested by this research what would you have done differently with approximately the same resources available to the original authors what would you have done differently with twice or half the resources available to the original authors how might this research have informed some other research you ve seen how does this research relate or compare to other research you have seen what are the limitations of the research o note that these topics are only suggestions please also explore other topics and ideas o focus both on what was done well in the research and what could have been improved list of questions o these questions can be things which you would like clarified or aspects of the paper that you did not understand if the paper was self explanatory the questions might instead be related to things you would like to ask the authors for example you may want to ask them to justify their choice of some treatment of the data or why they chose a specific game imagine that you are listening to the conference presentation of the paper and consider what some good questions regarding the work might be each critique should be approximately a page in length somewhere in the range of words but this isn t a firm requirement your grades will be based on your demonstrated understanding of the content the depth of your insight and the intellectual effort made improvement over the semester could be considered when calculating the final grade keeping the critiques short is an intentional choice i m not interested in you filling up pages with filler short and focused opinions and demonstration of understanding is key grading critiques grades will be given for each critique on the following scale excellent you exceeded the expectations of the assignment showing original thought and interesting insights great you demonstrated an understanding of the paper and made some solid points good you completed the critique but failed to show much original thought or insight ok you summarized the paper without giving your own thoughts or opinions poor you failed to meet expectations absolutely no late reading assignments will be accepted for credit absolutely no extensions will be provided for reading assignment due dates the grading scheme does map well to a through f the default grade will be good you will have to earn the greats and excellent seminar discussions students will be required to lead multiple seminar discussions based on the weekly readings quality of leadership of the discussion and preparedness for the discussion will contribute to the discussion component of the final grade the course instructor will assign the week of your seminar discussion leadership class participation class time will be used for content presentation examples case studies design exercises and group interaction the visual nature of the course content combined with the interactive nature of the content presentation means that class attendance is essential to success in this course all in class activities are improved when there is sufficient class participation as such the discussion seminar portion of the final grade will depend on class participation and will be assigned by the instructor project this course requires completion of a project which has several marked deliverables throughout the term the goal is to provide students with experience in research in computer games there are multiple stages to the project each with a milestone and deliverable please consult the course schedule early and often for timing of components and for details on the deliverables course description the course will cover a variety of topics related to the emerging area of social computing and participative web it will discuss theories technologies and human issues of web how people network online what communities they form why they participate and contribute and how to design infrastructures for successful online communities the course will have three interwoven components analytical which discusses how people act and participate in different kinds of communities based on observational studies the theoretical component will focus on analyzing the interactions in online communities from different perspectives physics and dynamics of complex emergent systems behavioral economics social psychology and sociology using the insights gained from theory the practical component of the course will focus on how to design and use environments to successfully support online communities and participative web applications prerequisites web programming class industrial experience in web programming textbook information recommended texts steven johnson emergence scribner ny albert laszlo barabasi linked plume james surowiecki wisdom of crowds random house dan ariely predictably irrational harper collins charlene li josh bernoff groundswell forrester research joshua porter designing for the social web new riders berkley ca malcolm gladwell the tipping point seth godin tribes penguin amy jo kim community building on the web secret strategies for successful online communities pdf available for purchase online http www peachpit com store product aspx isbn lecture topics social computing and participative web web list of topics communities o sharing content trading and playing o discussions and socialization forums bulletin boards chat im o publishing blogs and wikis wikipedia social networking o coding open source movement theories o analytic theories metcalfe law network effects scale free networks o economic theories wisdom of crowds games reciprocation and norms o behavioural theories social psychology sociology organizational science technologies o search for common meaning personal information management tagging community ontologies semantic web o recommender systems community awareness visualization o trust and reputation mechanisms social network analysis o motivating participation incentive mechanisms o building communities and putting them to work policies missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the application must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses http www arts usask ca students transition tips php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the course outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations section of the calendar digits is a wrist worn sensor that recovers the full pose hand tracking interaction mobile wearables of the user hand this enables a variety of freehand inter actions on the move the system targets mobile settings and introduction is specifically designed to be low power and easily repro our hands are extremely dexterous making them the pri ducible using only off the shelf hardware the electronics mary mechanism to manipulate and interact with the phys are self contained on the user wrist but optically image the ical world understandably a considerable focus of hci re entirety of the user hand this data is processed using a new search has been in transferring such natural hand manipu pipeline that robustly samples key parts of the hand such lations into the digital domain however current user inter as the tips and lower regions of each finger these sparse faces rarely leverage the full dexterity of our hands this is samples are fed into new kinematic models that leverage the largely due to the technical challenges in sensing the full biomechanical constraints of the hand to recover the pose pose of the hand with its many degrees of freedom dof of the user hand the proposed system works without the consequently systems constrain the problem along differ need for full instrumentation of the hand for example using ent dimensions such as limiting hand tracking to input data gloves additional sensors in the environment or depth only focusing on fingertips or other specific parts of cameras which are currently prohibitive for mobile scenarios the hand only supporting interactions through sur due to power and form factor considerations we demon faces and other tangible mediators or support strate the utility of digits for a variety of application sce ing a small discrete set of hand gestures until narios including spatial interaction with mobile devices recently real time hand tracking required full instrumen eyes free interaction on the move and gaming we conclude tation of the hand see researchers particularly in the with a quantitative and qualitative evaluation of our system computer vision community have begun to demonstrate al and discussion of strengths limitations and future work gorithms for real time hand pose recovery and acm classification keywords their applications for hci whilst sensing the full pose of the hand is becoming more h information interfaces and presentation user tractable particularly with the advent of consumer depth interfaces interaction styles cameras there are certain domains where it still remains a fundamental challenge the mobile domain is one such ex permission to make digital or hard copies of all or part of this work for ample and the focus of this paper in mobile settings com personal or classroom use is granted without fee provided that copies are putational cost power consumption form factor everyday not made or distributed for profit or commercial advantage and that copies use and self containment are all key requirements this bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific makes hand tracking solutions impractical that require the permission and or a fee sensor to be embedded in the environment leverage currently uist october cambridge massachusetts usa bulky and power inefficient depth cameras or that require the copyright acm hands of the user to be fully covered this paper explores this very challenge and enables dexter images these systems offer some of the most sophis ous hand interactions on the move our system called ticated mechanisms for hand tracking without the need digits is a compact form factor wrist worn device built us for direct user instrumentation however these approaches ing easily reproducible off the shelf hardware which is or clearly lack mobility requiring sensors embedded in the en ders of magnitude lower power than current depth cameras vironment and high computational cost the sensor is fully contained on the body requiring no ex depth cameras on the move the rise of consumer depth ternal sensing infrastructure in the environment and allows cameras has led to interest in interaction on the move re diverse bare hand interactions searchers have explored handheld shoulder or this focus on mobile use does not come at the cost of sens even shoe worn depth cameras all requiring line of sight ing fidelity we present efficient processing techniques for of the user hands none of these systems however sup robustly detecting features of the hand using the wrist worn port full hand pose recovery but instead focus on sensing device and new algorithms which leverage knowledge re touch interaction with planar or more complex physical garding the biomechanical constraints of the hand to recover surfaces or detect simple pinch gestures for interac a fully articulated model of the hand tion handheld systems can restrict freeform hand inter we leverage the recovered hand pose to recognize di actions that require both hands shoulder and shoe mounted verse hand gestures including discrete and continuous systems alleviate this issue but can suffer from occlusions of gestures we demonstrate interactive scenarios including the hands from other parts of the body in addition whilst eyes free spatial interactions on the move in air inter compelling in terms of sensor fidelity there are practical bar actions around the periphery of the mobile device combi riers in making depth cameras mobile in particular power nations of on surface and above the surface input on mobile consumption and form factor phones controlling large displays from a distance using free ultra mobile wearable gesture systems the wearables hand gestures and immersive gaming experiences literature has proposed many lightweight systems for mo our main contributions can be summarized as follows bile gestural interaction instead of supporting high fidelity sensing they use ir proximity sensors to detect coarse mo a wrist worn system that senses the full hand pose tion of fingers sense muscle or tendon without external sensing infrastructure or full instrumen activity to recognize a small set of discrete hand gestures tation of the hand unlike existing data gloves or leverage acoustics to coarsely localize touch on a real time signal processing pipeline that robustly tracks the body given their lightweight form factor a vari points on the user hand ety of on body placements including forearm and wrist two new kinematic models that allows for full reconstruc worn have been demonstrated tion of the hand pose from these sparse points our work builds upon these systems in that we aim to provide a demonstration of compelling interactive scenarios that always available body worn input but extend the interaction digits enables scope from sensing discrete events or coarse motion gestures to much richer continuous input an evaluation of the feasibility of this approach through preliminary qualitative and quantitative studies body worn camera systems one final class of wear able systems uses cameras to add higher fidelity sens related work ing without greatly compromising form factor these sys given their dexterity sensing the full pose of a user tems use either monochrome cameras and diffuse ir illumi hand is a technically challenging and active area of research nation or rgb cameras inferring one approach is to instrument the hand directly by wear the full pose of the hand is clearly challenging from such ing a glove with embedded sensors or markers see for input currently systems have only demonstrated simple an overview originally designed for ar vr such sensors pinch gestures or detecting fingers using marker have since been leveraged for mobile scenarios however or markerless approaches for simple pointing open gloves can be cumbersome and uncomfortable to wear can and closed hand gestures classify a wider set of degrade tactile sensation and limit interaction with capacitive discrete hand postures e g for sign language the form touch sensors which are now standard on mobile phones factor of these systems allows for various body worn place an approach that avoids user instrumentation is to place a ments see these include placement on shoulder or camera in the environment pointed directly at the user hand head and around the neck which have the many systems focus purely on fingertip detection for on sur benefit of capturing both hands for bimanual input but re face see or in air input see literature review in strict the interaction space to a fixed region directly in front other systems have recognized a small set of discrete of the user upper body interactions often cannot be subtle hand poses e g or leveraged shape and motion and are publicly visible a potential barrier for adoption in analysis for approximating freehand interactions public arm fatigue can be an issue for prolonged use the vision community has explored higher fidelity hand a viable option to closer couple hand and sensor are cam tracking methods see for a detailed survey real time eras directly looking across the hand of the user use a pose recovery is becoming feasible using either non watch like camera to count visible fingers for coarse input parametric methods that typically use nearest neighbor lookup presents a prototype of a wrist worn camera that looks into a database of rgb or depth images or at across the hand and images fingertips with markers placed tempt to fit a parametrized model of the hand to observed on each finger inferring hand pose from the fingertip estimate using inverse kinematics ik the system is fairly to more faithfully replicate the pose of the hand more fea large in terms of form factor and the requirement of wearing tures need to be sampled on the hand a complimentary markers on each finger is a clear limitation method uses a ring of modulated ir leds to uniformly illu design considerations as illustrated by the breadth of minate the user hand we demonstrate how to robustly ex the related work this is a rich and challenging design space tract the positions of fingertips from this ir image with with digits we aim to bring some of the high fidelity sens an associated coarse depth estimate by modeling the light ing found in data gloves and environment based vision tech falloff from the ir leds this fingertip sensing approach niques to mobile settings this requires the full recovery of can be coupled with the laser line sensing method to derive a articulated hand poses in real time without the require new inverse kinematics ik based algorithm for computing ment to wear a glove we strive to build a system that is the full joint angle configuration of the hand this method more practical than using today depth camera technologies allows for even more realistic reconstructions of the hand as which as outlined carry serious limitations for mobile use shown in fig resulting in higher dof input we identify an interesting area in this design space and focus these methods work together to help constrain the other on a wrist worn device to recover the full hand pose but wise ill posed problem of recovering the full hand pose do this with only a camera from a image because each method may be useful by we build upon learnings from the wearables literature that itself we keep the description of the two approaches sepa have shown the benefits of wrist worn devices rate this also aids in understanding the underlying concepts for supporting eyes free always available interactions as techniques and algorithms presented in this paper finally an well as overcoming some of the limitations of other sensor inertial measurement unit imu can be used to approximate placements including occlusions of the hands by other body wrist and forearm motions in see fig parts imposing interactions that are overly performance digits is primarily a new type of sensor and an enabling tech centric instead of private or subtle and physically con nology for interaction however before describing the system straining the interaction space due to line of sight require in full we demonstrate its interactive capabilities ments of the sensor interactive scenarios digits system overview we have explored a number of interactive scenarios enabled digits is a small camera based sensor attached to the wrist by our system which we briefly illustrate here each scenario that optically images a large part of the user bare hand the looks at a different configuration of output coupled with dig camera is placed so that the upper part of the palm and fin its the first scenario looks at spatial interaction with a situ gers are imaged as they bend inwards towards the device ated display see fig b for example interacting with two separate infrared ir illumination schemes are used to a tv at home or a large public display here the user inter simplify signal processing the use of ir allows the illumi acts at a distance using a digits device application scenar nation to be invisible to the user and offers a level of robust ios can include gaming or cad where the user can perform ness to ambient visible light both illumination schemes use a variety of continuous or discrete hand gestures to support low cost readily procurable and low power components spatial navigation pointing or selection in fig b from this tracked hand model discrete gestures can be robustly recognized by looking at the joint angle configura tion fig figure illustration of potential digits application scenar ios a b continuous interaction with content on a large figure digits main hardware components attached to a display c gesture recognition performed on reconstructed wrist brace hand model first an ir laser line generator projects a thin ir line across mobility was a strong motivation in our work we envi the user hand which intersects with the fingers as they bend sion digits will expand the physical interaction area of mo inwards this approach can be used to robustly sample a bile devices beyond the display in one application scenario single point on each of the fingers and thumb from these see fig b the user holds and interacts with a tablet or five sparse samples and by exploiting biomechanical con phone using the dominant hand and uses the non dominant straints we derive a forward kinematics fk algorithm to hand to provide input to the application for example se reconstruct a fully articulated hand skeleton this initial ap mantic zooming is initiated with an in air pinch gesture and proximation allows us to support a variety of hand poses the zoom factor is controlled with the remaining digits as shown in fig however each fingertip is essentially re an interesting possibility here is to support on screen inter duced to a single dimension of input limiting its motion to actions and simultaneous freehand interactions for exam curling either towards or away from the sensor see fig ple dividing between fine grained interactions on the touch the hardware is designed to be simple easily reproducible with off the shelf components and low power the setup is powered entirely over usb both laser and the leds powered and driven by strobes from the camera general pur pose input output gpio pins this results in a total power draw of less than for the laser for the ir leds and for remaining camera hardware this compares favorably to the to consumption of the current generation kinect cameras a digits device weighs around with the wireless imu and is lighter than standard watches with a metal wristband the device is attached to the forearm with a wide velcro band around the wrist and the contact area at the inner wrist figure digits application scenarios a b extending in teraction space around a mobile device into c d non is covered with soft padding visual uis allow users to manipulate application parameters signal processing without looking at or touching a physical device gui ele the main processing pipeline is broken down into the fol ments are for illustration only lowing steps background subtraction we reduce the influence of ambi screen and coarser navigation tasks using the non dominant ent ir light by capturing three consecutive frames from the hand and digits our tracker is gloveless and enables the user ir camera under different illumination conditions giving an to interact with the mobile device and operate digits with the effective frame rate of the first frame turns all ac same hand this allows standard touch gestures to be tive illumination off capturing only ambient ir this is sub coupled with above the surface interactions in for ex tracted from the other two frames the first with only the laser ample for quickly changing the z order of a selected ob on and the second with just leds on as shown ambient ir ject by first touching the target and then performing an in air light is greatly reduced in our input images e g from room pinch based zoom lights or sunlight fig finally the display could be removed entirely in an eyes free image rectification rectifies both actively lit images based interaction scenario the input capabilities of digits can on a previous intrinsic camera calibration step allow spatial interactions with invisible uis such as dials sliders or buttons without visual output see fig d finger separation splits the led lit ir image into regions for example we can leverage both proprioceptive knowl that correspond to different unique fingers or thumb edge and spatial memory to allow the user to set the volume laser line sensing triangulates the points where each on a mobile phone by directly reaching out and interacting finger or thumb intersects with the laser line generator with a virtual dial turning their hand to the right of the body forward kinematics these points are passed to a new and performing typing gestures on a virtual number pad to forward kinematics algorithm which reconstructs the full place a call or moving the hand to the left of the body and hand pose based on assumptions regarding the biomechani touching their thumb and individual fingers to activate other cal constraints of the hand phone functions one interesting possibility here is to detect the type of action by the initial shape of the hand for diffuse ir fingertip detection additionally depending on example if the user requires to change the volume they sim the hand pose we can also use the led illuminated image ply configure their hand as if they are holding a virtual dial to robustly extract high quality surface normals and coarse which can then be rotated to set the desired level depth estimation for robust detection of fingertips these scenarios illustrate the utility of the digits as a gen inverse kinematics the points sensed from the laser eral purpose platform for a variety of hand based interac and fingertip locations are passed onto a new ik model tions in the next sections we describe the system imple for higher dof recovery of hand poses mentation in full focusing on how we sense features of the finger separation one of the important initial pipeline hand and from these build different articulated models of the steps associates regions of the image with each of the digits hand with varying levels of fidelity of the hand a technique utilized in seam carving is used sensing hardware to disambiguate the main vertical boundary between pairs of the digits hardware is shown in fig a pointgrey fire fingers in the ir illuminated image a one dimensional sobel fly mv ir camera resolution capturing frames at filter finds vertical edges in the ir image we detect valleys is attached to a wristband worn on the anterior inner between two fingers as concavities in the traced hand side of the wrist an ir laser line generator gated cameo contour at each valley location we trace multiple vertical from global laser operating at with an paths along the edges and use dynamic programming to de gular spread is positioned at a fixed baseline from the camera tect the path with the lowest overall energy by penalizing diffuse ir leds osram chipled sfh again op paths not following the edge see fig this method di erating at are attached around the lens of the camera vides the image into five areas each mapped to a unique digit finally an imu x imu from x io technologies provides laser line sensing the laser line generator projects a absolute tri axis orientation data of the forearm at horizontal line above the palm that intersects with parts of shows the main finger bones and joints in a hand each fin ger is comprised of three bones namely proximal middle and distal phalanges from fingertip to palm these bones are interconnected by a revolute joint called the distal interphalangeal dip joint a revolute proximal inter phalangeal pip joint and a spherical joint called the metacarpophalangeal mcp joint figure background subtraction a active illumination off b ir laser background ir c ir leds background ir d background subtracted ir leds e background subtracted ir laser f finger separation via seam carving each finger these intersections appear as bright regions in the ir camera image and move towards the palm as the finger is bent and in the opposite direction when the finger figure a illustration of main finger bones and joints is straightened see fig with a fixed known baseline be b natural flexing of fingers proposed forward kinematics tween laser and the camera it is possible to triangulate the model reconstructs this behavior exact position of each laser line segment our current im plementation uses a baseline as a reasonable trade off these bones do not move in an entirely independent fash between depth accuracy and ergonomics see fig ion specifically it has been shown that the dip joint angle depends on the pip angle owing to interaction of tendons at in a one off process the camera and laser are calibrated the tached to middle and distal phalanges furthermore as camera intrinsic parameters are retrieved using a checker shown experimentally by during natural flex of the fin board calibration method and are used for image recti ger i e when not explicitly controlling mcp independently fication next the user moves the same target and intersects of other joints mcp joint angles depend on the pip angle it with the laser line see fig the extrinsic pose when bending the middle and distal phalanges we leverage of the target is computed relative to the camera center the interdependency of these bones and joints to map from the user clicks on an intersection point and the associated sparsely sensed locations on each finger to a articu point is recorded the process is repeated until three non lated model of the hand coplanar points are selected to define the laser plane relative to the camera specifically during natural flex of each finger see fig right a linear relationship exists between all three joints of to triangulate the intersections of the finger and laser the the finger such that both mcp and dip can be derived if the background subtracted and rectified image is first binarized pip angle is known kamper experimentally found the for connected component analysis intersections are clearly ratio between pip and dip is and for pip to mcp visible as elongated ellipsoids which are filtered based on respectively using these ratios we can approximate a com size and shape merged connected components when fin mon finger motion when an outstretched finger curls inwards gers are close to each other are separated using the previous until it touches the palm only with a single parameter seam carving output the centroid of each connected com ponent is reprojected using the camera intrinsics from the camera center a ray through the centroid is intersected with the derived laser plane see fig this defines a point for each finger relative to the camera figure left forward kinematics model for a single finger intersecting with laser line right graph mapping between laser distance and pip joint angle calculating joint angles figure a laser plane calibration procedure b repro to determine the articulation of each finger during natural jected ray intersecting with the laser plane flex we experimentally derived the mapping between intersection points and the pip joint angle using a simple a simple kinematic hand model forward kinematic model fig we simulate each of the after retrieving the intersections of laser with fingers we bones with predefined lengths respectively av use a new kinematic method to recover the hand pose fig erage bone lengths can be taken from the literature or measured per user we compute the pip angle at simula robustly detect fingertips which allows us to define a more tion time as p mcp and dip are calculated as complete kinematic model of the hand m ap and d bp respectively where a and b are the fingertip detection joint ratios derived from despite it being unfeasible to place a full depth camera on the during calibration we also derive the pose of the palm wrist due to form factor and power we demonstrate how with respect to the camera using a checkerboard placed on to generate high resolution normal maps and coarse depth the palm which allows us to determine the offset of the laser estimates by modeling the light falloff from the leds this and its direction with respect to the palm we simulate the provides a robust method for identifying fingertips in the forward kinematics model by changing the joint angle p image even when fingers are directly facing the camera at the finger is outstretched and fully bent at for each of the angles of p we measure the intersection between the laser ray and each of the bones and take the minimum distance the sampled data is plotted in fig and used to map from laser distance to p the graph can be fitted using the following cubic function where d is the distance to the intersection in mm p d figure our fingertip detection pipeline a background subtracted imaged fingers b estimated depth encoded in red color channel c normal map computed from depth as fingers have similar anatomy it is reasonable to assume map d response map from template matching darker is that this function is valid for all fingers we therefore provide closer match black rectangles mark fingertip candidates a simple one off online calibration process for each finger e recovered mesh viewed from the physical camera view where we plot the principal axis of motion for each finger f depth distortion visible when viewed from off center view new intersections are normalized along this axis because point note the distinct peaks caused by fingertips we normalize along a line this approach also works for depth approximation our approach adapts work on shape the thumb which moves more diagonally in the sensor im from shading sfs see overview in our scenario age while articulated thumb motion is reasonably tracked in makes sfs more tractable given partially known parame practice results could be further refined by explicitly build ters for led light position light power and radial inten ing a similar model as in fig purely for the thumb our sity falloff as well as the ability to approximate the skin model can be extended to lateral motions of fingers i e al reflectance model as purely lambertian given the typically lowing fingers to move left and right by mapping deviation small angle of incidence between surface and light source from the calibrated principal axis to a rotation which is applied to each finger after articulating finger bend we first estimate distance measurements for each pixel un der the inverse square law the intensity at distance d is i d2 solving for d this gives uspan initial distance es timate for each pixel u as d u i u this distance estimate is then attenuated according to the radial falloff in light intensity for pixels further away from the light central ray the final distance value is computed as p d u i u cos arctan u pp f l with known principal point pp and focal length f l from cam figure top various hand poses supported by our for era calibration this computes a depth map where each pixel ward kinematics model bottom user real hand poses can be reprojected as a point in camera coordinate space finally we compute surface normals for each pixel from ad strengths and limitations jacent pixels in the depth map see fig this approach provides a simple but yet natural approxi mation of hand poses fig and fig show a number of strengths and limitations this technique provides only real world hand poses and how these can be replicated using approximated depth values as pixel intensity depends on digits whilst the combination of laser line sensing and for many factors beyond distance to the light source and light ward kinematics model is powerful in its own right there are falloff in particular we do not model the shape and material limitations in particular it assumes a strong relationship be of the imaged surface nor do we take the surface orienta tween all of the joints of each finger however there are also tion into account while the resulting depth map looks plau common cases where for example the mcp joint moves inde sible when viewed from the cameras perspective distortions pendently of the pip to deal with these wider range of hand and non linearity of the signal become clearly visible when poses we need to sense other parts of the hand and provide viewed off center fig an extended kinematics model through experimentation we however this approach can be powerful in detecting finger have found that illumination from ir leds can be used to tips and computing a relative depth estimate because finger tips are fairly spherical in shape in particular when pointing distance tobs sensed from the laser lobs which we know towards the camera they produce a distinct signal as it can intersects one of the bones the specific bone is unknown be seen clearly in the normal map fig and the mesh is known ahead of time using the laser line to calculate rendering fig each fingertip produces a very distinct the minimum extent of each finger as described previ peak in depth very similar in shape to a gaussian sphere cen ously this allows us to calculate by applying a local tered around the finger most protruding part transform translation by bone length and rotation around to detect and track these peaks many methods are viable the joint angle m to so that r m t we have obtained robust results with just simple template and r p t matching based on matching scores as squared distances our aim now is to find the optimal combination of m and between a sliding synthetic fingertip template and the live p to best describe the observed data the location of the normal map fig f shows the final result of our tech fingertip sensed using the leds and point measured with nique reliably detecting fingertip locations in particular our the laser for our fingertip location we define the following technique works for fingertips pointing towards the camera energy function and multiple fingers touching each other situations in which simple techniques such as peak and valley algorithms or eled proj yobs connected component analysis alone would have difficulties a new kinematics model this function generates estimated positions for given vari fingertip estimates can be combined with the laser line in ations of m and p and projects these onto the image plane tersections to recover a more accurate hand model using in i using the intrinsic camera calibration parameters it has a verse kinematics ik ik typically derives joint angles from low energy for points that are close to the observed point on the position of the end effector the fingertip we do not the image plane yobs have an accurate measurement for fingertips and the our second energy function first calculates intersections be point sampled with the laser is not directly associated with tween the laser line and each bone in the finger based on the end effector however the two sensing modalities can be variations of m and p and takes the minimum combined to derive a new ik model enabling separate artic ulation of the mcp joint and the pip dip joints t min isect ld isect ld it then minimizes the distance between the observed laser point lobs and this estimated intersection elas tld lo lobs our full energy function is specified as arg min e eled led elas las figure ik model that articulates both the mcp joint and m p the pip dip joints based on an sensor fusion approach this allows us to weight the contribution of either the led fig illustrates a simplified parametrization of our ik or laser based sensing accordingly using a scalar  in model note for illustrative purposes we have reduced the our current implementation we evaluate this energy function problem to and combined pip and dip joints which across m and p in a brute force manner cannot be moved independently unless the finger is pressed and select the value with the minimum energy against a surface in this parametrization the palm is again this new kinematic model give us even higher dof input assumed to be resting directly on the x axis the position of sensing as shown in fig a wider range of hand poses can the mcp joint is given by the position of the pip joint is be more accurately predicted from the raw sensor data this and the end effector is at whilst the location of the includes a wide range of poses that are difficult to predict end effector is not known we can observe the projection of using our simpler kinematics model the combination of the the point yobs on the image plane i as this equates to the two sensing modalities both laser line and light falloff centroid of the detected fingertip in the ir image given the allow us to solve the otherwise ill posed ik problem calibration matrix of the camera we can project a ray from the camera center c through the image plane we know that exists somewhere along this ray the length of each of the bones of the finger are again assumed to be known either by measurement or from assuming predefined values we are solving for the mcp and pip joint angles given as m and p respectively we can figure truer hand pose recovery with inverse kinemat also parameterize the laser as an offset from the origin lo ics note pip and mcp joint angles recovered correctly and direction ld we also have an observed point with initial evaluation roni corrected  n reveals linear improve we performed a preliminary evaluation of digits to get a ment between blocks and p but no significant sense of the accuracy and repeatability of the system at re difference between blocks and p constructing different hand poses whilst not a detailed user repeatability as a measure of repeatability we compute or system evaluation this study was meant to provide an ini intraclass correlation coefficients icc the standard test for tial feasibility test for this approach to hand tracking repeatability for this kind of time series data for gesture feasibility experiment each of the six poses we computed icc scores per finger and to provide a first step towards quantifying accuracy and re per joint between the three blocks of repetition static phase peatability we asked participants to mimic hand postures only we repeatedly selected two blocks randomly then rendered on screen in an interactive application we used for each block a trial was selected randomly and a icc score ground truth data gathered using a vicon motion tracking was computed between them this procedure was repeated system as proposed in six static hand poses as shown times to ensure consistency and icc scores were averaged in fig were generated in the experiment we tested the together for brevity we only report individual scores for the full ik model described in the previous section pip angles but overall the icc scores were in the range of to moderate to strong correlation correlation be tween repetitions was found to be moderate for little finger icc and good for thumb icc to strong for index finger icc ring finger icc and middle finger icc overall these results figure hand postures in experiment a open palm suggest that digits produces moderate to strongly correlated b counting two c pointing d grasping small object e joint angle measurements when the same pose is repeated grasping large object f pinching multiple times and by multiple participants accuracy to determine accuracy data was averaged across participants we recruited participants aged to blocks and participants keeping joints and gestures separate with a mean age of participants were right handed averaging across fingers would not be very meaningful as none of the participants had any physical disabilities or lim finger motion is not entirely independent see discussion on ited range of motion related literature has shown that an biomechanical constraints earlier again for brevity we only thropometric differences in the upper limbs can make com report results for pip bend angles but results for pip bend and parisons across genders incorrect therefore for this first mcp bend and tilt angles are comparable experiment we only tested digits with male participants procedure and task the system was mounted on each user wrist as in fig the system was calibrated for each user hand including bone lengths and finger motion range and trajectory for left handed users the camera and laser placement was adjusted and a full camera laser calibra tion performed during the study phase participants were presented with the six reference hand configurations on screen fig a second live hand model driven by the participant input was rendered on top of the reference hand we ignored wrist and forearm reconstruction in this study and hence did not use the onboard imu left and right handed reference postures were selected based on the figure mean error in pip joint angle in degrees per participant handedness finger and for all six gestures error bars show std deviation participants started each trial in a neutral pose fully closed fig summarizes results for the five fingers and all six fist the experimenter triggered each task subsequently a gestures mean errors throughout the data remain relatively reference pose was rendered on screen and users were asked small for all fingers and gestures all the best ac to align their instrumented hands as closely as possible la curacy is usually found with the index finger min pose beled as the acquisition phase users self reported once d max pose a this could be attributed to the impor they were satisfied with their pose by pressing a button tance of the index finger for many everyday activities e g with their uninstrumented hand at which point they were pointing pinching scratching for most poses the error is asked to hold the pose for seconds labeled as the static largest for the thumb this maybe explained by the fact that phase each user completed blocks each consisting of our ik model treats the thumb as a regular finger while in trials which included all the hand postures in random or reality the thumb shows a more complex motion cf der users had minutes training time with the application the mean error for the pinky is also comparatively high min results and discussion the mean acquisition time was pose e max pose d this may be explained by ms the first block averaged ms second ms the camera placement in our setup to achieve a compact and third ms a repeated measures anova reveals that form factor the camera was moved very close to the palm so there is a significant main effect for acquisition time across that the pinky may not be visible in certain circumstances blocks p post hoc analysis bonfer although preliminary these overall results are promising the achieved accuracy of the tracking and specifically the our image processing techniques can cope better with lateral joint angle error rates are comparable to those reported in motion of the hand relative to the camera studies on data gloves furthermore the average ac our proof of concept implementation already is wearable curacy between and is better than that defined in the and not overly bulky but requires user instrumentation fur literature for manual goniometry between and which thermore the device is still tethered to a pc or laptop where is considered in clinical practice to be the gold standard of computations are being performed it is however conceivable joint angle measurement to further miniaturize the device until it is either a standalone discussion watch like device or fully integrated into a regular watch fu digits is a general purpose wearable hand tracker it ture depth camera technologies such as time of flight might avoids direct instrumentation of the user hand but instead enable such compact form factor devices for the time be is wrist worn we have shown how the full pose of the ing we argue that our approach is the best trade off between user hand can be inferred without requiring high fidelity practicality and availability concerns but also between com and currently impractical hardware such as a depth camera putational complexity power consumption and form factor to be worn on the body the preliminary evaluation of our always available input it is worthwhile noticing that with prototype system demonstrates tracking close to existing data digits input is not restricted to a fixed space around the user gloves but without direct instrumentation of the hand but instead moves with the user hand for example ges a tale of two kinematic models in this paper we have intro tures can be conducted in front of the body much like other duced two complementary ways to recover pose from a body worn systems however gestural input small number of samples on the user hand while the dif may also happen in a more subtle effortless way for exam ferent models based on forward and inverse kinematics re ple performing hand gestures whilst the hand is lowered by spectively provide different levels of interactive expressive the side of the body or resting on a physical surface avoiding ness they should not be seen as one fully replacing the other arm fatigue during long periods of use examples of this are both have their own strengths and weaknesses and utility in shown in the accompanying video figure different contexts our first model has the advantage of sim emergent interactions there are other emergent interactive plicity it allows reconstruction of rich hand poses from features of digits which can be fruitful to explore in future a single measurement on each finger which already enables work for example digits may be used to track the spatial a number of compelling spatial continuous gestural interac positions of fingers from the other non instrumented hand tions fig and fig and can facilitate gesture recognition when both hands interact using the palm of the reference in mobile applications the second model adds higher dof hand as a track pad or using the segments of fingers on the sensing and more independent movement of each finger joint reference hand to control sliders in a gui and therefore provides more fidelity in recovering the user whilst digits has been designed to be a general purpose in hand pose this model also allows a truer reconstruction of teraction platform we have demonstrated both in this pa the hand as shown in fig this can be useful for example per and accompanying video interactive scenarios using this in a physics enabled application where you wish to model technology we believe digits is particularly useful for mo grasping of an arbitrary virtual object or a medical appli bile scenarios where the sensor can coexist with existing per cation where accurate joint angle measurements are needed sonal devices such as mobile phones and tablets the combi we enable this by adding only simple additional hardware nation of touch and input in free space around the device although there is more algorithmic complexity is particularly interesting also of interest are eyes free in terfaces which allow for interactions without needing to re move mobile devices from our pockets one final application area for digits is in gaming where technologies such as the xbox kinect or nintendo wii do not currently support the level of fidelity of hand sensing again such a device could be complimentary to these existing sensing modalities for example combining the kinect full body tracker with high figure failure cases for our forward kinematics fk fidelity freehand interactions of digits model notice mismatch in pip angle left and mcp angle conclusion right using our inverse kinematics ik model we can mitigate such issues and offer a truer reconstruction we presented digits a wrist worn system for sensing the full pose of the user hand the system targets mobile limitations with our current implementation being a vision settings and is specifically designed to be low power and based technique occlusions resulting from crossed fingers easily reproducible using only off the shelf hardware that is overly bent thumb and handheld objects are problematic for both smaller and more power efficient than current consumer hand pose reconstruction however these are special cases depth cameras we have shown two complimentary methods and we anticipate they can be avoided by careful gesture de for robustly tracking features of the hand and we demon sign furthermore more advanced techniques for finger sep strated the evolution of a kinematics model for reconstruct aration and identification could be used to mitigate these is ing the full articulated hand from these sparse samples we sues in the current digits prototype we do not model wrist demonstrated the utility of digits for a variety of application bend and rotations about the forearm explicitly in partic scenarios including spatial interaction on mobile phones ular a fully flat or over arching hand is problematic while eyes free interaction on the move and gaming semaine has created a large audiovisual database as a part of an iterative approach to building sensitive artificial listener sal agents that can engage a person in a sustained emotionally colored conversation data used to build the agents came from interactions between users and an operator simulating a sal agent in different configurations solid sal designed so that operators displayed an appropriate nonverbal behavior and semi automatic sal designed so that users experience approximated interacting with a machine we then recorded user interactions with the developed system automatic sal comparing the most communicatively competent version to versions with reduced nonverbal skills high quality recording was provided by five high resolution high framerate cameras and four microphones recorded synchronously recordings total participants for a total of conversations with individual sal characters lasting approximately minutes each solid sal recordings are transcribed and extensively annotated raters per clip traced five affective dimensions and associated categories other scenarios are labeled on the same pattern but less fully additional information includes facs annotation on selected extracts identification of laughs nods and shakes and measures of user engagement with the automatic system the material is available through a web accessible database index terms emotional corpora affective annotation affective computing social signal processing  introduction o ne of the natural long term goals in affective comput ing is to develop systems that can engage a human being in a face to face conversation which is fluent humans show a range of responses to the system efforts from lively interaction to irritated disengagement innova tive techniques are used to label the material much of it in sustained and emotionally colored this paper considerable depth describes one of the first databases to be developed with the database provides resources for work on diverse that goal in mind as part of a project called sustained problems associated with fluent interaction describing emotionally colored machine human interaction using relevant processes particularly nonverbal processes as nonverbal expression semaine the database includes cognitive scientists do training systems to recognize high quality multimodal recordings showing a range of emotion related states as they appear in conversation related interactions at one end of the range are recordings particularly states that emerge in response to a machine showing pairs of people engaged in emotionally colored attempting to converse and finding ways to label these conversations at the other end are recordings of indivi states critically it offers support for work that aims not just duals interacting with an automatic system that simulates to describe or to build components but to build systems one of the parties in the human human recordings the that actually have face to face emotional interactions with human beings because they deal with a scenario developed specifically to make that possible g mckeown and r cowie are with the school of psychology queen university belfast david keir building belfast northern ireland united kingdom e mail g mckeown r cowie qub ac uk the motivation for the database m valstar is with the department of computing imperial college it has only gradually become apparent that affective london queen gate south kensington campus london computing might need data on something as specific as united kingdom e mail michel valstar imperial ac uk fluent sustained emotionally colored conversation but m pantic is with the department of computing imperial college london queen gate south kensington campus london united there are growing indications that building a system to kingdom and the university of twente the netherlands function in a particular context requires data from contexts e mail m pantic imperial ac uk that are quite similar emotion is inherently m schro der is with dfki gmbh language technology lab campus interactive and so the states that arise in a given situation stuhlsatzenhausweg d saarbru cken germany e mail marc schroeder dfki de and the signs associated with them are likely to be a manuscript received nov revised may accepted june function of the interactions that take place there published online july databases dedicated to emotionally colored face to face recommended for acceptance by b schuller e douglas cowie and a batliner conversations are in short supply many databases serve for information on obtaining reprints of this article please send e mail to related functions but very few serve that particular one taffc computer org and reference ieeecs log number taffcsi the ami meeting database shows realistic sustained inter digital object identifier no t affc actions but they are not rich in emotion various databases  ieee published by the ieee computer society ieee transactions on affective computing vol no january march of acted material show how people express underpin face to face fluent emotional interaction by emotion deliberately but not how it arises spontaneously in building systems that try to match them the data the course of an activity it does arise spontaneously in described here are set in that scenario and are designed sources that involve watching a film or undertaking a to let research exploit it challenge but the activity is not conversation the choice of scenario also means that the data have various databases derived from tv do show some features that are of no great research interest they are emotion arising from conversations but because they rarely a function of the expedients that give the system its minimal show both parties there are important dimensions that they linguistic competence hence research teams who use the do not capture considering face to face interaction there is a data need to ensure that they focus on what is of value and similar issue with databases that are unimodal such as the not on side issues but that is not a unique problem annotated part of the aibo database which is purely audio and others which are wholly or mainly visual the sal scenario all of these resources cited do meet other needs the point is simply that they are not ideal for work on face to face the sensitive artificial listener scenario had been emotionally colored conversations extensively trialed and refined before semaine adopted a few sources do contain multimodal recordings of both it it was originally suggested by tv chat shows not parties in a fluent emotionally charged conversation such always but often hosts use a simple strategy invite guests as the green persuasive recordings or to talk about topics that are emotionally significant for them arguably the spontaneous dialog in iemocap may be and encourage or provoke them to express the emotion considered in this category although the interactions strongly by inserting suitably chosen stock phrases at key points that model was developed over a substantial period involve acted scenarios two actors simulate scenes where into the scenario considered here e g one tells the other she is getting married they the interactions involve two parties a user who is raise another kind of issue they suit some research always human and an operator either a machine or a strategies notably describing human behavior patterns person simulating a machine the operator follows some verbally as psychology has traditionally done or building times approximately a script composed of phrases with components of an affective system e g to recognize user two key qualities one is low sensitivity to preceding verbal states however there are important strategies that they context that is it is usually possible to decide whether a do not support specifically they are not suited to a given phrase can be used as the next move in a strategy that tries to progress by building agents that conversation without knowing the words that the user has interact as nearly as possible in the way that the recordings just said though it may depend on registering the way they show identifying the problems that arise and using those were said the other is conduciveness that is the user is to drive progress likely to respond to the phrase by continuing the conversa iterative strategies are commonplace elsewhere as a tion rather than closing it down given a repertoire of complement to verbal description and building components phrases like that an operator can conduct a conversation fluent interaction is a topic where the case for that approach with quite minimal understanding of speech content seems particularly clear without building systems that early experiments with the script idea showed that interact or try to it is all too easy to overlook processes or conversation tended to break down unless users felt that the relationships that are actually crucial for the success of operator had a coherent personality and agenda given that interaction clearly the ways components interact but also the operator communicative skills center on detecting and rules of timing or responsiveness or coherence that are expressing emotion the natural way to define personalities critical but not obvious from human data because humans and agendas is in terms of emotions hence we defined almost never break them it is difficult to progress with that subscripts for four personalities with appropriately strategy using material like the green persuasive recordings chosen names spike is constitutionally angry he responds or because the interactions depend on competences empathically when the user expresses anger and critically that are far beyond an artificial agent at present specifically when he she expresses any other emotion which gives the they depend on accurate recognition of fluent speech and impression that he is trying to make the user angry subtle interpretation of informal language all in real time in similarly poppy is happy and tries to make the user contrast to the problems with these verbal competences the happy obadiah is gloomy and tries to make the user technology does exist to build agents that execute a gloomy and prudence is sensible and tries to make the substantial set of nonverbal skills in real time as semaine user sensible confirmed that opens the prospect of an iterative these techniques were evaluated using a system that we strategy provided that a scenario can be found where the have called powerpoint sal the part of the operator was combination of rich nonverbal competences and verbal played by a human who selected appropriate phrases from competences simple enough to be implemented is sufficient the prepared script and read them in a tone of voice that to sustain an interaction with a person suited the character and the context its name reflects the a scenario that seems to meet that requirement was fact that the sal scripts were transcribed onto powerpoint identified some time ago it is the sensitive artificial slides each one presenting phrases suited to a particular listener or sal for short it is introduced in the next context accompanied by buttons which allowed the section the point to be made here is that it offers a way to operator to change slides for instance if the operator was develop understanding of the nonverbal competences that simulating the poppy character and the user mood was mckeown et al the semaine database annotated multimodal records of emotionally colored conversations positive the operator would navigate to a slide showing the resulting data provide a basis for assessing the phrases that approved and encouraged happiness he she semaine traces in terms of independence reliability would then choose and speak one of them if the user and not least functionality within a working system became angry clicking a button would bring up a new slide displaying phrases that poppy might use to an angry interlocutor if the user then asked to speak to spike scenarios for semaine recordings another click would bring up a slide showing phrases that semaine recordings contrast with earlier sal material at spike might use to an angry interlocutor and so on several levels recording quality was much higher see recordings made with powerpoint sal in english section where the operator was a human it was much greek and hebrew have been used as data in their own easier for the user to regard him her as a disembodied right what is relevant here is that the work confirmed agent because the two were always in different rooms that users could have quite intense sustained interactions communicating via screens cameras loudspeakers and with an operator whose conversation consisted of phrases microphones most important the scenario was varied from a sal type script it also allowed the scripts to be systematically three basic scenarios were used solid sal revised in the light of difficulties that process generated the where human operators play the roles of the sal scripts used in the program of data collection reported here characters semi automatic sal where a human operator selects phrases from a predefined list but unlike power annotation point sal the system speaks them and automatic sal where an automated system chooses sentences and non annotating emotionally colored conversations is a chal verbal signals these generate a range of interaction types lenge in its own right once again the techniques described solid sal provides fuller operator user interaction than here are part of an iterative process powerpoint sal and three variants of semi automatic sal labeling with everyday emotion words faces multiple provide progressively less as a result the recordings show problems the states that occur in naturalistic data rarely fit user responses to different levels of system sophistication everyday words precisely it is difficult to capture the rise and fall of emotion and interrater agreement tends to be solid sal low labeling with dimensions has obvious attractions a key objective of the solid sal scenario was to record and it forms the core of the scheme used here behaviors mainly nonverbal that a human operator powerpoint sal data were annotated using the feel shows in fluent face to face conversation including their trace system it allows raters to annotate material in relationships to user behavior notably backchanneling terms of two long established emotion dimensions valence eye contact various synchronies and so on that kind of how positive or negative the person appears to feel and engagement does not occur if the operator is searching a activation or arousal how dynamic or lethargic the person script or even trying to recover phrases from memory appears to feel a rater watches and or listens to a hence the operator in solid sal was asked to act in the recording of a target individual and uses a cursor in an character of a sal agent rather than being constrained to adjacent window to indicate how positive or negative and using the exact phrases in a sal script acting in active or passive the individual appears to be at any given character involved adopting the relevant emotional stance time the result is a pair of traces which show how angry for spike gloomy for obadiah etc and using perceived valence and activation rise and fall as the short preferably stock utterances with the properties recording progresses note that for conversation perceived described earlier low sensitivity to preceding verbal emotion is what the system needs to know about it should respond as a person would even if the person would be context and conduciveness the appendix which can be wrong with naturalistic material the reliability of that found in the computer society digital library at http approach compares well with verbal ratings doi ieeecomputersociety org t affc the two dimensional representation runs throughout gives sample transcripts that convey the flavor powerpoint sal each character is associated with a users were encouraged to interact with the characters as region of the space obadiah spike and poppy with spontaneously as possible there was a single explicit different quadrants prudence with the center the same constraint users were told that the characters could not representation is used to organize scripts the utterances answer questions if they did ask questions the operator on any given slide are oriented toward a user in one of the reminded them that the sal characters could not answer same four regions hence a system that can match raters questions users talked to the characters in an order of their feeltrace annotation will be able to match operators own choice and the operator brought the recording session choice of the slide from which the next utterance should to a close when they had interacted with all four be selected the underlying principle is that annotation the result was not intended to mimic machine human should provide the information that a working system interaction but it still had important features in common needs to make its decisions with it the operator was visible to the participant through semaine annotations reflect the same principle the a teleprompter screen and audible through a set of trace technique was retained but because there are speakers the indirectness makes it easier to regard the important distinctions that the dimensions of valence and operator as a disembodied agent than it was in powerpoint activation fail to capture semaine considered a wider set sal probably more important the operator did not behave of traces each using a separate one dimensional scale like a human he she followed a simple conversational ieee transactions on affective computing vol no january march agenda in violation of norms that usually govern human human interaction it is difficult to judge from an abstract description what level of interaction that kind of a scenario might produce the best indicator comes from the labeling process described in section which gave raters several ways of identifying anomalous interactions together they were used in just over percent of ratings indicating that very little of the user behavior was either contrived or disengaged twenty four recording sessions used the solid sal scenario recordings were made of both the user and the operator and there were usually four character interactions in each recording session providing a total of character interactions and video clips semi automatic sal semi automatic sal was similar to powerpoint sal in that a human operator chose phrases from a predefined script these were made available to her him through a graphical user interface based on the powerpoint sal model navigation buttons allowed him her to bring up a page of utterances related to the current character and the user current emotional state when he she clicked on a phrase it fig the four sal character avatars clockwise from top left spike was then played using a prerecorded audio file spoken by poppy prudence and obadiah an actor whose voice had been judged appropriate for the character as before the user heard through loudspeakers therefore only the user videos are included database there and looked at a teleprompter its screen showed a were four character sessions for each recording in the semi simplified face designed to keep users looking in the automatic sal experiments these add a further videos general direction of the camera behind it in order to hold to the database see table for an overview attention the spectrum of the speech was placed just below the mouth of the face the fact that it changed in time automatic sal with the speech helped to create the impression that the in the fully automatic sal recordings the utterances and speech was associated with it nonverbal actions executed by the sal character were the semi automatic sal scenario included three var decided entirely automatically by the current version of the iants which gave the operator progressively less feedback semaine project system the system is described in detail from the user in the baseline condition experiment the elsewhere but a brief overview is given here for operator both saw and heard the user and could therefore completeness use information from both the user words and his her the user sat in front of a teleprompter as in semi nonverbal signals to choose an appropriate utterance in the automatic sal the system sensors were a grayscale remaining variants the operator had to choose utterances camera and microphones see section for details from on the basis of video with an audio either switched off the video input the system detected when a person face is experiment or with an audio filtered to remove verbal present significant gestures head nods and shakes and facial actions smiles eyebrow raising and lowering mouth information experiment the filter cut out frequencies opening from the audio it detected the presence or between and hz which leaves prosody largely absence of user speech emotion related prosodic features intact but only occasional words can be made out the and words that could be recognized with high confidence degradation made it harder for the operator to avoid these fed into different channels some governing actions inappropriate choices of the kind that the system used in e g initiating an utterance when the user stopped speak the automatic sal scenario would necessarily make ing or nodding in response to the user nod and some because it does not use linguistic information and using the information to infer the user emotions using the resulted in recordings where users showed various signs dimensional descriptors described in section poten of communication breakdown in the first experiment semi automatic sal record tially relevant utterances were chosen on the basis of ings provided character sessions with a procedure conversational norms e g avoiding repetition key words directly comparable to solid sal experiments and if they were available and inferred emotion used degraded versions of semi automatic sal in which the system outputs were audiovisual visual output two of the four character sessions were with the full semi consisted of avatars designed to represent the sal characters automatic sal system while the other two were degraded see fig with movements and expressions controlled by a further sessions took place in experiment and the analyses described above audio output consisted of in experiment with differing degrees of degradation of phrases from the relevant script spoken by a synthetic unit information to the operator the operator videos consist selection voice with a different voice for each character only of the operator interacting with the interface and show behavior depended critically on parameters governing little of interest regarding the conversational interaction weightings of different information sources and rules mckeown et al the semaine database annotated multimodal records of emotionally colored conversations fig images of the recording setup for both the user left and the operator right rooms response magnitudes and latencies and so on these were written consent for use of the recordings typical session adjusted during testing and account for the main differences duration for solid sal and semi automatic sal was about between the versions used in data collection see below minutes with an approximate interaction time of participants in the experiments interacted with two minutes per character though there were considerable versions of the system one with the best set of nonverbal individual variations participants were told to ask for a skills available and one with a degraded set hence they different character when they got bored annoyed or felt interacted with each of the four characters twice sessions they had nothing more to say to the character the operator were limited to approximately minutes or if the participant could also suggest a change of character if an interaction was did not engage with the system they were ended after a unusually long or had reached a natural conclusion the minimum of minutes there were three iterations of this automatic sal session duration was about hour with procedure using five versions of the system two degraded eight character interactions of approximately minutes versions that removed affective cues and three iterations of each the participants interacted with two versions of the the fully operational version of the sal system based on system with an intervening minute period in which variations of semaine system an initial experiment they completed psychometric measures examined the effect of the system perceptual abilities the interaction procedure was the same throughout the comparing a full version of the system based with a degraded experiments participants entered the recording studio version which ignored the user actual emotional state and where they sat in the user room and put on their head chose its responses at random fifteen participants were microphone the operator took her his place in a separate tested using this configuration adding character sessions recording room and recording starts as in fig details of to the database a second experiment used two different how face to face conversations were maintained while system versions a new full version with some bug fixes and recordings were made are given in the following section a new degraded system that removed most of the system the operator agent recited a brief introduction script and affective output no backchanneling or facial emotional the interaction began information and random utterance selection and flat affect after each session there was a debriefing session in the agent voices this examined the utility of the emotional allowing the user to ask more about the system information in the characters this added character sessions to the database the third experiment used the same synchronized multisensor recording setup degraded system as experiment and a different full version the database was created with two distinct types of use in with an improved dialogue management system and further mind the first is the analysis of this type of interaction by bug fixes this added a further character sessions to the cognitive scientists this means that the recordings should database additionally five pilot sessions recorded between be suitable for use by human raters second the data are experiments and added a further videos to the intended to be used for the creation of machines that can database a screen grab of the video output of the agent interact with humans by learning how to recognize social computer added agent videos to the database in total the signals these considerations guided the decisions on the automatic sal scenario provides videos to the database choice of sensors and how the sensors are placed examples of automatic sal character interactions are sensors video was recorded at frames per second available online and at a spatial resolution of pixels using avt stingray cameras both user and operator were recorded participants and procedure from the front by both a grayscale camera and a color camera in addition the user was recorded by a grayscale participants and procedure camera positioned on one side of the user to capture a the data set features participants the youngest profile view an example of the output of all five cameras is participant was the oldest and the average age is years old std thirty eight percent are male shown in fig the reason for using both a color and a participants come from eight different countries most were grayscale camera is directly related to the two target from a caucasian background participants were under audiences a color camera sacrifices spatial resolution for graduate and postgraduate students the overwhelming color machine vision methods usually prefer the sharper majority took part in only one scenario before taking part grayscale image over a blurrier color image for humans participants were briefed about the project and provided however it is more informative to use the color image ieee transactions on affective computing vol no january march fig frames grabbed at a single moment in time from all five video streams the operator left has humanid and the user right has humanid shown is the frame of the recording to record user and operator speech there were two h codec and stored in an avi container the video was microphones per person one placed on a table in front of compressed to kbit for the grayscale video and to the user operator and the second worn on the head by the kbit for the color video the recorded audio was user operator the wearable microphones were akg hc stored without compression because the total size of the l condenser microphones while the room microphones audio signal was much smaller were akg s microphones this results in four audio summary of the semaine recordings channels the wearable microphone was the main source for capturing the speech and other vocalizations made by tables and summarize the recordings the user operator while the room microphones were used to model the background noise audio was recorded at annotation and associated information khz and bits per sample trace annotation of participant states environment the user and the operator were located in separate rooms they heard each other through speakers building on experience with powerpoint sal trace style which played the audio recorded by the wearable micro continuous ratings were used to record raters impressions phone of their conversational partner they saw each other of user states primarily emotion related that appeared through teleprompters each teleprompter contained two potentially relevant to controlling an automatic system the cameras recording a person frontal view placed behind specific traces were chosen in consultation with the the semireflecting mirror that allowed the user and the semaine members involved in building automatic sal operator to have the sense of looking each other in the eye the main tracing system applied to solid sal and a pilot test used cameras placed on top of a screen which semiautomatic sal recordings involved two stages five showed the other party face but that did not give an core traces described in section below were provided impression of eye contact and greatly reduced the sense of by every rater for every clip after making those core traces a direct communication professional lighting was used to raters were offered a menu of optional descriptors listed in ensure an even illumination of the faces images of the two rooms can be seen in fig synchronization to do a multisensory fusion analysis table of the recordings it is essential that all sensor data are semi automatic sal recordings recorded with the maximum synchronization possible a system developed by lichtenauer et al was used to achieve that it uses the trigger of a single camera to accurately control when all cameras capture a frame this ensures all cameras record every frame at almost exactly the same time the same trigger was presented to the audio board and recorded as an audio signal together with the four microphone signals this allowed synchronized audio and video sensor data with a maximum time difference between data samples of sec data compression the amount of raw data generated by time is measured in minutes the visual sensors is large character interactions lasting table minutes on average recorded at frames second at automatic sal recordings a temporal resolution of pixels with bits per pixel for five cameras would result in terabyte this is impractical to deal with it would be too costly to store and it would take too long to download over the internet therefore the data were compressed using the lossy table solid sal recordings time is measured in minutes mckeown et al the semaine database annotated multimodal records of emotionally colored conversations section from it each rater independently chose four at ease not at ease that he she felt were definitely exemplified in the clip thoughtful not thoughtful more than four could be chosen if there seemed to be strong concentrating not concentrating instances of more than four categories but that rarely happened the rater then made a new trace for each of his interaction process analysis the descriptors offered here her choices indicating how strongly the user exhibited the are a subset of the system of categories used in interaction state in question from moment to moment hence each process analysis ipa categories are used in dialogue rater provided nine traces in all five core and four management and so ability to recognize instances would be optional for each clip practically useful the labels offered were five pairs core dimensions shows solidarity shows antagonism the five core dimensions were valence activation power shows tension releases tension anticipation expectation and intensity the first four reflect makes suggestion asks for suggestion an influential recent study which argues that they gives opinion asks for opinion account for most of the distinctions between everyday gives information asks for information emotion categories the first two have already been introduced the power dimension subsumes two related validity the final set of labels was intended to highlight concepts power and control these are not the same cases where the user was not communicating his or her conceptually power is mainly about internal resources feelings in a straightforward way among other things that control is about the relationship between those resources affects the way the material should be used carefully or not and external factors in practice raters find it natural to make at all in a training context the labels offered were a composite judgment dealing with the balance between the two anticipation expectation also subsumes various con cepts that can be separated expecting anticipating being breakdown of engagement this seeks to identify taken unawares again people find it intuitively meaningful periods where one or more participants are not to make a composite judgment related to control in the engaging with the interaction for example they are domain of information the last dimension overall intensity thinking of other things looking elsewhere ignoring is about how far the person is from a state of pure cool what the other party says rejecting the fiction that rationality whatever the direction logically one might they are speaking to or as sal characters rather hope that it could be derived from the others but that is than to or as the actual people involved something to be tested rather than assumed this trace anomalous simulation this label seeks to identify serves a function that is handled differently in other periods where there is a level of acting that suggests databases periods when the person is judged to be the material is likely to be structurally unlike unemotional are marked by low values in the intensity trace anything that would happen in a social encounter the main hallmark is that the expressive elements optional descriptors do not go together in a fluent or coherent way they the optional traces dealt with categories from everyday are protracted or separated or incongruous language or psychological theory and were identified by marked sociable concealment this is concerned with semaine partners as potentially relevant to system periods when it seems that a person is feeling a decisions they were of four main types definite emotion but is making an effort not to basic emotions seven labels of this type were offered fear show it in contrast to the two categories above anger happiness sadness disgust contempt and amuse this is something that occurs in everyday interac ment it is important to know whether they can be clearly tion it is an aspect of what ekman et al call identified in this kind of material or derived from other display rules descriptors because they are integral to existing techniques marked sociable simulation this is concerned with for e g generating facial expressions most of the items from periods when it seems that a person is trying to the best known list of basic emotions ekman were convey a particular emotional or emotion related included as options surprise was excluded because tracing state without really feeling it again this is some it would almost inevitably duplicate information that was thing that occurs in everyday interaction people already in the expectation anticipation trace at the cost of simulate interest or friendliness or even anger that information about another category conversely amusement they do not feel not necessarily to deceive but to is clearly an important category in this kind of conversation facilitate interaction this is the most convenient place to include it and some authors do consider it a basic emotion e g traces of engagement epistemic states these states were highlighted by baron the scheme described so far was applied to solid sal and cohen et al and have been viewed within the machine semiautomatic sal recordings time prevented applying it perception community as a significant resource for describ ing everyday emotion they are relatively self explana to automatic sal recordings however a related procedure tory the options of this type were provided information that is relevant both to system evaluation and to wider research questions as automatic certain not certain sal interactions took place a rater watching a live video agreeing not agreeing feed of the interaction traced the user apparent engage interested not interested ment in the interaction ieee transactions on affective computing vol no january march semaine annotations deemed most appropriate to nods and shakes valence arousal agreeing disagreeing at ease not at ease solidarity antagonism understanding the second used annotations derived from mcclave these were inclusivity intensification uncertainty direct quotes expression of mental images of characters deixis and referential use of space lists or alternatives lexical repairs backchanneling requests the results of preliminary analysis and greater detail regarding the annotations can be found in facs annotation facs is a coding scheme developed to objectively describe facial expressions in terms of visible muscle contractions relaxations to be able to test existing and or new automatic facs coding systems eight character interac tions received a sparse facs coding instances were fig instances of user and operator laughter for each character in labeled for the presence of action units specified by frame solid sal recordings number and whether they occur in combination with other action units or in isolation three certified facs coders at amount of annotation qub annotated selected frames in the eight interactions the amount of annotation provided reflects the time obtaining facial muscle action action unit codings in available solid sal was completed first and has the largest frames which was deemed to be sufficient to perform body of annotation followed by semi automatic sal both preliminary tests on this database action unit annotations are annotated with the five core dimensions and four are available with the database optional categories automatic sal has the least annota tion with traces of engagement only the number of traces evaluation and analysis of the data are as follows solid sal for user clips four sessions have been quality of interaction annotated by eight raters by and the remainder by one of the key evaluation issues is the quality of the at least three for operator clips three have been annotated interaction shown whether it is natural representative of by four raters the rest by one rater foreseeable types of human machine interaction or simply semi automatic sal eleven user sessions have been contrived semaine incorporated various ways to answer annotated by two raters those questions automatic sal all sessions have been annotated by a impressionistic judgments cannot be ignored the single rater verbal content of the exchanges gives some indication annotation is being extended gradually the transcripts in the appendix available in the online supplemental material illustrate what happened in the transcripts scenarios involving the most and least human like opera of the solid sal sessions were fully transcribed tors solid sal and automatic sal in solid sal the creating transcribed character interactions the tran operator is single minded but it is clear that that there is a scripts were time aligned with detected turn taking lively interchange in automatic sal the impression is changes none of the user interactions in the semiautomatic that it is hard to read the operator train of thought but a sal or automatic sal sessions have been transcribed but cooperative user like this one can find plausible direc the operator utterances are automatically recorded and tions to follow made available as log files the transcripts obscure the nonverbal behaviors which laughs signal participants engagement or lack of it in solid sal engagement was overwhelmingly the norm the labeling an initial subset of laughter was identified in the process incorporated several ways of identifying anomalous transcription process this was added using the semaine interactions together they were used in just over percent laugh detector which was manually corrected and aligned of ratings indicating that very little of the user behavior is these laughs are included in the aligned transcripts with the time of occurrence and the annotation laugh user either contrived or is engaged laughter was present in out of transcribed character in semi automatic and automatic sal sessions it was interactions the rates of laughter varied by character and clear that interaction sometimes broke down whether that number of instances of laughter for each character for both is a problem depends on the frequency of breakdown user and operator can be seen in fig several techniques were used to identify sessions where problems arose the experimental procedure in semi nods and shakes automatic and automatic sal included three questions to instances of nods and shakes were specifically identified users about the quality of the interaction how naturally within the database one hundred fifty four nods and do you feel the conversation flowed did you feel the head shakes were annotated by two raters using two avatar said things completely out of place if yes how annotation strategies the first was a subset of the main often and how much did you feel you were involved in mckeown et al the semaine database annotated multimodal records of emotionally colored conversations table table alpha coefficient for functionals associated with distribution of optional traces for the most used options each trace dimension indicates alpha no others reach per character or across characters the lowest value commonly considered acceptable indicates alpha almost always considered acceptable y indicates nonacceptable values trace to a list of values averages over sec bins and the conversation the sessions also included a yuck button which users were asked to press when the calculated the correlations between all the resulting pairs interaction felt unnatural or awkward in both semi of lists again cronbach alpha coefficients can be derived automatic and automatic sal each interaction was from the correlations followed by an open ended invitation to state the way the alpha was calculated for sets of traces each user felt about the conversation in automatic sal an describing a single clip on one of the core dimensions additional layer was available where an observer used a less than percent fail to reach the standard criterion of feeltrace type scale to rate each participant apparent alpha and more than percent meet a stringent level of engagement criterion of alpha there are reasons to be wary of the database includes information from all these sources alpha as a measure with this kind of data and for that a useful overall indicator is that in the final session with reason we developed an alternative method called qa for automatic sal average self ratings of engagement with qualitative agreement the relevant point here is just that poppy spike obadiah and prudence were respectively although the basis for calculating agreement is completely and on scale from none to complete different and specifically avoids the problem assumptions hence from the users point of view a substantial proportion it gives very similar conclusions about the overall level of of the interactions were thoroughly engaging in contrast agreement in the sample details of the method and the the malinteractions provide data relevant to recognizing results are in there are some differences between the problems that are likely to be important in human machine different types of trace for intensity valence and power interaction for the foreseeable future over percent of trace sets meet the stringent criterion reliability of main traces the figure is much lower for activation percent and much higher for expectation percent these differences the trace set available for solid sal allowed reliability to be invite exploration again cowie and mckeown give measured in two stages the first considered relationships more detail between clips using functionals derived automatically from each trace of each clip mean standard deviation average distribution of optional traces magnitude of continuous rises etc correlations can then the optional trace categories indicate where raters felt be used to measure agreement between the list of for that particular qualitative descriptors applied and show example mean valence ratings one for each clip produced how the chosen states appeared to change over time by any one rater and the corresponding list from any other table provides an overview of the most used options for from that the standard cronbach alpha measure of each of the characters for the sake of balance only data agreement can be calculated table summarizes the from the six raters who traced all the clips are included results overall the findings confirm that most of the ratings are reliable though not necessarily in the same responses are considered for each character because the respects average and maximum level are rated reliably for different characters do get quite different responses for all the traces except power and there the effect is just short instance sadness is rare overall but quite common in of the standard level beyond that judgments of intensity interaction with obadiah and showing antagonism is rare and valence seem to show consistent patterns of rises overall but common with spike though in different respects for intensity it is the it is clear that the vast majority of responses describe a magnitude of the rises on which raters agree for valence few core positions relative to the exchange after those it is their frequency come emotions directly related to the character of the it is more difficult to measure an intraclip agreement operator very few of the other categories feature at all that is agreement between raters on the way a single often the implication is that most of the information that measure say valence rises and falls in the course of a tracing can provide can be captured by quite a modest single clip as a straightforward option we reduced each number of traces considering intercorrelations among ieee transactions on affective computing vol no january march traces may show that it can be reduced further that is a dimensions valence and expectation they also reported research question that the data can be used to explore that the detection of events always adds information relevant to the problem that is when the detected events are combined with the signal level features the performance automatic analysis of the database always increases the quality and scale of the semaine corpus provides an opportunity to develop new ways of automatically analyz ing human behavior by detecting social signals the availability synchronous high quality audio and video streams the semaine data set is made freely available to the combined with the large amount of manual annotations research community it is available through a web allow audio and computer vision researchers to develop accessible interface with url http semaine db eu new systems and evaluate them on naturalistic data it has already been used in that capacity for a number of other organization related projects and their results illustrate the potential within the database the data are organized in units that we jiang et al reported on facial muscle action facs call a session in which the user speaks with a single action units aus detection on the semaine data they character there are also two special sessions per recording compared two appearance descriptors local binary pat the and sessions where the terns and local phase quantization and found that user operator prepares to do the experiment or ends it between the two local phase quantization performed best and in semi automatic and automatic sal there are they were able to detect aus with an average measure evaluation recordings although these sessions do not of percent however this was tested on only eight show the desired user character interaction they may still sessions of only two subjects the authors found that there be useful for training algorithms that do not need was a big difference in performance between the two interaction such as the facial point detectors or detectors subjects they reported that the temporal extension of lpq which sense the presence of a user called lpq top attained the highest performance the number of sensors associated with each session gunes and pantic proposed a system to automati depends on the originating scenario solid sal recordings cally detect head nods and shakes and continued to detect have nine sensors associated with them while all other the affective dimensions arousal expectation intensity scenarios have seven we call the sensor database entries power and valence to detect the head actions nodding tracks nine of these are the five camera recordings and the and shaking they first extracted global head motion based four microphone recordings see section in addition on optical flow the detected head actions together with the each session has two lower quality audio visual tracks global head motion vectors were then used to predict the showing the frontal color recordings of the user and the values of the five dimensions labeled in all recordings operator respectively both have audio from both speakers arousal expectation intensity power and valence in the the fact that these have both audio and video information process they addressed the notoriously difficult problem of makes them useful for annotation of the conversation by differences in interpretation by different observers by human raters to allow annotators to focus on only one modeling each annotator directly independent of the others person talking we stored the user audio in the left audio nicolaou and pantic developed a method to use channel and the operator audio in the right audio channel the continuous dimensional labels of multiple annotators a standard balance slider allows a rater to choose who to to automatically segment videos their aim was to listen to the low quality tracks are also small for con develop algorithms that produce ground truth by max venient download imizing intercoder agreement identify transitions between in our database all annotation files annotations are emotional states and that automatically segment audio associated with a track it is possible that a single visual data so it can be used by machine learning annotation belongs to multiple tracks for instance the techniques that require presegmented sequences they affective state of the user is associated with all tracks that tested their approach on the semaine corpus and feature the user other annotations can be associated with reported that the segmentation process appeared to be only a single track effective with the segments identified by their algorithm in the web accessible database interface sessions capturing the targeted emotional transitions well tracks and annotations are displayed conveniently in a eyben et al used the semaine corpus to first detect tree like structure a screenshot of the web interface can be a range of nonverbal audio visual events and then use these seen in fig one can click on the triangles in front of tree to predict the values of five dimensions valence arousal nodes to view all branches apart from the tracks and expectation intensity and power the visual events they annotations each session also shows information of the detected were face presence facial muscle actions facs people that are present in the associated recording this action units and the head actions nodding shaking and information about the people shown is anonymous it is head tilts the acoustic events they detected were laughter impossible to retrieve a name of the subject from the and sighs the events were detected on the basis of a short database in fact this information is not even contained in temporal window and combined into a single bag of words the database feature vector they reported that results using this string approximately one third of the recorded data are being based approach were at least as good as the traditional withheld from public access to allow for benchmarks signal based approaches and performed best for the procedures to be set up and for the organization of mckeown et al the semaine database annotated multimodal records of emotionally colored conversations fig form search some options and the search results very large source of information on the contingencies fig data organization of the database between these various elements and their relationship to the parties emotions and engagement challenges similar to the interspeech audio analysis series these developments are of interest to the human sciences e g and the fera facial expression recognition as well as to computing for example substantial theoretical challenge the database also defines a partitioning of issues hinge on the way facial gestures appear in sponta the publicly available data into a training development neous emotional expression but the scarcity of naturalistic and test set the training set would be used by researchers material and the labor of identifying facial actions has made to train their systems with all relevant parameters set to a it difficult to draw strong conclusions the issue specific value while the development set would then be affects not only the generation of emotion related signals used to evaluate the performance of the system given these but also the mechanisms needed to recover information parameters the partitioning information is specified in two from such signal configurations sal data offer a text files available from the website realistic prospect of addressing these questions deeper questions hinge on the point emphasized search throughout that interacting with an artificial agent is not to give researchers ready access we have implemented the same as interacting with a human up to a point they extensive database search options searching the database can be treated as separate problems however the contrast can be done either by using regular expressions or by also offers new ways to expose a multitude of factors that selecting elements to search for in a tree structured form make human human interaction what it is but whose effect the regular expression search is mainly intended for people is usually so automatic that we do not realize they are there pertinent services and attractive features however access smartphone security research has produced many useful to these capabilities also opens the door to new kinds of tools to analyze the privacy related behaviors of mobile security and privacy intrusions malware is an obvious apps however these automated tools cannot assess problem but a more prevalent problem is that a good people perceptions of whether a given action is number of legitimate apps gather sensitive personal legitimate or how that action makes them feel with information without users full awareness for example respect to privacy for example automated tools might facebook and path were found uploading users contact detect that a blackjack game and a map app both use lists to their servers which greatly surprised their users one location information but people would likely view and made them feel very uncomfortable the map use of that data as more legitimate than the game our work introduces a new model for privacy a number of research projects have looked at protecting namely privacy as expectations we report on the results mobile users privacy and security by leveraging of using crowdsourcing to capture users expectations of application analysis or proposing security what sensitive resources mobile apps use we also report extensions that provide app specific privacy controls to on a new privacy summary interface that prioritizes and users these systems are useful for capturing highlights places where mobile apps break people and analyzing an app usage of sensitive resources expectations we conclude with a discussion of however no purely automated technique today and implications for employing crowdsourcing as a privacy perhaps not ever can assess people perceptions of evaluation technique whether an action is reasonable or how that action makes users feel with respect to their privacy for example is a author keywords given app use of one location solely for the purpose of mental model privacy as expectations privacy summary supporting its core functionality it all depends on the crowdsourcing android permissions mobile app context for a blackjack game probably not but for a map acm classification keywords application very likely so however currently users have m information interfaces and presentation e g hci very little support in making good trust decisions miscellaneous regarding what apps to install general terms in this paper we frame mobile privacy in the form of design human factors people expectations about what an app does and does not do focusing on where an app breaks people introduction expectations there has been a lot of discussion about the number of smartphone apps has undergone expectations being an important aspect of privacy tremendous growth since the inception of app markets as we framed our inquiry on the psychological notion of of june the android market offered apps mental models that first introduced by craik and later with more than billion downloads since the market mentioned in other domains all people have a launch the apple app store offered more than simplified model that describes what people think an apps with over billion downloads since its launch object does and how it works in our case the object is an these mobile apps can make use of a smartphone app ideally if a person mental model aligns with what numerous capabilities such as users current location call the app actually does then there would be fewer privacy logs and other information providing users with more problems since that person is fully informed as to the permission to make digital or hard copies of all or part of this work for app behavior however in practice a person mental personal or classroom use is granted without fee provided that copies are model is never perfect we argue that by allowing people not made or distributed for profit or commercial advantage and that copies to see the most common misconceptions about an app we bear this notice and the full citation on the first page to copy otherwise or republish to post on servers or to redistribute to lists requires prior can rectify people mental models and help them make specific permission and or a fee better trust decisions regarding that app ubicomp sep sep pittsburgh usa copyright acm 501 we believe that this notion of privacy as expectations can requested permissions or not to install the app at all once be operationalized by combining two ideas the first is to granted permissions cannot be revoked unless users use crowdsourcing to capture people mental models of uninstall the app an app privacy related behaviors in a scalable manner there have also been several user studies looking at this requires some knowledge of an app actual usability issues of permission systems in warning users behaviors which can be obtained with app analysis tools before downloading apps kelley et al conducted such as taintdroid the second is to convey these semi structured interviews with android users and found expectations to users through better privacy summaries that users paid limited attention to permission screens that emphasize the surprises that the crowd had about a and had poor understanding of what these permissions given app imply permission screens generally lack adequate our long term goal is to build a system that leverages explanation and definitions felt et al found similar crowdsourcing and traditional security approaches to results from internet surveys and lab studies that current evaluate the privacy related behaviors of mobile apps android permission warnings do not help most users this paper presents the first step to understand the design make correct security decisions space and the feasibility of our ideas our work leverages this past work investigating we make the following research contributions android permissions we extend their ideas in two new we demonstrate a way of capturing people ways the first is using crowdsourcing as a way of expectations using crowdsourcing more specifically measuring people expectations regarding an app we conducted user studies on amazon mechanical behavior rather than relying solely on automated turk amt with android users surveying their techniques this allows us to capture a new aspect of expectations and subjective feelings about different mobile app privacy that past work has not the second is apps accessing sensitive resources such as location the design and evaluation of a new privacy summary contact lists and unique id in different conditions interface that emphasizes access to sensitive resources we identify two key factors that affect people that people did not expect mental model of a mobile app namely expectation mobile application analysis and security extensions and purpose and show how they impact users researchers have also developed many useful techniques subjective feelings and tools to detect the sensitive information leakage in we present an analysis which indicates that mobile apps by using informing users of why a given resource is being permission analysis e g static code analysis used can allay their privacy concerns since most e g network analysis e g or dynamic flow users have difficulty figuring out these purposes analysis e g their results identified the strong we present the design and evaluation of a new penetration of ads and analytics libraries and other privacy summary that emphasizes behaviors that did prevailing privacy violations including excessively not match the crowd expectations our results accessing sensitive information we used taintdroid suggest that our interface significantly increases in our work to investigate the ground truth of the top users privacy awareness and is easier to comprehend popular android apps on how and for what purpose than android current permission interface sensitive resources were used amini et al offered an related work vision of an cloud based service that leverages we have organized related work into three sections an crowdsourcing and traditional security approaches to overview of the android permission system research on analyze mobile applications our work follows this vision mobile app analysis and security extensions and relevant and demonstrates the feasibility of incorporating work in mental model analysis and design for privacy crowdsourcing in application analysis related user interfaces many security extensions have been developed to harden android permissions privacy and security mockdroid tissa and the android permission framework is intended to serve appfence substitute fake information into api calls two purposes in protecting users to limit mobile apps made by apps such that apps could still function but with access to sensitive resources and to assist users in zero disclosure of users private information nauman et making trust decisions before installing apps android al proposed apex which provided more fine grained apps can only access sensitive resources if they declare control over the resources usage based on context and permissions in their manifest files and get approved by runtime constraints to enable wide deployment jeon et users during the installation time on the official android al proposed an alternative solution that rewrote the market before installing an app users are shown a bytecode of mobile apps to enforce more privacy controls permission screen listing the resources an app will access instead of modifying the android system as the users can choose to either install the app with all the previous solutions though app analysis provides us with a better generated by the owner of the web site in our case understanding of apps behaviors it cannot infer people information is gathered through both crowdsourcing perceptions of privacy or distinguish between behaviors users mental models and profiling mobile apps using which are necessary for an app functionality versus dynamic taint analysis e g using taintdroid behaviors which are privacy intrusive similarly while crowdsourcing users mental models the security extensions above provide users with more in this section we present the design and results of our control over their private data it is unclear if lay users can study using crowdsourcing to capture users mental correctly configure these settings to reflect their real models about a mobile app behavior preferences our work complements this past work by suggesting an alternative way of looking at mobile taking a step back there are four reasons why privacy from the users perspective we study users crowdsourcing is a compelling technique for examining mental models of mobile privacy aiming to identify the privacy past work has shown that few people read end most pertinent information to help users make better user license agreements eulas or web privacy privacy related trust decisions policies because a there is an overriding desire to install the app or use the web site b reading these expectations of privacy mental model studies and privacy interface design policies is not part of the user main task which is to use the notion of expectations is fairly common in the app or web site c the complexity of reading these discussions of privacy for example in katz v policies and d a clear cost i e time with unclear united states supreme court put forward reasonable benefit crowdsourcing nicely addresses these problems expectation of privacy to test reasonableness of legal it dissociates the act of examining permissions from the privacy protections under the fourth amendment act of installing apps by paying participants we make palen and dourish and barth et al discussed how reading these policies part of the main task and also offer expectations are governed by norms past experiences clear monetary benefit lastly we can reduce the and technologies our notion of privacy as expectations is complexity of reading android permissions by having a narrower construct focusing primarily on people participants examine just one permission at a time rather mental models of what they think an app does and does than all of the permissions and by offering clearer not do our core contribution is in operationalizing explanations of what the permission means privacy in this manner in terms of using crowdsourcing study design to capture people expectations as well as reflecting the we recruited participants using amazon mechanical crowd expectations directly in a privacy summary to turk amt we designed each human intelligence task emphasize places where an app behavior did not match hit as a short set of questions about a specific android people expectations app and resource pair see figure participants were asked to read the provided screenshots and description of past work has looked at understanding people mental an app as retrieved from the official android market models regarding computer security for example camp then they were asked if they have used this app before discussed five different high level metaphors for how and what category this app belongs to the categorization people think about computer security wash questions were designed as an easy check to detect if identified eight mental models folk models of security participants were gaming our system e g clicking threats that users perceived and how these models can through hits without answering questions justify why users ignored security advice bravo lillo et al conducted studies to explore the psychological after these two questions participants were shown one of processes of users involving perceiving and responding to two sets of follow up questions one of the conditions computer alerts sadeh et al also studied the complexity referred to as the expectation condition was designed to of people location sharing privacy preferences capture users perceptions of whether they expected a this past research has a similar flavor as ours in terms of given app to access a sensitive resource and why they trying to understand the mental models people used to thought the app used this resource participants were also make trust decision our work extends this past work to a asked to specify how comfortable they felt letting this app new domain namely mobile app privacy access the resource using a point likert scale ranging from very comfortable to very uncomfortable in kelley et al proposed simple visualizations called the other condition referred to as the purpose condition privacy nutrition labels to inform user how their we wanted to see how people felt when offered more fine personal information is collected used and shared by a grained information participants were told that a certain web site our new proposed mobile privacy summary resource would be accessed by this app and given specific interface is inspired by their work our work differs in reasons e g user location information is accessed for how we acquire privacy related information in their target advertising we identified these reasons by work the expectation is that a nutrition label would be examining taintdroid logs and using knowledge about ad please read the application description carefully and answer the questions below suppose you have installed toss it on your android device app name toss it would you expect it to access your precise location required yes no toss it does access users precise location information could you think of any reason why this app would need to access this information required precise location is necessary for this app to serve its major functionality precise location is used for target advertisement or market analysis precise location is used to tag photos or other data generated by this app toss a ball of crumpled paper into a waste bin surprisingly addictive join the precise location is used to share among your friends or millions of android gamers already playing toss it the most addictive casual game people in your social network on the market free other reason please specify simple yet challenging game play toss paper balls into a trash can but don t forget to account for the wind i cannot think of any reason challenge your friends to a multiplayer game with scoreloop do you feel comfortable letting this app access your precise toss that paper through unique levels you can even throw an iphone glob location required and if you like toss it check out these other free games from myyearbook tic tac very comfortable toe live aiminesweeper minesweeper line of multiplayer game like connect somewhat comfortable four somewhat uncomfortable very uncomfortable have you used this app before required yes no based on our analysis toss it accesses user precise what category do you think this mobile app should belong to location information for targeted advertising required suppose you have installed toss it on your android device game application book music or video do you feel comfortable letting it access your precise location required the expectation condition or the purpose condition very comfortable please provide any comments of this app you may have below somewhat comfortable somewhat uncomfortable very uncomfortable figure sample questions in our study to capture users mental models participants were randomly assigned to one of the conditions in the expectation condition participants were asked to specify their expectations and speculate the purpose for this resource access in the purpose condition the purpose of resource access was given to participants in both conditions participants were asked to rate how comfortable they felt having the targeted app access their resources networks participants were then asked to provide their ensure a between subject design where a participant comfort ratings as in the expectation condition finally would only be exposed to one condition participants from both conditions were encouraged to to prevent other confounding factors such as cultural or provide optional comments on the apps in general the language issues we restricted our participants to those separation of the two conditions let us compare users who were located within the u s to guarantee the quality perceptions and subjective feelings when different of our data we also required participants to have a information was provided lifetime approval rate higher than i e the rate of we focused our data collection on four types of sensitive successfully completing previous tasks resources as suggested by appfence unique device all the hits of this study were completed over the course id contact list network location and gps location we of six days we collected a total of responses also restricted the pool of apps to the top most were discarded due to incomplete answers and were downloaded mobile apps on the android market overall discarded due to failing the quality control question of these apps requested access to unique phone id yielding valid responses there were verified to the contact list to gps location and to network android users in our study with an average lifetime location this resulted in app and resource pairs i e approval rate of sd the distribution of distinct hits for each hit we recruited unique android versions our participants used was very close to participants to answer our questions per condition google official numbers on average participants we used the following qualification test to limit our spent about one minute per hit m sd participants to android users as well as to filter out and were paid at the rate of per hit people who were not serious crowd participants were the most unexpected and the most uncomfortable asked to provide the android os version of their device our first analysis looked at what sensitive resource usages with instructions on where to find this information on were least expected by users based on data from the their android devices when reviewing participants expectation condition for each app and resource pair we qualification requests we also randomly assigned aggregated the data by calculating the percentage of qualified participants to different conditions by giving participants who expected the resources to be accessed them different qualification scores in this way we could and averaging the self reported comfort ratings ranging from very comfortable to very uncomfortable resource app name expected avg table summarizes the resource usages that less than comfort of participants said that they expected for example network brightest flashlight only of participants expected the brightest flashlight location toss it app would access users network location information angry birds and overall participants felt uncomfortable about this air control lite horoscope resource usage m sd similarly only gps brightest flashlight of participants expected the talking tom app would location toss it access users device id and of people expected shazam pandora to access their contact list device id brightest flashlight generally speaking when participants were surprised by talkingtom free an access to a sensitive resource they also found hard to mouse trap explain why this resource were needed note that in the dictionary expectation condition participants were only informed ant smasher horoscope about which resources were accessed they were not contact backgrounds hd informed about the purpose of why these resources were list wallpapers accessed this is similar to what the existing android pandora permission list conveys to users in this condition we go launcher ex observed a very strong correlation r between the percentage of expectations and the average comfort table the most unexpected resource usages identified in ratings in other words the perceived necessity of the the expectation condition i e resource usage expected by no more than of participants users felt resource access was directly linked to their subjective uncomfortable with these unexpected app behaviors for feelings thus guiding the way users make trust decisions each app and resource pair participants were surveyed on mobile apps as many participants also mentioned in the comfort rating was ranging from very their comments these surprises prompted them to take uncomfortable to very comfortable for all the apps different actions for example participant said about we surveyed there was a strong correlation r brightest flashlight app why does a flashlight need to between people expectation and their subjective feelings know my location i love this app but now i know it weatherbug application uses location for retrieving local access my location i may delete it said i didn t weather information as well as for targeted advertising know pandora can read my phone book but why can i turn it off i ll search for other internet radio app we compared the reasons our participants provided in the similarly showed a similar concern for the toss it expectation condition against the ground truth from our game i do not feel that games should ever need access analysis as shown in table in most cases the majority to your location i will never download this game of participants could not correctly state why a given app requested access to a given resource when the resources lay users have a hard time identifying the reason an app accesses a resource were accessed for functionality purposes participants another way to look at the expectation condition is that it generally had better answers however the accuracy never presented users with information comparable to what is exceeded when sensitive resources were used for provided by the android permission system namely what multiple purposes the accuracies tended to be much resources may be accessed we wanted to see to what lower we also note that participants had slightly better extent people understand the behaviors of apps in this answers of why their location information was needed optimal case where they were paid to read the privacy compared to the other two types of sensitive resources summaries based on our results even if users were fully note that these results are for the situation where aware of which resources were used they still had a hard participants were paid to carefully read the description time understanding why these resources were needed many of them had even already used some of these apps we used taintdroid to analyze all the mobile apps in before we believe for general android users their our study to identify the actions that triggered the ability to guess would be even worse this also indicates sensitive resource access and where the sensitive that simply informing users of what resources are used as information was sent to we then manually categorized today android permission screen does is not enough each app and resource pair into three categories for for users to make informed decision major functionality for sharing and tagging or clarifying the purpose may ease worries supporting other minor functions for target given the lack of clarity of why their resources are advertising or market analysis many resource usages fell accessed users have to deal with significant uncertainties into more than one category for example the when making trust decisions regarding installing and resource resource used for cnt of of comfort comfort type major functionality accurate no resource rating w rating w o tagging or sharing guess idea type purpose purpose df t p advertising or market analysis device id contact contact list list 54 network location gps gps location location table comparison of comfort ratings between the expectation condition column and the purpose condition column standard deviations are shown network between parentheses when participants were informed location of the purpose of resource access they generally felt more comfortable the differences were statistically significant for all four types of resources the comfort ratings were ranging from very uncomfortable to very comfortable device id in comfort rating when the purpose of a resource access was explained table participants had a difficult time speculating on this finding suggests that providing users with the the purposes of their sensitive resource usages the first reasons why their resources are used not only gives them column shows the type of resource accessed and the total number of apps accessing that resource the second more information to make better trust decisions but can column shows the ground truth of why the resource is also ease their concerns caused by uncertainties note that accessed the third column shows the number of apps in informing users about the purpose for collecting their each category e g apps access contact list for reason information is a common expectation in many legal and the third column shows the percentage of regulatory privacy frameworks our results confirm the participants stated the purpose correctly the last importance of this information this finding also provides column shows the percentages of participants who had us with strong rationale for including the purpose of no idea why the resource is accessed resource access in our new design of privacy summary interface using a given mobile app we wanted to see if providing users with more fine grained information especially the impact of previously using an app purposes of resource access would have any influence on we also wanted to see how previous experiences with an users privacy related subjective feelings to answer this app impacted participants expectations and level of question we compared the average comfort ratings from comfort to answer this question we compared the both conditions for each mobile app and resource pair responses between participants who had and hadn t used the app before the ratio of people who had and had not we observed that for all four types of sensitive resources used the apps in our study varied greatly some apps i e device id contact list network location and gps such as facebook and twitter saw high usage among location participants felt more comfortable when they our participants while others such as kakao talk were informed of the purposes of a resource access see messenger and horoscope had fairly low usage to make table the differences between the comfort ratings the comparison fair we only examined apps that had at were statistically significant in t tests for example with least responses in both the used and not used categories regard to accessing the device id the average comfort in our data the differences between participants who had rating in the purpose condition was higher than in the and had not used these apps before were not statistically expectation condition t p for some significant with respect to their expectation of sensitive apps informing people of the purpose led to totally resource access regarding their comfort level the only different feelings for example participants felt uneasy significant difference we observed is the average comfort when told the dictionary app accessed their network ratings for accessing the contact list participants who location mcomfort sd however when they used an app before felt more comfortable letting that app were informed that the location was only used to search access their contact list t p for the for trending words that people nearby are looking up they other three types of resources the experiences with apps felt much less concerned mcomfort sd didn t cause any statistically significant differences in similarly air control lite ebuddy shazam antivirus participants subjective feelings and other apps all demonstrate a significant increase this finding suggests that people who use an app do not necessarily have a better understanding of what the app is actually doing in terms of accessing their sensitive resources it also suggests that if we use crowdsourcing to capture users mental models of certain apps we do not have to restrict our participants to people who are already 87 familiar with these apps allowing us access to a potentially larger crowd new privacy summary interface 87 in the previous section we had identified that purpose and expectation are two key factors that impact users subjective feelings based on this finding we present the design of a new privacy summary interface highlighting the purposes of sensitive resource usage and people perceptions about app behaviors design rationale privacy summary interfaces such as the permission figure a mockup interface of our newly proposed privacy screen in current android are designed for users to summary screen taking the brightest flashlight and the review before downloading mobile apps by that time dictionary app as examples the new interface provides extra information of why certain sensitive resources are users have limited information to form their mental model needed and how other users feel about the resource usages of the targeted mobile app since they haven t had any warning sign will appear if more than half of the previous interaction with it in contrast with our crowdsourcing users were surprised about this resource access study we cannot rely on general users to carefully examine an app description or screenshots to understand storage contact list etc users could choose to check how this app works in reality in our new design we out other low risk resources by clicking see all directly leverage other users mental models the sorting the list based on expectation as captured underlying rationale is similar to the idea of patil et al through crowdsourcing we order the list so that the in the sense of incorporating others opinions in more surprising resource usages are shown first making privacy decisions our work differs from their highlighting important information we bold the work by aggregating users subject feedback from crowds sensitive resources mentioned in text and use instead of from one social circle and highlighting users warning sign and striking color to highlight the surprises by presenting the most common suspicious resource usages i e when the surprise misconceptions about an app we can rectify people value exceeds a certain threshold mental models and help them make better trust decisions we consider users expectations and the purposes of figure shows two examples of our new privacy resource access as the two key points that we want to summary interface to make the comparison more convey to users in our new summary interface symmetric our design uses the same background color and pattern are used in the current android permission previous research has discussed several problems with the screen the surprise numbers i e n of users were existing android permission screens including surprised used in these mockups were obtained from our the wording of the permission list contains too much crowdsourcing study where possible the surprise technical jargon for lay users numbers for other resources such as camera flashlight they offer little explanations and insight into the sd card were reasonable estimates made by our team potential privacy risk evaluation a long list of permissions make users experience we used amt to conduct a between subject user study to warning fatigue evaluate our new privacy summary interface participants with these problems in mind in addition to the two were randomly assigned to one of the two conditions in identified key features we proposed several principles for the same way as our previous study in the permission our own design condition participants were shown the permission screen using simple terms to describe the relevant that the current android market uses in the other resources e g instead of using coarse network condition referred as the new interface condition location we use the term approximate location participants were shown our new interfaces we used the only displaying the resources that have greater data we collected in our previously described impact on users privacy such as location device id crowdsourcing study to mock up the privacy summary p p of people mentioning privacy concerns out of accuracy max time spent sec app name permission new interface permission new interface p permission new interface p brightest flashlight 74 dictionary 92 horoscope 75 95 pandora 68 94 82 toss it table comparisons between the existing android permission screen permission condition and our newly proposed privacy summary new interface condition our new interface makes users more aware of the privacy implications and is easier to understand users in general spent less time on these newly proposed interfaces but got more fine grained information interfaces for five mobile apps namely brightest users understood the privacy summary this is measured flashlight dictionary horoscope pandora and toss it by the accuracy in answering questions about the app behavior the third is efficiency i e how long it took in both conditions the app name screenshots participants to understand the privacy summary measured description and the quality control question were by the number of seconds they spent on reading the presented the same way as in previous study the privacy privacy summary screens summary was then shown either the current permission screen or our newly proposed interface participants were the comparisons between the two conditions are asked whether they would recommend this app to a friend summarized in table generally speaking participants who might be interested in it and why or why not we in the new interface condition weighted their privacy used javascript to keep track of the time participants more when they made decisions about whether the app spent on reading the privacy summary before making was worth recommending more people in this condition their recommendation choices after this question mentioned privacy related concerns when they were privacy summary screens were covered by grey justifying their choices when we asked people in both rectangles participants could recheck the privacy conditions to specify the resources used by the target apps summaries by moving their mice over the grey rectangles of the target apps people in the new interface condition in this way we could accurately record the additional also demonstrated a significantly higher accuracy time participants spent on viewing privacy summary compared to their counterparts furthermore except for screens by monitoring the mouse hovering events we the pandora app participants in the new interface then added up all these time fragments to compute the condition on average spent less time reading the privacy total time participants spent on reading the privacy summaries on average though the time difference was not summary participants were tested on their understanding always statistically significant this finding suggests that of the presented privacy summary screen by specifying we can provide more useful information without requiring the resource usages suggested by the privacy summary users to spend more time to understand it for each condition per app unique participants were in our future work we plan to conduct lab studies to recruited participants could evaluate multiple apps within evaluate our new privacy summary interface in depth we the same condition a total of responses were will focus on the effectiveness of the new interface when submitted of which were discarded due to users only look at it briefly e g for secs since in incompletion and of which were discarded due to reality general users are not likely to devote a lot of time failing the quality control question sixty seven android to reading users participated in this study with an average lifetime discussion approval rate of sd thirty five in this section we discuss the potential implications of participants were assigned to the permission condition our work and how it fit into our vision of leveraging and thirty two were assigned to the new interface crowdsourcing for application analysis condition participants on average spent min and sec sd sec in completing each evaluation task and implications for privacy analysis were paid at the rate of hit a potential win win a major finding of our work is that users feel more comfortable when they are informed of we evaluated the new privacy summary interface from the reasons why their sensitive resources are needed in three perspectives to test its effectiveness and usability some cases it might be again tied to users expectations the first is privacy awareness i e whether users are more for example the trending popular and nearby search aware of the privacy implications this is measured by functionality provided by the dictionary app uses location counting the number of participants who mentioned information to retrieve the words that people nearby are privacy concerns when justifying their recommendation looking up it is a relatively minor function of this app decisions the second is comprehensibility i e how well and may not be expected even for users who are familiar with this app therefore when we asked participants to were not very good at speculating on the purpose of state the reasons for accessing location information most resource access which is not surprising and might be of them thought it was for targeted advertising purpose compensated by leveraging existing mobile app analysis hence rating the comfort level much lower than they were techniques however specifying their expectations is a informed about the actual reason we also observed relatively easy job for most people but cannot be several cases e g the weather channel gasbuddy addressed by existing app analysis tools compass where participants had correct answers as to as the first work of this kind we simplified the problem why the app was using one location but still felt less by focusing only on privacy although we realize that comfortable when compared to the condition where users may weigh utility over privacy when making participants were directly given the purpose it suggests decisions about installing an app future research will that when dealing with uncertainties users tend to be need to take utility into account in understanding how more concerned or even paranoid about their privacy our people make trust decisions results provide evidence that properly informing users with the purposes of resource usage can actually ease we also only captured people perceptions at a coarse their worries in other words it would potentially benefit granularity and with limited types of sensitive resources all parties including app developers market owners and we will extend our work to finer grained interactions e g advertisers whether users expect the yelp app to send their location to yelp com when they press search nearby restaurant currently the default android permission screen doesn t button we envision that this level of analysis could contain any explanations one possible approach for provide us more detailed information for evaluating getting this information is to scale up our crowdsourcing mobile apps and could possibly lead to better results approach but there is the potential for errors as we saw when asking the crowd why an app accesses a given in table another approach is to require app developers resource to include a rationale but this is an optimistic approach assuming that developers won t lie this also suggests in our crowdsourcing study it cost us and about that better tools are still needed for analyzing apps minutes deducted from the effective hourly rate behaviors in a more scalable and automated manner as reported by amt to examine one app and resource pair envisioned by amini et al with input from participants there is ample room to improve the crowdsourcing efficiency examples include privacy concerns of mobile advertising we observed extending the participant pool to all smartphone users that mobile advertising services were a consistent privacy minimizing the number of questions and so on there are concern for the most participants for all four types of also several techniques suggested by previous resources users felt the least comfortable when they were crowdsourcing work that we can leverage to used for advertising or market analysis we understand improve the overall efficiency e g dynamically that many developers rely on ads for income however publishing hits adaptively adjusting the compensation there is still space for app developers and ad networks to rate and the number of required responses given that it improve the user experience such as by providing users only took about one minute for our participants to with more informed consent and more explanations on complete a crowdsourcing task we believe this method how and why their personal information is used other would scale well though formal scalability analysis is potential ways include tweaking the sensitive resource still an open issue and will be included in our future work usage to a coarser level or using hashing or other methods to conceal users identities these technical alternatively crowdsourcing users perceptions could be methods can address users privacy concerns without achieved in conjunction with the exiting app rating sacrificing too much on the ads quality mechanism when users rate a mobile app they can also leveraging crowd for application analysis optionally specify their expectations of one aspect of the the long term vision of our work is to design a scalable target app as the number of rating grows the aggregated privacy evaluation system for mobile apps by combining perceptions will be more representative automated application analysis with crowdsourcing conclusion future work techniques the automated techniques are meant to a great deal of past work in mobile security and privacy capture an app behaviors involving sensitive resources research has focused on providing tools for automated whereas the crowdsourcing techniques capture people analysis however there is still no easy way to perceptions and expectations about an app behaviors distinguish whether accessing certain sensitive resource is necessary or how that action makes users feel with one important contribution of this paper is to demonstrate respect to their privacy our work demonstrates a new the feasibility of using crowdsourcing to capture users way for evaluating mobile app privacy we explore perceptions and to identify the strength and weakness of users mental models of mobile privacy by crowdsourcing the crowd in evaluating privacy based on our data users we present an approach to printing custom optical ele printing is becoming increasingly capable and affordable ments for interactive devices labelled printed optics printed we envision a future world where interactive devices can be optics enable sensing display and illumination elements to printed rather than assembled a world where a device with be directly embedded in the casing or mechanical structure of active components is created as a single object rather than an interactive device using these elements unique display a case enclosing circuit boards and individually assembled surfaces novel illumination techniques custom optical sen parts figure this capability has tremendous potential sors and embedded optoelectronic components can be dig for rapid high fidelity prototyping and eventually for produc itally fabricated for rapid high fidelity highly customized tion of customized devices tailored to individual needs and or interactive devices printed optics is part of our long term specific tasks with these capabilities we envision it will be vision for interactive devices that are printed in their en possible to design highly functional devices in a digital ed tirety in this paper we explore the possibilities for this vision itor importing components from a library of interactive afforded by fabrication of custom optical elements using to elements positioning and customizing them then pushing day printing technology print to have them realized in physical form in this paper we explore some of the possibilities for this vision afforded by today printing technology specifically we describe acm classification h information interfaces and pre an approach for using printed optical elements printed sentation user interfaces optics as one category of components within a greater li brary of reusable interactive elements keywords printing optics light sensing projection custom optical elements have traditionally been expensive display rapid prototyping additive manufacturing and impractical to produce due to the manufacturing pre cision and finishing required recent developments in printing technology have enabled the fabrication of high res permission to make digital or hard copies of all or part of this work for olution transparent plastics with similar optical properties to personal or classroom use is granted without fee provided that copies are plexiglastm one off printed optical elements can be not made or distributed for profit or commercial advantage and that copies designed and fabricated literally within minutes for signifi bear this notice and the full citation on the first page to copy otherwise or cantly less cost than conventional manufacturing greatly in republish to post on servers or to redistribute to lists requires prior specific permission and or a fee creasing accessibility and reducing end to end prototyping uist october cambridge massachusetts usa time printed optical elements also afford new optical copyright acm form factors that were not previously possible such as fab example applications that demonstrate how printed optics can be implemented and used in interactive devices in the remainder of this paper we introduce the technology that enables us to create printed optics and outline the fab rication process and its capabilities we then describe four categories of fabrication techniques for printed optics light pipes internal illumination sensing mechanical movement and embedded components we conclude with discussion of a b limitations and future research directions with the continu ing emergence of printing technology we believe now is figure we envision future interactive devices that an ideal time to explore the unique capabilities of printed are printed from individual layers a rather than optical elements for interactive devices assembled from individual parts b these devices will be fabricated from multiple materials to form active functional components within a single print printed optics printing allows digital geometry to be rapidly fabricated into physical form with micron accuracy usable optical el ricating structures within other structures printing multiple ements can be designed and simulated in software then materials within a single optical element and combining me printed from transparent material with surprising ease and chanical and optical structures in the same design affordability in this section of the paper we describe the fab rication process for printing optical elements and discuss printed optics opens up new possibilities for interaction the unique capabilities that this technology enables display surfaces can be created on arbitrary shaped objects using printed light pipes figure novel illumina fabrication tion techniques allow the internal space within a printed the fabrication process begins with a digital geometric model object to be used for illumination and display purposes fig that is converted into a series of slices to be physically fabri ure custom optical sensors can be printed with cated layer by layer printing of optical quality materials the structure of interactive devices to sense user input fig typically requires a photopolymer based process each layer ure optoelectronic components can be completely en is fabricated in sequence by selectively exposing a liquid closed inside optical elements to produce highly customiz photopolymer material to an ultra violet uv light source able and robust interactive devices figure causing the material to cure into a solid state tradition ally this has been achieved using stereolithography where our long term vision to digitally fabricate high fidelity highly a precise laser is traced through a vat of liquid photopolymer customized ready to go devices will be a powerful en other approaches include controlled exposure to uv light abling technology for hci research although much of this using a projector or physical deposition of liquid photopoly novel technology is still in the research stage mer in the presence of a uv light source the fundamental the simplest forms of printing are rapidly entering the process of layer by layer fabrication with photopolymer ma mainstream a recent cover story in the economist suggests terials is common throughout each approach printing is the manufacturing technology to change the world a host of consumer level printing devices the range of photopolymer materials for printing is rapidly are now available and the fundamental photopolymer print expanding with optical quality transparent plastic deformable ing technology behind printed optics has been demonstrated rubber and biocompatible polymers available on the mar for less than parts cost it is reasonable to expect ket in this work we used an objet printer that inexpensive optical printers will be available to re and objet veroclear transparent material to fabricate opti searchers in the very near future cal elements veroclear has similar optical properties to poly methyl methacrylate pmma commonly known as using today printing technology we aim to demonstrate plexiglastm with a refractive index of light that the design of optical systems for interactive devices can source several other manufacturers also provide similar be greatly enhanced we present the following contributions transparent materials including dsm somos watershed xc and systems accura clearvue a general approach for using printed optical elements printed optics embedded in interactive devices to display the objet has a print resolution of dpi mi information and sense user input crons that is significantly higher than fused deposition mod techniques for displaying information using printed eling fdm printers e g stratasys dimension maker optical elements including the use of printed light bot or reprap that are typically around dpi mi pipes and internal air pockets crons high resolution printing allows the creation of visibly smooth models without internal gaps model surfaces can be techniques for sensing user input with printed optical further enhanced with a manual finishing process to achieve elements including touch input with embedded sensors optical clarity this process consists of removing support mechanical displacement of printed light guides and material sanding the surfaces with incrementally finer sand movement sensed along printed mask patterns paper and then buffing capabilities fabricated in a single print simply by changing software printing technology enables the fabrication of custom parameters light pipes can be created with variable widths printed optical elements with unique capabilities that are oth rounded caps and joints with other light pipes in contrast erwise difficult to achieve conventional manufacturing requires considerable effort for individual fiber optic strands to be mechanically assembled multiple materials optical elements can be fabricated that fused deformed with heat or chemically bonded combine multiple chemically disparate materials in a single model printers can often use at least two materials si internal light pipe geometry can be embedded inside a larger multaneously model material and support material model model that has its own independent form factor such as a material constitutes the main model itself and support mate character figure mobile device figure or tangible rial is used as a sacrificial material to provide structural sup chess piece figure as each light pipe can be precisely port under model overhangs and in hollow areas typically fabricated at a given location the process of light pipe rout support material is removed and disposed of once the print ing to avoid intersections becomes a well defined software ing process is finished but can also be embedded inside the problem one current limitation of printed light pipes model to guide block and diffuse light a third material fabricated on the machine we have available is imperfect that can be utilized is air hollow pockets of air can be used light transmission with longer pipes or pipes that curve sig to guide and reflect light in a similar manner to mirrors and nificantly we outline the characteristics of this light loss in beamsplitters advanced printers can combine opaque the discussion section we have designed around this limita materials with optical quality transparent materials to mask tion to produce functional prototypes but envision our tech and block light niques can be expanded using future printers that are op timized for optical performance structures within structures as the printing process is performed additively layer by layer geometric structures can example applications be fabricated inside other structures to create optical ele we outline several example applications that demonstrate ments for example areas of transparent material can be uses for printed light pipes surrounded by a material with a different refractive index to transmit light from point to point using total internal re mobile projector displays mobile projectors have enabled a flection tir through the inside of a model opaque mate range of new interactive systems the small form rials printed within or around a transparent material can be factor of mobile projectors makes them well suited to tangi used to block the transmittance of light from one section of ble accessories that can map the projector display onto arbi the model to another this can be used to seal internal com trary surfaces we have developed a character accessory that partments and avoid optical crosstalk or to minimize light uses printed light pipes to guide projected light through leakage from the model that may be distracting to the user the inside of the model and onto outer surfaces the charac ter is printed with a grid of mm light pipes leading combined mechanical optical design printed optical from its feet to its eyes figure we paint the outer area of elements can be designed hand in hand with the mechani the character except for the ends of the light pipes when the cal design of a device for example optical elements can be feet are attached to a mobile projector the character eyes integrated into the body of a device to guide light through the become a display surface responding to user interaction such model act as lenses or house optical sensors this combined as sound or physical movement figure approach enables a rich new space for prototyping physical interfaces with low cost optical sensors such as buttons slid mobile touch sensing sensing touch has become an im ers dials and accelerometers a single mechanical optical portant part of interaction with many computing devices design can greatly reduce the number of individual parts and printing is well suited to implement touch and the manual labor required for assembly optical fiber bun grasp sensing on and around mobile devices printed dles that are typically made up of of individual fiber light pipes can sense touch by guiding light from arbitrary strands can be printed in a single pass with a solid me chanical structure printed optical elements currently have some limitations these include issues of light transmission surface finishing tangible clarity and hollow area fabrication we describe each of display surface these limitations in the discussion section we now intro duce four categories of fabrication techniques that demon printed strate the wide range of uses for printed optics light pipes light pipes mobile projector light pipes are printed optical elements similar to op tical fiber that can be used to guide light from point to point a b optical fiber has been used in interactive systems for both display and sensing purposes figure a printed mobile projector accessory unlike conventional optical fiber printed light pipes using embedded light pipes a to map a projected allow arbitrary geometries to be created in software and then image onto a characters eyes b ir emitters optical fibers tangible display surface printed light pipes touch points source display linear sensor surface a array b figure we envision touch sensing using printed light pipes embedded in the walls of future devices a b our proof of concept prototype a attaches to a mobile projector for gestural control of projected content b figure light pipes are printed inside tangible objects such as chess pieces a to create additional display surfaces for tabletop interaction b locations on the surface of a mobile device to a single sen sor array a key advantage of this technique is the ease with which light pipes can be printed into the walls of a device to ing display space that would otherwise be occluded fiducial create mechanically robust and exceptionally thin sub mm markers cut from transparent ir reflecting film tigold cor embedded sensing with minimal hardware assembly poration type are adhered to the bottom of each chess piece projected visible light passes through the transparent we have produced small scale printed prototypes that marker and into the light pipes while ir light is reflected demonstrate how light can be piped from a mm touch back to an ir camera to track and identity each chess piece point onto a 336 mm section of a ccd sensor when scaled to the size of typical mobile devices however light loss with fabrication techniques printed light pipes currently impacts upon sensing per printed light pipes function in a similar manner to op formance as a proof of concept prototype we instead use tical fiber with light reflecting internally as it travels along conventional optical fiber embedded in a printed case to the length of the pipe internal reflection is caused by a dif simulate this sensing technique we arrange light pipes ference in the refractive index of the internal core through so they are exposed on the sides of a mobile projector and which light travels and the external cladding which sur piped back to a single pixel photosensor array taos rounds the core figure we fabricate light pipes us the is capable of frame rates up to ing model material objet veroclear for the core and sup hz for extremely responsive touch interaction on or port material objet support resin for the cladding the near the device surface we use two infrared ir leds em model material cures into a rigid transparent material and the bedded in a laser cut piece of acrylic to provide constant il support material into a soft material designed to be easily lumination figure shows our proof of concept prototype broken apart for quick removal the difference in material demonstrating that the casing of a mobile projector can be density allows for tir to occur to create a mechanically embedded with light pipes for robust touch sensing single sound model we surround the brittle support material with and multi touch gestures can be used on the side of the device an outer casing of rigid material we can fabricate light pipes to scroll and zoom in on projected content figure down to a thicknesses of 25mm core with a cladding layer thickness of 084mm to create accurate digital geometry tangible displays tangible objects have been used to ex we programmatically generate light pipe grids using python tend the display area of tabletop and touchscreen surfaces inside the rhinoceros application www com with optical fiber bundles and prisms one of this geometry can then be exported into a mesh based for the inherent problems with optical fiber is maintaining align mat suitable for printing ment with large numbers or long lengths of fiber during me chanical assembly printed light pipes avoid this issue internal illumination as individual pipes can be accurately fabricated and enclosed printing is known for its ability to precisely fabricate the inside a simultaneously printed rigid support structure op outer form of physical objects and has been used to create timal packing patterns can be produced in software to max a variety of unique displays optically clear imize light transmission with large display grids although material allows for the design and fabrication of inner forms currently the optical quality is inferior to traditional optical elements light pipes do provide a highly accessible and cus casing model material tomizable alternative we have developed a set of tangible chess pieces that dis core model material play content piped from a diffuse illumination tabletop sur face a x 34 grid of mm light pipes is embed ded in the base of each chess piece and provides a display cladding support material area perpendicular to the tabletop surface figure con textual information such as chess piece location and sug figure light pipes consist of a rigid transparent core gested moves can be displayed on each individual piece us a soft cladding and a rigid outer casing sheet tube embedded dot heart shape a b figure sheets tubes and dots are primitive ele led ments for fabricating reflective pockets of air inside a print due to material settling digital geometry a differs slightly from the actual printed form b figure a toy character with an embedded heart shape made from tubes of enclosed air and illuminated within a printed model internal structures can be viewed with an led from the outside and highlighted with illumination inter nal illumination can be used with interactive devices to dis play information ranging from simple indicators to complex to dots using a z axis depth map an automated cam volumetric displays in this section of the paper we intro era calibration routine is used to compensate for any slight duce techniques for internal illumination by creating reflec mechanical misalignments tive pockets of air within a solid transparent model internal patterns printed tubes are hollow cylinder like example applications structures printed vertically figure tubes are particularly we outline several example applications that demonstrate well suited to represent arbitrary shapes inside a printed uses for internal illumination model because it is not yet possible to enclose arbitrary hol low shapes due to issues of overhang and support material volumetric displays printed internal structures are an extraction tubes represent a useful approximation we cre enabling technology for static volume volumetric displays ated a toy character with a glowing heart embedded inside allowing precise alignment of pixels within a volume figure the heart is made from a set of hollow tubes although volumetric displays have been implemented in a packed together and illuminated from below with an led number of ways printing allows a new level of figure although a simple example the internal struc control over the shape size and resolution of the display tures are very difficult to achieve with other manufacturing using the versatility of printing we implemented a volu techniques we found that tubes can be reliably created down metric display based on passive optical scatterers to mm and packed together with a minimum distance of mm between tubes this enables relatively good reso we explored a mobile form factor for a volumetric display lution to represent arbitrary internal shapes by embedding an x x array of mm dot shaped air pockets figure inside a x 80 x mm volume internal text printed sheets are flat rectangular air pock of transparent printed material figure the display ets that can be used to approximate lines figure text is mounted to a laser based mobile projector microvision within a printed structure can be fabricated to create ro showwx x pixel blocks are used to address each bust signage that is resistant to impact and everyday wear of the dots inside the volume pixel blocks are mapped the optical clarity of the material makes it possible to view internal text under normal daylight lighting conditions or with illumination at night using printed sheets to fab ricate internal text we created a nixie tube style numeric projected image z axis depth map display figure individual printed layers with em bedded numbers are mounted together and side illuminated using either an led or a mobile projector figure printed volumetric display mobile projector figure a mobile volumetric display created with a b embedded dots of air inside a printed volume a sphere is displayed inside the volume by remapping figure a numeric display a created with hollow pixels using a z axis depth map sheets of air that reflect light when illuminated b fabrication techniques by creating enclosed pockets of air within a solid transpar ent model light intersects with these air pockets and is trans mitted or reflected depending on the angle of incidence by carefully designing the geometry of the air pockets light can be guided internally within the model or externally out to a b wards the users eye air pockets can be created using printers that deposit small beads of material layer by layer to build up a model as individual beads of material are de posited they must have a supporting surface beneath them or they fall down with gravity however a small amount of step over overhang from layer to layer allows for hol low areas to be slowly closed in practice a step over angle c d equivalent to from vertical allows air pockets to be reli ably closed with minimum shape distortion greater step figure user inputs such as push a rotation over angles cause the material to fall into the air pocket and b linear movement c and acceleration d can be slowly raise the air pocket location vertically or fill it en sensed by the displacement of a printed light guide tirely figure shows a series of primitive elements that can be fabricated from air pockets the digital geometry sent to the printer left differs from the actual air pocket shape fab ricated right this difference is due to beads of material state figure elastic deformation causes the light guide settling without direct support structure during the fabrica to return to a high state when the user releases the button tion process to programmatically generate patterns from the the elasticity of the light guide provides clear haptic feed dot tube and sheet primitives we use the grasshopper envi back that can be accentuated with small surface detents on ronment www com inside rhinoceros this the button shaft allows primitive elements to be mapped to lines enclosed within solids or aligned with illumination sources we sense rotation with a screw dial that gradually lowers to displace the light guide figure when fully inserted sensing mechanical movement the receiver returns a low signal and returns to a high state optical sensing of mechanical movement has been achieved as the screw dial is extracted custom thread pitches can be in a number of form factors 35 our approach designed for precise control over how quickly the screw dial uses printing to create custom optical sensing embedded transitions between the two extremes in interactive devices we use low cost ir emitter receiver pairs to sense common user inputs such as rotation push we sense linear movement with a mechanical slider that de linear movement and acceleration our approach offers sev presses the light guide when moved from one side to the other eral benefits firstly custom sensors can be designed and figure the light guide is angled downward so that it prototyped with minimal effort printed sensors allow rests just below the slider at one end and is above the slider convenient fast accurate and repeatable sensor fabrication at the other end as the slider is moved by the user the light in many cases only a generic ir emitter receiver pair are re guide is displaced to register a low reading quired on the electronics side secondly custom sensors can be embedded in the casing or mechanical structure of a de we sense acceleration by printing extra material on the end vice this enables sensing of user input through the walls of of a light guide to enhance the effects of outside motion fig a device greatly simplifies hardware assembly and produces ure the extra weight causes the light guide to be dis robust high fidelity prototypes placed when the the user moves the device along an axis per pendicular to the light guide the effects of gravity can also example applications be perceived as the weighted light guide slumps downwards we introduce example applications that demonstrate how causing displacement printed optical elements can be used to detect common me chanical movements sensing displacement we have developed a library of me chanical movements that can be mapped to a displacement sensing scheme in this scheme a flexible light guide mounted below the surface of a device is physically displaced to change the amount of ir light traveling between an emitter receiver pair figure displacement sensing provides a ver satile way to sense a number of user inputs with printed optical elements a b we sense push and pressure with a button that applies linear figure rotary motion of a scroll wheel a and force to displace the light guide figure as the user linear motion of a slider b can be sensed with ir light presses the button the receiver changes from a high to a low passing through printed mask material ir receiver high state button ir emitter ir receiver scroll wheel ir emitter high state light guide mask material printed mask material enclosure acrylic cap ir emitter ftir ir receiver low state low state figure ftir touch sensing embedded within a printed enclosure a b bedded support material so that a distinctive change in light figure mechanical force moves a printed light guide below a button a and a rotary encoder pattern intensity is registered by the receiver figure illustrates within a scroll wheel b from a high state top to a low the high top and low bottom states of a rotary encoder state bottom we use standard or side look honeywell ir leds and light to voltage sensors taos sensing with encoders we fabricate custom linear and ro as the emitters and receivers for both displacement and en tary encoders with printing by creating a mask pattern coder sensing in this section modulation of the emitter from two different materials and sensing the change in light receiver pair would provide further robustness to varying am transmission through each material as the encoder moves bient light conditions we sense scroll wheel motion by embedding sections of embedded components mask material in the body of a scroll wheel figure as printing technology grows in sophistication we en an ir emitter receiver pair are mounted in the scroll wheel vision it will be possible to automatically drop in compo housing on either side as the scroll wheel is turned a high nents during the fabrication process using part low pattern is returned by the receiver that can be decoded placement robots similar to those employed for pc board into a relative rotation value based on the mask pattern de manufacture drop in embedded components are physically sign more complex absolute encoding schemes can also robust enable tight mechanical tolerances and allow easy be achieved using the same basic principle with multiple compartmentalization combining optical components with emitter receiver pairs transparent materials allows sensing display and illumina tion to take place through the casing without exposing com we sense slider motion by embedding a mask pattern into the ponents directly on the surface of a device eventually we shaft holding the slider figure an ir emitter receiver envision it will be possible to print electronic components pair are mounted inside the slider housing on either side of in their entirety although it is not yet possible to the shaft as the user moves the slider a high low pattern is automatically insert or print electronic components on com returned to the receiver mercial printers it is possible to simulate the results of embedded optical components by manually inserting them fabrication techniques during the print process in this section of the paper we in sensing displacement our displacement sensing scheme troduce a number of techniques for designing fabricating uses transparent material to create a flexible light guide be and interacting with optoelectronic components embedded in tween an ir emitter receiver pair light from the emitter transparent printed enclosures travels through a small aperture surrounded by light blocking mask support material and reflects inside the light guide un example applications til it meets the receiver in its normal state light travels into we introduce example applications that demonstrate uses for the receiver to register a high reading figure top the printed optical elements with embedded optoelectronic light guide can be displaced with mechanical force causing components the receiver to register a lower reading figure bottom a range of intermediate values are returned between these embedded ftir sensing frustrated total internal reflec two states in all cases the flexible light guide is positioned tion ftir based sensing is known for its robust and pre just below the surface of the sensor housing and mechani cise touch sensing performance typically individual com cally displaced by outside force emitters and receivers are ponents are mounted alongside an optical surface that users inserted into the housing walls with precisely aligned slots interact with in our approach we embed com ponents inside the optical surface to produce a very robust sensing with encoders to create rotary and linear encoders and completely enclosed ftir implementation in a basic we use an ir emitter receiver pair mechanically mounted on configuration an led emitter is embedded in a hollow area either side of a rotary shaft linear rail a printed mask within the print and light is emitted directly into a flat pattern is embedded into the rotary shaft linear rail to pro sensing surface figure an ir receiver is mounted per duce signal changes the mask pattern is created with em pendicular to the sensing surface and detects light reflected printed cap printed cap acrylic cap insert acrylic cap insert ir emitter led component insert b extra printed ir receiver material for surface smoothing printed way reflector ftir touch surface printed lens figure exploded view of a d pad made with embedded optoelectronic components a printed a c way reflector is used to direct light into a custom figure optoelectronic components are inserted shaped ftir touch surface capped and enclosed inside a printed model to create a custom lens a the quality of the lens sur face finish b can be improved by depositing additional by fingers coming into contact with the outer surface a more uniform layers of material c complex variation of the same sensing technique is shown in our four way d pad sensor figure a flat side look led honeywell is embedded facing a four way the component to be inserted support material printing must reflector that splits light towards four embedded ir receivers be disabled to ensure that it is not deposited inside the com taos figure an analog touch reading is ponent area if the component is inserted flush with the top of measured at each of the sensors as with standard ftir an the model printing is continued to completely enclose it in approximate measurement of pressure is returned based on side if an area of air is desired around the component a laser the amount of skin contact and light reflectivity cut transparent acrylic cap is affixed above the component to seal it in before printing continues figure lenses we embed optoelectronic inside printed plano convex and plano concave lenses to better control the di hollow cavities created for components do not have an opti rectionality of illumination or optical sensing the base of cally clear finish figure and it is not practical to sand the print makes up the planar side of the lens and the or buff these small cavities during the print process how curved lens surface is fabricated below the component then ever we discovered a unique technique to improve the sur smoothed with extra layers of material deposition figure face quality of cavities by depositing additional uniform lay shows two identical leds embedded with printed plano ers of material this material is deposited from above and convave a and plano convex b lenses falls into the cavity in a liquid state to smooth its surface figure shows the improved surface finish achieved when beamsplitters we fabricate rudimentary beamsplitters by ten additional layers of material are deposited creating prisms in front of embedded components based on the angle of incidence some light passes through the discussion prism and the remainder reflects off in another direction fig printed optical elements have unique limitations that should ure shows an led with light being split in two directions be considered during the design and fabrication process by the embedded prism light pipe transmission fabrication techniques printed light pipes fabricated with our current setup suffer we manually insert optical components during the print from limited light transmission to more accurately charac ing process in a similar manner to the digital geometric terize the conditions under which light loss occurs we per model is designed in such a way that a hollow area is left for formed light measurement tests with light pipes over varying distances and curvature to simulate a typical application scenario we used off the shelf components a red led emitter and a photodiode based receiver taos for each reading light was directed into the light pipe with the emitter and a voltage value measured from the receiver at the other end the emitter and detector were coupled to the a b c test pieces using custom slots similar to those shown in fig ure and without surface finishing figure two identical leds are embedded with a printed plano concave a and plano convex b for the distance test we compared the light transmittance of lens a beam splitter sends light in two directions c a printed light pipe to a commercially produced sensor response volts sensor response volts commercial pmma optical fiber straight light pipe printed curved light pipe light pipe 80 length mm arc radius mm a b figure light transmittance of printed light pipes compared to commercial optical fiber a and transmit tance of printed light pipes in arcs with increasing curvature radius b model material 5mm light pipe optical fiber made of pmma the printed light pipes consisted of a core cladding and casing configuration support material with distances ranging from 100mm in increments figure magnified view of the material intersections the commercial optical fiber was cut to the same lengths and in printed light pipes mounted in a rigid casing for the curvature test we created a series of light pipes following a arc each with an in crementally larger radius the length of the light pipes was ing flat surface finishing can be performed using power extended on each end to be precisely in total length tools such as a belt sander and buffing wheel curved sur faces require hand sanding and are more time intensive tex figure shows that printed light pipes currently suffer tured surfaces or surfaces with small micro facets are diffi from increasing light loss with distance adding curvature to cult to perform finishing on without specialized equipment a light pipe further increases light loss when compared to a internal surfaces can generally not be reached to perform straight light pipe of equal length figure we believe manual surface finishing but applying additional layers of this limited performance is due to two factors firstly the uniform material cover can improve the quality of the sur contours where two different materials intersect are currently face figure not perfectly smooth figure microscopic unevenness on internal surfaces result in light being lost or redirected clarity in unpredictable ways however we found the consistency based on our experiences with the objet we have between ten identical straight prints to be high with found that the clarity of printed optical elements currently a standard deviation of 0822v when measuring transmit depends on several factors model thickness thicker models tance as previously described secondly although we know tend to lose optical clarity and appear increasingly cloudy the refractive index of the core material no information is print direction the greatest clarity is seen when looking available for the refractive index of the cladding material perpendicular to the printed layers looking parallel to the because we use support material that is not intended as an printed layers appears blurrier uv exposure overexposure optical material its performance as cladding is understand to uv light during the curing process can cause a loss in opti ably low we can assume that the refractive index of each cal clarity making the model appear yellow in color surface material is close enough to restrict the angle of incidence to quality greater clarity can be achieved with extra sanding a relatively small value and thereby limit internal reflection steps during the finishing process although the current materials and printing process we use hollow areas are not optimized for optical performance promising mate as printed objects are built up layer by layer hollow areas rials science research has demonstrated that photopolymer typically require support material inside the hollow print waveguides can be fabricated in both planar and non ing completely enclosed hollow areas can seal the support planar forms with minimal light loss we are therefore material inside with no means for it to be extracted com optimistic about the possibilities for fabricating light pipes pletely enclosed hollow areas of air can be fabricated using in the next generation of optically optimized printers it jet deposition based machines by disabling support material is worth noting that many commercial display technologies and using internal geometry with minimal step over in the routinely encounter significant light loss e g resistive touch vertical axis this requirement for self supporting geometry screens reduce display light transmission by up to limits the design space when creating hollow areas in the ory internal geometry can be combined with arbitrary exter surface finishing nal geometry complete with overhangs and support material in current generation printers unfinished surfaces appear in practice the printer we made use of did not allow se smooth and transparent to the naked eye but not optically lective use of support material it must be either enabled or clear unfinished surfaces can be used when some amount disabled for each print this restriction however can be re of light scattering is acceptable however to maximize light solved with an improved software interface to the printer passing to from the interior of a printed optical element surface finishing should be performed i e sanding and buff despite these limitations by demonstrating the capabilities we present a multimodal data set for the analysis of human affective states the electroencephalogram eeg and peripheral physiological signals of participants were recorded as each watched one minute long excerpts of music videos participants rated each video in terms of the levels of arousal valence like dislike dominance and familiarity for of the participants frontal face video was also recorded a novel method for stimuli selection is proposed using retrieval by affective tags from the last fm website video highlight detection and an online assessment tool an extensive analysis of the participants ratings during the experiment is presented correlates between the eeg signal frequencies and the participants ratings are investigated methods and results are presented for single trial classification of arousal valence and like dislike ratings using the modalities of eeg peripheral physiological signals and multimedia content analysis finally decision fusion of the classification results from different modalities is performed the data set is made publicly available and we encourage other researchers to use it for testing their own affective state estimation methods index terms emotion classification eeg physiological signals signal processing pattern classification affective computing  introduction e motion is a psycho physiological process triggered by conscious and or unconscious perception of an object or situation and is often associated with mood tempera information in deciding upon proper actions to execute the goal of affective computing is to fill this gap by detecting emotional cues occurring during human computer interac ment personality and disposition and motivation emo tion and synthesizing emotional responses tions play an important role in human communication and characterizing multimedia content with relevant reli can be expressed either verbally through emotional voca able and discriminating tags is vital for multimedia bulary or by expressing nonverbal cues such as intonation information retrieval affective characteristics of multi of voice facial expressions and gestures most of the media are important features for describing multimedia contemporary human computer interaction hci systems content and can be presented by such emotional tags are deficient in interpreting this information and suffer implicit affective tagging refers to the effortless generation from the lack of emotional intelligence in other words they of subjective and or emotional tags implicit tagging of are unable to identify human emotional states and use this videos using affective information can help recommenda tion and retrieval systems to improve their performance the current data set is recorded with the goal of s koelstra and i patras are with the school of electronic engineering and creating an adaptive music video recommendation system computer science queen mary university of london mile end road in our proposed music video recommendation system a london united kingdom user bodily responses will be translated to emotions the e mail sander koelstra i patras eecs qmul ac uk c mu hl and a nijholt are with the human media interaction group emotions of a user while watching music video clips will university of twente postbus ae enschede the netherlands help the recommender system to first understand the user e mail muehlc ewi utwente nl a nijholt utwente nl taste and then to recommend a music clip which matches m soleymani and t pun are with the computer science department the user current emotion university of geneva battelle campus building a route de drize the database presented explores the possibility of carouge geneva ch switzerland e mail mohammad soleymani thierry pun unige ch classifying emotion dimensions induced by showing music j s lee is with the school of integrated technology yonsei university videos to different users to the best of our knowledge the korea e mail jong seok lee yonsei ac kr responses to this stimuli music video clips have never a yazdani and t ebrahimi are with the multimedia signal processing been explored before and the research in this field was group institute of electrical engineering iel ecole polytechnique fe de rale de lausanne epfl station lausanne ch switzer mainly focused on images music or nonmusic video land e mail ashkan yazdani touradj ebrahimi epfl ch segments in an adaptive music video recommender manuscript received nov revised apr accepted apr an emotion recognizer trained by physiological responses to published online june content of a similar nature music videos are better able to recommended for acceptance by b schuller e douglas cowie and a batliner fulfill its goal for information on obtaining reprints of this article please send e mail to various discrete categorizations of emotions have been taffc computer org and reference ieeecs log number taffcsi proposed such as the six basic emotions proposed by digital object identifier no 1109 t affc ekman et al and the tree structure of emotions proposed  ieee published by the ieee computer society koelstra et al deap a database for emotion analysis using physiological signals by parrott dimensional scales of emotion have also been found in and the mahnob hci database proposed such as plutchik emotion wheel and the consists of two experiments the responses including eeg valence arousal scale by russell in this work we use physiological signals eye gaze audio and facial expressions russell valence arousal scale widely used in research on of people were recorded in the first experiment affect to quantitatively describe emotions in this scale participants watched emotional videos extracted from each emotional state can be placed on a plane with movies and online repositories the second experiment was arousal and valence as the horizontal and vertical axes a tag agreement experiment in which images and short while arousal and valence explain most of the variation in videos with human actions were shown the participants first emotional states a third dimension of dominance can also without a tag and then with a displayed tag the tags were be included in the model arousal can range from either correct or incorrect and participants agreement with inactive e g uninterested bored to active e g alert the displayed tag was assessed excited whereas valence ranges from unpleasant e g sad there has been a large number of published works in the stressed to pleasant e g happy elated dominance domain of emotion recognition from physiological signals ranges from a helpless and weak feeling without control of these studies only a few to an empowered feeling in control of everything for self achieved notable results using video stimuli lisetti and assessment along these scales we use the well known self nasoz used physiological responses to recognize assessment manikins sam emotions in response to movie scenes the movie scenes emotion assessment is often carried out through analysis were selected to elicit six emotions namely sadness of users emotional expressions and or physiological amusement fear anger frustration and surprise they signals emotional expressions refer to any observable achieved a high recognition rate of percent for the recognition of these six emotions however the classifica verbal and nonverbal behavior that communicates emotion tion was based on the analysis of the signals in response to so far most of the studies on emotion assessment have preselected segments in the shown video known to be focused on the analysis of facial expressions and speech to related to highly emotional events determine a person emotional state physiological signals some efforts have been made toward implicit affective are also known to include emotional information that can be tagging of multimedia content kierkels et al proposed used for emotion assessment but they have received less a method for personalized affective tagging of multimedia attention they comprise the signals originating from the using peripheral physiological signals valence and arousal central nervous system cns and the peripheral nervous levels of participants emotions when watching videos were system pns computed from physiological responses using linear regres recent advances in emotion recognition have motivated sion quantized arousal and valence levels for a clip the creation of novel databases containing emotional were then mapped to emotion labels this mapping enabled expressions in different modalities these databases mostly the retrieval of video clips based on keyword queries so cover speech visual data or audiovisual data e g far this novel method achieved low precision 13 the visual modality includes facial yazdani et al proposed using a brain computer expressions and or body gestures the audio modality interface bci based on evoked potentials to covers posed or genuine emotional speech in different emotionally tag videos with one of the six ekman basic languages many of the existing visual databases include emotions their system was trained with eight only posed or deliberately expressed emotions participants and then tested on four others they achieved healey recorded one of the first affective a high accuracy on selecting tags however in their physiological data sets she recorded participants driving proposed system a bci only replaces the interface for around the boston area and annotated the data set by the explicit expression of emotional tags i e the method does drivers stress level responses of of the participants not implicitly tag a multimedia item using the participant are publicly available her recordings include electrocar behavioral and psycho physiological responses diogram ecg galvanic skin response gsr recorded in addition to implicit tagging using behavioral cues from hands and feet and electromyogram emg from the multiple studies used multimedia content analysis mca right trapezius muscle and respiration patterns for automated affective tagging of videos hanjalic and xu to the best of our knowledge the only publicly available introduced personalized content delivery as a multimodal emotional databases which include both phy valuable tool in affective indexing and retrieval systems siological responses and facial expressions are the enterface in order to represent affect in video they first selected emotional database and mahnob hci the first video and audio content features based on their relation one was recorded by savran et al this database includes to the valence arousal space then arising emotions were two sets the first set has electroencephalogram eeg estimated in this space by combining these features while peripheral physiological signals functional near infrared valence arousal could be used separately for indexing spectroscopy fnirs and facial videos from five male they combined these values by following their temporal participants the second data set only has fnirs and facial pattern this allowed for determining an affect curve videos from participants of both genders both databases shown to be useful for extracting video highlights in a recorded spontaneous responses to emotional images from movie or sports video the international affective picture system iaps an wang and cheong used audio and video features to extensive review of affective audiovisual databases can be classify basic emotions elicited by movie scenes audio was classified into music speech and environment signals and http www physionet org drivedb these were treated separately to shape an aural affective ieee transactions on affective computing vol no january march feature vector the aural affective vector of each scene was table fused with video based features such as key lighting and database content summary visual excitement to form a scene feature vector finally using the scene feature vectors movie scenes were classified and labeled with emotions soleymani et al proposed a scene affective char acterization using a bayesian framework arousal and valence of each shot were first determined using linear regression then arousal and valence values in addition to content features of each scene were used to classify every scene into three classes namely calm excited positive and excited negative the bayesian framework was able to incorporate the movie genre and the predicted emotion from the last scene or temporal information to improve the classification accuracy there are also various studies on music affective char acterization from acoustic features 34 rhythm tempo mel frequency cepstral coefficients mfcc pitch and zero crossing rate are among common features which have been used to characterize affect in music a pilot study for the current work was presented in 35 in that study six participants eeg and physiological signals were recorded as each watched music videos the participants rated arousal and valence levels and the eeg and physiological signals for each video were classified into low high arousal valence classes in the current work music video clips are used as the visual stimuli to elicit different emotions to this end a the layout of the paper is as follows in section the relatively large set of music video clips was gathered using stimuli selection procedure is described in detail the a novel stimuli selection method a subjective test was then performed to select the most appropriate test material for experiment setup is covered in section section provides each video a one minute highlight was selected automati a statistical analysis of the ratings given by participants cally thirty two participants took part in the experiment during the experiment and a validation of our stimuli and their eeg and peripheral physiological signals were selection method in section correlates between the eeg recorded as they watched the selected music videos frequencies and the participants ratings are presented the participants rated each video in terms of arousal valence method and results of single trial classification are given in like dislike dominance and familiarity for partici section the conclusion of this work follows in section pants frontal face video was also recorded this paper aims at introducing this publicly stimuli selection database the database contains all recorded signal data frontal face video for a subset of the participants and the stimuli used in the experiment were selected in several subjective ratings from the participants also included is the steps first we selected initial stimuli half of which subjective ratings from the initial online subjective annota were chosen semi automatically and the rest manually tion and the list of videos used due to licensing issues then a one minute highlight part was determined for each we are not able to include the actual videos but youtube stimulus finally through a web based subjective assess links are included table gives an overview of the ment experiment final stimuli were selected each of database contents these steps is explained below to the best of our knowledge this database has the initial stimuli selection highest number of participants in publicly available databases for analysis of spontaneous emotions from eliciting emotional reactions from test participants is a physiological signals in addition it is the only database difficult task and selecting the most effective stimulus that uses music videos as emotional stimuli materials is crucial we propose here a semi automated we present an extensive statistical analysis of the method for stimulus selection with the goal of minimizing participant ratings and of the correlates between the the bias arising from the manual stimuli selection eeg signals and the ratings preliminary single trial sixty of the initially selected stimuli were selected classification results of eeg peripheral physiological using the last music enthusiast website last fm allows signals and mca are presented and compared finally a users to track their music listening habits and receive fusion algorithm is utilized to combine the results of each recommendations for new music and events additionally modality and arrive at a more robust decision it allows the users to assign tags to individual songs thus http www eecs qmul ac uk mmv data sets deap http www last fm koelstra et al deap a database for emotion analysis using physiological signals creating a folksonomy of tags many of the tags carry the linear weights were computed by means of a emotional meanings such as depressing or aggressive relevance vector machine rvm from the rvm toolbox last fm offers an api allowing one to retrieve tags and provided by tipping the rvm is able to reject tagged songs uninformative features during its training hence no further a list of emotional keywords was taken from and feature selection was used for arousal and valence expanded to include inflections and synonyms yielding determination keywords next for each keyword corresponding tags the music videos were then segmented into one minute were found in the last fm database for each found segments with seconds overlap between segments affective tag the songs most often labeled with this tag content features were extracted and provided the input were selected this resulted in a total of 084 songs for the regressors the emotional highlight score of the the valence arousal space can be subdivided into four ith segment ei was computed using the following equation quadrants namely low arousal low valence lalv low q arousal high valence lahv high arousal low valence ei  halv and high arousal high valence hahv in order to ensure diversity of induced emotions from the the arousal ai and valence vi were centered therefore 084 songs were selected manually for each quadrant a smaller emotional highlight score ei is closer to the according to the following criteria neutral state for each video the one minute long segment does the tag accurately reflect the emotional content with the highest emotional highlight score was chosen to be examples of songs subjectively rejected according to this extracted for the experiment for a few clips the automatic criterion include songs that are tagged merely because the affective highlight detection was manually overridden this song title or artist name corresponds to the tag also in was done only for songs with segments that are particularly some cases the lyrics may correspond to the tag but the characteristic of the song well known to the public and actual emotional content of the song is entirely different most likely to elicit emotional reactions in these cases the e g happy songs about sad topics one minute highlight was selected so that these segments is a music video available for the song music videos were included for the songs were automatically retrieved from youtube given the one minute music video segments the corrected manually where necessary however many songs final selection of videos used in the experiment was do not have a music video made on the basis of subjective ratings by volunteers as is the song appropriate for use in the experiment since described in the next section our test participants were mostly european students we selected those songs most likely to elicit emotions for this online subjective annotation target demographic therefore mainly european or north from the initial collection of stimulus videos the final american artists were selected test video clips were chosen by using a web based in addition to the songs selected using the method subjective emotion assessment interface participants described above stimulus videos were selected manu watched music videos and rated them on a discrete point ally with videos selected for each of the quadrants in the scale for valence arousal and dominance a screenshot of arousal valence space the goal here was to select those the interface is shown in fig each participant watched as videos expected to induce the most clear emotional many videos as he she wanted and was able to end the reactions for each of the quadrants the combination of rating at any time the order of the clips was randomized manual selection and selection using affective tags pro but preference was given to the clips rated by the least duced a list of candidate stimulus videos number of participants this ensured a similar number of ratings for each video assessments per video were detection of one minute highlights collected it was ensured that participants never saw the for each of the initially selected music videos a one same video twice minute segment for use in the experiment was extracted in after all of the videos were rated by at least order to extract a segment with maximum emotional volunteers each the final videos for use in the content an affective highlighting algorithm is proposed experiment were selected to maximize the strength of soleymani et al used a linear regression method to elicited emotions we selected those videos that had the calculate arousal for each shot of in movies in their method strongest volunteer ratings and at the same time a small the arousal and valence of shots were computed using a variation to this end for each video x we calculated a linear regression on the content based features informative normalized arousal and valence score by taking the mean features for arousal estimation include loudness and energy rating divided by the standard deviation x x of the audio signals motion component visual excitement then for each quadrant in the normalized valence and shot duration the same approach was used to arousal space we selected the videos that lie closest to compute valence there are other content features such as the extreme corner of the quadrant fig shows the score color variance and key lighting that have been shown to be for the ratings of each video and the selected videos correlated with valence the detailed description of the highlighted in green the video whose rating was closest to content features used in this work is given in section the extreme corner of each quadrant is mentioned explicitly in order to find the best weights for arousal and valence of the selected videos were selected via last fm estimation using regression the regressors were trained on affective tags indicating that useful stimuli can be selected all shots in annotated movies in the data set presented in via this method ieee transactions on affective computing vol no january march fig x x value for the ratings of each video in the online assessment videos selected for use in the experiment are highlighted in green for each quadrant the most extreme video is detailed with the song title and a screenshot from the video experiment prior to the experiment each participant signed a consent form and filled out a questionnaire next they fig screenshot of the web interface for subjective emotion were given a set of instructions to read informing them of assessment the experiment protocol and the meaning of different scales used for self assessment an experimenter was also present experiment setup there to answer any questions when the instructions were materials and setup clear to the participant he she was led into the experiment the experiments were performed in two laboratory room after the sensors were placed and their signals environments with controlled illumination eeg and checked the participants performed a practice trial to peripheral physiological signals were recorded using a familiarize themselves with the system in this unrecorded biosemi activetwo on a dedicated recording pc trial a short video was shown followed by a self pentium ghz stimuli were presented using a assessment by the participant next the experimenter dedicated stimulus pc pentium ghz that sent started the physiological signals recording and left the synchronization markers directly to the recording pc for room after which the participant started the experiment by presentation of the stimuli and recording the users ratings pressing a key on the keyboard the presentation software by neurobehavioral the experiment started with a two minute baseline was used the music videos were presented on a inch recording during which a fixation cross was displayed to screen 280 024 hz and in order to minimize eye movements all video stimuli were displayed at the participant who was asked to relax during this period resolution filling approximately of the screen subjects then the videos were presented in trials each were seated approximately meter from the screen stereo consisting of the following steps philips speakers were used and the music volume was set at a second screen displaying the current trial a relatively loud level however each participant was asked number to inform the participants of their progress before the experiment whether the volume was comfortable a second baseline recording fixation cross and it was adjusted when necessary the minute display of the music video eeg was recorded at a sampling rate of hz using active agcl electrodes placed according to the interna tional 10 system thirteen peripheral physiological signals which will be further discussed in section were also recorded additionally for the first of the participants frontal face video was recorded in dv quality using a sony dcr consumer grade camcor der the face video was not used in the experiments in this paper but is made publicly available along with the rest of the data fig illustrates the electrode placement for acquisition of peripheral physiological signals experiment protocol thirty two healthy participants percent females aged between and mean age participated in the fig placement of peripheral physiological sensors four electrodes were used to record eog and four for emg zygomaticus major and http www biosemi com trapezius muscles in addition gsr blood volume pressure bvp http www neurobs com temperature and respiration were measured koelstra et al deap a database for emotion analysis using physiological signals fig a participant shortly before the experiment self assessment for arousal valence liking and dominance after trials the participants took a short break during fig the mean locations of the stimuli on the arousal valence plane the break they were offered some cookies and noncaffei av plane for the four conditions lalv halv lahv and hahv liking is encoded by color dark red is low liking and bright yellow is high nated nonalcoholic beverages the experimenter then liking dominance is encoded by symbol size small symbols stand for checked the quality of the signals and the electrodes low dominance and big for high dominance placement and the participants were asked to continue the second half of the test fig shows a participant shortly the valence scale ranges from unhappy or sad to happy before the start of the experiment or joyful the arousal scale ranges from calm or bored to stimulated or excited the dominance scale ranges from participant self assessment submissive or without control to dominant or in at the end of each trial participants performed a self control empowered a fourth scale asks for participants assessment of their levels of arousal valence liking and personal liking of the video this last scale should not be dominance self assessment manikins were used to confused with the valence scale this measure inquires visualize the scales see fig for the liking scale thumbs about the participants tastes not their feelings for down thumbs up symbols were used the manikins were example it is possible to like videos that make one feel displayed in the middle of the screen with the numbers sad or angry finally after the experiment participants printed below participants moved the mouse strictly were asked to rate their familiarity with each of the songs horizontally just below the numbers and clicked to indicate on a scale of never heard it before the experiment to their self assessment level participants were informed they knew the song very well could click anywhere directly below or in between the numbers making the self assessment a continuous scale analysis of subjective ratings in this section we describe the effect the affective stimula tion had on the subjective ratings obtained from the participants first we will provide descriptive statistics for the recorded ratings of liking valence arousal dominance and familiarity second we will discuss the covariation of the different ratings with each other stimuli were selected to induce emotions in the four quadrants of the valence arousal space lalv halv lahv and hahv the stimuli from these four affect elicitation conditions generally resulted in the elicitation of the target emotion aimed for when the stimuli were selected ensuring that large parts of the arousal valence plane are covered see fig wilcoxon signed rank tests showed that low and high arousal stimuli induced different valence ratings p 0001 and p 00001 similarly low and high valenced stimuli induced different arousal ratings p 001 and p 0001 the emotion elicitation specifically worked well for the high arousing conditions yielding relative extreme valence ratings for the respective stimuli the stimuli in the low fig images used for self assessment from the top valence sam arousing conditions were less successful in the elicitation of arousal sam dominance sam and liking strong valence responses furthermore some stimuli of the 24 ieee transactions on affective computing vol no january march fig the distribution of the participants subjective ratings per scale l general rating v valence a arousal d dominance and f familiarity for the four affect elicitation conditions lalv halv lahv and hahv lahv condition induced higher arousal than expected on fatigue we observed high positive correlations between the basis of the online study interestingly this results in a c liking and valence and between dominance and valence shape of the stimuli on the valence arousal plane also seemingly without implying any causality people liked observed in the well validated ratings for the iaps and music which gave them a positive feeling and or a feeling of the international affective digital sounds system iads empowerment medium positive correlations were observed indicating the general difficulty in inducing emotions with between arousal and dominance and between arousal and strong valence but low arousal the distribution of the liking familiarity correlated moderately positive with liking individual ratings per conditions see fig shows a large and valence as already observed above the scales of variance within conditions resulting from between stimulus valence and arousal are not independent but their positive and participant variations possibly associated with stimulus correlation is rather low suggesting that participants were characteristics or interindividual differences in music taste able to differentiate between these two important concepts general mood or scale interpretation however the sig stimulus order had only a small effect on liking and nificant differences between the conditions in terms of the dominance ratings and no significant relationship with the ratings of valence and arousal reflect the successful elicitation other ratings suggesting that effects of habituation and of the targeted affective states see table 2 fatigue were kept to an acceptable minimum the distribution of ratings for the different scales and in summary the affect elicitation was in general conditions suggests a complex relationship between ratings successful though the low valence conditions were par tially biased by moderate valence responses and higher we explored the mean intercorrelation of the different scales arousal high scale intercorrelations observed are limited to over participants see table as they might be indicative of the scale of valence with those of liking and dominance and possible confounds or unwanted effects of habituation or might be expected in the context of musical emotions the rest of the scale intercorrelations are small or medium in table 2 strength indicating that the scale concepts were well the mean values and standard deviations of the distinguished by the participants different ratings of liking 9 valence 9 arousal 9 dominance 9 and familiarity for each affect elicitation condition correlates of eeg and ratings for the investigation of the correlates of the subjective ratings with the eeg signals the eeg data were common average referenced downsampled to hz and high pass filtered with a 2 hz cutoff frequency using the toolbox we removed eye artifacts with a blind source separation technique then the signals from the last seconds of each trial video were extracted for further analysis to correct for stimulus unrelated variations in power over time table the eeg signal from the seconds before each video was the means of the subject wise intercorrelations between the scales of valence arousal liking dominance familiarity and extracted as baseline the order of the presentation i e time for all stimuli the frequency power of trials and baselines between 3 and hz was extracted with welch method with windows of samples the baseline power was then subtracted from the trial power yielding the change of power relative to the pre stimulus period these changes of power were averaged over the frequency bands of theta 3 hz alpha 13 hz beta hz and gamma hz for the correlation statistic we computed the spearman correlated coefficients significant correlations p 05 according to fisher method are http sccn ucsd edu eeglab indicated by stars http www cs tut fi gomezher projects eeg aar htm koelstra et al deap a database for emotion analysis using physiological signals fig the mean correlations over all participants of the valence arousal and general ratings with the power in the broad frequency bands of theta hz alpha 13 hz beta hz and gamma hz the highlighted sensors correlate significantly p 05 with the ratings between the power changes and the subjective ratings and for higher arousal matches the findings from our earlier computed the p values for the left positive and right tailed pilot study 35 and an inverse relationship between alpha negative correlation tests this was done for each partici power and the general level of arousal has been reported pant separately and assuming independence the before 42 resulting p values per correlation direction positive valence showed the strongest correlations with eeg negative frequency band and electrode were then com signals and correlates were found in all analyzed frequency bined to one p value via fisher method bands in the low frequencies theta and alpha an increase of fig shows the average correlations with significantly valence led to an increase of power this is consistent with p 05 correlating electrodes highlighted below we will the findings in the pilot study the location of these effects report and discuss only those effects that were significant over occipital regions thus over visual cortices might with p 01 a comprehensive list of the effects can be indicate a relative deactivation or top down inhibition of found in table these due to participants focusing on the pleasurable sound for arousal we found negative correlations in the theta for the beta frequency band we found a central alpha and gamma bands the central alpha power decrease decrease also observed in the pilot and an occipital and table the electrodes for which the correlations with the scale were significant p 01 p 001 the most negative r and the most positive correlation r also shown is the mean of the subject wise correlations r ieee transactions on affective computing vol 3 no january march right temporal increase of power increased beta power over to give an indication of how unbalanced the classes are the right temporal sites was associated with positive emotional mean and standard deviation over participants of the self induction and external stimulation in similarly percentage of videos belonging to the high class per rating onton and makeig have reported a positive correlation scale are as follows arousal percent 15 percent valence of valence and high frequency power including beta and percent 9 percent and liking percent 12 percent gamma bands emanating from anterior temporal cerebral in light of this issue in order to reliably report results we sources correspondingly we observed a highly significant report the core which is commonly employed in increase of left and especially right temporal gamma power information retrieval and takes the class balance into however it should be mentioned that the emg muscle account contrary to the mere classification rate in addition activity is also prominent in the high frequencies especially we use a naive bayes classifier a simple and generalizable over anterior and temporal electrodes classifier which is able to deal with unbalanced classes in the liking correlates were found in all analyzed small training sets frequency bands for theta and alpha power we observed first the features for the given modality are extracted for increases over left fronto central cortices liking might be each trial video then for each participant the measure associated with an approach motivation however the was used to evaluate the performance of emotion classifica observation of an increase of left alpha power for a higher tion in a leave one out cross validation scheme at each step liking conflicts with findings of a left frontal activation of the cross validation one video was used as the test set leading to lower alpha over this region often reported for and the rest were used as training set we use fisher linear emotions associated with approach motivations this discriminant j for feature selection contradiction might be reconciled when taking into account that it is quite possible that some disliked pieces induced an j 2 j jf angry feeling due to having to listen to them or simply due  to the content of the lyrics which is also related to an where and are the mean and standard deviation for approach motivation and might hence result in a left ward feature f we calculate this criterion for each feature and decrease of alpha the right temporal increases found in the then apply a threshold to select the maximally discriminat beta and gamma bands are similar to those observed for ing ones this threshold was empirically determined at 3 valence and the same caution should be applied in general a gaussian naive bayes classifier was used to classify the the distribution of valence and liking correlations shown in test set as low high arousal valence or liking fig seem very similar which might be a result of the high the naive bayes classifier g assumes independence of intercorrelations of the scales discussed above the features and is given by summarizing we can state that the correlations observed partially concur with observations made in the pilot study y n and in other studies exploring the neuro physiological fn  argmax pc c pfi fi jc c c correlates of affective states they might therefore be taken as valid indicators of emotional states in the context of where f is the set of features and c the classes pfi multimodal musical stimulation however the mean fi jc c is estimated by assuming gaussian distributions correlations are seldom larger than which might be of the features and modeling these from the training set due to high interparticipant variability in terms of brain the following section explains the feature extraction activations as individual correlations between were steps for the eeg and peripheral physiological signals observed for a given scale correlation at the same electrode section 2 presents the features used in mca classification frequency combination the presence of this high inter in section 3 we explain the method used for decision participant variability justifies a participant specific classi fusion of the results finally section presents the fication approach as we employ it rather than a single classification results classifier for all participants eeg and peripheral physiological features most of the current theories of emotion agree that single trial classification physiological activity is an important component of an in this section we present the methodology and results of emotion for instance several studies have demonstrated single trial classification of the videos three different the existence of specific physiological patterns associated modalities were used for classification namely eeg with basic emotions signals peripheral physiological signals and mca condi the following peripheral nervous system signals were tions for all modalities were kept equal and only the feature recorded gsr respiration amplitude skin temperature extraction step varies electrocardiogram blood volume by plethysmograph elec three different binary classification problems were tromyograms of zygomaticus and trapezius muscles and posed the classification of low high arousal low high electrooculogram eog gsr provides a measure of the valence and low high liking to this end the participants resistance of the skin by positioning two electrodes on the ratings during the experiment are used as the ground truth distal phalanges of the middle and index fingers this the ratings for each of these scales are thresholded into two resistance decreases due to an increase of perspiration which classes low and high on the 9 point rating scales the usually occurs when one is experiencing emotions such as threshold was simply placed in the middle note that for stress or surprise moreover lang et al discovered that some subjects and scales this leads to unbalanced classes the mean value of the gsr is related to the level of arousal koelstra et al deap a database for emotion analysis using physiological signals 27 a plethysmograph measures blood volume in the table participant thumb this measurement can also be used features extracted from eeg and physiological signals to compute the heart rate hr by identification of local maxima i e heart beats interbeat periods and heart rate variability hrv blood pressure and hrv correlate with emotions since stress can increase blood pressure pleasant ness of stimuli can increase peak heart rate response 20 in addition to the hr and hrv features spectral features derived from hrv were shown to be a useful feature in emotion assessment skin temperature and respiration were recorded since they vary with different emotional states slow respiration is linked to relaxation while irregular rhythm quick variations and cessation of respiration correspond to more aroused emotions like anger or fear regarding the emg signals the trapezius muscle neck activity was recorded to investigate possible head move ments during music listening the activity of the zygoma ticus major was also monitored since this muscle is activated when the participant laughs or smiles most of the power in the spectrum of an emg during muscle contraction is in the frequency range between and hz thus the muscle activity features were obtained from the energy of emg signals in this frequency range for different muscles the rate of eye blinking is another feature which is correlated with anxiety eye blinking affects the eog signal and results in easily detectable peaks in that signal for further reading on psychophysiology of emotion we refer the reader to all the physiological responses were recorded at a hz sampling rate and later downsampled to hz to reduce prcoessing time the trend of the ecg and gsr signals was removed by subtracting the temporal low frequency drift hsv by the standard deviation of the values v in hsv the low frequency drift was computed by smoothing the color variance was obtained in the cie luv color space by signals on each ecg and gsr channels with a points computing the determinant of the covariance matrix of l moving average u and v in total features were extracted from peripheral hanjalic and xu 29 showed the relationship between physiological responses based on the proposed features in video rhythm and affect the average shot change rate and the literature 26 54 see also table 5 shot length variance were extracted to characterize video from the eeg signals power spectral features were rhythm fast moving scenes or objects movements in extracted the logarithms of the spectral power from theta consecutive frames are also an effective factor for evoking 4 hz slow alpha 10 hz alpha 12 hz beta excitement to measure this factor the motion component 12 hz and gamma hz bands were extracted from was defined as the amount of motion in consecutive frames all electrodes as features in addition to power spectral computed by accumulating magnitudes of motion vectors features the difference between the spectral power of all the for all b and p frames symmetrical pairs of electrodes on the right and left colors and their proportions are important parameters to hemisphere was extracted to measure the possible asym elicit emotions a 20 bin color histogram of hue and metry in the brain activities due to emotional stimuli the lightness values in the hsv space was computed for each total number of eeg features of a trial for electrodes is i frame and subsequently averaged over all frames the table 5 summarizes the list of features extracted from resulting bin averages were used as video content based features the median of the l value in hsl space was the physiological signals computed to obtain the median lightness of a frame 2 mca features finally visual cues representing shadow proportion music videos were encoded into the mpeg format to visual excitement grayness and details were also deter extract motion vectors and i frames for further feature mined according to the definition given in extraction the video stream has been segmented at the shot sound also has an important impact on affect for level using the method proposed in example loudness of speech energy is related to evoked from a movie director point of view lighting key arousal while rhythm and average pitch in speech signals and color variance are important tools to evoke are related to valence the audio channels of the videos emotions we therefore extracted lighting key from frames were extracted and encoded into mono mpeg 3 format at a in the hsv space by multiplying the average value v in sampling rate of khz all audio signals were normalized ieee transactions on affective computing vol 3 no 1 january march table table 7 low level features extracted from audio signals average accuracies acc and scores average of score for each class over participants stars indicate whether the score distribution over subjects is significantly higher than 5 according to an independent one sample t test p 01 p 05 for comparison expected results are given for classification based on random voting voting according to the majority class and voting with the ratio of the classes p to the same amplitude range before further processing a satisfy m 1 and m m 1 determine how much total of low level audio features were determined for each each modality contributes to the final decision and of the audio signals these features listed in table are represents the modality reliability commonly used in audio and speech processing and audio we adopt a simple method where the weighting factors classification 60 mfcc formants and the pitch of are fixed once their optimal values are determined from the audio signals were extracted using the praat software training data the optimal weight values are estimated by package exhaustively searching the regular grid space where each weight is incremented from to 1 by 01 and the weighting 3 fusion of single modality results values producing the best classification results for the fusion of the multiple modalities explained above aims at training data are selected improving classification results by exploiting the comple mentary nature of different modalities in general ap 4 results and discussion proaches for modality fusion can be classified into two table 7 shows the average accuracies and scores broad categories namely feature fusion or early integra average score for both classes over participants for tion and decision fusion or late integration in each modality and each rating scale we compare the feature fusion the features extracted from signals of results to the expected values analytically determined of different modalities are concatenated to form a composite voting randomly voting according to the majority class in feature vector and then input to a recognizer in decision the training data and voting for each class with the fusion on the other hand each modality is processed probability of its occurrence in the training data for independently by the corresponding classifier and the determining the expected values of majority voting and outputs of the classifiers are combined to yield the final class ratio voting we used the class ratio of each result each approach has its own advantages for participant feedback during the experiment these results example implementing a feature fusion based system is are slightly too high as in reality the class ratio would straightforward while a decision fusion based system can have to be estimated from the training set in each fold of be constructed by using existing unimodal classification the leave one out cross validation systems moreover feature fusion can consider synchro voting according to the class ratio gives an expected nous characteristics of the involved modalities whereas score of 5 for each participant to test for significance decision fusion allows us to model asynchronous char an independent one sample t test was performed compar acteristics of the modalities flexibly ing the distribution over participants to the 5 baseline an important advantage of decision fusion over feature as can be seen from the table eight out of the nine scores fusion is that since each of the signals is processed and obtained are significantly better than the class ratio baseline classified independently in decision fusion it is relatively the exception is the classification of liking using eeg easy to employ an optimal weighting scheme to adjust the signals p 0 when voting according to the majority relative amount of the contribution of each modality to the class relatively high accuracies are achieved due to the final decision according to the reliability of the modality imbalanced classes however this voting scheme also has the weighting scheme used in our work can be formalized the lowest scores as follows for a given test datum x the classification result overall classification using the mca features fares of the fusion system is significantly better than eeg and peripheral p 0 0001 for both while eeg and peripheral scores are not ym m significantly different p 0 41 tested using a two sided c arg max pi xj m  repeated samples t test over the concatenated results from i each rating scale and participant where m is the number of modalities considered for fusion the modalities can be seen to perform moderately m is the classifier for the mth modality and pi xj m  is its complementarily where eeg scores best for arousal output for the ith class the weighting factors m which peripheral for valence and mca for liking of the different koelstra et al deap a database for emotion analysis using physiological signals 29 table scores for fusion of the best two modalities and all three modalities using the equal weights and optimal weights scheme for comparison the score for the best single modality is also given rating scales valence classification performed best fol swiss national foundation for scientific research and the lowed by liking and last arousal nccr interactive multimodal information management table 8 gives the results of multimodal fusion two the authors also thank sebastian schmiedeke and fusion methods were employed the method described in pascal kelm at the technische universita t berlin for section 3 and the basic method where each modality is performing the shot boundary detection on this data set weighed equally the best results were obtained when only the two best performing modalities were considered because of the lack of reliable gps signals inside a building this paper addresses reliable and accurate indoor localiza researchers have explored the use of wifi beacons tion using inertial sensors commonly found on commodity magnetometer vision or ultrasound to name smartphones we believe indoor positioning is an impor a few for coarse level indoor positioning but few of them tant primitive that can enable many ubiquitous computing have achieved the needed meter level accuracy for the above applications to tackle the challenges of drifting in estima mentioned applications besides wifi and other infrastruc tion sensitivity to phone position as well as variability in ture assisted approaches rely on installation of beacons user walking profiles we have developed algorithms for reliable detection of steps and heading directions and accurate other approaches rely on inertial sensors or imu short for estimation and personalization of step length we ve built inertial measurement unit to track a user by continuously es an end to end localization system integrating these modules timating displacement from a known location the so called and an indoor floor map without the need for infrastruc pedestrian navigation system pns is an inture assistance we demonstrated for the first time a meter stance of such a dead reckoning approach without the need level indoor positioning system that is infrastructure free for infrastructure assistance while most of these existing phone position independent user adaptive and easy to de pns approaches rely on a dedicated sensor device on the ploy we have conducted extensive experiments on users user body for tracking few has exploited the now widely with smartphone devices with over subjects walking over available smartphone with inertial sensors e g acceleroman aggregate distance of over kilometers evaluation re eter gyro for indoor positioning with acceptable accuracy sults showed our system can achieve a mean accuracy of for the in hand case and for the in pocket case in a in this paper we developed a practical indoor localization testing area system which relies on smartphone sensors only to the best of our knowledge this is the first system that is able to re author keywords liably provide the meter level positioning accuracy for com indoor localization inertial tracking pedestrian model mon smartphone users without any infrastructure assistance specifically the contributions of this work are fourfold acm classification keywords c communication networking and information technol we designed a reliable algorithm for step detection the ogy mobile computing support service algorithm is robust to random bouncing motions whether the user is walking or not and to variations in position and general terms orientation of the device on the user body algorithms design experimentation measurement we developed a personalization algorithm that adapts the introduction stride length estimation to each user the algorithm pro indoor navigation is an important enabling technology for vides an adaptive estimate at every step and is position applications such as finding a conference room in an office and orientation free building safety egress during an emergency or targeted re tail advertisement in a shopping mall while gps or cell we designed a heading inference algorithm that deter signals are commonly used for navigation in an outdoor en mines the heading direction of the user at each step vironment robust and accurate indoor positioning remains we built an end to end system for indoor positioning us as an unsolved problem ing smartphones and demonstrated that the system can achieve reliable meter level accuracy permission to make digital or hard copies of all or part of this work for in the rest of this paper we first describe the challenges of in personal or classroom use is granted without fee provided that copies are door localization using smartphone inertial sensors discuss not made or distributed for profit or commercial advantage and that copies related work introduce the overall architecture of our indoor bear this notice and the full citation on the first page to copy otherwise or positioning system and then present algorithms for step de republish to post on servers or to redistribute to lists requires prior specific permission and or a fee tection heading inference and personalization of step length ubicomp sep sep pittsburgh usa models we evaluate the performance of each algorithmic copyright acm 421 module and the end to end system the paper ends with dis computes both the receiver as well as the ap positions along cussions on the system and future work with parameters of the propagation model by solving a set of simultaneous nonlinear equations but this can suffer from challenges the convergence problem during the nonlinear equation solv in a typical pns system the position displacement is de ing as well as the poor signal quality termined by aggregating individual steps thus the key is wifi fingerprinting does not require prior knowledge about to accurately and reliably detect step boundary and estimate the associated step length for every step doing this on com aps and a propagation model instead it builds a rssi fin modity smartphones with noisy inertial sensor readings faces gerprint map and looks up the associated location by match significant challenges ing the signature with the fingerprint map the method can be sensitive to device heterogeneity and requires build a typical method to detect step boundaries is to analyze the ing the fingerprint map other infrastructure based localiza phone inertial sensor readings and thus determine the user tion solutions have also been proposed with some walking motion there are several problems which make re limitations due to the limited coverage of infrastructure or liable step detection hard during walking the position of heavy deployment and training requirement the phone such as in a hand or pocket can affect the sensor readings in different ways making reliable detection across the work on pns has focused on imu positions more challenging the random bouncing of mo based solutions woodman and harle proposed a dead bile phones caused by putting phone in a pocket switching reckoning approach using a foot mounted imu device for from left to right hand operating on touch screen or taking data collection klingbeil and wark requires the user to a phone call can generate false positives in the detection carry a sensor board at a fixed orientation the work of leverages the phone inertial sensors for outdoor localization once a step is detected the system needs to estimate the step and for turn detection indoors length the step length of a person walking can vary quite a pedestrian model including step detection and a step model a lot over time due to speed terrain and other environmen tal constraints a common approach to estimation is to relate is a key component of a pns system step detection typi the sensor readings to step length using a model the step cally relies on peak detection over accelerometer data length model may be trained using collected user data of which is sensitive to noise and other irrelevant motion pro fline but a one size fit all model is going to produce large ducing a high rate of false positives other methods rely on errors for people with more extreme walking characteristics detecting features such as zero cross or flat dynamic time warping was used to align similar waves it is well understood that people with different physical pro or classify activities files such as height weight sex or walking style have dif several step detection solutions require a user to carry a ferent step length even for the same person the step length can vary due to differences in shoes or ground surface what sensing device and attach it to the body such as on the foot is desired is the ability to adapt a trained generic model for making the detection problem easier detection methods a specific user over time as the more personal sensor data is based on phone sensors have also been proposed being accumulated with limited accuracy in step counting and detection a number of step models have been proposed based on ac another challenge for the system is to infer the heading di celeration walking frequency or a combination of rection at each step accurately despite the fact the phone can them personalization of step model has also been con be at any position on the body an overall positioning sys sidered with proposing to automatically learn model pa tem needs to integrate the step length estimate together with rameters using gps readings outdoors the heading information to produce a reliable displacement estimate due to the well known problem in drifting of in overview of the system ertial sensors one needs additional constraints to keep the the indoor localization system is comprised of several mod error accumulation at bay one source of constraints is the ules the step detector step length estimator heading estima indoor floor map which constrains the motion with corridors tor particle filter and a personalization module for adapting and walls a step model to an individual user as in figure the system interacts with a user to get the initial location through user related work input and provides the current position estimate on an in many existing efforts leverage some form of infrastructure door map in addition a sensor module provides imu read assistance for positioning wifi based techniques are com ings to these estimators continuously monly used due to the broad availability of wifi infrastruc ture wifi triangulation makes use of receiving signal strengths distance estimation in the system the position update is from access points aps and a propagation model of the performed at each step every time a new step is reported signal to compute the receiver location however as this by the step detector a new length estimate is generated by requires information about the ap positions and the propa the step length estimator based on a step model triggering gation model can vary significantly indoors the accuracy of the particle filtering the step model can be either an initial the wifi triangulation can be uneven the ez algorithm generic model or a personalized model learned locally from figure the overall system architecture figure peak detection on acceleration waveforms the latest step data of the user to identify a distinct step an example of the peak detection user heading inference the heading estimator fuses data is given in figure two thresholds a and t are used from the compass gyro and accelerometer to ensure high to filter out false peaks caused by acceleration jitters that are reliability and accuracy of user heading inference at each either too small in magnitude or too short in time duration step no matter which position the phone is put into the orientation of the phone itself can be obtained for example however not all the detected peaks correspond to meaning on windows phone via the motion api or computed ful step boundaries hence the false positives of steps acci from the raw sensor data dental bouncing of a phone could produce such a false step the algorithm further reduces false positives using heuristic particle filtering we use the particle filter algorithm a non constraints and dynamic time warping dtw validation parametric form of bayesian estimation commonly used in computer vision and tracking to compute the overall po after analyzing the acceleration magnitudes of over sition the algorithm integrates information from the step real step data points from subjects we applied two heuris length estimator and heading estimator in addition to the tics to reduce erroneous detections maximum time du constraint from the floor map to arrive at the posterior distri ration of a step e g second and minimum max bution of the position it is customized to support on the fly imum changes in acceleration magnitudes during one step personalization of step model which differentiates it from e g and previous algorithms dtw has been widely used as an efficient way to measure reliable step and heading detection the similarity between two discrete waveforms a lower the step detection and heading inferences are two key mod dtw distance denotes a higher similarity for step detec ules in the overall location system as described above once tion considering in normal walking when a user alternates the system is initialized it continuously detects any step of her left and right foot it can be expected similar waveform a user with high reliability and for every step detected it will repeat at every other step based on this observation we estimates the heading direction using a heading inference al designed a dtw validation algorithm to determine whether gorithm a step detected via peaks is a real step the dtw validation works as follows suppose the peak step detection detection yields a series of detected steps sn accelerometer has been commonly used for step detection then for si the algorithm calculates the dtw distance be considering the fact a mobile phone can be at any position tween si and si if it is lower than a given threshold on the user body with time varying orientation change we both si and si are considered as real steps otherwise si use the magnitude of axis accelerometer reading instead of will be temporally labeled as false until si when si has its vertical and horizontal components another chance to be validated after computing the distance between si and si the distance is calculated over nor to recover the true periodicity of the signal the step detec malized accelerometer waveforms to reduce the influence of tion algorithm first uses a low pass fir digital filter with a amplitude in our test cases the distance threshold is set to certain cut off frequency to remove the high frequency noise based on the statistics gathered over step data points and spikes in raw acceleration magnitudes the cut off fre mentioned earlier quency is currently set to based on the statistics of our sample data on walking frequencies of the test subjects and is tunable based on the subject profile user heading inference heading determination is an important component of iner after smoothing the raw acceleration data the algorithm tial sensor based positioning the phone heading may be searches for the peaks and valleys of the waveform in order different from the user heading during walking with the dif ference called phone heading offset as the orientation of a phone can vary during walking reliable user heading infer ence can be an extremely difficult task to the best of our knowledge there has been no reliable solution in this study we address the problem by first considering the case where the phone is put in a user pant pocket and develop a novel heading estimation method we then relax the assumptions made about the pocket case and introduce a more general mechanism for correcting errors in the heading estimation the assumptions about the phone in pant pocket case are as follows assumption the initial phone heading offset is known figure inference point user acceleration top and inferred heading this is reasonable as we need to initialize the relation be direction bottom tween user heading and phone heading so that the user head ing can be tracked based on the sensor tracking of the phone heading direction update heading an example is when a user looks at her phone with the phone pointing in the direction of her walking and then because of the periodicity of the leg movement during walk puts the phone in the pocket ing we identify a point during each step when the relative orientation of the phone to the user body is the same as in assumption during the walking the phone heading is the original state i e when the phone was just put into the relatively stable with the leg movement this is also quite pocket we call this point the inference point as mentioned reasonable as the phone moves with the leg in the pocket earlier at this point the transformation matrix from vp to exhibiting a periodic pattern in heading change vu is the same as the one initially computed we can thus infer the user heading based on the current phone orientation we use the following example to illustrate how the user head and the transformation matrix ing direction is computed let the euler angles yaw pitch roll denotes the user heading the user initially operates on the top of figure shows an example of the inference points the phone at this point the yaw of the phone heading is the at which the acceleration crosses the normal earth gravity same as that of the user heading while the roll of the phone value from peak to valley this corresponds to when the is zero regardless of the pitch we use orientation quater left leg bypasses the right leg during walking the bottom nion to represent the phone orientation so that the so called figure plots the inferred direction computed using the trans gimbal lock problem can be avoided formation matrix one can see at these inference points the inferred direction is consistently close to the ground truth heading direction we labeled initial orientation inference after a user puts the phone into her pocket we need to get heading direction adjustment its orientation quaternion using the following method we now relax the previous assumptions about phone head if the user remains stationary during the phone motion ing and also account for heading biases caused for example ending in the pocket we can directly get the orientation by magnetic interference as we described earlier the par quaternion ticle filter applies the estimated heading direction to move particles a perturbation is usually added to account for sen if not we can get the orientation quaternion at the so sor noise called inference point of the user first step the infer ence point is a point in time with a particular combination likewise we develop an algorithm to automatically adjust of movement characteristics that repeats at every step we the particle direction in the particle filtering process to ac will explain it in detail below count for heading estimation errors at each step the motion direction of a particle i i is computed as assuming the original orientation quaternion of the phone is vh and the orientation quaternion of the phone in the pocket i ie i is vp we have where e is the estimated heading at current step and i the noise following a certain zero mean distribution such as vh q vp a zero mean gaussian we now add an additional correction where q is the transformation matrix since vh is similar term to the particles at the next step computed as to the orientation quaternion of the user vu with the only pn a i difference in pitch they have the same heading direction  k i hence we use q as the transformation matrix from vp to vu n which has no influence on the heading direction inference where n is the number of surviving particles after the cur figure an illustrative example of heading adjustment rent step and k is a scaling factor for how aggressively one figure step data collecting and labeling system wants to adjust the heading direction an example is where a magnetic interference causes the head ing estimate to consistently biased towards a wall leading many particles to hit the wall and perish the surviving par ticles are those whose perturbation noises have likely helped to correct their course leading to their survival thus the correction term computed from these surviving particles can help the particles to steer away from the wall as illustrated in figure adaptive step length estimation there are several barriers to getting an accurate step model that works for everyone all the time it is impractical to ask each user to manually label her step data so that the system can learn her personal step model figure step length vs walking frequency before the system can start to work on the user phone as an alternative a generic step model should be provided at the time of service delivery model represented as lg a f b people step models vary a lot making the generic model less accurate physical profiles such as height and weight affect the model accuracy if we take a closer look at each person data in figure the linear frequency model fits well with differences only even for the same person her step model may change over in the coefficients with this observation the goal of our time due to variations in shoes ground types or health personalization algorithm is to determine the value of the status making model personalization a continuous pro coefficients a and b for each person as described next cess position update and personalization addressing the above challenges we propose a personaliza position update is performed on a per step basis every time tion algorithm that starts with a generic step model and then a step is detected our system will acquire the step length collects the user data on the fly to learn a personal model it based on step model and step frequency and infer the head is designed to adapt to model changes over time ing direction of that step as well both step length and head ing direction are fed into the particle filter module together there are a number of step models proposed in the literature with the map information among which frequency models have been widely used we set up a system to collect over steps from users with before describing the specific algorithm let look at how diverse physical characteristics figure gives a snapshot of the particle filtering works each particle moves according the tool we have implemented to label the ground truth of to the dynamics of the model step by step and is constrained collected steps by the map if a particle hits a wall it will die and a new particle will be generated around another live particle par after we have got all the ground truth data we analyze the ticles whose trajectories align with the observation will have relationship between the walking speed and actual step length a higher probability to live thus the positions of live parti as shown in figure there is a clear trend that the step cles reflect the estimated position of the user length has a linear relation to the walking frequency based on that we choose a frequency model as our generic step we design the step model personalization using the particle filtering over an appropriate state space as we have stated pute the change of heading direction and compare it against above the goal of personalization is to get the pair of the co a given threshold th when the heading direction change efficients a b that best describe the ground truth model of goes above the threshold a turn is reported specifically the the user thus if we associate each particle a different co turn detection is performed in three steps efficient pair we can imagine that over time particles with coefficient pairs closer to ground truth will have higher prob median and mean filtering an n point median filter is abilities to live as a result their coefficient pairs can better first applied to the heading direction to filter out sharp reflect the ground truth coefficient pair i e the user per direction changes not caused by making turns then an sonal model m point mean filter is applied to remove random error of estimated directions in our implementation we set n to the customized particle filtering algorithm perform position and m to update as well as personalization specifically the algorithm updates the following state model turn detection given a filtered direction i for current step i we first compute the mean direction of all the steps x x y p a b detected since last turn then the direction change at step i is computed as where x y represents the coordinate of a particle in a i space p a b represents the coefficient pair i e the step x i i j model associated with the particle we refer to it as the per j sonalization model below if ki k th a turn is detected initially all particles spread around an initial point speci fied by the user during walking the re sampling step re turn initiation and termination once a turn is detected moves the particles with very low weight generates new we can get the exact point in time by searching for a salient particles from the distribution and normalizes the weight direction change point in a small window around the cur of each particle as for the particles representing the model rent step the size of the window is related to the size of coefficients initially they are placed around a b i e the median and mean filter the turn termination point can generic model specifically for a given particle its person likewise be determined by searching for the point when alization model is represented as the heading direction goes back to a stable direction again p a b a a b b once the turn termination is determined the re sampled par ticles adopt the personalization models of those nearby where a and b are noise with a zero mean uniform distri bution accordingly the initial expectation of the personal particle propagation all particles update their coordinates ization model is the generic model according to x t x t l t l t cos  t  t during walking if a particle dies and needs to be re sampled y t y t l t l t sin  t  t the algorithm assigns a new personalization model to it as stated above the basic idea of personalization is to remove where l t and  t are the estimated step length and in any particle model that is not well aligned with the ground ferred heading direction while l t and  t zero mean truth if a particle dies because of inaccuracy in its person gaussian noise on the length and heading respectively ad alization model it will be re sampled with the same person ditionally a heading correction term may be added as dis alization model as the nearby live particle otherwise its per cussed earlier the variances of l t and  t are deter sonalization model remains unchanged this is because be mined by the confidence of step length estimation and head sides the particle model heading direction also affects parti ing inference cle propagation even with a perfect personalization model a particle may still hit the side wall due to heading direction particle correction this step sets the weight for every par error in this case the personalization model should not be ticle if a particle crosses a wall during propagation we set responsible for the particle death and need not to be replaced the weight of that particle to if the particle is in the middle part of the road we increase the weight of the particle this how do we tell when a personalization model is inaccurate is based on the assumption that people tend to walk in the in our implementation we use turn detection to determine middle of a corridor and cannot walk across a wall whether to replace the model specifically while walking along a corridor heading direction error is the main reason after the particle correction the user location is updated to for particles to hit the wall and die so the personalization the centroid of the particles at the end of each session model remains unchanged before and after re sampling on the personalization model is also summarized and stored for the other hand when a user is making a turn model inac subsequent uses as follows we first get p particles with curacy becomes the main reason for particle death due to the highest weights we then choose q frequencies that lie deviations from the turn in this case the personalization within a normal walking frequency band applying the mod model is replaced after the re sampling els of these particles to the frequencies generates qp points of frequency step length the summary personalization turn detection the basic idea of turn detection is to com model is obtained by a linear regression on these points table false positives of step detection table false positive of step detection algorithms pd pd h pd h dtw algorithm phd nsc phd ped phd nw fp fp table false positive and false negative of step detection for the in table accuracy of step detection for the in hand and in pocket cases hand and in pocket cases algorithms pd pd h pd h dtw algorithm phd nsc ped nw fp error fn can clearly see the trend of decreasing false positive with the evaluation heuristics and dtw validation being applied the cost is a in this section we evaluate the performance of the key mod slightly increased false negative since in some rare cases a ules as well as the end to end system for indoor localization real step might be erroneously filtered out by the heuristics we have implemented or dtw validation this is acceptable since the benefit more than compensates for the cost here step detection it should be noted that for since those applications only false negative fn and false positive fp are the key met report the final count instead of a specific time stamp for rics for the step detection algorithm and directly affect the each detected step it is impossible to get false positive and accuracy of the overall localization we consider two test false negative for them instead we use the step detection cases for the evaluation error to evaluate the performance of each application test case the subject operates on the phone such as table shows the step detection result for with respect making a call texting playing games or moving the phone to to ensure the fairness for any pair of devices un randomly without taking any step der comparison we bind the devices together for similar motion input in the table phd is our algorithm with the test case the subject walks in free style with her pd h dtw combination it can be seen our algorithm pro phone at a certain position the ground truth will be labeled duces far fewer false positives than any of the three bench by recording the entire walking process mark applications we use the following benchmarks for comparing the perfor table shows the step detection result for with respect mance of the step detection algorithm to as in the earlier case the subjects walk with their phones being held in the hand or put in the pant pocket two benchmark the baseline peak detection algorithm phones among the three benchmark applications nsc is based on peaks and valleys described earlier and widely the only one with a reasonable accuracy at both positions used in the literature by comparing against it we exam for in hand and for in pocket using our algo ine the effects of the heuristics and dtw validation in our rithm the error can be further reduced to and algorithm respectively benchmark off the shelf step detection applica heading inference tions on widely used smartphone platforms nsc symbian in this experiment subjects were asked to walk along a nokia step counter ped ios pedometer and nw an given rectangle trajectory the ground truth is the direc droid noom walk tion of the pathway in each round of the test the user first opened our application put the phone into the pocket and table shows the step detection result for with respect then started to walk to pd refers to the baseline peak detection algorithm pd h the algorithm with the heuristics pd h dtw the figure a shows the heading inference result for one trace algorithm with the heuristics and dtw validation in the one can clearly see that the inferred heading direction is experiment people with different physical profiles used close to the ground truth direction their phones in free style for a certain time period without taking any step we then apply each algorithm to the sensor figure b shows the cdf of the heading direction infer data collected and compute the total steps it can be seen ence error on all the runs of the users in cases our that pd produces false steps which can be reduced to algorithm yields less than degrees in error the average by applying the heuristics the dtw validation further error is degrees while the maximum at degrees after reduces the number of false steps to applying the heading direction adjustment discussed earlier the percentile error is reduced to degrees and the table shows the step detection result for with respect maximum error to degrees to all subjects walk with their phones being held in the hand and put in the pant pocket two separate phones one a estimated heading vs ground truth figure system prototype on windows phone b heading inference error figure heading inference result figure floor map of the indoor test site the walking path at every and recorded the entire walk end to end system evaluation ing process as shown in the figure then we carefully we have built a prototype on windows phone and tested labeled the coordinates of a subject and the associated times it on htc mazza as in figure the floor map is divided tamp for every step the positioning error is calculated as into small tiles and each tile is labeled as pathway room the projected distance between the estimated position and space or wall the map is loaded into the phone in the form labeled ground truth along the walking path of an xml file which contains both the map image and as sociated metadata every time when a user opens the appli positioning accuracy cation a map list will pop out asking the user to select one after the map is selected and displayed on the screen the we evaluate the accuracy of the basic algorithm and the ef user taps and holds to input the current location the sys fect of dtw validation and personalization in improving the tem then automatically starts to track the user and updates accuracy figure shows the positioning results of the sys the location on a per step basis on the map the user can tem for the in hand case with the generic step model the also set parameters such as the sensor sampling rate and par ticle number with a tradeoff between positioning accuracy and cost the default setting in the following experiments are sampling rate for all the imu sensors and particles for the particle filter we have tested our system in an office building with the floor map shown in figure twenty laps of data were col lected from subjects with different physical profiles each subject walked along a area shown as red solid line on the map taking two phones with one in the hand and the a walking b opening the door other in the pant pocket to label the ground truth we place tags on the ground along figure snapshots of the data collection process a in hand b in pocket figure distribution of positioning error alg generic model no dtw validation alg generic model with dtw validation alg personal model with dtw validation figure estimated trajectories vs ground truth without top or with bottom personalization mean error is at without dtw validation in step detec tion and the percentile error at with the dtw held in the hand the trajectory is still as specified in fig validation the mean error is reduced by to and ure each time the system failed to track a subject the user the percentile error to when the personalization re initialized the location to get the system back on track is applied the mean error can be further reduced by another with the generic model only the system reports failures to and the percentile error to with the help of personalization the failure number is re duced to the personalization significantly reduces the for the in pocket case the mean error is at using the failure number since most of the failures come from the sub generic model without the dtw validation and the jects with extreme step models where the benefit of person percentile error at the dtw validation reduces the alization is more obvious mean error by to and the percentile error to the personalization further reduces the error to personalization effectiveness lower and respectively compared to the in hand case the in pocket case performs slightly worse since personalization improves positioning accuracy by adapting the accuracies of both step detection and heading inference the step model to each user figure shows the trajectories are lower estimated with and without personalization with person alization the estimated trajectories match the ground truth it should be noted that in the above experiments there are more closely although one can still see parts of the trajec cases when the system fails to track users due to extremely tories not being well aligned with the ground truth such as large errors in distance and heading estimation we did an those near the bottom left corner it should be noted that additional experiment to evaluate the number of failure cases most of them occur at the first turn when personalization of the system has not yet been performed we collected laps of data from people with phones we also took a deep look at how personalization improves password patterns as used on current android phones and other shape based authentication schemes are highly usable and memorable in terms of security they are rather weak since the shapes are easy to steal and reproduce in this work we introduce an implicit authentication approach that en hances password patterns with an additional security layer transparent to the user in short users are not only authenti cated by the shape they input but also by the way they per form the input we conducted two consecutive studies a lab and a long term study using android applications to collect and log data from user input on a touch screen of standard commercial smartphones analyses using dynamic time warping dtw provided first proof that it is actually possi ble to distinguish different users and use this information to increase security of the input while keeping the convenience figure standard layout of the password pattern authentica for the user high tion system the blank screen left and an exemplary shape right author keywords security implicit authentication password pattern in a study by clarke et al of their respondents expressed concerns with respect to pins and alphanumeric acm classification keywords passwords supporting the need for alternative authentica information interfaces and presentation user inter tion techniques in comparison to these approaches shape faces input devices and strategies evaluation based authentication better supports the way the brain re members and stores information the shape can be remem general terms bered as an image therefore exploiting the pictorial superi experimentation human factors measurement ority effect additionally since the pattern is drawn manually in exactly the same way every time and repeated introduction regularly the user motor memory further improves with the introduction of the android operating system for the memorability this effect was shown to be effective mobile phones an alternative to pin authentication on mo even when the shapes are performed by the user bile devices was introduced and widely deployed for the first gaze time the password pattern similar to shape based authenti despite its manifold advantages this approach has major cation approaches like draw a secret or passshapes drawbacks the most important one being security drawn enables user authentication by drawing a shape on the passwords are very easy to spy on which makes screen the shape consists of an arbitrary number of strokes shoulder surfing a common attack in public settings a or lines between nine dots as shown in figure serious threat other attacks include the infamous smudge attack in which finger traces left on the screen are used to permission to make digital or hard copies of all or part of this work for extract the password due to its weak security properties this personal or classroom use is granted without fee provided that copies are authentication approach does not fully meet the requirement not made or distributed for profit or commercial advantage and that copies of adequately protecting the user data stored on the device bear this notice and the full citation on the first page to copy otherwise nowadays not only private but also valuable business infor or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee mation is stored on the user handheld therefore re chi may austin texas usa sistance to attacks is a major concern when designing respec copyright acm tive authentication systems session i am how i touch authenticating users chi may austin texas usa in this work we extended the password pattern approach respectively tanviruzzaman et al developed a mobile with an implicit authentication layer to improve its security system called epet which uses the user gait and location whenever a user authenticates the system not only checks if information as behavioral cues epet continuously checks the shape was correct but also how it was entered to deter against anomalous user behavior and denies further access on mine if the person should gain access to achieve this we the mobile device in case anomalies are detected continuous used touch screen data of current smartphones pressure authentication on mobile devices was also introduced by coordinates size speed time etc to distinguish between the yazji et al their system observes activities on the mo rightful user and an attacker bile file system as well as its network access due to the per manent re authentication their system has a latency of five we performed two consecutive user studies to verify the via minutes with an accuracy of continuous authentica bility of this approach in the first study we examined simple tion is always bound to latency leaving the system unsecured unlocks e g a horizontal stroke while the second study used for a certain amount of time when no additional means of password patterns both times enhanced with implicit authen authentication are taken tication the results show that it is possible to distinguish different users thus increasing security during the authentica the approach presented in this work employs behavioral tion process at the same time the complexity of the ap biometrics the way a user performs the password pattern but proach is hidden since the users only interact with the famil immediately authenticates the user using common touch iar password pattern system thus the main contribution of screen data makes the need for any additional hardware this work is to provide first proof that this implicit authentica obsolete furthermore we combine behavioral biometrics tion approach actually works with the input of graphical passwords biometric authentication threat model apart from something you know authentication schemes we assume an attacker that is already in possession of the e g biometrics is an often used alternative user password pattern the shape that is the first securi according to wood there are two types of biometric ty barrier has already been breached how the attacker got authentication approaches physiological and behavioral this information is of no concern for this work in addition biometrics physiological biometrics relies on something the attacker managed to retrieve the mobile device e g the users are in sonkamble et al present an over using pickpocketing and wants to gain access to valuable view of different possible features including the users fin information on it for this as for other commercial systems gerprint face hand geometry voice or iris as physiological the attacker has three tries until the device will be blocked biometrics in multiple biometric features are com the approach presented in this work relies on implicit au bined to implement a person identification system bio thentication and has been designed to provide security metric authentication systems on mobile devices were for against such an attack thus even after losing the mobile instance used by rokita et al using face and hand fea device and the authentication credential the proposed sys tures marcel et al implemented a mobile authentica tem should still provide the required security tion system based on simultaneous face and voice recogni tion using built in sensors of the mobile device the ad unlock user study vantage of physiological approaches is that they work in the main idea of the pilot user study was to collect as much stantaneous in general however they require additional data as possible using simple unlocks as known from hardware e g fingerprint scanners in addition there are smartphones like the iphone or android devices our objec also user concerns related to the storage of physiological tive was to gain first insights into the possibilities of identify features ing and distinguishing users based on the data collected with behavioral biometrics on the other hand is more common a capacitive touch screen ly used for continuous authentication as the term behav we developed an android application that was used for data ioral implies these approaches are based on the users be collection figure shows four different unlock screens that havioral cues and authentication may happen implicitly we implemented two of them horizontal and vertical were exemplary cues are the user gait location infor based on unlocks from existing devices the remaining two mation or keystroke patterns shi et al were newly developed for this study to add unlocks that also proposed the use of behavioral biometrics as replace would produce more data the application logged all data ment for password based authentication or as second level of available from the touch screen pressure how hard the fin authentication their authentication system is based on mul ger presses size area of the finger touching the screen x tiple cues such as location information or communication coordinate y coordinate and time the only exception to this these features are combined with cloud computing to reduce was the vertical unlock with two fingers which had two sets the energy consumption on the mobile device in of xy coordinates pressure and size one for each finger acceleration signals are used for user identification whereas depending on the workload of the device one event with all used typing patterns to authenticate users on mobile of the previously mentioned data was collected every nine to devices using static classifiers and neural network classifiers twelve milliseconds no other sensor data was logged session i am how i touch authenticating users chi may austin texas usa figure the four different unlock screens as used in the first user study from left to right horizontal vertical vertical with two fingers and diagonal unlock other users the second day ended with a final questionnaire user study design as study design a repeated measures within participants collecting opinions about the different unlock screens as factorial design was used the independent variable was un incentives gift vouchers were given to the participants lockscreen with four levels horizontal vertical vertical with participants two fingers diagonal we recruited participants for the study with an average the task was to unlock the device times with each of the age of years the youngest participant was the oldest four different unlock screens over a period of two days the years old were female male the majority of the order of the unlock screens was counterbalanced to minimize participants were students the remaining ones came learning effects from different professions each participant owned at least one mobile phone smartphones at the time the study procedure was conducted of the participants were left handed before the experiment started the study was explained in detail to each participant after that they were asked to fill having participants allowed for perfectly counter out an initial questionnaire mainly collecting demographics balancing the four unlock screens that is each during this process each participant was assigned an id to permutation was performed by exactly two users a allow for anonymous data collection and b determine data analysis the order in which the unlock screens would be presented to for the analysis of the data dynamic time warping dtw the user was used this algorithm originates from speech recognition two android nexus one mobile devices were put on the and allows for comparing two sets time sequences of table in front of the participant one device had the study data with each other the algorithm looks for similarities application installed while the second was used for a distrac between the sets and calculates the costs to match one onto tion task during the test each participant was asked to un the other the result is a warp distance that can be used to lock the screens times with each setup at first each user determine how similar a set is to the reference set a warp got the application device in test mode and could play around distance of zero indicates absolute identical sets the big with the current unlock screens until they felt familiar with it ger the distance the more different the sets are thus it is they were also instructed to perform the task in the same highly appropriate for the purpose of this work for which we way for each unlock using the same finger every un used the dtw implementation for r by toni giorgino locks the application stopped and the participants were asked in this work a sequence consists of a time series of touch to perform a distraction task for this purpose they had to screen data all combinations of x coordinate y input a text message on the second device when resuming coordinate pressure size time again the exception the unlock task they were reminded by the experimenter was the two fingers vertical unlock which had two sets of which finger they used in case they did not remember xy coordinates two pressure and two size values within two days after the experiment the participants were it has to be noted that the choice of using dtw is not obvi asked to come to the lab again and performed the exact same ous however from related work we knew that if this ap experiment a second time this was done to collect more proach can work there is a high chance that dtw will show realistic and less biased data by minimizing habituation it we do not claim that dtw is better or worse than other effects and to see how the performance would change over approaches e g machine learning time each unlock was additionally used as an attack for the session i am how i touch authenticating users chi may austin texas usa reference sets to identify a user a reference set is required this set rep resents the baseline for comparison and acts as the finger print of the user for each unlock screen the reference set was created by taking the first unlocks each one a single unlock for each user each of them was compared to the other un locks using dtw then the average warp distance for the respective unlock was calculated this is a common ap proach found in related work in the end the unlock with the lowest average warp distance was chosen as the reference set taking the first unlocks is based on the fact that the rest was required to measure false positive and false negative rates figure identifying valid unlocks the warp distance to the in a second step the reference set was again compared to reference set is calculated if the result is within the interval the remaining unlocks the warp distances were then lower than the threshold the user is valid used to calculate the mean median minimum maximum and standard deviation those values are used to define the accuracy measurement upper border or the threshold for the interval starting from to calculate the accuracy on a percentage level we used in which an unlock is considered as valid this approach the following formula is depicted in figure the green line represents the upper border of the interval a possible value for this border is number of correct assessments number of all assessments mean standard deviation the main assumption behind this is that additional unlocks performed by a user are likely accuracy tn tp to be within these intervals while the ones of other users attackers should lie above tn tp fn fp it should be noted that unbalanced amounts of true positive logins and attacks and true negative rates easily influences the accuracy in the unlocks that were not used for creating the reference our work there are much more attacks than valid unlocks set were compared to the reference set using dtw to which means that false positives have a much higher influ check the system resistance to attacks the unlocks of all ence on the accuracy than false negatives therefore this other participants were compared to the reference set thus number can only be seen as an indicator and we also need the success of the system was measured along the following to look at the values separately rather than the accuracy parameters true positives tp correctly accepted users only true negatives tn correctly rejected attackers false positives fp wrongly accepted attackers false negatives logins and attacks fn wrongly rejected users it turned out that for all screens similar upper borders of valid warp distances were the best performing based on to compare the unlocks the previously mentioned thresh their accuracy this was the value in the middle between olds for valid unlocks were used in different variations the mean and the maximum plus the standard deviation furthermore overheads raising the threshold value of t mean max std the following results for all and were added to the intervals screens are therefore based on this value mean maximum and analyzed as well the comparison itself was repeated and standard deviation are based on the warp distances be several times using all possible combinations of the collect tween the reference set and the remaining unlocks of the ed data pressure size coordinates etc this was done to first unlocks find out which combination would perform best for the re spective unlock screen in short the analysis was performed table shows the best results for the different unlock using different thresholds and different parameter combina screens separated by the two days of the study desirable tions values are high true positive and true negative rates with low false positive and false negative rates at first glance a results very interesting trend can be identified while the true the results are based on unlocks per partici negative rate stays constant over the two days the true posi pant due to the big amount of data that was analyzed using tive rate decreases for all unlock screens in the worst case different combinations and intervals the analysis was per horizontal unlock it decreases by however the ac formed on a grid engine curacy stays more or less constant confirms the strong in fluence of higher numbers of attacks on the accuracy as mentioned before this explains why the overall accuracy session i am how i touch authenticating users chi may austin texas usa true true false false accuracy day positives negatives positives negatives accuracy horizontal day pressure 245 horizontal day pressure 749 vertical day time xy 555 vertical day time xy 093 230 finger vertical day time finger vertical day time 516 diagonal day size pressure 191 214 diagonal day size pressure 102 438 table number of false positives false negatives true positives and true negatives as well as the accuracy for all unlock screens separated by the two study days this table shows only the best results out of the different combinations of intervals and touch screen data for the diagonal unlock is the highest because it had the discussion best true negative rates in the experiment a decreasing true the unlock approach provided a convenient way to quickly positive rate with a constant true negative rate means that gather big amounts of data for analysis based on the re attackers stay as different from the user as before while the sults we could investigate whether it is possible to differen user unlock differentiates more the more time passed tiate users based on the way they perform unlocks espe overall the vertical unlock with two fingers performed cially the high true positive rates are encouraging worst due to a very low true negative rate this having shown how well users could be identified and thus means that of all attacks were successful this result is how well the approach performs in terms of usability there confirmed by the low overall accuracy of again this is a major drawback of this approach in the best case the was the best result for the vertical unlock with two fingers true negative rate was this means that a little bit more which was achieved using the event time than four out of ten attacks would have been successful even though its accuracy is not the highest in the analysis from a security point of view this is not a very satisfying the vertical unlock with one finger performed best among result the four unlock screens with a very high true positive rate even though there is room for improvements for the unlock on the first and on the second day the optimal approach the most promising way to go seemed to be using result was achieved using a combination of event time and a method that allows for collecting significant more data the xy coordinates for the analysis this means that in the per data set therefore we decided to take the lessons worst case of valid unlock attempts failed at the same learned from the pilot study and use them to enhance the time of attacks were successful having a closer look security of the password pattern approach at the data revealed more interesting findings that support the vertical unlock with one finger as being the best ap the lessons learned strongly influenced the design and proach in the study of all the valid users there was not a analysis of the subsequent password pattern user study the single user who could never be correctly identified at the most important lessons learned are same time of users were always correctly identified factor time the results of the unlock study showed that the another interesting trend is worth being mentioned for all usability of the system went down on the second day this unlock screens around of the attackers were responsi can be drawn back to the significant break between the two ble for more than of successful attacks study parts users did not remember exactly how they per session i am how i touch authenticating users chi may austin texas usa formed the pattern which influenced the results this influ ence is big enough to justify the claim that the data collec tion period of the next study should be done using a long term design to gather more realistic data as the system has to work in everyday use lab setting the unlock study was performed in a con trolled lab environment this might have positively influ enced the results a more realistic study design is therefore preferable for the password pattern study informed participants we instructed the participants to always perform the unlock in exactly the same way in an optimal setting the system should work and provide securi figure password descriptions for the participants the red ty without this knowledge working implicitly thus unin lines indicate that a point has to be skipped formed users seem to be the more realistic choice for the follow up study user study design the approach was evaluated using a repeated measures with password pattern user study in participants longitudinal design the task was to authenti one of the main weaknesses of the pilot study was that data cate once a day using the test application overall the partic was collected using only two sessions within these two ipants were asked to perform the authentication task meetings users were very likely biased to performing the times resulting in a three weeks study unlock the same way this effect is increased by the fact that the first day started with a training the first mode of the test they were told to perform the unlock as similar as possible application this data was not used for the analysis collect overall this leads to a strong positive bias that is not desired ing the training data over a week would not have been feasi to test real world applicability ble and the expected dropout rate would have been too high another problem was that a simple unlock only allowed for collecting a small amount of touch screen data as seen in the procedure pilot study the unlock screens that created longer time series on the first day of the study the participants received an e not necessarily more data as shown by the bad results of the mail with detailed instructions on how to install the applica two fingers approach had a tendency to lead to better results tion and how to perform the training task the android ap plication was provided via a download link and not via the these problems were addressed in the password pattern android market this decision was made to keep the applica study the password pattern approach allowed for the collec tion private to the participants and to reduce delays in de tion of much longer time series additionally we decided not ployment the application was installed on the participants to use a lab study but a long term real world study instead to android phones after the training task the input days gather more realistic data the pattern password has the addi started the daily input approach was introduced to further tional advantage that many users are already familiar with it minimize the learning and habituation effects encountered in for the study we developed an android application that the pilot study could easily be deployed to a bigger group of users the ap each participant was given a unique password pattern sent plication had two modes on first start the application was in with the e mail that was randomly assigned to the user based training mode allowing the users to train their password pat on an anonymous participant id the patterns consisted of tern until they felt familiar with it after that mode was end five strokes which make up a password space of pos ed there was no way to return to it in study mode the appli sible patterns around times higher than the password cation allowed exactly one authentication per calendar day space of a four digit pin there were three different catego and closed automatically after one correct or three failed au ries of patterns easy medium hard easy pattern consisted thentication attempts of simple strokes only medium patterns had one stroke for the standard layout known from android phones as shown which a point had to be skipped while hard patterns had at in figure was used for the prototype several other layouts least two skips figure shows two patterns medium and were tested but after informal studies the decision was made hard as provided to the participants as opposed to our ex to pick the layout the users are already used to pectations the difficulty did not influence the results in any way and thus will not be mentioned again later in this paper this study copes with the problems of the pilot study result ing in more realistic data and overall longer time series more in order to ensure that the users would not forget the input an sensor data per authentication attempt this enabled us to e mail reminder was sent every day around noon in case a check whether changes over days as observed during the user still forgot the input an extension by one day was grant pilot study are acceptable if more complex gestures pass ed that is in the best case the study took days overall word patterns are used extensions for ten participants were granted the maxi session i am how i touch authenticating users chi may austin texas usa mum extension of the study was days for one participant dian minimum maximum and standard deviation to the who forgot the input five times reasons for forgetting the other four was calculated and used for comparison with the input included weekend trips or days out remaining authentications depending on the number of identified possible reference sets this comparison was done when each participant had performed their inputs they up to four times per user were asked to come to a meeting and to bring their mobile device with them during this event the logged data was since the number of attacks and own inputs is more even copied after this a second application was installed on the than for the unlock study the accuracy measurement as device of the user the user was asked to input the correct introduced in the pilot study is a much more meaningful password patterns of all other users three times with this new indicator of the quality in this study still it is important to app the objective was to simulate attacks on all participants look at the single numbers in detail the respective log files were copied as well logins and attacks till the end of the study the participants were not instructed since the data was collected using the mobile devices of the to perform the authentication in exactly the same way this users we had to deal with a big variety of hardware setups was done to avoid bias and to get insights on standard behav including different screen resolution and quality the differ ior influenced by the users daily routine after the attack ent hardware would have influenced the results if we had experiment the participants were debriefed the final meet compared every participant with every other participant in ing ended with a questionnaire covering usability and securi order to avoid this only users that owned the same type of ty questions we also asked them to reflect on their behavior devices were compared to each other the biggest group e g if they tried to perform the password pattern in the same within those was using the nexus one with overall us way each time which was required to analyze the data ers in the end there were five users with unique hardware setups thus for those users no valid attacks existed and participants they were removed from calculating the overall accuracy the long term study started with participants the only prerequisite was the possession of an android mobile phone as for the unlock study the true positive true negative android or higher participants completed the study false positive and false negative rates together with the ac the dropout rate was four users in addition to those curacy were taken as a measure of performance again all four users three users had to be removed from the data sets possible combinations of touch screen parameters x in two cases the experimenter accidently gave away the pur coordinate y coordinate size pressure time speed were pose and description of the study before the participants had taken into account also different variations of mean medi finished their tasks the third person had to be removed since an standard deviation minimum maximum together with it turned out that he had someone else performing his tasks different overheads were tested this means that overall the study was correctly finished by results participants and thus the results are based on their data the following results are based on the data of the valid the average age of these participants was years the participants overall there were valid authentication youngest was and the oldest years participants were attempts including the data that was used to create the ref female were male the incentive was the chance to win a erence set and attacks we removed six inputs since popular gaming console at the end of the study the password pattern was wrongly input keep in mind that five users did not have valid attacks and therefore accuracy data analysis is calculated using the data of participants as for the pilot study we used dynamic time warping dtw for the analysis data sets consisted of a series of touch for the analysis of true positives and false negatives the screen events xy coordinates pressure size time speed first five valid authentication attempts that were used to as opposed to the pilot study the parameter speed was intro create the reference set were not taken into account based duced the time passed between two different coordinates on the fact that users could fail to authenticate by using a wrong shape the maximum number of true positive plus reference sets false negatives per person was this required significant in this study the reference set to identify a user was created ly less comparisons than in the pilot study highly decreas by taking the first five valid authentication attempts and ing complexity of the analysis the calculations were per comparing them to each other the overall approach is ana formed using a standard personal computer log to the reference set creation of the pilot study with the exception that different possible reference sets were tested logins and attacks smallest average warp distance smallest median warp dis after the analysis the reference set based on the smallest tance smallest min and smallest max in short the warp median showed the best results in a combination with using distance to the other four authentications were calculated the maximum warp distance as the threshold for valid in and the one with the lowest value median mean min puts the parameter combination that performed best con max was selected as the reference set then the mean me sisted of pressure size and speed session i am how i touch authenticating users chi may austin texas usa true false true false accu positives negatives negatives positives racy 231 false rejection rate false acceptance rate accuracy in table results for the reference set based on the smallest me dian in combination with the maximum as an upper border parameters are pressure size and speed true false true false accu positives negatives negatives positives racy 93 0 figure threshold increased for five users with low true table top accuracies of participants in the password pat positive rates the graphs show that with increased threshold tern study warp distance 0 till the accuracy significantly improves peaks show up at different overheads table shows the results for these combinations overall the accuracy is with a false rejection rate and formed a small experiment to see whether this theory holds false acceptance rate the focus of the analysis was to we iteratively increased the threshold for these users that go for high true positive rates to keep the system convenient is higher warp distances between two authentication at and satisfying for the users as mentioned before in the tempts were accepted as valid we conducted this for the final questionnaire the participants were asked whether previously mentioned five users the results are shown in they tried to perform the input in the same way each time figure for each participant the overall accuracy could be the users that stated to apply this approach had a higher increased significantly in one case it was improved from average accuracy 81 than the users that did not think to this was due to the fact that false acceptance about this during input interestingly the difficulty rates stayed constant while true positives increased of the password pattern does not influence the accuracy discussion when compared to the pilot study the overall accuracy the improvements on the approach and the study lead to increased by more than even though the data was more realistic and less biased data triggering much better collected with significant breaks in between two consecu results at first glance this seems odd but was achieved by tive inputs and under much more realistic circumstances using password patterns instead of a simple unlock enhanc the password pattern allows for much more accurate meas ing them with implicit behavioral authentication allows for urements of the users identity still at first glance creating a convenient authentication system with good secu accuracy seems unsatisfying rity properties under the worst circumstances attacker in however when looking at the results in more detail the possession of the mobile device and the password pattern picture becomes positive for instance out of the partic the results of the second study support our claim that pass ipants for whom valid attacks existed six reached an accu word patterns create data that is distinct enough to distinguish racy of or higher the top user reached an accuracy of between different users overall it can be stated that using with one false negative and two false positives out of touch screen data to identify users works to a certain degree table shows the top participants and their results this is supported by the fact that increasing the threshold for in addition one specific group of users drew our attention valid authentication attempts improves overall accuracy as its users had extremely low false acceptance rates mostly shown in figure the comparison to the pilot study also zero false positives but at the same time their false rejec indicates that this approach works the better the more sensor tion rate was rather high or unsatisfying in the worst case data is available there was only one true positive the second best user see an interesting observation was that informed users per table was part of this group by setting the threshold too formed better than uninformed ones that is users that tried low almost all attackers were excluded but many valid to perform the authentication in the same way each time attempts failed as well looking at these users reference considering the finger to use speed etc achieved on aver sets showed that their thresholds were quite low compared age higher accuracy values than the ones that did it random to other users again taking into account the hypothesis ly this means that the security and performance can be that users are more similar to themselves than to attackers influenced by the user the more consistently the authentica in the way they perform the password patterns we per tion is done the smaller the threshold making it harder for an session i am how i touch authenticating users chi may austin texas usa attacker however uninformed users achieved good results work in the field of behavioral biometrics once the pass as well which indicates that the approach can work in nor word pattern is input the system decides instantly whether mal everyday use without specific precautions this is very the user is authorized the results of two studies provide first promising from a usability point of view it means that users proof that it is possible to distinguish users and to improve can rely on the highly usable and memorable password pat the security of password patterns and even screen unlocks terns at highly improved security the results also show that the more data points a data set consists of the easier it is to make this distinction this limitations and improvements means that by increasing the password length positive ef as opposed to the pilot study the password pattern study fects on accuracies might be observed however this would had much less inputs as it is hard to collect realistic data come at the costs of decreased usability and memorability over a longer period of time with time frames longer than three weeks the dropout rates of an experiment of this kind the two main open points for future work are a we are can quickly increase still more data would help to further currently implementing a prototype based on the presented support our findings the lack of attackers for some of the approach that does the calculation on the mobile device to participants is undesirable in the best case in future work perform another long term study based on this application attacks should be performed using the participant the this way dynamic reference sets can be tested in real time victim own device however convincing users to give additionally it will enable us to perform shoulder surfing out their mobile phone which usually holds private data tests to further evaluate and judge the security of the ap seems hard thus a solution could be to hand out mobile proach b the accuracy of the system has to be increased phones for the purpose of the experiment on the other while in this work the focus was on showing that implicit hand this would negatively influence the realistic setting authentication works in future work less nave approaches have to be compared to improve the accuracy of the system as the experiment with increased threshold showed there is for instance dtw should be compared to for instance ma still a lot of open space to improve the approach for this chine learning approaches purpose both the reference set creation and the comparison algorithm can be optimized for instance the results of the access to the data second study suggest that a dynamic reference set or a refer interested in getting access to the anonymized data of the ence consisting of several unlocks authentication attempts two studies just contact the first author of this paper which changes over time as the user makes use of the system can positively influence accuracy the idea of a dynamic acknowledgments reference set and thus a dynamic threshold is further sup this work was funded by a google research award embodying designers judgments about valid ways to in this essay i explore several facets of research through design in order to contribute to discussions about how the address the possibilities and problems implicit in such approach should develop the essay has three parts in the situations and reflection on these results allow a range of first i review two influential theories from the philosophy topical procedural pragmatic and conceptual insights to be of science to help reflect on the nature of design theory articulated the output of this work takes the form concluding that research through design is likely to produce primarily of artefacts and systems sometimes with theories that are provisional contingent and aspirational associated accounts of how these are used in field tests but in the second part i discuss three possible interpretations increasingly includes a variety of methods conceptual for the diversity of approaches to research through design frameworks and theories presented separately from and suggest that this variation need not be seen as a sign of accounts of practice inadequate standards or a lack of cumulative progress in the as design practice has become more prevalent in chi field but may be natural for a generative endeavour in the however there has also grown an undercurrent of final section i suggest that rather than aiming to develop questioning within the design community itself about the increasingly comprehensive theories of design practice nature and standards of research through design based research might better view theory as annotation of these discussions revolve around the desirability of realised design examples and particularly portfolios of integrating design research methods approaches and related pieces overall i suggest that the design research outcomes in hci both as a way of consolidating community should be wary of impulses towards knowledge and to establish criteria for research done from convergence and standardisation and instead take pride in this perspective this agenda calls for the development of its aptitude for exploring and speculating particularising agreed methodological standards hand in hand with a firm and diversifying and especially its ability to manifest theoretical foundation research through design it is said the results in the form of new conceptually rich artefacts lacks clear expectations and standards for what constitutes good design research and thus would benefit from some author keywords actionable metrics for bringing rigor in critique of design research through design theory annotation portfolios research the community should develop philosophy of science protocols descriptions and guidelines for its processes procedures and activities in ways assumed to be like any acm classification keywords other research approach in part this depends h m information interfaces and presentation on developing research through design as a proper research miscellaneous design methodology that can produce relevant and rigorous theory conceptual contributions from this introduction perspective should exhibit a level of extensibility and over the last number of years design practitioners have verifiability if they are to comprise theory designers can become increasingly integrated within the hci research apply in research in practice community their work often takes the form of research through design in which design practice is as a long term practitioner of research through design i brought to bear on situations chosen for their topical and have some sympathy with these concerns nonetheless i am uneasy about some of the tendencies that seem to underlie both the diagnoses and the treatments that have permission to make digital or hard copies of all or part of this work for been suggested seeking conformance to agreed upon personal or classroom use is granted without fee provided that copies are standards and processes may be a route towards not made or distributed for profit or commercial advantage and that copies disciplinary legitimacy within hci helping clarify bear this notice and the full citation on the first page to copy otherwise or republish to post on servers or to redistribute to lists requires prior expectations for research through design both within and specific permission and or a fee outside the design community there is a risk however chi may austin texas usa that such standards might lead to a form of self policing copyright acm that would be overly restrictive of a form of research that i session critical perspectives on design chi may austin texas usa value for its ability to continually and creatively challenge the designs their students have made that they elaborate this status quo thinking moreover insofar as hci is prone to theory scientism in its cultural assumptions despite moves to design researchers often borrow conceptual perspectives legitimise other forms of research based for instance on from other disciplines and discuss their applicability for the humanities or to characterize it as a design design examples that are widespread in the design discipline attempts to establish standards for research community include notions of affordances context and through design may adopt or be interpreted in terms of situatedness cited in borrowed theories or inappropriate scientific models of research and theory for concepts are often used both to inspire new designs and to the field articulate existing ones in doing so the perspectives are in this essay my intention is to temper calls for disciplinary usually translated for use by designers for instance when consolidation with remarks on what we might reasonably wiltse and stolterman suggest that interaction design expect from research through design first i explore the should draw on the sensibilities of architecture they do not kinds of theory that have been developed in association refer to physical spaces but rather the social spaces opened with design using two accounts of scientific theory drawn by computational media translation can ultimately give from the philosophy of science contrasting accounts are rise to new concepts for instance overbeeke and chosen to complicate notions of what makes theories wensveen describe how they originally drew scientific in order to undermine any overly simple inspiration from gibson ecological psychology e g assumptions about how research should proceed and to and particularly the notion of affordances but adapted this draw out fundamental features of the conceptual work that to focus on emotional appeal under the new name of accompanies research through design this comparison irresistables suggests that instead of being extensible and verifiable manifestos are a third form of theory often produced as a theory produced by research through design tends to be part of research through design practice these go beyond provisional contingent and aspirational second calls for theoretical treatments drawn from other disciplines or standards formalisation protocols and guidelines assume developed from reflection on practice to suggest certain that disciplinary convergence is a prerequisite for approaches to design as both as desirable and productive of cumulative growth of understanding in design i explore future practice for example in three perspectives on the lack of convergence in research introducing their notion of reflective design sengers and through design and suggest that while one of these her colleagues suggest that reflection on unconscious perspectives implies that the community should work values embedded in computing and the practices that it towards greater shared understandings the other two imply supports can and should be a core principle of technology either that there is no problem to solve or that progress in design similarly gaver advocated a ludic approach to this area will be marked by proliferation rather than design as an antidote to assumptions that technology agreement third an emphasis on agreed upon theories and should provide clear efficient solutions to practical methods seems to assume that these are the hallmark of a problems typically such manifestos will describe respectable research discipline and that the production of design practice to illustrate their approach and borrow an endless stream of design examples is an inadequate basis theories to justify it but their primary function is to build for its pursuit i argue on the contrary that an endless string an account of a practice to be pursued in the future of design examples is precisely at the core of how design research should operate and that the role of theory should frameworks for design play a similar role to manifestos be to annotate those examples rather than replace them but tend to downplay both their theoretical commitments and normative stance for example forlizzi product theory in research through design ecology outlines a number of factors involved in conceptual work appears in many forms as a routine part of designing for products to be used together and suggests research through design work whenever practitioners methods to approach each stage the framework is describe their influences discuss the rationales for design intended to allow flexible design centred research decisions and articulate their assessment of what they have planning and opportunity seeking and avoids prescribing made and its importance they engage in a form of implicit appropriate methods in characterising a class of design conceptual work by highlighting important issues situations as it does however and in its ontology for dimensions of similarity and criteria for choices and describing the factors involved the framework nevertheless success to the extent these conceptual statements are implies a conceptual orientation and can be considered a articulated in general terms and applied to multiple form of theoretical output from research through design examples they become recognisable as theories in their own right for instance djajadiningrat and his colleagues a final genre of theory from research through design seeks develop a rich account of the importance of embodied to characterise research through design itself and often to movement that is substantially informed from reflections suggest normative standards for how research through about a series of designs they and their students have design should be conducted what should count as research produced they start from a conceptual perspective that through design and the appropriate standards for work values bodily movement but it is through consideration of done in this way for instance zimmerman forlizzi session critical perspectives on design chi may austin texas usa argue that the hci community will more easily accept popper the probability that a particular theory is correct is contributions from research through design if it has an zero no matter how much positive evidence is amassed see agreed upon form of practice evaluation and outcome and p in contrast popper suggests a single suggest this should come through the development of incompatible result can disprove a theory feynman extensible systemic approaches to theory development makes a similar point in describing the search for a new stolterman in contrast highlights the tendency for physical law design to produce ultimate particulars and suggests this first we guess it then we compute the poses problems for developing generalisable theories consequences of the guess to see what would be although these sorts of treatments can be considered as implied if this law that we guessed is right then we wide ranging manifestos this essay which itself is compare the result of the computation to nature with intended as a contribution to meta theory addresses the experiment or experience compare it directly with more specific content oriented theories described earlier observation to see if it works if it disagrees with arguably designers in the hci community have produced experiment it is wrong in that simple statement is an ample amount of conceptual work arising from the key to science p articulations of practice borrowed ideas manifestos and from this perspective the scientific process creates a kind frameworks but how good is this conceptual work should of artificial evolution of theories by eliminating weaker we be proud of the corpus of conceptualisations that have theories only the strongest survive popper point is that grown or is there something inadequate about our for this to work theories must be falsifiable in principle achievements to explore these questions i compare the irrefutability is not a virtue of a theory as people often theories produced by research through design with several think but a vice p influential accounts of scientific theory scientific research programmes scientific and design theories popper emphasis on falsifiability though influential in in this section i describe two contrasting accounts from the popular accounts of scientific reasoning has been widely philosophy of science primarily to shed light on the criticised as an inaccurate description of scientific practice characteristics of research through design but also to according to kuhn for example normal science problematise the potentially tacit accounts of science that involves puzzle solving in which researchers apply seem to underlie some views of research although other accepted theories to known problems or new domains in accounts of theory for instance from the humanities can normal circumstances failure to solve a problem is also shed valuable light on design there are several considered the fault of the scientist using the theory not the reasons why accounts of scientific theory are a useful foil fault of the theory itself p against which to consider the conceptual work produced as a result of design practice first fields such as the moreover the idea that scientific theories are rejected when philosophy of science and science and technology studies evidence contradicts them is historically inaccurate even sts offer sophisticated and well developed discussions new theories may not accord with data newton for of the nature of theory and how it operates within a example admitted when he published his gravitational scientific context see for a valuable overview which theory that it could not account for the moon orbit may serve as a fertile source of insight about theory in any feynman describes how a new theory can win out over field in addition one of the fundamental issues addressed established evidence in recounting how the equations for within the philosophy of science concerns how to weak decay were established in a passage striking for its distinguish scientific fields from non scientific ones inconsistency with his previous account occasioned for example by arguments about evolution v only the equation was guessed the special creation science these discussions highlight a number of difficulty this time was that the experiments were all issues that can be useful in considering the nature of design wrong how can you guess the right answer if when as a research endeavour and the likely characteristics of you calculate the result it disagrees with theory likely to appear as a result of that endeavour experiment you need courage to say the falsifiability experiments must be wrong p one influential account of science holds that the criterion how can scientists find the courage to champion theories of the scientific status of a theory is its falsifiability or over evidence lakatos proposes an alternative refutability or testability emphasis in the original account of scientific activity that emphasises scientific popper proposed this criterion as a corrective to notions research programmes rather than individual theories that scientific theories are primarily produced by induction programmes according to his account are characterised by and thus are more powerful the more phenomena they a hard core of theory surrounded by a protective belt of agree with as he notes it is easy to obtain confirmations auxiliary hypotheses and a powerful problem solving or verifications for nearly every theory if we look for mechanism used to generate evidence and make it confirmations p but an endless number of compatible with theory in lakatos description using these confirmations cannot prove a theory in fact according to session critical perspectives on design chi may austin texas usa components a research programme digests anomolies and homo ludens suppleness or simultaneous even turns them into positive evidence p analogue control of multiple parameters the problem programmes can change and evolve to adapt to new is the argument is not that x will always lead to successful evidence not by altering the hard core of theory but the designs however success is evaluated which would protective belt that surrounds it for example when clearly be open to refutation how could it be there are uranus was discovered not to move as predicted by too many other factors involved in a design project to make newtonian theory rather than rejecting the theory that kind of guarantee instead there is always an implicit scientists predicted the discovery of the planet neptune sometimes in statements about how to design successfully to reflect the myriad of factors that remain untheorised yet the ability for contradictory evidence to be turned into crucial to a project success assertions that x will novel discoveries by research programmes is key to sometimes lead to successful outcomes however are lakatos account for him research programmes do not unfalsifiable because no number of unsuccessful design succeed because they are unchallenged by disconfirming efforts would actually disprove the assertion after all the evidence instead he writes next attempt might be successful all the research programmes i admire have one underspecification makes falsification difficult in practice characteristic in common they all predict novel as well as in principle even where potentially falsifiable facts facts which had been either undreamt of or assertions are offered tests which are simultaneously have indeed been contradicted by previous or rival unambiguous and ecologically valid are inconceivable programmes p consider for example a key claim made in a paper arguing lakatos account of scientific research programmes paints a for the benefits of ambiguity in design very different picture of science from the popperian one by impelling people to interpret situations for instead of stressing scientific activity as a matter of trying themselves ambiguity encourages them to start to discredit theories it emphasises scientific research as a grappling conceptually with systems and their dynamic machine for generating new knowledge new contexts and thus to establish deeper and more understandings and new discoveries personal relations with the meanings offered by those systems why design theory is unfalsifiable research through design is clearly unscientific if popper although this is in principle a falsifiable statement testing criterion of falsifiablity is accepted theories are too it in the popperian sense would mean arranging a vague and practice is usually intended to confirm theories comparison in which no difference in conceptual grappling not falsify them to be sure popper account is widely or the depth and intimacy of personal relations to meaning criticised both in philosophy of science and by practicing could be established between comparable ambiguous and scientists still it holds a potentially unflattering mirror up unambiguous systems the problem is in determining how to the theories produced as a part of research through to construct such a systems to be comparable simply design if confirmation cannot be used as proof how do we tweaking an unambiguous system to be ambiguous might validate them rather than seeing difficulties in validation address the assertion in a narrow sense but it would be as a problem for design theory i would suggest this reveals unrepresentative of the ways designers harness untheorised two important characteristics of the conceptual work factors of a design to support its intended effects in other produced in association with research through design words the synthetic nature of design is incompatible with the controlled experiments useful for theory testing theory underspecifies design design often addresses wicked problems which are design is generative complex enough that no correct solutions exist a priori and what i am arguing is that if designers were to change their for which formulating the situation is integral to addressing practices to design for comparison or refutation they would it moreover design is an activity that involves many no longer be doing research through design the notion of different decisions dealing with many different and making falsifiable statements or of arranging tests to refute potentially independent factors of an artefact all situated such statements runs against the grain of the within the specific circumstances of production and use methodological approach of research through design c f finally is productive in the sense that it changes design and research through design is generative rather the context of its own activities in short theory by than making statements about what is design is concerned necessity under specifies design activities with creating what might be and moreover in zimmerman et al formulation on making the right thing the implication of this underspecification is that the theories produced by research through design are not the issue for designers is not to show that design theory falsifiable in principle whether they build on borrowed can sometimes lead to bad designs on the contrary their theory or observation of the world or of specific design concern is to sometimes create good ones similarly the examples such theories take the form designing for x can goal of conceptual work in research through design is not to lead to successful outcomes where x may be the self develop theories that are never wrong it is to create session critical perspectives on design chi may austin texas usa theories that are sometimes right as stolterman puts appreciated for its proliferation of new realities and its it designers can be prepared for action but not guided in theory considered as annotation of the artefacts that are its action by detailed prescriptive procedures p the fundamental achievement generative nature of design has deeper implications for research through design as i will argue later for now it is convergence in research through design enough to note that from popper perspective one of the calls for disciplinary reform in research through design features that distinguishes design from science is its appeal for more agreement about the methods approaches tendency to make generative statements rather than and outcomes in the field greater integration would make falsifiable ones assessment of new contributions fairer and more rigorous moreover if we could all agree about the proper lakatos characterisation of scientific research programmes approaches methods and outcomes to use we could build in contrast paints a picture of theory seems more upon one another research in a cumulative fashion compatible with the nature of research through design currently however there must be dozens of different from this perspective it is of little matter that conceptual manifestos for design each seeking to establish a research approaches such as reflective design or supple programme and each with its own configuration of goals interfaces or designing for the self are not issues methods and realised exemplars why is there not falsifiable instead what is at stake is their ability to inspire more convergence in the field new designs it is understandable why so many design manifestos or examples of borrowed theory are convergence in the natural sciences is a given lakatos accompanied by examples of realised designs what popper research programmes for example describe scientific would reject as merely confirmatory evidence is from this research in terms of historical movements in which point of view testimony to the fertility of the overarching researchers share a common set of practices and theory the difficulty of verifying design theory at least understandings a similar description is developed by kuhn through falsification is not a flaw for research through in his seminal account of scientific revolutions design as long as that theory can lead to productive according to kuhn the reason normal science can proceed research programmes is because researchers share a common paradigm which in kuhn usage sometimes means an exemplar of successful theory from research through design research and sometimes a set of common understandings to be clear i am not presenting popper and lakatos about how to describe relevant phenomena relevant issues accounts as particularly representative of philosophy of acceptable methodologies and outputs both accounts science in fact by extracting these accounts from that describe scientific research as convergent complex and dynamic field i wish in part to illustrate how even more striking however lakatos and kuhn accounts unsettled and controversial accounts of science are this both suggest that as a normal state of affairs only one lack of resolution i would suggest undermines attempts to programme or paradigm will dominate a particular characterise research through design or design in general scientific field at any one time aristotelian physics gives either as a science or as a non science personally i tend to way to newtonian physics which gives way in its turn to think that the nature of design practice and research is relativity for kuhn paradigm shifts occur when tension distinct from the ideals and often practice of science but grows about a given paradigm inability to resolve in the end whether research through design can be anomalies a new possible view arises which solves those considered a science depends on the account of science one anomalies or makes them irrelevant and scientists choose adopts similarly taking seriously the disagreements about to join the new paradigm because paradigms involve the nature of science subverts any easy scientism in hci different world views including ontologies philosophies more generally because it destabilises the use of science methodologies etc choosing between two candidates as shorthand for a set of practices and criteria assumed to cannot be done on rational grounds according to kuhn be desirable it is difficult to call for research to be more lakatos objects to this suggesting the relative productivity scientific if what it means to be scientific is under question of competing programmes as the grounds for scientists see choice amongst them both agree however that multiple instead what this discussion suggests is the desirability of programmes or paradigms cannot co exist over time clearly articulating the methodological and conceptual why do multiple research programmes coexist in design features of current research through design practice as well in the following sections i explore three possibilities as the standards one might want to advocate for them here i have suggested that the theory produced from design design as pre paradigmatic research practice tends to underspecify practice and to be generative according to kuhn the beginning of a scientific study in nature and thus that it is provisional contingent and of a given field comes with the widespread adoption of a aspirational these are all features that limit the potential single paradigm for its study until that time research is extensibility and verifiability of design theory seen by characterised by competition amongst a number of different some as possible and desirable instead as i discuss in the schools of thought usually drawing on different remainder of this paper research through design should be philosophical foundations and orienting towards the field in session critical perspectives on design chi may austin texas usa different ways valuable contributions may be made by instead their discourse tends to focus on controversies and disparate approaches but without shared assumptions about debates around new findings and unresolved anomalies the correct way to approach a field individual researchers from this perspective scientific discourse may be must establish the rationale behind their approach from the characterised by far more controversy than is implied by very basics every time they seek to make a contribution kuhn account of normal paradigm driven research and because there are no agreed standards of importance there there may be more of a shared sense of understanding is little sense of priority or order to the findings that are guiding research through design than sometimes seems amassed progress is slow and in kuhn view does not apparent really add up to a science if the natural sciences appear unified to an extraordinary when a particularly influential body of research is finally degree able to build upon a solid foundation of agreed established usually in the form of a theory that ties knowledge this may be a function of their presentation in together multiple troublesome phenomena and suggests the popular media particularly with the advantage of new paths for research everything changes researchers no hindsight in contrast to kuhn characterisation of normal longer need to justify the basics of their research but can science as consisting of puzzle solving other researchers concentrate on increasingly detailed and exacting paint a less cozy picture of scientific discourse bruno contributions they become more confident about devoting latour for instance characterises scientific activity in resources on studies which are difficult and time terms of controversy in his description of the networks consuming since they know they will be understood and instruments inscriptions and theories deployed by valued by their fellow researchers in general working scientists to gain acceptance for their ideas accounts such together on the issues suggested by a given paradigm and as this suggest that although scientific work may be based using methods and techniques suggested by it allows on many fundamental agreements about how that work science to make significant progress pre paradigmatic should be conducted as lakatos and kuhn research in contrast must founder suggest the day to day scientific discourse may well exhibit just as much uncertainty interpretation and debate it is clearly possible to recognise the current state of as does research through design research through design in this description of affairs there is a proliferation of research programmes and little conversely research through design may come much agreement about the values for which we should design the closer to comprising a research programme or sharing a appropriate methods for doing so standards for evaluation paradigm or set of paradigms than we appreciate after or agreed forms of output methods and techniques all most of us working in research through design seem to proliferate and when they are taken up by others it is often share a set of common values for instance most pursue without their underlying methodological orientation even some variation on user centred design agreeing that some what it means to undertake research as a form of design is contact with the potential audiences for the things we make subject to disagreement with some suggesting that design is desirable before during or after design work itself most projects should be constructed in part to explicitly test and of us assume that exploring a wide space of potential produce theory while others including myself designs whether through sketching scenarios narratives or believe that theory should be allowed to emerge from design proposals is crucial in achieving a good outcome situated design practice most of us appreciate the value of craft and detail in our work most fundamentally most of us agree that the from this perspective research through design lacks a practice of making is a route to discovery and that the shared paradigm without it we cannot join forces around synthetic nature of design allows for richer and more agreed issues or find common standards for work we agree situated understandings than those produced through more is solid enough to build upon research through design analytic means perhaps we do not all agree with all of cannot make progress this is a tempting conclusion one these assumptions all of the time and perhaps it is endemic seemingly behind recent calls for formalism and to design that all assumptions should be tested but there standardisation two other accounts however may certainly seems to be the outlines of a broad consensus also explain the seeming disunity of research through underlying research through design design when compared to research in the natural sciences again the shared assumptions underlying research through the invisibility of consensus design practice are precisely those unlikely to be addressed one alternative account to that of the supposed pre in the literature it is the speculative ideas the novel and paradigmatic nature of research through design would the disagreements that we are most likely to discuss this suggest that the balance of agreement and controversy is may lead us to underestimate the discord of science not so different in research through design than it is in however and to overestimate the divergence of research many fields of scientific research this interpretation through design from this point of view calls for hinges on the observation that consensus is usually easier to standardisation formalisation overarching theory and the see from a distance than it is to experience from within like are misplaced we already share many of the attributes after all as kuhn suggests researchers do not need continually to restate the agreed fundamentals of their field session critical perspectives on design chi may austin texas usa of a research paradigm and seeking to reduce diversity its implication is that development in these fields does not just cutting edge will just inhibit progress take the form of accumulation of incontrovertible results but of reactions reconsiderations and fresh beginnings the many worlds of design there is yet another possible interpretation for the apparent social sciences too change the world in which they lack of convergence in research through design which operate osborne and rose for example describe how sidesteps the opposing views that we either need a shared the development of public opinion research gave rise to paradigm to make progress or already have a shared public opinion itself as something to be identified paradigm and need to recognise controversy as a sign of measured and considered in policy formation and product progress in this view whether or not research through development law and urry observe that labelling design is built on certain assumptions about its conduct it theory as proposed by deviance theorists led to policies of will inevitably be characterised by greater diversity and less de institutionalisation and suggest that by bringing social inequality into view british sociologists gave rise to the convergence than the natural sciences because of the policies intended to address it the social world in this inherent nature of its field of study view is not the subject of objective independent many other fields lack the convergence seen over time in observation instead the natural sciences for instance kuhn explains that his account of the paradigmatic nature of the natural sciences different research practices might be making was inspired in part by the continual disagreements he multiple worlds and that such worlds might be witnessed amongst social scientists c f equally valid equally true but simply unlike one multiple schools co exist in the arts architecture fashion another p and product design in all these fields controversies are if research is performative in this way they suggest then seldom settled in such a way that new work can build on the research is no longer concerned primarily with accepted results the way science seems to do instead new epistemology how we know about our object of study but artistic movements overturn the basic values of their also involves ontological politics a concern with what is predecessors hemlines go up and down and even the most being made basic of household fittings are the subject of continual redesign and rethinking these disciplines do not converge from this point of view the reason that research through as the sciences do instead they are cumulative in the way a design is not convergent is that it is a generative discipline conversation is elaborating on what has gone before but able to create multiple new worlds rather than describing a seldom aiming for or finding resolution why do single existing one its practitioners may share many disciplines such as these fail to show the convergence of assumptions about how to pursue it but equally they may science build as many incompatible worlds as they wish to live in we may wish to improve the standards of research within for the natural sciences there is a strong presumption that the field but from this perspective we should realise that the object of study is a single unitary world that pre exists what we mean by improve what criteria we propose even and is independent of its observers this core belief seems the assumption that shared standards are necessary naturally to lead to convergence simply because if there possible or desirable are potentially repressive acts of are two incompatible accounts of the same physical ontological politics domain one must be better than the other see for one definition of better research and practice in design and theory as annotation the arts in contrast do not describe a single independent what is an appropriate role for theory in research through world they are generative investigating how to create new design on the one hand it is plausible to argue that if ones debates may rage about better ways to go about this design is to contribute to hci research we should turn but these cannot refer to evidence from a single greater attention to theory making both as a way of independent world as an ally c f multiple capturing and communicating new learning to the research incompatible worlds co exist routinely for these fields community and as a way of guiding design practice most practicing designers however do not engage moreover design and the arts change the context in which with major theoretical approaches in hci instead they use they operate when the original ipad was designed for a more eclectic mix of design techniques and orienting instance tablet computers were not widely known or concepts cited in in addition designers often available now anybody seeking to research or develop turn to existing examples of design to inform the tablet computers or anything at all for that matter is development of their own ideas why might this be so i designing for a different world one in which the ipad suggest it is both because of the provisional nature of exists one of the implications of the way generative theory and the definite nature of designed artefacts disciplines change the context in which they work is that this sets the conditions for a feedback loop in which the as i have suggested a great deal of design theory tends to development and adoption of a new design sets the scene be generative and suggestive rather than verifiable through for the development of variations accessories applications falsification this seems self evident in the case of and reactions whole new areas of reality another session critical perspectives on design chi may austin texas usa manifestos but also of more grounded generalisations annotated portfolios from particular design examples e g the problem beyond single artefacts however annotated portfolios may here is not just that theory underspecifies design so that serve an even more valuable role as an alternative to more practitioners will be faced by innumerable decisions formalised theory in conceptual development and practical whatever theory they use but that theory is underspecified guidance for if a single design occupies a point in by design in the sense that many aspects of a successful design space a collection of designs by the same or design will not be captured by a given theory thus in order associated designers a portfolio establishes an area in to understand accounts of how to achieve simultaneous that space comparing different individual items can make control over multiple functions successfully for example it clear a domain of design its relevant dimensions and the is useful to consider specific designs such as a rotary designer opinion about the relevant places and controller for a microwave oven configurations to adopt on those dimensions c f to put it another way multiple examples can start to tease the design examples are indispensible to design theory because individual concerns and judgements involved in a single artefacts embody the myriad choices made by their situated design out of the particular configuration to which designers with a definiteness and level of detail that would they were applied making clear both the dimensions along be difficult or impossible to attain in a written or which a designer choices may range and the invariances diagrammatic account as stolterman suggests among them design is concerned with the ultimate particular a concept he suggests that has the same dignity and importance as as artefacts are to theory from this perspective design truth in science p theories may be provisional portfolios are to research programmes for instance dieter but designed artefacts as opposed to demonstrators for rams work for braun and vitsoe has been characterised as example are not as carroll and kellogg have argued defining an elegant legible yet rigorous visual language a designed artifact is a theory nexus the choices made by for its products www designmuseum org design dieter designers reveal both the issues they think are important rams but to appreciate what this might mean one must and their beliefs about the right way to address those issues turn to a portfolio of his work figure comparing the the implicit theories embodied in objects from this variety of stereo equipment kitchenware and grooming perspective range from the philosophical what values appliances that he designed one begins to appreciate the should designs serve to the functional how should those qualities that hold across the examples and those that are values be achieved in interaction to the social what will more particular to specific designs rams offered a the people who use this be like to the aesthetic what particular form of conceptual writing to help with this form and appearance is appropriate for the context defining his approach to good design in ten concise moreover artefacts do not address these issues analytically principles e g good design is unobtrusive good design is but represent the designer best judgement about how to thorough down to the last detail accompanied by short address the particular configuration of issues in question explanations these annotate the portfolio drawing attention to important features and to salient details that another metaphor for much the same point is that designs might otherwise be overlooked the passages may seem can be seen as occupying a point in design space or too terse to serve as substantial conceptual contributions by perhaps more accurately creating a design space around themselves but in the presence of the portfolio they give a themselves see in this account designers can strong sense of rams style and philosophy one can explore the implications of a given design by moving imagine designing in the style of dieter rams without the around the point it inhabits to explore new design need for a detailed formal theory to direct decisions possibilities or even by jumping away from it along understood dimensions whether to apply some of the same the notion of annotated portfolios is not a formal one decisions to new domains or to react against them what is defining to the concept is not how materials are presented but that a balance is achieved between if artefacts embody theory however they do not encode it descriptions of specific detailed examples of design and if they occupy a point in a design space they do not practice and articulations of the issues values and themes highlight the salient or fruitful dimensions of variation that which characterise the relations among the collection and space offers one of the valuable roles of design theory to which the examples suggest answers the appropriate from this perspective is in making accessible the kinds of presentation of an annotated portfolio may vary depending decisions and rationales that comprise an artefact on material purpose and audience for instance the embodied theory or give dimensionality to its design detailed annotations shown in figure are extremely space in this case however then instead of theories compact and focus more on highlighting features of the predominating with design examples serving as mere artefacts than on elaborating the overarching conceptual illustrations design theory is best considered a form of themes a different presentation of rams work at annotation serving to explain and point to features of ultimate particulars the truths of design many of the ideas here about annotated portfolios have been developed in collaboration with john bowers session critical perspectives on design chi may austin texas usa images from flickr com creative commons credits nick wade rene spitz brett wayn figure sketch design for an annotated portfolio of dieter rams designs for braun and vitsoe note that annotated portfolios are not defined by their graphic presentation www vitsoe com emphasises his design principles over the theoretical contribution appropriate to research through portfolio of products themselves a more detailed design more fundamentally i am suggesting that examination of rams philosophy might well take the form however valuable generalised theories may be their role is of an illustrated essay what links all these presentations is limited to inspiration and annotation it is the artefacts we the mutually informative juxtaposition of conceptual create that are the definite facts of research through design annotations with specific design examples in such a way that neither dominates and neither is subservient conclusion in this essay i have explored some of the issues that annotated portfolios are in several respects the converse complicate calls for disciplinary consolidation in the hci of alexander design patterns they are not intended to research through design community abstract regularities from repeated attempts to design for the same domains instead they maintain the particularity first i suggest that we should moderate expectations of of individual examples while articulating the ideas and creating extensible and verifiable theory comparisons with issues that join and differentiate them juxtaposing designs accounts from the philosophy of science indicate both how with annotations supports appreciation of the conceptual provisional contingent and aspirational design theories dimensions of designs on the one hand and by yoking tend to be but also how such conceptual work may them to particular design manifestations grounds and nonetheless inspire thriving research programmes in specifies theoretical concepts on the other portfolios can addition continuing controversies about how to support multiple conceptual perspectives and similar characterise science should help undermine assumptions perspectives can be applied to different portfolios about research that draw on hci tendencies towards reflecting the lack of convergence in the field as a virtue scientism finally they also indicate the futility of debating most fundamentally annotated portfolios respect the whether design is or should be a science rather than ultimate particular of the designed artefact rather than worrying about accepting or rejecting some ideal version of abstracting across instances as pattern languages do while science i suggest we should reflect on the appropriate allowing for the extensibility and verifiability for which ways to pursue our research on its own terms some of the hci design community have called second i suggest that attempts to establish disciplinary i am not proposing here that annotated portfolios subsume norms of process or outcome are political acts to be all other forms of design theory theoretical writing approached with care considering possible accounts of a remains important in articulating the issues rationales and lack of convergence in research through design suggests lessons that are embodied by design particularly for an that greater consensus may be emerging in research through interdisciplinary audience i am suggesting however that design than is sometimes acknowledged at the same time we develop annotated portfolios as a serious form of convergence may not be the only or best model for users can articulate an interest network of social applications on the web let users track and follow the people or things by defining a set of individuals or artifacts activities of a large number of others regardless of location like blogs or rss feeds to pay attention to in doing so or affiliation there is a potential for this transparency to users immediately subscribe to a stream of events and radically improve collaboration and learning in complex actions other individuals take thus the social web provides knowledge based activities based on a series of in depth an unprecedented level of transparency in the form of interviews with central and peripheral github users we visibility of others actions on public or shared artifacts examined the value of transparency for large scale the question remains however what benefits this distributed collaborations and communities of practice we transparency provides particularly in the large scale i e find that people make a surprisingly rich set of social across a community inferences from the networked activity information in github such as inferring someone else technical goals previous work on awareness has explored the value of and vision when they edit code or guessing which of activity information for small groups this work has several similar projects has the best chance of thriving in found that notifying members of actions on shared artifacts the long term users combine these inferences into effective helps them maintain mental models of others activities strategies for coordinating work advancing technical skills and avoid potential coordination conflicts however and managing their reputation activity awareness has largely been examined in the context of well defined small groups within organizations online author keywords individuals participate in large scale ill defined transparency awareness coordination collaboration open communities that often have hundreds if not thousands of source software development social computing members transparency of others actions in this type of setting may have very different benefits as a function of the acm classification keywords larger scale and the fact that interactions are no longer information interfaces and presentation e g hci embedded in an organizational context group and organization interfaces visible cues of others behavior on a social website are likely to support a variety of interpretations about their general terms motivations and the community more generally people are human factors design social creatures and make inferences about others from what they observe e g surfacing information about introduction people actions on artifacts is no longer a technological the internet has become increasingly social in the last ten challenge what is more interesting and less understood is to fifteen years but the productivity implications of these what people are able to infer from such a collection of changes remain unclear we can track a person moment information and how these inferences help them carry out by moment status updates on facebook location on their collective work in this research we were interested in foursquare and updates on twitter blogs and wikis these the collaborative utility of activity transparency in a large applications all have a common set of important community engaged in knowledge based work we address the following two research questions to advance our permission to make digital or hard copies of all or part of this work for understanding of transparency in online social sites personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies what inferences do people make when transparency is bear this notice and the full citation on the first page to copy otherwise integrated into a web based workspace or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee what is the value of transparency for collaboration in cscw february seattle washington knowledge based work copyright acm 1277 session toolkits and software development february seattle wa usa in order to address these questions we examined a small set of developer actions versus general awareness successful social site called github github com a code across a project as a whole noting the effort required to hosting repository based on the git version control system proactively provide status information e g github is an iconic example of a knowledge based in addition collaborative awareness systems have typically workspace this site integrates a number of social features utilized momentary notification of the most recent activity that make unique information about users and their as opposed to a history of actions on shared artifacts only a activities visible within and across open source software handful of systems have attempted to provide visualizations projects we interviewed light and heavy users of the site of activity over long periods of time e g history flow having them walk us through a typical session and system which visualizes collaborator edits to wikipedia describing their activity within projects we found that they articles we do not yet understand how these types of made a rich set of inferences from the visible information activity traces influence collaborative action particularly in such as inferring someone technical goals based on online production settings like wikipedia where hundreds actions on code developers combined these inferences into or thousands of collaborators can be involved on an article effective strategies for coordinating projects advancing their technical skills and managing their reputation in the social computing and software development next sections we consider related research describe the social computing technologies such as micro blogging github context and our study present the results of our activity feeds and social annotations facilitate lightweight interviews and finally discuss implications of our findings interactive information sharing within the web browser social computing technologies shift the focus of interaction the value of awareness to individual contributors and their activities with electronic previous work on collaboration and awareness suggests that artifacts individuals articulate their interests likes and providing visibility of actions on shared artifacts supports dislikes as well as their social network when cooperative work in a variety of ways more recent work combined with information visualization techniques these has shown the utility of social tools and systems for tools may help individuals make sense of activity and relationship management in the workplace however these contribution on a much wider scale literatures have not yet articulated the value of integrating social networking functionality with activity awareness previous work on social media for work purposes has shown that systems like facebook and linkedin are useful collaboration and awareness for maintaining weak tie relationships and bridging collaborators who are physically collocated have some organizational boundaries and organization internal level of awareness of each other activities because of social networking sites like beehive in ibm support similar frequent opportunities for interaction the types of boundary spanning activities in an organization affordances of collocation documented through careful at the same time these systems have been integrated with field study of software developers and other work artifacts in a limited way e g bookmarks in knowledge based workers include overhearing shared visual space and shared memory of discussion around when social computing technologies are used in a software artifacts these affordances support awareness development context there is an opportunity to leverage of others work state and expertise both useful for articulated social networks and observed code related coordinated action in collaborative projects activity simultaneously to support the type of awareness previous only available to collocated teams e g in systems awareness systems attempt to provide distributed like software engineering is only beginning to collaborators with the same type of mutual knowledge make exploratory use of social computing technologies to these systems have taken various forms with a variety of enhance collaboration for example tagging foci including informal social awareness to support casual searchable graphs of heuristically linked artifacts and interaction structural awareness of group member workspace awareness have shown some promise roles and status and workspace awareness of actions on for supporting coordination in software development shared artifacts 18 the theory of social however the utility of these technologies has been translucence suggests these types of awareness systems are examined in isolation from the rich ecology of open useful because they can make socially significant software development we know relatively little about information visible support awareness of collaborators how developers actually adopt and adapt these tools in the behavior and make the viewer accountable for that process of their work and when and how they improve information coordination and performance for the most part however collaborative awareness tools method have been designed for and evaluated within small groups it is unclear to what extent they scale across a wider range research setting previous work on awareness and software development has in order to address our research questions we examined indicated a tradeoff between specific awareness of a very collaboration among users of a large open source software hosting service github github provides a set of session toolkits and software development february seattle wa usa social coding tools built around the git version control actions visualizations on the site such as the network system http git scm com and incorporates social view provide access to the history of commits over time functionality that makes a developer identity and across all forks of a particular project see figure activities visible to other users the github site is unique in that it makes user identities internal project artifacts and actions on them publicly visible across a wide community people on the github site developers create profiles that can be optionally populated with identifying information including a gravatar an image representing them throughout the site their name email address organization location and webpage a developer profile is visible to other users and displays all the repositories that person is working on and a list of their latest activities on the site see figure figure github user profile with projects and public activity code artifacts github currently hosts over one million code repositories and has registered contributors while a majority of the projects on github are single developer code dumps many are active multi developer projects of significant scale that have been running for some time each repository on github has a dedicated project page that hosts the source code files commit history open issues and other data associated with the project developers can create permanent urls to link to specific lines within a code file this functionality allows information about figure feed of actions on code artifacts artifacts within the site to flow outside of the github community to the web at large actions actions in github occur when a person changes an artifact or interacts with another person through the site these actions can be code related communication or subscription actions on code or associated with code include committing forking and submitting a pull request project owners can make commits i e changes to the code by directly modifying the contents of code files developers figure network view sequence of actions on code artifacts without commit rights to a project must fork a project to get a sense of how visible information on github creating a personal copy of the code that they can change influenced the nature of collaboration and interaction we freely they can then submit some or all of the changes to interviewed a set of developers who use github we first the original project by issuing a pull request the project examined the types of inferences they made based on the owner or another member with commit rights can then visible information in the site and next examined what merge in their changes developers can also communicate types of higher level activities these inferences supported around code related actions by submitting a comment on a commit an issue or a pull request data collection we conducted a series of semi structured interviews with the record of all action information combined with user github users our goal in these interviews was to subscription allows activity updates to flow across the site document and understand in more detail the different ways subscription actions include following and watching github functionality was used by our participants we developers can follow other developers and watch other solicited participants via email and conducted our repositories subscribing them to a feed of actions and interviews in person or via phone participants were chosen communications from those developers or projects figure for equal representation across peripheral and heavy users with frequent updates for active projects with greater than watchers on at least one of their oss actions on artifacts also become artifacts themselves as the projects this was done because we thought that serious history of user actions on code artifacts is recorded over and hobby users might have different purposes and time the feed presents a recent history of following strategies and very different information loads table watching commit issues pull requests and comment summarizes the participants in our sample session toolkits and software development february seattle wa usa hobbyist work use work use as a signal of commitment or investment at the individual non sw org sw org and project level peripheral visible information about other developers actions users influenced perceptions of their commitment and general heavy interests recent activity gave a sense of the level of users investment in a project the feed of developer actions across projects helped other developers infer their current table summary of interview participants interests one respondent described following a friend to participants were asked to walk us through their last session stay up to date on what he was up to through his commits on github describing how they interpreted information the amount of commits to a single project signaled displayed on the site as they managed their projects and commitment or investment to that project while the type of interacted with other users projects remote participants commits signaled interest in different aspects of the project shared their screen during the interview using adobe visible social representative quote connect so we could ask specific questions about data on cues inferences the site and users could demonstrate their activities on the recency and interest and this guy on mongoid is just site interviews lasted approximately minutes to one volume of level of a machine he just keeps hour these interviews were then transcribed verbatim to activity commitment cranking out code support further analysis the interviews videos and field commits tell a story convey notes supported our analysis process sequence of intention direction you are trying to go actions over with the code revealing behind action data analysis time what you want to do we applied a grounded approach to analyze the the number of people transparency related inferences in our interview responses watching a project or people we first identified instances of these types of inferences attention to interested in the project importance to in five interview transcripts for each example analyzed we artifacts and obviously it a better project community than versus something that identified what information was made visible by the github people has no one else interested in system what inferences the participant was making based it on that information and the associated higher level goal we then conducted open coding on these responses detailed if there was something in the personal feed that would preclude a comparing each instance with previously examined information relevance feature that i would want it about an examples and grouping examples that were conceptually action and impact would give me a chance to add similar this process revealed categories of transparency input to it related inferences and higher level behaviors these table visible cues and the social inferences they generated inferences supported we used this first set of categories to code the remaining interviews revealing additional recent activity signaling project liveness and maintenance categories and refining our original coding scheme to as with many open source hosting sites dead and represent the dataset as a whole we repeatedly discussed abandoned projects greatly outnumber live ones that people the codes and transcripts in a highly collaborative and continue to contribute and pay attention to it can be tedious iterative process we continued this process until the to figure out which are which yet it is important to do so interviews no longer revealed new behaviors not captured since one does not want to adopt or contribute to a dead or in our existing set of categories theoretical saturation dying project in github developers described getting a sense of how live or active a project was by the amount of results commit events showing up in their feed our analysis revealed that individuals made a rich set of inferences based on information on github these commit activity in the feeds shows that the project is alive that people are still adding code inferences were a function of four sets of visible cues summarized in table users also relied on historical activity to make inferences about how well the project was managed and maintained recency volume and location of actions signaling lots of open pull requests indicated that an owner was not commitment and interests particularly conscientious in dealing with people external to as with other low cost hosting sites github has a mix of the project since each open pull request indicates an offer projects that are little more than code dumps and serious of code that is being ignored rather than accepted rejected projects that continue to receive attention and effort there or commented upon is also a mix of hobbyists who make occasional contributions and move on and dedicated developers who sequence of actions conveying meaning provide project stewardship over the longer term our visible actions on artifacts carried meaning often as a interviewees often used the recency and volume of activity function of their sequence or ordering with respect to other session toolkits and software development february seattle wa usa actions when considering the actions of an individual inferences about who would see a particular action also developer they signaled intentions competence and influenced perceived value of engaging in that action experience in the project context actions on code and who inference of the size of a potential audience was cited as a carried them out allowed others to make inferences about motivation to contribute respondents indicated the number the structure of the project and collaborator roles of forks or watchers of a project signaled that lots of other people will benefit from this change or that commits conveying developer intention someone would find this useful the size of the at the lowest level commit information connected with potential audience was also frequently cited as a reason for other commits comments or issues conveyed meaning or using github in the first place based on general intention behind actions several respondents were able to community interest in other forums such as mailing lists look at a sequence of commits and infer what the developer was trying to accomplish with their changes e g attention signaling developer status linking commits and issues similarly the number of followers a developer had was interpreted as communicated the reasoning behind a change to the code a signal of status in the community developers with lots of followers were treated as local celebrities e g dhh their history of activity signaling competence activities were retold almost as local parable our at the developer level information about past commits interviewees knew a great deal about them and paid number of projects created versus forked and activity in attention to their actions those projects all fed into perceptions of developer skill several respondents noted that github profiles now act as a attention signaling project quality portfolio of work and factor into the hiring process at many visible information about community interest in the form of companies in part because they watcher and fork counts for a project seemed to be an provide a sense of a developer work style and pace important indicator that a project was high quality and worthwhile several respondents indicated using the history of activity signaling project structure and roles number of watchers of a project or forks of a project as a developers also seemed able to use the history of actions on signal that a project had community interest and so was code to make sense of a project evolution over time or likely to be good or of interest as one developer put it history the active public record of who contributed what the way you know how useful something is is how much aspects of the code when meant developers were able to community there is behind it describe how a project got into its current state who originally founded the project what happened across these signals of attention at the same time put pressure on different releases and who was responsible for what areas developers since their visible actions affected attributions of the code developers described using this commit history about project quality several respondents indicated an information to infer other expertise with parts of a project awareness of being watched noting that updates about their respondent indicated that he would look at who made changes would flow to everyone watching and that changes to which file to find who had expertise on a piece everyone can see what you re doing in some cases of the system essentially inferring who knows what from they inferred the identity of this audience e g users of their the record of activities on the project project based on who they knew was watching or had forked the project noting for example people are watching attention signaling community support because they depend on it visible cues about who was attending to something served as an important signal of community support or lack action details signaling personal relevance thereof developers interpreted activity traces of attention certain properties of actions in the feed signaled potential following watching and comment activity as an indicator personal impact these inferences were highly dependent that the community cared about that person project or on the developer own work and interests rather than the action signals of attention also seemed to lead to developer community at large attention to that person artifact or event actions signaling contribution opportunities attention signaling action or artifact importance several respondents inferred contribution opportunities respondents used signals of other users attention to a feed from action information they would see in the feed for item as indicators that an artifact or action was important example one respondent described continually watching in particular comments on a commit suggested the commit the feed for issue submissions or comments on commits was interesting controversial or worth looking at both representing a chance for him to add something actions signaling attention to a project or person watching to the code or to the ongoing discussion or following similarly signaled it or they were interesting in some way and prompted developers to look more closely actions signaling potential problems respondents also inferred potential problems from commit events they would view in the feed or in the recent commit session toolkits and software development february seattle wa usa list these inferences were based on cues that a commit was i saw somebody trying to use it with rails master i m like well connected with specific files on a project or had comments crap i don t know if it works with rails master so let me check suggesting the change would affect their own projects so that type of stuff has been useful just to get a sense of the comments on a commit also signaled a potentially kinds of things people might like to see you know contentious or problematic change in almost all cases these user modifications represented innovations that extended the project in interesting ways social inferences informing joint action making it compatible with other systems or more generally the social inferences that individual developers made based useful on visible cues of others behavior fed into three types of higher level collaborative activities project management although observable behavior in the forks provided learning through observing and reputation management information about user needs developers often sought direct interaction with users to get feedback on their needs project management several developers mentioned also posting information all of the developers we interviewed had github projects about a change using a direct url to the code into a they were primarily responsible for either because they project mailing list in many cases this was to were owners of the project or centrally involved in get user input or buy in for a new design decisions that development certain types of social inferences described would go into the next release of a project e g users above supported project management activities would also contact developers directly to let them know about a change they wanted to make to the code or an issue recruiting developers they were having with the project these several of the developers we talked with actively recruited interactions helped surface user needs but were also seen as others or were recruited by others to contribute to a a nuisance in some cases when responses were sought in a project more heavily this recruitment was fueled by private channel such as e mail rather than a public channel visible information about the other developers competence where everyone could observe the interaction and investment in the project for example watched commits occurring in the various forks of his project to managing incoming code contributions identify skilled and committed developers who could perhaps the most important project management activity contribute more actively in some cases this was based on developers engaged in on github was managing incoming past successful contributions to the project and observed code contributions as noted above users and other commitment for example a newer member of github developers could submit changes to a project by forking the was recruited by a project owner after submitting several project and then making a pull request a request that the good commits the project owner began sending him tasks changes be merged back into the master branch such as requests to address incoming issues intense interest developers were constantly making decisions about what in the project inferred from a high volume of commit code to accept back into the project actions in a short period of time sent a strong signal that a for very large and popular projects contributor was invested in the project and could be trusted owners dealt with many pull requests per day to contribute more centrally in the cases as noted above they described making inferences about the observed this perceived investment seemed to translate into quality of a code contribution based on its style efficiency trust with project owners granting commit rights allowing thoroughness e g were tests included and in some cases new members to influence project vision and sometimes the submitter competence some even turning over ownership to the newcomer after they developers indicated prioritizing requests that were tied to had demonstrated high levels of investment issues or integrated a feature that users had been requesting not all of the projects on github used the pull request mechanism in some cases because of legacy identifying user needs reasons patches to a project had to be sent to a mailing list transparency also supported identification of user skills for approval where the community would chime and needs here the term user refers to other developers in on their acceptability interestingly in this way the who make use of a particular project in their own work github pull request mechanism centralized control over becoming dependent on that project for certain functionality changes by allowing managers to bypass a public mailing developers inferred user needs by watching their list notification and discussion mechanism activity in forks personal copies of the project visibility across forks or copies of a project took the for example one developer described awareness pressure off of project owners to accept all changes and that users were forking his project to deal with various allowed niche versions of a project to co exist with the incompatibilities with a new version of another piece of official release thus contributors could build directly on software they used in concert he was made aware based on each other work even if the project owner did not their activity in the forks which incompatibility issues were approve of the changes as one developer put it particularly problematic for his users session toolkits and software development february seattle wa usa i can ignore bad changes but know that the network of managing dependencies with other projects experimenters can continue cross project visibility allowed project owners to the cross fork visibility also meant that project owners proactively manage the dependencies their code had with could proactively solicit changes from developers as they other projects project owners were in almost all cases were working and use the transparency to track the status users of the code of many other project owners meaning of ongoing changes by others several respondents changes to those projects would affect the functioning of indicated using the network view figure to identify the their own project because of this they leading wave of changes to their project or the newest code attended closely to change events from projects they were as noted above they could see what people dependent on they watched for commit events in the feed and reported paying special attention to new releases were trying to do with their code which likely contained new features they could make use i would look at this network view and actually find folks who of and changes to files they knew their project used had uploaded a patch and say hey are you planning on sending that back to my project this is what i think of it here some changes you could make here some suggestions popular website their entire engineering team uses my and that kind of got the ball rolling project and so they keep an eye out for any changes as well because when i do a release it breaks something then i in some cases the changes would not be submitted back essentially broke popular website entire development for a because the person did not finish doing what they had day or something intended to with the project here respondents described pinging the developer to solicit a pull request in some cases they were watching for changes they knew or receiving a ping or asking when they would were coming because they had heard about them in other finish the change in some cases if the change was novel or forums mailing lists blogs etc or had discussed them directly with project owners or other developers useful enough the project owner would take over the code and finish it themselves when changes occurred that affected their code developers as with user needs in many cases project owners needed often directly contacted the project owner or contributor to directly communicate around a code contribution this who had made a specific change or joined in on was sometimes an attempt to solicit and motivate changes discussion about a proposed change for example as described above more often however this interaction one project owner showed us a case where a third party consisted of negotiation around incoming pull requests chimed in on the discussion around a pull request someone there was a clear sense that project owners had a view of else had submitted because the change affected the trajectory for the project and there was a need for functionality his company depended on others to get buy in before making changes developers would also handle conflicting or problematic project owners would often see potential problems that a changes by directly modifying the dependent project to code submission would cause with other parts of the code address the problem transparency or with changes they wanted to make in the future in both supported this behavior because the code artifacts of the cases their reaction was based on implicit knowledge about dependent project were open and accessible and the code organization or their future plans for the code their visibility of changes meant the project owner knew exactly vision for the project as they often called it why something was no longer working the project owners i could tell that was actually going to cause some serious in this case were users of others projects and then had to problems down the road so i just responded i always thank lobby and negotiate with the dependent project owner to get them because it a big help when people contribute back but it their changes accepted e g wouldn t work so i kind of explained to him why it didn t work learning from others interestingly the transparency on github supported this information was not transparent to submitters and learning from the actions of other developers being able to could only be elicited through direct communication around watch how someone else coded what others paid attention the code similarly the developer reasoning behind a to and how they solved problems all supported learning change or the organization of a code submission was not better ways to code and access to superior knowledge always clear to the project owner in some cases several rounds of comments around a pull request were required to following rockstars establish shared understanding of what the developer was developers in our sample described following the actions of trying to accomplish and why the inline interaction other developers because they deemed them particularly with code supported negotiation around a submission so good at coding they referred to these developers with that in some examples the developer submitting a change thousands of followers as coding rockstars and would be able to modify the code he had submitted to reported interest in how they coded what projects they address concerns a project owner might have about were working on and what projects they were following in potential conflicts or conformity to project style norms most cases this was because these developers were deemed session toolkits and software development february seattle wa usa to have special skill and knowledge about the domain owners saw this as a process of ramping up users to in part as a function of their large following eventually become full fledged contributors e g watching watching managing reputation and status developers also reported interest in which projects other the public visibility of actions on github led to identity users were looking at and described certain users as acting management activities centered around gaining greater almost as curators of the github project space attention and visibility for work as one developer put it self promotion i follow people if they work on interesting projects then i m interested in the projects they re interested in visibility for work was recognized as a valuable aspect of the github community the developers we certain developers were deemed to have a knack for interviewed talked about the positive utility of visibility finding useful projects in a particular interest area which led to increased use of a project extension by others this guy has good taste in projects he curates for me ideas from a broader audience and exposure for other watching him is like watching the best of objective c that projects created by the owner github has to offer at the same time self promotion active attempts to gain this interest in finding the hottest new projects through additional visibility for work was recognized as a what others were watching highlighted the importance that somewhat distasteful activity and something the developers github users seemed to place on novelty more generally said they wouldn t do regardless many developers consciously managed their self image to promote their i learn about new projects and new technologies way faster work through consistent branding giving their project and than ever before and it just encouraged me to get dialed in to a bunch of different tech communities i never would have had blog the same name or using the same twitter handle and access to before github user id and by publicizing their work on other platforms outside of github as one identifying new technical knowledge user noted developers were also interested in watching the actions of i think a lot of people that use github are trying to promote other developers and other projects to find new technical themselves this is very self promotional it like i have this knowledge in some cases other projects served as a project you will be interested in it resource to see how other developers had solved a similar the attention associated with self promotion was problem to theirs developers were also motivating for some of the developers we talked to one interested in watching development over time within developer noted that watchers kept him working on projects that were similar in nature to their own something he might have otherwise abandoned when i find a project that solves a problem that i had and i m going to continue to have then i will watch it p19 watching lets me know someone cares by watching these projects and getting updates on the social capital identity and recognition changes they made as they happened they learned how because watching was recognized as a signal of project their technical neighbors were approaching related quality it carried meaning as a sign of community approval problems informing their own development for a project as well several developers we talked to mentioned watching a friend project to increase their direct feedback social capital on the site or promote their work developers also learned from others through direct this was also done explicitly by posting projects to external interaction through comments on pull requests developers sites such as hackernews a common source of information got feedback about their code from more experienced for developers in the github community or suggesting a developers this was sometimes comments about good project for railscast projects featured there were known to form or the right way to do things in terms of coding receive a boost in watchers the reciprocal visibility of style or what was normative this was also feedback about actions in github meant that a certain amount of face code correctness or more efficient ways of writing the same management was associated with behavior on the site code e g these interactions helped improve the developers did not want to offend others by for example quality of the code submissions publicly rejecting code contributions from long time communication also supported learning about another contributors p21 or not following someone who developer project and getting help with attempts to build followed them p16 on that project p16 some developers were extremely being onstage forthcoming with this type of help checking their irc many of the heavy users of github expressed a clear channels and issue requests constantly to find and address awareness of the audience for their actions p6 those in need e g p11 p19 for some this was an this awareness influenced how they behaved and opportunity to grow a potential contributor and project session toolkits and software development february 2012 seattle wa usa constructed their actions for example making changes less happen when transparency broke down there was certain frequently p6 because they knew that everyone is information developers could not directly observe watching and could see my changes as soon as i people seemed to work independently until certain events make them p13 there was a concern to get things right brought them together making the dependency more because of how public changes to the code would be one salient such as when a potentially problematic change developer contrasted his heavily watched project with a would show up in the feed or when a pull request would niche project noting that he could be more experimental in create problems for other aspects of the code direct the niche project because no one was watching p21 communication functioned in these cases much as mutual another developer directly compared it with the pressure of adjustment allowing individuals to directly share performing unobservable information about their rationale why they try and make sure my commit messages are snappy and my were doing what they were doing and plans what they code is clean because i know that a lot of people are watching were planning to do next and negotiate mutually it like being on stage you don t want to mess up you re compatible solutions to a conflict these negotiations were giving it your best you ve got your hollywood smile supported by direct communication interactions in code comments irc channels campfire mailing lists and a discussion variety of other web based communication tools three interesting themes cross cut the observations in our data about the value of visibility and transparency in the thus although passive activity traces of others behavior are github community the micro supply chain ecosystem on powerful in some ways they are limited when joint action github the value of observation versus direct interaction is required in part this is because of the lack of feedback or and the affordances of attention signals interactivity these visible traces provide our results suggest these traces support knowing what someone has done and visibility across micro supply chains who might be looking at something at the individual level we found that transparency in github allowed work to and when new collaborative actions introduce new progress and projects to evolve to become more general as a dependencies two way communications are required function of micro supply chain management because all artifacts are visible on the github site users of a particular signals of attention project can access its contents and are made aware of visible signals of attention provided notification of other changes to the project on a continuous basis this developers behavior on the github site interestingly awareness and visibility supported direct feedback and these signals of attention seemed to help users manage the interaction between project owners and their users creating downsides of transparency across a large scale network what we refer to as a micro supply chain visibility visible cues of what others were watching or commenting between the supplier project owner and consumer user helped developers find interesting or useful projects and meant that the owner could infer more clearly who their events in their words these signals when aggregated user base was how they were using the project and when also gave some users higher status because they indicated they were having problems consumers were notified about community approval or admiration as one user put it i m changes to the product meaning they could anticipate kind of giving them some token of my attention i m saying problematic modifications and provide immediate feedback i like what you re doing signals of attention about them once notified consumers could directly functioned to provide awareness of what other users cared communicate with the project owner about changes being about or were looking at this awareness is one aspect of made to discuss their consequences or request adaptations social translucence as described by that would suit their needs but they could also directly these signals of attention also in some cases motivated modify the product and customize it to suit their needs with behavior giving developers a feeling that someone cared or without direct communication if they so desired in about what they were doing this connects with the notion contrast to relatively static and sequential supply chain of accountability in social translucence and collective relationships what emerged was a far more interactive effort this affordance of transparency relates to research producer consumer relationship characterized by reciprocal investigating how working with others affects one own dependencies productivity through social pressure e g the flow of ideas and help our findings suggest that the visibility of communication occurs at the limits of transparency actions might act to facilitate information flows and help communication generally seemed to be a response to the giving both of which have important implications for the limits of transparency when the information and inferences quantity and quality of work afforded by transparency were insufficient for the purpose at hand users interacted when conflicts arose between two conclusion dependent projects or when negotiating modifications to in this work we examined how individuals interpreted and pull requests in each case communication seemed to made use of information about others actions on code in an open social software repository we found that four key to a range of workers and focused support for various task paid crowd work offers remarkable opportunities for for example anyone with access to the internet can improving productivity social mobility and the global perform micro tasks on the order of seconds using economy by engaging a geographically distributed platforms such as amazon mechanical turk while more workforce to complete complex tasks on demand and at skilled workers can complete multi hour tasks on scale but it is also possible that crowd work will fail to professional online marketplaces such as odesk or work for achieve its potential focusing on assembly line piecework months to solve r d challenges on open innovation can we foresee a future crowd workplace in which we platforms e g innocentive incentives and work structures would want our children to participate this paper frames also vary tremendously ranging from crowdsourcing the major challenges that stand in the way of this goal contests awarding prizes to winners e g programming drawing on theory from organizational behavior and tasks on topcoder to micro labor platforms that pay distributed computing as well as direct feedback from workers per task workers we outline a framework that will enable crowd work that is complex collaborative and sustainable the while not all jobs are amenable to being sent down a wire framework lays out research challenges in twelve major there are portions of almost any job that can be performed areas workflow task assignment hierarchy real time by the crowd we foresee a world in which crowd response synchronous collaboration quality control work continues to expand unlocking an incredible number crowds guiding ais ais guiding crowds platforms job of opportunities for careers and skilled work in online design reputation and motivation marketplaces however we also foresee a serious risk that crowd work will fall into an intellectual framing focused on author keywords low cost results and exploitative labor with diminished crowdsourcing crowd work organization design visibility and communication channels vis a vis traditional research vision workplaces workers may be treated as exchangeable and untrustworthy having low or static skill sets and strong acm classification keywords motivations to shirk workers may become equally cynical h k having fewer bonds enforceable contracts and power than introduction with traditional workplaces such concerns may grow crowdsourcing rapidly mobilizes large numbers of people ever sharper unless this trajectory is somehow altered to accomplish tasks on a global scale for example this work originally emerged from the question can we volunteer based collective projects such as wikipedia owe foresee a future crowd workplace in which we would want their success and longevity to the ongoing efforts of our children to participate we suggest that this question thousands of individual contributors around the world has a number of attractive properties as a banner around complementing volunteer based crowdsourcing a paying which to rally research as well as serving as an anchor to crowd work industry is now quickly growing in scope and ground speculation it is simple enough to convey ambition crowd work today spans a wide range of skill concisely involving an evaluative component that and pay levels with commercial vendors providing access everyone with or without children can make we intentionally keep the we ambiguous so that readers with permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are different values and cultural contexts may try on the not made or distributed for profit or commercial advantage and that copies question providing a conceptual lens easily refocused and bear this notice and the full citation on the first page to copy otherwise encourage discussion regarding the challenges this question or republish to post on servers or to redistribute to lists requires prior poses to deciding what is valuable and pride worthy work specific permission and or a fee cscw february san antonio texas usa what new services systems or features are needed for a copyright acm 1331 future of crowd work that the reader would be proud to see crowding out the competition february san antonio tx usa his or her children take on as their livelihood is this a volunteers typically engage in some form of paying work in desirable path for the next generation order to sustain themselves moreover we believe there will always be some forms of work needed by society looking toward that positive future in this paper we which are not amenable to gamification and volunteering contribute an analytic framework for research in crowd and for which demand will outstrip supply via unpaid work we lay out a vision for a possible future for crowd channels as such we are interested in developing a future work that entails of paid crowd work that extends paid work into the online worker considerations such as motivation feedback environment in addition we exclude offline crowd work and pay these may be addressed by mechanisms to such as day labor as it does not possess the same maintain reputation provide better interaction with opportunities for distribution and global scalability as requesters and increase motivation online work for the remainder of the paper we will use the requester considerations such as coordination task term crowd work to refer to the performance of online tasks decomposition and quality control these may be by crowd workers who are financially compensated by addressed through workflow mechanisms including requesters electronically mediated collaboration we intentionally focus coverage on areas that may be of our analytic framework is organized around a greater interest to the cscw community especially issues multidisciplinary survey of the literature that speaks to related to computer science psychology and organization these challenges and helps to envision a positive future we science we also draw on other important areas where also include specific comments from crowd workers we appropriate e g labor economics ethics law and surveyed in order to elicit their thoughts and suggestions acknowledge these as critical to the future economy many we translate our findings into a set of pragmatic design aspects of these issues lie beyond the traditional purview of considerations that we believe are crucial in guiding design scientists or designers e g labor regulations however we and motivating research in this field we are following in recognize that addressing them will be necessary for a the tradition of a set of research contributions which positive future of crowd work delineated design principles as part of a call for action pros and cons 98 102 crowd work has the potential to support a flexible crowd work workforce and mitigate challenges such as shortages of a variety of terminology is currently used in regard to experts in specific areas e g it work or geographical crowds e g crowdsourcing collective intelligence human locations for individuals crowd work also creates new computation serious games peer production and citizen opportunities for income and social mobility in regions of science 90 we focus this paper on paid online the world where local economies may be stagnant and local crowd work which we define here as the performance of governmental structures discourage investment tasks online by distributed crowd workers who are however crowd work can be a double edged sword financially compensated by requesters individuals groups equally capable of enhancing or diminishing the quality of or organizations in this sense crowd work is a socio workers lives we may see echoes of past labor abuses in technical work system constituted through a set of globally distributed crowd work extremely low pay for relationships that connect organizations individuals labor with marketplaces such as amazon mechanical technologies and work activities online crowd work turk reported to effectively pay an average of hour takes place in marketplaces that allow requesters to seek 126 with no benefits or worker protections the per workers and support workers in finding work for this task payment structure used in most crowd work markets is paper we surveyed a number of contemporary popular akin to piecework compensation in manufacturing crowd work platforms these platforms include general and can offer an invitation for gaming behavior which can purpose marketplaces e g mechanical turk odesk negatively influence quality moreover crowds can be freelancer crowdflower mobileworks manpower as deployed in the service of questionable goals to break well as markets for specific expertise e g topcoder captchas to mine gold in games and even potentially to utest while these platforms are intended for locate dissidents the recent film in time legitimate tasks these and other platforms are sometimes provided a pop culture depiction of how such a society appropriated for illegal or nefarious purposes e g gold might function where continual performance of menial farming captcha solving and crowdturfing tasks was literally required for worker survival many through our definition we necessarily omit a wide range of writers have painted similarly bleak pictures voluntary crowd work such as wikis games with a crowd work may also displace current workers and has the purpose captchas and citizen science potential to replace some forms of skilled labor with much has already been written about these systems e g unskilled labor as tasks are decomposed into smaller and 108 however not only is paid work the cornerstone smaller pieces tasks such as speech transcription and of our existing economy and labor markets but even copyediting are increasingly being accomplished with crowding out the competition february san antonio tx usa crowd labor and researchers have found that even some complex and expert tasks such as writing product design or translation may be amenable to novice crowd workers with appropriate process design and technological support this displacement is coupled to a new form of taylorism 141 in which organizations optimize cognitive efficiency at the expense of education and skill development taylorism yielded to more enlightened job design after several decades and protracted struggles by workers but given the short time commitment between figure current crowd work processes crowd worker and requester it is easy to imagine heightened exploitation and dehumanization crowdsourced labor markets can be viewed as large as scientists engineers and designers we can propose and distributed systems in which each person such as a worker evaluate new structures for crowd work and help imagine on mechanical turk is analogous to a processor that can and bring about more positive futures we can do so both solve a task requiring human intelligence in this way a through the intentional creation of desirable work crowdsourcing market could be seen as a loosely coupled environments as well as the cultivation of increased demand distributed computing system fleshing out this analogy for work and workers in particular we suggest a role for we develop here the beginnings of a framework for the researchers in conceptualizing and prototyping new forms future of crowd work that integrates the human aspects of of crowd work that go beyond the simple independent and organizational behavior with the automation and scalability deskilled tasks that are common today with the goal of of the distributed computing literature blazing a trail for organizations and platforms that will form both distributed organizations and computing systems face the foundation of future crowd work many common fundamental challenges in accomplishing envisioning future crowd work complex work key challenges in distributed computing how can we move towards a future of crowd work that is include partitioning computations into tasks that can be more attractive for both requesters and workers than done in parallel mapping tasks to processors and existing systems even more ambitiously can we design a distributing data to and between processors 25 132 future of crowd work that is more attractive and more many of these challenges map to coordination effective than traditional labor systems dependencies identified by malone crowston that also apply to human organizations below we discuss two current crowd work typically consists of small categories of overlap between coordination dependencies independent and homogenous tasks as shown in figure discussed in organizational science their analogs in workers are paired with an instance of each task to produce distributed computing and their implications for the an output such simple small scale work has engendered beginnings of a framework for the future of crowd work low pay piece rate reward structures in part due to the perception that workers are homogenous and unskilled the managing shared resources current model is also insufficient to support the complexity whenever a limited resource needs to be shared creativity and skills that are needed for many kinds of coordinating how that resource is allocated becomes professional work that take place today nor can it drive important allocating a fixed pool of workers to multiple factors that will lead to increased worker satisfaction such tasks that must be completed under a deadline is a classic as improved pay skill development and complex work example of managing shared resources malone structures crowston suggest a number of examples of task allocation mechanisms ranging from first come first serve theories of organizational behavior and distributed to markets to managerial decisions in distributed computing computing systems managing shared resources is of much professional work consists of complex sets of similarly vital importance tasks must be mapped to interdependent tasks that need to be coordinated across processors requiring functions to govern task partitioning individuals with different expertise and capabilities reorganization of this mapping must be possible as well for example producing a book an academic paper or a for example if a processor fails or takes a long time to new car all may involve many individuals working in return results e g mapreduce structured teams each with different skills and roles collaborating on a shared output to address these more managing producer consumer and task subtask complex goals we draw on concepts from both the relationships organizational behavior and the distributed in many situations one activity produces something computing literatures 132 we propose that required as input for another activity for example the structure of an article needs to be decided on before the crowding out the competition february san antonio tx usa sections can be written these same requirements exist in with fewer repercussions than in traditional organizations distributed computing in which tasks need to be scheduled such as to reference letters or work histories the worker so that they can be completed in the correct sequence and in power is also limited requesters do not make a long term a timely manner with data being transferred between commitment to the worker and endure few penalties if they computing elements appropriately deciding how to divide renege on their agreement to pay for quality work in a task into subtasks and managing those subtasks is also a distributed computing systems by contrast requesters challenging problem especially for complex and programmers have fewer problems with motivating and interdependent tasks this is true whether a directing their workers computers however machines manager in an organization is trying to plan a large project cannot match the complexity creativity and flexibility that or a programmer is trying to parallelize a complex task human intelligence manifests combining ideas from furthermore top down approaches in which a single person human and computer organization theories may thus e g the task creator specifies all subtasks a priori may not provide complementary benefits and address be possible or subtasks may change as the task evolves complementary weaknesses over using either alone crowd specific factors framework unlike traditional organizations in which workers possess figure presents a framework that integrates the challenges job security and managers can closely supervise and posed by managing shared resources such as assigning appropriately reward or sanction workers or distributed workers to appropriate tasks managing producer consumer computing systems in which processors are usually highly relationships such as decomposing tasks and assembling reliable crowd work poses unique challenges for both them into a workflow and crowd specific factors such as workers and requesters ranging from job satisfaction to motivation rewards and quality assurance many of its direction setting coordination and quality control for elements combine insights from organizational behavior example organizations can maintain high quality work and distributed computing for example the task through management worker incentives and sanctions decomposition and task assignment functions use both while some of these methods are available in crowd work human and computational processes e g how much to reward workers whether to reject their the goal of this framework is to envision a future of crowd work or impose a reputation penalty their power is work that can support more complex creative and highly attenuated due to factors such as lack of direct supervision valued work at the highest level a platform is needed for and visibility into their work behavior lack of nuanced and managing pools of tasks and workers complex tasks must individualized rewards and the difficulty of imposing be decomposed into smaller subtasks each designed with stringent and lasting sanctions since workers can leave particular needs and characteristics which must be assigned to appropriate groups of workers who themselves must be properly motivated selected e g through reputation and organized e g through hierarchy tasks may be structured through multi stage workflows in which workers may collaborate either synchronously or asynchronously as part of this ai may guide and be guided by crowd workers finally quality assurance is needed to ensure each worker output is of high quality and fits together because we are concerned with issues of design the technical and organizational mechanisms surrounding crowd work we highlight in the process model twelve specific research foci figure that we suggest are necessary for realizing such a future of crowd work these foci are grouped into three key dimensions foci relevant to the work process the computation guiding guided by and underlying the work and the workers themselves our foci overlap each other in places however in total they provide a wide ranging multidisciplinary view that covers current and prospective crowd work processes for example workflow techniques may be useful for handling the flow of documents through a set of tasks but the effectiveness of these techniques can be amplified through figure proposed framework for future crowd work clever job design that divides tasks and allocates incentives processes to support complex and interdependent work in a way that benefits both workers and requesters cf crowding out the competition february san antonio tx usa our model is based on empirical as well as theoretical common today such as aggregating multiple independent input in forming this model we gathered feedback from judgments through voting or majority rule complex tasks both task requesters and workers the authors of this paper have dependencies changing requirements and require have been requesters have designed crowd workflows and multiple types of expertise instead workflows are needed have worked for platform companies and so requester and that facilitate decomposing tasks into subtasks managing platform company issues are represented we wished to the dependencies between subtasks and assembling the also represent the voices of workers we chose one popular results while initial research has shown that enabling more crowdsourcing platform amazon mechanical turk and complex workflows can result in large differences in output asked questions of workers in the two countries with the quality even with small differences in instructions rewards largest number of crowd workers the united states and and task order we have barely begun to india who had completed and had approved more than understand the broader design space of crowd workflows tasks as enforced by the platform workers were paid related work traditional organizations have developed usd to comment on and add design suggestions for the expertise in workflow design and management the foci discussed below fifty two workers responded from division of labor is a core tenet of task coordination adam the us and from india four of the responses were smith in his classic the nature and causes of the wealth of removed from the sample because of incomplete or nations described the associated efficacy benefits inconsistent answers workers in india had a mean age of via division of labor a greater pool of agents can work in  workers in the us had a mean age of  parallel specialize for the tasks they perform and complete in india of workers were female and in the us an assignment with less time lost to switching tasks 10 were female in both countries responders had considerable coordination is difficult among distributed workers but experience the mean total lifetime tasks were organizational coordination techniques can be profitably  and  for indian and u s applied to crowd work e g 90 97 systems workers respectively the purpose of the survey was to and formal languages support workflows in traditional provide workers a vehicle through which they could firms ranging from purely computational to contribute their own insights in general the workers hybrid approaches where tasks are self selected and then responses were thorough and insightful and we have automatically routed onward integrated their ideas into this paper quoting their answers where appropriate even though our survey is informal and in the context of crowds workflow could involve a much relatively small in scale we believe that it enriches this larger scale of operation and a much more heterogeneous paper by providing a variety of workers perspectives on set of actors the design space ranges from massively the 12 topics we discuss next redundant independent tasks e g contests that choose one research foci entry 19 to highly serial processes with work passed in the sections below we survey and analyze the 12 from one worker to the next e g passing a task from research foci that comprise our model first we consider worker to worker for improvements recent systems the future of the work processes and how the work is and toolkits pursued a flare and focus approach for organized and accomplished second we consider the complex work by exploring a space of options and then integration of crowd work and computation including the drilling down to flesh out those options 150 symbiosis between human cognition artificial intelligence crowd workers can guide workflows as well ai and computationally enabled crowd platforms research proposal crowd workflows are still quite brittle finally we consider crowd workers and how we can and are most successful with highly targeted tasks to develop jobs reputation systems motivations and improve existing workflows we must experiment and incentives that will benefit them in each subsection we iterate on a large space of parameters instructions state our motivation and goals briefly review related work incentives and decompositions costs of doing so may be and propose research issues and themes reduced through models of worker behavior or by the future of crowd work processes encapsulating and reusing proven design patterns 73 increasing the value and meaning of crowd work will then we must push crowd workflows toward more general require that it move beyond simple deskilled tasks to tasks and wicked problems that have no clearly defined complex professional work in this section we focus on the solution rather than edit text for example crowd key challenges that must be met in order to enable complex workflows should be able to support complex goals such as crowd work processes designing workflows assigning creativity and brainstorming essay writing music tasks supporting hierarchical structure enabling real time composition or civic planning crowd workers reminded us crowd work supporting synchronous collaboration and in our survey responses that they also need help managing controlling quality their own workflows as they juggle tasks from different requesters workflow motivation goals complex crowd work cannot be task assignment accomplished using the simple parallel approaches that are motivation goals sharing limited resources requires crowding out the competition february san antonio tx usa coordination allocating a fixed pool of workers to integration 80 quality oversight of each others multiple tasks with deadlines is a classic example contributions and leader elections to represent ideally requesters will see their tasks completed quickly collective opinions odesk and mobileworks identify while workers are continuously employed with tasks that and empower workers to serve in leadership roles no match their interests in the worst case workers are comparative analyses of the effectiveness of these matched to tasks that are uninteresting or too difficult and approaches exist yet won t make the income they want or deserve research proposal the fluid nature of crowd work opens related work management scientists have developed the door to new kinds of hierarchy where workers transition techniques such as first come first serve task queues between roles continuously crowd management could markets and managerial decisions computer science entail a layered tree made up of worker leaders requesters research adds useful abstractions drawn from data machine learning systems and algorithms in such a setup partitioning os scheduling and failover e g participants might be leaf nodes workers in one job but currently workers are usually forced to sort queues by task managers in another realizing this vision will require volume and recency but some algorithms form teams improvements to the platforms e g odesk teams and the dynamically based on expertise creation of systems that take advantage of it alternatively organized groups of workers might begin applying to jobs research proposal task assignment has typically involved as a single entity but there may be resistance to hierarchy either a first come first served model e g the esp game one worker wrote i like the way it is there does not seem galaxy zoo 106 or a market model e g odesk to be a hierarchy in fact this is one of the most satisfying mechanical turk in either case task designers must guess aspects of mturk anyone can be their own boss this at the right combination of incentives and iterate until comment suggests the value of further empirical study of success this process is both time consuming and the willingness of crowd members to manage or be expensive better theoretical models markets or automatic managed by each other perhaps current forms of computational matching processes e g could organizational structure will yield to new ones in which the drastically reduce development costs and address search processes of managing and being managed will be more friction an important issue in labor economics the intertwined and are conceptually different than what we assignment of tasks in relation to individuals abilities has currently experience also been studied as part of business workflow research several workers in our survey complained that realtime crowd work they spent too much energy finding appropriate tasks one motivation goal for work with tight completion time suggested that platforms provide automatic constraints we will need to create flash crowds groups of recommendations of possible next tasks based on workers individuals who arrive moments after a request and can previous task choices these comments suggest a research work synchronously any application that wants to embed question are the workers or the platforms better suited to on demand crowdsourcing e g is limited by the manage task assignment that is should tasks be pulled by problem of crowd latency but current crowd tasks can take workers or pushed by platforms hours or days hierarchy related work fast recruitment has been the major research motivation goal hierarchy has become the primary thrust in realtime crowdsourcing so far early attempts were management strategy in traditional organizations it benefits motivated by time limited tasks such as searching for a coordination decision making quality control and assigns missing person and timed competitions in incentives and sanctions 92 hierarchies decompose paid crowdsourcing researchers began e mailing a set of large and complex tasks such as developing and workers the night before the study and announced a time for manufacturing an automobile by clarifying legitimate the experiment keeping workers busy with old tasks authority and workflow across organizations hierarchies in brought wait times down to a half minute to one minute crowd work could enable groups of workers to tackle new and paying workers a small wage to stay on call is classes of complex work increase efficiency and support enough to draw a crowd together roughly two to three consistency and integration hierarchies may also allow seconds later this technique can be modeled using workers to act more like teams for example developing queuing theory and adapted to bring together crowds in as accountability standards decision making and conflict little as milliseconds resolution processes and review policies research proposal the two core challenges for realtime related work volunteer crowdsourcing platforms have crowdsourcing will be scaling up to increased demand evolved their own hierarchies and decision making for realtime workers and making workers efficient processes appropriating techniques from other enough to collectively generate results ahead of time online communities where appropriate most paid deadlines what happens as more tasks need such approaches have workers make hierarchical decisions responses and as the size of the crowd increases is it collectively for example task decomposition and crowding out the competition february san antonio tx usa possible to support a large number of realtime in the extreme cheat or game the system for example crowdsourcing tasks competing for workers attention 30 or more of submissions on mechanical turk may be low quality one worker warned us people workers who arrive quickly can still be slow at completing collude to agree on wrong answers to trick the system work it is possible to design algorithms and that is quality filters based on consensus may be fooled by workflows to guide workers to complete synchronous tasks workers who agree to coordinate answers workers who quickly so far these techniques are restricted to particular have low expertise and requesters who provide unclear domains but general approaches may be possible instructions also contribute to subpar responses problems what would it take to begin with a sketch on a napkin find arise even for workers who are highly motivated these a designer to mock up interface alternatives a usability eager beavers often make well intentioned but analyst to test those prototypes and a front end engineer to counterproductive contributions in our survey implement the best one all in a single afternoon workers workers saw quality control as a major issue that affected seem interested in such tasks one suggested providing their compensation and they expressed a dislike for their more communications options with the requester peers who lowered quality standards through misbehavior something better than just emailing them like some sort of however many complained about requesters one said immediate chat another wrote i d like to see some sort too often the job itself is badly designed or is messed up of worker alert system on the dashboard for such events and there is a degree of misunderstanding between the synchronous collaboration worker and the job engineer motivation goal many tasks worth completing require related work of the research foci quality control has cooperation yet crowdsourcing has largely focused on arguably received the most attention so far approaches for independent work distributed teams have always faced challenges in cultural differences and coordination but quality control largely fall into two camps up front task crowd collaboration now must create rapport over much design and post hoc result analysis task design aims to shorter timescales e g one hour and possibly wider design tasks that are resistant to low quality work for cultural or socioeconomic gaps example requesters can split work into fault tolerant subtasks 75 apply peer review or agreement related work many attempts at collaborative crowd work filters 17 75 optimize instructions 72 highly structure the communication between participants and manipulate incentives for example crowds can solve distributed problems by observing the behavior of neighbors letting the system worker output approaches filter out poor quality work after choose which suggestions to focus on continuously results have been submitted workers results can be compared to gold standard data on a pre labeled set of the elect new leaders and pass on knowledge to new tasks 43 including gold standards can prevent members these techniques reduce the damage that a single poor or malicious crowd worker can do while also workers inherent biases from dominating the results limiting the types of collaboration possible they also set however authoring gold data can be burdensome and gold up opportunities for feedback and learning standards may not be possible for subjective or generative tasks e g writing an essay other common methods scale unstructured collaboration also shows promise for the influence of a submission according to how well that example by giving workers a task and placing them in worker agrees with others 36 or according to collaborative text authoring software these techniques the workers votes however recruiting multiple workers draw on synchronous collaboration research e g 66 costs more agreement may not be possible for subjective or generative tasks and the approach is susceptible to research proposal to shift from independent workers to collusion false identities are increasingly being teams of on demand collaborators we must revisit and created as an attack on quality assurance methods extend traditional cscw work on distributed teamwork short periods of intense crowd collaboration call for fast a promising approach that addresses some worker output teambuilding and may require the automatic assignment of issues examines the way that workers do their work rather group members to maximize collective intelligence 149 than the output itself using machine learning and or finally it will be a major research undertaking to invent visualization to predict the quality of a worker output and describe the tasks and techniques that succeed with from their behavior similar but simpler synchronous collaboration approaches provide requesters more visibility into worker quality control behavior such as odesk worker diary which periodically motivation goal quality problems are a serious challenge takes snapshots of workers computer screens while to the mass adoption of crowd work the most appealing powerful such techniques must address privacy and aspects of crowd work such as high throughput low autonomy concerns if widely deployed transaction costs and complex subjective tasks also research proposal while quality control is improving for make it susceptible to quality control issues workers tasks with a closed set of possible answers we still have satisfice minimizing the amount of effort they expend and few techniques for open ended work and highly skilled crowding out the competition february san antonio tx usa tasks is it possible to robustly gauge workers skills at combination of active learning and semi supervised tasks such as audio engineering art critique or poetry learning to collect the most informational labels should we rely on peer evaluation or data mine low level ais guiding crowds activity to predict output quality do quality metrics map motivation goal while large groups are increasingly adept well across different marketplaces at completing straightforward parallel tasks 139 they in the long term we must move from reducing poor quality can struggle with complex work participants have varied work to scaffolding truly excellent work to do so we can skill levels even well intentioned contributions can optimize workflows for creativity innovation and introduce errors and errors are amplified as they propagate discovery this vision will involve recruiting experts through the crowd it is possible to integrate crowds directly metrics that enable us to evaluate such factors and reward inside of software 15 17 and use the software to help structures that value high quality artifacts guide crowd work for example a machine learning model can determine which work products may still be improved the future of crowd computation and then assign workers most likely to make such crowd labor is already mediated by computation however improvements these systems may also be able to computation has the opportunity to step into a much more predict their expertise needs in advance then train and active role in helping recruit and manage workers as well as adapt workers in an online fashion via automated tutoring contribute directly to work processes hybrid human or peer learning 18 computer systems could tap into the best of both human and machine intelligence we structure our discussion around related work computational approaches for designing and the potential for reciprocal benefits between crowd workers integrating workflow incentive and instruction patterns and computational systems crowds guiding ais considers have shown promise as well as techniques that how crowd intelligence can help train supervise and trade off the strengths of crowds and artificial intelligence supplement automation ais guiding crowds considers how ais could also serve as a reflective aids machine intelligence can help make the crowd more encouraging the crowd to learn by pointing out what others efficient skilled and accurate finally we also consider the have done in similar contexts e g design and evaluation of crowdsourcing platforms research proposal the research community should crowds guiding ais examine whether algorithmic management is an motivation goal in human computation people act as improvement over traditional organization management computational components and perform the work that ai techniques in an algorithmic organization crowds will need systems lack the skills to complete by tapping into to be able to raise exceptions as well as halt and restart crowd intelligence computational systems can support a processes and ais will need to know when then can much broader set of tasks it should be understood that this proceed when they need human help and when they need area out of all areas least excites crowd workers and to help the workers in addition workers should be able to perhaps with good reason crowd workers may end up modify their supporting ais as needed in the end both training machines to replace them workers and ais should improve to accomplish this we related work paid crowds have gathered large amounts of will need to move from a setting where simple ais data to train algorithms for example crowds can match completely determine a workflow to a richer mixed expert annotations on natural language processing tasks initiative setting where crowds and ais jointly teach each such as word sense disambiguation generate other and jointly control the work process speech corpora for spoken language research crowdsourcing platforms annotate objects and people in images and help motivation goal crowdsourcing platforms provide the with graphics tasks such as identifying depth layers central nexus between requesters and workers as a result crowds can also solve algorithmic problems such as graph platform design offers the opportunity to change our coloring 91 perceptions and understanding of crowd work in general as well as shape the relationships and practical interactions research proposal while algorithms will continue to between workers and requesters in practice while several benefit from crowd generated training data there are platforms already exist limiting ourselves to existing opportunities to integrate crowds more deeply into platforms greatly restricts the scope and nature of change algorithms rather than treating crowd data as ground truth we can enact by contrast novel platforms can help drive labels it may be profitable to understand and model the the diffusion of new designs and techniques for crowd biases and intuitions that human cognition brings it work may be possible to design machine learning algorithms that more deeply understand the human nature of these labels related work platform research tends to optimize existing algorithms may also more directly model the tradeoff processes or reach out to new populations for example between cost and performance for example by using a crowdflower experiments on its own gold standard metrics to minimize the number of times a question is asked crowding out the competition february san antonio tx usa platforms may also manage a large number of real time games may reduce boredom entertainment represents a requests or route tasks to ensure that all tasks have steady fairly superficial form of work satisfaction we believe the streams of incoming workers mobileworks promotes future of crowd work depends on creating jobs that achieve its own workers to management positions based on their both organizational performance and worker satisfaction performance to find new populations mobileworks related work in traditional firms when managers design and mclerk engage with the developing world through jobs that provide skill variety task identity and task specifically tailored mobile phone interfaces 100 significance workers find their jobs more meaningful incentives can be used to recruit local experts as well for when workers have autonomy in their jobs and receive example a vending machine served up exam grading tasks feedback they experience increased responsibility for and and dispersed candy as a reward understanding of the impact of their work combined these research proposal while designing and building a new lead to increased performance for the firm and reduced platform may seem daunting examples like brin and employee problems such as absenteeism and turnover page google 21 show that two graduate students can unfortunately many paid crowd work platforms do not disrupt the commercial landscape and dramatically change provide as much skill variety task identity and task the way people work we challenge the community to significance as volunteer platforms such as wikipedia similarly revolutionize our conception of what a there are direct payoffs when requesters convey the crowdsourcing platform is and can achieve innovation identity and significance of tasks to crowd workers should articulate a vision for crowd work that is effective timely and task specific feedback from peers and efficient and fair beyond mere technology negotiating the requesters as well as opportunities for self assessment help balance of power between interested parties is central to workers to learn persevere and produce better results platforms and markets as crowd work already faces challenges related to power inequalities similar to research proposal an ideal crowd work system would those encountered in offline labor markets future platforms allow workers to complete a whole and identifiable piece of may shape or be constrained by future regulatory work in a way that is satisfying and measurable cf intervention on for example the use of independent the system would explain the significance of the job offer contracting for regular recurring work beyond peer to peer and expert feedback and encourage self money platforms might also support labor exchanged for assessment these systems could offer a variety of ways to virtual goods or performed in virtual environments which complete the task and thereby not only provide autonomy raises additional policy issues privacy is another issue for the worker but also reduce errors achieving this how can platforms disclose enough information to be vision will require communication with workers about the trusted as a source of worker quality while also maintaining scope and impact of their work as well as verification that privacy experience with markets such as ebay and workers understand their impact amazon suggests that greater transparency may be helpful providing more context has tradeoffs however on the one but such mechanisms must be carefully managed to avoid hand more context enables workers to better judge how the abuse fruits of their effort will be used so they can make informed security concerns will also continue to grow in importance decisions about whether or not to perform work on the for example recognition of trusted workers may lead to other hand reducing context may streamline work leading identity theft and fraudulent use of compromised accounts to greater efficiencies for both requesters and workers security research must consider both new attacks on moreover requesters may not always want to fully disclose platforms and use of platforms for launching attacks the context of work due to privacy security or intellectual the future of crowd workers property concerns this suggests a need to balance distinct crowd work involves a partnership between requesters and and competing concerns how much information a worker workers thus when designing the future of crowd work it needs before consenting to provide labor how much is important to develop tools to support not only the work information should be shared to motivate and retain itself but also those performing the work below we workers how to share context without introducing identify and discuss three important research challenges for inefficiency and how much information may be legitimately supporting the crowd workers of the future job design withheld to protect interests of requesters reputation and credentials and motivation and rewards reputation and credentials job design motivation goals in traditional firms reputation and motivation goals some tasks that need to be done are just credentials e g letters of reference certifications work dull motivating workers to accomplish such tasks can be history are critical to recruiting creating lasting rewards challenging and may lead to reduced engagement with the and sanctions and managing work quality for example system it would be better if some of the task assignments institutions with brand names such as google and apple are weren t so monotonous i don t see the long term payoff likely to attract software engineers to apply for jobs and in and it discourages me while dressing up such tasks as turn such names on the resumes of software engineers crowding out the competition february san antonio tx usa attract other prospective employers however many complete piecework by small monetary rewards or highly existing crowd work systems have only coarse reputation skilled professionals who work on large higher paid tasks and credentialing systems for example a worker history without management oversight however crowd workers on mechanical turk primarily measures the percentage of are a diverse and multifaceted population with a range of work that has been approved in contrast traditional motives and experiences many workers in micro task employers judge a prospective employee education and environments are ambitious individuals desiring to be work history through a variety of instruments including ceos or top rate school teachers yet few researchers interviews transcripts and references likewise applicants have grappled with the diversity and richness of the motives can investigate employers reputation and crowd workers of the individuals comprising the crowd one worker in our survey wanted better reputation rankings for reminded us that requesters as well as workers need to be employers to be established within the platform 130 motivated related work robust mechanisms for supporting trust we could definitely use more motivation we assurance and accountability can support volunteer based perform task for mere pennies mturk should crowds and collaboration online likewise in the encourage and reward requesters that provide context of monetary rewards reputation can have serious clear instructions quick payment and higher financial consequences motivating people to pay rewarding them would create more manipulate systems for their own benefit for example worthwhile tasks that we would take more workers may build networks of sybil identities that is seriously and work hard on good credible create pseudonyms to enhance their reputations and hits are few and far between to foil quality control methods workers may also boost related work research in psychology sociology their ratings by agreeing implicitly or explicitly to management and marketing provide insights into human recommend each other reciprocally the design and motivation that are applicable to crowd work management evaluation of future crowd work reputation systems must research illustrates the challenge of clearly understanding address these issues communicating and rewarding desired behavior research proposal the core challenge with reputation is workers seek to understand which activities are rewarded trading off the benefits of pseudonymous low transaction and then tend to do those activities to the virtual exclusion cost hiring with the richer but higher transaction cost hiring of others other studies find mixed results on the effect of decisions most firms make today for example employers financial incentives on the quality of workers outputs and know little about workers on mechanical turk but such underscore the performance and satisfaction benefits of workers can be hired and engaged in work nearly harnessing intrinsic motivations in task design such as non instantaneously meanwhile platforms such as odesk with financial awards and recognition meaningfulness of tasks richer reputation systems incur correspondingly higher and the feeling of contributing towards the greater good transaction costs in hiring workers involving negotiation 72 115 and handshakes between workers and employers past research suggests requesters should clearly to address this challenge reputation systems will need to be understand and communicate desired behaviors robust to cheating and gaming while preserving the benefits understand and align worker motivations and incentives of pseudonymity and supporting low transaction cost with these desired behaviors and 3 design the requests hiring one possible solution would be to create a web of and incentive structures in order to achieve both effective trust in which requesters and workers validate each other as task completion and worker satisfaction this also requires trustworthy however malicious workers and requesters to understand variations in crowd workers requesters can infiltrate the community and spread their motivations e g 11 for example in own web of trust interfaces may support detection of bad competence enjoyment connectedness prosocial actors for example by highlighting topology orientation and autonomy statistical patterns 93 and behavior research proposal the future of crowd work requires that the creation of technical tools for sharing information requesters and platform designers consider a broad set of about workers should be coupled with more robust systems motivations not just financial incentives we must create for monitoring and reporting requester abuses lastly frameworks that acknowledge the dynamic nature of these initiatives towards enhanced reputation systems motivation and its dependence on context for example it should be balanced with the need to preserve privacy as is not clear that payment alone will be the optimal well as the potential payoffs of anonymous collaboration motivator for expert crowdsourcing markets such for both workers and requesters frameworks should enable us to move from analysis to design of new motivational schemes research should motivation and rewards overcome the dichotomous emphases on dehumanizing motivation goals requesters often envision crowd workers piece work and frictionless virtual collaboration in order to as either anonymous individuals who are motivated to provide a more holistic framework within which to crowding out the competition february san antonio tx usa understand and build systems that support workers diverse examples of what is expected task interfaces may be motivations e g 50 52 81 82 107 poorly designed or even have bugs that make it impossible to complete a task gold labels used as trap questions may next steps many of the really difficult problems that workers and be far more subjective than requesters realize leading to requesters face will require advances on multiple foci at the mistaken rejection of work same time below we describe three design goals what might we do to address these problems designers demonstrating how the integration of multiple foci can lead can make it easier and faster for requesters to create to concrete next steps and calls to actions that will create a effective tasks for example platforms might provide task better environment for crowd workers and a better set of templates showing examples of proven task designs to human resources for requesters encourage shared mental models and improve quality create career ladders motivation job design assurance platforms could also help educate requesters reputation hierarchy about the impact of job design and task assignment on crowd work today is largely a dead end job offering few resulting quality best practices and common errors to be opportunities for career advancement and economic avoided platforms might even offer a first pass service in mobility as workers demonstrate aptitude and diligence it which a set of trusted workers known to the platform test is to the advantage of both requesters and workers to out the task and report any issues encountered recognize workers potential to take on new tasks requiring platforms might also provide a wider array of greater knowledge effectiveness and responsibility as all communication channels between requesters and workers organizations benefit from making the best use of available supporting synchronous collaboration and real time crowd talent and workers diverse abilities more skilled workers work workers we surveyed were adamant that the should be paid for their expertise and encouraged to train perception of poor crowd work quality was due at least in less skilled workers for example mobileworks promotes part to unclear instructions and insufficient feedback and workers to management jobs based on performance that they need more guidance to better understand what is as workers demonstrate proficiency they might be invited expected they suggested instant chat with requesters to to create gold labels for verifying the quality of work from clarify jobs though we note this would require the other less established workers which could be used to continual presence of requesters while work is being build webs of trusted workers proficient trusted workers performed they also requested feedback during or just could also manage other workers respond to reported after a task both would be consistent with the practices of issues and provide first pass triage of new task designs to good managers and workers in other labor contexts and so catch problems or make suggestions before new tasks go more experimenting with channels of communication could live crowd workers could eventually become employees potentially have a large effect on both worker satisfaction themselves or develop the skills needed to launch their own and job quality as a first step requesters might provide business using crowd work for example a career trajectory ways for workers to clarify tasks in real time with the might proceed as entry level untrusted worker 2 requester or with a more experienced worker and when trusted worker 3 hourly contractor employee first work is in progress they might provide informal feedback steps toward building such a ladder include studying worker through these same channels motivations in order to develop better job designs creating facilitate learning quality assurance ais guiding lasting and transferable mechanisms for reputation and crowds crowds guiding ais task assignment reputation credentialing for workers and building greater support for and credentials platform hierarchy in the form of structured teams that provide crowd work naturally involves learning and assessment training for novices by skilled workers workers may need to acquire new skills to perform unfamiliar tasks before or in the midst of performing the improve task design through better communication quality assurance job design task assignment realtime actual work workers may also polish and refine existing crowd work synchronous collaboration platform skills while completing more familiar tasks requesters there is a popular myth that the poor quality of some crowd must continually engage in quality assurance such a work stems largely from workers being lazy stupid or training assessment cycle of work offers potentially deceitful in practice both we and our surveyed workers exciting synergies with online education by doing for have observed many cases where poor quality work instead example duolingo duolingo com explores this direction arises from poorly designed crowdsourcing tasks for for foreign language learning this idea can be generalized example a requester might assume a task obvious to them much further for example content generation tasks could should be equally obvious to everyone else however even be designed to better assess and enhance writing skills a highly educated workers may have difficulty understanding self sustaining cycle might involve ais guiding crowds on exactly what the requester actually wants task instructions which tasks to complete task assignment depending on the are often incomplete or ambiguous do not address worker and requester skill development and quality boundary cases and do not provide clarifying input output assurance goals and then using the crowd generated data to automate some of the simpler tasks crowds guiding ais crowding out the competition february san antonio tx usa the potential of crowd work based education is enormous and also close observation of its effects in crowd work we and multi faceted benefiting all parties by producing more have two important affordances an ability to constitute new skilled and employable workers online tutoring systems forms of organization in short amounts of time and an perhaps augmented with human tutoring could provide a ability to situate these organizations in an experimental path toward delivering more scalable education to the context while organization science has been built slowly public at large 146 moreover tracking and mining of based on observation the proliferation of crowd work work history could support personalized instruction and makes large scale organizational experiments comparing feedback as well as recommending new tasks and learning distinct management strategies and task designs possible modules as workers master new skills and are assessed these comparisons will help us understand how to improve badges or credentials could document this proficiency so crowd platforms workers skills and requesters that others can recognize and utilize this enhanced skill set assignments perhaps instead of a hadron collider the field platforms themselves can also be an important element of of crowd work needs a social collider in which different learning data from crowd work can reveal what kinds of forms of organization can be tested the goal should be requests attract talented workers patterns of learning and better systems better requests better work and better skill building among workers over time the valuation and experience we hope the community observational interaction between extrinsic and intrinsic rewards and experimental design and technical skills will play a vital which types of tasks are most appropriate for which types role in shaping the future of crowd work and the next of workers generation of workers the problem space for this scenario was very open ended there was no identifiable problem that needed to be improved or fixed alternatively the new wap technology provided op portunities to create new facilities and experiences for people one of the main assumptions is that people want to be kept informed of up to the minute news sports stocks and share prices wherever they are other assumptions included that people want to be able to decide what to do in an evening while on their way home from work checking listings movies making restaurant reservations that people want to be able to interact with information on the move reading on the train that users are prepared to put up with a very small display and will be happy browsing and interacting with information using a restricted set of commands via a small number of tiny buttons that people will be happy doing things on a mobile phone that they normally do using their reading surfing the web playing video games doing their shopping it is reasonable to assume that people want flexibility they like to be able to find out about news and events wherever they are just look at the number of people who take a radio with them to a soccer match to find out the scores of other matches being played at the same time people also like to use their time productively when traveling as in making phone calls thus it is reasonable to assume they would like to read and send on the move the most troublesome assumption is whether people are prepared to interact with the range of services proposed using such a restricted mode of interactivity in particular it is questionable whether most people are prepared to give up what they have been used to large screen estate ability to type messages using a normal sized keyboard for the flexibility of having access to very restricted internet based information via a cell phone they can keep in their pocket one of the benefits of working through your assumptions for a problem space before building anything is that it can highlight problematic concerns in so doing it can identify ideas that need to be reworked before it becomes too late in the de sign process to make changes having a good understanding of the problem space can also help greatly in formulating what it is you want to design another key as pect of conceptualizing the problem space is to think about the overall structure of what will be built and how this will be conveyed to the users in particular this in volves developing a conceptual model conceptual models the most important thing to design is the user conceptual model everything else should be subordinated to making that model clear obvious and substantial that is almost exactly the opposite of how most software is designed david liddle p chapter understanding and interaction by a conceptual model is meant a description of the proposed system in terms of a set of integrated ideas and concepts about what it should do behave and look like that will be understandableby the users in the manner intended to develop a conceptual model involves envisioning the proposed product based on the users needs and other requirements identified to ensure that it is designed to be understandable in the manner intended requires doing iterative testing of the product as it is developed a key aspect of this design process is initially to decide what the users will be doing when carrying out their tasks for example will they be primarily searching for information creating documents communicating with other users recording events or some other activity at this stage the interaction mode that would best support this needs to be considered for example would al lowing the users to browse be appropriate or would allowing them to ask questions directly to the system in their native language be more effective decisions about which kind of interaction style to use whether to use a menu based system speech input commands should be made in relation to the interaction mode thus decisions about which mode of interaction to support differ from those made about which style of interaction to have the former being at a higher level of abstraction the former are also concerned with determining the nature of the users activities to support while the latter are concerned with the selection of specific kinds of interface once a set of possible ways of interacting with an interactive system has been identified the design of the conceptual model then needs to be thought through in terms of actual concrete solutions this entails working out the behavior of the interface the particular interaction styles that will be used and the look and feel of the interface at this stage of fleshing out it is always a good idea to explore a number of possible designs and to assess the merits and problems of each one another way of designing an appropriate conceptual model is to select an in terface metaphor this can provide a basic structure for the conceptual model that is couched in knowledge users are familiar with examples of well known interface metaphors are the desktop and search engines which we will cover in section interaction paradigms can also be used to guide the formation of an appropriate conceptual metaphor they provide particular ways of thinking about interaction design such as designing for desktop applications or ubiquitous computing these will also be covered in section as with any aspect of interaction design the process of fleshing out conceptual models should be done iteratively using a number of methods these include sketching out ideas describing possible scenarios and prototyping aspects of the proposed behavior of the system all these methods will be covered in chapter which focuses on doing conceptual design here we describe the dif ferent kinds of conceptual models interface metaphors and interaction paradigms to give you a good understanding of the various types prior to thinking about how to design them conceptual models there are a number of different kinds of conceptual models these can be bro ken down into two main categories those based on activities and those based on objects conceptual models based on activities the most types of activities that users are likely to be engaged in when in teracting with systems are instructing conversing manipulating and navigating exploring and browsing a first thing to note is that the various kinds of activity are not mutually exclusive as they can be carried out together for example it is possible for someone to give instructions while conversing or navigate an environment while browsing how ever each has different properties and suggests different ways of being developed at the interface the first one is based on the idea of letting the user issue instruc tions to the system when performing tasks this can be done in various interaction styles typing in commands selecting options from menus in a windows environ ment or on a touch screen speaking aloud commands pressing buttons or using a combination of function keys the second one is based on the user conversing with the system as though talking to someone else users speak to the system or type in questions to which the system replies via text or speech output the third type is based on allowing users to manipulate and navigate their way through an environ ment of virtual objects it assumes that the virtual environment shares some of the properties of the physical world allowing users to use their knowledge of how physical objects behave when interacting with virtual objects the fourth kind is based on the system providing information that is structured in such a way as to allow users to find out or learn things without having to formulate specific ques tions to the system a company is building a wireless information system to help tourists find their way around an unfamiliar city what would they need to find out in order to develop a conceptual model comment to begin they would need to ask what do tourists want typically they want to find out lots of things such as how to get from a to b where the post office is and where a good chi nese restaurant is they then need to consider how best to support the activity of requesting information is it preferable to enable the tourists to ask questions of the system as if they were having a conversation with another human being or would it be more appropriateto allow them to ask questions as if giving instructions to a machine alternatively would they prefer a system that structures information in the form of lists maps and recommendations that they could then explore at their leisure chapter understanding and conceptualizing interaction instructing this kind of conceptual model describes how users carry out their tasks through in structing the system what to do examples include giving instructions to a system to perform operations like tell the time print a file and remind the user of an ap pointment a diverse of devices has been designed based on this model in cluding vcrs hi fi systems alarm clocks and computers the way in which the user issues instructions can vary from pressing buttons to typing in strings of char acters many activities are readily supported by giving instructions operating systems like unix and dos have been specifically designed as com mand based systems to which the user issues instructions at the prompt as a com mand or set of commands in windows and other gui based systems control keys or the selection of menu options via a mouse are used well known applications that are command based include word processing and cad typically a wide range of functions is provided from which users choose when they want to do some thing to the object they are working on for example a user writing a report using a word processor will want to format the document count the numbers of words typed and check the spelling the user will need to instruct the system to do these opera tions by issuing apprbpriate commands typically commands are carried out in a se quence with the system responding appropriately or not as instructed one of the main benefits of an instruction based conceptual model is that it supports quick and efficient interaction it is particularly suited to repetitive kinds of actions performed on multiple objects examples include the repetitive actions of saving deleting and organizing messages or files there are many different kinds of vending machines in the world each offers a range of goods requiring the user initially to part with some money figure shows photos of two different vending machines one that provides soft drinks and the other a range of snacks both support the interaction style of issuing instructions however the way they do it is quite different what instructions must be issued to obtain a can of soft drink from the first machine and a bar of chocolate from the second why has it been necessary to design a more complex mode of interaction for the second vending machine what problems can arise with this mode of interaction comment the first vending machine has been designed on a very simple instruction based conceptual model there are a small number of drinks to choose from and each is represented by a large button displaying the label of each drink the user simply has to press one button and hopefully this will have the effect of returning the selected drink the second machine is more complex offering a wider range of snacks the trade off for providing more choices however is that the user can no longer instruct the machine by using a simple one press ac tion but is required to use a more complex process involving reading off the code under the item chosen then ii keying this into the number pad adjacent to the dis played items and iii checking the price of the selected option and ensuring that the amount of money inserted is the same or more depending on whether or not the machine provides change problems that can arise from this mode of interaction are the customer conceptual models figure two vending machines a one selling soft drinks b the other selling a range of snacks misreading the code and or mistyping in the code resulting in the machine not issuing the snack or providing the wrong sort a better way of designing an interface for a large number of choices of variable cost is to continue to use direct mapping but use buttons that show miniature versions of the snacks placed in a large matrix rather than showing actual versions this would use the available space at the front of the vending machine more economically the customer would need only to press the button of the object chosen and put in the correct amount of money much research has been carried out on how to optimize command based and other instruction giving systems with respect to usabilty goals the form of the commands the use of abbreviations full names icons labels their syntax how best to combine different commands and their organization how to structure options in different menus are examples of some of the main areas that have been investigated shneiderman in addition various cogni tive issues have been investigated that we will look at in the next chapter such as the problems people have in remembering the names of a set of commands less chapter understanding and conceptualizinginteraction research has been carried out however on the best way to design the ordering and sequencing of button pressing for physical devices like cell phones calculators re mote controls and vending machines another ubiquitous vending machine is the ticket machine typically a number of instruc tions have to be given in a sequence when using one of these consider ticket machines de signed to issue train tickets at railway stations how often have you or the person in front of you struggled to work out how to purchase a ticket and made a mistake how many in structions have to be given what order are they given in is it logical or arbitrary could the interaction have been designed any differently to make it more obvious to people how to issue instructions to the machine to get the desired train ticket comment ticketing machines vary enormously from country to country and from application to appli cation there seems to be little attempt to standardize therefore a person knowledge of the eurostar ticketing machine will not be very useful when buying a ticket for the sydney monorail or cinema tickets for the sometimes the interaction has been designed to get you to specify the type of ticket first adult child the kind of ticket single re turn special saver then the destination and finally to insert their money others require that the user insert a credit card first before selecting the destination and the type of ticket conversing this conceptual model is based on the idea of a person conversing with a system where the system acts as a dialog partner in particular the system is designed to respond in a way another human being might when having a conversation with someone else it differs from the previous category of instructing in being intended to reflect a more two way communication process where the system acts more like a partner than a machine that simply obeys orders this kind of conceptual model has been found to be most useful for applications in which the user needs to find out specific kinds of information or wants to discuss issues examples include advi sory systems help facilities and search engines the proposed tourist application described earlier would fit into this category the kinds of conversation that are supported range from simple voice recognition menu driven systems that are interacted with via phones to more complex guage based systems that involve the system parsing and responding to user queries typed in by the user examples of the former include banking ticket booking and train time inquiries where the user talks to the system in single word phrases yes no three in response to prompts from the system examples of the latter include search engines and help systems where the user types in a specific query how do i change the margin widths to which the system responds by giving various answers a main benefit of a conceptual model based on holding a conversation is that it allows people especially novices to interact with a system in a way they are already familiar with for example the search engine ask jeeves for kids allows chil dren to ask a question in a way they would when asking their teachers or rather than making them reformulate their question in terms of key words and boolean logic a disadvantage of this approach however is the misunderstandings that can arise when the search engine is unable to answer the child question in the conceptual models you asked how many legs does a have jeeves knows these answers where can i find a definition for the math term leg where can i find a concise article on centipedes where can i see an image of the human appendix why does my leg or other limb fall asleep where can i find advice on controlling the garden pest millipedes and centipedes figure theresponsefrom ask ources from britannica com on jeevesfor kids search engine when asked how many legsdoesacen tipede have way the child expects for example a child might type in a seemingly simple question like how many legs does a centipede have which the search engine finds difficult to answer instead the search engine replies by suggesting a number of possible sites that may be relevant but as can be seen in figure can be off the mark another problem that can arise from a conversational based conceptual model is that certain kinds of tasks are transformed into cumbersome and one sided interactions this is especially the case for automated phone based systems that use auditory menus to advance the conversation users have to listen to a voice providing several options then make a selection and repeat through further layers of menus before accomplishing their goal reaching a real human pay ing a bill here is the beginning of a dialog between a user who wants to find out about car insurance and an insurance company reception system dials an insurance company welcome to st paul insurance company press if new customer if you an existing customer presses thank you calling st paul company if you house insurance press insurance travel insurance health insurance press other press presses you have reached the car insurance division if you re quire information about fully comprehensive insurance press insurance press chapter understanding and conceptualizingintera ion a recent development based on the conversing conceptual model is animated agents various kinds of characters ranging from real people appearing at the interface videoed personal assistants and guides to cartoon characters virtual and imaginary creatures have been designed to act as the partners in the conversation with the system in so doing the dialog partner has become highly visible and tangible appearing to both act and talk like a human being or crea ture the user is able to see hear and even touch the partner when it is a physi cal toy they are talking with whereas with other systems based on a dialog partner help systems they can only hear or read what the system is saying many agents have also been designed to exhibit desirable human like qualities humorous happy enthusiastic pleasant gentle that are conveyed through facial expressions and lifelike physical movements head and lip movements body movements others have been designed more in line with disney like car toon characters exhibiting exaggerated behaviors funny voices larger than life facial expressions animated agents that exhibit human like or creature like physical behavior as well as talk can be more believable the underlying conceptual model is con veyed much more explicitly through having the system act and talk via a visible agent an advantage is that it can make it easier for people to work out that the in terface agent or physical toy they are conversing with is not a human being but a synthetic character that has been given certain human qualities in contrast when the dialog partner is hidden from view it is more difficult to discern what is behind it and just how intelligent it is the lack of visible cues can lead users into thinking it is more intelligent than it actually is if the dialog partner then fails to understand their questions or comments users are likely to lose patience with it moreover conceptual models they are likely to be less forgiving of it having been fooled into thinking the dialog partner is more intelligent than it really is than of a dialog partner that is repre sented as a cartoon character at the interface having only assumed it was a simple partner the flip side of imbuing dialog partners with a physical presence at the in terface however is that they can turn out to be rather annoying for more on this topic see chapter manipulating and navigating this conceptual model describes the activity of manipulating objects and navigat ing through virtual spaces by exploiting users knowledge of how they do this in the physical world for example virtual objects can be manipulated by moving select ing opening closing and zooming in and out of them extensions to these actions can also be included such as manipulating objects or navigating through virtual in ways not possible in the real world for example some virtual worlds have been designed to allow users to teleport from place to place or to transform one object into another a well known instantidtion of this kind of conceptual model is direct manip ulation according to ben shneiderman who coined the term manipulation interfaces possess three fundamental properties continuous representation of the objects and actions of interest rapid reversible incremental actions with immediate feedback about the object of interest physical actions and button pressing instead of issuing commands with complex syntax benefits of direct manipulation interfaces include helps beginners learn basic functionality rapidly experienced users can work rapidly on a wide range of tasks infrequent users can remember how to carry out operations over time no need for error messages except very rarely users can immediately see if their actions are furthering their goals and if not do something else experience less anxiety users gain confidence and mastery and feel in control apple computer inc was one of the first computer companies to design an op erating environment using direct manipulation as its central mode of interaction the highly successful macintosh desktop demonstrates the main principles of di rect manipulation see figure to capitalize on people understanding of what happens to physical objects in the real world they used a number of visual and auditory cues at the interface that were intended to emulate them one of chapter figure original macintosh desktop interface their assumptions was that people expect their physical actions to have physical results so when a drawing tool is used a corresponding line should appear and when a file is placed in the trash can a corresponding sound or visual cue show ing it has been successfully thrown away is used apple computer inc a number of specific visual and auditory cues were used to provide such feedback including various animations and sounds shrinking and expanding icons ac companied with shhhlicc and sounds to represent opening and closing of files much of this interaction design was geared towards providing clues to the user to know what to do to feel comfortable and to enjoy exploring the interface many other kinds of direct manipulation interfaces have been developed in cluding video games data visualization tools and cad systems virtual environ ments and virtual reality have similarly employed a range of interaction mechanisms that enable users to interact with and navigate through a simulated physical world for example users can move around and explore aspects of a environment the interior of a building while also moving objects around in the virtual environment rearranging the furniture in a simulated living room figure on color plate shows screen shots of some of these while direct manipulation and virtual environments provide a very versatile mode of interaction they do have a number of drawbacks at a conceptual level some people may take the underlying conceptual model too literally and expect certain things to happen at the interface in the way they would in the physical world a well known example of this phenomenon is of new mac users being conceptual models fied of dragging the icon of their floppy disk to the trash can icon on the desktop to eject it from the computer for fear of deleting it in the same way files are when placed in the trash can the conceptual confusion arises because the designers opted to use the same action dropping on the same object trash can for two completely different operations deleting and ejecting another problem is that not all tasks can be described by objects and not all actions can be done directly some tasks are better achieved through issuing instructions and having textual descrip tions rather than iconic representations imagine if messages were repre sented as small icons in your mailbox with abbreviations of who they were from and when they were sent moreover you could only move them around by drag ging them with a mouse very quickly they would take up your desk space and you would find it impossible to keep track of them all exploring and browsing this conceptual model is based on the idea of allowing people to explore and browse information exploiting their knowledge of how they do this with existing media books magazines radio libraries pamphlets brochures when people go to a tourist office a bookstore or a dentist surgery often they scan and flick through parts of the information displayed hoping to find something interest ing to read cd roms web pages portals and e commerce sites are applications based on this kind of conceptual model much thought needs to go into structuring the information in ways that will support effective navigation allowing people to search browse and find different kinds of information what conceptual models are the following applications based on a a video game say a car racing game with a steering wheel and tactile audio and visual feedback b the windows environment c a web browser commenf a a video game is based on a direct environment conceptual model b the windows environment is based on a hybrid form of conceptual model it com bines a manipulating mode of interaction where users interact with menus scrollbars documents and icons an instructing mode of interaction where users can issue com mands through selecting menu options and various function keys and a conversational model of interaction where agents clippy are used to guide users in their actions c a web browser is also based on a hybrid form of conceptual model allowing users to explore and browse information via hyperlinks and also to instruct the network what to search for and what results to present and save chapter and conceptualizinginteraction a downloading music off the web b programming comment a the activity involves selecting saving cataloging and retrieving large files from an external source users need to be able to browse and listen to samples of the music and then instruct the machine to save and catalog the files in an order that they can readily access at subsequent times a conceptual model based on instructing and navigating would seem appropriate b programming involves various activities including checking debugging copying li braries editing testing and annotating an environment that supports this range of tasks needs to be flexible a conceptual model that allows visualization and easy ma nipulation of code plus efficient instructing of the system on how to check debug copy etc is essential conceptual models based on the second category of conceptual models is based on an object or artifact such as a tool a book or a vehicle these tend to be more specific than conceptual models based on activities focusing on the way a particular object is used in a particular context they are often based on an analogy with something in the physical world an example of a highly successful conceptual model based on an object is the spreadsheet winograd the object this is based on is the ledger sheet the first spreadsheet was designed by dan bricklin and called it en abled people to carry out a range of tasks that previously could only be done very laboriously and with much difficulty using other software packages a calculator or by hand see figure the main reasons why the spreadsheet has become so successful are first that bricklin understood what kind of tool would be useful to people in the financial world like accountants and second he knew how to design it so that it could be used in the way that these people would find useful thus at the outset he understood i the kinds of activities involved in the financial side of business and ii the problems people were having with existing tools when trying to achieve these activities a core financial activity is forecasting this requires projecting financial results based on assumptions about a company such as projected and actual sales invest ments infrastructure and costs the amount of profit or loss is calculatedfor different projections for example a company may want to determine how much loss it will incur before it will start making a profit based on different amounts of investment for different periods of time financial analysts need to see a spread of projectionsfor dif ferent time periods doing this kind of multiple projecting by hand requires much ef fort and is subject to errors using a calculator can reduce the computational load of doing numerous sums but it still requires the person to do much key pressing and writing down of partial results again making the process vulnerable to errors to tackle these problems bricklin exploited the interactivity provided by micro computers and developed an application that was capable of interactive financial chapter understanding and conceptualizing interaction figure reference card showing annotated screen dump for modeling key aspects of his conceptual model were i to create a spreadsheet that was analogous to a ledger sheet in the way it looked with columns and rows which allowed people to capitalize on their familiarity with how to use this kind of repre sentation ii to make the spreadsheet interactive by allowing the user to input and change data in any of the cells in the columns or rows and to get the computer to perform a range of different calculations and recalculations in response to user input for example the last column can be programmed to display the sum of all the cells in the columns preceding it with the computer doing all the calculations to gether with an easy to learn and use interface users were provided with an easy to understand tool moreover it gave them a new way of effortlessly working out any conceptual models number of forecasts greatly extending what they could do before with existing tools another popular accounting tool intended for the home market based on a con ceptual model of an object is quicken this used paper checks and registers for its basic structure other examples of conceptual models based on objects include most operating environments windows and the mac desktop and web portals all provide the user with a familiar frame of referencewhen starting the application chapter understanding and conceptualizinginteraction a case of mix and match as we have pointed out which kind of conceptual model is optimal for a given ap plication obviously depends on the nature of the activity to be supported some are clearly suited to supporting a given activity using manipulation and naviga tion for a flight simulator while for others it is less clear what might be best writing and planning activities may be suited to both manipulation and giving in structions in such situations it is often the case that some form of hybrid concep tual model that combines different interaction styles is appropriate for example the tourist application in activity may end up being optimally designed based on a combination of conversing and exploring models the user could ask specific questions by typing them in or alternatively browse through information shopping on the internet is also often supported by a range of interaction modes sometimes the user may be browsing and navigating other times communicating with an agent at yet other times parting with credit card details via an instruction based form fill in hence which mode of interaction is active depends on the stage of the activity that is being carried out interface metaphors the down side of mixing interaction is that the underlying conceptual model can end up being more complex and ambiguous making it more difficult for the user to understand and learn for example some operating and word pro cessing systems now make it possible for the user to carry out the same activity in a number of different ways to delete a file the user can issue a command like speak to the computer by saying delete file or drag an icon of the file to the recycle bin users will have to learn the different styles to decide which they prefer inevitably the learning curve will be steeper but in the long run the benefits are that it enables users to decide how they want to interact with the system interface metaphors another way of describing conceptual models is in terms of interface metaphors by this is meant a conceptual model that has been developed to be similar in some way to aspects of a physical entity or entities but that also has its own be haviors and properties such models can be based on an activity or an object or both as well as being categorized as conceptual models based on objects the desktop and the spreadsheet are also examples of interface metaphors another example of an interface metaphor is a search engine the tool has been de signed to invite comparison with a physical object a mechanical engine with several parts working together with an everyday action searching by looking through numerous files in many different places to extract relevant information the functions supported by a search engine also include other features besides those belonging to an engine that searches such as listing and prioritizing the re sults of a search it also does these actions in quite different ways from how a me chanical engine works or how a human being might search a library for books on a given topic the similarities alluded to by the use of the term search engine therefore are at a very general conceptual level they are meant to conjure up the essence of the process of finding relevant information enabling the user to leverage off this anchor further understanding of other aspects of the function ality provided interface metaphors are based on conceptual models that combine familiar knowledge with new concepts as mentioned in box the star was based on a conceptual model of the familiar knowledge of an office paper folders filing cabi nets and mailboxes were represented as icons on the screen and were designed to possess some of the properties of their physical counterparts dragging a document icon across the desktop screen was seen as equivalent to picking up a piece of paper in the physical world and moving it but of course is a very different action similarly dragging an electronic document onto an electronic folder was seen as being analogous to placing a physical document into a physical cabinet in addition new concepts that were incorporated as part of the desktop metaphor were opera tions that couldn t be performed in the physical world for example electronic files could be placed onto an icon of a printer on the desktop resulting in the computer printing them out chapter understanding and interaction interface metaphors are often actually composites they combine quite different pieces of familiar knowledge with the system functionality we already mentioned the search en gine as one such example can you think of any others comment some other examples include scrollbar combines the concept of a scroll with a bar as in bar chart toolbar combinesthe idea of a set of tools with a bar portal website a gateway to a particular collection of pages of networked information benefits of interface metaphors interface metaphors have proven to be highly successful providing users with a familiar orienting device and helping them understand and learn how to use a sys tem people find it easier to learn and talk about what they are doing at the interface puter interface in terms familiar to them whether they are computer phobic or highly experienced programmers metaphorically based commands used in unix like lint and pipe have very concrete meanings in everyday language that when used in the context of the unix operating system metaphorically represent some aspect of the operations they refer to although their meaning may appear obscure especially to the novice they make sense when understood in the context of programming for example unix allows the programmer to send the output of one program to another by using the pipe symbol once explained it is easy to imagine the output from one container going to another via a pipe can you think of any bizarre computing metaphors that have become common parlance whose original source of reference is or always was obscure a couple of intriguing ones are java the programing language java originally was called oak but that name had already been taken it is not clear how the developers moved from oak to java java is a name commonly associated with coffee other java based metaphors that have been spawned include java beans a reusable software component and the steaming coffee cup icon that appears in the top left hand corner of java applets bluetooth bluetooth is used in a computing context to describe the wireless technol ogy that is able to unite technology communication and consumer electronics the name is taken from king harald blue tooth who was a century legendary viking king responsible for uniting scandinavia and thus getting people to talk to each other opposition to using interface metaphors a mistake sometimes made by designers is to try to design an interface metaphor to look and behave literally like the physical entity it is being compared with this misses the point about the benefit of developing interface metaphors as stressed earlier they are meant to be used to map familiar to unfamiliar knowl edge enabling users to understand and learn about the new domain designing interface metaphors only as literal models of the thing being compared with has understandably led to heavy criticism one of the most outspoken critics is ted nelson who considers metaphorical interfaces as using old half ideas as crutches p other objections to the use of metaphors in interaction design include breaks the rules several commentators have criticized the use of interface metaphors because of the cultural and logical contradictions involved in accommo dating the metaphor when instantiated as a gui a pet hate is the recycle bin for merly trash can that sits on the desktop logically and culturally in the real world it should be placed under the desk if this same rule were followed in the virtual desktop users would not be able to see the bin because it would be oc cluded by the desktop surface a counter argument to this objection is that it does chapter understanding and interaction not matter whether rules are contravened once people understand why the bin is on the desktop they readily accept that the real world rule had to be broken moreover the unexpected juxtaposition of the bin on the desktop can draw to the user attention the additional functionality that it provides too constraining another argument against interface metaphors is that they are too constraining restricting the kinds of computational tasks that would be useful at the interface an example is trying to open a file that is embedded in several hundreds of files in a directory having to scan through hundreds of icons on a desktop or scroll through a list of files seems a very inefficient way of doing this as discussed earlier a better way is to allow the user to instruct the computer to open the desired file by typing in its name assuming they can remember the name of the file conflicts with design principles by trying to design the interface metaphor to fit in with the constraints of the physical world designers are forced into making bad design solutions that conflict with basic design principles ted nelson sets up the trash can again as an example of such violation a hideous failure of consis tency is the garbage can on the macintosh which means either destroy this or eject it for safekeeping nelson not being able to understand the system functionality beyond the metaphor it has been argued that users may get fixed in their understanding of the system based on the interface metaphor in so doing they may find it difficult to see what else can be done with the system beyond the actions suggested by the interface metaphor nelson also argues that the similarity of interface metaphors to any real objects in the world is so tenuous that it gets in the way more than it helps we would argue the opposite because the link is tenuous and there are only a cer tain number of similarities it enables the user to see both the dissimilarities and how the metaphor has been extended overly literal translation of existing bad designs sometimes designers fall into the trap of trying to create a virtual object to resemble a familiar physical object that is itself badly designed a well known example is the virtual calculator which is designed to look and behave like a physical calculator the interface of many physical calculators however has been poorly designed in the first place based on poor conceptual models with excessive use of modes poor labeling of functions and difficult to manipulate key sequences mullet and sano the design of the calculator in figure has even gone as far as replicating functions needing shift keys deg and hex which could have been re designed as dedicated software buttons trying to use a virtual calculator that has been designed to emulate a poorly designed physical calculator is much harder than using the physical device itself a better approach would have been for the designers to think about how to use the computational power of the computer to support the kinds of tasks people need to do when doing calculations cf the spreadsheet design the calculator in figure has tried to do this to some extent by moving the buttons closer to each other minimizing the amount of mousing and providing flexible display modes with one to one mappings with different functions interface metaphors figure two virtual calculators where a has been designed too literally and b more appropriately for a computer screen limits the designer imagination in conjuring up new paradigms and models designers may on tired ideas based on well known technologies that they know people are very familiar with examples include travel and books for repre senting interaction with the web and hypermedia one of the dangers of always looking backwards is that it restricts the designer in thinking of what new function ality to provide for example gentner and nielsen discuss how they used a book metaphor for designing the user interface to sun microsystems online docu mentation in hindsight they realized how it had blinkered them in organizing the online material preventing them from introducing desirable functions such as the ability to reorder chapters according to their relevance scores after being searched clearly there are pitfalls in using interface metaphors in interaction design in deed this approach has led to some badly designed conceptual models that have resulted in confusion and frustration however this does not have to be the case provided designers are aware of the dangers and try to develop interface metaphors that effectively combine familiar knowledge with new functionality in a meaningful way then many of the above problems can be avoided moreover as we have seen with the spreadsheet example the use of analogy as a basis for a con ceptual model can be very innovative and successful opening up the realm of com puters and their applications to a greater diversity of people chapter understandingand conceptualizing interaction amine a web browser interface and describe the various forms of analogy and composite erface metaphors that have been used in its design what familiar knowledge has been combined functionality many aspects of a web browser have been combined to create a compositeinterface metaphor a range of toolbars such as a button bar navigation bar favorite bar history bar tabs menus organizers search engines guides bookmarks favorites icons for familiar objects like stop lights home these have been combined with other operations and functions including saving search ing downloading listing and navigating interaction paradigms at a more general level another source of inspiration for informing the design of a conceptual model is an interaction paradigm by this it is meant a particular philos ophy or way of thinking about interaction design it is intended to orient designers to the kinds of questions they need to ask for many years the prevailing paradigm in interaction design was to develop applications for the desktop intended to be used by single users sitting in front of a cpu monitor keyboard and mouse a dominant part of this approach was to design software applications that would run using a gui or wimp interface windows icons mouse and pull down menus al ternatively referred to as windows icons menus and pointers as mentioned earlier a recent trend has been to promote paradigms that move beyond the desktop with the advent of wireless mobile and handheld technolo gies developers started applications that could be used in a diversity of ways besides running only on an individual desktop machine for example in september the clothes company levis with the dutch electronics company started selling the first commercial e jacket incorporating wires into the lining of the jacket to create a body area network ban for hooking up various devices mobile phone microphone and headphone see figure in color plate if the phone rings the player cuts out the music automatically to let the wearer listen to the call another innovation was handheld interactive devices like the for which a range of applications were programmed one was to program the lot as a multipurpose identity key guests to check in to certain hotels and enter their room without having to interact with the receptionist at the front desk a number of alternative interaction paradigms have been proposed by re searchers intended to guide future interaction design and system development see figure these include ubiquitous computing technology embedded in the environment pervasive computing seamless integration of technologies wearable computing or wearables interaction paradigms i figure examples of new interaction paradigms a some of the original devices devel oped as part of the ubiquitous computing paradigm tabs are small hand sized wireless computers which know where they are and who they are with pads are paper sized devices connected to the system via radio they know where they are and who they are with boards are large wall sized devices the dangling string created by artist natalie was attached directly to the that ran overhead in the ceiling it spun around depending on the level of digital traffic b ishii and ulmer mit lab tangible bits from of desktop to tangible user interfaces the paradigm is concerned with establishing a new type of hci called tangible user interfaces tuis tuis augment the real physical world by coupling digi tal information to everyday physical objects and environments c affective computing the project called is creating devices with embedded technology that gather information about people this face with movable eyebrows eyes and mouth tracks your movements and facial expressions and responds accordingly chapter understanding and interaction tangible bits augmented reality and integration attentive environments computers attend to user needs the workaday world social aspects of technology use ubiquitous computing the late mark weiser an influen tial visionary proposed the interaction paradigm of ubiquitous computing figure his vision was for computers to disappear into the environment so that we would be no longer aware of them and would use them without thinking about them as part of this process they should invisibly enhance the world that al ready exists rather than create artificial ones existing computing technology multimedia based systems and virtual reality currently do not allow us to do this instead we are forced to focus our attention on the multimedia representations on the screen buttons menus scrollbars or to move around in a virtual simu lated world manipulating virtual objects so how can technologies be designed to disappear into the background weiser did not mean ubiquity in the sense of simply making computers portable so that they can be moved from the desk into our pockets or used on trains or in bed he meant that technology be designed to be integrated seamlessly into the physical world in ways that extend human capabilities one of his prototypes was a tabs pads and boards setup whereby hundreds of computer devices equivalent in size to post it notes sheets of paper and blackboards would be embedded in offices like the spreadsheet such devices are assumed to be easy to use because they cap italize on existing knowledge about how to interact and use everyday objects also like the spreadsheet they provide much greater computational power one of weiser ideas was that the tabs be connected to one another enabling them to be come multipurpose includingacting as a calendar diary identification card and an interactive device to be used with a pc ubiquitous computing will produce nothing fundamentally new but by making everything faster and easier to do with less strain and fewer mental gymnastics it will transform what is apparently possible weiser p pervasive computing pervasive computing is a direct follow on of ideas arising from ubiquitous computing the idea is that people should be able to access and in teract with information any place and any time using a seamless integration of technologies such technologies are often referred to as smart devices or informa tion appliances designed to perform a particular activity commercial products include cell phones and handheld devices like on the domestic front other examples being prototyped include intelligent fridges that signal the user when stocks are low interactive microwave ovens that allow users to ac cess information from the web while cooking and smart pans that beep when the food is cooked wearable computing many of the ideas behind ubiquitous computing have since inspired other researchers to develop technologies that are part of the envi ronment the mit media lab has created several such innovations one example is wearable computing mann the combination of multimedia and wireless interaction paradigms communication presented many opportunities for thinking about how to embed such technologies on people in the clothes they wear jewelry head mounted caps glasses shoes and jackets have all been experimented with to provide the user with a means of interacting with digital information while on the move in the physical world applications that have been developed include automatic diaries that keep users up to date on what is happening and what they need to do throughout the day and tour guides that inform users of relevant information as they walk through an exhibition and other public places rhodes et al tangible bits augmented reality and integration another de velopment that has evolved from ubiquitous computing is tangible user interfaces or tangible bits ishii and ullmer the focus of this paradigm is the integra tion of computational augmentations into the physical environment in other words finding ways to combine digital information with physical objects and sur faces buildings to allow people to carry out their everyday activities exam ples include physical books embedded with digital information greeting cards that play a digital animation when opened and physical bricks attached to virtual ob jects that when grasped have a similar effect on the virtual objects another illus tration of this approach is the one described in chapter of an enjoyable interface in which a person could use a physical hammer to hit a physical key with corre sponding virtual representations of the action being displayed on a screen another part of this paradigm is augmented reality where virtual representa tions are superimposed on physical devices and objects as shown in figure on color plate bridging the gulf between physical and virtual worlds is also cur rently undergoing much research one of the earlier precursors of this work was the digital desk wellner physical office tools like books documents and paper were integrated with virtual representations using projectors and video cameras both virtual and real documents were seamlessly combined attentive environments and transparent computing this interaction paradigm proposes that the computer attend to user needs through anticipating what the user wants to do instead of users being in control deciding what they want to do and where to go the burden should be shifted onto the computer in this sense the mode of interaction is much more implicit computer interfaces respond to the user ex pressions and gestures sensor rich environments are used to detect the user cur rent state and needs for example cameras can detect where people are looking on a screen and decide what to display accordingly the system should be able to de termine when someone wants to make a call and which they want to visit at particular times project is developing a range of computational devices that use non obtrusive sensing technology including videos and micro phones to track and identify users actions this information is then analyzed with respect to where users are looking what they are doing their gestures and their fa cial expressions in turn this is coded in terms of the users physical emotional or informational state and is then used to determine what information they would like for example a computer could become active when a user first walks into a room firing up any new messages that have arrived if the user shakes his or her head it would be interpreted by the computer as i don t want to read them and instead show a listing of their appointments for that day chapter understanding and conceptualizing interaction the workaday world in the new paradigms mentioned above the emphasis is on exploring how technological devices can be linked with each other and digital information in novel ways that allow people to do things they could not do before in contrast the workaday world paradigm is driven primarily by conceptual and mundane concerns it was proposed by tom moran and bob anderson when working at xerox parc they were particularly concerned with the need to understand the social aspects of technology use in a way that could be useful for designers the workaday world paradigm focuses on the essentialcharacter of the workplace in terms of people everyday activities relationships knowledge and resources it seeks to unravel the set of patterns that convey the richness of the settings in which technologies live the complex unpredictable multiform rela tionships that hold among the various aspects of working life p from conceptual models to physical design as we emphasize throughout this book interaction design is an iterative process it involves cycling through various design processes at different levels of detail pri marily it involves thinking through a design problem understanding the user needs coming up with possible conceptual models prototyping them evaluating them with respect to usability and user experience goals thinking about the design implications of the evaluation studies making changes to the prototypes with re spect to these evaluating the changed prototypes thinking through whether the changes have improved the interface and interaction and so on interaction design may also require going back to the original data to gather and check the require ments throughout the iterations it is important to think through and understand whether the conceptual model being developed is working in the way intended and to ensure that it is supporting the user tasks throughout this book we describe the way you should go about doing interac tion design each iteration should involve progressing through the design in more depth a first pass through an iteration should involve essentially thinking about the problem space and identifying some initial user requirements a second pass should involve more extensive information gathering about users needs and the problems they experience with the way they currently carry out their activities see chapter a third pass should continue explicating the requirements lead ing to thinking through possible conceptual models that would be appropriate see chapter a fourth pass should begin fleshing out some of these using a vari ety of user centered methods a number of user centered methods can be used to create prototypes of the potential candidates these include using storyboarding to show how the interaction between the users and the system will take place and the laying out of cards and post it notes to show the possible structure of and navi gation through a throughout the process the various prototypes of the conceptual models should be evaluated to see if they meet users needs informally asking users what they think is always a good starting point see chapter a number of other techniques can also be used at different stages of the develop ment of the prototypes depending on the particular information required see chapters and from conceptual to many issues will need to be addressed when developing and testing initial pro totypes of conceptual models these include the way information is to be presented and interacted with at the interface what combinations of media to use whether to use sound and animations the kind of feedback that will be provided what combinations of input and output devices to use whether to use speech keyboard plus mouse handwriting recognition whether to provide agents and in what format whether to design operations to be hardwired and activated through physical buttons or to represent them on the screen as part of the software what kinds of help to provide and in what format while working through these design decisions about the nature of the interac tion to be supported issues concerning the actual physical design will need to be addressed these will often fall out of the conceptual decisions about the way infor mation is to be represented the kind of media to be used and so on for example these would typically include information presentation which dialogs and interaction styles to use form fill ins speech input menus how to structure items in graphical objects like windows dialog boxes and menus how many items where to place them in relation to each other feedback what navigation mechanisms to provide forward and backward buttons media combination which kinds of icons to use many of these physical design decisions will be specific to the interactive prod uct being built for example designing a calendar application intended to be used by business people to run on a handheld computer will have quite different con straints and concerns from designing a tool for scheduling trains to run over a large network intended to be used by a team of operators via multiple large displays the way the information will be structured the kinds of graphical representations that will be appropriate and the layout of the graphics on the screens will be quite different these kinds of design decisions are very practical needing user testing to en sure that they meet with the usability goals it is likely that numerous trade offs will surface so it is important to recognize that there is no right or wrong way to resolve these each decision has to be weighed with respect to the others for example if you decide that a good way of providing visibility for the calendar application on the handheld device is to have a set of soft navigation buttons permanently as chapter and interaction from conceptual models to physical design chapter understanding and interaction part of the visual display you then need to consider the consequences of doing this for the rest of the information that needs to be interacted with will it still be possi ble to structure the display to show the calendar as days in a week or a month all on one screen this part of the design process is highly dependent on the context and essen tially involves lots of juggling between design decisions if you visit our you can try out some of the interactivities provided where you have to make such deci sions when designing the physical layout for various interfaces here we provide the background and rationale that can help you make appropriate choices when faced with a series of design decisions primarily chapters and for example we ex plain why you shouldn t cram a screen full of information why certain techniques are better than others for helping users remember how to carry out their tasks at the interface and why certain kinds of agents appear more believable than others assignment the aim of this assignment is for you to think about the appropriateness of different kinds of conceptual model that have been designed for similar kinds of physical and electronic artifacts a describe the conceptual model that underlie the design of a personal pocket sized one week to a page a wall calendar one month to a page usually with a a wall planner displaying the whole year what is the main kind of activity and object they are based on how do they differ for each of the three artifacts what metaphors have been used in the design of their physical interface think about the way time is conceptualized for each of them do users understand the conceptual models these are based on in the ways intended ask a few people to explain how they use them do they match the dif ferent user needs b now describe the conceptual models that underlie the design of an electronic personal calendar found on a personal organizer or handheld computer a shared calendar found on the web how do they differ from the equivalent physical artifacts what new functionality has been provided what interface metaphors have been used are the functions and interface metaphor well integrated what problems do users have with these interactive kinds of calendars why do you think this is summary this chapter has explained the importance of conceptualizing interaction design before try ing to build anything it has stressed throughout the need always to be clear and explicit about the rationale and assumptions behind any design decision made it described a taxon omy of conceptual models and the different properties of each it also discussed interface metaphors and interaction paradigms as other ways of informing the design of conceptual models references key points it is important to have a good understanding of the problem space specifying what it is you are doing why and how it will support users in the way intended a fundamental aspect of interaction design is to develop a conceptual model there are various kinds of conceptual models that are categorized according to the activ ity or object they are based on interaction modes conversing instructing provide a structure for thinking about which conceptual model to develop interaction styles menus form fill ins are specific kinds of interfaces that should be decided upon after the conceptual model has been chosen decisions about conceptual design also should be made before commencing any physical design designing an icon interface metaphors are commonly used as part of a conceptual model many interactive systems are based on a hybrid conceptual model such models can pro vide more flexibility but this can make them harder to learn realism is not necessarily better than or other forms of representation when in stantiating a conceptual model what is most effective depends on the users activities when interacting with a system general interaction paradigms like wimp and ubiquitous computing provide a particu lar way of thinking about how to design a conceptual model further reading laurel b ed the art of human computer de sign has a number of papers on conceptual models and inter face metaphors that are definitely worth reading are tom erickson working with interface metaphors pp which is a practical hands on guide to designing in terface metaphors covered later in this book and ted nel son polemic the right way to think about software design pp which is a scathing attack on the use of interface metaphors johnson m and g metaphors we live by the university of chicago press those wanting to find out more about how metaphors are used in everyday con versationsshould take a look at this text there are many good articles on the topic of interface agents a classic is lanier j agents of alienation acm interactions the art of human computer design also pro vides several thought provoking articles including one called interface agents metaphors with character by brenda laurel pp and another called guides characterizing the interface by tim et al pp bannon l problems in human machine interac tion and communication proc san francisco bannon presents a critical review of the agent approach to interface design media lab is a good starting place to find out what is currently happening in the world of agents wearables and other new interaction paradigms chapter understanding and conceptualizing interaction this i mean a human dialog not in the sense of using ordinary language but in the sense of thinking about the sequence and the of interaction so i think interaction design is about designing a space for peo ple where that space has to have a temporal flow it has to have a dialog with the person on hat topic his book bringing design brings the perspectives of a number of re searchers and designers see color plate for an example of his latest research tell me about your and how you moved into interactiondesign i got into interaction design through a couple of intermediate steps i started out doing research into artificial intelligence i became interested in how peo ple interact with computers in particular when using ordinary language it became clear after years of working on that however that the computer was a long way off from matching human abilities more over using natural language with a computer when it doesn t really understand you can be very frustrating and in fact a very bad way to interact with it so rather than trying to get the computer to imitate the person i became interested in other ways of taking advantage of what the computer can do well and what the person can do well that led me into the general field of hci as i began to look at what was going on in that field and to study it it became clear that it was not the same as other areas of computer science the key issues were about how the technology fits with what people could do and what they wanted to do in contrast most of computer science is really domi nated by how the mechanisms operate i was very attracted to thinking more in the style of design disciplines like product design urban de sign architecture and so on i realized that there was an approach that you might call a design way that puts the technical asspects into the background with respect to understanding the interaction through looking at these design disciplines i realized that there was something unique about interaction design which is that it has a dialogic temporal element by yr could you tell me a bit more about what you think is involved in interaction design one of the biggest influences is product design i think that interaction design overlaps with it be cause they both take a very strong user oriented view both are concerned with finding a user group under standing their needs then using that understanding to come up with new ideas they may be ones that the users don t even realize they need it is then a matter of trying to translate who it is what they are doing and why they are doing it into possible innovations in the case of product design it is products in the case of interaction design it is the way that the computer system interacts with the person yr what do you think are important inputs into the design process one of the characteristics of design fields as op posed to traditional engineering fields is that there is much more dependence on case studies and examples than on formulas whereas an engineer knows how to calculate something an architect or a designer is working in a tradition where there is a history over time of other things people have done people have said that the secret of great design is to know what to steal and to know when some element or some way of doing things worked before will be appropriate to your setting and then adapt it of course you can t apply it directly so i think a big part of doing good design is experience and exposure you have to have seen a lot of things in practice and understood what is good and bad about them to then use these to inform your design yr how do you see the relationship between study ing interaction design and the practice of it is there a good dialog between research and practice tw academic study of interaction design is a tricky area because so much of it depends on a kind of tacit knowledge that comes through experience and interview exposure it is not the kind of thing you can set down easily as say you can scientific formulas a lot of design tends to be methodological it is not about the design per se but is more about how you go about doing design in particular knowing what are the appropriate steps to take and how you put them together yr how do you see the field of interaction design taking on board the current explosion in new tech nologies for example mobile ubiquitous infrared and so on is it different say from years ago when it was just about designing software applications to sit on the desktop i think a real change in people thinking has been to move from interface design to interaction de sign this has been pushed by the fact that we do have all kinds of devices nowadays interface design used to mean graphical interfaces which meant designing menus and other widgets but now when you re talk ing about handheld devices gesture interfaces tele phone interfaces and so on it is clear that you can t focus just on the widgets the widgets may be part of any one of these devices but the design thinking as a whole has to focus on the interaction yr advice would you give to a student coming into the field on what they should be learning and for i think a student who wants to learn this field should think of it as a kind of dual process that is what donald schon calls reflection in action needing both the action and the reflection it is im portant to have experience with trying to build things that experience can be from outside work projects and courses where you are actually en gaged in making something work at the same time you need to be able to step back and look at it not as what do i need to do next but from the perspec tive of what you are doing and how that fits into the larger picture yr are there any classic case studies that stand out as good exemplars of interactiondesign you need to understand what has been impor tant in the past i still use the xerox star as an exem plar because so much of what we use today was there when you go back to look at the star you see it in the context of when it was first created i also think some exemplars that are very interesting are ones that never succeeded commercially for example i use the system that was developed for pen com puters by go again they were thinking fresh they set out to do something different and they were much more conscious of the design issues than somebody who was simply adapting the next version of something that already existed is another good exam ple because they looked at the problem in a different way to make something work another interesting ex emplar which other people may not agree with is mi crosoft bob not because it was a successful program because it wasn t but because it was a first exploration of a certain style of interaction using animated agents you can see clearly from these exemplars what design trade offs the designers were making and why and then you can look at the consequences yr finally what are the biggest challenges facing people in this area tw i think one of the biggest challenges is what pelle ehn calls the dialectic between tradition and transcendence that is people work and live in cer tain ways already and they understand how to adapt that within a small range but they don t have an un derstanding or a feel for what it would mean to make a radical change for example to change way of doing business on the internet before it was around or to change their way of writing from pen and paper when word processors weren t around i think what the designer is trying to do is envision things for users that the users can t yet envision the hard part is not fixing little problems but designing things that are both innovative and that work chapter understanding users introduction what is cognition knowledge from the physical world to the digital world conceptual frameworks for cognition mental models processing external cognition informing design from to practice introduction imagine trying to drive a car by using just a computer keyboard the four arrow keys are used for steering the space bar for braking and the return key for acceler ating to indicate left you need to press the key and to indicate right the key to sound your horn you need to press the key to switch the headlights on you need to use the key and to switch the windscreen wipers on the key now imagine as you are driving along a road a ball is suddenly kicked in front of you what would you do bash the arrow keys and the space bar madly while pressing the key how would you rate your chances of missing the ball most of us would balk at the very idea of driving a car this way many early video games however were designed along these lines the user had to press an ar bitrary combination of function keys to drive or navigate through the game there was little if any consideration of the user capabilities while some users regarded mastering an arbitrary set of keyboard controls as a challenge many users found them very limiting frustrating and difficult to use more recently computer con soles have been designed with the user capabilities and the demands of the activ ity in mind much better ways of controlling and interacting such as through using joysticks and steering wheels are provided that map much better onto the physical and cognitive aspects of driving and navigating in this chapter we examine some of the core cognitive aspects of interaction de sign specifically we consider what humans are good and bad at and show how this knowledge can be used to inform the design of technologiesthat both extend human capabilities and compensate for their weaknesses we also look at some of the influ ential cognitively based conceptual frameworks that have been developed for ex plaining the way humans interact with computers other ways of conceptualizing chapter understandingusers human behavior that focus on the social and affective aspects of interaction design are presented in the following two chapters the main aims of this chapter are to explain what cognition is and why it is important for interaction design describe the main ways cognition has been applied to interaction design provide a number of examples in which cognitive research has led to the de sign of more effective interactive products explain what mental models are give examples of conceptualframeworksthat are useful for interaction design enable you to try to elicit a mental model and be able to understand what it means what is cognition cognition is what goes on in our heads when we carry out our everyday activities it involves cognitive processes like thinking remembering learning daydreaming decision making seeing reading writing and talking as figure indicates there are many different kinds of cognition norman distinguishes between two general modes experiential and reflective cognition the former is a state of mind in which we perceive act and react to events around us effectively and effortlessly it requires reaching a certain level of expertise and engagement examples include driving a car reading a book having a conversation and playing a video game in contrast reflective cognition involves thinking comparing and decision making this kind of cognition is what leads to new ideas and creativity examples include designing learning and writing a book norman points out that both modes are essential for everyday life but that each requires different kinds of technological support what goes on in the mind figure what goes on in the mind what is cognition cognition has also been described in terms of specific kinds of processes these include attention perception and recognition memory learning reading speaking and listening problem solving planning reasoning decision making it is important to note that many of these cognitive processes are interdepen dent several may be involved for a given activity for example when you try to learn material for an exam you need to attend to the material perceive and recog nize it read it think about it and try to remember it thus cognition typically in volves a range of processes it is rare for one to occur in isolation below we describe the various kinds in more detail followed by a summary box highlighting core design implications for each most relevant and most thoroughly researched for interaction design is memory which we describe in greatest detail attention is the process of selecting things to concentrate on at a point in time from the range of possibilitiesavailable attention involves our auditory vi sual senses an example of auditory attention is waiting in the dentist waiting room for our name to be called out to know when it is our time to go in an exam ple of attention involving the visual senses is scanning the football results in a news paper to attend to information about how our team has done attention allows us to focus on information that is relevant to what we are doing the extent to which this process is easy or difficult depends on i whether we have clear goals and ii whether the information we need is salient in the environment i our if we know exactly what we want to find out we try to match this with the information that is available for example if we have just landed at an air port after a long flight and want to find out who had won the world cup we might scan the headlines at the newspaper stand check the web call a friend or ask someone in the street when we are not sure exactly what we are looking for we may browse through information allowing it to guide our attention to interesting or salient items for example when we go to a restaurant we may have the general goal of eating a meal but only a vague idea of what we want to eat we peruse the menu to find things that whet our appetite letting our attention be drawn to the imaginative descrip tions of various dishes after scanning through the possibilitiesand imagining what each dish might be like plus taking into account other factors such as cost who we are with what the specials are what the waiter recommends whether we want a two or three course meal and so on we may then make a decision ii information presentation the way information is displayed can also greatly in fluence how easy or difficult it is to attend to appropriate pieces of information look at figure and try the activity here the information searching tasks are very precise requiring specific answers the information density is identical in both chapter understandingusers figure two different ways of struc turing the same information at the inter face one makes it much easier to find information than the other look at the top screen and i find the price for a double room at the quality inn in co lumbia ii find the phone number of the days inn in charleston then look at the bottom screen and i find the price of a double room at the holiday in bradley ii find the phone number of the quality inn in which took longer to do in an early study tullis found that the two screens produced quite different results it took an average of seconds to search the top screen and seconds to find the same kind of information in the bottom screen why is this so considering that both displays have the same density of information the primary reason is the way the characters are grouped in the display in the top they are grouped into vertical categories of information place kind of accommodation phone number and rates that have columns of space be tween them in the bottom screen the in formation is bunched up together making it much harder to search through displays however it is much harder to find the information in the bottom screen than in the top screen the reason for this is that the information is very poorly structured in the bottom making it difficult to find the information in the top the information has been ordered into meaningful categories with blank spacing be tween them making it easier to select the necessary information perception refers to how information is acquired from the environment via the different sense organs eyes ears fingers and transformed into experiences of objects events sounds and tastes roth it is a complex process involving other cognitive processes such as memory attention and language vision is the what is cognition most dominant sense for sighted individuals followed by hearing and touch with respect to interaction design it is important to present information in a way that can be readily perceived in the manner intended for example there are many ways to design icons the key is to make them easily distinguishable from one an other and to make it simple to recognize what they are intended to represent not like the ones in figure combinations of different media need also to be designed to allow users to rec ognize the composite information represented in them in the way intended the use of sound and animation together needs to be coordinated so they happen in a logical sequence an example of this is the design of lip synch applications where the animation of an avatar or agent face to make it appear to be talking must be carefully synchronized with the speech that is emitted a slight delay between the two can make it difficult and disturbing to perceive what is happening as some times happens when film dubbing gets out of synch a general design principle is chapter understanding users figure poor icon set what do you think the icons mean and why are they so bad that information needs to be represented in an appropriate form to facilitate the perception and recognition of its underlying meaning memory involves recalling various kinds of knowledge that allow us to act ap propriately it is very versatile enabling us to do many things for example it al lows us to recognize someone face remember someone name recall when we last met them and know what we said to them last simply without memory we would not be able to function it is not possible for us to remember everything that we see hear taste smell or touch nor would we want to as our brains would get completely overloaded a filtering process is used to decide what information gets further processed and memorized this filtering process however is not without its problems often we what is cognition forget things we would dearly love to remember and conversely remember things we would love to forget for example we may find it difficult to remember every day things like people names and phone numbers or academic knowledge like mathematical formulae on the other hand we may effortlessly remember trivia or tunes that cycle endlessly through our heads how does this filtering process work initially encoding takes place determin ing which information is attended to in the environment and how it is interpreted the extent to which it takes place affects our ability to recall that information later the more attention that is paid to something and the more it is processed in terms of thinking about it and comparing it with other knowledge the more likely it is to be remembered for example when learning about a topic it is much better to re flect upon it carry out exercises have discussions with others about it and write notes than just passively read a book or watch a video about it thus how informa tion is interpreted when it is encountered greatly affects how it is represented in memory and how it is used later another factor that affects the extent to which information can be subse quently retrieved is the context in which it is encoded one outcome is that some times it can be difficult for people to recall information that was encoded in a different context from the one they currently are in consider the following sce nario you are on a train and someone comes up to you and says hello you don t recognize him for a few moments but then realize it is one of your neighbors you are only used to seeing your neighbor in the hallway of your apartment block and seeing him out of context makes him to recognize initially another well known memory phenomenon is that people are much better at rec ognizing things than recalling things furthermore certain kinds of information are easier to recognize than others in particular people are very good at recognizing thousands of pictures even if they have only seen them briefly before try to remember the dates of all the members of your family and your closest friends birthdays how many can you remember then try to describe what is on the cover of the last or record you bought which is easiest and why comment it is likely that you remembered much better what was on the cover the image the colors the title than the birthdays of your family and friends people are very good at remembering visual cues about things for example the color of items the location of objects a book being on the top shelf and marks on an object a scratch on a watch a chip on a cup in contrast people find other kinds of information persistently difficult to learn and remember especially arbitrary material like birthdays and phone numbers instead of requiring users to recall from memory a command name from a pos sible set of hundreds or even thousands provide visually based options that chapter understanding users users can browse through until they recognize the operation they want to perform see figure and b likewise web browsers provide a facility of ing or saving favorite urls that have been visited providing a visual list this means that users need only recognize a name of a site when scanning through the saved list of urls figure a dos based interface requiring the user to type in commands what is cognition figure a windows based interface with menus icons and buttons what strategies do you use to help you remember things comment people often write down what they need to remember on a piece of paper they also ask others to remind them another approach is to use various mental strategies like mnemon ics a mnemonic involves taking the first letters of a set of words in a phrase or set of con cepts and using them to make a more memorable phrase often using bizarre and idiosyncratic connections for example some people have problems working out where east is in relation to west and vice versa is it to the left or right a mnemonic to help figure this out is to take the first letters of the four main points of the compass and then use them in the phrase never eat shredded wheat mentally recited in a clockwise sequence a growing problem for computer users is file management the number of documents created images and videoclips downloaded and attachments saved bookmarked and so on increases every day a major problem is find ing them again naming is the most common means of encoding them but trying to remember a name of a file you created some time back can be very difficult espe cially if there are tens of thousands of named files how might such a process be fa cilitated bearing in mind people memory abilities mark lansdale a british psychologist has been researching this problem of information retrieval for many chapter understandingusers what is cognition years he suggests that it is profitable to view this process as involving two memory processes recall directed followed by recognition based scanning the first refers to using memorized information about the required file to get as close to it as possi ble the more exact this is the more success the user will have in tracking down the desired file the second happens when recall has failed to produce what a user wants and so requires reading through directories of files to illustrate the difference between these two processes consider the following scenario a user is trying to access a couple of visited the day before that compared the selling price of cars offered by different dealers the user is able to re call the name of one she types this in and the appears this is an example of successful recall directed memory however the user is unable to remember the name of the second one she vaguely remembers it was something like but typing this in proves unsuccessful in stead she switches to scanning her going to the list of most re cent ones saved she notices two or three that could be the one desired and on the second attempt she finds the she is looking for in this situation the user initially tries recall directed memory and when this fails adopts the second strategy of recognition basedscanning which takes longer but eventually resultsin success lansdale proposes that file management systems should be designed to opti mize both kinds of memory processes in particular systems should be devel oped that let users use whatever memory they have to limit the area being searched and then represent the information in this area of the interface so as to maximally assist them in finding what they need based on this theory he has developed a prototype system called memoirs that aims at improving users recall of information they had encoded so as to make it easier to recall later lansdale and edmunds the system was designed to be flexible provid ing the user with a range of ways of encoding documents mnemonically includ ing stamping see figure flagging and attribution color text icon sound or image more flexible ways of helping users track down the files they want are now be ginning to be introduced as part of commercial applications for example various search and find tools like apple sherlock have been designed to enable the user to type a full or partial name or phrase that the system then tries to match by listing all the files it identifies containing the requested this method how ever is still quite limited in that it allows users to encode and retrieve files using only alphanumericals chapter understanding users x pixels full sized document figure memoirs tool what is cognition how else might banks solve the problem of providing a secure system while making the memory load relatively easy for people wanting to use phone banking how does phone banking compare with online banking comment an alternative approach is to provide the customers with a pin number it could be the same as that of their atm card and ask them to key this in on their phone keypad followed by asking one or two questions like their zip or post code as a backup online banking has similar security risks to phone banking and hence this requires a number of security mea sures to be enforced these include that the user sets up a nickname and a password for ex ample some banks require typing in three randomly selected letters from a password each time the user logs on this is harder to do online than when asked over the phone mainly chapter understandingusers because it interferes with the normally highly automated process of typing in a password you really have to think about what letters and numbers are in your password for example has it got two letter f after the number or just one learning can be considered in terms of i how to use a computer based appli cation or ii using a computer based application to understand a given topic jack carroll and his colleagues have written extensively about how to design inter faces to help learners develop computer based skills a main observation is that peo ple find it very hard to learn by following sets of instructions in a manual instead they much prefer to learn through doing and direct interfaces are good environments for supporting this kind of learning by supporting exploratory interaction and importantly allowing users to undo their actions return to a previous state if they make a mistake by clicking on the wrong option carroll has also suggested that another way of helping learners is by using a training wheels approach this involves restricting the possible functions that can be carried out by a novice to the basics and then extending these as the novice becomes more experi enced the underlying rationale is to make initial learning more tractable helping the learner focus on simple operations before moving on to more complex ones there have also been numerous attempts to harness the capabilities of differ ent technologies to help learners understand topics one of the main benefits of in teractive technologies such as web based multimedia and virtual reality is that they provide alternative ways of representing and interacting with information that are not possible with traditional technologies books video in so doing they have the potential of offering learners the ability to explore ideas and concepts in different ways ask a grandparent child or other person who has not used a cell phone before to make and answer a call using it what is striking about their behavior comment first time users often try to apply their understanding of a land line phone to operating a cell phone however there are marked differences in the way the two phones operate even for the simplest of tasks like making a call first the power has to be switched on when using a cell phone by pressing a button but not so with land line phones then the number has to be keyed in including at all times the area code in the uk even if the callee is in the same area but not so with land lines and finally the make a call button must be pressed but not so with land line phones first time users may intuitively know how to switch the phone on but not know which key to hit or that it has to be held down for a couple of seconds they may also forget to key in the area code if they are in the same area as the person they are calling and to press the make a call key they may also forget to press the end a call button this is achieved through putting the receiver down with a land line phone likewise when an swering a call the first time user may forget to press the accept a call button or not know which one to press these additional actions are quick to learn once the user understands the need to explicitly instruct the cell phone when they want to make accept or end a call reading speaking and listening these three forms of language processing have both similar and different properties one similarity is that the meaning of what is cognition sentences or phrases is the same regardless of the mode in which it is conveyed for example the sentence computers are a wonderful invention essentially has the same meaning whether one reads it speaks it or hears it however the ease with which people can read listen or speak differs depending on the person task and context for example many people find listening much easier than reading specific differences between the three modes include written language is permanent while listening is transient it is possible to reread information if not understood the first time round this is not possi ble with spoken information that is broadcast chapter users reading can be quicker than speaking or listening as written text can be rapidly scanned in ways not possible when listening to serially presented spo ken words listening requires less cognitive effort than reading or speaking children especially often prefer to listen to narratives provided in multimedia or based learning material than to read the equivalent text online written language tends to be grammatical while spoken language is often ungrammatical for example people often start a sentence and stop in sentence letting someone else start speaking there are marked differences between people in their ability to use lan guage some people prefer reading to listening while others prefer listening likewise some people prefer speaking to writing and vice versa dyslexics have difficulties understanding and recognizing written words making it hard for them to write grammatical sentences and spell correctly people who are hard of hearing or hard of seeing are also restricted in the way they can process language many applications have been developed either to capitalize on people reading writing and listening skills or to support or replace them where they lack or have difficulty with them these include interactive books and web based material that help people to read or learn foreign languages speech recognition systems that allow users to provide instructions via spo ken commands word processing dictation home control devices that respond to vocalized requests speech output systems that use artificially generated speech text to speech systems for the blind natural language systems that enable users to type in questions and give text based responses ask jeeves search engine cognitive aids that help people who find it difficult to read write and speak a number of special interfaces have been developed for people who have problems with reading writing and speaking see edwards various input and output devices that allow people with various disabili ties to have access to the web and use word processors and other software packages helen petrie and her team at the sensory disabilities research lab in the uk have been developing various interaction techniques to allow blind people to ac cess the web and other graphical representations through the use of auditory navi gation and tactile diagrams problem solving planning reasoning and decision making are all cognitive processes involving reflective cognition they include thinking about what to do what the options are and what the consequences might be of carrying out a given action they often involve conscious processes being aware of what one is thinking what is cognition i about discussion with others or oneself and the use of various kinds of artifacts maps books and pen and paper for example when planning the best route to get somewhere say a foreign city we may ask others use a map get instructions from the web or a combination of these reasoning also involves working through different scenarios and deciding which is the best option or solution to a given problem in the route planning activity we may be aware of alternative routes and reason through the advantages and disadvantages of each route before deciding on the best one many a family argument has come about because one member thinks he or she knows the best route while another thinks otherwise comparing different sources of information is also common practice when seeking information on the web for example just as people will phone around for a range of quotes so too will they use different search engines to find sites that give the best deal or best information if people have knowledge of the pros and cons of different search engines they may also select different ones for different kinds of queries for example a student may use a more academically oriented one when looking for information for writing an essay and a more commercially based one when trying to find out what happening in town the extent to which people engage in the various forms of reflective cognition depends on their level of experience with a domain application or skill novices tend to have limited knowledge and will often make assumptions about what to do using other knowledge about similar situations they tend to act by trial and error exploring and experimenting with ways of doing things as a result they may start off being slow making errors and generally being inefficient they may also act ir rationally following their superstitionsand not thinking ahead to the consequences of their actions in contrast experts have much more knowledge and experience and are able to select optimal strategies for carrying out their tasks they are likely to be able to think ahead more considering what the consequences might be of opting for a particular move or solution as do expert chess players chapter understanding users applying knowledge from the physical world to the digital world as well as understanding the various cognitive processes that users engage in when interacting with systems it is also useful to understand the way people cope with the demands of everyday life a well known approach to applying knowledge about everyday psychology to interaction design is to emulate in the digital world the strategies and methods people commonly use in the physical world an as sumption is that if these work well in the physical world why shouldn t they also work well in the digital world in certain situations this approach seems like a good idea examples of applications that have been built following this approach include electronic post it notes in the form of stickies electronic to do lists and reminders of meetings and other events about to take place the stickies application displays differentcolored notes on the desktop in which text can be in serted deleted annotated and around enabling people to use them to re mind themselves of what they need to do analogous to the kinds of externalizing they do when using paper stickies moreover a benefit is that electronic stickies are more durable than paper ones they don t get lost or fall off the objects they are stuck to but stay on the desktop until explicitly deleted in other situations however the simple emulation approach can turn out to be counter productive forcing users to do things in bizarre inefficient or inappropri ate ways this can happen when the activity being emulated is more complex than is assumed resulting in much of it being oversimplified and not supported effec tively designers may notice somethingsalient that people do in the physical world and then fall into the trap of trying to copy it in the electronic world without think ing through how and whether it will work in the new context remember the poor design of the virtual calculator based on the physical calculator described in the previous chapter consider the following classic study of real world behavior ask yourself first whether it is useful to emulate at the interface and second how it could be ex tended as an interactive application tom carried out a study of the natural history of physical of fices he interviewed people and studied their offices paying particular attention to their filing methods and how they organized their papers one of his findings was that whether people have messy offices or tidy offices may be more significant than people realize messy offices were seen as being chaotic with piles of papers every where and little organization tidy offices on the other hand were seen as being well organized with good use of a filing system in analyzing these two types of of fices suggested what they reveal in terms of the underlying cognitive be haviors of the occupants one of his observations was that messy offices may appear chaotic but in reality often reflect a coping strategy by the person docu ments are left lying around in obvious places to act as reminders that somethinghas to be done with them this observation suggests that using piles is a fundamental strategy regardless of whether you are a chaotic or orderly person such observations about people coping strategies in the physical world bring to mind an immediate design implication about how to support electronic file knowledge from the physical world to the digital world management to capitalize on the pile phenomenon by trying to emulate it in the electronic world why not let people arrange their electronic files into piles as they do with paper files the danger of doing this is that it could heavily constrain the way people manage their files when in fact there may be far more effective and flexible ways of filing in the electronic world mark lansdale points out how introducing unstructured piles of electronic documents on a desktop would be counterproductive in the same way as building planes to flap their wings in the way birds do someone seriously thought of doing this but there may be benefits of emulating the pile phenomenon by using it as a kind of interface metaphor that is extended to offer other functionality how might this be achieved a group of interface designers at apple computer mandler et al tackled this problem by adopting the philosophy that they were going to build an application that went beyond physical world capabilities providing new functionality that only the computer could provide and that enhanced the interface to begin their design they carried out a detailed study of office behavior and ana lyzed the many ways piles are created and used they also examined how people use the default hierarchical file management systems that computer operating sys tems provide having a detailed understanding of both enabled them to create a conceptual model for the new functionality whichwas to provide various interac tive organizational elements based around the notion of using piles these included providing the user with the means of creating ordering and visualizing piles of files files could also be encoded using various external cues including date and color new functionality that could not be achieved with physical files included the provision of a scripting facility enabling files in piles to be ordered in relation to these cues see figure emulating real world activity at the interface can be a powerful design strat egy provided that new functionality is incorporated that extends or supports the users in their tasks in ways not possible in the physical world the key is really to understand the nature of the problem being addressed in the electronic world in re lation to the various coping and externalizing strategies people have developed to deal with the physical world chapter understanding users conceptual frameworks for cognition in the previous section we described the pros and cons of applying knowledge of people coping strategies in the physical world to the digital world another ap proach is to apply theories and conceptual frameworks to interaction design in this section we examine three of these approaches which each have a different perspec tive on cognition mental models information processing external cognition mental models in chapter we pointed out that a successful system is one based on a conceptual model that enables users to readily learn a system and use it effectively what hap pens when people are learning and using a system is that they develop knowledge of how to use the system and to a lesser extent how the system works these two kinds of knowledge are often referred to as a user mental model having developed a mental model of an interactive product it is assumed that people will use it to make inferences about how to carry out tasks when using the interactive product mental models are also used to fathom what to do when some thing unexpected happens with a system and when encountering unfamiliar sys tems the more someone learns about a system and how it functions the more their mental model develops for example tv engineers have a deep mental model of how work that allows them to work out how to fix them in contrast conceptual frameworks for cognition an average citizen is likely to have a reasonably good mental model of how to oper ate a tv but a shallow mental model of how it works within cognitive psychology mental models have been postulated as internal constructions of some aspect of the external world that are manipulated enabling predictions and inferences to be made craik this process is thought to in volve the fleshing out and the running of a mental model johnson laird this can involve both unconscious and conscious mental processes where images and analogies are activated o illustrate how we use mental models in our everyday reasoning imagine the following a you arrive home from a holiday on a cold winter night to a cold house you have a small baby and you need to get the house warm as quickly as possible your house is centrally heated do you set the thermostat as high as possible or turn it to the de sired temperature b you arrive home from being out all night starving hungry you look in the fridge and find all that is left is an uncooked pizza the instructions on the packet say heat the oven to f and then place the pizza in the oven for minutes your oven is elec tric how do you heat it up do you turn it to the specified temperature or higher comment most people when asked the first question imagine the scenario in terms of what they would do in their own house and choose the first option when asked why a typical explanation that is given is that setting the temperature to be as high as possible increases the rate at which the room warms up while many people may believe this it is incorrect thermostats work by switching on the heat and keeping it going at a constant speed until the desired tem perature set is reached at which point they cut out they cannot control the rate at which heat is given out from a heating system left at a given setting thermostats will turn the heat on and off as necessary to maintain the desired temperature when asked the second question most people say they would turn the oven to the speci fied temperature and put the pizza in when they think it is at the desired temperature some people answer that they would turn the oven to a higher temperature in order to warm it up more quickly electric ovens work on the same principle as central heating and so turning the heat up higher will not warm it up any quicker there is also the problem of the pizza burning if the oven is too hot why do people use erroneous mental models it seems that in the above sce narios they are running a mental model based on a general valve theory of the way something works kempton this assumes the underlying principle of more is more the more you turn or push something the more it causes the desired ef fect this principle holds for a range of physical devices such as taps and radio con trols where the more you turn them the more water or volume is given however it does not hold for thermostats which instead function based on the principle of an on off switch what seems to happen is that in everyday life people develop a core set of abstractions about how things work and apply these to a range of de vices irrespective of whether they are appropriate i chapter understanding users using incorrect mental models to guide behavior is surprisingly common just watch people at a pedestrian crossing or waiting for an elevator lift how many times do they press the button a lot of people will press it at least twice when asked why a common reason given is that they think it will make the lights change faster or ensure the elevator arrives this seems to be another example of following the more is more philosophy it is believed that the more times you press the but ton the more likely it is to result in the desired effect another common example of an erroneous mental model is what people do when the cursor freezes on their computer screen most people will bash away at all manner of keys in the vain hope that this will make it work again however ask them how this will help and their explanations are rather vague the same is true when the tv starts acting up a typical response is to hit the top of the box repeat edly with a bare hand or a rolled up newspaper again ask people why and their reasoningabout how this behavior will help solve the problem is rather lacking the more one observes the way people interact with and behave towards inter active devices the more one realizes just how strange their behavior can especially when the device doesn t work properly and they don t know what to do indeed research has shown that people mental models of the way interactive de vices work is poor often being incomplete easily confusable based on inappropriate analogies and superstition norman not having appropriate mental models available to guide their behavior is what causes people to become very often resulting in stereotypical venting behavior like those described above on the other hand if people could develop better mental models of interactive systems they would be in a better position to know how to carry out their tasks ef ficiently and what to do if the system started acting up ideally they should be able to develop a mental model that matches the conceptual model developed by the designer but how can you help users to accomplish this one suggestion is to edu cate them better however many people are resistant to spending much time learning about how things work especially if it involves reading manuals and other documentation an alternative proposal is to design systems to be more transpar ent so that they are easier to understand this doesn t mean literally revealing the guts of the system cf the way some phone handsets see figure on color plate and are made of transparent plastic to reveal the colorful electronic circuitry inside but requires developing an easy to understand system image see chapter for explanation of this term in relation to conceptual models specifi cally this involves providing useful feedback in response to user input easy to understand and intuitive ways of interacting with the system in addition it requires providing the right kind and level of information in the form of clear and easy to follow instructions appropriate online help and tutorials context sensitiveguidancefor users set at their level of experience explaining how to proceed when they are not sure what to do at a given stage of a task conceptual frameworks for cognition it look how of ond o hfr tm fttlin j lucky tm t chapter understanding users processing another approach to conceptualizing how the mind works has been to use metaphors and analogies see also chapter a number of comparisons have been made including conceptualizing the mind as a reservoir a telephone net work and a digital computer one prevalent metaphor from cognitive psychology is the idea that the mind is an information processor information is thought to enter and exit the mind through a series of ordered processing stages see figure within these stages various processes are assumed to act upon mental rep resentations processes include comparing and matching mental representations are assumed to comprise images mental models rules and other forms of knowl edge the information processingmodel provides a basis from which to make predic tions about human performance hypotheses can be made about how long some one will take to perceive and respond to a stimulus also known as reaction time and what bottlenecks occur if a person is overloaded with too much information the best known approach is the human processor model which models the cogni tive processes of a user interacting with a computer card et al based on the information processing model cognition is conceptualized as a series of pro cessing stages where perceptual cognitive and motor processors are organized in relation to one another see figure the model predicts which cognitive processes are involved when a user interacts with a computer enabling calculations to be made of how long a user will take to carry out various tasks this can be very useful when comparing different interfaces for example it has been used to com pare how well different word processors support a range of editing tasks the information processing approach is based on modeling mental activities that happen exclusively inside the head however most cognitive activities involve people interacting with external kinds of representations like books documents and computers not to mention one another for example when we go home from wherever we have been we do not need to remember the details of the route be cause we rely on cues in the environment we know to turn left at the red house right when the road comes to a and so on similarly when we are at home we do not have to remember where everything is because information is out there we decide what to eat and drink by scanning the items in the fridge find out whether any messages have been left by glancing at the answering machine to see if there is a flashing light and so on to what extent therefore can we say that information processing models are truly representative of everyday cognitive activities do they adequately account for cognition as it happens in the real world and specifically how people interact with computers and other interactive devices output or or stimuli response figure human information processing model conceptual frameworks for cognition figure the human proces sor model several researchers have argued that existing information processing ap proaches are too impoverished the traditional approach to the study of cognition is to look at the pure intellect isolated from distractions and from artificial aids experimentsare performed in closed isolated rooms with a minimum of distracting lights or sounds no other people to assist with the task and no aids to memory or thought the tasks are arbitrary ones invented by the researcher model builders build simulations and descriptions of these isolated situations the theoretical analyses are self contained little structures isolated from the world isolated from any other knowledge or abilities person norman p instead there has been an increasing trend to study cognitive activities in the context in which they occur analyzing cognition as it happens in the wild chapter understanding users hutchins a central goal has been to look at how structures in the environ ment can both aid human cognition and reduce cognitive load a number of alter native frameworks have been proposed including external cognition and distributed cognition this chapter we look at the ideas behind external cogni tion which has focused most on how to inform interaction design distributed cognition is described in the next chapter external cognition people interact with or create information through using a variety of external rep resentations books multimedia newspapers web pages maps diagrams notes drawings and so on furthermore an impressive range of tools has been de veloped throughout history to aid cognition including pens calculators and puter based technologies the combination of external representations and physical tools have greatly extended and supported people ability to carry out cognitive ac tivities norman indeed they are such an integral part that it is difficult to imagine how we would go about much of our everyday life without them external cognition is concerned with explaining the cognitive processes involved when we interact with different external representations scaife and rogers a main goal is to explicate the cognitive benefits of using different representations for different cognitive activities and the processes involved the main ones include externalizing to reduce memory load computational offloading annotating and cognitive tracing externalizing to reduce memory load a number of strategies have been developed for transforming knowledge into external representations to reduce memory load one such strategy is exter nalizing things we find difficult to remember such as birthdays appointments and addresses diaries personal reminders and calendars are examples of cognitive ar tifacts that are commonly used for this purpose acting as external reminders of what we need to do at a given time buy a card for a relative birthday other kinds of external representations that people frequently employ are notes like shopping lists and to do lists where these are placed in the environment can also be crucial for example people often place post it notes in prominent positions such as on walls on the side of computer monitors by the front door and sometimes even on their hands in a deliberate attempt to ensure they do remind them of what needs to be done or remembered people also place things in piles in their offices and by the front door indicating what needs to be done urgently and what can wait for a while externalizing therefore can help reduce people memory burden by reminding them to do something to get something for their mother birthday conceptual frameworks for cognition reminding them of what to do to buy a card reminding them of when to do something send it by a certain date computational offloading computational offloading occurs when we use a tool or device in conjunction with an external representation to help us carry out a computation an example is using pen and paper to solve a math problem a multiply by in your head easy now try multiplying by in your head not as easy try doing the sum using a pen and paper then try again with a calcula tor why is it easier to do the calculation with pen and paper and even easier with a calculator b try doing the same two sums using roman numerals comment a carrying out the sum using pen and the paper is easier than doing it in your head be cause you offload some of the computation by writing down partial results and using them to continue with the calculation doing the same sum with a calculator is even easier because it requires only eight simple key presses even more of the com putation has been offloaded onto the tool you need only follow a simple internal ized procedure key in first number then the multiplier sign then next number and finally the equals sign and then read of the result from the external display b using roman numerals to do the same sum is much harder by becomes 111 and by becomes the first calculation may be possible to do in your head or on a bit of paper but the second is incredibly diffi cult to do in your head or even on a piece of paper unless you are an expert in using roman numerals or you cheat and transform it into arabic numerals calculators do not have roman numerals so it would be impossible to do on a calculator hence it is much harder to perform the calculations using roman numerals than alge braic numerals even though the problem is equivalent in both conditions the reason for this is the two kinds of representation transform the task into one that is easy and more diffi cult respectively the kind of tool used also can change the nature of the task to being more or less easy annotating and cognitive tracing another way in which we externalizeour cognition is by modifying representations to reflect changes that are taking place that we wish to mark for example people often cross things off in a to do list to show that they have been completed they may also reorder objects in the environment say by creating different piles as the nature of the work to be done changes these two kinds of modification are called annotating and cognitive tracing annotating involves modifying external representations such as crossing off or underlining items users cognitive tracing externally manipulating items into different orders or structures annotating is often used when people go shopping people usually begin their shopping by planning what they are going to buy this often involves looking in their cupboards and fridge to see what needs stocking up however many people are aware that they remember all this in their heads and so often externalize it as a written shopping list the act of writing may also remind them of other items that they need to buy they may not have noticed when looking through the cupboards when they actually go shopping at the store they may cross off items on the shopping list as they are placed in the shopping basket or cart this provides them with an annotated externalization allowing them to see at a glance what items are still left on the list that need to be bought cognitive tracing is useful in situations where the current state of play is in a state of flux and the person is trying to optimize their current position this typi cally happens when playinggames such as in a card game the continued rearrangement of a hand of cards into suits as cending order or same numbers to help determine what cards to keep and which to play as the game progresses and tactics change in scrabble where shuffling around letters in the tray helps a person work out the best word given the set of letters maglio et al it is also a useful strategy for letting users know what they have studied in an online learning package an interactive diagram can be used to highlight all the nodes vis ited exercises completed and units still to study a cognitive principle for interaction design based on the external cog nition approach is to provide external representations at the interface that reduce memory load and facilitate computational offloading different kinds of informa tion visualizations can be developed that reduce the amount of effort required to make inferences about a given topic financial forecasting identifying informing design from to practice figure information visualization visual in sights site map showing web page use each page appears as a color rod and is positioned radially with the position showing the location of the page in the site gramming bugs in so doing they can extend or amplify cognition allowing people to perceive and do activities that they couldn t do otherwise for example a num ber of information visualizations have been developed that present masses of data in a form that makes it possible to make cross comparisons between dimensions at a glance see figure can also be designed to reduce memory load sig nificantly enabling users to rely more on external representations to guide them through their interactions informing design from theory to practice theories models and conceptual frameworks provide abstractions for thinking about phenomena in particular they enable generalizations to be made about cog nition across different situations for example the concept of mental models pro vides a means of explaining why and how people interact with interactive products in the way they do across a range of situations the information processing model has been used to predict the usability of a range of different interfaces theory in its pure form however can be difficult to digest the arcane terminol ogy and jargon used can be quite off putting to those not familiar with it it also re quires much time to become familiar with it something that designers and engineers can t afford when working to meet deadlines researchers have tried to help out by making theory more accessible and practical this has included translating it into design principles and concepts design rules analytic methods design and evaluation methods chapter understanding users a main emphasis has been on transforming theoretical knowledge into tools that can be used by designers for example card et al psychological model of the human processor mentioned earlier was simplified into another model called goms an acronym standing for goals operators methods and selection rules the four components of the goms model describe how a user performs a computer based task in terms of goals save a file and the selection of meth ods and operations from memory that are needed to achieve them this model has also been transformed into the keystroke level method that essentially provides a formula for determining the amount of time each of the methods and operations takes one of the main attractions of the goms approach is that it allows quantita tive predictionsto be made see chapter for more on this another approach has been to produce various kinds of design principles such as the ones we discussed in chapter more specific ones have also been proposed for designing multimedia and virtual reality applications rogers and scaife thomas green has also proposed a framework of cognitive dimensions his overarching goal is to develop a set of high level concepts that are both valuable and easy to use for evaluating the designs of informational artifacts such as software ap plications an example dimension from the framework is viscosity which simply refers to resistance to local change the analogy of stirring a spoon in syrup high viscosity versus milk low viscosity quickly gives the idea having understood the concept in a familiar context green then shows how the dimension can be further explored to describe the various aspects of interacting with the information structure of a software application in a nutshell the concept is used to examine how much extra work you have to do if you change your mind different kinds of viscosity are described such as knock on viscosity where performing one goal related action makes necessary the performance of a whole train of extraneous actions the reason for this is constraint density the new structure that results from performing the first action violates some constraint that must be rectified by the second action which in turn leads to a different violation and so on an example is editing a document using a word processor without widow control the action of inserting a sentence at the beginning of the document means that the user must then go through the rest of the document to check that all the headers and bodies of text still lie on the same page assignment summary the aim of this assignment is for you to elicit mental models from people particular the goal is for you understand the nature ofpeople knowledge about an interactive product in terms of how to use it and how it works a first elicit your own mental model write down how you think a cash machine atm works then answer the following questions abbreviated from payne how much money are you allowed to take out if you took this out and then went to another machine and tried to withdraw the same amount what would happen what is on your card how is the information used what happens if you enter the wrong number why are there pauses between the steps of a transaction how long are they what happens if you type ahead during the pauses what happens to the card in the machine why does it stay inside the machine do you count the money why next ask two other people the same set of questions b now analyze your answers do you get the same or different explanations what do the findings indicate how accurate are people mental models of the way atms work how transparent are the atm systems they are talking about c next try to interpret your findings with respect to the design of the system are any interface features revealed as being particularly problematic what design recom mendations do these suggest d finally how might you design a better conceptual model that would allow users to develop a better mental model of atms assuming this is a desirable goal this exercise is based on an extensive study carried out by steve payne on people mental models of atms he found that people do have mental models of atms frequently resorting to analogies to explain how they work moreover he found that people explanations were highly variable and based on ad reasoning summary this chapter has explained the importance of understanding users especially their cognitive aspects it has described relevant findings and theories about how people carry out their everyday activities and how to learn from these when designing interactive products it has provided illustrations of what happens when you design systems with the user in mind and what happens when you don t it has also presented a number of conceptual frameworks that allow ideas about cognition to be generalized across different situations key points cognition comprises many processes including thinking attention learning memory perception decision making planning reading speaking and listening chapter understandingusers the way an interface is designed can greatly affect how well people can perceive attend learn and remember how to carry out their tasks the main benefits of conceptual frameworks and cognitive theories are that they can ex plain user interaction and predict user performance the conceptual framework of mental models provides a way of conceptualizing the user understanding of the system research findings and theories from cognitive psychology need to be carefully reinter preted in the context of interaction design to avoid oversimplification and misapplication further reading mullet k and d designing visual inter faces new jersey press this is an excellent book on the do and don ts of interactive graphical design it in cludes many concrete examples that have followed or not design principles based on cognitive issues carroll j ed designing interaction cambridge cambridge university press this edited volume provides a good collection of papers on cognitive aspects of interaction design norman d the psychology of everyday things new york basic books norman d things that make us smart reading ma addison wesley these two early books by don nor man provide many key findings and observations about peo ple behavior and their use of artifacts they are written in a stimulatingand thought provoking way using many exam ples from everyday life to illustrate conceptual issues he also presents a number of psychological theories including external cognition in an easily digestibleform rogers y rutherford and p eds models in the mind orlando academic press this volume provides a good collection of papers on eliciting interpret ing and theorizing about mental models in hci and other domains for more on dynalinking and interactivity see designing for boration and communication introduction social mechanisms in communication and collaboration conversational mechanisms designing collaborative technologies to support conversation coordinationmechanisms designing collaborative technologies to support coordination awareness mechanisms designing collaborative technologies to support awareness ethnographic studies of collaboration and communication conceptual frameworks the framework distributed cognition introduction imagine going into school or work each day and sitting in a room all by yourself with no distractions at first it might seem blissful you d be able to get on with your work but what if you discovered you had no access to phones the in ternet and other people on top of that there is nowhere to get coffee how long would you last probably not very long humans are inherently social they live to gether work together learn together play together interact and talk with each other and socialize it seems only natural therefore to develop interactive systems that support and extend these different kinds of sociality there are many kinds of sociality and many ways of studying it in this chapter our focus is on how people communicate and collaborate in their working and everyday lives we examine how collaborative technologies also called group ware have been designed to support and extend communication and collabora tion we also look at the social factors that influencethe success or failure of user adoption of such technologies finally we examine the role played by ethnographic studies and theoretical frameworksfor informing system design chapter design for collaboration and communication the main aims of this chapter are to explain what is meant by communication and collaboration describe the main kinds of social mechanisms that are used by people to communicate and collaborate outline the range of collaborative systems that have been developed to sup port this kind of social behavior consider how field studies and socially based theories can inform the design of collaborative systems social mechanisms in communication and collaboration a fundamental aspect of everyday life is talking during which we pass on edge to each other we continuously update each other about news changes and developments on a given project activity person or event for example friends and families keep each other posted on what happening at work school at the pub at the club next door in soap operas and in the news similarly people who work together keep each other informed about their social lives and everyday hap penings as well as what is happening at work for instance when a project is about to be completed plans for a new project problems with meeting deadlines rumors about closures and so on the kinds of knowledge that are circulated in different social circles are di verse varying among social groups and across cultures the frequency with which knowledge is disseminated is also highly variable it can happen continuously throughout the day once a day weekly or infrequently the means by which com munication happens is also flexible it can take place via face to face conversa tions telephone videophone messaging fax and letters non verbal communication also plays an important role in augmenting face to face conversa tion involving the use of facial expressions back channeling the aha and voice intonation gesturing and other kinds of body language all this may appear self evident especially when one reflects on how we inter act with one another less obvious is the range of social mechanisms and practices that have evolved in society to enable us to be social and maintain social order various rules procedures and etiquette have been established whose function is to let people know how they should behave in social groups below we describe three main categories of social mechanisms and explore how technological systems have been and can be designed to facilitate these the use of conversational mechanisms to facilitate the flow of talk and help overcome breakdowns during it the use of coordination mechanisms to allow people to work and interact together the use of awareness mechanisms to find out what is happening what others are doing and conversely to let others know what is happening social mechanisms in communication and collaboration conversational mechanisms talking is something that is effortless and comes naturally to most people and yet holding a conversation is a highly skilled collaborative achievement having many of the qualities of a musical ensemble below we examine what makes up a conver sation we begin by examining what happens at the beginning a hi there b hi c hi a all right c good how it going a fine how are you c good b ok how life treating you such mutual greetings are typical a dialog may then ensue in which the partic ipants take turns asking questions giving replies and making statements then when one or more of the participants wants to draw the conversation to a close they do so by using either implicit or explicit cues an example of an implicit cue is when a participant looks at his watch signaling indirectly to the other participants that he wants the conversation to draw to a close the other participants may choose to acknowledge this cue or carry on and ignore it either way the first par ticipant may then offer an explicit signal by saying well i must be off now got work to or oh dear look at the time must dash have to meet someone following the acknowledgment by the other participants of such implicit and ex plicit signals the conversation draws to a close with a farewell ritual the different participants take turns saying bye bye then see you repeating themselves several times until they finally separate such conversational mechanisms enable people to coordinate their talk with one another allowing them to know how to start and stop throughout a conversa tion further turn taking rules are followed enabling people to know when to lis ten when it is their cue to speak and when it is time for them to stop again to allow the others to speak sacks schegloff and jefferson who are famous for their work on conversation analysis describe these in terms of three basic rules rule the current speaker chooses the next speaker by asking an opinion question or request rule another person decides to start speaking rule the current speaker continues talking the rules are assumed to be applied in the above order so that whenever there is an opportunity for a change of speaker to occur someone comes to the end of a sentence rule is applied if the listener to whom the question or opinion is addressed does not accept the offer to take the floor the second rule is applied and chapter design for collaboration and communication someone else taking part in the conversation may take up the opportunity and offer a view on the matter if this does not happen then the third rule is applied and the current speaker continues talking the rules are cycled through recursively until someone speaks again to facilitate rule following people use various ways of indicating how long they are going to talk and on what topic for example a speaker might say right at the beginning of their turn in the conversation that he has three things to say a speaker may also explicitly request a change in speaker by saying ok that all i want to say on that matter so what do you think to a listener more subtle cues to let others know that their turn in the conversation is coming to an end include the lowering or raising of the voice to indicate the end of a question or the use of phrases like you know what i mean or simply ok back channeling huh mmm body orientation moving away from or closer to someone gaze staring straight at someone or glancing away and gesture raising of arms are also used in different combinations when talking to signal to others when someone wants to hand over or take up a turn in the conversation another way in which conversations are coordinated and given coherence is through the use of adjacency pairs shegloff and sacks utterances are as sumed to come in pairs in which the first part sets up an expectation of what is to come next and directs the way in which what does come next is heard for exam ple a may ask a question to which b responds appropriately a so shall we meet at b um can we make it a bit later say sometimes adjacency pairs get embedded in each other so it may take some time for a person to get a reply to their initial request or statement a so shall we meet at b wow look at him a yes what a funny hairdo b um can we make it a bit later say for the most part people are not aware of following conversational mechanisms and would be hard pressed to articulate how they can carry on a conversation fur thermore people don t necessarily abide by the rules all the time they may inter rupt each other or talk over each other even when the current speaker has clearly indicated a desire to hold the floor for the next two minutes to finish an argument alternatively a listener may not take up a cue from a speaker to answer a question or take over the conversation but instead continue to say nothing even though the speaker may be making it glaringly obvious it is the listener turn to say some thing many a time a teacher will try to hand over the conversation to a student in a seminar by staring at her and asking a specific question only to see the student look at the floor and say nothing the outcome is an embarrassing silence fol lowed by either the teacher or another student picking up the conversation again other kinds of breakdowns in conversation arise when someone says something that is ambiguous and the other person misinterprets it to mean something else in social mechanisms in communication and collaboration such situations the participants will collaborate to overcome the misunderstanding by using repair mechanisms consider the following snippet of conversation be tween two people a can you tell me the way to get to the multiplex ranger cinema b you go down here for two blocks and then take a right pointing to the right go on till you get to the lights and then it is on the left a oh so i go along here for a couple of blocks and then take a right and the cinema is at the lights pointing ahead of him a no you go on this street for a couple of blocks gesturing more vigorously than before to the street to the right of him while emphasizing the word this b ahhhh i thought you meant that one so it this one pointing in same di rection as the other person a uh hum yes that right this one detecting breakdowns in conversation requires the speaker and listener to be at tending to what the other says or does not say once they have understood the na ture of the failure they can then go about repairing it as shown in the above example when the listener misunderstands what has been communicated the speaker repeats what she said earlier using a stronger voice intonation and more ex aggerated gestures this allows the speaker to repair the mistake and be more ex plicit to the listener allowing her to understand and follow better what they are saying listeners may also signal when they don t understand something or want fur ther clarification by using various tokens like huh quoi or what gloff together with giving a puzzled look usually frowning this is especially the case when the speaker says something that is vague for example they might say i want it to their partner without saying what it is they want the partner may reply using a token or alternatively explicitly ask what do you mean by it taking turns also provides opportunities for the listener to initiate repair or re quest clarification or for the speaker to detect that there is a problem and to initi ate repair the listener will usually wait for the next turn in the conversation before interrupting the speaker to give the speaker the chance to clarify what is being said by completing the utterance suchman how do people repair breakdowns in conversations when using the phone or comment in these settings people cannot see each other and so have to rely on other means of ing their conversations furthermore there are more opportunities for breakdowns to occur and fewer mechanisms available for repair when a breakdown occurs over the phone peo ple will often shout louder repeating what they said several times and use when a breakdown occurs via people may literally spell out what they meant making things much more explicit in a subsequent if the message is beyond repair they may resort to another mode of communication that allows greater flexibility of sion either telephoning or speaking to the recipient face to face chapter design for collaboration and communication kinds of conversations conversations can take a variety of forms such as an argument a discussion a heated debate a chat a or giving someone a telling off a well known distinction in conversation types is between formal and informal communi cation formal communication involves assigning certain roles to people and prescribing a the types of turns that people are allowed to take in a conversa tion for example at a board meeting it is decided who is allowed to speak who speaks when who manages the turn taking and what the participants are allowed to talk about in contrast informal communication is the chat that goes on when people so cialize it also commonly happens when people bump into each other and talk briefly this can occur in corridors at the coffee machine when waiting in line and walking down the street informal conversations include talking about impersonal things like the weather a favorite and the price of living or more personal things like how someone is getting on with a new roommate it also provides an opportu nity to pass on gossip such as who is going out to dinner with whom in office set tings such chance conversations have been found to serve a number of functions including coordinating group work transmitting knowledge about office culture establishing trust and general team building kraut et al it is also the case that people who are in physical proximity such as those whose offices or desks are close to one another engage much more frequently in these kinds of informal chats than those who are in different corridors or buildings most companies and organi zations are well aware of this and often try to design their office space so that peo ple who need to work closely together are placed close to one another in the same physical space designing collaborative technologies to support conversation as we have seen talk and the way it is managed is integral to coordinating social activities one of the challenges confronting designers is to consider how the differ ent kinds of communication can be facilitated and supported in settings where there may be obstacles preventing it from happening naturally a central con cern has been to develop systems that allow people to communicate with each other when they are in physically different locations and thus not able to communi cate in the usual face to face manner in particular a key issue has been to deter mine how to allow people to carry on communicating as if they were in the same place even though they are geographically separated sometimes many thousands of miles apart videoconferencing videophones computer conferencing chatrooms and messaging are well known examples of some of the collaborative technologies that have been developed to enable this to happen other less familiar systems are collaborative virtual environments cves and media spaces cves are virtual worlds where people meet and chat these can be graphical worlds where users explore rooms and other spaces by teleporting themselves around in the guise of avatars see figure on color plate or text and graphical spaces often called and where users communicate with each other via some social mechanisms in communication and collaboration form of messaging media spaces are distributed systems comprising audio video and computer systems that extend the world of desks chairs walls and ceilings harrison et al enabling people distributed over space and time to commu nicate and interact with one another as if they were physically present the various collaborative technologies have been designed to support different kinds of communication from informal to formal and from one to one to many to many conversations collectively such technologies are often referred to as mediated communication cmc do you think it is better to develop technologies that will allow people to talk at a dis tance as if they were face to face or to develop technologies that will support new ways of conversing comment on the one hand it seems a good idea to develop technologies supporting people cating at a distance that emulate the way they hold conversations in face to face situations after all this means of communicating is so well established and second nature to people phones and videoconferencing have been developed to essentially support face to face con versations it is important to note however that conversations held in this way are not the same as when face to face people have adapted the way they hold conversations to fit in with the constraints of the respective technologies as noted earlier they tend to shout more when misunderstood over the phone they also tend to speak more loudly when talking on the phone since they can t monitor how well the person can hear them at the other end of the phone likewise people tend to project themselves more when videoconferencing turn taking appears to be much more explicit and greetings and farewells more ritualized on the other hand it is interesting to look at how the new communication technologies have been extending the way people talk and socialize for example sms text messaging has provided people with quite different ways of having a conversation at a distance people especially teenagers have evolved a new form of fragmentary conversation called ting that they continue over long periods the conversation comprises short phrases that are typed in using the key pad commenting on what each is doing or thinking allowing the other to keep posted on current developments these kinds of streamlined conversations are coordinated simply by taking turns sending and receiving messages online chatting has also enabled effectively hundreds and even thousands of people to take part in the same conversations which is not possible in face to face settings the range of systems that support computer mediated communication is quite diverse a summary table of the different types is shown in table highlighting how they support extend and differ from face to face communication a conven tionally accepted classificationsystem of cmc is to categorize them in terms of ei ther synchronous or asynchronous communication we have also included a third category systems that support cmc in combination with other collaborative ac tivities such as meetings decision making learning and collaborative authoring of documents although some communication technologies are not strictly speak ing computer based phones video conferencing we have included these in the classification of cmc as most now are display based and interacted with or controlled via an interface for more detailed overviews of cmc see dix et al chapter and baecker et al part and iv table classification of computer mediated communication cmc into three types i synchronous communication ii asynchronouscommunication and iii cmc combined with other activity i synchronous communication where conversations in real time are supported by letting people talk with each other either using their voices or through typing both modes seek to support non verbal communication to varying degrees examples talking with voice video phones video conferencing desktop or wall media spaces talking via typing text messaging typing in messages using cell phones instant messaging real time interaction via chatrooms collaborative virtual environments cves new kinds of functionality cves allow communication to take place via a combination of graphical representations of self in the form of avatars with a separate or overlaying speech bubbles cves allow people to represent themselves as virtual characters taking on new personas opposite gender and expressing themselves in ways not possible in face to face settings cves and chatrooms have enabled new forms of conversation mechanisms such as multi turn taking where a number of people can contribute and keep track of a multi streaming text based conversation instant messaging allows users to multitask by holding numerous conversations at once benefits not having to physically face people may increase shy people confidence and self esteem to converse more in virtual public it allows people to keep abreast of the goings on in an organization without having to move from their office it enables users to send text and images instantly between people using instant messaging in offices instant messaging allows users to fire off quick questions and answers without the time lag of or phone tag problems lack of adequate bandwidth has plagued video communication resulting in poor quality images that frequently break up judder have shadows and appear as unnatural images it is difficult to establish eye contact normally an integral and subconscious part of face to face conversations in cves video conferencing and videophones having the possibility of hiding behind a persona a name or an avatar in a gives people the opportunity to behave differently sometimes this can result in people becoming aggressive or intrusive ii asynchronouscommunication where communication between participants takes place remotely and at different times it relies not on dependent turn taking but on participants initiating communication and responding to others when they want or are able to do so examples bulletin boards newsgroups computer conferencing new kinds offunctionality attachments of different sorts including annotations images music for and computer conferencing can be sent messages can be archived and accessed using various search facilities benefits ubiquity can read any place any time flexibility greater autonomy and control of when and how to respond so can attend to it in own time rather than having to take a turn in a conversation at a particular cue powerful can send the same message to many people makes some things easier to say do not have to interact with person so can be easier to say things than when face to face announcing sudden death of colleague providing feedback on someone performance continued problems flaming when a user writes incensed angry expressed in uninhibited language that is much stronger than normally used when interacting with the same person face to face this includes the use of impolite statements exclamation marks capitalized sentencesor words swearing and superlatives such charged communication can lead to misunderstandings and bad feelings among the recipients overload many people experience message overload receiving over or other messages a day they find it difficult to cope and may overlook an important message while working through their ever increasing pile of if they have not read it for a few days various interface mechanisms have been designed to help people manage their better including filtering threading and the use of signaling to indicate the level of importance of a message via the sender or recipient through color coding bold font or exclamation marks placed beside a message false expectations an assumption has evolved that people will read their messagesseveral times a day and reply to them there and then however many people have now reverted to treating more like postal mail replying when they have the time to do so combined with other activity people often talk with each other while carrying out other activities for example designing requires people to brainstorm together in meetings drawing on whiteboards making notes and using existing designs teaching involves talking with students as well as writing on the board and getting students to solve problems collaboratively various meeting and decision support systems have been developed to help people work or learn while talking together examples customized electronic meeting rooms have been built that support people in face to face meetings via the use of networked workstations large public displays and shared software tools together with various techniques to help decision making one of the earliest systems was the university of arizona see figure figure schematic diagram of a group meeting room showing relationshipof work station whiteboardsand video projector continued chapter design for collaboration and communication table continued networked classrooms recently schools and universities have realized the potential of using combinations of technologies to support learning for example wireless communication portable devices and interactive whiteboards are being integrated in classroom settings to allow the teacher and students to learn and communicate with one another in novel interactive ways see figure argumentation tools which record the design rationale and other arguments used in a discussion that lead to decisions in a design conklin and these are mainly designed for people working in the same physical location shared authoring and drawing tools that allow people to work on the same document at the same time this can be remotely over the web shared authoring tools like or on the same drawing surface in the same room using multiple mouse cursors benford et al new kinds of functionality allows new ways of collaboratively creating and editing documents supports new forms of collaborative learning integrates different kinds of tools benefits supports talking while carrying out other activities at the same time allowing multi tasking which is what happens in face to face settings speed and efficiency allows multiple people to be working an same document at same time greater awareness allows users to see how one another are progressing in real time problems wysiwis what you see is what i see it can be difficult to see what other people are referring to when in remote locations especially if the document is large and different users have different parts of the document on their screens floor control users may want to work on the same piece of text or design potentially resulting in file conflicts these can be overcome by developing various social and technological floor control policies social mechanisms in communication and collaboration i e of the earliest technological innovations besides the telephone and telegraph devel ed for supporting conversations at a distance was the videophone despite numerous at tempts by the various phone companies to introduce them over the last years see figure they have failed each time why do you think this is so comment one of the biggest problems with commercial videophones is that the bandwidth is too low resulting in poor resolution and slow refresh rate the net effect is the display of unaccept able images the person in the picture appears to move in sudden jerks shadows are left be hind when a speaker moves and it is difficult to read lips or establish eye contact there is also the social acceptability issue of whether people want to look at pocket sized images of each other when talking sometimes you don t want people to see what state you are in or where you are another innovation has been to develop systems that allow people to com municate and interact with each other in ways not possible in the physical world rather than try to imitate or facilitate face to face communication like the above systems designers have tried to develop new kinds of interactions for ex ample was developed to enable facial expressions of participants to be made visible to others by using a transparent board that showed their face to the others ishii et al was designed to provide an environ ment in which the participants could feel they were in the same virtual place even figure a one of british telecom early videophonesand b a recent mobile visual phone developed in japan chapter design for collaboration and communication social mechanisms in communication and collaboration i chapter figure hypermirror in action showing perception of virtual personal space a a woman is in one room indicated by arrow on screen b while a man and another woman in the other room chat to each other they move apart when they notice they are overlap ping her and c virtual personal space is established though they were physically in different places morikawa and maesako mirror reflections of people in different places were synthesized and projected onto a single screen so that they appeared side by side in the same virtual space in this way the participants could see both themselves and others in the same seamless virtual space observations of people using the system showed how quickly they adapted to perceiving themselves and others in this way for exam ple participants quickly became sensitized to the importance of space moving out of the way if they perceived they were overlapping someone else on the screen see figure coordination mechanisms coordination takes place when a group of people act or interact together to achieve something for example consider what is involved in playing a game of basketball teams have to work out how to play with each other and to plan a set of tactics that they think will outwit the other team for the game to proceed both teams need to follow and sometimes contravene the rules of the game an in credible amount of coordination is required within a team and between the com peting teams in order to play in general collaborative activities require us to coordinate with each other whether playing a team game moving a piano navigating a ship working on a large software project taking orders and serving meals in a restaurant constructing a bridge or playing tennis in particular we need to figure out how to interact with one another to progress with our various activities to help us we use a number of coordinating mechanisms primarily these include verbal and non verbal communication schedules rules and conventions shared external representations verbal and non verbal communication when people are working closely together they talk to each other issuing com mands and letting others know how they are progressing with their part for exam ple when two or more people are collaborating together as in moving a piano they shout to each other commands like down a bit left a bit now straight for ward to coordinate their actions with each other as in a conversation nods shakes winks glances and hand raising are also used in combination with such co ordination to emphasize and sometimes replace it in formal settings like meetings explicit structures such as agendas memos and minutes are employed to coordinate the activity meetings are chaired with secretaries taking minutes to record what is said and plans of actions agreed upon such minutes are subsequently distributed to members to remind them of what was agreed in the meeting and for those responsible to act upon what was agreed for time critical and routinized collaborative activities especially where it is difficult to hear others because of the physical conditions gestures are fre quently used radio controlled communication systems may also be used vari ous kinds of hand signals have evolved with their own set of standardized syntax and semantics for example the arm and baton movements of a conductor coor dinate the different players in an orchestra while the arm and baton movements of a ground marshal at an airport signal to a pilot how to bring the plane into its allocated gate uch communication is non verbal watch a soap opera on the and turn down the and look at the kinds and frequency of gestures that are used are you able to un derstand what is going on how do radio soaps compensate for not being able to use non verbal gestures how do people compensate when chatting online comment soaps are good to watch for observing non verbal behavior as they tend to be overcharged with actors exaggerating their gestures and facial expressions to convey their emotions it is often easy to work out what kind of scene is happening from their posture body move ment gestures and facial expressions in contrast actors on the radio use their voice a lot more relying on intonation and surrounding sound effects to help convey emotions when chatting online people use emoticons and other specially evolved verbal codes schedules rules and conventions a common practice in organizations is to use various kinds of schedules to orga nize the people who are part of it for example consider how a university manages to coordinate the people within it with available resources a core task is allo cating the thousands of lectures and seminars that need to be run each week with the substantially smaller number of rooms available a schedule has to be devised chapter design for collaboration and communication that allows students to attend the lectures and seminars for their given courses tak ing into account numerous rules and constraints these include a student cannot attend more than one lecture at a given time a professor cannot give more than one lecture or seminar at a given time a room cannot be allocated to more than one seminar or lecture at a given time only a certain number of students can be placed in a room depending on its size shared external representations shared external representations are commonly used to coordinate people we have already mentioned one example that of shared calendars that appear on user monitors as graphical charts reminders and dialog boxes other kinds that are commonly used include forms checklists and tables these are pre sented on public noticeboards or as part of other shared spaces they can also be attached to documents and folders they function by providing external informa tion of who is working on what when where when a piece of work is supposed to be finished and who it goes to next for example a shared table of who has com pleted the checking of files for a design project see figure provides the nec essary information from which other members of the group can at a glance update their model of the current progress of that project importantly such external rep resentations can be readily updated by annotating if a project is going to take longer than planned this can be indicated on a chart or table by extending the line representing it allowing others to see the change when they pass by and glance up at the whiteboard shared externalizations allow people to make various inferences about the changes or delays with respect to their effect on their current activities accordingly figure an external representation used to coordinate collaborative work in the form of a print out table showing who has completed the checking of files and who is down to do what chapter design for collaboration and communication they may need to reschedule their work and annotate the shared workplan in so doing these kinds of coordination mechanisms are considered to be tangible pro viding important representations of work and responsibility that can be changed and updated as and when needed designing collaborative technologies to support coordination shared calendars electronic schedulers project management tools and workflow tools that provide interactive forms of scheduling and planning are some of the main kinds of collaborative technologies that have been developed to support coordination a specific mechanism that has been implemented is the use of con ventions for example a shared workspace system called that sup ported and document sharing to allow politicians to work together at different sites introduced a range of conventions these included how folders and files should be organized in the shared workspace interestingly when the system was used in practice it was found that the conventions were often violated mark et al for example one convention that was set up was that users should always type in the code of a file when they were using it in practice very few peo ple did this as pointed out by an administrator they don t type in the right code i must correct them i must sort the documents into the right archive and that annoying the tendency of people not to follow conventions can be due to a number of reasons if followingconventions requires additional work that is extraneous to the users ongoing work they may find it gets in the way they may also perceive the convention as an unnecessary burden and forget to follow it all the time such productive laziness rogers is quite common a simple analogy to every day life is forgetting to put the top back on the toothpaste tube it is a very simple convention to follow and yet we are all guilty sometimes or even all the time of not doing this while such actions may only take a tiny bit of effort people often don t do them because they perceive them as tedious and unnecessary however the consequence of not doing them can cause grief to others when designing coordination mechanisms it is important to consider how so cially acceptable they are to people failure to do so can result in the users not using the system in the way intended or simply abandoning it a key part is getting the right balance between human coordination and system coordination too much system control and the users will rebel too little control and the system breaks down consider the example of file locking which is a form of concurrency control this is used by most shared applications shared authoring tools file sharing systems to prevent users from clashing when trying to work on the same part of a shared document or file at the same time with file locking whenever someone is working on a file or part of it it becomes inaccessible to others information about who is using the file and for how long may be made available to the other users to show why they can t work on a particular file when file locking mechanisms are used in this way however they are often considered too rigid as a form of coordi nation primarily because they don t let other users negotiate with the first user about when they can have access to the locked file social mechanisms in communication and collaboration a more flexibleform of coordination is to include a social policy of floor con trol whenever a user wants to work on a shared document or he must initially request the floor if no one else is using the specified section or file at that time then he is given the floor that part of the document or file then becomes locked preventing others from having access to it if other users want access to the file they likewise make a request for the floor the current user is then notified and can then let the requester know how long the file will be in use if not acceptable the requester can try to negotiate a time for access to the file this kind of coordination mechanism therefore provides more scope for negotiation between users on how to collaborate rather than simply receiving a point blank permission denied re sponse from the system when a file is being used by someone else why are whiteboardsso useful for coordinating projects how might electronic whiteboards be designed to extend this practice comment physical whiteboards are very good as coordinating tools as they display information that is external and public making it highly visible for everyone to see furthermore the informa tion can be easily annotated to show up to date modifications to a schedule whiteboards also have a gravitational force drawing people to them they provide a meeting place for people to discuss and catch up with latest developments electronic whiteboards have the added advantage that important information can be ani mated to make it stand out important information can also be displayed on multiple dis plays throughout a building and can be extracted from existing databases and software thereby making the project coordinator work much easier the boards could also be used to support on the fly meetings in which individuals could use electronic pens to sketch out ideas that could be electronically in such settings they could also be interacted with via wireless handheld computers allowing to be scraped off or squirted onto the whiteboard awareness mechanisms awareness involves knowing who is around what is happening and who is talk ing with whom dourish and bly for example when we are at a party we move around the physical space observing what is going on and who is talking to whom eavesdropping on others conversations and passing on gossip to others a specific kind of awareness is peripheral awareness this refers to a person abil ity to maintain and constantly update a sense of what is going on in the physical and social context through keeping an eye on what is happening in the periphery of their vision this might include noting whether people are in a good or bad mood by the way they are talking how fast the drink and food is being consumed who has entered or left the room how long someone has been absent and whether the lonely guy in the corner is finally talking to someone all while we are having a conversation with someone else the combination of direct observa tions and peripheral monitoring keeps people informed and updated of what is happening in the world similar ways of becoming aware and keeping aware take place in other con texts such as a place of study or work importantly this requires fathoming when is an appropriate time to interact with others to get and pass information on seeing a professor slam the office door signals to students that this is defi nitely not a good time to ask for an extension on an assignment deadline con versely seeing teachers with beaming faces chatting openly to other students suggests they are in a good mood and therefore this would be a good time to ask them if it would be all right to miss next week seminar because of an important family engagement the knowledge that someone is amenable or not rapidly spreads through a company school or other institution people are very eager to pass on both good and bad news to others and will go out of their way to gossip loitering in corridors hanging around at the photocopier and coffee machine spreading the word social mechanisms in communication and collaboration figure an external representation used to signal to others a person availability in addition to monitoring the behaviors of others people will organize their work and physical environment to enable it to be successfully monitored by others this ranges from the use of subtle cues to more blatant ones an example of a sub tle cue is when someone leaves their dorm or office door slightly ajar to indicate that they can be approached a more blatant one is the complete closing of their door together with a do not disturb notice prominently on it signaling to every one that under no circumstancesshould they be disturbed see figure overhearing and overseeing people who work closely together also develop various strategies for coordinating their work based on an up to date awareness of what the others are doing this is especially so for interdependent tasks where the outcome of one person activity is needed for others to be able to carry out their tasks for example when putting on a show the performers will constantly monitor what one another is doing in order to coordinate their performance efficiently the metaphorical expression closely knit teams exemplifies this way of col laborating people become highly skilled in reading and tracking what others are doing and the information they are attending to a well known study of this phe nomenon is described by christian heath and paul luff who looked at how two controllers worked together in a control room in the london underground an overriding observation was that the actions of one controller were tied very closely to what the other was doing one of the controllers was responsible for the movement of trains on the line controller a while the other was responsible for providing information to passengers about the current service controller b in many instances it was found that controller b overheard what controller a was doing and saying and acted accordingly even though controller a had not said anything explicitly to him for example on overhearing controller a discussing a problem with a train driver over the in cab intercom system controller b inferred from the ensuing conversation that there was going to be a disruption to the service design for collaboration and communication and so started announcing this to the passengers on the platform before controller a had even finished talking with the train driver at other times the two con trollers keep a lookout for each other monitoring the environment for actions and events which they might have not noticed but may be important for them to know about so that they can act appropriately hat do you think happens when one person of a closely knit team does not see or hear ething or misunderstands what has been said while the others in the group assume they have seen heard or understood what has been said comment in such circumstances the person is likely to carry on as normal in some cases this will sult in inappropriate behavior repair mechanisms will then need to be set in motion the knowledgeable participants may notice that the other person has not acted in the manner expected they may then use one of a number of subtle repair mechanisms say coughing or glancing at something that needs attending to if this doesn t work they may then re sort to explicitly stating aloud what had previously been signaled implicitly conversely the unaware participant may wonder why the event hasn t happened and likewise look over at the other people cough to get their attention or explicitly ask them a question the kind of mechanism employed at a given moment will depend on a number of factors including the relationship among the participants whether one is more se nior than the others this determines who can ask what perceived fault or responsibility for the breakdown and the severity of the outcome of not acting there and then on the new information designing collaborative technologies to support awareness the various observations about awareness have led system developers to con sider how best to provide awareness information for people who need to work to gether but who are not in the same physical space various technologies have been employed along with the design of specific applications to convey informa tion about what people are doing and the progress of their ongoing work as mentioned previously audio video links have been developed to enable remote colleagues to keep in touch with one another some of these systems have also been developed to provide awareness information about remote partners allow ing them to find out what one another is doing one of the earliest systems was portholes developed at xerox parc research labs dourish and bly the system presented regularly updated digitized video images of people in their of fices from a number of different locations in the us and uk these were shown in a matrix display on people workstations clicking on one of the images had the effect of bringing up a dialog box providing further information about that in dividual name phone number together with a set of lightweight action but tons the person listen to a pre recorded audio snippet the system provided changing images of people throughout the day and night in their offices letting others see at a glance whether they were in their offices what they were working on and who was around see figure informal evaluation of the social mechanisms in communicationand collaboration figure a screen dump of portholes showing resolution monochrome images from offices in the us and uk parc sites permission from xerox research centre europe set up suggested that having access to such information led to a shared sense of community the emphasis in the design of these early awareness systems was largely on supporting peripheral monitoring allowing people to see each other and their progress dourish and bellotti refer to this as shared feedback more recent distributed awareness systems provide a different kind of awareness information rather than place the onus on participants to find out about each other they have been designed to allow users to notify each other about specific kinds of events thus there is less emphasis on monitoring and being monitored and more on ex plicitly letting others know about things notification mechanisms are also used to provide information about the status of shared objects and the progress of collabo rative tasks hence there has been a shift towards supporting a collective stream of con sciousness that people can attend to when they want to and likewise provide in formation for when they want to an example of a distributed awareness system is developed at the university of queensland segall and arnold which provides a range of client services a highly successful client is tickertape which is a lightweight instant messaging system showing small color coded messages that scroll from right to left across the screen fitzpatrick et it has been most useful as a chat and local organizing tool allowing people in different locations to effortlesslysend brief messages and requests to the public tickertape display see figure it has been used for a range of functions including organizing shared events lunch dates making announcements and as an always on communi cation tool for people working together on projects but who are not physically located it is also often used as a means of mediating help between people for example when i was visiting the university of queensland i asked for help over tickertape within minutes i was inundated with replies from people logged onto the system who did not even know me at the time i was having problems working out the key mappings between the pc that i was using in australia and a edi tor i couldn t find a way of quitting from on a remote machine in the the sug gestions that appeared on tickertape quickly led to a discussion among the participants and within five minutes someone had come over to my desk and sorted the problem out for me in addition to presenting awareness information as streaming text messages more abstract forms of representation have been used for example a communica tion tool called babble developed at ibm erickson et al provides a dy namic visualization of the participants in an ongoing chat like conversation a large circle is depicted with colored marbles on each user monitor marbles inside the circle convey those individuals active in the current conversation mar bles outside the circle convey users involved in other conversations the more ac tive a participant is in the conversation the more the corresponding marble is moved towards the center of the circle conversely the less engaged a person is in the ongoing conversation the more the marble moves towards the periphery of the circle see figure the interface dynamic visualization of participants in ongoing conversation studies of collaboration and communication ethnographic studies of collaboration and communication one of the main approaches to informing the design of collaborative technolo gies that takes into account social concerns is carrying out an ethnographic study a type of field study observations of the setting be it home work school pub lic place or other setting are made examining the current work and other col laborative practices people engage in the way existing technologies and everyday artifacts are used is also analyzed the outcome of such studies can be very illuminating revealing how people currently manage in their work and everyday environments they also provide a basis from which to consider how such existing settings might be improved or enhanced through the introduction of new technologies and can also expose problematic assumptions about how collaborative technologies will or should be used in a setting for more on how to use ethnography to inform design see chapter how to do ethnography is cov ered in chapter many studies have analyzed in detail how people carry out their work in differ ent settings plowman et al the findings of these studies are used both to inform the design of a specific system intended for a particular workplace and more generally to provide input into the design of new technologies they can also highlight problems with existing system design methods for example an early study by lucy looked at the way existing office technologies were being designed in relation to how people actually worked she observed what really happened in a number of offices and found that there was a big mismatch between the way work was actually accomplished and the way people were supposed to work using the office technology provided she argued that designers would be much better positioned to develop systems that could match the way people be have and use technology if they began by considering the actual details of work practice in her later much cited study of how pairs of users interacted with an interac tive help system intended as a facility for using with a photocopier suchman again stressed the point that the design of interactive systems would greatly benefit from analyses that focused on the unique details of the user particular uation rather than being based on preconceived models of how people ought to and will follow instructions and procedures her detailed analysis of how the help system was unable to help users in many situations highlighted the inade quacy of basing the design of an interactive system purely on an abstract user model since suchman seminal work a large number of ethnographic studies have examined how work gets done in a range of companies fashion design multi media newspapers and local government other settings have also recently come under scrutiny to see how technologies are used and what people do at home in public places in schools and even cyberspace here the objective has been to un derstand better the social aspects of each setting and then to come up with implica tions for the design of future technologies that will support and extend these for more on the way user studies can inform future technologies see the interview at the end of this chapter with abigail chapter design for collaboration and communication conceptual frameworks a number of conceptual frameworks of the social have been adapted from other disciplines like sociology and anthropology as with the conceptual frameworks derived from cognitive approaches the aim has been to provide analytic frame works and concepts that are more amenable to design concerns below we briefly describe two well known approaches that have quite distinct origins and ways of informing interaction design these are framework distributed cognition the first describes how a model of the way people communicate was used to in form the design of a collaborative technology the second describes a theory that is used primarily to analyze how people carry out their work using a variety of technologies the framework the basic premise of the framework is that people act through lan guage winograd and flores it was developed to inform the design of sys tems to help people work more effectively through improving the way they communicate with one another it is based on various theories of how people use language in their everyday activities most notably speech act theory speech act theory is concerned with the functions utterances have in conversa tions austin searle a common function is a request that is asked indi rectly known as an indirect speech act for example when someone says it hot in here they may really be asking if it would be ok to open the window because they need some fresh air speech acts range from formalized statements i hereby declare you man and wife to everyday utterances how about dinner there are five categories of speech acts assertives commit the speaker to something being the case commissives committhe speaker to some future action declarations pronouncesomething has happened directives get the listener to do something expressives express a state of affairs such as apologizingor praising someone each utterance can vary in its force for example a command to do something has quite a different force from a polite comment about the state of affairs the approach was developed further into a framework called conversations for action essentially this framework describes the se quence of actions that can follow from a speaker making a request of someone else it depicts a conversation as a kind of dance see figure involvinga se ries of steps that are seen as following the various speech acts different dance steps ensue depending on the speech acts followed the most straightforward kind of dance involves progressing from state through to state of the conversation conceptual frameworks a declare figure 13 conversation for action diagram from and flores p in a linear order for example a state may request b to do homework state b may promise to do it after she has watched a program state b may then report back to a that the homework is done state and a having looked at it declares that this is the case state reality conversation dances tend to be more complex for example a may look at the homework and see that it is very shoddy and request that b complete it properly the conversation is thus moved back a step b may promise to do the homework but may in fact not do it at all thereby canceling their promise state or a may say that b doesn t need to do it any more state b may also suggest an alternative like cooking dinner moving to state the framework was used as the basis of a conceptual model for a com mercial software product called the coordinator the goal was to develop a system to facilitate communication in a variety of work settings like sales finance general management and planning the coordinator was designed to enable electronic messages to be sent between people in the form of explicit speech acts when send ing someone a request say could you get the report to me the sender was also required to select the menu option request this was placed in the subject header of the message thereby explicitly specifying the nature of the speech act other speech act options included offer promise inform and question see figure the system also asked the user to fill in the dates by which the request should be completed another user receiving such a message had the option of responding with another labeled speech act these included acknowledge promise counter offer decline free form chapter design for collaboration and communication figure menu items for initiating a conversation thus the coordinator was designed to provide a straightforward conversa tional structure allowing users to make clear the status of their work and like wise to be clear about the status of others work in terms of various commitments to reiterate a core rationale for developing this system was to try to improve people ability to communicate more effectively earlier research had shown how communication could be improved if participants were able to distinguish among the kinds of commitments people make in conversation and also the time scales for achieving them these findings suggested to and flores that they might achieve their goal by designing a communication system that enabled users to develop a better awareness of the value of using speech acts users would do this by being explicit about their intentions in their messages to one another normally the application of a theory backed up with empirical research is re garded as a fairly innocuous and systematic way of informing system design how ever in this instance it opened up a very large can of worms much of the research community at the time was incensed by the assumptions made by and flores in applying speech act theory to the design of the coordinator system many heated debates ensued often politically charged a major concern was the extent to which the system prescribed how people should communicate it was pointed out that asking users to specify explicitly the nature of their implicit speech acts was contrary to what they normally do in conversations forcing people to communicate in such an artificial way was regarded as highly undesirable while some people may be very blatant about what they want doing when they want it done by and what they are prepared to do most people tend to use more subtle and indirect forms of communication to advance their collaborations with others the problem that and flores came up against was people resistance to radically change their way of communicating indeed many of the people who tried using the coordinator system in their work organizations either abandoned it or resorted to using only the free form message facility which had no explicit demands associated with it in these conceptual frameworks texts the system failed because it was asking too much of the users to change the way they communicated and worked however it should be noted that the coordi nator was successful in other kinds of organizations namely those that are highly structured and need a highly structured system to support them in particular the most successful use of the coordinator and its successors has been in organizations like large manufacturing divisions of companies where there is a great need for considerable management of orders and where previous support has been mainly in the form of a hodgepodge of paper forms and inflexible task specific data pro cessing applications winograd distributed cognition in the previous chapter we described how traditional approaches to modeling cog nition have focussed on what goes on inside one person head we also mentioned that there has been considerable dissatisfaction with this approach as it ignores how people interact with one another and their use of artifacts and external repre sentations in their everyday and working activities to redress this situation ed and his colleagues developed the distributed cognition approach as a new paradigm for conceptualizing human work activities hutchins see fig ure the distributed cognition approach describes what happens in a cognitive sys tem typically this involves explaining the interactions among people the artifacts figure comparison of traditional and distributed cognition approaches chapter design for collaboration and communication i air traffic controller control center propagation of representational states atc gives clearance to pilot to fly to higher altitude verbal pilot changes altitude meter mental and physical captain observes pilot visual captain flies to higher altitude mental and physical figure 16 a cognitive system in which information is propagated through different media they use and the environment they are working in an example of a cognitive sys tem is an airline cockpit where a top level goal is to fly the plane this involves the pilot co pilot and air traffic controller interacting with one another the pilot and co pilot interacting with the instruments in the cockpit the pilot and co pilot interacting with the environment in which the plane is flying sky runway a primary objective of the distributed cognition approach is to describe these interactions in terms of how information is propagated through different media by this is meant how information is represented and re represented as it moves across individuals and through the array of artifacts that are used maps instrument readings scribbles spoken word during activities these transformations of infor mation are referred to as changes in representational state this way of describing and analyzing a cognitive activity contrasts with other cognitive approaches the information processing model in that it focuses not on what is happening inside the heads of each individual but on what is happening across individuals and artifacts for example in the cognitive system of the cockpit a number of people and artifacts are involved in the activity of flying to a higher altitude the air trafficcontroller initially tells the co pilot when it is safe to fly to a higher altitude the co pilot then alerts the pilot who is flying the plane by mov ing a knob on the instrument panel in front of them indicating that it is now safe to fly see figure 16 hence the information concerning this activity is transformed through different media over the radio through the co pilot and via a change in the position of an instrument a distributed cognition analysis typically involves examining the distributed problem solving that takes place including the way people work together to solve a problem the role of verbal and non verbal behavior including what is said what is implied by glances winks etc and what is not said the various coordinating mechanisms that are used rules procedures the various communicative pathways that take place as a collaborative activ ity progresses how knowledge is shared and accessed in addition an important part of a distributed cognition analysis is to identify the problems breakdowns and concomitant problem solving processes that emerge to deal with them the analysis can be used to predict what would happen to the way information is propagated through a cognitive system using a different arrangement of technologies and artifacts and what the consequences of this would be for the current work setting this is especially useful when designing and evalu ating new collaborative technologies chapter design collaboration and communication there are several other well known conceptual frameworks that are used to analyze how people collaborate and communicate including activity theory nomethodology situated action and common ground theory assignment the aim of this design activity is for you to analyze the design of a collaborative virtual envi ronment with respect to how it is designed to support collaboration and communication visit an existing many are freely downloadable such as v chat one of the many worlds away environments or the palace try to work out how they have been designed to take into account the following a general social issues what is the purpose of the cve what kinds of conversation mechanisms are supported what kinds of coordination mechanisms are provided what kinds of social protocols and conventions are used what kinds of awareness information is provided does the mode of communication and interaction seem natural or awkward b specific interaction design issues what form of interaction and communication is supported textlaudiolvideo what other visualizations are included what information do they convey how do users switch between different modes of interaction exploring and chatting is the switch seamless are there any social phenomena that occur specific to the context of the cve that wouldn t happen in face to face settings flaming c design issues what other features might you include in the cve to improve communication and collaboration summary further reading in this chapter we have looked at some core aspects of sociality namely communication and collaboration we examined the main social mechanisms that people use in different settings in order to collaborate a number of collaborative technologies have been designed to sup port and extend these mechanisms we looked at representative examples of these high lighting core interaction design concerns a particular concern is social acceptability that is critical for the success or failure of technologies intended to be used by groups of people working or communicating together we also discussed how ethnographic studies and theo retical frameworks can play a valuable role when designing new technologies for work and other settings key points social aspects are the actions and interactions that people engage in at home work school and in public the three main kinds of social mechanism used to coordinate and facilitate social aspects are conversation coordination and awareness talk and the way it is managed is integral to coordinating social activities many kinds of computer mediated communication systems have been developed to en able people to communicate with one another when in physically different locations external representations rules conventions verbal and non verbal communication are all used to coordinate activities among people it is important to take into account the social protocols people use in face to face collabo ration when designing collaborative technologies keeping aware of what others are doing and letting others know what you are doing are important aspects of collaborative working and socializing ethnographic studies and conceptual frameworks play an important role in understand ing the social issues to be taken into account in designing collaborative systems getting the right level of control between users and system is critical when designing col laborative systems further reading a j g and r human computer interaction upper saddle river nj prentice hall this textbook provides a comprehensive overview of groupware systems and the field of cscw in chapters 13 and 14 engestrom y and middleton d eds cog nition and communication at work cambridge cam bridge university press a good collection of classic ethnographic studies that examine the relationship be tween different theoretical perspectives and field studies of work practices j online communities designing usability supporting sociability new york john wiley and sons this book combines usability and sociability issues to do with designing online communities r m grudin j buxton w a s and greenberg s eds readings in human computer interaction toward the year second edition san francisco ca morgan kaufmann baecker r m ed readings in groupware and computer supported cooperative work assisting human collaboration san mateo ca morgan kaufmann these two collections of readings include a number of repre sentative papers from the field of cscw ranging from so cial to system architecture issues munro a j hook k and d eds social navigation of information space new york springer ver lag provides a number of illuminating papers that explore how people navigate information spaces in real and virtual worlds and how people interact with one another in them chapter design collaboration and communication abigail is a senior re searcher at hewlett packard labs in uk her work involves carrying out user studies to inform the development of future prod ucts including appliances and web based services she has a background in coanitive science and human factors engineering having obtained her doctor ate at the university of cali fornia prior to this abiaail worked at xerox research labs in cambridge uk computer inc she has also worked as an academic researcher at the computer systems research institute at the university of toronto canada and the applied psychology unit in cam bridge uk she has written widely on the social and cognitive aspects of paper use video input devices human memory and human error ail with an eye to the de sign of new technologies yr could you tell me what you do at packard research labs as sure i ve been at hp labs for a number of years now as a member of its user studies and design group this is a smallish group consisting of five so cial scientists and three designers our work can best be described as doing three things we do that are group led around particular themes for ex ample how people use digital music or how people capture documents using scanning technology we do consulting work for development teams at hp and thirdly we do a little bit of our own individual work like writing papers and books and giving talks yr right could you tell me about user studies what they are and why you consider them important as ok user studies essentially involve looking at how people behave either in their natural habitats or in the laboratory both with old technologies and with new ones i think there are many different questions that these kinds of studies can help you answer let me name a few one question is who is going to be the potential user for a particular device or service that you are thinking of developing a second ques tion which i think is key is what is the potential value of a particular product for a user once we know this we can then ask for a particular situation or task what features do we want to deliver and how best should we deliver those features this includes for example what would the interface look like fi nally i think user studies are important to understand how users lives may change and how they will be af fected by introducing a new technology this has to take into account the social physical and technologi cal context into which it will be introduced yr so it sounds like you have a set of general questions you have in mind when you do a user study could you now describe how you would do a user study and what kinds of things you would be looking for as well i think there are two different classes of user studies and both are quite different in the ways you go about them there are evaluation studies where we take a concept a prototype or even a devel oped technology and look at how it is used and then try to modify or improve it based on what we find the second class of user studies is more about discov ering what people unmet needs may be this means trying to develop new concepts and ideas for things that people may never have thought of before this is difficult because you can t necessarily just ask people what they would like or what they would use instead you have to make inferences from studying people in different situations and try to understand from this what they might need or value yr in the book we mention the importance of tak ing into account social aspects such as awareness of others how people communicate with each other and so on do you think these issues are important when you are doing these two kinds of user studies as well yes and in particular i think social aspects really are playing to that second class of user study i mentioned where you are trying to discover what people unmet needs or requirements may be here you are trying to get rich descriptions about what people do in the context of their everyday whether this is in their working lives their home lives or lives on the move i d say getting the social aspects understood is often very important in trying to under stand what value new products and services might interview bring to people day to day activities and also how they would fit into those existing activities yr and what about cognitive aspects such as how people carry out their tasks what they remember what they are bad at remembering is that also im portant to look into when you are doing these kinds of studies as yes if you think about evaluation studies then cognitive aspects are extremely important looking at cognitive aspects can help you understand the nature of the user interaction in particular what processes are going on in their heads this includes issues like learning how users perceive a device and how they form a mental model of how something works cogni tive issues are especially important to consider when we want to contrast one device with another or think about new and better ways in which we might design an interface yr i wonder if you could describe to me briefly one of your recent studies where you have looked at cog nitive and social aspects as how about a recent study we did to do with building devices for reading digital documents when we first set out on this study before we could begin to think about how to build such devices we had to begin by asking what do we mean by reading it turned out there was not a lot written about the dif ferent ways people read in their day to day lives so the first thing we did was a very broad study looking at how people read in work situations the technique we used here was a combination of asking people to fill out a diary about their reading activities during the course of a day and interviewing them at the end of each day the interviews were based around what was written in the diaries which turned out to be a good way of unpacking more details about what people had been doing that initial study allowed us to categorize all the different ways people were reading what we found out is that actually you can t talk about reading in a generic sense but that it falls into at least different categories for example sometimes people skim read sometimes they read for the purpose of writing something and sometimes they read very reflectively and deeply marking up their documents as they go what quickly emerged from this first study was that if you re designing a device for reading it might look very different depending on the kind of reading the users are doing so for example if they re reading by themselves the screen size and viewing angle may not be as important as if they re reading with others if they re skim reading the ability to quickly flick through pages is important and if they re reading and writing then this points to the need for a based interface all of these issues become important design considerations this study then led to the development of some design concepts and ideas for new kinds of reading devices at this stage we involved designers to de velop different props to get feedback and reactions from potential users a prop could be anything from a quick sketch to an animation to a styrofoam once you have this initial design work you can then begin to develop working prototypes and test them with realistic tasks in both laboratory and natural settings some of this work we have already completed but the project has had an impact on sev eral different research and development efforts yr would you say that user studies are going to be come an increasingly important part of the interaction design process especially as new technologies like ubiquitous computing and handheld devices come into being and where no one really knows what ap plications to develop as yes i think the main contribution of user stud ies say 15 years ago was in the area of evaluation and usability testing i think that role is changing now in that user studies researchers are not only those who evaluate devices and interfaces but also those who de velop new concepts also another important devel opment is a change in the way the research is carried out more and more i am finding that teams are draw ing together people from other disciplines such as so ciologists marketing people designers and people from business and technology development yr so they are essentially working as a multidisci plinary team finally what is it like to work in a large organization like hp with so many different departments as one thing about working for a large organiza tion is that you get a lot of variety in what you can do you can pick and choose to some extent and de pending on the organization don t have to be tied to a particular product if on the other hand you work chapter design for collaboration and communication for a smaller organization such as a start up teams they put huge pressures on you because they pany inevitably there is lots of pressure to get things have huge pressures on them you really have to out the door quickly things are often very focused work at effectively incorporating user studies find whether large or small however i think one of the ings into the development process this can be in hardest things i have found in working for corporate credibly challenging but it also satisfying to have research is learning to work with the development an impact on real products understanding how interfaces affect users introduction what are affective aspects expressive interfaces user frustration a debate the application of anthropomorphism to interaction design virtual characters agents kinds of agents general design concerns believability of virtual characters introduction an overarching goal of interaction design is to develop interactive systems that elicit positive responses from users such as feeling at ease being comfortable and enjoying the experience of using them more recently designers have become in terested in how to design interactive products that elicit specific kinds of emotional responses in users motivating them to learn play be creative and be social there is also a growing concern with how to design that people can trust that make them feel comfortable about divulging personal information or making a purchase we refer to this newly emerging area of interaction design as affective aspects in this chapter we look at how and why the design of computer systems cause cer tain kinds of emotional responses in users we begin by looking in general at ex pressive interfaces examining the role of an interface appearance on users and how it affects usability we then examine how computer systems elicit negative re sponses user frustration following this we present a debate on the controver sial topic of anthropomorphism and its implications for designing applications to have human like qualities finally we examine the range of virtual characters de signed to motivate people to learn buy listen etc and consider how useful and appropriate they are chapter understanding how interfaces affect users the main aims of this chapter are to explain what expressive interfaces are and the affects they can have on people outline the problems of user frustration and how to reduce them debate the pros and cons of applying anthropomorphism in interaction design assess the believability of different kinds of agents and virtual characters enable you to critique the persuasive impact of e commerce agents on customers what are affective aspects in general the term affective refers to producing an emotional response for ex ample when people are happy they smile affective behavior can also cause an emotional response in others so for example when someone smiles it can cause others to feel good and smile back emotional skills especially the ability to ex press and recognize emotions are central to human communication most of us are highly skilled at detecting when someone is angry happy sad or bored by recog nizing their facial expressions way of speaking and other body signals we are also very good at knowing what emotions to express in given situations for example when someone has just heard they have failed an exam we know it is not a good time to smile and be happy instead we try to empathize it has been suggested that computers be designed to recognize and express emotions in the same way humans do the term coined for this ap proach is affective computing a long standing area of research in artificialintel ligence and artificial life has been to create intelligent robots and other computer based systems that behave like humans and other creatures one well known project is cog in which a number of researchers are attempting to build an artificial two year old one of the offsprings of cog is kismet breazeal which has been designed to engage in meaningfulsocial interactions with hu mans see figure our concern in this chapter takes a different approach how can interactive systems be designed both deliberately and inadvertently to make people respond in certain ways figure kismet the robot expressing surprise anger and happiness expressive interfaces expressive interfaces a well known approach to designing affective interfaces is to use expressive icons and other graphical elements to convey emotional states these are typically used to indicate the current state of a computer for a hallmark of the apple computer is the icon of a smiling mac that appears on the screen when the machine is first started see figure the smiling icon conveys a sense of friendliness inviting the user to feel at ease and even smile back the appearance of the icon on the screen can also be very reassuring to users indicating that their computer is working fine this is especially useful when they have just rebooted the computer after it has crashed and where previous attempts to reboot have failed usually in dicated by a sad icon face see figure other ways of conveying the status of a system are through the use of dynamic icons a recycle bin expanding when a file is placed into it animations a bee flying across the screen indicating that the computer is doing something like checking files spoken messages using various kinds of voices telling the user what needs to be done various sounds indicating actions and events window closing files being dragged new arriving one of the benefits of these kinds of expressive embellishments is that they provide reassuring feedback to the user that can be both informative and fun the style of an interface in terms of the shapes fonts colors and graphical el ements that are used and the way they are combined influences how pleasurable it is to interact with the more effective the use of imagery at the interface the more engaging and enjoyable it can be mullet and sano conversely if little thought is given to the appearance of an interface it can turn out looking like a dog dinner until recently hci has focused primarily on getting the usability right with little attention being paid to how to design aesthetically pleasing inter faces interestingly recent research suggests that the aesthetics of an interface can figure a smiling and b sad apple chapter understanding how interfaces affect users have a positive effect on people perception of the system usability sky moreover when the look and feel of an interface is pleasing beautiful graphics nice feel to the way the elements have been put together designed fonts elegant use of images and color users are likely to be more tolerant of its usability they may be prepared to wait a few more seconds for a to download as we have argued before interaction design should not just be about usability per se but should also include aesthetic design such as how pleasur able an interface is look at or listen to the key is to get the right balance be tween usability and other design concerns like aesthetics see figure on color plate a question of style or stereotype figure shows two differently designed dialog boxes describe how they differ in terms of style of the two which one do you prefer why which one do you think i europeans would like the most and americans would like the most comment aaron marcus a graphic designer created the two designs in an attempt to provide appealing interfaces dialog box a was designed for white american females while dialog box b was designed for european adult male intellectuals the rationale behind marcus ideas was that european adult male intellectuals like suave prose a restrained treatment of information density and a classicalapproach to font selection the use of serif type in axial symmetric layouts similar to those found in elegant bronze european building identification signs in contrast white american females prefer a more detailed presentation curvilinear shapes and the absence of some of the more brutal terms favored by male software engineers when the different interfaces were empirically tested by et al their re sults did not concur with marcus assumptions in particular they found that the european dialog box was liked the best by all people and was considered most appropriate for all users moreover the round dialog box designed for women was strongly disliked by every one the assumption that women like curvilinear features clearly was not true in this con text at the very least displaying the font labels in a circular plane makes them more difficult to read when in the conventionally accepted horizontal plane another popular kind of expressive interface is the friendly interface agent a general assumption is that novices will feel more at ease with this kind of compan ion and will be encouraged to try things out after listening watching following and interacting with them for example pioneered a new class of based software called bob aimed at new computer users many of whom were seen as computer phobic the agents were presented as friendly characters in cluding a friendly dog and a cute bunny an underlying assumption was that having these kinds of agents on the screen would make the users feel more comfortable and at ease with using the software an interface metaphor of a warm cozy living room replete with fire furnishings and furniture was provided see figure again intended to convey a comfortable feeling since the creation of bob microsoft has developed other kinds of agents in cluding the infamous clippy a paper clip that has human like qualities as part figure square and round dialog boxes designed by aaron marcus a dialog box designed for white american women and b dialog box designed for european adult male intellectuals how interfaces affect users figure at home with bob software of their windows operating environment the agents typically appear at the bottom of the screen whenever the system thinks the user needs help carrying out a particular task they too are depicted as cartoon characters with friendly warm personalities as mentioned in chapter one of the problems of using agents in this more general context is that some users do not like them more expe rienced users who have developed a reasonably good mental model of the system often find such agent helpers very trying and quickly find them annoying intrusions especially when they distract them from their work we return to anthropomor phism and the design of interface agents later in section users themselves have also been inventive in expressing their emotions at the computer interface one well known method is the use of emoticons these are keyboard symbols that are combined in various ways to convey feelings and emo tions by facial expressions like smiling winking and frowning on the screen the meaning of an emoticon depends on the content of the message and where it is placed in the message for example a smiley face placed at the end of a message can mean that the sender is happy about a piece of news she has just writ ten about alternatively if it is placed at the end of a comment in the body of the message it usually indicates that this comment is not intended to be taken seri ously most emoticons are designed to be interpreted with the viewer head tilted over to the left a result of the way the symbols are represented on the screen some of the best known ones are presented in table a recently created short hand language used primarily by teenagers when online chatting or texting is the use of abbreviated words these are formed by keying in various numbers and the mac version of microsoft office clippy was replaced by an anthropomorphized mac computer with big feet and a hand that conveys variousgestures and moods user frustration table some commonly used emoticons emotion expression emoticon possible meanings smile or i happiness or ii previous comment not to be taken seriously sad mouth down or disappointed unhappy cheeky wink or previous comment meant as tongue in cheek mad brows raised mad about something very angry angry face very angry cross embarrassed mouth open embarrassed shocked sick looking sick feeling ill schoolboyish look smiley wearing a dunce cap to convey that the sender is about to ask a stupid question ters in place of words i cu as well as being creative the short hand can convey emotional connotations expressive forms like emoticons sounds icons and interface agents have been primarily used to i convey emotional states elicit certain kinds of emo tional responses in users such as feeling at ease comfort and happiness however in many situations computer interfaces inadvertently elicit negativeemotional responses by far the most common is user frustration to which we now turn our attention user frustration everyone at some time or other gets frustrated when using a computer the effect ranges from feeling mildly amused to extremely angry there are myriads of rea sons why such emotional responses occur when an application doesn t work properly or crashes when a system doesn t do what the user wants it to do when a user expectations are not met when a system does not provide sufficient information to let the user know what to do when error messages pop up that are vague obtuse or condemning when the appearance of an interface is too noisy garish gimmicky or patronizing when a system requires users to carry out many steps to perform a task only to discover a mistake was made somewhere along the line and they need to start all over again chapter comment often user frustration is caused by bad design no design inadvertent design or ill thought out design it is rarely caused deliberately however its impact on users can be quite drastic and make them abandon the application or tool here we pre sent some examples of classic user frustration provokers that could be avoided or reduced by putting more thought into the design of the conceptual model gimmicks cause when a users expectations are not met and they are instead presented with a gimmicky display level of frustration mild this can happen when clicking on a link to a only to discover that it is still under construction it can be still more annoying when the displays a road sign icon of men at work see figure although the owner may think such signs amusing it serves to underscore the viewer frustration at having made the effort to go to the only to be told that it is incomplete or not even started in some cases clicking on links that don t work is also frustrating how to avoid or help reduce the frustration by far the best strategy is to avoid using gimmicks to cover up the real crime in this example it is better to put material live on the web only when it is com plete and working properly people very rarely return to sites when they see icons like the one in figure error messages cause when a system or application crashes and provides an unexpected error message level of frustration high error messages have a long history in computer interface design and are notorious for their incomprehensibility for example describes an early system that was developed that allowed only for one line of error messages whenever the figure men at work icon sign indicating under construction ac cording to there were over million containing the phrase under construction in january user frustration error message was too long the system truncated it to fit on the line which the users would spend ages trying to decipher the full message was available only by pressing the help key function key while this may have seemed like a natural design solution to the developers it was not at all obviousto the users a much better design solution would have been to use the one line of the screen to indicate how more information about the current error press the key for explanation the use of cryptic language and developer jargon in error messages is a major contributing factor in user frustration it is one thing to have to cope when some thing goes wrong but it is another to have to try to understand an obscure message that pops up by way of explanation one of my favorites which sometimes appears on the screen when i m trying to do something perfectly reasonable like paste some text into a document using a word processor is the application word wonder has unexpectedly quit due to a type error it is very clear from what the system has just done closed the application very rapidly that it has just crashed so such feedback is not very helpful letting the user know that the error is of a type 2 kind is also not very useful how is the aver age user meant to understand this is there a list of error types ready at hand to tell the user how to solve the problem for each error moreover such a reference in vites the user to worry about how many more error types there might be the tone of the message is also annoying the adjective unexpectedly seems condescend ing implying almost that it is the fault of the user rather than the computer why include such a word at all after all how else could the application have quit one could never imagine the opposite situation an error message pops up saying the application has expectedly quit due to poor coding in the operating system how to avoid or help reduce the frustration ideally error messages should be treated as messages instead of explicating what has happened they should state the cause of the problem and what the user needs to do to fix it shneiderman has developed a detailed set of guidelines on how to develop helpful messages that are easy to read and under stand box summarizes the main recommendations chapter understanding how interfaces affect users below are some common error messages expressed in harsh computer jargon that can be quite threatening and offensive rewrite them in more usable useful and friendly language that would help users to understand the cause of the problem and how to fix it for each message imagine a specificcontext where such a problem might occur syntax error invalid filename invalid data application zeta has unexpectedly quit due to a type 4 error drive error abort retry or fail comment how specific the given advice can be will depend on the kind of system it is here are tions for hypotheticalsystems syntax error there is a problem with the way you have typed the command check for typos invalid filename choose another file name that uses only characters or less and is lower case without any spaces invalid data there is a problem with the data you have entered try again checking that no decimal points are used application zeta has unexpectedly quit due to a type 4 error the application you were working on crashed because of an internal mem ory problem try rebooting and increasing the amount of allocated memory to the application drive error abort retry or fail there is a problem with reading your disk try inserting it again overburdening the user cause upgrading software so that users are required to carry out excessive house keeping tasks level of frustration medium to high another pervasive frustrating user experience is upgrading a piece of software it is now common for users to have to go through this housekeeping task on a regular basis especially if they run a number of applications more often than not it tends to be a real chore being very time consuming and requiring the user to do a whole range of things like resetting preferences sorting out extensions checking other configurations and learning new ways of doing things often problems can de velop that are not detected till some time later when a user tries an operation that worked fine before but mysteriously now fails a common problem is that settings get lost or do not copy over properly during the upgrade as the number of options for customizing an application or operating system increases for each new upgrade so too does the headache of having to reset all the relevant preferences wading through myriads of dialog boxes and menus and figuring out which to 4 user figure typical message in dialog box that appears when trying to run an applet on a that needs a plug in the user does not have click on can be a very arduous task to add to the frustration users may also dis cover that several of their well learned procedures for carrying out tasks have been substantially changed in the upgrade a pet frustration of mine over the years has been trying to run various that require me to install a new plug in achieving this is never straightforward i have spent huge amounts of time trying to install what i assume to be the correct plug in only to discover that it is not yet available or incompatible with the oper ating system or machine i am using what typically happens is visit a tempting new only to discover that my browser is not suitably equipped to view it when my browser fails to run the applet a helpful dialog box will pop up saying that a plug in of x type is re quired it also usually directs me to another from where the plug in can be downloaded see figure that offer such plug ins however are not organized around my specific needs but are designed more like hardware stores a bad conceptual model offering hundreds maybe even thousands of plug ins covering all manner of applications and systems getting the right kind of plug in from the vast array available requires knowing a number of things about your ma chine and the kind of network you are using in going through the various options figure directory of plug ins available on a plug in site directed to from netscape chapter understanding how interfaces affect users to narrow down which plug in is required it is easy to overlook something and end up with an inappropriate plug in even when the right plug in has been down loaded and placed in the appropriate system folder it may not work a number of other things usually need to be done like specifying mime type and suffix the whole process can end up taking huge amounts of time rather than the couple of minutes most users would assume how to avoid or help reduce the frustration users should not have to spend large amounts of time on housekeeping tasks upgrading should be an effortlessand largely automatic process designersneed to think carefully about the trade offs incurred when introducing upgrades especially the amount of relearning required plug ins that users have to search for down load and set up themselves should be phased out and replaced with more powerful browsers that automatically download the right plug ins and place them in the ap propriate desktop folder reliably or better still interpret the different file types themselves 4 appearance cause when the appearance of an interface is unpleasant level of frustration medium as mentioned earlier the appearance of an interface can affect its usability users get annoyed by that are overloaded with text and graphics making it difficult to find the information desired and slow to access flashing animations especially banner ads which are very distracting the copious use of sound effects and especially when selecting op tions carrying out actions starting up running tutorials or watching demos featuritis an excessive number of operations represented at the interface as banks of icons or cascading menus childish designs that keep popping up on the screen such as certain kinds of helper agents poorly laid out keyboards pads control panels and other input devices that cause the user to press the wrong keys or buttons when trying to do some thing else how to avoid or help reduce the frustration interfaces should be designed to be simple perceptually salient and elegant and to adhere to usability design principles well thought outgraphic design princi ples and ergonomic guidelines mullet and sano dealing with user frustration one way of coping with computer inducedfrustration is to vent and take it out on the computer or other users as mentioned in chapter a typical response to see ing the cursor freeze on the screen is repeatedly to bash every key on the keyboard a debate the application of anthropomorphism to interaction design another way of venting anger is through flaming when upset or annoyed by a piece of news or something in an message people may overreact and re spond by writing things in that they wouldn t dream of saying face to face they often use keyboard symbols to emphasize their anger or frustration ex clamation marks capital letters why did you do that and re peated question marks that can be quite offensive to those on the receivingend while such venting behavior can make the user feel temporarily less frustrated it can be very unproductive and can annoy the recipients anyone who has received a flame knows just how unpleasant it is in the previous section we provided some suggestions on how systems could be improved to help reduce commonly caused frustrations many of the ideas dis cussed throughout the book are also concerned with designing technologies and in terfaces that are usable useful and enjoyable there will always be situations however in which systems do not function in the way users expect them to or in which the user misunderstands something and makes a mistake in these circum stances error messages phrased as how to fix it advice should be provided that explain what the user needs to do another way of providing information is through online help such as tips handy hints and contextualized advice like error messages these need to be de signed to guide users on what to do next when they get stuck and it is not obvious from the interface what to do the signaling used at the interface to indicate that such online help is available needs careful consideration a cartoon based agent with a catchy tune may seem friendly and helpful the first time round but can quickly become annoying a help icon or command that is activated by the users themselves when they want help is often preferable a debate the application of anthropomorphism to interaction design in this section we present a debate read through the arguments for and against the motion and then the evidence provided afterwards decide for yourself whether you support the motion chapter understandinghow interfacesaffect users i the motion the use of anthropomorphism in interaction design is an effective technique and should be exploited further background a controversial debate in interaction design is whether to exploit the phenomenon of anthropomorphism the propensity people have to attribute human qualities to objects it is something that people do naturally in their everyday lives and is com monly exploited in the design of technologies the creation of humanlike ani mals and plants in cartoon films the design of toys that have human qualities the approach is also becoming more widespread in interaction design through the in troduction of agents in a range of domains what is anthropomorphism it is well known that people readily attribute human qualities to their pets and their cars and conversely are willing to accept human attributes that have been assigned by others to cartoon characters robots toys and other inanimate objects advertisers are well aware of this phenomenon and often create humanlike characters out of inanimate objects to promote their products for example breakfast cereals butter and fruit drinks have all been transmogrified characters with human qualities they move talk have person alities and show emotions enticing the viewer to buy them children are espe cially susceptible to this kind of magic as witnessed in their love of cartoons where all manner of inanimate objects are brought to life with humanlike qualities of its application to system design the finding that people especially children have a propensity to accepting and en joying objects that have been given humanlike qualities has led many designers into capitalizing on it most prevalently in the design of human computer dialogs modeled on how humans talk to each other a range of animated screen charac ters such as agents friends advisors and virtual pets have also been developed anthropomorphism has also been used in the development of cuddly toys that are embedded with computer systems commercial products have been designed to try to encourage children to learn through playing with the cuddly toys for example barney attempts to motivate play in children by using human based speech and movement strommen the toys are programmed to react to the child and make comments while watching together or working together on a computer based task see figure 2 in color plate in particular barney is programmed to congratulate the child whenever he or she gets a right an swer and also to react to the content on screen with appropriate emotions cheering at good news and expressingconcern at bad news arguments for exploiting this behavior an underlying argument in favor of the anthropomorphic approach is that furnish ing interactive systems with personalities and other humanlike attributes makes them more enjoyable and fun to interact with it is also assumed that they can a debate the application of anthropomorphism to interaction vate people to carry out the tasks suggested learning material purchasing goods more strongly than if they are presented in cold abstract computer lan guage being addressed in first person hello chris nice to see you again welcome back now what were we doing last time oh yes exercise let start again is much more endearing than being addressed in the impersonal third per son user commence exercise especially for children it can make them feel more at ease and reduce their anxiety similarly interacting with screen char acters like tutors and wizards can be much pleasanter than interacting with a cold dialog box or blinking cursor on a blank screen typing a question in plain english using a search engine like ask which impersonates the well known ficti tious butler is more natural and personable than thinking up a set of keywords as required by other search engines at the very least anthropomorphic interfaces are a harmless bit of fun arguments against exploiting this behavior there have been many criticisms of the anthropomorphic approach shneiderman one of the best known critics has written at length about the problems of attributing human qualities to computer systems his central argument is that an thropomorphic interfaces especially those that use first person dialog and screen characters are downright deceptive an unpleasant side effect is that they can make people feel anxious resulting in them feeling inferior or stupid a screen tutor that wags its finger at the user and says now chris that not right try again you can do better is likely to feel more humiliating than a system dialog box saying try again anthropomorphism can also lead people into a false sense of belief enticing them to confide in agents called software bots that reside in chatrooms and other electronic spaces pretending to be conversant human beings by far the most com mon complaint against computers pretending to have human qualities however is that people find them very annoying and frustrating once users discover that the system cannot really converse like a human or does not possess real human quali ties like having a personality or being sincere they become quickly disillusioned and subsequently distrust it e commerce sites that pretend to be caring by present ing an assortment of virtual assistants receptionists and other such helpers are seen for what they really are artificial and flaky children and adults alike also are quickly bored and annoyed with applications that are fronted by artificial screen characters tutor wizards and simply ignore whatever they might suggest evidence for the motion a number of studies have investigated people reactions and responses to comput ers that have been designed to be more humanlike a body of work reported by reeves and nass has identified several benefits of the anthropomorphic ap proach they found that computers that were designed to flatter and praise users when they did something right had a positive impact on how they felt about them selves for example an educational program was designed to say your question makes an interesting and useful distinction great job after a user had contributed chapter understanding how interfaces affect users a new question to it students enjoyed the experience and were more willing to con tinue working with the computer than were other students who were not praised by the computer for doing the same things in another study walker et al com pared people responses to a talking face display and an equivalent text only one and found that people spent more time with the talking face display than the only one when given a questionnaire to fill in the face display group made fewer mistakes and wrote down more comments in a follow up study sproull et al again found that users reacted quite differently to the two interfaces with users presenting themselves in a more positive light to the talking face display and generally interacting with it more evidence against the motion sproull et studies also revealed however that the talking face display made some users feel somewhat disconcerted and displeased the choice of a stern talk ing face may have been a large contributing factor perhaps a different kind of re sponse have been elicited if a friendlier smiling face had been used nevertheless a number of other studies have shown that increasing the human ness of an interface is counterproductive people can be misled into believing that a computer is like a human with human levels of intelligence for example one study investigating user responses to interacting with agents at the interface rep resented as human guides found that the users expected the agents to be more hu manlike than they actually were in particular they expected the agents to have personality emotion and motivation even though the guides were portrayed on the screen as simple black and white static icons see figure furthermore the users became disappointed when they discovered the agents did not have any of these characteristics et al in another study comparing an anthropo morphic interface that spoke in the first person and was highly personable hi there john it s nice to meet you i see you are ready now with a mechanistic one that spoke in third person press the enter key to figure 8 guides of histori cal characters virtual characters agents begin session the former was rated by college students as less honest and it made them feel less responsible for their actions quintanar et al casting your vote on the basis of this debate and any other articles on the topic see section 6 and the recommended readings at the end of this chapter together with your experiences with anthropomorphic interfaces make up your mind whether you are for or against the motion 6 virtual characters agents as mentioned in the debate above a whole new genre of cartoon and life like char acters has begun appearing on our computer screens as agents to help us search the web as e commerce assistants that give us information about products as char acters in video games as learning companions or instructors in educational pro grams and many more the best known are videogame stars like lara croft and super mario other kinds include virtual pop stars see figure 9 on color plate 6 virtual talk show hosts virtual bartenders virtual shop assistants and virtual newscasters interactive pets aibo and other artificial anthropomorphized characters pokemon creatures that are intended to be cared for and played with by their owners have also proved highly popular 6 kinds of agents below we categorize the different kinds of agents in terms of the degree to which they anthropomorphize and the kind of human or animal qualities they emulate these are 1 synthetic characters 2 animated agents emotional agents and 4 embodied conversational interface agents 1 synthetic characters these are commonly designed as characters in video games or other forms of entertainment and can appear as a first person avatar or a third person agent much effort goes into designing them to be lifelike exhibiting realistic human movements like walking and running and having distinct personalities and traits the design of the characters appearance their facial expressions and how their lips move when talking are also considered important interface design concerns bruce blumberg and his group at mit are developing autonomous animated creatures that live in virtual environments the creatures are autonomous in that they decide what to do based on what they can sense of the world and how they feel based on their internal states one of the earliest creatures to be de veloped was silas t dog blumberg the dog looks like a cartoon crea ture colored bright yellow but is designed to behave like a real dog see figure 10 for example he can walk run sit wag his tail bark cock his leg chase sticks and rub his head on people when he is happy he navigates through his world by using his nose and synthetic vision he also has been programmed with various internal goals and needs that he tries to satisfy including wanting to play chapter understanding how interfaces affect users and have company he responds to events in the environment for example he be comes aggressive if a hamster enters his patch a person can interact with silas by making various gesturesthat are detected by a computer vision system for example the person can pretend to throw a stick which is recognized as an action that silas responds to an image of the person is also pro jected onto a large screen so that he can be seen in relation to silas see figure 10 depending on his mood will run after the stick and return it when he is happy and playful or cower and refuse to fetch it when he is hungry or sad 2 animated agents these are similar to synthetic characters except they tend to be designed to play a collaborating role at the interface typically they appear at the side of the screen as tutors wizards and helpers intended to help users perform a task this might be designing a presentation writing an essay or learning about a topic most of the characters are designed to be cartoon like rather than resemble human beings an example of an animated agent is herman the bug who was developed by in tellimedia at north carolina state university to teach children from kindergarten to high school about biology lester et al herman is a talkative quirky insect that flies around the screen and dives into plant structures as it provides solving advice to students see figure 11 on color plate when providing its ex planations it performs a range of activities including walking flying shrinking expanding swimming jumping acrobatics and teleporting its behavior in cludes animated segments canned audio clips and a number of songs herman offers advice on how to perform tasks and also tries to motivatestudents to do them 3 emotional agents these are designed with a predefined personality and set of emotions that are ma nipulated by users the aim is to allow people to change the moods or emotions of agents and see what effect it has on their behavior various mood changers are 6 virtual characters agents vided at the interface in the form of sliders and icons the effect of requesting an animated agent to become very happy sad or grumpy is seen through changes to their behavior for example if a user moves a slider to a scared position on an emotional scale the agent starts behaving scared hiding behind objects and mak ing frightened facial expressions the woggles are one of the earliest forms of emotional agents bates a group of agents was designed to appear on the screen that played games with one another such as hide and seek they were designed as different colored bouncy balls with cute facial expressions users could change their moods from happy to sad by moving various sliders which in turn changed their movement they bounced less facial expression they no longer smiled and how willing they were to play with the other woggles see figure 12 on color plate 4 embodied conversational interface agents much of the research on embodied conversational interface agents has been con cerned with how to emulate human conversation this has included modeling vari ous conversational mechanismssuch as recognizing and responding to verbal and non verbal input generating verbal and non verbal output coping with breakdowns turn taking and other conversational mechanisms giving signals that indicate the state of the conversation as well as contribut ing new suggestionsfor the dialog cassell p in many ways this approach is the most anthropomorphic in its aims of all the agent research and development rea is an embodied real estate agent with a humanlike body that she uses in humanlike ways during a conversation cassell in particular she uses eye gaze body posture hand gestures and facial expressions while talking see figure 13 on color plate 8 although the dialog appears relatively simple it involves a sophisticated underlying set of conversational mechanisms and gesture recognition techniques an example of an actual interaction with rea is mike approaches the screen and rea turns to face him and says hello how can i help you mike i m looking to buy a place near mit rea nods indicating she is following rea i have a house to show you picture of a house appears on the screen it is in somerville mike tell me about it rea looks up and away while she plans what to say rea it big rea makes an expansive gesture with her hands chapter understanding how interfaces affect users mike brings his hands up as if to speak so rea does not continue waiting for him to speak mike tell me more about it rea sure thing it has a nice garden which of the various kinds of agents described above do you think are the most convincing is it those that try to be as humanlike as possible or those that are designed to be simple car toon based animated characters comment we argue that the agents that are the most successful are ironically those that are least like humans the reasons for this include that they appear less phony and are not trying to pretend they are more intelligent or human than they really are however others would argue that the more humanlike they are the more believable they are and hence the more convincing 6 2 general design concerns believability of virtual characters one of the major concerns when designing agents and virtual characters is how to make them believable by believability is meant the extent to which users inter acting with an agent come to believe that it has its own beliefs desires and person ality lester and stone p in other words a virtual character that a person can believe in is taken as one that allows users to suspend their disbelief a key aspect is to match the personality and mood of the character to its actions this requires deciding what are appropriate behaviors jumping smiling sitting raising arms for different kinds of emotions and moods how should the emotion very happy be expressed through a character jumping up and down with a big grin on its face what about moderately happy through a character jumping up and down with a small grin on its face how easy is it for the user to distinguish be tween these two and other emotions that are expressed by the agents how many emotions are optimal for an agent to express appearance the appearance of an agent is very important in making it believable parsimony and simplicity are key research findings suggest that people tend to prefer simple car toon based screen characters to detailed images that try to resemble the human form as much as possible scaife and rogers other research has also found that simple cartoon like figures are preferable to real people pretending to be artificial agents a project carried out by researchers at apple computer inc in the found that people reacted quite differently to different representations of the same inter face agent the agent in question called phil was created as part of a promotional 6 virtual characters agents figure 14 two versions of phil the agent assistant that appeared in apple promo tional video called the knowledge navigator a as a real actor pretending to be a computer agent and b as a cartoon being an agent phil was created by doris mitsch and the actor phil was scott freeman video called the knowledge navigator he was designed to respond and behave just like a well trained human assistant in one version he was played by a real actor that appeared on a university professor scomputer screen thus he was portrayed as an artificial agent but was played by a real human the actor was a smartly dressed assistant wearing a white shirt and bow tie he was also extremely polite he per formed a number of simple tasks at the computer interface such as reminding the professor of his appointments for that day and alerting him to phone calls waiting many people found this version of phil unrealistic after viewing the promotional video people complained about him saying that he seemed too stupid another version phil was designed as a simple line drawncartoon with limited animation see figure 14 and was found to be much more likeable see laurel behavior another important consideration in making virtual characters believable is how convincing their behavior is when performing actions in particular how good are they at pointing out relevant objects on the screen to the user so that the user knows what they are referring to one way of achieving this is for the virtual char acter to lead with its eyes for example the dog turns to look at an object or a person before he actually walks over to it to pick the object up or to invite the person to play a character that does not lead with its eyes looks very mechan ical and as such not very life like maes as mentioned previously an agent actions need also to match their underly ing emotional state if the agent is meant to be angry then its body posture move ments and facial expression all need to be integrated to show this how this can be achieved effectively can be learned from animators who have a long tradition in this field for example one of their techniques is to greatly exaggerate expressions and movements as a way of conveying and drawing attention to an emotional state of a character mode of interaction the way the character communicates with the user is also important one approach has been towards emulating human conversations as much as possible to make the character way of talking more convincing however as mentioned in the debate above a drawback of this kind of masqueradingis that people can get annoyed eas ily and feel cheated paradoxically a more believable and acceptable dialog with a virtual character may prove to be one that is based on a simple mode of in teraction in which prerecorded speech is played at certain choice points in the in teraction and the user responses are limited to selecting menu options the reason why this mode of interaction may ultimately prove more effective is because the user is in a better position to understand what the agent is capable of doing there is no pretence of a stupid agent pretending to be a smart human assignment this assignment requires you to write a critique of the persuasive impact of virtual sales agents on customers consider what it would take for a virtual sales agent to be believable trustwor thy and convincing so that customers would be reassured and happy to buy something based on its recommendations a look at some e commerce sites that use virtual sales agents use a search engine to find sites or start with miss boo at boo com which was working at time of printing and answer the following what do the virtual agents do what type of agent are they do they elicit an emotional response from you if so what is it what kind of personality do they have how is this expressed what kinds of behavior do they exhibit what are their facial expressions like what is their appearance like is it realistic or cartoon like where do they appear on the screen how do they communicate with the user text or speech is the level of discourse patronizing or at the right level are the agents helpful in guiding the customer towards making a purchase are they too pushy what gender are they do you think this makes a difference would you trust the agents to the extent that you would be happy to buy a prod uct from them if not why not what else would it take to make the agents persuasive further reading b next look at an e commerce that does not include virtual sales agents but is based on a conceptual model of browsing how does it com pare with the agent based sites you have just looked at is it easy to find information about products what kind of mechanism does the site use to make recommendations and guide the user in making a purchase is any kind of personalization used at the interface to make the user feel welcome or special would the site be improved by having an agent explain your reasons either way c finally discuss which site you would trust most and give your reasons for this summary this chapter has described the different ways interactive products can be designed both de liberately and inadvertently to make people respond in certain ways the extent to which users will learn buy a product online chat with others and so on depends on how comfort able they feel when using a product and how well they can trust it if the interactive product is frustrating to use annoying or patronizing users easily get angry and despondent and often stop using it if on the other hand the system is a pleasure enjoyable to use and makes the users feel comfortable and at ease then they are likely to continue to use it make a purchase return to the continue to learn etc this chapter has described various interface mechanisms that can be used to elicit positive emotional responses in users and ways of avoiding negative ones key points affective aspects of interaction design are concerned with the way interactive systems make people respond in emotional ways well designed interfaces can elicit good feelings in people aesthetically pleasing interfaces can be a pleasure to use expressive interfaces can provide reassuring feedback to users as well as be informative and fun badly designed interfaces often make people frustrated and angry anthropomorphism is the attribution of human qualities to objects an increasingly popular form of anthropomorphism is to create agents and other characters as part of an interface people are more accepting of believable interface agents people often prefer simple cartoon like agents to those that attempt to be humanlike the fallacy of supply and demand why the price of pearls and everything else is up in the air t the onset of world war ii an italian diamond dealer james assael fled europe for cuba there he found a new livelihood the american army needed waterproof watches and assael through his contacts in switzerland was able to fill the demand when the war ended assael deal with the u s govern ment dried up and he was left with thousands of swiss watches the japanese needed watches of course but they didn t have any money they did have pearls though many thousands of them before long assael had taught his son how to barter swiss watches for japanese pearls the busi ness blossomed and shortly thereafter the son salvador as sael became known as the pearl king the pearl king had moored his yacht at saint tropez one day in when a dashing young frenchman jean claude predictably irrational brouillet came aboard from an adjacent yacht brouillet had just sold his air freight business and with the proceeds had purchased an atoll in french polynesia a blue lagooned paradise for himself and his young tahitian wife brouillet explained that its turquoise waters abounded with black lipped oysters pinctada margaritifera and from the black lips of those oysters came something of note black pearls at the time there was no market for tahitian black pearls and little demand but brouillet persuaded assael to go into business with him together they would harvest black pearls and sell them to the world at first assael marketing efforts failed the pearls were gunmetal gray about the size of mus ket balls and he returned to polynesia without having made a single sale assael could have dropped the black pearls alto gether or sold them at a low price to a discount store he could have tried to push them to consumers by bundling them together with a few white pearls but instead assael waited a year until the operation had produced some better speci mens and then brought them to an old friend harry win ston the legendary gemstone dealer winston agreed to put them in the window of his store on fifth avenue with an out rageously high price tag attached assael meanwhile com missioned a full page advertisement that ran in the glossiest of magazines there a string of tahitian black pearls glowed set among a spray of diamonds rubies and emeralds the pearls which had shortly before been the private business of a cluster of black lipped oysters hanging on a rope in the polynesian sea were soon parading through man hattan on the arched necks of the city most prosperous di vas assael had taken something of dubious worth and made it fabulously fine or as mark twain once noted about tom the fallacy of supply and demand sawyer tom had discovered a great law of human action namely that in order to make a man covet a thing it is only necessary to make the thing difficult to attain how did the pearl king do it how did he persuade the cream of society to become passionate about tahitian black pearls and pay him royally for them in order to answer this question i need to explain something about baby geese a few decades ago the naturalist konrad lorenz discov ered that goslings upon breaking out of their eggs become attached to the first moving object they encounter which is generally their mother lorenz knew this because in one ex periment he became the first thing they saw and they fol lowed him loyally from then on through adolescence with that lorenz demonstrated not only that goslings make ini tial decisions based on what available in their environment but that they stick with a decision once it has been made lorenz called this natural phenomenon imprinting is the human brain then wired like that of a gosling do our first impressions and decisions become imprinted and if so how does this imprinting play out in our lives when we encounter a new product for instance do we accept the first price that comes before our eyes and more importantly does that price which in academic lingo we call an anchor have a long term effect on our willingness to pay for the product from then on it seems that what good for the goose is good for hu mans as well and this includes anchoring from the begin ning for instance assael anchored his pearls to the finest gems in the world and the prices followed forever after similarly once we buy a new product at a particular price predictably irrational we become anchored to that price but how exactly does this work why do we accept anchors consider this if i asked you for the last two digits of your social security number mine are then asked you whether you would pay this number in dollars for me this would be for a particular bottle of cotes du rhone would the mere suggestion of that number influence how much you would be willing to spend on wine sounds preposterous doesn t it well wait until you see what happened to a group of mba students at mit a few years ago now here we have a nice cotes du rhone jaboulet paral lel said drazen prelec a professor at mit sloan school of management as he lifted a bottle admiringly it a at the time sitting before him were the students from his marketing research class on this day drazen george loewenstein a professor at carnegie mellon university and i would have an unusual request for this group of future mar keting pros we would ask them to jot down the last two dig its of their social security numbers and tell us whether they would pay this amount for a number of products including the bottle of wine then we would ask them to actually bid on these items in an auction what were we trying to prove the existence of what we called arbitrary coherence the basic idea of arbitrary coher ence is this although initial prices such as the price of as sael pearls are arbitrary once those prices are established in our minds they will shape not only present prices but also future prices this makes them coherent so would think ing about one social security number be enough to create the fallacy of supply and demand an anchor and would that initial anchor have a long term influence that what we wanted to see for those of you who don t know much about wines drazen continued this bottle received eighty six points from wine spectator it has the flavor of red berry mocha and black chocolate it a medium bodied medium intensity nicely balanced red and it makes for delightful drinking drazen held up another bottle this was a hermitage jaboulet la chapelle with a point rating from the wine advocate magazine the finest la chapelle since drazen intoned while the students looked up curi ously only cases made in turn drazen held up four other items a cordless track ball trackman marble fx by logitech a cordless keyboard and mouse itouch by logitech a design book the perfect package how to add value through graphic design and a one pound box of belgian chocolates by neuhaus drazen passed out forms that listed all the items now i want you to write the last two digits of your social security number at the top of the page he instructed and then write them again next to each of the items in the form of a price in other words if the last two digits are twenty three write twenty three dollars now when you re finished with that he added i want you to indicate on your sheets with a simple yes or no whether you would pay that amount for each of the products when the students had finished answering yes or no to each item drazen asked them to write down the maximum amount they were willing to pay for each of the products their bids once they had written down their bids the stu dents passed the sheets up to me and i entered their responses into my laptop and announced the winners one by one the predictably irrational student who had made the highest bid for each of the products would step up to the front of the class pay for the product and take it with them the students enjoyed this class exercise but when i asked them if they felt that writing down the last two digits of their social security numbers had influenced their final bids they quickly dismissed my suggestion no way when i got back to my office i analyzed the data did the digits from the social security numbers serve as anchors re markably they did the students with the highest ending social security digits from to bid highest while those with the lowest ending numbers to bid lowest the top per cent for instance bid an average of for the cordless key board the bottom percent bid an average of in the end we could see that students with social security numbers ending in the upper percent placed bids that were to percent higher than those of the students with social security numbers ending in the lowest percent see table on the facing page now if the last two digits of your social security number are a high number i know what you must be thinking i ve been paying too much for everything my entire life this is not the case however social security numbers were the anchor in this experiment only because we requested them we could have just as well asked for the current temperature or the manufacturer suggested retail price msrp any question in fact would have created the anchor does that seem rational of course not but that the way we are goslings after all t the price the highest bidder paid for an item was based not on his own bid but on that of the second highest bidder this is called a second price auction william vickrey received the nobel prize in economics for demonstrating that this type of auction creates the conditions where it is in people best interest to bid the maximum amount they are willing to pay for each item this is also the general logic behind the auction system on ebay twhen i ve tried this kind of experiment on executives and managers at the mit execu the fallacy of supply and demand average prices paid for the various products for each of the five groups of final digits in social security numbers and the correlations between these digits and the bids submitted in the auction range of last two digits of ss number products 79 correlations cordless trackball cordless keyboard design book 82 18 82 00 neuhaus chocolates 64 64 42 cotes du rhone 64 hermitage 45 18 55 correlation is a statistical measure of how much the movement of two variables is related the range of possible correlations is between and where a correlation of o means that the change in value of one variable has no bearing on the change in value of the other variable the data had one more interesting aspect although the willingness to pay for these items was arbitrary there was also a logical coherent aspect to it when we looked at the bids for the two pairs of related items the two wines and the two com puter components their relative prices seemed incredibly logi cal everyone was willing to pay more for the keyboard than for the trackball and also pay more for the hermitage than for the cotes du rhone the significance of this is that once the participants were willing to pay a certain price for one product their willingness to pay for other items in the same product category was judged relative to that first price the anchor tive education program i ve had similar success making their social security numbers influence the prices they were willing to pay for chocolates books and other products predictably irrational this then is what we call arbitrary coherence initial prices are largely arbitrary and can be influenced by re sponses to random questions but once those prices are estab lished in our minds they shape not only what we are willing to pay for an item but also how much we are willing to pay for related products this makes them coherent now i need to add one important clarification to the story i ve just told in life we are bombarded by prices we see the manufacturer suggested retail price msrp for cars lawn mowers and coffeemakers we get the real estate agent spiel on local housing prices but price tags by themselves are not necessarily anchors they become anchors when we con template buying a product or service at that particular price that when the imprint is set from then on we are willing to accept a range of prices but as with the pull of a bungee cord we always refer back to the original anchor thus the first anchor influences not only the immediate buying deci sion but many others that follow we might see a inch lcd high definition television on sale for for instance the price tag is not the anchor but if we decide to buy it or seriously contemplate buying it at that price then the decision becomes our anchor henceforth in terms of lcd television sets that our peg in the ground and from then on whether we shop for another set or merely have a conversation at a backyard cookout all other high definition televisions are judged relative to that price anchoring influences all kinds of purchases uri simon sohn a professor at the university of pennsylvania and george loewenstein for example found that people who move to a new city generally remain anchored to the prices they paid for housing in their former city in their study they found that people who move from inexpensive markets say lubbock the fallacy of supply and demand texas to moderately priced cities say pittsburgh don t in crease their spending to fit the new market rather these people spend an amount similar to what they were used to in the previous market even if this means having to squeeze themselves and their families into smaller or less comfortable homes likewise transplants from more expensive cities sink the same dollars into their new housing situation as they did in the past people who move from los angeles to pittsburgh in other words don t generally downsize their spending much once they hit pennsylvania they spend an amount similar to what they used to spend in los angeles it seems that we get used to the particularities of our housing markets and don t readily change the only way out of this box in fact is to rent a home in the new location for a year or so that way we adjust to the new environment and after a while we are able to make a purchase that aligns with the local market so we anchor ourselves to initial prices but do we hop from one anchor price to another flip flopping if you will continually changing our willingness to pay or does the first anchor we encounter become our anchor for a long time and for many decisions to answer this question we decided to conduct another experiment one in which we attempted to lure our participants from old anchors to new ones for this experiment we enlisted some undergraduate stu dents some graduate students and some investment bankers who had come to the campus to recruit new employees for their firms once the experiment started we presented our the result was not due to wealth taxes or other financial reasons predictably irrational participants with three different sounds and following each asked them if they would be willing to get paid a particular amount of money which served as the price anchor for hear ing those sounds again one sound was a second high pitched hertz sound somewhat like someone screaming in a high pitched voice another was a second full spectrum noise also called white noise which is similar to the noise a television set makes when there is no reception the third was a second oscillation between high pitched and low pitched sounds i am not sure if the bankers under stood exactly what they were about to experience but maybe even our annoying sounds were less annoying than talking about investment banking we used sounds because there is no existing market for an noying sounds so the participants couldn t use a market price as a way to think about the value of these sounds we also used annoying sounds specifically because no one likes such sounds if we had used classical music some would have liked it better than others as for the sounds themselves i selected them after creating hundreds of sounds choosing these three because they were in my opinion equally annoying we placed our participants in front of computer screens at the lab and had them clamp headphones over their ears as the room quieted down the first group saw this mes sage appear in front of them in a few moments we are go ing to play a new unpleasant tone over your headset we are interested in how annoying you find it immediately after you hear the tone we will ask you whether hypothetically you would be willing to repeat the same experience in exchange for a payment of cents the second group got the same message only with an offer of cents rather than cents would the anchor prices make a difference to find out we turned on the sound in this case the irritating second hertz squeal some of our participants grimaced oth ers rolled their eyes when the screeching ended each participant was pre sented with the anchoring question phrased as a hypotheti cal choice would the participant be willing hypothetically to repeat the experience for a cash payment which was cents for the first group and cents for the second group after answering this anchoring question the participants were asked to indicate on the computer screen the lowest price they would demand to listen to the sound again this decision was real by the way as it would determine whether they would hear the sound again and get paid for doing so soon after the participants entered their prices they learned the outcome participants whose price was suffi ciently low won the sound had the unpleasant opportu nity to hear it again and got paid for doing so the participants whose price was too high did not listen to the sound and were not paid for this part of the experiment what was the point of all this we wanted to find out whether the first prices that we suggested cents and cents had served as an anchor and indeed they had those who first faced the hypothetical decision about whether to listen to the sound for cents needed much less money to be willing to listen to this sound again 33 cents on average relative to those who first faced the hypothetical decision about whether to listen to the sound for cents this sec ond group demanded more than twice the compensation to ensure that the bids we got were indeed the lowest prices for which the participants would listen to the annoying sounds we used the becker degroot marschak procedure this is an auction like procedure in which each of the participants bids against a price randomly drawn by a computer cents on average for the same annoying experience do you see the difference that the suggested price had but this was only the start of our exploration we also wanted to know how influential the anchor would be in fu ture decisions suppose we gave the participants an opportu nity to drop this anchor and run for another would they do it to put it in terms of goslings would they swim across the pond after their original imprint and then midway swing their allegiance to a new mother goose in terms of goslings i think you know that they would stick with the original mom but what about humans the next two phases of the experiment would enable us to answer these questions in the second phase of the experiment we took partici pants from the previous cents and cents groups and treated them to seconds of a white wooshing noise hy pothetically would you listen to this sound again for cents we asked them at the end the respondents pressed a button on their computers to indicate yes or no ok how much would you need to be paid for this we asked our participants typed in their lowest price the com puter did its thing and depending on their bids some partici pants listened to the sound again and got paid and some did not when we compared the prices the cents group offered much lower bids than the cents group this means that al though both groups had been equally exposed to the suggested cents as their focal anchoring response to hypotheti cally would you listen to this sound again for cents the first anchor in this annoying sound category which was cents for some and cents for others predominated why perhaps the participants in the cents group said something like the following to themselves well i listened previously to that annoying sound for a low amount this sound is not much different so if i said a low amount for the previous one i guess i could bear this sound for about the same price those who were in the cents group used the same type of logic but because their starting point was dif ferent so was their ending point these individuals told themselves well i listened previously to that annoying sound for a high amount this sound is not much different so since i said a high amount for the previous one i guess i could bear this sound for about the same price indeed the effect of the first anchor held indicating that anchors have an enduring effect for present prices as well as for future pnces there was one more step to this experiment this time we had our participants listen to the oscillating sound that rose and fell in pitch for seconds we asked our cents group hypothetically would you listen to this sound again for cents then we asked our cents group would you lis ten to this sound again for cents having flipped our anchors we would now see which one the local anchor or the first anchor exerted the greatest influence once again the participants typed in yes or no then we asked them for real bids how much would it take for you to listen to this again at this point they had a history with three anchors the first one they encountered in the experi ment either cents or cents the second one cents and the most recent one either cents or cents which one of these would have the largest influence on the price they demanded to listen to the sound again it was as if our participants minds told them if i listened to the first sound for x cents and listened to the second sound for x cents as well then i can surely do this one for x cents too and that what they did those who had first encountered the cent anchor accepted low prices even after cents was suggested as the anchor on the other hand those who had first encountered the cent anchor kept on demanding much higher prices regardless of the an chors that followed what did we show that our first decisions resonate over a long sequence of decisions first impressions are important whether they involve remembering that our first dvd player cost much more than such players cost today and realizing that in comparison the current prices are a steal or remem bering that gas was once a dollar a gallon which makes ev ery trip to the gas station a painful experience in all these cases the random and not so random anchors that we en countered along the way and were swayed by remain with us long after the initial decision itself now that we know we behave like goslings it is important to understand the process by which our first decisions trans late into long term habits to illustrate this process consider this example you re walking past a restaurant and you see two people standing in line waiting to get in this must be a good restaurant you think to yourself people are stand ing in line so you stand behind these people another per son walks by he sees three people standing in line and thinks this must be a fantastic restaurant and joins the line others join we call this type of behavior herding it happens when we assume that something is good or bad on the basis of other people previous behavior and our own actions follow suit but there also another kind of herding one that we call self herding this happens when we believe something is good or bad on the basis of our own previous behavior es sentially once we become the first person in line at the res taurant we begin to line up behind ourself in subsequent experiences does that make sense let me explain recall your first introduction to starbucks perhaps sev eral years ago i assume that nearly everyone has had this experience since starbucks sits on every corner in america you are sleepy and in desperate need of a liquid energy boost as you embark on an errand one afternoon you glance through the windows at starbucks and walk in the prices of the coffee are a shock you ve been blissfully drinking the brew at dunkin donuts for years but since you have walked in and are now curious about what coffee at this price might taste like you surprise yourself you buy a small coffee enjoy its taste and its effect on you and walk out the following week you walk by starbucks again should you go in the ideal decision making process should take into account the quality of the coffee starbucks versus dunkin donuts the prices at the two places and of course the cost or value of walking a few more blocks to get to dunkin donuts this is a complex computation so instead you resort to the simple approach i went to starbucks be fore and i enjoyed myself and the coffee so this must be a good decision for me so you walk in and get another small cup of coffee in doing so you just became the second person in line standing behind yourself a few days later you again walk by starbucks and this time you vividly remember your past decisions and act on them again voila you become the third person in line standing behind yourself as the weeks pass you enter again and again and every time you feel more strongly that you are acting on the basis of your preferences buying coffee at starbucks has become a habit with you bur the story doesn t end there now that you have gotten used to paying more for coffee and have bumped yourself up onto a new curve of consumption other changes also become simpler perhaps you will now move up from the small cup for to the medium size for or to the venti for even though you don t know how you got into this price bracket in the first place moving to a larger coffee at a rela tively greater price seems pretty logical so is a lateral move to other offerings at starbucks caffe americano caffe misto macchiato and frappuccino for instance if you stopped to think about this it would not be clear whether you should be spending all this money on coffee at starbucks instead of getting cheaper coffee at dunkin do nuts or even free coffee at the office but you don t think about these trade offs anymore you ve already made this decision many times in the past so you now assume that this is the way you want to spend your money you ve herded yourself lining up behind your initial experience at starbucks and now you re part of the crowd however there something odd in this story if anchor ing is based on our initial decisions how did starbucks man age to become an initial decision in the first place in other words if we were previously anchored to the prices at dunkin donuts how did we move our anchor to starbucks this is where it gets really interesting when howard shultz created starbucks he was as intuitive a businessman as salvador assael he worked diligently to separate starbucks from other coffee shops not through price but through ambience accordingly he designed starbucks from the very beginning to feel like a continental coffeehouse the early shops were fragrant with the smell of roasted beans and better quality roasted beans than those at dunkin donuts they sold fancy french coffee presses the show cases presented alluring snacks almond croissants biscotti raspberry custard pastries and others whereas dunkin do nuts had small medium and large coffees starbucks offered short tall grande and venti as well as drinks with high pedigree names like caffe americano caffe misto macchi ato and frappuccino starbucks did everything in its power in other words to make the experience feel different so dif ferent that we would not use the prices at dunkin donuts as an anchor but instead would be open to the new anchor that starbucks was preparing for us and that to a great extent is how starbucks succeeded george drazen and i were so excited with the experi ments on coherent arbitrariness that we decided to push the idea one step farther this time we had a different twist to explore do you remember the famous episode in the adventures of tom sawyer the one in which tom turned the whitewash ing of aunt polly fence into an exercise in manipulating his friends as i m sure you recall tom applied the paint with gusto pretending to enjoy the job do you call this work tom told his friends does a boy get a chance to whitewash a fence every day armed with this new information his friends discovered the joys of whitewashing a fence before long tom friends were not only paying him for the privi lege but deriving real pleasure from the task a win win outcome if there ever was one from our perspective tom transformed a negative expe rience to a positive one he transformed a situation in which compensation was required to one in which people tom friends would pay to get in on the fun could we do the same we thought we d give it a try one day to the surprise of my students i opened the day lecture on managerial psychology with a poetry selection a few lines of whoever you are holding me now in hand from walt whitman leaves of grass whoever you are holding me now in hand without one thing all will be useless i give you fair warning before you attempt me further i am not what you supposed but far different who is he that would become my follower who would sign himself a candidate for my affections the way is suspicious the result uncertain perhaps destructive you would have to give up all else i alone would expect to be your sole and exclusive standard your novitiate would even then be long and exhausting the whole past theory of your life and all conformity to the lives around you would have to be abandon d therefore release me now before troubling yourself any further let go your hand from my shoulders put me down and depart on your way after closing the book i told the students that i would be conducting three readings from walt whitman leaves of grass that friday evening one short one medium and one long owing to limited space i told them i had decided to hold an auction to determine who could attend i passed out sheets of paper so that they could bid for a space but before they did so i had a question to ask them i asked half the students to write down whether hypo thetically they would be willing to pay me for a minute poetry recitation i asked the other half to write down whether hypothetically they would be willing to listen to me recite poetry for ten minutes if i paid them this of course served as the anchor now i asked the students to bid for a spot at my poetry reading do you think the initial anchor influenced the ensuing bids before i tell you consider two things first my skills at reading poetry are not of the first order so asking someone to pay me for minutes of it could be considered a stretch second even though i asked half of the students if they would pay me for the privilege of attending the recitation they didn t have to bid that way they could have turned the tables completely and demanded that i pay them and now to the results drumroll please those who an swered the hypothetical question about paying me were indeed willing to pay me for the privilege they offered on average to pay me about a dollar for the short poetry reading about two dollars for the medium poetry reading and a bit more than three dollars for the long poetry reading maybe i could make a living outside academe after all but what about those who were anchored to the thought of being paid rather than paying me as you might expect they demanded payment on average they wanted to listen to the short poetry reading to listen to the me dium poetry reading and to endure the long poetry reading much like tom sawyer then i was able to take an ambig uous experience and if you could hear me recite poetry you would understand just how ambiguous this experience is and arbitrarily make it into a pleasurable or painful experience neither group of students knew whether my poetry reading was of the quality that is worth paying for or of the quality that is worth listening to only if one is being financially com pensated for the experience they did not know if it is pleasur able or painful but once the first impression had been formed that they would pay me or that i would pay them the die was cast and the anchor set moreover once the first decision had been made other decisions followed in what seemed to be a logical and coherent manner the students did not know whether listening to me recite poetry was a good or bad expe rience but whatever their first decision was they used it as input for their subsequent decisions and provided a coherent pattern of responses across the three poetry readings of course mark twain came to the same conclusions if tom had been a great and wise philosopher like the writer of this book he would now have comprehended that work con sists of whatever a body is obliged to do and that play con sists of whatever a body is not obliged to do mark twain further observed there are wealthy gentlemen in england who drive four horse passenger coaches twenty or thirty miles on a daily line in the summer because the privilege costs them considerable money but if they were offered wages for the service that would turn it into work and then they would resign where do these thoughts lead us for one they illustrate the many choices we make from the trivial to the profound in which anchoring plays a role we decide whether or not to purchase big macs smoke run red lights take vacations in patagonia listen to tchaikovsky slave away at doctoral dis sertations marry have children live in the suburbs vote republican and so on according to economic theory we base these decisions on our fundamental values our likes and dislikes but what are the main lessons from these experiments about our lives in general could it be that the lives we have so carefully crafted are largely just a product of arbitrary co herence could it be that we made arbitrary decisions at some point in the past like the goslings that adopted lorenz as their parent and have built our lives on them ever since assuming that the original decisions were wise is that how we chose our careers our spouses the clothes we wear and the way we style our hair were they smart decisions in the first place or were they partially random first imprints that have run wild descartes said cogito ergo sum i think therefore i am but suppose we are nothing more than the sum of our first naive random behaviors what then these questions may be tough nuts to crack but in terms of our personal lives we can actively improve on our irrational we will return to this astute observation in the chapter on social and market norms chapter behaviors we can start by becoming aware of our vulnera bilities suppose you re planning to buy a cutting edge cell phone the one with the three megapixel zoom digital camera or even a daily cup of gourmet coffee you might begin by questioning that habit how did it begin second ask yourself what amount of pleasure you will be getting out of it is the pleasure as much as you thought you would get could you cut back a little and better spend the remaining money on something else with everything you do in fact you should train yourself to question your repeated behav iors in the case of the cell phone could you take a step back from the cutting edge reduce your outlay and use some of the money for something else and as for the coffee rather than asking which blend of coffee you will have today ask yourself whether you should even be having that habitual cup of expensive coffee at we should also pay particular attention to the first deci sion we make in what is going to be a long stream of deci sions about clothing food etc when we face such a decision it might seem to us that this is just one decision without large consequences but in fact the power of the first decision can have such a long lasting effect that it will perco late into our future decisions for years to come given this effect the first decision is crucial and we should give it an appropriate amount of attention socrates said that the unexamined life is not worth living perhaps it time to inventory the imprints and anchors in our own life even if they once were completely reasonable are they still reasonable once the old choices are reconsidered i am not claiming that spending money on a wonderful cup of coffee every day or even a few times a day is necessarily a bad decision i am saying only that we should question our decisions we can open ourselves to new decisions and the new op portunities of a new day that seems to make sense all this talk about anchors and goslings has larger impli cations than consumer preferences however traditional economics assumes that prices of products in the market are determined by a balance between two forces production at each price supply and the desires of those with purchasing power at each price demand the price at which these two forces meet determines the prices in the marketplace this is an elegant idea but it depends centrally on the as sumption that the two forces are independent and that to gether they produce the market price the results of all the experiments presented in this chapter and the basic idea of arbitrary coherence itself challenge these assumptions first according to the standard economic framework consumers willingness to pay is one of the two inputs that determine market prices this is the demand but as our experiments demonstrate what consumers are willing to pay can easily be manipulated and this means that consumers don t in fact have a good handle on their own preferences and the prices they are willing to pay for different goods and experiences second whereas the standard economic framework as sumes that the forces of supply and demand are independent the type of anchoring manipulations we have shown here suggest that they are in fact dependent in the real world anchoring comes from manufacturer suggested retail prices msrps advertised prices promotions product introduc tions etc all of which are supply side variables it seems then that instead of consumers willingness to pay influenc ing market prices the causality is somewhat reversed and it is market prices themselves that influence consumers willing ness to pay what this means is that demand is not in fact a completely separate force from supply and this not the end of the story in the framework of ar bitrary coherence the relationships we see in the marketplace between demand and supply for example buying more yo gurt when it is discounted are based not on preferences but on memory here is an illustration of this idea consider your cur rent consumption of milk and wine now imagine that two new taxes will be introduced tomorrow one will cut the price of wine by percent and the other will increase the price of milk by percent what do you think will happen these price changes will surely affect consumption and many people will walk around slightly happier and with less calcium but now imagine this what if the new taxes are accompanied by induced amnesia for the previous prices of wine and milk what if the prices change in the same way but you do not re member what you paid for these two products in the past i suspect that the price changes would make a huge im pact on demand if people remembered the previous prices and noticed the price increases but i also suspect that with out a memory for past prices these price changes would have a trivial effect if any on demand if people had no memory of past prices the consumption of milk and wine would re main essentially the same as if the prices had not changed in other words the sensitivity we show to price changes might in fact be largely a result of our memory for the prices we have paid in the past and our desire for coherence with our past decisions not at all a reflection of our true prefer ences or our level of demand thesame basic principle would also apply if the govern ment one day decided to impose a tax that doubled the price of gasoline under conventional economic theory this should cut demand but would it certainly people would initially com pare the new prices with their anchor would be flabbergasted by the new prices and so might pull back on their gasoline consumption and maybe even get a hybrid car but over the long run and once consumers readjusted to the new price and the new anchors just as we adjust to the price of nike sneak ers bottled water and everything else our gasoline consump tion at the new price might in fact get close to the pretax level moreover much as in the example of starbucks this process of readjustment could be accelerated if the price change were to also be accompanied by other changes such as a new grade of gas or a new type of fuel such as corn based ethanol fuel i am not suggesting that doubling the price of gasoline would have no effect on consumers demand but i do believe that in the long term it would have a much smaller influence on demand than would be assumed from just observing the short term market reactions to price increases another implication of arbitrary coherence has to do with the claimed benefits of the free market and free trade the basic idea of the free market is that if i have something that you value more than i do let say a sofa trading this item will benefit both of us this means that the mutual ben efit of trading rests on the assumption that all the players in the market know the value of what they have and the value of the things they are considering getting from the trade but if our choices are often affected by random initial anchors as we observed in our experiments the choices and trades we make are not necessarily going to be an accurate re flection of the real pleasure or utility we derive from those prod ucts in other words in many cases we make decisions in the marketplace that may not reflect how much pleasure we can get from different items now if we can t accurately compute these pleasure values but frequently follow arbitrary anchors instead then it is not clear that the opportunity to trade is necessarily going to make us better off for example because of some un fortunate initial anchors we might mistakenly trade something that truly gives us a lot of pleasure but regrettably had a low initial anchor for something that gives us less pleasure but ow ing to some random circumstances had a high initial anchor if anchors and memories of these anchors but not preferences determine our behavior why would trading be hailed as the key to maximizing personal happiness utility so where does this leave us if we can t rely on the market forces of supply and demand to set optimal market prices and we can t count on free market mechanisms to help us maxi mize our utility then we may need to look elsewhere this is especially the case with society essentials such as health care medicine water electricity education and other critical re sources if you accept the premise that market forces and free markets will not always regulate the market for the best then you may find yourself among those who believe that the gov ernment we hope a reasonable and thoughtful government must play a larger role in regulating some market activities even if this limits free enterprise yes a free mark t based on supply demand and no friction would be the ideal if we were truly rational yet when we are not rational but irrational policies should take this important factor into account chapter the cost of zero cost why we often pay too much when we pay nothing ave you ever grabbed for a coupon offering a free package of coffee beans even though you don t drink coffee and don t even have a machine with which to brew it what about all those free extra helpings you piled on your plate at a buffet even though your stomach had already started to ache from all the food you had consumed and what about the worthless free stuff you ve accumulated the promotional t shirt from the radio station the teddy bear that came with the box of valentine chocolates the magnetic calendar your insurance agent sends you each year it no secret that getting something free feels very good zero is not just another price it turns out zero is an emo tional hot button a source of irrational excitement would you buy something if it were discounted from cents to cents maybe would you buy it if it were discounted from cents to two cents maybe would you grab it if it were dis counted from cents to zero you bet what is it about zero cost that we find so irresistible why does free make us so happy after all free can lead us into trouble things that we would never consider purchasing become incredibly appealing as soon as they are free for instance have you ever gathered up free pencils key chains and notepads at a conference even though you d have to carry them home and would only throw most of them away have you ever stood in line for a very long time too long just to get a free cone of ben and jerry ice cream or have you bought two of a product that you wouldn t have chosen in the first place just to get the third one for free zero has had a long history the babylonians invented the concept of zero the ancient greeks debated it in lofty terms how could something be nothing the ancient indian scholar pingala paired zero with the numeral to get double digits and both the mayans and the romans made zero part of their numeral systems but zero really found its place about ad when the indian astronomer aryabhata sat up in bed one morning and exclaimed sthanam sthanam dasa gunam which translates roughly as place to place in times in value with that the idea of decimal based place value nota tion was born now zero was on a roll it spread to the arab world where it flourished crossed the iberian peninsula to eu rope thanks to the spanish moors got some tweaking from the italians and eventually sailed the atlantic to the new world where zero ultimately found plenty of employment to gether with the digit in a place called silicon valley so much for a brief recounting of the history of zero but the concept of zero applied to money is less clearly understood in fact i don t think it even has a history nonetheless free has huge implications extending not only to discount prices and promotions but also to how free can be used to help us make decisions that would benefit ourselves and society if free were a virus or a subatomic particle i might use an electron microscope to probe the object under the lens stain it with different compounds to reveal its nature or somehow slice it apart to reveal its inner composition in be havioral economics we use a different instrument however one that allows us to slow down human behavior and exam ine it frame by frame as it unfolds as you have undoubtedly guessed by now this procedure is called an experiment in one experiment kristina shampanier a phd student at mit nina mazar a professor at the university of toronto and i went into the chocolate business well sort of we set up a table at a large public building and offered two kinds of chocolates lindt truffles and hershey kisses there was a large sign above our table that read one chocolate per cus tomer once the potential customers stepped closer they could see the two types of chocolate and their prices for those of you who are not chocolate connoisseurs lindt is produced by a swiss firm that has been blending fine cocoas for years lindt chocolate truffles are particularly prized exquisitely creamy and just about irresistible they cost about cents each when we buy them in bulk hershey kisses on the other hand are good little chocolates but let face it they are rather ordinary hershey cranks out million kisses a day in hershey pennsylvania even the streetlamps are made in the shape of the ubiquitous hershey kiss we posted the prices so that they were visible only when people got close to the table we did this because we wanted to make sure that we did not attract different types of people in the different conditions avoiding what is called self selection so what happened when the customers flocked to our table when we set the price of a lindt truffle at cents and a kiss at one cent we were not surprised to find that our cus tomers acted with a good deal of rationality they compared the price and quality of the kiss with the price and quality of the truffle and then made their choice about percent of them chose the truffle and percent chose a kiss now we decided to see how free might change the situation so we offered the lindt truffle for cents and the kisses free would there be a difference should there be after all we had merely lowered the price of both kinds of chocolate by one cent but what a difference free made the humble hershey kiss became a big favorite some percent of our customers up from percent before chose the free kiss giving up the opportunity to get the lindt truffle for a very good price meanwhile the lindt truffle took a tumble customers choos ing it decreased from to percent what was going on here first of all let me say that there are many times when getting free items can make perfect sense if you find a bin of free athletic socks at a department store for instance there no downside to grabbing all the socks you can the critical issue arises when free becomes a struggle be tween a free item and another item a struggle in which the presence of free leads us to make a bad decision for instance imagine going to a sports store to buy a pair of white socks the kind with a nicely padded heel and a gold toe fifteen minutes later you re leaving the store not with the socks you came in for but with a cheaper pair that you don t like at all without a pad ded heel and gold toe but that came in a package with a free second pair this is a case in which you gave up a better deal and settled for something that was not what you wanted just because you were lured by the free to replicate this experience in our chocolate experiment we told our customers that they could choose only a single sweet the kiss or the truffle it was an either or decision like choosing one kind of athletic sock over another that what made the customers reaction to the free kiss so dramatic both chocolates were discounted by the same amount of money the relative price difference between the two was unchanged and so was the expected pleasure from both according to standard economic theory simple cost benefit analysis then the price reduction should not lead to any change in the behavior of our customers before about percent chose the kiss and percent chose the truffle and since nothing had changed in relative terms the response to the price reduction should have been exactly the same a passing economist twirling his cane and espousing conven tional economic theory in fact would have said that since everything in the situation was the same our customers should have chosen the truffles by the same margin of preference and yet here we were with people pressing up to the table to grab our hershey kisses not because they had made a reasoned cost benefit analysis before elbowing their way in but simply because the kisses were free how strange but predictable we humans are this conclusion incidentally remained the same in other experiments as well in one case we priced the hershey kiss at two cents one cent and zero cents while pricing the truffle correspondingly at cents cents and cents for a more detailed account of how a rational consumer should make decisions in these cases see the appendix to this chapter we did this to see if discounting the kiss from two cents to one cent and the truffle from cents to cents would make a difference in the proportion of buyers for each it didn t but once again when we lowered the price of the kiss to free the reaction was dramatic the shoppers overwhelm ingly demanded the kisses we decided that perhaps the experiment had been tainted since shoppers may not feel like searching for change in a purse or backpack or they may not have any money on them such an effect would artificially make the free offer seem more attractive to address this possibility we ran other ex periments at one of mit cafeterias in this setup the choco lates were displayed next to the cashier as one of the cafeteria regular promotions and the students who were interested in the chocolates simply added them to the lunch purchase and paid for them while going through the cashier line what happened the students still went overwhelmingly for the free option what is it about free that so enticing why do we have an irrational urge to jump for a free item even when it not what we really want i believe the answer is this most transactions have an up side and a downside but when something is free we forget the downside free gives us such an emotional charge that we perceive what is being offered as immensely more valu able than it really is why i think it because humans are intrinsically afraid of loss the real allure of free is tied to this fear there no visible possibility of loss when we choose a free item it free but suppose we choose the item that not free uh oh now there a risk of having made a poor decision the possibility of a loss and so given the choice we go for what is free for this reason in the land of pricing zero is not just an other price sure cents can make a huge difference in de mand suppose you were selling millions of barrels of oil but nothing beats the emotional surge of free this the zero price effect is in a category all its own to be sure buying something for nothing is a bit of an oxymoron but let me give you an example of how we often fall into the trap of buying something we may not want sim ply because of that sticky substance free i recently saw a newspaper ad from a major electronics maker offering me seven free dvd titles if i purchased the maker new high definition dvd player first of all do i need a high definition player right now probably not but even if i did wouldn t it be wiser to wait for prices to de scend they always do and today high definition dvd player will very quickly be tomorrow machine second the dvd maker had a clear agenda behind its offer this company high definition dvd system is in cutthroat competition with blu ray a system backed by many other manufacturers right now blu ray is ahead and could pos sibly dominate the market so how much is free when the machine being offered may find its way into obsolescence like betamax vcrs those are two rational thoughts that might prevent us from falling under the spell of free but gee those free dvds certainly look good getting something free is certainly a draw when we talk about prices but what would happen if the offer was not a free price but a free exchange are we as susceptible to free products as we are to getting products for free a few years ago with halloween drawing near i had an idea for an ex periment to probe that question this time i wouldn t even have to leave my home to get my answers early in the evening joey a nine year old kid dressed as spider man and carrying a large yellow bag climbed the stairs of our front porch his mother accompanied him to ensure that no one gave her kid an apple with a razor blade inside by the way there never was a case of razor blades be ing distributed in apples on halloween it is just an urban myth she stayed on the sidewalk however to give joey the feeling that he was trick or treating by himself after the traditional query trick or treat i asked joey to hold open his right hand i placed three hershey kisses in his palm and asked him to hold them there for a moment you can also get one of these two snickers bars i said showing him a small one and a large one in fact if you give me one of those hershey kisses i will give you this smaller snickers bar and if you give me two of your hershey kisses i will give you this larger snickers bar now a kid may dress up like a giant spider but that doesn t mean he stupid the small snickers bar weighed one ounce and the large snickers bar weighed two ounces all joey had to do was give me one additional hershey kiss about ounce and he would get an extra ounce of snick ers this deal might have stumped a rocket scientist but for a nine year old boy the computation was easy he d get more than six times the return on investment in the net weight of chocolate if he went for the larger snickers bar in a flash joey put two of his kisses into my hand took the two ounce snickers bar and dropped it into his bag joey wasn t alone in making this snap decision all but one of the kids to whom i presented this offer traded in two kisses for the bigger candy bars zoe was the next kid to walk down the street she was dressed as a princess in a long white dress with a magic wand in one hand and an orange halloween pumpkin bucket in the other her younger sister was resting comfortably in their father arms looking cute and cuddly in her bunny outfit as they approached zoe called out in a high cute voice trick or treat in the past i admit that i have some times devilishly replied trick most kids stand there baf fled having never thought through their question to see that it allowed an alternative answer in this case i gave zoe her treat three hershey kisses but i did have a trick up my sleeve i offered little zoe a deal a choice between getting a large snickers bar in exchange for one of her hershey kisses or getting the small snickers bar for free without giving up any hershey kisses now a bit of rational calculation which in joey case was amply demonstrated would show that the best deal is to forgo the free small snickers bar pay the cost of one ad ditional hershey kiss and go for the large snickers bar on an ounce for ounce comparison it was far better to give up one additional hershey kiss and get the larger snickers bar two ounces instead of a smaller snickers bar one ounce this logic was perfectly clear to joe and the kids who encountered the condition in which both snickers bars had a cost but what would zoe do would her clever kid mind make that rational choice or would the fact that the small snickers bar was free blind her to the ratio nally correct answer as you might have guessed by now zoe and the other kids to whom i offered the same deal was completely blinded by free about percent of them gave up the better deal and took the worse deal just because it was free just in case you think kristina nina and i make a habit of picking on kids i ll mention that we repeated the experi ment with bigger kids in fact students at the mit student center the results replicated the pattern we saw on hallow een indeed the draw of zero cost is not limited to monetary transactions whether it products or money we just can t resist the gravitational pull of free so do yo u think you have a handle on free ok here a quiz suppose i offered you a choice be tween a free amazon gift certificate and a gift cer tificate for seven dollars think quickly which would you take if you jumped for the free certificate you would have been like most of the people we tested at one of the malls in boston but look again a gift certificate for seven dollars delivers a profit that clearly better than getting a certificate free earning can you see the irrational be havior in action let me tell you a story that describes the real influence of free on our behavior a few years ago amazon com started offering free shipping of orders over a certain amount some one who purchased a single book for might pay an additional 95 for shipping for instance but if the cus we also conducted the experiment offering the gift certificate for one dollar and the certificate for eight dollars this time most of the participants jumped for the certificate tomer bought another book for a total of they would get their shipping free some of the purchasers probably didn t want the second book and i am talking here from personal experience but the free shipping was so tempting that to get it they were willing to pay the cost of the extra book the people at ama zon were very happy with this offer but they noticed that in one place france there was no increase in sales is the french consumer more rational than the rest of us unlikely rather it turned out the french customers were reacting to a different deal here what happened instead of offering free shipping on orders over a certain amount the french division priced the shipping for those orders at one franc just one franc about cents this doesn t seem very different from free but it was in fact when amazon changed the promotion in france to include free shipping france joined all the other countries in a dramatic sales increase in other words whereas shipping for one franc a real bargain was virtu ally ignored by the french free shipping caused an enthu siastic response america online aol had a similar experience several years ago when it switched from pay per hour service to a monthly payment schedule in which you could log in as many hours as you wanted for a fixed 95 per month in preparation for the new price structure aol geared up for what it estimated would be a small increase in demand what did it get an overnight increase from to customers logging into the system and a doubling of the average time online that may seem good but it wasn t good aol customers encountered busy phone lines and soon aol was forced to lease services from other online providers who were only too happy to sell bandwidth to aol at the premium of snow shovels in a snowstorm what bob pittman the president of aol at the time didn t realize was that consumers would respond to the allure of free like starving people at a buffet when choosing between two products then we often overreact to the free one we might opt for a free checking account with no benefits attached rather than one that costs five dollars a month but if the five dollar checking account includes free traveler checks online billing etc and the free one doesn t we may end up spending more for this package of services with the free account than with the five dollar account similarly we might choose a mortgage with no closing costs but with interest rates and fees that are off the wall and we might get a product we don t really want simply because it comes with a free gift my most recent personal encounter with this involved a car when i was looking for a new car a few years ago i knew that i really should buy a minivan in fact i had read up on honda minivans and knew all about them but then an audi caught my eye at first through an appealing offer free oil changes for the next three years how could i resist to be perfectly honest the audi was sporty and red and i was still resisting the idea of being a mature and responsible father to two young kids it wasn t as if the free oil change completely swayed me but its influence on me was from a rational perspective unjustifiably large just because it was free it served as an additional allure that i could cling to so i bought the audi and the free oil a few months later while i was driving on a highway the transmission broke but that is a different story of course with a cooler head i might have made a more rational calculation i drive about miles a year the oil needs to be changed every miles and the cost per change is about over three years then i would save about or about percent of the purchase price of the car not a good reason to base my decision on it gets worse though now i have an audi that is packed to the ceiling with action figures a stroller a bike and other kids paraphernalia oh for a minivan the concept of zero also applies to time time spent on one activity after all is time taken away from another so if we spend 45 minutes in a line waiting for our turn to get a free taste of ice cream or if we spend half an hour filling out a long form for a tiny rebate there is something else that we are not doing with our time my favorite personal example is free entrance day at a museum despite the fact that most museums are not very expensive i find it much more appealing to satisfy my desire for art when the price is zero of course i am not alone in this desire so on these days i usually find that the museum is overcrowded the line is long it is hard to see anything and fighting the crowds around the museum and in the cafeteria is unpleasant do i realize that it is a mistake to go to a mu seum when it is free you bet i do but i go nevertheless zero mayalso affect food purchases food manufacturers have to convey all kinds of information on the side of the box they have to tell us about the calories fat content fiber etc is it possible that the same attraction we have to zero price could also apply to zero calories zero trans fats zero carbs etc if the same general rules apply pepsi will sell more cans if the label says zero calories than if it says one calorie suppose you are at a bar enjoying a conversation with some friends with one brand you get a calorie free beer and with another you get a three calorie beer which brand will make you feel that you are drinking a really light beer even though the difference between the two beers is negligible the zero calorie beer will increase the feeling that you re doing the right thing healthwise you might even feel so good that you go ahead and order a plate of fries so you can maintain the status quo with a cent fee as in the case of amazon shipping in france or you can start a stampede by offering something free think how powerful that idea is zero is not just another discount zero is a dif ferent place the difference between two cents and one cent is small but the difference between one cent and zero is huge if you are in business and understand that you can do some marvelous things want to draw a crowd make some thing free want to sell more products make part of the purchase free similarly we can use free to drive social policy want people to drive electric cars don t just lower the registration and inspection fees eliminate them so that you have cre ated free in the same way if health is your concern focus on early detection as a way to eliminate the progression of severe illnesses want people to do the right thing in terms of getting regular colonoscopies mammograms cholesterol checks diabetes checks and such don t just decrease the cost by decreasing the co pay make these critical proce dures free i don t think most policy strategists realize that free is an ace in their hand let alone know how to play it it cer tainly counterintuitive in these times of budget cutbacks to make something free but when we stop to think about it free can have a great deal of power and it makes a lot of sense appendix chapter let me explain how the logic of standard economic theory would apply to our setting when a person can select one and only one of two chocolates he needs to consider not the ab solute value of each chocolate but its relative value what he gets and what he gives up as a first step the rational con sumer needs to compute the relative net benefits of the two chocolates the value of the expected taste minus the cost and make a decision based on which chocolate has the larger net benefit how would this look when the cost of the lindt truffle was cents and the cost of the hershey kiss was one cent the rational consumer would estimate the amount of pleasure he expects to get from the truffle and the kiss let say this is pleasure units and five pleasure units respec tively and subtract the displeasure he would get from paying cents and one cent let say this is displeasure units and one displeasure unit respectively this would give him a total expected pleasure of pleasure units for the truffle and a total expected pleasure of four pleasure units for the kiss the truffle leads by points so it an easy choice the truffle wins hands down what about the case when the cost is reduced by the same amount for both products truffles cost cents and the kiss is free the same logic applies the taste of the chocolates has not changed so the rational consumer would estimate the pleasure to be and five pleasure units respectively what has changed is the displeasure in this setting the rational consumer would have a lower level of displeasure for both chocolates because the prices have been reduced by one cent and one displeasure unit here is the main point be cause both products were discounted by the same amount their relative difference would be unchanged the total ex pected pleasure for the truffle would now be pleasure units and the total expected pleasure for the kiss would now be five pleasure units 0 thetruffle leads by the same points so it should be the same easy choice the truffle wins hands down this is how the pattern of choice should look if the only forces at play were those of a rational cost benefit analysis the fact that the results from our experiments are so differ ent tells us loud and clear that something else is going on and that the price of zero plays a unique role in our deci sions chapter the cost of social norms why we are happy to do things but not when we are paid to do them ou are at your mother in law house for thanksgiving dinner and what a sumptuous spread she has put on the table for you the turkey is roasted to a golden brown the stuffing is homemade and exactly the way you like it your kids are delighted the sweet potatoes are crowned with marshmallows and your wife is flattered her favorite recipe for pumpkin pie has been chosen for dessert the festivities continue into the late afternoon you loosen your belt and sip a glass of wine gazing fondly across the table at your mother in law you rise to your feet and pull out your wallet mom for all the love you ve put into this how much do i owe you you say sincerely as silence descends on the gathering you wave a handful of bills do you think three hundred dollars will do it no wait i should give you four hundred this is not a picture that norman rockwell would have painted a glass of wine falls over your mother in law stands up red faced your sister in law shoots you an angry look and your niece bursts into tears next year thanksgiving celebration it seems may be a frozen dinner in front of the television set what s going on here why does an offer for direct pay ment put such a damper on the party as margaret clark judson mills and alan fiske suggested a long time ago the answer is that we live simultaneously in two different worlds onewhere social norms prevail and the other where market norms make the rules the social norms include the friendly requests that people make of one another could you help me move this couch could you help me change this tire social norms are wrapped up in our social nature and our need for community they are usually warm and fuzzy instant pay backs are not required you may help move your neighbor couch but this doesn t mean he has to come right over and move yours it like opening a door for someone it provides pleasure for both of you and reciprocity is not immediately required the second world the one governed by market norms is very different there nothing warm and fuzzy about it the exchanges are sharp edged wages prices rents interest and costs and benefits such market relationships are not neces sarily evil or mean in fact they also include self reliance inventiveness and individualism but they do imply compa rable benefits and prompt payments when you are in the domain of market norms you get what you pay for that just the way it is when we keep social norms and market norms on their separate paths life hums along pretty well take sex for in stance we may have it free in the social context where it is we hope warm and emotionally nourishing but there also market sex sex that is on demand and that costs money this seems pretty straightforward we don t have husbands or wives coming home asking for a trick nor do we have prostitutes hoping for everlasting love when social and market norms collide trouble sets in take sex again a guy takes a girl out for dinner and a movie and he pays the bills they go out again and he pays the bills once more they go out a third time and he still springing for the meal and the entertainment at this point he hoping for at least a passionate kiss at the front door his wallet is getting perilously thin but worse is what going on in his head he having trouble reconciling the social norm court ship with the market norm money for sex on the fourth date he casually mentions how much this romance is costing him now he crossed the line violation she calls him a beast and storms off he should have known that one can t mix social and market norms especially in this case with out implying that the lady is a tramp he should also have remembered the immortal words of woody allen the most expensive sex is free sex a few years ago james heyman a professor at the univer sity of st thomas and i decided to explore the effects of so cial and market norms simulating the thanksgiving incident would have been wonderful but considering the damage we might have done to our participant family relationships we chose something more mundane in fact it was one of the most boring tasks we could find there is a tradition in social science of using very boring tasks in this experiment a circle was presented on the left side of a computer screen and a box was presented on the right the task was to drag the circle using the computer mouse onto the square once the circle was successfully dragged to the square it disappeared from the screen and a new circle appeared at the starting point we asked the participants to drag as many circles as they could and we measured how many circles they dragged within five minutes this was our measure of their labor output the effort that they would put into this task how could this setup shed light on social and market ex changes some of the participants received five dollars for participating in the short experiment they were given the money as they walked into the lab and they were told that at the end of the five minutes the computer would alert them that the task was done at which point they were to leave the lab because we paid them for their efforts we expected them to apply market norms to this situation and act accordingly participants in a second group were presented with the same basic instructions and task but for them the reward was much lower cents in one experiment and cents in the other again we expected the participants to apply mar ket norms to this situation and act accordingly finally we had a third group to whom we introduced the tasks as a social request we didn t offer the participants in this group anything concrete in return for their effort nor did we mention money it was merely a favor that we asked of them we expected these participants to apply social norms to the situation and act accordingly how hard did the different groups work in line with the ethos of market norms those who received five dollars dragged on average circles and those who received cents dragged on average circles as expected more money caused our participants to be more motivated and work harder by about percent what about the condition with no money did these participants work less than the ones who got the low mon etary payment or in the absence of money did they apply social norms to the situation and work harder the results showed that on average they dragged circles much more than those who were paid cents and just slightly more than those who were paid five dollars in other words our participants worked harder under the nonmonetary social norms than for the almighty buck ok cents perhaps we should have anticipated this there are many examples to show that people will work more for a cause than for cash a few years ago for instance the aarp asked some lawyers if they would offer less expensive services to needy retirees at something like an hour the lawyers said no then the program manager from aarp had a brilliant idea he asked the lawyers if they would offer free services to needy retirees overwhelmingly the lawyers said yes what was going on here how could zero dollars be more attractive than when money was mentioned the law yers used market norms and found the offer lacking relative to their market salary when no money was mentioned they used social norms and were willing to volunteer their time why didn t they just accept the thinking of themselves as volunteers who received because once market norms enter our considerations the social norms depart a similar lesson was learned by n achum sicherman an economics professor at columbia who was taking martial arts lessons in japan the sensei the master teacher was not charging the group for the training the students feeling that this was unfair approached the master one day and sug gested that they pay him for his time and effort setting down his bamboo shinai the master calmly replied that if he charged them they would not be able to afford him in the previous experiment then those who got paid cents didn t say to themselves good for me i get to do this favor for these researchers and i am getting some money out of this and continue to work harder than those who were paid nothing instead they switched themselves over to the market norms decided that cents wasn t much and worked halfheartedly in other words when the market norms entered the lab the social norms were pushed out but what would happen if we replaced the payments with a gift surely your mother in law would accept a good bottle of wine at dinner or how about a housewarming present such as an eco friendly plant for a friend are gifts methods of exchange that keep us within the social exchange norms would participants receiving such gifts switch out of the so cial norms and into market norms or would offering gifts as rewards maintain the participants in the social world to find out just where gifts fall on the line between social and market norms james and i decided on a new experi ment this time we didn t offer our participants money for dragging circles across a computer screen we offered them gifts instead we replaced the cent reward with a snickers bar worth about cents and the five dollar incentive with a box of godiva chocolates worth about five dollars the participants came to the lab got their reward worked as much as they liked and left then we looked at the results as it turned out all three experimental groups worked about equally hard during the task regardless of whether got a small snickers bar these participants dragged on average circles the godiva chocolates these participants dragged on average circles or nothing at all these participants dragged on average circles the conclusion no one is offended by a small gift because even small gifts keep us in the social exchange world and away from market norms but what would happen if we mixed the signals for the two types of norms what would happen if we blended the market norm with the social norm in other words if we said that we would give them a so cent snickers bar or a five dollar box of godiva chocolates what would the partici pants do would a cent snickers bar make our participants work as hard as a snickers bar made them work or would it make them work halfheartedly as the so cents made them work or would it be somewhere in the middle the next experiment tested these ideas as it turned out the participants were not motivated to work at all when they got the cent snickers bar and in fact the effort they invested was the same as when they got a payment of cents they reacted to the explicitly priced gift in exactly the way they reacted to cash and the gift no longer invoked social norms by the mention of its cost the gift had passed into the realm of market norms by the way we replicated the setup later when we asked passersby whether they would help us unload a sofa from a truck we found the same results people are willing to work free and they are willing to work for a reasonable wage but offer them just a small payment and they will walk away gifts are also effective for sofas and offering people a gift even a small one is sufficient to get them to help but men tion what the gift cost you and you will see the back of them faster than you can say market norms these results show that for market norms to emerge it is sufficient to mention money even when no money changes hands but of course market norms are not just about effort they relate to a broad range of behaviors including self reliance helping and individualism would simply get ting people to think about money influence them to behave differently in these respects this premise was explored in a set of fantastic experiments by kathleen vohs a professor at the university of minnesota nicole mead a graduate stu dent at florida state university and miranda goode a graduate student at the university of british columbia they asked the participants in their experiments to com plete a scrambled sentence task that is to rearrange sets of words to form sentences for the participants in one group the task was based on neutral sentences for example it cold outside for the other group the task was based on sentences or phrases related to money for example high paying salary would thinking about money in this man ner be sufficient to change the way participants behave in one of the experiments the participants finished the unscrambling task and were then given a difficult puzzle in which they had to arrange disks into a square as the ex perimenter left the room he told them that they could come to him if they needed any help who do you think asked for this general procedure is called priming and the unscrambling task is used to get participants to think about a particular topic without direct instructions to do so help sooner those who had worked on the salary sen tences with their implicit suggestion of money or those who had worked on the neutral sentences about the weather and other such topics as it turned out the students who had first worked on the salary task struggled with the puzzle for about five and a half minutes before asking for help whereas those who had first worked on the neutral task asked for help after about three minutes thinking about money then made the participants in the salary group more self reliant and less willing to ask for help but these participants were also less willing to help oth ers in fact after thinking about money these participants were less willing to help an experimenter enter data less likely to assist another participant who seemed confused and less likely to help a stranger an experimenter in dis guise who accidentally spilled a box of pencils overall the participants in the salary group showed many of the characteristics of the market they were more selfish and self reliant they wanted to spend more time alone they were more likely to select tasks that required individual input rather than teamwork and when they were deciding where they wanted to sit they chose seats farther away from whomever they were told to work with indeed just thinking about money makes us behave as most economists believe we behave and less like the social animals we are in our daily lives this leads me to a final thought when you re in a restau rant with a date for heaven sake don t mention the price of the selections yes they re printed clearly on the menu yes this might be an opportunity to impress your date with the caliber of the restaurant but if you rub it in you ll be likely to shift your relationship from the social to the market norm yes your date may fail to recognize how much this meal is setting you back yes your mother in law may assume that the bottle of wine you ve presented is a blend when it a special reserve merlot that the price you have to pay though to keep your relationships in the social domain and away from market norms so we ljve in two worlds one characterized by social ex changes and the other characterized by market exchanges and we apply different norms to these two kinds of relation ships moreover introducing market norms into social ex changes as we have seen violates the social norms and hurts the relationships once this type of mistake has been com mitted recovering a social relationship is difficult once you ve offered to pay for the delightful thanksgiving dinner your mother in law will remember the incident for years to come and if you ve ever offered a potential romantic partner the chance to cut to the chase split the cost of the courting process and simply go to bed the odds are that you will have wrecked the romance forever my good friends uri gneezy a professor at the university of california at san diego and aldo rustichini a professor at the university of minnesota provided a very clever test of the long term effects of a switch from social to market norms a few years ago they studied a day care center in israel to determine whether imposing a fine on parents who arrived late to pick up their children was a useful deterrent uri and aldo concluded that the fine didn t work well and in fact it had long term negative effects why before the fine was in troduced the teachers and parents had a social contract with social norms about being late thus if parents were late as they occasionally were they felt guilty about it and their guilt compelled them to be more prompt in picking up their kids in the future in israel guilt seems to be an effective way to get compliance but once the fine was imposed the day care center had inadvertently replaced the social norms with market norms now that the parents were paying for their tardiness they interpreted the situation in terms of mar ket norms in other words since they were being fined they could decide for themselves whether to be late or not and they frequently chose to be late needless to say this was not what the day care center intended but the real story only started here the most interesting part occurred a few weeks later when the day care center re moved the fine now the center was back to the social norm would the parents also return to the social norm would their guilt return as well not at all once the fine was removed the behavior of the parents didn t change they continued to pick up their kids late in fact when the fine was removed there was a slight increase in the number of tardy pickups after all both the social norms and the fine had been removed this experiment illustrates an unfortunate fact when a so cial norm collides with a market norm the social norm goes away for a long time in other words social relationships are not easy to reestablish once the bloom is off the rose once a so cial norm is trumped by a market norm it will rarely return the fact that we live in both the social world and the mar ket world has many implications for our personal lives from time to time we all need someone to help us move something or to watch our kids for a few hours or to take in our mail when we re out of town what the best way to motivate our friends and neighbors to help us would cash do it a gift perhaps how much or nothing at all this social dance as i m sure you know isn t easy to figure out especially when there a risk of pushing a relationship into the realm of a mar ket exchange here are some answers asking a friend to help move a large piece of furniture or a few boxes is fine but asking a friend to help move a lot of boxes or furniture is not espe cially if the friend is working side by side with movers who are getting paid for the same task in this case your friend might begin to feel that he being used similarly asking your neighbor who happens to be a lawyer to bring in your mail while you re on vacation is fine but asking him to spend the same amount of time preparing a rental contract for you free is not the delicate balance between social and market norms is also evident in the business world in the last few decades companies have tried to market themselves as social companions that is they d like us to think that they and we are family or at least are friends who live on the same cul de sac like a good neighbor state farm is there is one famil iar slogan another is home depot gentle urging you can do it we can help whoever started the movement to treat customers socially had a great idea if customers and a company are family then the company gets several benefits loyalty is paramount mi nor infractions screwing up your bill and even imposing a modest hike in your insurance rates are accommodated relationships of course have ups and downs but overall they re a pretty good thing but here what i find strange although companies have poured billions of dollars into marketing and advertising to create social relationships or at least an impression of so cial relationships they don t seem to understand the nature of a social relationship and in particular its risks for example what happens when a customer check bounces if the relationship is based on market norms the bank charges a fee and the customer shakes it off business is business while the fee is annoying it nonetheless accept able in a social relationship however a hefty late fee rather than a friendly call from the manager or an automatic fee waiver is not only a relationship killer it a stab in the back consumers will take personal offense they ll leave the bank angry and spend hours complaining to their friends about this awful bank after all this was a relationship framed as a social exchange no matter how many cookies slogans and tokens of friendship a bank provides one viola tion of the social exchange means that the consumer is back to the market exchange it can happen that quickly what the upshot if you re a company my advice is to remember that you can t have it both ways you can t treat your customers like family one moment and then treat them impersonally or even worse as a nuisance or a competitor a moment later when this becomes more convenient or profit able this is not how social relationships work if you want a social relationship go for it but remember that you have to maintain it under all circumstances on the other hand if you think you may have to play tough from time to time charging extra for additional ser vices or rapping knuckles swiftly to keep the consumers in line you might not want to waste money in the first place on making your company the fuzzy feel good choice in that case stick to a simple value proposition state what you give and what you expect in return since you re not setting up any social norms or expectations you also can t violate any after all it just business companies have also tried to establish social norms with their employees it wasn t always this way years ago the workforce of america was more of an industrial market driven exchange back then it was often a nine to five time clock kind of mentality you put in your hours and you got your paycheck on friday since workers were paid by the hour they knew exactly when they were working for the man and when they weren t the factory whistle blew or the corporate equivalent took place and the transaction was finished this was a clear market exchange and it worked adequately for both sides today companies see an advantage in creating a social exchange after all in today market we re the makers of intangibles creativity counts more than industrial machines the partition between work and leisure has likewise blurred the people who run the workplace want us to think about work while we re driving home and while we re in the shower they ve given us laptops cell phones and blackberries to bridge the gap between the workplace and home further blurring the nine to five workday is the trend in many companies to move away from hourly rates to monthly pay in this work environment social norms have a great advantage they tend to make employees passionate hard working flexible and concerned in a market where employ ees loyalty to their employers is often wilting social norms are one of the best ways to make workers loyal as well as motivated open source software shows the potential of social norms in the case of linux and other collaborative projects you can post a problem about a bug on one of the bulletin boards and see how fast someone or often many people will react to your request and fix the software using their own leisure time could you pay for this level of service most likely but if you had to hire people of the same caliber they would cost you an arm and a leg rather people in these communities are happy to give their time to society at large for which they get the same social benefits we all get from helping a friend paint a room what can we learn from this that is applicable to the business world there are social rewards that strongly motivate behavior and one of the least used in corporate life is the encouragement of social rewards and reputation in treating their employees much as in treating their customers companies must understand their implied long term commitment if employees promise to work harder to achieve an important deadline even canceling family obliga tions for it if they are asked to get on an airplane at a mo ment notice to attend a meeting then they must get something similar in return something like support when they are sick or a chance to hold on to their jobs when the market threatens to take their jobs away although some companies have been successful in creat ing social norms with their workers the current obsession with short term profits outsourcing and draconian cost cut ting threatens to undermine it all in a social exchange after all people believe that if something goes awry the other party will be there for them to protect and help them these be liefs are not spelled out in a contract but they are general obligations to provide care and help in times of need again companies cannot have it both ways in particular i am worried that the recent cuts we see in employees benefits child care pensions flextime exercise rooms the cafeteria family picnics etc are likely to come at the ex pense of the social exchange and thus affect workers pro ductivity i am particularly worried that cuts and changes in medical benefits are likely to transform much of the employer employee social relationship to a market relationship if companies want to benefit from the advantages of social norms they need to do a better job of cultivating those norms medical benefits and in particular comprehensive medical coverage are among the best ways a company can express its side of the social exchange but what are many companies doing they are demanding high deductibles in their insur ance plans and at the same time are reducing the scope of benefits simply put they are undermining the social con tract between the company and the employees and replacing it with market norms as companies tilt the board and em ployees slide from social norms to the realm of market norms can we blame them for jumping ship when a better offer ap pears it really no surprise that corporate loyalty in terms of the loyalty of employees to their companies has become an oxymoron organizations can also think consciously about how peo ple react to social and market norms should you give an employee a gift worth or pay him or her an extra in cash which is better if you ask the employees the majority will most likely prefer cash over the gift but the gift has its value though this is sometimes ill understood it can provide a boost to the social relationship between the employer and the employee and by doing so provide long term benefits to everyone think of it this way who do you suppose is likely to work harder show more loyalty and truly love his work more someone who is getting in cash or someone who is getting a personal gift of course a gift is a symbolic gesture and to be sure no one is going to work for gifts rather than a salary for that matter no one is going to work for nothing but if you look at companies like google which offers a wide variety of ben efits for employees including free gourmet lunches you can see how much goodwill is created by emphasizing the social side of the company worker relationship it remarkable how much work companies particularly start ups can get out of people when social norms such as the excitement of building something together are stronger than market norms such as salaries stepping up with each promotion if corporations started thinking in terms of social norms they would realize that these norms build loyalty and more important make people want to extend themselves to the degree that corporations need today to be flexible con cerned and willing to pitch in that what a social relation ship delivers tms question of social norms in the workplace is one we should be thinking about frequently america productivity depends increasingly on the talent and efforts of its workers could it be that we are driving business from the realm of social norms into market norms are workers thinking in terms of money rather than the social values of loyalty and trust what will that do to american productivity in the long run in terms of creativity and commitment and what of the social contract between government and the citizen is that at risk as well at some level we all know the answers we understand for instance that a salary alone will not motivate people to risk their lives police officers firefighters soldiers they don t die for their weekly pay it the social norms pride in their profession and a sense of duty that will motivate them to give up their lives and health a friend of mine in miami once accompanied a u s customs agent on a patrol of the offshore waters the agent carried an assault rifle and could certainly have pounded several holes into a fleeing drug boat but had he ever done so no way he replied he wasn t about to get himself killed for the government salary he received in fact he confided his group had an unspoken agreement with the drug couriers the feds wouldn t fire if the drug dealers didn t fire perhaps that why we rarely if ever hear about gun battles on the edges of america war on drugs how could we change this situation first we could make the federal salary so good that the customs agent would be willing to risk his life for it but how much money is that compensation equal to what the typical drug trafficker gets for racing a boat from the bahamas to miami alternatively we could elevate the social norm making the officer feel that his mission is worth more than his base pay that we honor him as we honor our police and firefighters for a job which not only stabilizes the structure of society but also saves our kids from all kinds of dangers that would take some inspi rational leadership of course but it could be done let me describe how that same thought applies to the world of education i recently joined a federal committee on incentives and accountability in public education this is one aspect of social and market norms that i would like to ex plore in the years to come our task is to reexamine the no child left behind policy and to help find ways to motivate students teachers administrators and parents my feeling so far is that standardized testing and performance based salaries are likely to push education from social norms to market norms the united states already spends more money per student than any other western soci ety would it be wise to add more money the same con sideration applies to testing we are already testing very frequently and more testing is unlikely to improve the qual ity of education i suspect that one answer lies in the realm of social norms as we learned in our experiments cash will take you only so far social norms are the forces that can make a difference in the long run instead of focusing the attention of the teach ers parents and kids on test scores salaries and competi tion it might be better to instill in all of us a sense of purpose mission and pride in education to do this we certainly can t take the path of market norms the beatles proclaimed some time ago that you can t buy me love and this also applies to the love of learning you can t buy it and if you try you might chase it away so how can we improve the educational system we should probably first rethink school curricula and link them in more obvious ways to social goals elimination of poverty and crime elevation of human rights etc technological goals boosting energy conservation space exploration nanotech nology etc and medical goals cures for cancer diabetes obesity etc that we care about as a society this way the students teachers and parents might see the larger point in education and become more enthusiastic and motivated about it we should also work hard on making education a goal in itself and stop confusing the number of hours students spend in school with the quality of the education they get kids can get excited about many things baseball for example and it is our challenge as a society to make them want to know as much about nobel laureates as they now know about base ball players i am not suggesting that igniting a social passion for education is simple but if we succeed in doing so the value could be immense money as it turns out is very often the most expensive way to motivate people social norms are not only cheaper but often more effective as well so what good is money in ancient times money made trading easier you didn t have to sling a goose over your back when you went to market or decide what section of the goose was equivalent to a head of lettuce in modern times money has even more benefits as it allows us to specialize borrow and save but money has also taken on a life of its own as we have seen it can remove the best in human interactions so do we need money of course we do but could there be some aspects of our life that would be in some ways better without it that a radical idea and not an easy one to imagine but a few years ago i had a taste of it at that time i got a phone call from john perry barlow a former lyricist for the grate ful dead inviting me to an event that proved to be both an important personal experience and an interesting exercise in creating a moneyless society barlow told me that i had to come to burning man with him and that if i did i would feel as if i had come home burning man is an annual week long event of self expression and self reliance held in black rock desert nevada regularly attended by more than 000 people burning man started in on baker beach in san francisco when a small crowd designed built and eventually set fire to an eight foot wooden statue of a man and a smaller wooden dog since then the size of the man be ing burned and the number of people who attend the festivi ties has grown considerably and the event is now one of the largest art festivals and an ongoing experiment in temporary community burning man has many extraordinary aspects but for me one of the most remarkable is its rejection of market norms money is not accepted at burning man rather the whole place works as a gift exchange economy you give things to other people with the understanding that they will give something back to you or to someone else at some point in the future thus people who can cook might fix a meal psy chologists offer free counseling sessions masseuses massage those lying on tables before them those who have water of fer showers people give away drinks homemade jewelry and hugs i made some puzzles at the hobby shop at mit and gave them to people mostly people enjoyed trying to solve them at first this was all very strange but before long i found myself adopting the norms of burning man i was surprised in fact to find that burning man was the most accepting social and caring place i had ever been i m not sure i could easily survive in burning man for all weeks of the year but this experience has convinced me that life with fewer market norms and more social norms would be more satisfy ing creative fulfilling and fun theanswer i believe is not to re create society as burn ing man but to remember that social norms can play a far greater role in society than we have been giving them credit for if we contemplate how market norms have gradually taken over our lives in the past few decades with their em phasis on higher salaries more income and more spending we may recognize that a return to some of the old social norms might not be so bad after all in fact it might bring quite a bit of the old civility back to our lives cheerleaders and vicarious fans the prius hybrid is a new car based on a hundred year old technology that no domestic carmaker cared enough about to develop today there a long list of brands following toyota the tribe has turned into a movement this is astounding the biggest staidest consumer product industry turning itself upside down in just a few years if struggling high overhead car companies can launch a technology and find market acceptance imagine what you can do with this new leverage what do you do for a living what do you make leaders make a ruckus leading from the bottom the skeptical among us look at the idea of leadership and we hesitate we hesitate because it feels like something we need to be ordained to do that without authority we can t lead that big organizations reserve leadership for the ceo not for us perhaps you work at a big organization perhaps you feel as though there just too much resistance to change here a question is your organization stiffer than the pentagon more bureaucratic or formalized thomas barnett changed the pentagon from the bottom no he wasn t on kp duty but he was close he had no status no rank he was just a researcher with a big idea here what the wall street journal said mr barnett overhauled the concept to address more directly the post world the result is a three hour powerpoint presentation that more resembles performance art than a pentagon briefing it making mr barnett years old a key figure in the debate currently raging about what the modern military should look like senior military officials say his decidedly controversial ideas are influencing the way the pentagon views its enemies vulnerabilities and future structure it simple really barnett led a tribe that was passionate about change he galvanized them inspired them and connected them through his idea one man with no authority suddenly becomes a key figure tribes give each of us the very same opportunity skill and attitude are essential authority is not in fact authority can get in the way the grateful dead and jack it worth taking a second to think about what it really means to be a tribe in permission marketing years ago i wrote about how marketers must earn the right to deliver anticipated personal and relevant messages to people who want to get them and that still correct as far as it goes but tribes go much further that because in addition to the messages that go from the marketer or the leader to the tribe there are the messages that go sideways from member to member and back to the leader as well the grateful dead understood this they created concerts to allow people not just to hear their music but to hear it together that where the tribe part comes in i just heard about jack an occasional restaurant run by danielle sucher and dave turner in brooklyn they open the restaurant only about twenty times a year on saturday nights by appointment go online and you can see the menu in advance then you book and pay if you want to go instead of seeking diners for their dishes danielle and dave get to create dishes for their diners instead of serving anonymous patrons they throw a party danielle is the food columnist for the popular gothamist web site and she and dave run the food blog habeas brle that means they already interact with the tribe it means that once the restaurant is up and running it becomes the central clearinghouse the place to hang out with the other tribe members if the food is daring and the service is generous jack can t fail the market requires change and that requires leadership if leadership is the ability to create change your tribe believes in and the market demands change then the market demands leaders managers manage by using the authority the factory gives them you listen to your manager or you lose your job a manager can t make change because that not his job his job is to complete tasks assigned to him by someone else in the factory leaders on the other hand don t care very much for organizational structure or the official blessing of whatever factory they work for they use passion and ideas to lead people as opposed to using threats and bureaucracy to manage them leaders must become aware of how the organization works because this awareness allows them to change it leadership doesn t always start at the top but it always manages to affect the folks at the top in fact most organizations are waiting for someone like you to lead them what does it take to create a movement if we look at two nobel prize winners and their movements muhammad yunus and al gore some parallels become clear and they directly relate to the tactics available to you as you lead your tribe microfinance as a tool to fight poverty and the effort to recognize and stem global warming have both become movements but as yasmina zaidman at the acumen fund told me both problems and their solutions were recognized more than thirty years ago we weren t lacking the answer muhammad yunus had it all along so why did it take thirty years for the idea to gain steam the answer as you ve probably guessed is that there a difference between telling people what to do and inciting a movement the movement happens when people talk to one another when ideas spread within the community and most of all when peer support leads people to do what they always knew was the right thing great leaders create movements by empowering the tribe to communicate they establish the foundation for people to make connections as opposed to commanding people to follow them this is how skype spread around the world cofounder niklas zennstrm understood that overthrowing the tyranny of the phone companies was too big a project for a small company but if he could empower the tribe to do it themselves to connect to one another and to spread the word he would be able to incite a movement malcolm gladwell wrote about the fall of the berlin wall and it involved much the same dynamic the collapse of east germany wasn t the work of one hardworking activist instead it was the gradual but inexorable growth of the tribe a loosely coordinated movement of activists that gained in force until it couldn t be stopped one after another intractable problems fall in the face of movements improving a tribe as we saw earlier it takes only two things to turn a group of people into a tribe a shared interest a way to communicate the communication can be one of four kinds leader to tribe tribe to leader tribe member to tribe member tribe member to outsider so a leader can help increase the effectiveness of the tribe and its members by transforming the shared interest into a passionate goal and desire for change providing tools to allow members to tighten their communications and leveraging the tribe to allow it to grow and gain new members most leaders focus only on the third tactic a bigger tribe somehow equals a better tribe in fact the first two tactics almost always lead to more impact every action you take as a leader can affect these three elements and the challenge is to figure out which one to maximize the american automobile association has millions of members but it arguably has far less impact on the world than do the two thousand people who go to the ted conference each year one is about big and the other is about change the national rifle association has a huge impact on the political culture of the united states far in excess of the organization actual size that because the tribe is extraordinarily well connected communicating up down and sideways and because they have a passionate mission not just a common idea the new tools and technologies available to groups are transforming what it means to think of tribal communication smart leaders are grabbing those tools and putting them to work what tribes leave behind build a company and you ll leave a trace a factory advertising the nonrecyclable junk produced as a result of your efforts thinking about stuff is easy because we can see and touch and hold stuff stuff seems to matter because it here right now tribes though aren t about stuff they re about connection one of my favorite organizations the acumen fund just celebrated its seventh anniversary this nonprofit funds entrepreneurs in the developing world using trade and ownership and commerce as a replacement for achievement stifling aid acumen makes connections it is growing a tribe of committed talented people who are spreading a message of empowerment respect and growth here what amazed me though unlike the residue of stuff the tribal connections you can create with leadership grow they don t fade as the organization matures and touches more people those connections lead to more connections the tribe thrives it delivers value and it spreads internet folks call this viral activity or a virtuous cycle the better you do the better you do connections lead to connections great ideas spread anatomy of a movement senator bill bradley defines a movement as having three elements a narrative that tells a story about who we are and the future we re trying to build a connection between and among the leader and the tribe something to do the fewer limits the better too often organizations fail to do anything but the third wikipedia how did wikipedia become one of the top ten sites on the internet it has only about a dozen full time employees and had no source of revenue other than small donations the way that jimmy wales wikipedia cofounder built the tribe is instructive he attracted a small group of people only five thousand people account for the vast majority of work on the articles on the site and engaged them in a vision he didn t tell them what to do he didn t manage the effort he led it wales connected the tribe members to one another with ever evolving technology that made it easier and easier for them to engage and communicate and he gave the tribe a platform they could use to engage the outside world that it three steps motivate connect and leverage leading from the bottom with a newsletter in at the age of twenty four i joined a tiny software company called spinnaker based in cambridge massachusetts we were crazy enough to embrace the audacious goal of inventing the first generation of educational computer games i was the thirtieth employee after my summer internship spinnaker offered me a job starting a new brand they wanted me to acquire science fiction stories and turn them into literary adventure games byron preiss had already sold us the rights to fahrenheit and a few other novels and i had to acquire others and turn them all into products ready for stores nationwide the problem was that no one worked for me no secretary no staff no programmers spinnaker was busy building dozens of products and about forty programmers in the engineering department were allocated on a rotating basis to various projects i was lent precisely three programmers i needed more a lot more if i was going to make my christmas ship date so i started a newsletter the newsletter highlighted the work of every person who worked on one of my products it highlighted their breakthroughs and talked about the new ground we were breaking music in a game i made photocopies and distributed the newsletter to the interoffice mailbox of every person in the company by then about a hundred people twice a week the newsletter went out twice a week i talked about our quest twice a week i chronicled the amazing work of our tiny tribe the newsletter connected the tribe members it turned a disparate group of career engineers into a working community within a month six engineers had defected to the tribe working with me in their spare time then it was twenty soon every person in the entire department was either assigned to my project or moonlighting on it we shipped five products in time for christmas and every one went gold selling millions of dollars worth of copies and saving the company did engineers switch because of the newsletter of course not they switched for the journey they wanted to be part of something that mattered twenty years later people on that team still talk about what we built and i the twenty four year old with no experience and no staff got to go on the ride of a lifetime is that all i did launch a newsletter of course not i did difficult things pushed obstacles out of the way lived and breathed the project and injected it with a soul thirty of us slept in the office every night for a month to make the ship date twenty nine highly skilled technical people and me everyone had a job to do that month and mine was to help everyone else communicate everything i did was for us not for me i didn t manage i led crowds and tribes two different things a crowd is a tribe without a leader a crowd is a tribe without communication most organizations spend their time marketing to the crowd smart organizations assemble the tribe crowds are interesting and they can create all sorts of worthwhile artifacts and market effects but tribes are longer lasting and more effective marketing changes everything but it mostly changes the market the market wants you to be remarkable the most important tribes are bored with yesterday and demand tomorrow most of all the market has demonstrated that ideas that spread win and the ideas that are spreading are the remarkable ones for fifty years established brands with efficient factories and effective marketing carried the day pepsi the salvation army and the local hardware store were the cornerstones of the marketplace suddenly though the oldest brands are no longer the fastest growing ones suddenly the most experienced businesspeople are no longer the most successful ones and suddenly the safest jobs are not so safe anymore the marketplace has raised its voice it now clear that we want novelty and style and most of all stuff that great if you want us to follow you don t be boring good enough stopped being good enough a long time ago so why not be great the difference between average and mediocre management often works to maintain the status quo to deliver average products to average people in a stable environment this is exactly the right strategy build reliability and predictability cut costs and make a profit traditional marketing the marketing of push understands this the most stable thing to do is push a standard product to a standard audience and succeed with discounts or distribution but for tribes average can mean mediocre not worth seeking out boring life too short to fight the forces of change life too short to hate what you do all day life way too short to make mediocre stuff and almost everything that standard is now viewed as mediocre is there a difference between average and mediocre not so much average stuff is taken for granted not talked about and certainly not sought out the end result of this is that many people many really good people spend all day trying to defend what they do trying to sell what they ve always sold and trying to prevent their organizations from being devoured by the forces of the new it must be wearing them out defending mediocrity is exhausting how many fans do you have in an article posted on his technium web site kevin kelly brilliantly described the world of true fans a true fan he argues is a member of the tribe who cares deeply about you and your work that person will cross the street to buy from you or bring a friend to hear you or invest a little extra to support you an individual artist needs only a thousand true fans in her tribe it enough it enough because a thousand fans will bring you enough attention and support to make a great living to reach more people to do great work it enough because a thousand fans true fans form a tribe a true fan brings three friends with him to a john mayer concert or to the opening of a chuck close exhibit a true fan pays extra to own the first edition or buys the hardcover instead of just browsing around on the web site most important a true fan connects with other true fans and amplifies the noise the artist makes a corporation or a nonprofit or a church may need more than that perhaps a million fans if you re starbucks or fifteen million if you re running for president but it beyond doubt that there a number you can figure out what it is and it probably fewer people than you imagine too many organizations care about numbers not fans they care about hits or turnstile clicks or media mentions what they re missing is the depth of commitment and interconnection that true fans deliver instead of always being on the hunt for one more set of eyeballs true leaders have figured out that the real win is in turning a casual fan into a true one fans true fans are hard to find and precious just a few can change everything what they demand though is generosity and bravery twitter and trust and tribes and true fans most people who see twitter com don t get it it seems invasive or time consuming or even dumb the converts though understand the true power of twitter twitter is deceptively simple it a web protocol that makes it easy to instant message people with short notes like going to the gym in fact the limit is characters about half the length of this paragraph the difference between an instant message and twits though is that your instant message goes to one person and a twit goes to anyone who has chosen to follow you example laura fitton a young mom in boston has thousands of people following her on twitter every time she types in a short blurb they see it over time twit by twit laura has built trust which has led to a successful career as a consultant and a worldwide speaking practice she met fascinating people and changed the way her tribe sees the world she now has true fans people who seek her out and talk about her laura couldn t have done this with one speech or one blog post but by consistently touching a tribe of people with generosity and insight she earned the right to lead personally i can t imagine the technology mattering much blogs and twitter and all manner of other tools will come and go possibly by the time you read this the tactics are irrelevant and the technology will always be changing the essential lesson is that every day it gets easier to tighten the relationship you have with the people who choose to follow you the status quo organizations that destroy the status quo win individuals who push their organizations who inspire other individuals to change the rules thrive again we re back to leadership which can come from anyone anywhere in the organization the status quo could be the time that everyone knows it takes you to ship an order or the commission rate that everyone knows an agent ought to be paid the status quo might be the way everyone expects a product to be packaged or the pricing model that everyone accepts because it been around so long whatever the status quo is changing it gives you the opportunity to be remarkable initiative happiness look around you ll see that the marketplace every marketplace rewards innovation things that are fresh stylish remarkable and new the fastest growing churches are the newest ones the best selling books are always the surprise hits that come out of nowhere the tax shelter that everyone is talking about is the one based on the latest rulings products and services like those require initiative to produce you can t manage your way to initiative interesting side effect creating products and services that are remarkable is fun doing work that fun is engaging so not surprisingly making things that are successful is a great way to spend your time there you go initiative happiness crowbars with a long enough crowbar you can rip nails out of a board with a long enough teeter totter you can lift a sumo wrestler off the ground with enough leverage you can change your company your industry and the world the levers just got longer for everyone the web and word of mouth and viruses and outsourcing and the long tail and the other factors involved in social media mean that everyone every person all six billion of us has far more power than ever before the king and the status quo are in big trouble wait you might have glossed over that last paragraph perhaps because it so short but especially because it so challenging what i m saying is that one person can make a video that reaches fifty million viewers what i m saying is that one person can invent a pricing model that turns an industry upside down what i m saying is that one person okay what i really mean is you has everything everything you need to build something far bigger than yourself the people around you realize this and they are ready to follow if you re ready to lead scott beale party here a simple example of a tribe enabled by new technology scott beale is an impresario with a long history of innovation and leadership his company laughing squid does everything from web hosting to t shirts from laser engraving to arts listings in short he leads an eclectic tribe at the sxsw conference in scott got tired of waiting in line to get into the google party so he walked down the street found a deserted bar grabbed some tables in the back and fired up his cell phone using twitter he announced alta vista party at ginger man within minutes eight people showed up shortly thereafter fifty then there was a line out the door no it not a political movement sure it a tribe the energy and connection of the tribe are palpable multiply this effect by a million similar tribes and now you understand what happening tribes are just waiting to be turned into movements and occasionally to stop to have a beer together it important to note that twitter merely enabled the event it didn t cause it to occur unless scott had earned the respect and permission of the tribe that follows him he would have been all alone at the bar the party didn t take four minutes to organize it took four years a brief history of the factory part the beginning two things conspired to bring us the factory the first is pretty obvious factories are efficient starting a factory and filling it with factory workers is a good way to make a profit by factory i don t necessarily mean a place with heavy machinery greasy floors and a din i mean any organization that cranks out a product or a service does it with measurable output and tries to reduce costs as it goes i mean any job where your boss tells you what to do and how to do it the second reason we have factories has nothing to do with efficiency and a lot to do with human nature part of us wants stability we want the absence of responsibility that a factory job can give us the idea of i m doing what you told me to is very compelling especially if the alternative is foraging for food or begging on the streets so when factories showed up we ran to join them on a recent trip i took to india this mind set was made crystal clear ask almost anyone there what the perfect job would be and the answer is working as a government bureaucrat not only do you have air conditioning but you aren t even asked to take initiative the job is steady the pay is good and there are no surprises the factory is part of the fabric of our lives it there because it pays and it there because it steady and it there because we want it what you won t find in a factory is a motivated tribe making a difference and what you won t find waiting outside the factory is a tribe of customers excited about what to come a brief history of the factory part ii the end somewhere along the way perhaps when twenty thousand ford workers lost their jobs in one day or when it became clear that soft drink companies were losing all their growth to upstarts the factory advantage began to fade it wasn t so safe to have a factory job after all and in an age of leverage in an age where smarts and style were beating machines every time doing what your boss said wasn t so enticing either if you could have any job in the world what would it be did you say a low level bureaucrat working in the social security office in yonkers new york did you say a midlevel supervisor at a struggling gm plant in ohio did you say fry cook at mcdonald somehow i doubt it now it seems the air conditioning and the illusion of deniability aren t worth so much now when we envision our dream jobs we re imagining someone who reaps huge rewards as a result of her insight or someone who has control over what he does all day creating products or services that he actually proud of it certainly involves having authority over your time and your effort and having input into what you do none of which have anything to do with working in a factory so is it really a free agent nation the author dan pink coined the term free agent nation to describe a movement of smart people leaving organizations to go out on their own that not what i m talking about though organizations are more important than ever before it the factories we don t need organizations give us the ability to create complex products they provide the muscle and consistency necessary to get things to market and to back them up most important organizations have the scale to care for large tribes but organizations don t have to be factories not anymore factories are easy to outsource factories can slow you down the organizations of the future are filled with smart fast flexible people on a mission the thing is that requires leadership if you don t have a time tested manual you can t manage your way through this in unstable times growth comes from leaders who create change and engage their organizations instead of from managers who push their employees to do more for less the f word so if tribes reward innovation and if initiators are happier then why doesn t everyone do it because of fear i ve encountered thousands it might be tens of thousands of people walking around with great ideas some of the ideas really are great some are merely pretty good there doesn t seem to be a shortage of ideas ordinary folks can dream up remarkable stuff fairly easily what missing is the will to make the ideas happen in a battle between two ideas the best one doesn t necessarily win no the idea that wins is the one with the most fearless heretic behind it a lot of us would like to believe that there a bureau of idea approval or the bia if you like acronyms the bia sits in judgment of ideas and blesses the best ones go ahead and hone your remarkable concept submit it to the bia and let them do the rest alas it not going to happen like that any time soon thinking your way out of the fear fear an emotion no doubt about it one of the strongest oldest and most hardwired the media love to glamorize the rare downfall of the heretic who doesn t quite make it we re already primed to hear about the person who got into trouble who lost his job his house his family his happiness because he had the hubris and audacity to challenge the status quo and since we re eager for this news we notice it the few times it happens what interesting about the folks i meet who are engaged and are clearly heretics is that they ve actively talked themselves out of the fear i mean the fear is still there but it drowned out by a different story it the story of success of drive of doing something that matters it an intellectual story about what the world or your industry or your project needs and how your insight can help make a difference i believe you can talk over the fear laying out a game plan that makes the fear obsolete it not about some clever tactic or a better way to write a memo to your boss it about making it clear to yourself and to others that the world is now demanding that we change and fast wait we need to stop again it clear that just a few paragraphs aren t going to be sufficient to undo a lifetime of having fear beaten into you so stop for a second and think about this the only shortcut in this book the only technique or how to or inside info is this the levers are here the proof is here the power is here the only thing holding you back is your own fear not easy to admit but essential to understand the peter principle revisited dr laurence peter is famous for proposing that in a hierarchy every employee tends to rise to his level of incompetence in other words when you do a great job you get promoted and that process repeats itself until finally you end up in a job you can t handle i d like to paraphrase the peter principle i think what actually happens is that in every organization everyone rises to the level at which they become paralyzed with fear the essence of leadership is being aware of your fear and seeing it in the people you wish to lead no it won t go away but awareness is the key to making progress when it all falls apart it so common but it doesn t really have a name i m talking about the people who struggle for years but never seem to get anywhere this lack of traction is often most noticeable in small businesses but you ll also find it at well meaning nonprofits and large corporations you work and work following all the rules pushing really hard but nothing happens all pain no gain what happening i think these people are becoming ever better at following but are never learning to lead they re following instructions following directions following the pack and honing their skills but hiding hiding from the fear of leading when you are leading a tribe a tribe that you belong to the benefits increase the work gets easier and the results are more obvious that the best reason to overcome the fear worth criticizing a remarkable product or service is like a purple cow brown cows are boring purple ones are worth mentioning those ideas spread those organizations grow the essence of what happening in the market today revolves around creating purple cows here the marketing math ideas that spread win boring ideas don t spread boring organizations don t grow working in an environment that static is no fun even worse working for an organization that is busy fighting off change is horrible so why haven t you and your team launched as many purple cows as you d like fear of failure is overrated fear of failure is actually overrated as an excuse why because if you work for someone then more often than not the actual cost of the failure is absorbed by the organization not by you if your product launch fails they re not going to fire you the company will make a bit less money and will move on what people are afraid of isn t failure it blame criticism we choose not to be remarkable because we re worried about criticism we hesitate to create innovative movies launch new human resource initiatives design a menu that makes diners take notice or give an audacious sermon because we re worried deep down that someone will hate it and call us on it that the stupidest thing i ve ever heard what a waste of money who responsible for this sometimes the criticism doesn t even have to be that obvious the fear of hearing i m surprised you launched this without doing more research is enough to get many people to do a lot more research to study something to death and then kill it hey at least you didn t get criticized fear of criticism is a powerful deterrent because the criticism doesn t actually have to occur for the fear to set in watch a few people get criticized for being innovative and it pretty easy to convince yourself that the very same thing will happen to you if you re not careful constructive criticism of course is a terrific tool if a critic tells you i don t like it or this is disappointing he done no good at all in fact quite the opposite is true he used his power to injure without giving you any information to help you do better next time worse he hasn t given those listening any data with which to make a thoughtful decision on their own not only that but by refusing to reveal the basis for his criticism he being a coward because there no way to challenge his opinion i admit it when i get a bad review my feelings are hurt after all it would be nice if every critic said a title of mine was a breakthrough an inspirational thoughtful book that explains how everything works but sometimes they don t which is about enough to ruin my day but it not enough it not enough to ruin my day because i realize that my book got noticed most people loved it a few hated it but by and large most books are ignored one bad review doesn t ruin my day because i realize what a badge of honor it is to get a bit of criticism at all it means that i confounded expectations that i didn t deliver the sequel or the simple practical guide that some expected it means that in fact i did something worth remarking on the lesson here is this if i had written a boring book there d be no criticism no conversation the products and services that get talked about are the ones that are worth talking about how was your day if your answer is fine then i don t think you were leading so the challenge as you contemplate your next opportunity to be boring or remarkable is to answer these two questions if i get criticized for this will i suffer any measurable impact will i lose my job get hit upside the head with a softball bat or lose important friendships if the only side effect of the criticism is that you will feel bad about the criticism then you have to compare that bad feeling with the benefits you ll get from actually doing something worth doing being remarkable is exciting fun profitable and great for your career feeling bad wears off and then once you ve compared the bad feeling and the benefits and you ve sold yourself on taking the remarkable path answer this one how can i create something that critics will criticize the cult of the heretic heretics are engaged passionate and more powerful and happier than everyone else and they have a tribe that they support and that supports them in turn challenging the status quo requires a commitment both public and private it involves reaching out to others and putting your ideas on the line or pinning your ninety five theses to the church door heretics must believe more than anyone else in an organization it the person who challenging the status quo the one who is daring to be great who is truly present and not just punching a clock who must have confidence in her beliefs can you imagine steve jobs showing up for the paycheck it nice to get paid it essential to believe should they build a statue of you how much ego is involved in being a leader david chang is a fantastic chef with a loyal tribe his restaurants are blogged about incessantly and people spend hours trying to get in to them they take photos of the items he makes and post them online together with reviews like david chang is a genius it clear to me that if they built statues for chefs they d build one for david but is david doing it for the glory or is he doing it for the tribe i think you know the answer great leaders focus on the tribe and only the tribe pema chodron is a buddhist nun working in a monastery in nova scotia millions of people across the world revere her work read her books listen to her recordings and visit her if they can is she a raging egomaniac of course not listen to her for three minutes and you ll know that she not doing what she does for glory she doing it to help which is true of all great leaders from david chang in his new york city kitchen to nancy pearl seattle favorite librarian they re generous they exist to help the tribe find something to enable the tribe to thrive but they understand that the most powerful way to enable is to be statueworthy by getting out front by making a point by challenging convention and by speaking up those are brave acts and bravery begets statues it easy to hesitate when confronted with the feeling that maybe you re getting too much attention great leaders are able to reflect the light onto their teams their tribes great leaders don t want the attention but they use it they use it to unite the tribe and to reinforce its sense of purpose when you abuse the attention you are taking something from the tribe when fidel castro gave six or seven hour long speeches with mandatory attendance he was diminishing the energy of his tribe when a ceo takes the spoils of royalty and starts acting like a selfish monarch he no longer leading he taking the world best coach watching meghan mcdonald coach the members of team rock is hardly awe inspiring mostly she just talks quietly one on one to someone who needs to hear from her over the course of a few hours meghan will have dozens of conversations like that she occasionally talks to the entire team but she never raises her voice no one cries no one is belittled no one is bullied after just a few weeks amazing things start to happen the members of the team start coaching each other a ten year old novice offers a pointer to a veteran recently back from the national competition meghan leaves the building and practice continues sports analogies rarely work for me they re too unrealistic too testosterone filled for the real world meghan however isn t just a coach she someone who understands authentic leadership and she realizes what it means to create a tribe she doesn t lead the way other people lead and that fine because there isn t a right technique a proven tactic a right way and a wrong way deciding to lead not manage is the critical choice meghan connects and inspires she doesn t manage tighter the first thing a leader can focus on is the act of tightening the tribe it tempting to make the tribe bigger to get more members to spread the word this pales however when juxtaposed with the effects of a tighter tribe a tribe that communicates more quickly with alacrity and emotion is a tribe that thrives a tighter tribe is one that is more likely to hear its leader and more likely still to coordinate action and ideas across the members of the tribe steve jobs at apple has tightened the tribe of apple fanatics in a variety of ways by creating substantial new products and announcing them online he made it a ritual for apple fanatics to tune in to hear what new within hours of a new product announcement the word has spread to millions or even tens of millions of users all electronically all online at the same time apple has enjoyed an interesting side effect of jobs obsession with secrecy about new products online rumor sites and speculation further fuel the conversations among apple fans users will prototype imagined products and share pictures and even dig up obscure patents to prove their points this tightening can happen without technology and it can happen when there is no profit motive keith ferrazzi leads a tribe of smart celebrities and opinion leaders from meg ryan to ben zander and he leads this unleadable group merely by tightening the tribe he introduces people he invites them to dinner he finds areas of common interest and then gets out of the way tactics and tools for tightness the internet and the explosion in social media have made it easier than ever to market the first kind of marketing the act of spreading the word and reaching the unreached allows tribes of all sorts to form sites like meetup com and craigslist make it easy for people who aren t connected to become connected i m more interested in the second kind of marketing the act of tightening your organization and spreading the word within the tribe a blog is an easy way to see this method in action a blogger has a free nearly effortless tool to send regular daily hourly messages to the people who want to read them and with comments and trackbacks the members of the tribe can talk back and to each other discussions take place ideas are shared decisions are made quickly i could write an entire book about the power of a blog to disseminate a leader ideas an unpublished poet previously doomed to railing against the system is now published if he wants to be if the ideas are great they ll spread the spread of these ideas can attract a tribe and the poet goes from anonymity to leadership blogs can work within existing organizations as well i needed a photocopied newsletter to galvanize the engineers i worked with in you can use a blog and reach more people more powerfully and for free internet companies have taken the original idea behind blogs and amplified it into a set of tools that anyone can use to tighten a tribe with twitter tiny driplike updates reach the thousands of people who are waiting to hear from you and follow your lead facebook goes in the opposite direction of twitter instead of forcing you to use just a few characters it enables a huge range of images text and connections to be created facebook surfaces what some are calling the social graph who you know how you know them who knows whom it takes the hidden world of tribes and illuminates it with bright digital light basecamp is a third form of online interaction very different from twitter and facebook in that it quite deliberate perfect for managing projects and tracking work by accessing the stuff that used to be in private e mails or handwritten journals basecamp makes it easy for the entire tribe to track progress and feel the momentum that you re building nothing online is even close to a substitute for the hard work and generosity that comes from leadership but these tools make leadership more powerful and productive regardless of who in your tribe discomfort leadership is scarce because few people are willing to go through the discomfort required to lead this scarcity makes leadership valuable if everyone tries to lead all the time not much happens it discomfort that creates the leverage that makes leadership worthwhile in other words if everyone could do it they would and it wouldn t be worth much it uncomfortable to stand up in front of strangers it uncomfortable to propose an idea that might fail it uncomfortable to challenge the status quo it uncomfortable to resist the urge to settle when you identify the discomfort you ve found the place where a leader is needed if you re not uncomfortable in your work as a leader it almost certain you re not reaching your potential as a leader followers of course a tribe needs followers too an organization any organization needs people who aren t just willing to follow but are eager to follow i think though it a mistake to believe that your best tribe recruits are blind sheep folks who do nothing but mindlessly follow instructions let you down in two ways first they re not going to do the local leadership required when tribe members interact they re going to be so busy following the playbook that they ll hesitate about engaging in the interactions that make a tight tribe such a vibrant organization people don t engage merely to remind one another of the status quo instead they eagerly engage when they want something to improve this microleadership is essential to the health of your organization second they re not going to do a very good job of recruiting new members to your tribe that because evangelism requires leadership leading someone toward giving up one worldview and embracing yours isn t easy and it not always comfortable consider any vibrant group political activists nonprofit volunteers or brand fanatics in each case it the microleaders in the trenches and their enthusiastic followers who make the difference not the honcho who is ostensibly running the group leaning in backing off doing nothing groups create vacuums small pockets where stasis sets in where nothing is happening imagine a cocktail party in its early stages where everyone is standing around waiting for something to happen or a marketplace before it opens filled with shoppers but with all the stores boarded up with nothing to create energy or excitement there are no tribes here only isolated individuals in groups with no motion leaders figure out how to step into those vacuums and create motion they work hard to generate movement the sort of movement that can transform a group into a tribe a student can sit in a classroom and accept what the teacher is sending out then do the work and get by or she can take initiative and lead she can provoke and question and ask for more a marketer can offer a product take orders and move on or he can use interactions with prospects to create something more to surprise and delight and generate far more than just a customer who got her money worth this posture of leaning in is rare and valuable in the spring of i announced a paid summer internship for students more than well educated students from all over the world applied as an experiment i set up a private facebook group for the applicants and invited each one to participate sixty of them joined immediately no tribe existed yet just sixty strangers in an online forum within hours a few had taken the lead posting topics starting discussions leaning in and leading they called on their peers to contribute and participate and the rest they lurked they sat and they watched they were hiding afraid of something that wasn t likely to happen whom would you hire how could the lurkers imagine that doing nothing would increase the chances that they d be selected were they hoping that they d meet someone interesting or discover something new by just watching the experiment was perfect in that there were no externalities no side discussions no special cases just sixty or so people each demonstrating behavior that came naturally not all leadership involves getting in the face of the tribe it takes just as much effort to successfully get out of the way jimmy wales leads wikipedia not by inciting but by enabling others to fill the vacuum my leadership of the internship application process involved setting the stage and stepping back not pushing at every step along the way the one path that never works is the most common one doing nothing at all nothing at all feels safe and it takes very little effort it involves a lot of rationalization and a bit of hiding as well the difference between backing off and doing nothing may appear subtle but it not a leader who backs off is making a commitment to the power of the tribe and is alert to the right moment to step back in someone who is doing nothing is merely hiding leadership is a choice it the choice to not do nothing lean in back off but don t do nothing participating isn t leading twenty percent of the population of canada now uses facebook many of those users have the false impression that joining a group somehow matters it doesn t and canadians aren t the only ones with the same impression sending in your rsum showing up at the networking reception hanging out at the singles bar these are dumb ways to lead the tribe and they re not even useful ways to be seen as a valued member showing up isn t sufficient friending ten or twenty or a thousand people in facebook might be good for your ego but it has zero to do with any useful measure of success case studies crossfit com and patientslikeme com crossfit is a tribe of slightly crazy okay really crazy fitness fanatics these are people who on any given day will do a routine like this one fifteen handstand push ups followed by one pull up followed by thirteen handstand pushups followed by three pull ups followed by eleven handstand push ups followed by five pull ups followed by nine handstand push ups followed by seven pull ups followed by seven handstand push ups followed by nine pull ups followed by five handstand push ups followed by eleven pull ups followed by three handstand push ups followed by thirteen pull ups followed by one handstand push up followed by fifteen pull ups and they ll do it in a timed competition against thousands of people around the world on the day i checked their site more than four hundred people had posted their times on this particular workout there are certification courses across the country and they are invariably sold out weeks or months in advance a growing cadre of certified trainers are opening gyms around the world each gym finding its own new members of the crossfit tribe all coordinated by the central web site the crossfit tribe is strong and getting stronger and it largely the work of greg glassman otherwise known as coach coach has built the crossfit tribe from scratch inspiring and cajoling and laying down the rules no coach no tribe glassman innately understands how to lead the tribe he pushes them to the limit every day he creates an environment where the tribe not only wants to share news and ideas and camaraderie with one another but is able to and the tribe grows because individuals proudly segregate themselves and speak up on behalf of the tribe simultaneously recruiting and hazing new members compare this to patientslikeme com a web site i discovered via an article in the new york times here a tribe that appears to be leaderless there are more than seven thousand ill people each sharing all the details of his or her diagnosis and current health status from dosages to side effects the group is building an ever growing database of real world data about treatments for parkinson and other debilitating diseases and they re supporting one another with enthusiasm and comfort as they go there is no greg glassman or oprah winfrey cheering them on they cheer one another on and who better because no one can appreciate what they re going through more than they can but the founders of patientslikeme com are leaders nonetheless they found a tribe that desperately wanted to communicate and they gave them the tools to do so they made the tribe tighter that leadership as well leaning in or backing off but not doing nothing three hungry men and a tribe when you get a chance head over to http com this blog is obsessively chronicling every restaurant in a sixteen block square of seattle for each restaurant most of them are asian they include details like the length of the chopsticks and the contents of the bonus fortune cookie here a quote i was looking forward to this place cause some amazon buddies rated it quite highly it a small place requiring us to eat in the neighboring food court which is awesome cause i like hanging out with crack addicts as is typical i ordered typical fare menu item number the tonkatsu it advertised that it contained slice of pork which just wasn t going to work for me i opted for extra pork this ramen is like a bowl of fatty pork in butter with some noodles added for texture i admire their bravery in serving this to me it should come with a carton of newports for clearly my health is not their concern the broth though flavorful is overwhelmed with the fatness of the pork however the pork is fantastic delicious and cooked to the point where it falls apart i don t know about you but i want in i want to eat at every one of these restaurants i want to post my own reviews i want to join this tribe if they ask me to pitch in i will i m in others will scoff and move on wondering what the obsession is all about that what makes it a tribe of course there are insiders and outsiders curiosity a fundamentalist is a person who considers whether a fact is acceptable to his religion before he explores it as opposed to a curious person who explores first and then considers whether or not he wants to accept the ramifications a curious person embraces the tension between his religion and something new wrestles with it and through it and then decides whether to embrace the new idea or reject it curious is the key word it has nothing to do with income nothing to do with education and certainly nothing to do with organized religion it has to do with a desire to understand a desire to try a desire to push whatever envelope is interesting leaders are curious because they can t wait to find out what the group is going to do next the changes in the tribe are what are interesting and curiosity drives them curious people count not because there are a lot of them but because they re the ones who talk to people who are in a stupor they re the ones who lead the masses in the middle who are stuck the masses in the middle have brainwashed themselves into thinking it safe to do nothing which the curious can t abide it easy to underestimate how difficult it is for someone to become curious for seven ten or even fifteen years of school you are required to not be curious over and over and over again the curious are punished i don t think it a matter of saying a magic word boom and then suddenly something happens and you re curious it more about a five or ten or fifteen year process where you start finding your voice and finally you begin to realize that the safest thing you can do feels risky and the riskiest thing you can do is play it safe once recognized the quiet yet persistent voice of curiosity doesn t go away ever and perhaps it such curiosity that will lead us to distinguish our own greatness from the mediocrity that stares us in the face what we re seeing is that fundamentalism really has nothing to do with religion and everything to do with an outlook regardless what your religion is the plurality myth in order to win an election you need more than half the votes ideally more than half of the population will support you but you win if you get more than half the voters in order to lead a tribe no such rule applies all you need to do is motivate people who choose to follow you the rest of the population is free to ignore you or disagree with you or move on starbucks doesn t serve coffee to the majority of people in the united states the new york city crochet guild appeals to just a small percentage of the people who encounter it that okay you don t need a plurality or even a majority in fact in nearly every case trying to lead everyone results in leading no one in particular this leads to an interesting thought you get to choose the tribe you will lead through your actions as a leader you attract a tribe that wants to follow you that tribe has a worldview that matches the message you re sending if you are leading a tribe focused on saving the world by fighting global warming the tribe will of course have a worldview that includes the idea that global warming is a problem and that it can be addressed through its actions they come to the tribe with that in mind and your leadership resonates with them if on the other hand you choose to work to persuade a different group one with a very different worldview they will likely reject you al gore started leading his tribe when he didn t know who they were he stated his message and people found him ultimately people are most easily led where they wanted to go all along while that may seem as if it limits your originality or influence it true fox news didn t persuade millions of people to become conservatives they just assembled the tribe and led them where they were already headed the schoolteacher experiment imagine two classrooms with similar teachers one has fifteen students the other thirty two which group gets a better education all other things being equal the smaller class will always do better the teacher has more time to spend customizing the lesson to each student she has fewer students hence fewer disruptions as well now flip the experiment around what if the fifteen students are begrudgingly taking the course as a requirement for graduation while the thirty two had to apply to be admitted and are excited to be there no contest tribes are increasingly voluntary no one is forced to work for your firm or attend your services people have a choice of which music to listen to and which movies to watch so great leaders don t try to please everyone great leaders don t water down their message in order to make the tribe a bit bigger instead they realize that a motivated connected tribe in the midst of a movement is far more powerful than a larger group could ever be the virtuous cycle versus the exclusive tribe some businesses get better when they get bigger some nonprofits do as well tribes that work better when they re bigger get bigger political parties for example thrive when they re the majority facebook works precisely because everyone uses it you have a fax machine only because everyone you work with does too but bigger isn t always the answer some tribes do better when they re smaller more exclusive harder to get into some tribes thrive precisely because they re small push to make one of these tribes bigger and you might just ruin the entire thing no one goes there anymore it too popular it always a choice your choice most people don t matter so much most people like the products they already have so marketers ignore them most people work hard to fit in so others don t notice them most people like eating at places where they ve eaten before most people think this book is a bad idea most people would like the world to stay just as it is but calmer most people are afraid most people didn t use google until last year most people aren t curious you re not most people you re not the target market for most marketers and you re certainly not a manager not only aren t leaders most people but the members of the most important tribes aren t most people either you re not going to be able to grow your career or your business or feed the tribe by going after most people most people are really good at ignoring new trends or great employees or big ideas you can worry about most people all day but i promise you that they re not worried about you they can t hear you regardless of how hard you yell almost all the growth that available to you exists when you aren t like most people and when you work hard to appeal to folks who aren t most people does the status quo ruin your day every day how was your day are you stuck with the way things were instead of busy turning things into what they could be heretics have a plan they understand that changing the status quo is not only profitable but fun too being a heretic an outsider and a rabble rouser feels scary why bother they burn heretics at the stake they also drown them denounce them ignore them and hang them from the rafters i should have used the past tense none of that is true anymore now we invite heretics to davos heretics get elected to congress heretics make a fortune when their companies go public heretics not only love their jobs they get a private jet too the image of the stake is hard to forget it touches us in a way that almost primal but it also obsolete marketing has made sure of that the same forces that taught us to drink coke for breakfast or spend on a handbag are now at work on the status quo heretics are too numerous to burn at the stake so we celebrate them the wrong question we re almost there but some of you are itching to ask me exactly the wrong questions which are how do i do this or even worse how do i get my boss to let me do this or to be really blunt what the risk free way to insinuate myself into the system so i get approval to make change surely there a method of making change without being burned at the stake it turns out that there is but you already know what it is belief nobody is going to listen to your idea for change sagely shake his head and say sure go do that no one anoints you as leader nobody is going to see your powerpoint presentation and hand you a check change isn t made by asking permission change is made by asking forgiveness later all you need to know is two things the first thing you need to know is that individuals have far more power than ever before in history one person can change an industry one person can declare war one person can reinvent science or politics or technology the second thing you need to know is that the only thing holding you back from becoming the kind of person who changes things is this lack of faith faith that you can do it faith that it worth doing faith that failure won t destroy you our culture works hard to prevent change we have long had systems and organizations and standards designed to dissuade people from challenging the status quo we enforce our systems and call whoever is crazy enough to challenge them a heretic and society enforces the standards by burning its heretics at the stake either literally or figuratively but the world has changed a lot there are heretics everywhere you look it so asymmetrical that burning heretics isn t particularly effective any longer as a result more and more people good people people on a mission people with ideas that matter are stepping forward and making a difference just about every system whether it political financial or even religious has become asymmetrical the process has turned upside down scale isn t the same as power in fact scale can hurt we ve seen this in the war in iraq as much as we ve seen it in the war in the soda aisle or in the growth of new religions in each case an individual or a small group has the power to turn an existing system on its head now most of the time we call heretics leaders the heretics are winning you can and must join them the balloon factory and the unicorn i m not sure you ve ever visited a balloon factory probably not the people who work in the balloon factory are timid afraid even they re very concerned about pins needles and porcupines they don t like sudden changes in temperature sharp objects are a problem as well the balloon factory isn t really a bad place to work if you rationalize a bit it steady work with a bit of a rush around new year the rest of the time it quiet and peaceful and not so scary except when the unicorns show up at first the balloon factory folks shush the unicorn and warn him away that often works but sometimes the unicorn ignores them and wanders into the factory anyway that when everyone runs for cover it amazingly easy for a unicorn to completely disrupt a balloon factory that because the factory is organized around a single idea the idea of soft quiet stability the unicorn changes all that the balloon factory is all about the status quo and leaders change the status quo leaders are generous in today supercharged political and tv environment it easy to believe that in order to lead you need to be an egomaniac a driven superstar intent on self glorification and aggrandizement in fact the opposite is nearly always the case leaders who set out to give are more productive than leaders who seek to get even more surprising is the fact that the intent of the leader matters the tribes can sniff out why someone is asking for their attention looking out for number one is an attitude and it one that doesn t pay so we have ceos who sit in cubicles just like everyone else we find successful religious leaders who don t fly by private jet or have a limo waiting for them outside we watch eighty fouryear old former president jimmy carter building houses for the poor the benefits to these leaders aren t monetary or based on status instead they get their compensation from watching the tribe thrive as the ability to lead a tribe becomes open to more people it interesting to note that those who take that opportunity and those who succeed most often are doing it because of what they can do for the tribe not because of what the tribe can do for them don t forget the big mac and the microwave oven in just outside of pittsburgh a third tier mcdonald franchisee named jim delligatti broke the rules and invented a new sandwich within a year the big mac was on the menu of mcdonald restaurants around the world they even serve a meatless version in india jim wasn t focused on managing his franchise at the expense of everything else instead he became a leader not blessed with a title or official sanction jim led the entire corporation in a new direction in percy spencer a low ranking engineer at the raytheon corporation was trying to improve radar technology when he accidentally melted a chocolate candy bar being pretty smart percy realized that he had invented the microwave oven next step microwave popcorn within decades the microwave oven was a must have appliance in almost every american home the remarkable thing about these two stories is how rare they are we keep hearing about the invention of post it notes and other apocryphal tales precisely because there aren t that many to choose from for a long time if you wanted to get something done you started at the top or you got really lucky leverage came from cash and organizational commitment if bill gates or jack welch or lyndon johnson thought something was a good idea it was much more likely to get done welcome to the age of leverage bottom up is a really bad way to think about it because there is no bottom in an era of grassroots change the top of the pyramid is too far away from where the action is to make much of a difference it takes too long and it lacks impact the top isn t the top anymore because the streets are where the action is the new leverage available to everyone means that the status quo is more threatened than ever and each employee now has the responsibility to change the rules before someone else does this isn t about working your way up to the top by following the rules and then starting down the path of changing your world instead these innovations are examples of leadership about one heretic someone with a vision who understood the leverage available who went ahead and changed things a few industries do fine by embracing the status quo the list is getting shorter every day though if you ship oil around the world or sell credit cards or want to get elected village supervisor you can probably coast for a while longer embracing the old rules but not that much longer it seems that every factory is under pressure every balloon maker not only fears the unicorn but desperately needs one kellogg owns hundreds of millions of dollars worth of cereal factories they have a welltrained sales force miles of shelf space and tons of advertising so why was bear naked able to build a significant business right under their nose without expensive factories or a huge sales force bear naked took a very simple very traditional product and changed the way many people buy their breakfast bear naked didn t try to manage a portfolio of assets they didn t try to protect the factory they didn t have one to protect instead they led the way down a different path one based on fashion and change and leverage odds are that growth and success are now inextricably linked to breaking the old rules and setting your organization new rules loose in an industry too afraid to change climbing rocks chris sharma is a heretic who climbs rocks chris changed the rules of an entire sport and along the way influenced the way tens of thousands of people think about personal achievement for hundreds of years rock climbers followed a simple principle one foot and one hand on the wall at all times if you re anchored with two out of your four limbs you can do a pretty good spiderman imitation without risking your life right left right left up you go little risk plenty of progress instead of staying glued to the wall chris jumps it called a dyno chris didn t invent the dyno but he certainly pushed it further than anyone ever expected it could go chris can climb routes that were previously deemed impossible when he gets to a dead end he looks up and jumps no legs no arms just air straight up two or three or four feet grabbing a small clump of rock with two fingers and continuing his climb for a while this was controversial it wasn t right it was risky and then bit by bit the guys in the factory came around they discovered that it was a reasonable but surprising solution to a large number of rock climbing problems suddenly impossible routes weren t impossible any longer the guess is because chris fits the stereotype of the typical heretic you re not convinced he a loner he risking his life and doing absolutely absurd things forty feet over the mediterranean and landing on his back in the water on a regular basis it easy to look at chris and say i could never do that and you d be right you and i will never dyno a rock arch the lesson isn t that you need to risk your fingers not to mention your life on a rock the lesson is that one person with a persistent vision can make change happen whether climbing rocks or delivering services here a simple way to think about it obe carrion former u s rock climbing champion won a tournament in an unusual way obe was one of four finalists and each had to climb a very difficult route up a steep wall the first three finalists did the same thing they entered the roped off area inspected the route and then slowly began climbing one hold at a time working their way up to the top two made it with a slip or two one fell obe was scheduled to go last he came out of the isolation area inspected the route took twenty steps back and he ran up the wall he didn t hesitate or interpolate or hedge his bets he just committed it turns out that this was the easiest way up the wall leaning into the problem made the problem go away who settles settling is no fun it a malignant habit a slippery slope that takes you to mediocrity managers settle all the time they don t really have a choice because there are too many competing priorities heretics don t settle they re not good at that managers who are stuck who compromise to keep things quiet who battle the bureaucracy every day they re the ones who settle what else can they do the art of leadership is understanding what you can t compromise on fear faith and religion people who challenge and then change the status quo do something that quite difficult they overcome the resistance of people they trust people they work for people in their community every step along the way it far easier to stop and accept the thanks of the balloon factory workers for finally giving up than it is to persist and risk the humiliation of failure so why do it faith is the unstated component in the work of a leader and i think faith is underrated paradoxically religion is vastly overrated faith goes back a long way faith leads to hope and it overcomes fear faith gave our ancestors the resilience they needed to deal with the mysteries of the pre science world faith is the dividing line between humans and most other species we have faith that the sun will rise tomorrow faith that newton laws will continue to govern the way a ball travels and faith that our time in med school will pay off twenty years from now because society is still going to need doctors chris sharma is able to do a dyno on a rock face one hundred feet above ground because he has faith that it ll work out okay if you watch kids learning how to dyno you ll see that the secret to developing the skill isn t about building their muscles or learning some exotic technique it is merely about developing the faith that it ll work merely of course is a huge step it nothing but a few neurons worth of faith just the knowledge that you can do it but without faith the leap never works faith is critical to all innovation without faith it suicidal to be a leader to act like a heretic religion on the other hand represents a strict set of rules that our fellow humans have overlaid on top of our faith religion supports the status quo and encourages us to fit in not to stand out there are countless religions in our lives not just the capital r religions like zoroastrianism or judaism there the ibm religion of the for example which included workplace protocols dress codes and even a precise method for presenting ideas on an overhead projector there the religion of broadway which determines what a musical is supposed to look and feel like there the religion of the mba right down to the standard curriculum and perceptions of what is successful a job at bain company and what sort of flaky going to work for a brewery religion works great when it amplifies faith that why human beings invented religion it why we have spiritual religions and cultural religions and corporate religions religion gives our faith a little support when it needs it and it makes it easy for your peers to encourage you to embrace your faith religion at its best is a sort of mantra a subtle but consistent reminder that belief is okay and that faith is the way to get where you re going the reason we need to talk about this though is that often religion does just the opposite religion at its worst reinforces the status quo often at the expense of our faith they had a religion at woolworth department store and sticking without variation to the principles that made the store great prevented them from turning it into a new better kind of experience the store is long gone of course they have a religion at the country club down the street as well a set of convictions and rules that is just too hard to change as a result an entire generation of professional women won t join that club and it going to fade and blow away soon challenge religion and people wonder if you re challenging their faith the reason it so difficult to have a considered conversation about religion is that people feel threatened not by the implied criticism of the rituals or irrationality of a particular religious practice but because it feels like criticism of their faith faith as we ve seen is the cornerstone that keeps our organizations together faith is the cornerstone of humanity we can t live without it but religion is very different from faith religion is just a set of invented protocols rules to live by for now heretics challenge a given religion but do it from a very strong foundation of faith in order to lead you must challenge the status quo of the religion you re living under of course religion and faith go together you can remind yourself of your faith by wearing the company uniform or uttering the mantra of your current religion you can embrace the support of the community by showing up at church or at the company picnic and following the rituals of whichever religion is being practiced without religion it easier for faith to flag it no wonder that religion has been around forever it reinforces faith and we can t succeed without it so successful heretics create their own religions fast company magazine was a new testament for a new religion it brought together a new group of friends new supporters new rituals the same thing happens at companies that embrace heretical behavior like ideo and at blogs or even at buck restaurant in silicon valley or the ted conference or other places where leaders like to hang out these religions exist for one reason to reinforce our faith you can do this on purpose you can recognize the need for faith in your idea you can find the tribe you need to support you and yes you can create a new religion around your faith steve jobs did it on purpose at apple and phil knight is famous for doing it at nike switching religions without giving up faith a recent study by the pew research center for the people and the press found that about a third of all americans have left the religion they grew up with the study mistakenly uses the word faith but in fact few of these people have lost faith what they ve done instead is change the system they use for reinforcing that faith when you fall in love with the system you lose the ability to grow faith is what you do if religion comprises rules you follow faith is demonstrated by the actions you take when you lead without compensation when you sacrifice without guarantees when you take risks because you believe then you are demonstrating your faith in the tribe and its mission of course it difficult but leaders will tell you that it worth it a word for it religion and faith are often confused someone who opposes faith is called an atheist and widely reviled but we don t have a common word for someone who opposes a particular religion heretic will have to do if faith is the foundation of a belief system then religion is the faade and the landscaping it easy to get caught up in the foibles of a corporate culture and the systems that have been built over time but they have nothing at all to do with the faith that built the system in the first place change is made by people by leaders who are proud to be called heretics because their faith is never in question in the year the council of trent wrote this about heretics finally all the faithful are commanded not to presume to read or possess any books contrary to the prescriptions of these rules or the prohibition of this list and if anyone should read or possess books by heretics or writings by any author condemned and prohibited by reason of heresy or suspicion of false teaching he incurs immediately the sentence of excommunication boy are you in trouble better get rid of this book over the top underdog bravery for about a decade i ve carried a coin in my gear bag it is one of seventy coins i gave to the team i led at yoyodyne a company i started attached to the coin is a little tag that celebrates our group and our over the top underdog bravery leadership almost always involves thinking and acting like the underdog that because leaders work to change things and the people who are winning rarely do what we did was and what you do is courageous it requires bravery managing doesn t and following the rules to make a living doesn t it might be hard work but it feels safe changing things pushing the envelope and creating a future that doesn t exist yet at the same time you re criticized by everyone else requires bravery and over the top that easy ordinary thinking and ordinary effort are almost never enough to generate leadership that because our inclination is to do barely enough it takes something extraordinary a call to action that is irresistible and a cause worth fighting for to make people actually join in if you re not over the top you re not going to have any chance at all of making things happen the easiest thing the easiest thing is to react the second easiest thing is to respond but the hardest thing is to initiate reacting as zig ziglar has said is what your body does when you take the wrong kind of medicine reacting is what politicians do all the time reacting is intuitive and instinctive and usually dangerous managers react responding is a much better alternative you respond to external stimuli with thoughtful action organizations respond to competitive threats individuals respond to colleagues or to opportunities response is always better than reaction but both pale in comparison to initiative initiating is really and truly difficult and that what leaders do they see something others are ignoring and they jump on it they cause the events that others have to react to they make change take the follow the merits of leadership are so ingrained that it natural to say i ll take the lead sometimes though it may make more sense to take the follow leading when you don t know where to go when you don t have the commitment or the passion or worst of all when you can t overcome your fear that sort of leading is worse than none at all it takes guts to acknowledge that perhaps this time right now you can t lead so get out of the way and take the follow instead the difference between things that happen to you and things you do in the old model things happened to you at work factories opened people were hired bosses gave instructions you got transferred there were layoffs you got promoted factories closed leaders on the other hand don t have things happen to them they do things in the middle of the mortgage crisis i spent some time with a few thousand realtors at their annual convention what i discovered might surprise you the group was completely split some of the realtors saw what the media bear stearns the banks and the public were doing to them and to their hard won careers they were angry even bitter about the end of a long run of increasing housing prices and they were scared about their futures these realtors didn t know how they were going to cope with what had happened they wanted to manage their careers but change was making it impossible the other realtors were palpably excited they were eager to get to work they saw the change in the outside world as an opportunity a chance for them to dramatically increase their business they knew that the current problems wouldn t last forever and they understood that the problems would wipe out the opportunity seekers leaving the professionals standing some or percent of the realtors were going to quit and the leaders the ones who were going to stay realized that this change was a very good thing the same way soldiers realize that it war that makes generals these brokers were ready and motivated to use change as a chance to really wreak some havoc on the status quo permeability perhaps you work for boeing or monsanto or some other corporate behemoth it more likely though that you work for a small organization perhaps as small as just a few people either way it worth taking a minute to remind yourself of how it used to be it used to be that executives had secretaries who had secretaries that you sent a memo to your boss and only your boss and then waited a week or a month for a response that you didn t share a new idea with a coworker the direction of information was preferably down or sometimes up then down but never sideways art kleiner deeply researched classic the age of heretics tells stories one after another of corporate heretics who ended up demoted fired disgraced and unhappy corporations might as well have been run by joseph stalin they had unalterable five year plans sharply controlled channels of communication and a royal court surrounding the monarch organizations used to be managed with no place for leaders no use for heretics growing up i used to visit my dad office i still remember the sign next to the corporate office men room no plant workers not only weren t the skilled and smart lathe operators allowed to use the men room in the adjoining office but they weren t often invited to share what they knew with their bosses either the system was rigid kodak for example literally kept its workers in the dark toiling in a pitch black factory as they made film while the process required darkness it didn t require rigid management or the hoarding of information and power that just came with the territory the problem with this approach is that it doesn t respond well to a changing world and it certainly doesn t do well when information comes in from many directions from many sources when everyone you worked with read the harvard business review and the same study from mckinsey it was easy top management now wants leaders it wants heretics who will create change before change happens to them top management understands that they need followers that they have to engage the tribe with change and remarkable initiative but the rank and file hesitates we hesitate because we ve seen what happened before we re afraid of failure of criticism of making a mistake and of getting caught we worry that we ll lose our jobs if we stop managing and start leading the age of leverage changes this but the fear remains the old stories of what happened to joe or bob or sue thirty years ago are told over and over we use them to stoke our fear to rationalize our desire to hide news flash the heretics not only live to tell about it now they actually thrive jerry shereshewsky was a heretic at young rubicam where his brash nature didn t sit well with the buttoned down culture of a advertising agency no worries jerry went on to make a name for himself at bmg and then with me at yoyodyne then at yahoo and now at a web start up called grandparents com quite a career if he had kept his mouth shut he d still be marketing coffeemakers leaders go first everyone will think it stupid everyone says it impossible guess what everyone works in the balloon factory and everyone is wrong the status quo is persistent and resistant it exists because everyone wants it to everyone believes that what they ve got is probably better than the risk and fear that come with change everyone in the developing world believes that things are going to be the way they were so when entrepreneurship and technology show up in a village in kenya everyone resists everyone at a fading record company believes that the only way to make a living is to own the income stream from selling cds or digital downloads so when new business models present themselves everyone ignores them or worse sues everyone at microsoft believed that the company was invincible and that the piddling search engines and internet companies in the valley couldn t possibly represent a threat steve ballmer ceo of microsoft said google not a real company it a house of cards he also said there can t be any more deep technology in facebook than what dozens of people could write in a couple of years that for sure over and over everyone is wrong unless you believe that innovation can change things that heretics can break the rules and that remarkable products and services spread if you believe that then you re not everyone then you re right watching the music business die it not as if they didn t see it coming it took almost a decade for this thriving hyperprofitable industry to cave in on itself the reasons are truly simple music industry executives didn t have the heretic they needed no one stood up and made change happen they forgot to embrace their tribe taking a look at the music business is a useful education for any heretic it demonstrates how exceedingly intelligent people in a fairly new industry willfully ignored the world around them and hid those lessons apply to just about every industry you can imagine the first rule the music business failed to understand is that at least at first the new thing is rarely as good as the old thing was if you need the alternative to be better than the status quo from the very start you ll never begin soon enough the new thing will be better than the old thing but if you wait until then it going to be too late feel free to wax nostalgic about the old thing but don t fool yourself into believing that it going to be here forever it won t the second rule they missed is that past performance is no guarantee of future success every single industry changes and eventually fades while you may have made money doing something a certain way yesterday there no reason to believe you ll succeed at it tomorrow the music business had a spectacular run alongside the baby boomers starting with the beatles and dylan music industry executives kept minting money the expanded purchasing power of teens combined with the birth of rock the invention of the transistor and changing social mores meant a long long growth curve as a result the music business built huge systems they created top heavy organizations dedicated superstores a loss leader touring industry extraordinarily high profit margins mtv and more it was a well greased system but the key question is why did it deserve to last forever it didn t yours doesn t either the music business was built around five pillars free radio promotion a limited number of competing music labels the high cost of production requiring musicians to get financing from labels the top hits based focus of the baby boomer generation a high margin nonreproducible medium the lp notice that none of these five pillars has anything to do with tribes or leadership one by one each of these five pillars has crumbled over the past five years the result is that while there is still plenty of music the music business is in trouble the innovation use digital distribution and the internet like radio but do it better be in the services souvenir business instead of suing customers and yearning for the old days find thousands of tribes for thousands of musicians and lead them where they want to go the best time to change your business model is while you still have momentum it not so easy for an unknown artist to start from scratch and build a career self publishing not so easy for her to find fans one at a time and build an audience very very easy for a record label or a top artist to do so so the time to jump was yesterday too late okay how about today the sooner you do it the more assets and momentum you have to put to work don t panic when the new business model isn t as clean as the old one it not easy to give up the idea of manufacturing cds with a percent gross margin and switch to a blended model of concerts and souvenirs of communities and greeting cards and special events and what feels like gimmicks get over it it the only option if you want to stay in this business you re not going to sell a lot of cds in five years are you if there a business here the first few in will find it the rest will lose everything the industry willfully failed to read the writing on the wall industries don t die by surprise it not as if you didn t know it was coming it not as if you didn t know whom to call or hire what was missing was leadership an individual a heretic ready to describe the future and build the coalitions necessary to get there this isn t about having a great idea it almost never is the great ideas are out there for free on your neighborhood blog nope this is about taking initiative and making things happen the last person to leave the current record business won t be the smartest and he won t be the most successful either getting out first and staking out the new territory almost always pays off i know it hard to believe but the good old days are yet to happen even for the music business the thing is the guys who ran it in the old days won t be around when it regroups because they won t be welcome sheepwalking i define sheepwalking as the outcome of hiring people who have been raised to be obedient and giving them brain dead jobs and enough fear to keep them in line you ve probably encountered someone who is sheepwalking the tsa screener who forces a mom to drink from a bottle of breast milk because any other action is not in the manual a customer service representative who will happily read aloud a company policy six or seven times but never stop to consider what the policy means a marketing executive who buys millions of dollars worth of tv time even though she knows it not working she does it because her boss told her to it ironic but not surprising that in our age of increased reliance on new ideas rapid change and innovation sheepwalking is actually on the rise that because we can no longer rely on machines to do the brain dead stuff we ve mechanized what we could mechanize what left is to cost reduce the manual labor that must be done by a human so we write manuals and race to the bottom in our search for the cheapest possible labor and it not surprising that when we go to hire that labor we search for people who have already been trained to be sheeplike training a student to be a sheep is a lot easier than the alternative teaching to the test ensuring compliant behavior and using fear as a motivator are the easiest and fastest ways to get a kid through school so why does it surprise us that we graduate so many sheep and graduate school because the stakes are higher opportunity cost tuition and the job market students fall back on what they ve been taught to be sheep well educated sheep of course but compliant nonetheless and many organizations go out of their way to hire people who color inside the lines who demonstrate consistency and compliance and then these organizations give these people jobs where they are managed via fear which leads to sheepwalking i might get fired the fault doesn t lie with the employee at least not at first and of course the pain is often borne by both the employee and the customer is it less efficient to pursue the alternative what happens when you build an organization that flat and open and treats employees with respect what happens when you expect a lot and trust the people you work with at first it seems crazy there too much overhead too little predictability and way too much noise this isn t the top down model of the factory or the king and his court it chaos it easy to reject out of hand then over and over we see something happen when you hire amazing people and give them freedom they do amazing stuff and the sheepwalkers and their bosses watch and shake their heads certain that this is an exception and that it is way too risky for their industry or their customer base i was at a google conference last month and i spent some time in a room filled with pretty newly minted google sales reps i talked to a few of them for a while about the state of the industry and it broke my heart to discover that they were sheepwalking consider the receptionist at a publishing company i visited a week later there she was doing nothing sitting at a desk minding her own business bored out of her skull she acknowledged that the front office is very slow and that she just sits there reading romance novels and waiting and she been doing it for two years or consider the mba student i met yesterday who is taking a job at a major packaged goods company because they offered her a great salary and promised her a well known brand she going to stay for just ten years then have a baby and leave and start my own gig she ll get really good at running coupons in the sunday paper but not particularly good at solving new problems what a waste step one is to give the problem a name sheepwalking done step two is for those of you who see yourself in this mirror to realize that you can always stop you can always claim the career you deserve merely by refusing to walk down the same path as everyone else just because everyone else is already doing it the biggest step though comes from anyone who teaches or hires and that to embrace nonsheep behavior to reward it and cherish it as we ve seen just about everywhere there been growth lately is where the good stuff happens i just reread these paragraphs and i m betting some people will think i m being way too harsh that depends it depends on whether you believe that people have a considerable amount of innate potential that work is too time consuming to be dull and that organizations need passion from employees and from customers if they want to grow into tribes and movements it depends on whether you believe that the relationship between marketers and the people they touch is important enough to invest in i think if you believe all that if you believe in yourself and your coworkers then this isn t nearly harsh enough we need to hurry we need to wake up how was your day it four a m and i can t sleep so i m sitting in the lobby of a hotel in jamaica checking my e mail a couple walks by obviously on their way to bed having pushed the idea of vacation a little too hard the woman looks over to me and in a harsh whisper a little quieter than a yell says to her friend isn t that sad that guy comes here on vacation and he stuck checking his e mail he can t even enjoy his two weeks off i think the real question the one they probably wouldn t want to answer was isn t it sad that we have a job where we spend two weeks avoiding the stuff we have to do fifty weeks a year it took me a long time to figure out why i was so happy to be checking my e mail in the middle of the night it had to do with passion other than sleeping there was nothing i d rather have been doing in that moment because i m lucky enough to have a job where i get to make change happen even though i don t have many people working for me i m in the business of leading people taking them somewhere we want to go on the other hand most people have jobs for now where they fight change where they work overtime to defend the status quo it exhausting maintaining a system in the face of change will grind you down think for a second about the people you know who are engaged satisfied eager to get to work most of them i ll bet make change they challenge the status quo and push something forward something they believe in they lead life too short is repeated often enough to be a clich but this time it true you don t have enough time to be both unhappy and mediocre it not just pointless it painful instead of wondering when your next vacation is maybe you ought to set up a life you don t need to escape from the amazing thing is that not only is this sort of life easier to set up than ever before but it also more likely to make you successful and happy so how was your day the thermometer and the thermostat a thermostat is far more valuable than a thermometer the thermometer reveals that something is broken the thermometer is an indicator our canary in the coal mine thermometers tell us when we re spending too much or gaining market share or not answering the phone quickly enough organizations are filled with human thermometers they can criticize or point out or just whine the thermostat on the other hand manages to change the environment in sync with the outside world every organization needs at least one thermostat these are leaders who can create change in response to the outside world and do it consistently over time your micromovement this is the heart of the matter every leader cares for and supports a movement a movement like the free speech movement at berkeley or the democracy movement in tiananmen square or the civil rights movement in mississippi or maybe a movement like the obsession with hand roasted coffee in brooklyn or the worldwide collection of people obsessed with tattoos today you can have a narrow movement a tiny movement a movement in a silo your movement can be known by ten or twenty or a thousand people people in your community or people around the world and most often it can be the people you work with or for or those who work for you the web connects people that what it does and movements take connected people and make change what marketers and organizers and people who care are discovering is that they can ignite a micromovement and then be propelled by the people who choose to follow it the key elements in creating a micromovement consist of five things to do and six principles publish a manifesto give it away and make it easy for the manifesto to spread far and wide it doesn t have to be printed or even written but it a mantra and a motto and a way of looking at the world it unites your tribe members and gives them a structure make it easy for your followers to connect with you it could be as simple as visiting you or e mailing you or watching you on television or it could be as rich and complex as interacting with you on facebook or joining your social network on ning make it easy for your followers to connect with one another there that little nod that one restaurant regular gives to another recognized regular or the shared drink in an airport lounge even better is the camaraderie developed by volunteers on a political campaign or insiders involved in a new product launch great leaders figure out how to make these interactions happen realize that money is not the point of a movement money exists merely to enable it the moment you try to cash out is the moment you stunt the growth of your movement track your progress do it publicly and create pathways for your followers to contribute to that progress principles transparency really is your only option every failed televangelist has learned this the hard way the people who follow you aren t stupid you might go down in scandal or more likely from ennui people can smell subterfuge from a mile away your movement needs to be bigger than you an author and his book for example don t constitute a movement changing the way people apply to college does movements that grow thrive every day they get better and more powerful you ll get there soon enough don t mortgage today just because you re in a hurry movements are made most clear when compared to the status quo or to movements that work to push the other direction movements do less well when compared to other movements with similar goals instead of beating them join them exclude outsiders exclusion is an extremely powerful force for loyalty and attention who isn t part of your movement matters almost as much as who is tearing others down is never as helpful to a movement as building your followers up that building down the street i think it is a boat club but perhaps it a political party or even a corporate headquarters it might even be a franchise business or a local nonprofit all i know is that there a tribe working overtime to maintain the status quo the congregation shows up every week and does the same ritual it did last week goes through the same motions and nothing changes in fact nothing changes precisely because of the ritual the tribe exists apparently to stamp out change the customer service staff shows up and follows the handbook and treats every customer exactly the same and can t figure out why they re being disrespected in return the volunteers go through the motions of supporting the nonprofit but they re the same motions they ve always gone through and they re getting the same results they ve always gotten some tribes are engaged in change many are not and it doesn t matter whether it a church or a corporation the symptoms are the same the religion gets in the way of the faith static gets in the way of motion rules get in the way of principle people show up because they have to not because they want to desire is defeated by fear and the status quo calcifies leading to the long slow death of the stalled organization it so sad to watch and it so common leadership is the antidote and it works in every building if you let it every tribe is a media channel time magazine is a media channel so are cnn and yahoo the advantage of traditional media channels is that they re available for rent send in some money and buy some time the time gets you eyeballs or possibly even attention and that attention might lead to sales google realized that every search more than a billion a day is a media channel as well and they ve profited by selling those channels one click at a time tribes are different tribes are the most effective media channels ever but they re not for sale or for rent tribes don t do what you want they do what they want which is why joining and leading a tribe is such a powerful marketing investment how to be wrong john zogby the successful pollster was completely utterly wrong about al gore in florida by ten points and he was wrong about john kerry and wrong about his prediction for the new hampshire primaries in but notice that i said successful pollster not disgraced pollster if he wasn t willing to be wrong he d be unable to be right as often as he is isaac newton was totally fantastically wrong about alchemy the branch of science he spent most of his career on he was as wrong as a scientist could be and yet he widely regarded as the most successful scientist and mathematician ever steve jobs was wrong about the apple iii wrong about the next computer wrong about the newton insanely wrong you know the rest the secret of being wrong isn t to avoid being wrong the secret is being willing to be wrong the secret is realizing that wrong isn t fatal the only thing that makes people and organizations great is their willingness to be not great along the way the desire to fail on the way to reaching a bigger goal is the untold secret of success i ve been waiting for you to ask for the shortcut the error free failure free way to get people to do what you want to make change happen without risk or fear to magically alter the status quo that after all is the best way to sell you on the ideas here if i could just give you the answer you d be leading a tribe right now the honest answer is there isn t an easy way it isn t easy for middle managers or ceos or heretics the truth is that they appear to risk everything but in fact the risk isn t so bad the downsides are pretty small because few of us are likely to get burned at the stake the secret of leadership is simple do what you believe in paint a picture of the future go there people will follow the timing of leadership it rare that it obvious when to lead sure there are times when you know you need to stand up take a position spread an idea clear out an obstacle and be brave but more often than not great leadership happens when the tribe least expects it the nonobvious moments are the ones that count like now perhaps the reactionary tribe so far we ve been talking about tribes as leadership loving fast moving progressive organizations that thrive on change and most tribes especially as they grow are just that but sooner or later tribes get stuck let look at wikipedia again wikipedia is a nonprofit that is run by a conservative board and several thousand dedicated volunteers and most of them don t want anything at all to change in recent months wikipedia volunteers have gone on a campaign to delete tens of thousands of pages that don t meet the tribe vague standards at the same time florence nibart devouard the chairwoman of the wikipedia board is actively campaigning to ensure that no one makes particularly large donations to the foundation she was quoted by the new york times as saying that she would make some noise if an aggressive outsider was to try to become a board member what to do with a tribe like this if your goal is to make change it foolish to try to change the worldview of the majority if the majority is focused on maintaining the status quo the opportunity is to carve out a new tribe to find the rabble rousers and change lovers who are seeking new leadership and run with them instead yes i think it okay to abandon the big established stuck tribe it okay to say to them you re not going where i need to go and there no way i m going to persuade all of you to follow me so rather than standing here watching the opportunities fade away i m heading off i m betting some of you the best of you will follow me possibility of risk i was listening to a talking head on the radio and he was prattling on about a probability of risk related to some course of action in the future people are so afraid of risk they can t even use the word risk after all is a probability of failure right so this guy was warning us of a probability of a probability he couldn t even say it it all a risk always that not true actually the only exception it a certainty that there risk the safer you play your plans for the future the riskier it actually is that because the world is certainly definitely and more than possibly changing when tribes replace what you re used to the brilliant venture capitalist fred wilson got me thinking about what purpose a traditional firm corporation nonprofit church whatever serves he quotes ronald coase the nobel laureate in economics there are a number of transaction costs to using the market the cost of obtaining a good or service via the market is actually more than just the price of the good other costs including search and information costs bargaining costs keeping trade secrets and policing and enforcement costs can all potentially add to the cost of procuring something with a firm this suggests that firms will arise when they can arrange to produce what they need internally and somehow avoid these costs in other words we start formal organizations when it cheaper than leading a tribe instead having employees for example gives you a tight interaction of communication and output that used to be difficult to accomplish from a less formal tribe having soldiers for example is seen as more reliable than earning the trust and support of the entire population the internet changes this because you can build a bigger faster cheaper tribe than you used to be able to the new economy changes this because the transaction costs are falling fast while the costs of formal organizations offices benefits management keep increasing many big organizations are getting bigger as a way of fighting off the power of tribes they buy other companies hoping that the formal nature of their bigness will somehow successfully fight off the flexible fast and sometimes free power of the tribe i think that unlikely initiative the timid leave a vacuum workers in the balloon factory are always afraid particularly of something happening things that happen are rarely good because they disturb the status quo that why initiative is such an astonishingly successful tool because it rare even a little bit of action a few new ideas or a tiny bit of initiative can fill the vacuum it a big deal to spill just a few drops of hawaiian punch on a spotless white tablecloth people notice when barbara barry the now famous furniture designer was looking for a manufacturing partner for her first line of sofas she invited executives from a leading manufacturer to her showroom in los angeles before she did that though she took some initiative first she managed to place a wholesale order for reams of fabric that the manufacturer traditionally used on its own furniture she rented an office big enough to turn into a showroom she designed a line of furniture that was bold and even breathtaking and then she had a local shop build one of each piece upholstered in the company signature fabric when the executives arrived expecting a sales pitch and some drawings they saw finished sofas made from their materials with their brand label sewn on after the fact it easy to say that it wasn t much a few thousand dollars worth of custom furniture but in that moment for that industry it was more than enough it changed the rules barbara wasn t managing her career or asking permission from the furniture executives she was leading and enjoying every moment of it the organizations that need innovation the most are the ones that do the most to stop it from happening it a bit of a paradox but once you see it it a tremendous opportunity stuck on stupid my colleague gil likes to quote u s army lieutenant general russel honor pointing out that too many people get stuck on stupid i m imagining that your colleagues aren t stupid but when the world changes the rules change and if you insist on playing today games by yesterday rules you re stuck stuck with a stupid strategy because the world changed some organizations are stuck others move quickly in a changing world who having more fun mark rovner nonprofit heretic mark has been challenging the status quo of the nonprofit world for years he very successful at it and he having a ball here one example of the sort of trouble that leaders need to cause mark started an online debate about the future of direct mail fund raising this income stream is the lifeblood of most nonprofits and it drying up the internet of course is supposed to be the solution to all problems but as mark points out it not the era of cheap direct mail and high response rates in acquisitions is over the economics of direct mail are failing that is more or less an uncontroverted fact it costs more to mail and fewer new donors come back with each mailing this trend has been masked somewhat by higher average gifts by donors you already have but sooner or later the acquisition crisis is going to affect bottom lines for some it already has what currently passes for an online fundraising model is at best a stopgap my take i despair for most of the top fifty nonprofits in the united states these are the big guys and they re stuck far more than the fortune not known for being cutting edge in themselves the top charities rarely change if you re big you re used to being big and you expect to stay big that means that generation after generation of staff has been hired to keep doing what working big risks and crazy schemes are certainly frowned upon the good news is this the internet is not a replacement for direct mail fund raising it is in fact something much bigger than that for just about every nonprofit as soon as commerce started online many nonprofits generated lots of income from their web sites this was mistakenly chalked up to brilliant conversion and smart marketing in fact it was the result of technologically advanced donors using a more convenient method to send in money they would have sent in anyway the big win is in changing the very nature of what it means to support a charity the idea of i gave at the office and of giving money in the last week in december speaks to obligation many people donate to satisfy a guilty feeling or to please a friend this doesn t scale not one bit it super easy to ignore a direct mail solicitation when all you have to do is hit delete and no one notices the big win is in turning donors into patrons and activists and participants the biggest donors are the ones who not only give but also do the work the ones who make the soup or feed the hungry or hang the art my mom was a volunteer for years at the albright knox art gallery in buffalo new york and there no doubt at all that we gave more money to the museum than we would have if they d sent us a flyer once a month the internet allows some organizations to embrace long distance involvement it lets charities flip the funnel not through some simple hand waving but by reorganizing around the idea of engagement online this is the new leverage it means opening yourself up to volunteers and encouraging them to network to connect with one another and yes even to mutiny it means giving every one of your professionals a blog and the freedom to use it it means mixing it up with volunteers so they have something truly at stake this is understandably scary for many nonprofits but i m not so sure you have a choice do you have to abandon the old ways today of course not but responsible stewardship requires that you find and empower the heretics and give them the flexibility to build something new instead of trying to force the internet to act like direct mail with free stamps the posture of a leader if you hear my idea but don t believe it that not your fault it mine if you see my new product but don t buy it that my failure not yours if you attend my presentation and you re bored that my fault too if i fail to persuade you to implement a policy that supports my tribe that due to my lack of passion or skill not your shortsightedness if you are a student in my class and you don t learn what i m teaching i ve let you down it really easy to insist that people read the manual it really easy to blame the user student prospect customer for not trying hard for being too stupid to get it or for not caring enough to pay attention it might even be tempting to blame those in your tribe who aren t working as hard at following as you are at leading but none of this is helpful what helpful is to realize that you have a choice when you communicate you can design your products to be easy to use you can write so your audience hears you you can present in a place and in a way that guarantees that the people you want to listen will hear you most of all you get to choose who will understand and who won t this paper models the messages embedded by spatial least significant bit lsb matching as independent noises to the cover image and reveals that the histogram of the differences between pixel gray values is smoothed by the stego bits despite a large distance between the pixels using the characteristic function of difference histogram dhcf we prove that the center of mass of dhcf dhcf com decreases after messages are embedded accordingly the dhcf coms are calculated as distinguishing features from the pixel pairs with different distances the features are calibrated with an image generated by average operation and then used to train a support vector machine svm classifier the experimental results prove that the features extracted from the differences between nonadjacent pixels can help to tackle lsb matching as well keywords steganalysis lsb matching difference histogram characteristic function support vector machine z xia x wang x sun jiangsu engineering center of network monitoring nanjing university of information science technology nanjing china e mail com x wang e mail com x sun e mail sunnudt com z xia x wang x sun school of computer software nanjing university of information science technology nanjing china q liu lmba campus de tohannic universit de bretagne sud f vannes france e mail quansheng liu univ ubs fr n xiong school of computer science colorado technical university colorado spring co usa e mail xiongnaixue gmail com multimed tools appl 1962 introduction nowadays lots of information is transmitted through the public communication channel which incurs many security problems encryption is the general method to protect the information as a complement to encryption steganography tries to transmit secret messages by hiding them in cover objects generally steganography and cryptography could be combined together in a way that the message may be encrypted first and then embedded in a cover to provide additional security specifically a general model of steganography is shown in fig lsb matching is such a steganography method that embeds messages by adding or to the pixel values of the cover image the messages embedded by lsb matching can be modeled as the additive noises and introduce two types of changes on the cover image firstly the histogram of an image is smoothed secondly the correlation among neighboring pixels is disturbed many existing steganalysis methods are proposed by using these two types of changes for instance methods are proposed based on the fact that lsb matching can smooth the image histogram and methods extract features by examining the correlation among neighboring pixels this paper proposes a steganalysis method in a novel way we reveal that the histogram of the differences between pixel gray values will be smoothed by the stego bits introduced by lsb matching although the distance between the pixels is large we extract features from the differences between nonadjacent pixels which are used to train a support vector machine svm classifier in the rest of the paper section presents some related works section describes the feature extraction experiments are presented in section and conclusions are drawn in section related works in this paper the existing methods are divided into two classes histogram feature based and correlation feature based methods the histogram feature based methods are some early ones harmsen and pearlman considered message embedding as the disturbance of additive noise and calculated the center of mass of histogram characteristic function hcf com to detect the hidden message ker improved this method by calculating features from the adjacency histogram instead of the usual histogram and significant improvement have been achieved in order to discover the hidden message in images key encryption algorithm encrypted message key secret message embedding algorithm decryption algorithm extraction algorithm stego object stego object cover object alice secret message communication channel steganalysis algorithm warden encrypted message bob fig secret messages are transmitted through public communication channel by using steganography method multimed tools appl 1962 with large noise component zhang et al extracted features by observing the change of local extremum of the image histogram cancelli et al improved this method by extracting more relevant features another type of methods extracts features by examining the correlation among neighboring pixels these methods usually extract multiple correlation features and learn the differences between cover and stego images in a multidimensional feature space to train a classifier the trained classifier is then utilized to disclose the existence of the concealed messages these methods can be also named the learning based method first of all avcibas et al proposed a framework for learning based steganalysis see fig this work inspired many works based on various kinds of features liu et al measured the correlation in the least and second least significant bit plane of the image as features lyu and farid decomposed the image with wavelet transformation and calculated high order magnitude and phase statistics as features goljan et al extracted higher order statistic features from the wavelet coefficients cai et al renormalized the difference histogram with its center elements and used the difference histogram elements as features xiong et al decomposed images with local linear transform llt and extracted features with the llt coefficients zheng et al revealed that the noise variance of stego image is the sum of its corresponding cover image and the stego noise and proved the local variance histogram lvh would stretch rightwards after data embedding then the authors measured the right stretching effect by the center of mass com of the lvh and set it as the feature guo et al analyzed the correlation between the image pixels by co occurrence matrix and constructed sum features of average cooccurrence matrix zhao and liu first calculated the second order difference of the image pixels in the horizontal and vertical directions obtaining the difference matrix next the authors extracted the second order markov transformation matrix from the difference matrix as the features pevny et al revealed that the distribution of differences between the adjacent pixels could be used to indicate the image correlation and used markov model to extract features from the differences this method obtained very good detection accuracy zhang at el improved pevny et al method by extracting features from parallel subtractive pixels and grayscale inverted image afterwards pevny et al further expanded the features by using high order difference and markov chain and utilized ensemble classifier to deal with the high dimensional feature vector original image set stego image set feature extraction feature vector set of original images feature vector set of stego images testing image feature extraction feature vector of the image classifier train the classifier classification algorithm stego or original image fig the framework of training and testing process of learning based steganalysis multimed tools appl 1962 feature extraction in this paper steganalysis is considered as a problem of two class classification i e classifying the test image into either the original image class or stego image class as illustrated in fig feature extraction is the crucial step for classification problem most of the existing steganalysis methods extracted features based on the two types of changes caused by lsb matching embedding for instance some methods extracted features based on the fact that lsb matching will smooth the image histogram and some others extracted features by observing the change of dependency among neighboring pixels this paper proposes a steganalysis method in a novel way we reveal that the histogram of the differences between pixel values will be smoothed by the stego noises introduced by lsb matching even though the distance between the pixels is large thus we try to extract features by using the differences between nonadjacent pixels embedding strategy of lsb matching during the embedding process of lsb matching if the lsb of the image pixel matches the secret data bit this pixel will be kept intact otherwise the pixel will be altered by adding at random according to formula the embedding process of lsb matching conducts low pass filtering to the histogram of differences using the filter f namely the histogram of the differences is smoothed by lsb matching with small x and y the difference histogram hd of an image can be approximated by gaussian distribution due to the strong dependency among adjacent pixels the smoothness caused by lsb matching is apparent especially at the peak of the distribution see fig a this change at the peak is widely used by previous steganalysis methods to disclose the hidden messages according to formula the histogram of differences between nonadjacent pixels is also smoothed by lsb matching with large x and y the difference histogram hd cannot be approximated by uniform distribution functions in this case the smoothness caused by lsb matching occurs throughout all the elements of hd although this type of smoothness is not as apparent as that at the peak of gaussian distribution see fig c this change is usually ignored by steganalysis researchers our work tries to use this type of change to detect hidden messages characteristic function of the difference histogram in this paper the smoothness of the difference histogram is measured in frequency domain for a sequence g g n n n its discrete fourier transform dft is written as natural images are highly diverse so are the features extracted from images thus the feature distortion caused by stego noises is likely to be obscured by the variety of features to deal with this problem we use a calibration mechanism to restrain the impact caused by the image content for an image i we generate a calibration image i with an average operation defined as i x x i y jn xj y n x where n x is the deleted neighborhood of x and jn xj denotes its cardinality we denote the dhcf com calculated from image i as f i and calculate the features from both i and i then the final feature is calculated as f i  f i  f i  f i  feature extraction procedure in total the feature extraction process includes five steps which are illustrated in fig first the difference matrix is generated from image with the difference parameter x y secondly the normalized histogram of the difference matrix is calculated thirdly the difference histogram characteristic function dhcf is calculated fourthly the center of mass of the characteristic function dhcf com is computed as the feature finally the feature is calibrated note that a set of features will be generated with different parameter x y experiments the proposed method is tested with two image sets imgboss including images provided in boss nrcs including images which are generated from color images the performance of method is compared with the three previous ones namely hcf com ale and spam among the three methods hcf fig the procedure of feature extraction multimed tools appl 1962 com is a special detection method which extracts only one feature ale and spam are learning based methods ale extracted features by observing the changes of image histogram while spam extracted features by measuring the dependency between the adjacent pixels an appropriate size of the neighborhood n x needs to be set so as to construct a good calibration mechanism experimentally we choose a deleted neighborhood n x with the size of in the experiment the calibrated dhcf coms calculated with different distance parameters x y are firstly tested individually secondly a set of features are extracted with the distance parameter x y with the exception of this set of features is named dhcf coms thirdly a set of features are extracted with the distance parameter x y which is named dhcf coms we roughly consider the features in dhcf coms are the ones extracted from adjacent pixel pairs and consider the features in dhcf coms are the ones extracted from nonadjacent pixel pairs the features in dhcf coms and dhcf coms are used to construct feature vectors respectively the feature vectors are used to train svm classifiers whose performances are compared with each other and the previous methods support vector machine svm svm is utilized to train the classifier with features in this paper svm considers the data as two sets of points in a multidimensional space and builds a separating hyperplane by lagrangian multipliers to differentiate the negative data points from the positive ones intuitively a good separation is achieved when a separating hyperplane has the largest distance to the boundary points of both classes in fig we present two basic kinds of svm i e a linear separable and b linear non separable linear separable svm given a training set of instance label pairs xi yi i l where xi rn yi the linear separable svm classifier is a linear hyperplane hp that separates positive and negative points as it illustrated in fig the hyperplane is formulated as follows wt x  b a fig three kinds of svm a linear separable and b linear non separable b multimed tools appl 1962 in fig b w is the perpendicular distance from the origin to the hyperplane and denotes the euclidean norm the separating hyperplane is chosen so as to maximize the margin w therefore it yields an optimization problem min kwk t wt xi  b if yi wt xi  b if yi linear non separable svm it is possible that the training data do not uniformly lie on either side of a separating hyperplane as it illustrated in fig this situation can be handled by modifying the initial constraints in formula with slack factor i i l as follows if yi wt xi  b i wt xi  b  i if yi l in this case the objective of the optimization problem is to minimize wt w  c i where c is a parameter to be chosen by the user a larger c corresponds to a higher penalty to classification errors the solution of this optimization problem is similar to that described in previous subsection libsvm is a free software about support vector classification libsvm implements four basic kernels and provides a tool named cross validation and grid search to search the appropriate penalty parameters in our experiment libsvm is directly utilized to train classifies with the proposed features training and testing image sets each image set above is divided into two parts the training set and testing set to respectively train and test the svm classifiers for nrcs the training image set consists of cover images and corresponding stego images among the stego images the images of four embedding rates i e and bit per pixel bpp are equally included the test image set includes cover images and stego images with the four embedding rate i e stego images similarly for imgboss the training image set consists of cover images and corresponding stego images the test image set is made up of images parameters of svm the libsvm with linear kernel is used to train classifiers in the experiments the tool cross validation and grid search is utilized to search the penalty parameter c for the two image sets imgboss and nrcs the penalty parameter c used in ale spam and our method are listed in table table penalty parameter c used in the training phase image set methods ale spam dhcf coms dhcf coms spam and dhcf coms nrcs imgboss multimed tools appl 1962 detection results in this paper the detection performance of steganalysis method is evaluated by the minimized classification error defined as pe min pfa  pm d pfa  pfa where pfa and pmd are respectively the probabilities of false alarm and missed detection detection performances of the individual features are illustrated in fig first the results show that the features extracted with larger y can attack lsb matching successfully to some extent although the performance is a little worse than that with smaller y secondly the messages hidden in nrcs images with lsb matching are harder to detect than that in imgboss because nrcs images contain more noises than imgboss ones usually the features based on image correlation are not good at detecting hidden messages in noisy images thirdly hugo preserves the joint statistic of differences among four neighboring pixels in four directions our method employs the differences of pixel pairs with larger distance and is thus expected to obtain a good result however the experimental result is not quite satisfying by examining hugo carefully we found that hugo alters the image pixels according to a criterion minimizing the distortion of a model not randomly therefore the stego noises introduced by hugo are not all the same as lsb matching the feature set dhcf coms and are used to train the svm classifier we roughly consider that the features in dhcf coms are the ones extracted from adjacent pixel pairs and the features in dhcf coms are the ones extracted from nonadjacent pixel pairs the results in table show that dhcf coms achieves better detection accuracy than dhcf coms in the detection of nrcs image set while achieves a little worse detection accuracy than dhcf coms in the detection of imgboss image set and the features in dhcf coms outperform hcf com and perform better than ale in the detection of imgboss we combine the features in dhcf coms with that used in spam since spam exploits the dependency between adjacent pixels adequately and in contrast the features in dhcf coms mainly utilize change of differences between nonadjacent pixels the combination is to arrange the features in dhcf coms and the features used in spam as a feature vector then the samples of feature vector will be calculated from each training image finally these samples are used to train classifier the experiment results show that the features extracted from nonadjacent pixel pairs can help to detect hidden message embedded in nrcs images which are considered to have a large noise component generally it is more difficult for the conclusions and future works for the first time this paper presents a steganalysis method to attack lsb matching by using the differences between nonadjacent pixels we theoretically proved that the histogram of the differences between pixels is smoothed by lsb matching even though the pixels have a large distance with each other we calculate the center of mass of difference histogram characteristic function as features which are then calibrated with an image generated by average operation finally the calibrated features are used to train a support vector machine classifier so as to detect the existence of hidden messages the experimental results show that the features based on the differences between nonadjacent pixels can successfully attack lsb matching to some extent as shown in our experiments the improvement achieved by the features extracted from nonadjacent pixel pairs is not remarkable in the future the differences of pixels with multispan in multi directions will be further studied with other statistical models acknowledgments this work is supported by the nsfc 61173142 61373132 2013dfg12860 open fund of jiangsu engineering center of network monitoring and papd fund this paper presents a comparative study of color and texture descriptors considering the web as the environment of use we take into account the diversity and large scale aspects of the web considering a large number of descriptors color and texture descriptors including both traditional and recently proposed ones the evaluation is made on two levels a theoretical analysis in terms of algorithms complexities and an experimental comparison considering efciency and effectiveness aspects the experimental comparison contrasts the performances of the descriptors in small scale datasets and in a large heterogeneous database containing more than thousand images although there is a signicant correlation between descriptors performances in the two settings there are notable deviations which must be taken into account when selecting the descriptors for large scale tasks an analysis of the correlation is provided for the best descriptors which hints at the best opportunities of their use in combination  elsevier inc all rights reserved introduction this paper presents a comparative study of global color and texture descriptors considering the web as the environment of use in the latest years the amount of digital images has grown rapidly among the main reasons for that one may mention digital cameras and high speed internet connections those elements have created a simple way to generate and publish visual content worldwide that means that a huge amount of visual information becomes available everyday to a growing number of users much of that visual information is available on the web which has become the largest and most heterogeneous image database so far in that scenario there is a crucial demand for image retrieval systems which could be satised by content based image retrieval cbir systems in cbir systems the image descriptor is a very important element it is responsible for assessing the similarities among images descriptors can be classied depending on the image property analyzed like for example color or texture descriptors that analyze color or texture properties respectively in cbir systems the searching process works as follows the user queries the system usually by using a query image its properties are encoded into feature vectors and then compared against corresponding author e mail addresses penatti ic unicamp br o a b penatti dovalle dca fee unicamp br e valle rtorres ic unicamp br ricardo da s torres see front matter  elsevier inc all rights reserved doi j jvcir the feature vectors of the database images that had been previously extracted the comparisons are made by computing distance values and those values are used to rank the database images according to their similarities to the query image the most similar images are nally shown to the user the image descriptor is involved in this process in the extraction of images properties and in the distances computations therefore it is clear the critical importance of image descriptors for cbir systems it is known that many image descriptors are application dependent that is their performances vary from one application to another therefore conducting comparative evaluation of image descriptors considering different environments of use is very important literature presents several comparative studies for color texture and shape descriptors a recent study compares a large number of image descriptors in ve different image collections for tasks of classication and image retrieval other studies are specic to certain properties shape descriptors texture descriptors or color descriptors surveys and comparisons markedly related to ours can be found in sections and in comparative studies of descriptors the web is rarely considered as the environment of use in general the amount of descriptors considered is small and the application analyzed is specic besides that asymptotic theoretical analysis and other efciency considerations are generally not discussed in detail our study has many novel aspects first it considers a large number of descriptors color and texture descriptors including both traditional and recently proposed ones our evaluation is o a b penatti et al j vis commun image r made in two levels a theoretical analysis of algorithms complexities and an experimental comparison the experimental comparison is made over specic and heterogeneous collections descriptors are analyzed in a web environment with a database containing more than thousand images with very heterogeneous content the experimental analysis considers efciency and effectiveness aspects the effectiveness evaluation in the web environment takes into account how much the descriptors agree with the human perception of semantic similarity by asking a pool of users to annotate the relevance of answers for each query another important aspect of our study is related to scalability and diversity how does a descriptor perform as the size of the collection increases considerably and how does the heterogeneity of the collection affect the descriptors effectiveness our experiments in the web environment address both issues the large scale heterogeneous nature of the web can benet from the use of descriptors in combination although the combination of descriptors is a complex topic with many competing techniques and thus outside the scope of this work we perform a general analysis of the complementarity of the best descriptors which should be taken into account when selecting them for combined use we concentrate our evaluation on global image descriptors since local descriptors have a radically different cost benet compromise especially in the context of information retrieval involving high level semantic contexts local image detectors and descriptors have been extensively surveyed in respectively we have decided not to include shape descriptors in our study because almost all of them are segmentation dependent as is known segmentation is still a hard and extremely application dependent task therefore shape descriptors are still not mature for a heterogeneous environment like the web readers interested in shape descriptors should refer to one of the comparative studies in specic environments another less common kind of descriptor called spatial relationship descriptor tries to encode the spatial relationships between objects however those descriptors also depend on the segmentation of images the paper is organized as follows section presents the image descriptor denition used throughout this work and taxonomies for color and texture descriptors section presents the results of theoretical analysis of the descriptors discussing the evaluation criteria used and the theoretical comparative tables section presents the experimental evaluation showing the experimental measures adopted the implementation details of each descriptor and the results achieved for the specic collections section presents the evaluation for the web environment in section we conclude the paper descriptors both the effectiveness and the efciency of content based image retrieval systems are very dependent on the image descriptors that are being used the image descriptor is responsible for characterizing the image properties and to compute their similarities in other words the image descriptor makes it possible to rank images according to their visual properties denition the image descriptor can be conceptually understood as responsible for quantifying how similar two images are formally an image descriptor d can be dened as a pair d dd where d is a feature extraction algorithm and dd is a function suitable to compare the feature vectors generated d encodes image visual properties into feature vectors as shown in fig a feature vector contains information related to the image visual properties like color texture shape and spatial relationship of objects dd compares two feature vectors as shown in fig given two feature vectors the function computes a distance or similarity value between these vectors the distance or similarity between the vectors is considered as the distance or similarity between the images from which the vectors were extracted it is worth noting that in some papers from literature what we call here as feature vector is considered the descriptor with the distance function being accounted elsewhere we adopt the denition of in which the descriptor also includes the distance function since feature vectors need a specic distance function to establish the geometry of the description space the vectors alone without the metric are meaningless that is better understood if we observe that the same type of feature vector may have radically different performances when used with different distance functions distance functions distance functions are very important for the image descriptors their choice has a huge impact in the descriptor performance the most common distance functions are the function also named manhattan or city block the function also known as euclidean distance and the function those common functions or variations of them are largely used there are also more complex functions like the earth mover distance emd color descriptors one of the most important visual properties identied by human vision is color making it one of the most used properties in cbir systems taxonomy for color descriptors literature presents three main approaches for color analysis as shown in fig the global approach considers the image color information globally as no partitioning or pre processing stage is applied to the image during features extraction descriptors from this approach usually have simple and fast algorithms for extracting feature vectors however as no information about the spatial distribution of colors is encoded those descriptors can have little discriminating power many descriptors from global approach generate histograms as feature vectors like for example the global color histogram and the cumulative global color histogram the xed size regions approach divides an image into cells of xed size and extracts color information from each cell separately the descriptors from this approach encode more spatial information at the cost of usually generating larger feature vectors examples of descriptors from xed size regions approach are local color histogram and cell color histogram the segmentation based approach divides an image in regions that may differ in size and quantity from one image to another this division is usually made by a segmentation or clustering algorithm what introduces an extra complexity to the features extraction process another kind of segmentation is the classication of pixels before feature extraction descriptors from that approach usually present better effectiveness although they are often more complex examples of descriptors from segmentation based approach are color based clustering and dominant color authors often do not give their methods neither a name nor an acronym to refer to those methods less awkwardly through the text we have taken the liberty of giving them a short descriptive o a b penatti et al j vis commun image r fig image descriptor components fig taxonomy for color descriptors name and acronym for methods already named in the original publication we have of course used their standard designation previous comparisons on color descriptors besides the studies mentioned in the introduction studies focused on the mpeg descriptors are especially related to ours due to the large number of descriptors involved on the standard a comparative study of the color descriptors from the mpeg standard shows that csd has the best performance while cld is pointed as the most sensitive to noise another comparative study of the mpeg color descriptors points that csd has the best effectiveness being better than scd cld and dcd in this order according to the study dcd although being the most computationally complex descriptor yields the worst results since it focuses on parts of images and not on images as a whole the use of color in local features is a subject in itself and outside the scope of this work in any case the use of local features often imply computational costs which limits their use in web like environments the interested reader is nevertheless encouraged to refer to the comprehensive study of van de sande et al texture descriptors texture is an important property for the characterization and recognition of images this fact is observed by the great amount of research involving texture analysis of images texture is easily recognized in images as can be shown by elements like sand leaves clouds bricks etc however it is difcult to provide a formal denition for it literature gives a variety of definitions as shown in its structure is simply attributed to the repetitive patterns in which elements or primitives are arranged according to a placement rule an image texture is described by the number and types of its primitives and the spatial organization or layout of its primitives according to texture can be described by spatial frequency or perceptual properties in a general way texture can be understood as a set of intensity variations that follow certain repetitive patterns fig taxonomy for texture descriptors differently from color texture is difcult to be analyzed considering the value of a single pixel since it occurs mainly due to the variation in a neighborhood of pixels that makes it possible to name some attributes for textures according to texture has attributes of roughness contrast directionality regularity coarseness and line likelihood the rst three ones being the most important although most texture descriptors work on gray scale only few of them specify how color images should be converted in order to optimize the descriptor performance taxonomy for texture descriptors there are several approaches for texture extraction the taxonomy presented in fig is the one found in one of the most traditional ways to analyze the spatial distribution of the gray levels of an image is by statistical analysis as for example by computing the probability of co occurrence of gray values in different distances and orientations the statistics can be computed over the values of single pixels rst order statistics or over the values of pairs of pixels second order statistics methods that characterize textures by means of histograms also have statistical information about texture one of the most popular statistical methods is the co occurrence matrix geometrical methods analyze textures by texture elements or primitives this analysis is made considering the geometrical properties of the primitives like size shape area and length having the primitives being identied in an image placement rules are extracted from them like grids or statistics from relative vectors that join the primitives centroids that kind of analysis becomes difcult for natural textures because the primitives and the placement rules can be irregular for example describing a wall of bricks by the primitive brick and a grid as placement rule can be simple however describing the clouds in the sky is much more difcult since the element cloud can have variable size and shape and the positioning of them is more complex model based methods rely on the construction of image models that can be used to describe and synthesize textures the parameters from the model capture the essential perceived qualities of o a b penatti et al j vis commun image r texture for example texture elements can be modeled as a dark or a bright spot an horizontal or vertical transition corners lines etc descriptors from that approach work well for regular textures the local binary pattern descriptor is an example of model based descriptor signal processing methods characterize textures by applying lters over the image both spatial domain and frequency domain lters can be used descriptors based on wavelets and gabor lters follow that approach like the homogeneous texture descriptor for instance we have addressed the issue of methods originally without a name or acronym in the same way as we did for the color descriptors section previous comparisons on texture descriptors literature has some comparative studies on texture descriptors a study compares a fourier based descriptor with a gabor based descriptor trying to identify the descriptor that is the most robust to noise the results show that the fourier based descriptor has better performance in images with no noise and that the gabor based descriptor is better for noisy images another study compares the texture descriptors of mpeg standard considering features extraction cost the study shows that tbd is the most expensive and that ehd is the cheapest one besides that the study points out that htd captures global information while tbd captures global and local information and ehd captures only local information the study also shows that tbd is not indicated for cbir tasks being more useful for image browsing and for defects detection additionally htd is suggested for being used in cbir and in texture segmentation tasks while ehd is recommended for cbir tasks the study also shows that htd and tbd are less sensitive to noise while ehd is not recommended in environments with noise the experiments realized by the study show that tbd has low effectiveness and that htd is not good for images with rotation that last problem of htd is noticed in the literature given the number of versions proposing htd with rotation invariance like the descriptors htdr htdi and han gabor present in our study theoretical comparison the comparative study performed in this paper comprises two analyses this section presents the rst one a theoretical evaluation conducted for color descriptors and for texture descriptors the theoretical evaluation considers the asymptotic complexities of the descriptors algorithms the next section presents the second analysis an experimental evaluation for the most promising color and texture descriptors in the following section color and texture descriptors are tested in a web like environment evaluation criteria based on the main elements related to the search process in a content based image retrieval system as explained in section the following criteria are used to compare the descriptors features extraction complexity distance function complexity storage requirements effectiveness and validation environment features extraction complexity the features extraction algorithm of a descriptor is used whenever the features from an image need to be extracted the extraction is required basically in two moments in a web cbir system the rst one is when the images are being collected from the web to be included in the local database which is usually an offline process the second moment is when a query is performed which is an on line process the on line part directly affects the response time of the system therefore it is important for the feature extraction algorithm to be fast our complexity analysis is informed in asymptotic big o notation which ignores additive and multiplicative constants as such it should be taken with a grain of salt since two o n algorithms though theoretically similar in behavior may vary considerably in practice because of the hidden constants it will be seen that almost all extractors are linear or at worse when they work on the frequency domain log linear in the number of image pixels there are notably exceptions though for example methods based on scale spaces must take into account the number of scales analyzed the rare methods which perform complex adaptive clustering are more expensive distance function complexity the distance function of a descriptor is very time consuming when a query is being processed in a cbir system distance functions need to be fast because during the search process the query image will be compared to a large number of candidate images the distance function is also important for indexing issues the use of indexing structures is important for systems with large databases and therefore critical in a web environment without indexing structures the response time would be unacceptable indexing however imposes restrictions on distance functions because simple axis monotone norm based functions are much friendlier to indexing structures than elaborated and unpredictable decision procedures the interaction between descriptors and indexing structures is however complex depending on the statistical characteristics of the feature vector the image dataset and the properties of the index employed prospective studies on that subject may be found in again in our theoretical analysis the complexity is given asymptotically it will be seen that almost all methods are based on simple distances manhattan euclidean etc which are linear in the feature vector size storage requirements the image descriptor stores into feature vectors the encoded image properties each image managed by a cbir system is associated with one or more feature vectors as a result the storage space required for feature vectors is proportional to the amount of images in the database in a web scenario in addition to the very large database size there is the issue of image heterogeneity making almost indispensable to employ several descriptor at once as a consequence the storage space required is also proportional to the array of descriptors employed seldom the feature vector size for a method is xed more usually it depends on a set of parameters which can be adapted from application to application for color descriptors for example many descriptors allow to customize how many colors are to be chosen on a quantization step methods based on frequency domain transforms wavelets fourier dct often have a choice of selecting how many transform coefcients are considered many methods can be seen as multidimensional histograms and as such grow as fast as the product of the number of bins in each dimension our analysis considers those mentioned possible variations in vectors sizes dimensionality reduction techniques may be employed in order to alleviate the storage requirements of the larger descriptors however often but not always they also impose a loss in effectiveness there are many dimensionality reduction techniques including the linear projection techniques like principal component analysis pca and linear discriminant analysis lda and metric embedding techniques both linear and non linear the interaction between dimensionality reduction and descriptor performance are however far from trivial and outside the scope of heterogeneous and heterogeneous from the web mean that a non standard heterogeneous collection was employed in the latter case the images were collected from the web b evaluation measures and descriptors names that appear in italic are not used in this study but their explanations can be found in the papers where they are mentioned o a b penatti et al j vis commun image r databases and the databases are usually small rarely having more than images one of the image databases commonly used is corel in whole or in parts but it is often used in non standard subsets table also shows that some descriptors are not compared with other descriptors the global color histogram gch comes closer to a standard method again which most other methods tend to compare but this is not an absolute rule some methods for example compare only with versions of themselves the most common evaluation measures are precision recall and anmrr there is seemingly little interest in measuring efciency retrieval and extraction times are rarely offered and no formal comparison of the complexities is ever considered analysis for texture descriptors table presents the results of the theoretical analysis of texture descriptors in terms of algorithms complexity for features extraction and distance computation table also shows the feature vectors sizes and presents the taxonomy class of each texture descriptor descriptors are sorted in reverse chronological order inside each taxonomy class from table it is noticed that several descriptors present features extraction complexity equal to o nlogn most of these more complex descriptors are based on image ltering algorithms using gabor lters in many cases considering the complexities of distance functions it is predominant the use of linear distance functions which shows that the most laborious step related to texture description is the features extraction phase from the analysis of feature vectors sizes we highlight the compact feature vectors generated by the descriptors tbdi tbd ehd and lbp table shows the validation environments of each texture descriptor considering the databases used for validation there is a predominance in the use of brodatz database which indicates a certain standardization in the validation of texture descriptors however some works use only part of the brodatz database instead of using the complete collection it is also noteworthy that because of the nature of the brodatz textures many articles use classication experimental designs instead of retrieval ones few descriptors are validated in heterogeneous databases showing the lack of validation of texture descriptors for general image retrieval tasks considering the descriptors used as baselines for comparisons there is a clear predominance in the use of descriptors based on gabor lters there are also descriptors that are compared with no other descriptor or only compared with variations of themselves the evaluation measures most used are related to average retrieval rate there are some works worried about extraction and retrieval times but they are still rare experimental comparison in small datasets given the large amount of descriptors selected for theoretical evaluation we have to choose the most promising descriptors for the experimental comparison in order to contrast with the largescale web like environment experiments were performed aiming to evaluate the descriptors performance in a controlled small scale scenario all those experiments were performed in the eva tool which creates a standardized environment for the evaluation of the image descriptors details of the eva tool can be found in http www ic unicamp br penatti eva as of september selected descriptors in this section we present the descriptors selected for the experimental evaluation justifying each choice the implementation details are shown in sections and for color and texture descriptors respectively color descriptors global color histogram descriptor gch cumulative global color histogram cgch local color histogram descriptor lch and color coherence vectors ccv are popular descriptors from literature usually used as baseline for comparisons lch is a traditional descriptor based on xed size regions and ccv is segmentation based gch cgch and lch are all very simple descriptors descriptors based on correlograms are promising because they encode spatial information as the traditional color autocorrelogram descriptor acc and the more elaborated joint autocorrelogram descriptor jac spatial information is usually lost when using simple color histograms the border interior pixel classication descriptor bic and color based clustering descriptor cbc are descriptors based on segmentation cbc has a complex extraction algorithm while bic has a very simple one bic has also presented promising results in heterogeneous collections the color bitmap descriptor was selected because it analyzes image color properties globally and by using xed size regions color structure descriptor csd was selected because according to previous comparisons among the mpeg color descriptors it achieves the best performance cw hsv and cw luv descriptors are very simple and generate very compact feature vectors the chromaticity moments descriptor cm is very simple and generates compact feature vectors texture descriptors the local binary pattern descriptor lbp is very popular in the literature and has simple algorithms presenting also good effectiveness and invariance to rotations and gray scale variation the homogeneous texture descriptor htd is a traditional descriptor from mpeg standard the steerable pyramid decomposition descriptor with scale and rotation invariance m spyd is used in our experimental comparison due to its good results and its invariance to scale and rotation the statistical analysis of structural information descriptor sasi was chosen because it presented better results than descriptors based on gabor lters as reported in the color co occurrence matrix ccom was chosen due to the popularity of the co occurrence matrix for analyzing textures it also aggregates color information to the original gray level cooccurrence matrix gcom unser descriptor was chosen due to its ability to generate more compact feature vectors to have lower complexity and to present effectiveness similar to the gcom descriptor the quantized compound change histogram qcch was chosen due to its simple extraction algorithm its fast distance function and its compact feature vector the main reason for choosing local activity spectrum descriptor las was its simplicity for both feature vectors extraction and distance computations evaluation measures efciency in features extraction and distance computations the times required for features extraction and distance computations of each descriptor were measured in seconds milliseconds or microseconds time measures were taken by the eva tool which measures the average and standard deviation value for each full color images categorized in classes apples pears tomatoes cows dogs horses cups and cars each class containing images each image is composed by one object in the center over a homogeneous background the objects appear in different positions and point of views in the following sections the results of the experimental evaluation with the color descriptors are presented the relative values are computed using global color histogram gch as the reference descriptor because gch is one of the most popular descriptors from the literature and it is also often used as baseline for comparisons efciency in features extraction table a shows the average time for extracting one feature vector as well as the standard deviation for each descriptor sorted by average time table a also shows the relative times of each descriptor in relation to gch descriptor the values were obtained from a total of features extractions for each descriptor times for each of the images only ccv csd cw luv jac and cbc are signicantly slower than gch for features extraction none of the tested color descriptors is signicantly faster than gch as observed cbc is the most time consuming descriptor for features extraction in the theoretical evaluation cbc is the most complex descriptor for features extraction having complexity o nlogn while all the other descriptors are o n although all other descriptors are o n there still were large differences in their actual time measurements in distance computations table b shows the average time for one distance computation and the standard deviation for each descriptor sorted by average time the times relative to gch are also shown the values were obtained from distance computations for each descriptor 280 the descriptors bic ccv csd acc lch and jac are considerably slower than gch descriptor on the other hand cm cwhsv cw luv and color bitmap are considerably faster than gch descriptor for distance computations according to the theoretical analysis cbc was the only descriptor with distance function more complex than o vs that higher complexity is observed in the times showed in table b where cbc was the slowest descriptor the differences in the times considering the other descriptors that are o vs were basically due to the feature vector sizes o a b penatti et al j vis commun image r in feature vectors storage considering the number of values in the feature vectors generated by each descriptor we computed the feature vector sizes in bytes table c shows the absolute and relative feature vectors sizes of each color descriptor as cbc descriptor generates variable size feature vectors the size showed in table c is the average size for eth database the descriptors acc and jac that are based on the idea of correlogram generate large feature vectors jac in particular generates very large feature vectors because it computes the joint autocorrelogram for more than one image property the lch descriptor also generates large feature vectors because it computes a color histogram for each image region to illustrate the impact of the differences in size of feature vectors in a web scenario assume a database with images cwhsv descriptor would use nearly gb of space for storing feature vectors while color bitmap would require gb gch gb csd gb and jac would require tb for example effectiveness fig shows the average precision recall curves for all the evaluated color descriptors considering all eth images as queries for recall value equal to the best precision values are very similar for bic csd jac acc and color bitmap descriptors they are the only descriptors with precision values over they remain the best descriptors until of recall however after of recall the precision values are similar for all descriptors except for color bitmap which is far above and for cm which is far below which makes it difcult to point out which descriptors are the best nevertheless if we analyze the curves considering a web environment it is better if the descriptor has high precision values when recall values are small the reason is because that only a small portion of the relevant images on the web will be retrieved and showed to the user which means small recall consequently we can consider bic csd jac acc and color bitmap as the most effective results for texture descriptors the texture descriptors showed in section were implemented according to the details showed in table given our application scenario we had to adjust some descriptors implementations our implementation of lbp uses values for the parameter r radio and for p neighbors also we use distance instead of the original proposed for lbp which is non symmetric the original m spyd descriptor has invariances to scale and rotation independently in the implemented version however both invariances are computed at the same time in other words the feature vector generated by our implementation of m spyd is invariant to scale and rotation at the same time htd m spyd and sasi had database dependent normalization steps which were forgone as they are incompatible with the highly dynamic weblike scenario intended our implementation of las descriptor quantizes components non uniformly to avoid generating histograms with an overwhelming majority of values in the rst bin when the descriptor proposed no distance function which was the case for unser we used the texture descriptors were tested on brodatz database brodatz is one of the most popular image databases for the evaluation of texture descriptors as observed in section brodatz collection was the most widely used in the evaluation of the texture descriptors present on this work brodatz contains different textures as done in the great majority of papers each texture is divided into xed size cells our experiments divided each texture in blocks totalizing images the resulting database is composed by categories having images each table texture descriptors comparison absolute and relative times for a each feature vector extraction in milliseconds b for each distance computation in microseconds and c absolute and relative size of a single feature vector average values are shown with their respective standard deviation relative measures against lbp descriptor tables are sorted by the average column descriptor as it takes more than on average to compare each pair of feature vectors there are considerable differences in distance times between the descriptors in relation to gch all the descriptors above gch in table b are faster and all below are slower except for cgch and cbc in feature vectors storage the feature vectors sizes shown in table c were computed with the same parameters used for the small scale experiments the sizes are the same for all descriptors except for cbc and ccom these two descriptors as discussed generate variable size feature vectors whose average sizes for websample are shown in table c and reveals a large increase in comparison to the sizes in eth and brodatz databases respectively cbc vectors increased on average more than times and ccom vectors more than times cbc vectors are larger because its clustering algorithm groups similar color and websample database has many more complex images than eth database ccom vectors are larger because color is present in websample images while it was absent in brodatz textures table c shows that there is little correlation between vector size and the type of descriptor color or texture jac descriptor generates by far the largest feature vectors that could have an important impact in the storage requirements of a cbir system effectiveness the effectiveness evaluation in websample collection was based on a set of queries with an associated pool of relevants for each query this set has images and each query has a pool of relevant images that were selected by real users the query images are shown in table the main reason for this kind of evaluation is that websample database has no standard categorization moreover involving users in the evaluation process can measure descriptor effectiveness in a real scenario taking into account the semantic variability of nal users judgment to create the pool of relevants the user oriented evaluation interface provided by eva tool was used a set of human evaluators composed by people graduate and undergraduate students analyzed the images retrieved by a set of descriptors and marked them as relevant or non relevant for each query it is important to say that there was no information about which descriptor retrieved each image and no extra information was given about each query image therefore the users had no inuence when interpreting the query image table shows the sizes of each pool of relevants using the pool of relevant images for each query the effectiveness evaluation could be conducted automatically by analyzing the images retrieved by each descriptor and verifying if they are present in the pool of the corresponding query the pool sizes shown in table include the query image however for the computation of the effectiveness measures the query was ignored from the pool table presents the average r1 and values among all query images for each descriptor while fig shows the box plot for each descriptor considering the for all queries in general the measures tell that the effectiveness of the descriptors is poor on average the best descriptors are able to retrieve around relevant images between the rst results however it is important to note that the descriptors evaluated here characterize the image information globally without having any notion of which image region is more important for the user o a b penatti et al j vis commun image r the precision and recall values indicate that the best overall effectiveness is achieved by descriptors jac bic and acc which have reached more than of and more than of in the average case the three are all color descriptors the best texture descriptor is ccom with average over local color histogram lch also has average of fig shows that many descriptors present very poor effectiveness having values close to zero for most of the queries this was the case for descriptors cgch cm color bitmap cwluv lbp m spyd qcch and unser we can observe that there is a large variation in the behavior of most descriptors having queries with both very low and very high precisions acc bic and jac which present the best average precisions also have similar medians in fig except that bic has more queries with higher values for example jac reached of for one query while bic lch and ccom reached of in one case to illustrate the effectiveness of the descriptors in some queries we have selected four queries corresponding to the two easiest queries and two cases that illustrate well the problem of semantic gap the worst descriptors in terms of effectiveness cgch cm color bitmap cw luv lbp m spyd qcch and unser were eliminated from the following analysis since their precision was zero or close to zero for almost all queries considered table shows the precision and recall values for the two easiest queries despite that only descriptors had a top precision above showing the challenge of the task query is very stereotypical and query has a large pool of relevant answers which are both factors that help to ease the task however we have observed that those are neither necessary nor sufcient conditions to explain query difculty table and fig shows the results for two queries that illustrate well the semantic gap phenomenon in general color descriptors performed better than texture descriptors but there was a large variability among queries as we hinted above the most common explanations size of the pool of relevants simplicity of the image were not sufcient to explain that variability we have observed that the specic choice of the query image may have an important effect on the user interpretation of its contents which poses a challenge for query by example systems based on a single query and reinforces the importance of query renement user feedback comparing effectiveness in small and large scale experiments in general it is remarkable the effect of heterogeneity in the descriptors effectiveness to evaluate that effect we have computed the correlation between the effectiveness measures in the small scale experiments and the results in the websample database fig shows the correlation between the rank of the descriptors in each scenario and plots the linear regression model for them the rank is based on the map for the small scale and the for the large scale experiment for both color and texture we have provided an interface for the visualization of all queries results http www recod ic unicamp br eva as of september o a b penatti et al j vis commun image r table average and values for queries and sorted by texture descriptors appear in italic fig box plot with each descriptor values on all queries values above or below the interquartile range were considered outliers and indicated as small circles color descriptors appear in light gray while texture ones appear in the right in dark gray descriptors are sorted alphabetically few texture descriptors designed with heterogeneous database in mind accordingly it has the best relative performance on the large scale experiment though its performance on the brodatz dataset is unremarkable descriptors combination table average and values for queries and sorted by texture descriptors appear in italic descriptors the correlation of relative effectiveness on the two experiments is signicant but with notable departs color bitmap relative effectiveness is so much better in the small scale experiment than in the large scale one that we had to consider it an outlier in the regression computation with it the correlation coefcient falls to r other notable departs are the lch gch cw hsv which perform relatively better in the large scale experiment for texture descriptors the correlation observed is smaller reecting the more specic nature both of many texture representation and the special purpose brodatz dataset ccom was one of considering the challenges of retrieving relevant images in a web scenario the combination of more than one descriptor is often recommended the exact way descriptors should be combined e g early fusion of the feature vectors late fusion of the distances adaptive methods etc in order to achieve maximum performance is an open research question and outside the scope of this paper however we follow the example of and provide an indicative metric considering the degree of agreement among the best descriptors studied over the queries which is useful to determine when the combination can be useful descriptors with a high degree of agreement tend to reinforce each other while descriptors with a low agreement can be used to complement each other and both effects must be taken into consideration when designing combination algorithms to avoid disrupting the analysis we have chosen only the bestperforming descriptors and computed their correlations over the queries on the websample dataset that pairwise analysis is shown in table a more delicate analysis takes into account all the best descriptors at once and studies the cross correlations using a principal component analysis the two main components which account for of the variability are plotted in fig and show that two groups of descriptors tend to agree jac and lch on one hand and acc bic and ccom on the other hand note also that the overall agreement between those best descriptors is signicant the plot in fig also reveals how individual queries performed on those descriptors horizontally queries to the left of the plot tended to be easier than queries to the right vertically queries to the center tended to perform equally well or bad for all descriptors while queries on top or bottom showed the largest differences conclusion the experiments in the web environment have shown interesting aspects of the descriptors performances considering efciency table query images their identication numbers and the sizes of their respective pool of relevants table average r1 and values over all the query images for websample database sorted by texture descriptors appear in italic the bold values show the sorting criterium descriptor in this paper we propose an effective method to recognize human actions from sequences of depth maps which provide additional body shape and motion information for action recognition in our approach we project depth maps onto three orthogonal planes and accumulate global activities through entire video sequences to generate the depth motion maps dmm histograms of oriented gradients hog are then computed from dmm as the representation of an action video the recognition results on microsoft research msr dataset show that our approach significantly outperforms the state of the art methods although our representation is much more compact in addition we investigate how many frames are required in our framework to recognize actions on the msr dataset we observe that a short sub sequence of frames is sufficient to achieve comparable results to that operating on entire video sequences as the imaging technique advances e g the launch of microsoft kinect it has become feasible to capture color image sequences as well as depth maps in real time by rgbd sensors the depth maps are able to provide additional body shape and motion information to distinguish actions that generate similar projections from a single view which motivates recent research work to explore action recognition based on depth maps a bag of points or silhouettes method was proposed in to represent postures by sampling points from depth maps an action graph was then employed to model the sampled points to perform action recognition their experimental results on msr dataset validated the superiority of silhouettes from depth maps over silhouettes from a single view categories and subject descriptors i applications general terms algorithms performance experimentation keywords action recognition rgbd camera depth maps feature representation introduction automatic human action recognition has many real world applications including content based video search humancomputer interaction video surveillance health care and etc in the past decades research of human action recognition mainly concentrates on video sequences captured by traditional rgb cameras the spatio temporal volume based methods have been extensively used for recognizing actions through measuring similarities between action volumes in order to facilitate accurate similarity measurements various detection and representation methods of spatio temporal volumes have been proposed trajectory based approaches have been explored for recognizing human activities as well in this case human actions are figure the sampled sequences of depth maps for actions of a tennis serve b golf swing and c pickup throw permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee mm october november nara japan copyright acm area chair ansgar scherp in this paper we focus on recognizing human actions using sequences of depth maps fig illustrates the depth maps for actions tennis serve golf swing and pickup throw as shown in this figure depth maps provide additional shape and motion information however point cloud of depth maps also incur a great amount of data that might result in expensive computations here we propose an effective and efficient approach to recognize human actions by extracting histograms of oriented gradients hog descriptors from depth motion maps dmm the dmm are generated by stacking motion energy of depth maps projected onto three orthogonal cartesian planes the stacked motion energy of each action category produces specific appearances and shapes on dmm which can be used to characterize corresponding action categories motivated by the success of hog in human detection we adopt hog descriptors to represent dmm compared to the original depth data the proposed dmm hog representation is more compact and more discriminative in addition we investigate how many frames are sufficient to perform action recognition using dmm hog the experiments on msr dataset demonstrate that a short subsequence e g frames is sufficient to obtain reasonably accurate recognition results for human action recognition this observation is important to make online decisions and to reduce observational latency when humans interact with computers related work the spatio temporal volume based methods are widely used in action recognition from videos captured by traditional rgb camera these approaches mainly focus on detection and representation of space time volumes for example bobick and davis accumulated foreground regions of a person as motion history images mhi to explicitly track shape changes tian et al employed harris detector and local hog descriptor on mhi to perform action recognition and detection similar to mhi the proposed dmm also stacks foreground motion regions to record where and how actions are performed however there are main differences mhi only keeps most recent motions to capture the recency of action while dmm accumulates global activities through entire video sequences to represent the motion intensity our method stacks motion regions from front side top views i e three orthogonal projections of depth maps while only a single view is used in mhi in the most recent work local spatio temporal features have been extensively used as object recognition using sparse local features in images an action system first detects interest points and then computes descriptors based on the detected local spatio temporal volumes the local features are then combined e g bag of words to model different activities the fundamental difference between those systems and our method is that they designed features based on video sequences instead of depth maps that include supplementary information of body shape and motions figure the framework of computing dmm hog hog descriptors extracted from depth motion map of each projection view are combined as dmm hog which is used to represent the entire action video sequences computaion of dmm hog the framework to compute action representation of dmm hog is demonstrated in fig we project depth frames onto three planes and compute associated motion energy which are then stacked to obtain dmm hog descriptors are extracted from three depth motion maps and concatenated as the final action representation of dmm hog depth motion maps dmm in order to make use of the additional body shape and motion information from depth maps each depth frame is projected onto three orthogonal cartesian planes we then set the region of interest of each projected map as the bounding box of foreground i e non zero region which is further normalized to a fixed size this normalization is able to reduce intra class variations e g subject heights and motion extents of different subjects when they perform the same action so each depth frame generates three maps according to front side and top views i e and as for each projected map we obtain its motion energy by computing and thresholding the difference between two consecutive maps the binary map of motion energy indicates motion regions or where movement happens in each temporal interval it provides a strong clue of the action category being performed we then stack the motion energy through entire video sequences to generate the depth motion map for each projection view with the release of rgbd sensors research of action recognition based on depth information has been explored li et al used silhouettes for action recognition they sampled a set of representative points from depth maps to characterize the posture being performed in each frame they first projected depth maps onto three orthogonal cartesian planes and sampled points at equal distance along contours of the three projections the points were then retrieved in depth maps according to the contour points however the sampled points of each frame generated a considerable amount of data which resulted in expensive computations in clustering training videos of all classes yang and tian proposed an eigenjoints based action recognition system by using a nbnn classifier the compact representation of eigenjoints employed joints differences to capture action information of static postures consecutive motions and overall dynamics however the positions of skeleton joints might be complete wrong if there are sever occlusions where denotes the projection view is the projected map of the th frame under projection view is the is the binary map of number frames motion energy and is the threshold we empirically set in our experiments as shown in fig the dmm generated from an action video of pickup throw demonstrate specific shows action recognition accuracies of dmm with different normalization sizes under a variety of test sets the overall recognition rates of most test sets are similar across different dmm normalization sizes as for and the size of achieves the best results while for the size of outperforms the others although lower resolutions are able to reduce computational cost in computing hog we extract hog descriptors only from the three depth motion maps instead of each video frame so for each video the difference of computation time between different sizes is limited the following experimental results are based on the size of as shown in fig while the performances in are promising the recognition accuracies in and are relatively low in cross subject test different subjects perform actions with great variations but the amount of subjects is limited which results in considerable intra class variations furthermore some actions in are quite similar e g draw x draw tick and draw circle which generates small inter class variations the performances on cross subject test might be improved by adding in more subjects appearances and shapes which characterize the accumulated motion distribution and intensity of this action the dmm representation encodes the information of body shape and motion in three projected planes meanwhile significantly reduces considerable data of depth sequences to just three maps dmm hog descriptor hog is able to characterize the local appearance and shape on dmm pretty well by the distribution of local intensity gradients the basic idea is to compute gradient orientation histograms on a dense grid of uniformly spaced cells and perform local contrast normalization in each cell different normalizations i e norm sqrt and hys are computed based on adjacent histograms as for each depth motion map we evenly sample non overlapping cells and gradient orientation generates a descriptor with the bins so each dimension of as shown in fig we concatenate as the dmm hog descriptor which is the input to a linear svm classifier to recognize human actions experiments and discussions the proposed method is evaluated on the msr dataset we extensively compare our approach with the state of theart methods under a variety of experimental settings we further investigate how many frames are sufficient to recognize actions using dmm hog experimental setup the msr is a public dataset with sequences of depth maps captured by a rgbd camera it includes action categories performed by subjects facing to the camera during performance each action was performed or times by each subject the depth maps are with the resolution of the action categories are chosen in the context of interactions with game consoles as illustrated in fig actions in this dataset reasonably capture a wide range of motions related to arms legs torso and their combinations figure the recognition rates of dmm with different normalization sizes under a variety of test sets how many frames are sufficient in order to facilitate a fair comparison we follow the same experimental settings as to split categories into three subsets as listed in table as for each subset there are three different tests i e test one one test two two and cross subject test crsub in test one of the subset is used as training the rest as testing in test two of the subset is used as training and the rest as testing in cross subject test half subjects are used for training and the rest ones used for testing most existing systems recognize actions by operating on entire video sequences we perform experiments to investigate how many frames are sufficient for action recognition with reasonably accurate results in our framework the recognition rates using different amount of frames under a variety of test sets are demonstrated in fig the sub sequences are chosen from the first frames of a given video as shown in this figure in most cases frames are sufficient to achieve comparable results to the ones using entire sequences with quite limited gains or even some loss as more frames are added in as affect recognition in the temporal segments of an action can be intuitively approximated by the statuses of neutral onset apex and offset the most discriminative information is within the status of apex and onset which are probably covered by the first frames of the msr dataset the sequences after apex contribute little or even incur more noise this observation provides important guides to reduce latency of action recognition systems where decisions have to be made on line the following results are based on the sub sequence of first frames table action subsets and tests used in our experiments action set action set action set horizontal wave hammer forward punch high throw hand clap bend tennis serve pickup throw high wave hand catch draw x draw tick draw circle hands wave forward kick side boxing high throw forward kick side kick jogging tennis swing tennis serve golf swing pickup throw comparisons to the state of the art evaluations of dmm hog we compare our dmm hog approach with the state of the art methods including silhouettes and eigenjoints on the msr dataset in table the recognition accuracies of silhouettes and eigenjoints are obtained from the we first evaluate the effect of dmm normalization size to recognition performances as discussed in section we much of the existing work on action recognition combines simple features with complex classiers or models to represent an action parameters of such models usually do not have any physical meaning nor do they provide any qualitative insight relating the action to the actual motion of the body or its parts in this paper we propose a new representation of human actions called sequence of the most informative joints smij which is extremely easy to interpret at each time instant we automatically select a few skeletal joints that are deemed to be the most informative for performing the current action based on highly interpretable measures such as the mean or variance of joint angle trajectories we then represent the action as a sequence of these most informative joints experiments on multiple databases show that the smij representation is discriminative for human action recognition and performs better than several state of the art algorithms  elsevier inc all rights reserved introduction human motion analysis has remained as one of the most important areas of research in computer vision over the last few decades a large number of methods have been proposed for human motion analysis see the surveys by moeslund et al and turaga et al and most recently by aggarwal and ryoo for a comprehensive analysis in general all methods use a parametric representation of human motion and develop algorithms for comparing and classifying different instances of human activities under these representations one of the most common and intuitive methods for representation of human motion is a temporal sequence of approximate human skeletal congurations the skeletal congurations represent hierarchically arranged joint kinematics with body segments reduced to straight lines in the past extracting accurate skeletal congurations from monocular videos was a difcult and unreliable process especially for arbitrary human poses motion capture systems on the other hand could provide very accurate skeletal congurations of human actions based on active or passive markers positioned on the body however the data acquisition was limited to controlled indoor environments methods for human corresponding author e mail addresses foi eecs berkeley edu f oi rizwanch cis jhu edu r chaudhry gregorij eecs berkeley edu g kurillo rvidal cis jhu edu r vidal bajcsy eecs berkeley edu r bajcsy see front matter  elsevier inc all rights reserved http dx doi org j jvcir motion analysis that relied heavily on accurate skeletal data therefore became less popular over the years as compared to the image feature based activity recognition methods in the latter spatiotemporal interest points are extracted from monocular videos and the recognition is based on learned statistics on large datasets recently with the release of several low cost and relatively accurate capturing systems such as the microsoft kinect realtime data collection and skeleton extraction have become much easier and more practical for the applications of natural human computer interaction gesture recognition and animation thus reviving interest in the skeleton based action representation existing skeleton based methods for human action recognition are primarily focused on modeling the dynamics of either the full skeleton or a combination of body segments to represent the dynamics of normalized positions of joints or joint angle congurations most of the methods use linear dynamical systems lds or non linear dynamical systems nlds e g in or hidden markov models hmm see e g the earlier work by yamato et al and a review of several others in recently taylor et al proposed using conditional restricted boltzman machines crbm to model the temporal evolution of human actions while these methods have been very successful for both human activity synthesis and recognition their representation of human motion is in general not easy to interpret in connection to the physical and qualitative properties of the human motion for example the parameters obtained from the lds modeling of the skeletal joint trajectories will likely describe positions and velocities of the f oi et al j vis commun image r individual joints which do not directly convey any information about the changes in the skeletal conguration of the human body as the action is performed when humans perform an action we can observe that each individual performs the same action with a different style generating dissimilar joint trajectories however all individuals activate the same set of joints contributing to the overall movement roughly in the same order in our approach we take advantage of this observation to capture invariances in human skeletal motion for a given action given an action we propose to identify the most informative joints in a particular temporal window by nding the relative informativeness of all the joints in that window we can quantify the informativeness of a joint using for example the entropy of its joint angle time series in the case of a gaussian random variable its entropy is proportional to the logarithm of its variance therefore the joint that has the highest variance of motion as captured by the change in the joint angle can be dened as the most informative assuming the joint angle data are independent and identically distributed i i d samples from a one dimensional gaussian such a notion of informativeness is very intuitive and interpretable during performance of an action we can observe that different joints are activated at different times with various degree therefore the ordered sequence of informative joints in a full skeletal motion implicitly encodes the temporal dynamics of the motion based on these properties we recently proposed in the sequence of the most informative joints smij as a new representation for human motion based on the temporal ordering of joints that are deemed to be the most informative for performing an action in we briey compared the performance of the smij representation to other action representations based on the histograms of motion words as well as the methods that explicitly model the dynamics of the skeletal motion in this paper we provide a more comprehensive description of the smij representation and other feature representations and further evaluate their quality using action classication as our performance test in addition we propose a different metric for comparison of smij features based on normalized edit distance which outperforms the normalized levenshtein distance applied in our previous work furthermore we show that our simple yet highly intuitive and interpretable representation performs much better than standard methods for the task of action recognition from skeletal motion data sequence of the most informative joints smij the human body is an articulated system that can be represented by a hierarchy of joints that are connected with bones forming a skeleton different joint congurations produce different skeletal poses and a time series of these poses yields the skeletal motion an action can thus simply be described as a collection of time series of positions i e trajectories of the joints in the skeleton hierarchy this representation however lacks important properties such as invariance with respect to the choice of the reference coordinate system and scale of the human a better description is obtained by computing the joint angles between any two connected limbs and using the time series of joint angles as the skeletal motion data let ai denote the joint angle time series of joint i i e ai fait where t is the number of frames in an action sequence an action sequence can then be seen as a collection of such time series data from different joints i e a aj where j is the number of joints in the skeleton hierarchy hence a is the t j matrix of joint angle time series representing an action sequence common modeling methods such as lds or hmm model the evolution of the time series of joint angles however instead of directly using the original joint angle time series data a one can also extract various types of features from a that in general reduce the size of data yet preserve the information that is discriminative of each action for the sake of generality we denote this operation with the mapping function o in the remainder of this paper unless an explicit specication is necessary here oa rjaj r is a function that maps a time series of scalar values to a single scalar value furthermore one can extract such features either across the entire action sequence i e global features or across smaller segments of the time series data i e local features the former case describes an action sequence with its global statistics whereas the latter case emphasizes more the local temporal statistics of an action sequence examples include the mean or variance of joint angle time series or the maximum angular velocity of each joint as observed over the entire action sequence or inside a small temporal window motivation of the proposed representation in this paper we approach the action recognition with the following hypothesis different actions require humans to engage different joints of the skeleton at different intensity energy levels at different times hence the ordering of joints based on their level of engagement across time should reveal signicant information about the underlying dynamics i e the invariant temporal structure of the action itself in order to visualize this phenomenon let us consider the labeled joint angle conguration shown in fig a and perform a simple analysis on the berkeley multimodal human action database berkeley mhad see section for details about the datasets the analysis is based on the following steps i partition the joint angle time series of an action sequence into a number of congruent temporal segments ii compute the variance of the joint angle time series of each joint over each temporal segment note that the mapping function o is dened to be the variance operator in this particular case iii rank order the joints within each segment based on their variance in descending order iv repeat the rst three steps to get the orderings of joints for all the action sequences in the dataset below we investigate the resulting set of joint orderings for different actions in the berkeley mhad fig b shows the distribution of the most informative i e the top ranking joint for different actions across all repetitions from all subjects in the berkeley mhad specically each entry in the gure shows the percentage of the time that the trajectory of a given joint within a segment has the highest variance notice that simple actions such as punch and wave one engage only a small number of joints while more complex actions such as sit stand engage several joints in different parts of the body nevertheless the set of the most informative joints are different for different actions joint relbow is the most informative joint of the time followed by joint rarm of the time in the wave one action both joints relbow and lelbow are the most informative joints more than of the time in the punch action on the other hand almost half of the joints appear as the most informative at some point in the actions sit stand sit down and stand up however the differences across the sets of engaged joints in each of these three actions are still noticeable for instance joint lknee is engaged more in the sit stand action than in the sit down and stand up actions fig shows the stacked histogram distributions of the most informative joints for four different actions taken from the berkeley mhad even though the overall set of the most informative joints looks similar for the actions jump and jumping jacks there are signicant differences particularly in the distribution of joints at different rankings for different actions specically joints rknee and lknee appear more than of the time as either the or ranking joint for the jump action whereas this ratio is between and for the jumping jacks action furthermore temporal orderings of the most informative joint for the actions sit down and stand up across the rst seven subjects in the berkeley mhad each row of a plot represents the sequence of the most informative joint extracted from the rst action recording of the corresponding subject that even though the overall set of the most informative joints look very similar for the actions sit down and stand up the temporal ordering of the most informative joint can be used to distinguish these two actions successfully specically the orange colored joints i e joints rknee and lknee are engaged the most at the beginning of the sit down action as opposed to being engaged the most towards the end of the stand up action conversely the blue green colored joints i e joints rarm relbow larm and lelbow are engaged the most interchangeably at the end of the sit down action as opposed to being engaged the most at the beginning of the stand up action the observed temporal ordering corresponds to the nature of the action as the subjects used their arms for support when rst getting out of the chair and conversely when sitting down in summary our initial analysis suggests that different sets of joints as well as their temporal ordering reveal discriminative information about the underlying structure of the action this is precisely the main motive to propose sequences of the top n most informative joints as a new feature representation for human skeletal action recognition hence the new feature representation which we refer to as sequence of the most informative joints smij has two main components i the set of the most informative joints in each time segment and ii the temporal ordering of the set of the most informative joints over all of the time segments segmentation to extract the smij representation from the joint angle timeseries data we rst need to partition the action sequence into a number of say n temporal segments let aik aik tk be a e segment of ai where tks and tke denote the start and the end frames for the segment k respectively then the joint angle time series data of joint i a i can be written as a collection of temporal motion segments ai aik ns in general the length of the motion segments should be such that signicant information about the underlying dynamics of the movement is contained within the segment we hypothesize that the size and the type of partitioning will inuence the discriminative properties of the proposed smij representation since the atomic motion units will be captured differently depending on the size and the number of the segments the temporal segmentation of human motion into such atomic motion units is however still an open research problem some of the top down approaches in this domain rely on creating models of atomic motion units a priori from expert knowledge and training data some of the bottom up segmentation techniques on the other hand utilize the principle component analysis and data compression theory the problem of temporal segmentation of human activities is however beyond the scope of this study in this paper we thus examine two different elementary methods to partition the action sequences a segmentation with fixed number of segments in this approach we divide the time series data associated with one sample action sequence into a xed number of congruent segments of variable size the resulting feature representation thus have the same size for all action sequences however this method yields shorter temporal segments for shorter action sequences and longer temporal segments for longer action sequences which can be interpreted as a temporal normalization of the actions note that this is the same segmentation approach used in our previous work on smij representation b segmentation with fixed temporal window in the second approach we divide the time series data associated with one sample action sequence into a variable number of congruent segments of a given xed size this alternative approach is novel for the smij representation and leads to uniform temporal analysis of all action sequences unlike the rst approach where the feature representations are of the same length for all the actions this approach yields different number of segments for different action sequences hence the resulting feature representation has different size for different action sequences the xed temporal window based segmentation is prevalent in several domains such as speech audio processing or signal processing in general since it is more intuitive and generalizable we likewise hypothesize that the xed temporal window approach will yield more discriminative smij representations of different actions than the xed number of segments approach we analyze the results of these two partitioning approaches in section examination of more advanced temporal segmentation methods remains to be our future work measure of information once we partition the action sequence into congruent temporal segments we apply the mapping function o to each segment and write the action sequence as a collection of features f ff k ns where h i f k o o o ajk f oi et al j vis commun image r the feature function oaik  provides a measure of informativeness of the joint i in the temporal segment k in information theory one measure of information of a signal is the entropy which is dened as hx  z f x log f xdx x where x is a continuous random variable with probability density function f whose support is a set x for a gaussian distribution with variance the entropy can be calculated as hx  log  therefore the entropy of a gaussian random variable is proportional to the logarithm of its variance assuming that the joint angle time series data ai are i i d samples from a one dimensional gaussian distribution we can measure the informativeness of ai in each temporal segment by computing the corresponding variance in each segment based on this assumption we choose the mapping function o to be the variance operator in the remainder of this paper a more sophisticated information theoretic measure of informativeness that also considers signal noise remains to be a future work ordering after the temporal segmentation and mapping of each joint angle time series we rank order all the joints in f k within each temporal segment k based on their informativeness i e oaik  and dene smij features as s skn ns n skn idof sortf k  n where the sort operator sorts the joints based on their local o score in descending order the idof  n operator returns the id or equivalently the label of a joint that ranks nth in the joint ordering and n species the number of top ranking joints included in the representation resulting in a n n dimensional feature descriptor in other words the smij features represent an action sequence by encoding the set of n most informative joints at a specic time instant by rank ordering and keeping the top ranking n joints as well as the temporal evolution of the set of the most informative joints throughout the action sequence by preserving the temporal order of the top ranking n joints fig shows the most informative joints at the key frames of two actions selected from the dataset see section for details about the datasets we acknowledge that using the proposed representation of an action as a sequence of rank ordered joints signicantly reduces the amount of information contained in the original time series data we argue however that the remaining information is discriminative enough to distinguish different actions in our experiments we show that such an extreme abstraction in the feature representation yields satisfactory results for action recognition a more detailed feature representation will become necessary when the set of the most informative joints and their orderings are very similar for two different actions in such case we can enrich the representation by retaining not only the ranking of the most informative joints but also some information about the motion of the most informative joint such as the direction of the motion of the most informative joint within each temporal segment we leave this avenue as a future research direction metrics for comparing smij each smij is dened over a xed alphabet the joint labels therefore comparison of the smij features from two different se quences si and sj is equivalent to comparison of strings the distance metric as a measure of similarity between two strings with nite sequence of symbols is often dened using edit distance functions which consist of counting the minimum number of edit operations needed to transform one string into the other the edit operations include insertion deletion substitution of a single character or transposition of two adjacent characters these four edit operations were rst introduced by damerau who applied them to automatic detection and correction of spelling errors subsequent to damerau work levenshtein introduced in the corresponding edit distance to deal with multiple edit operations such as deletion insertion and reversals but excluded transposition his distance metric is known as the levenshtein distance wagner and fischer rst proposed a dynamic programming algorithm for calculating the levenshtein distance in and then extended the set of allowable edit operations to include transposition of two adjacent characters in interestingly none of the aforementioned algorithms considered normalization of the distance metric that would appropriately rate the weight of the edit errors with respect to the length of the sequences strings that are compared even though the normalization may not be crucial for comparing strings of the same length it becomes critical for comparing strings of different lengths as pointed out by marzal and vidal in in their seminal work marzal and vidal proposed an algorithm called the normalized edit distance ned based on nding the minimum of w p lp  not only the minimum of w p where p is an editing path between si and sj w p is the sum of the weights of the elementary edit operations of p and lp is the number of these operations length of p they also emphasized that this minimization cannot be carried out by rst minimizing w p  and then normalizing it by the length of the obtained path lp  which they refer to as postnormalization in our original work on the activity recognition using smij features we used the post normalized levenshtein distance to compare the sequences of features all the action sequences were partitioned into xed number of segments generating feature vectors of the same length the post normalized levenshtein distance was thus able to properly describe the level of similarity between the sequences in this paper we extend the temporal segmentation by including segmentation with the xed temporal window which generates sets of sequences of various lengths the levenshtein distance does not properly account for the length variations between the sequences since it only seeks to nd the minimum number of edit operations in other words minimizes w p only instead we apply a more sophisticated distance metric i e the normalized edit distance which considers variable lengths of feature vectors and a proper normalization of the distance metric using the normalized edit distance we dene the distance between two different smij representations si and sj as follows n x ds si sj nedsi n sj n  where n refers to the nth column of s i e the sequence of nthranking joints and n is the number of the most informative joints included in the smij representation alternative feature representations in this section we briey describe three alternative feature representations against which we compare the results of the proposed smij representation we consider two standard methods linear dynamical system parameters ldsp with a linear system identication approach and histogram of motion words hmw with a the most informative joints are highlighted along the key frames of two different actions from dataset deposit oor on the left and throw basketball on the right bag of words model in addition we compare our results with the histogram of the most informative joints hmij which was also proposed originally in as an alternative to smij we believe that the three alternative feature representations i e hmij hmw and ldsp adopted from a wide range of popular approaches allow us to demonstrate the power of the proposed smij features in terms of discriminability and interpretability for human action recognition histogram of motion words hmw histogram of motion words is based on the popular bagof words method for visual categorization in this approach we rst cluster the set of all f k for a given action sequence into k clusters i e motion words using k means or k medoids next we count for each sequence the number of motion words by assigning each f k to its closest motion word after normalization we obtain the histogram of motion words hmw representation which captures the overall distribution of motion words in the form of a histogram for each sequence since the hmw ignores temporal relations between smaller action units a sequence with scrambled f k will yield the same hmw representation as the original sequence as the distance metric for comparing hmw features we use distance dened as follows hi hj i j k h h x k k hi  hj k k i i j j where hi hk  and hj hk  are two normalized histograms and k is the number of bins in the histograms or equivalently the number of clusters that has to be decided a priori for all the experiments we chose k which will be discussed further in section note that since the nal clustering result depends on the initial condition the nal recognition rate can change based on the motion words computed during the clustering stage we therefore perform runs for each clustering experiments and compute the corresponding hmw representations for each action sequence we provide the mean and standard deviation of the classication rates achieved over these runs using methods such as nearest neighbor nn with distance on histograms and sup port vector machine svm with a gaussian kernel using distance on histograms linear dynamical system parameters ldsp as mentioned earlier one of the most common techniques to analyze human motion data is based on modeling the motion with a linear dynamical system over the entire sequence e g see for more details and using the lds parameters ldsp as an alternative feature representation a general lds is dened by the following set of equations yt l  cxt  wt xt axt  bv t where yt rp is the output of the lds at time t and is linearly related to the hidden state xt rm furthermore xt depends linearly on only the previous state xt l rp is the mean output and vt rmv is a zero mean unit variance i i d gaussian process that drives the state process xt similarly wt rp n i is a zero mean uncorrelated output noise process the joint angle time series is hence modeled as the output of an lds and can therefore be represented by the tuple a c b l  where a rm m is the dynamics matrix c rp m is the observation matrix b rm mv mv m is the input to state matrix is the identical covariance of each output dimension and is the initial state of the system given a feature time series these parameters can be computed using system identication for which several methods exist e g and em we choose to use the sub optimal but very fast method by doretto et al to identify the system parameters for the joint angle time series of a given action sequence once these parameters are identied for each of the action sequences various metrics can be used to dene the similarity between these ldss in particular three major types of metrics are i geometric distances based either on subspace angles between the observability subspaces of the ldss or on an alignment distance between two lds parameters under a group action ii algebraic metrics such as the binet cauchy kernels and iii information theoretic metrics such as the kl divergence we use the geometric distance known as the martin distance as f oi et al j vis commun image r the metric between dynamical systems for classication based on ldsp using methods such as nn and svm the ldsp representation as opposed to the aforementioned smij and hmw representations does not require segmentation of the joint angle trajectories therefore it captures only the global temporal information about an action while ignoring the details on the local levels we collected repetitions of each action yielding a total of action sequences after excluding one erroneous sequence we then extracted the skeleton data by post processing the optical motion capture data the actions lengths vary from to frames corresponding to approximately the set of actions consisted of jump jumping jacks bend punch wave one hand wave two hands clap throw sit down stand up and sit down stand up histograms of the most informative joints hmij finally we propose an alternative feature representation which is based on a similar idea to smij but disregards the temporal information instead of stacking the most informative n joints from all temporal segments into a matrix of symbols while keeping the temporal order of the joints intact we create histograms separately for the ranking joints ranking joints and so on from all temporal segments the histograms are then concatenated as a feature descriptor called histograms of the most informative joints hmij to represent each action sequence in the following form hmij hist fidof sortf k  ns n here the hist operator creates a j bin normalized histogram from the input joint sequence resulting in jn dimensional feature descriptor since hmij is a histogram based representation we use the distance given in to compute the distance between hmij features for classication based on nn and svm with a gaussian kernel it is important to note that the hmij feature representation ignores the temporal order of the most informative n joints and hence it will be useful for evaluating the importance of preserving the temporal ordering in the feature representation which is preserved by the smij feature representation experiments in this section we compare our proposed feature representation smij described in section against the baseline feature representations explained in section on the datasets outlined in section using action recognition as a test and provide experimental results in section datasets we evaluate the performance of each feature representation described above on three different human action datasets of skeleton data two of the datasets were obtained using a high quality motion capture system while the third one contains skeleton data obtained from a single viewpoint depth sensor each dataset has almost completely distinct set of actions with different frame rates different skeleton extraction method and hence skeleton data of various dynamic properties and data delity the goal of including such diverse input data was to examine how discriminative the proposed smij method is with respect to the varying properties of these datasets the diversity is relevant in the rst set of experiments where we aim to evaluate the performance of the feature representations on a wide range of actions for the second set of experiments where we evaluate the action recognition across datasets we select a small subset of actions that are shared between the rst two datasets motion capture database from the popular database we arbitrarily selected actions performed by subjects in this dataset subjects performed each action with various number of repetitions resulting in action sequences in total the motion capture data which was captured with the frequency of hz also includes the corresponding skeleton data the duration of the action sequences ranges from to frames corresponding to approximately the set of actions consisted of deposit oor elbow to knee grab high hop both legs jog kick forward lie down oor rotate both arms backward sneak squat throw basketball jump jumping jacks throw sit down and stand up msr database finally we also evaluated the action recognition performance on the msr dataset consisting of the skeleton data obtained from a depth sensor similar to the microsoft kinect with hz due to missing or corrupted skeleton data in some of the available action sequences we selected a subset of actions performed by subjects with repetitions of each action the subset consisted of action sequences in total with the duration of the sequences ranging from to frames corresponding to approximately the set of actions included high arm wave horizontal arm wave hammer hand catch forward punch high throw draw x draw tick draw circle hand clap two hand wave side boxing forward kick side kick jogging tennis swing and tennis serve database standardization before proceeding with the action recognition experiments on the aforementioned datasets we need to consider some important factors that can potentially inuence the recognition results the three databases have different acquisition frame rates which affect the maximal number of temporal segments that can be extracted from a sequence another important factor to be considered is the number of repetitions of an action unit in a particular sample the berkeley mhad database for instance contains ve repetitions of an action unit in each sample sequence e g the person jumps or claps ve times whereas in the other two datasets the subject performs only one repetition of similar action unit for the purpose of consistent and objective comparison of the classication performance we standardize the datasets with respect to the frame rate and the action unit segmentation the action sequences of the berkeley mhad dataset with multiple repetitions of the same action were split into individual sequences with only one repetition thus extending the dataset by several shorter action sequences furthermore we downsampled the extended berkeley mhad dataset from fps to fps and upsampled the msr dataset from fps to fps to match the frame rate for all the datasets using lowpass decimation and interpolation respectively action recognition results berkeley multimodal human action database berkeley mhad we recently collected a dataset that contains actions performed by subjects using an active optical motion capture system phasespace inc san leandro ca the motion data was recorded with active led markers at hz for each subject in this section we examine the quality of different feature representations by evaluating their classication performance using well established methods such as nearest neighbor nn and support vector machine svm with the corresponding distance f oi et al j vis commun image r metrics introduced earlier in the respective subsections for svm based classication we follow one vs one classication scheme and use gaussian kernel k   e cd  h with an appropriate distance function d  depending on the feature type listed above as for the svm hyperparameters we set the regularization parameter c to and the gaussian kernel function parameter c to the inverse of the mean value of the distances between all training sequences as suggested in in order to determine the number of clusters k for the hmw representation we performed preliminary classication experiments for different values of k and observed that the performance saturates for k in addition we aim to match the dimensions of the histograms in the hmw representation to the dimensions of the histograms in the hmij representation since the histograms in the hmij representation can be to we chose k for the hmw representation action recognition on the same database in the rst set of experiments we performed action recognition on each of the aforementioned datasets separately we used roughly of the data for training and the remainder for testing specically we used subjects action sequences for training and subjects action sequences for testing on the berkeley mhad database subjects action sequences for training and subjects action sequences for testing on the database and nally subjects action sequences for training and subjects action sequences for testing on the msr database fig shows the classication results for the rst three feature types i e smij hmij and hmw on three different datasets for a range of values for the xed number of segments approach specically the plots in different columns correspond to different feature types i e smij hmij and hmw from left to right respectively the plots in different rows show recognition results from different datasets i e berkeley mhad and msr from top to bottom respectively in all plots the vertical axis represents the classication performance and the horizontal axis represents the number of segments ranging from to with a step size of different colors in the smij and hmij plots represent different number of the most informative joints n included in the feature representation on the other hand different colors in the hmw plots represent different clustering methods i e k means and kmedoids used to obtain nal feature representation the solid lines in the plots show svm based classication results whereas the dotted lines show nn based classication results the hmw plots show the mean and standard deviation of the classication rates achieved over runs as explained in section arranged identically to fig fig shows the classication results for a range of different window sizes for the xed temporal window approach we observe in fig that as we increase the number of segments in the xed number of segments approach the classication performance rst improves and then saturates when the number of segments is sufciently large especially for berkeley mhad and datasets on the contrary we see the opposite trend in fig that is the classication performance in general tends to decrease as we increase the temporal window size in the xed temporal window approach these two observations are consistent since increasing the number of segments corresponds to for the sake of exactness we note that we do not compute the square of the distance function hi hj  when we compute the corresponding kernel since the distance function already returns squared distances by denition recall from section that the dimension of the histograms in the hmij representation depends on the number of joint angles in the associated skeleton structure there are and joint angles in the associated skeleton structures of the msr berkeley mhad and databases respectively ing the segment size and vice versa nevertheless determining a proper window size is important a very large window size results in poorer time resolution that yields over smoothed statistical information about the underlying time series data whereas a really short window size results in unreliable i e noisy statistical information that degrades the quality of the representation therefore a window size that matches or is reasonably proportional to the duration of atomic action units under consideration should be sought for the best performance of the feature representation for the set of databases we examine in this paper our experimental results suggest that a window size of or ms in general yields the optimal performance for all segmentation based feature representations i e smij hmij and hmw which is further discussed in the remainder of this section next we can observe in figs and that the recognition results improve when using more than the single most informative joint n the blue lines both solid and dotted in the smij and hmij plots indicate lower classication rates for n with respect to the other different colored lines which represent the classication results for n another observation common to the plots in figs and is that svm based classication results solid lines are in general better than the nn based classication results dotted lines in all plots note also that the overall performance of hmij is usually worse than that of smij such performance is expected since hmij does not capture the temporal ordering of the sequence of the ordered joints and therefore loses discriminability for hmw we see that classication results based on k medoids clustering outperforms those based on k means clustering table a summarizes the best classication performances attained by different feature types on different datasets using different classication methods all extracted from fig similarly table b shows the best classication performances extracted from fig note that the ldsp results are identical in both tables since ldsp features do not depend on partitioning of the skeleton data the pair of numbers in parenthesis noted together with the classication rates for smij and hmij indicate the number of the most informative joints n and the number of segments ns respectively at which the corresponding classication results are achieved similarly the number in parenthesis provided together with the classication rate for hmw indicate the number of segments at which the corresponding classication result is achieved the best classication performance is obtained for different values of n for different number of segments or for different length temporal windows for different datasets as shown in tables a and b however the best classication rates are achieved mostly when using segments for the xed number of segments approach or ms windows for the xed temporal window approach this observation is also in accordance with our previous discussion about determining the temporal window size on the other hand there is a risk of over tting for the smij representation since the number of classication parameters increases as n or the number of segments ns increases while the amount of training data remains the same in general the best classication results are obtained by the smij representation for berkeley mhad and datasets and by the hmij representation for the msr dataset for both of the aforementioned segmentation methods the smij features perform worse than the other reference features in the msr dataset due to its low frame rate at the capture time and more noisy skeleton data more importantly the classication results obtained by the xed temporal window approach are in general better than the ones obtained by the xed number of recall from section that they are on the contrary obtained by lds modeling of the entire joint angle time series data as a whole classication results for the xed number of segments approach for data segmentation the plots in different columns correspond to different feature representations i e smij hmij and hmw from left to right respectively the plots in different rows are based on different datasets i e berkeley mhad and msr from top to bottom respectively in all plots the vertical axis is classication performance and the horizontal axis is the number of segments ranging from to with a step size of for the smij and hmij plots different colors represent different number of the most informative joints n included in the representation for the hmw plots different colors represent different clustering methods i e k means and k medoids for all plots the solid lines demonstrate the svm based classication results whereas the dotted lines demonstrate the nn based classication results note that the hmw plots show the mean and standard deviation of the classication rates achieved over runs as explained in section segments approach this observation conrms our hypothesis stated in section and renders the smij representation based on the xed temporal window segmentation method more exible and easily applicable in practice there are still two critical factors that remain to be addressed among different databases in the future one of the factors is the properties of the underlying acquisition systems used to collect the action data specically the berkeley mhad and databases are obtained using accurate high frame rate motion capture systems from which it is possible to extract clean and smooth skeleton data on the contrary the msr database is obtained by an early version of the microsoft kinect device and therefore the skeleton data extracted from the depth data is not as accurate or smooth as the skeleton data obtained from a motion capture system and has low frame rate thus the classication performance of any feature representation suffers from high noise existing in the skeleton data the other critical factor that remains to be addressed among different databases is the set of actions they contain which eventually impacts the classication performance in order to further investigate the reasons behind the poor performance of the smij feature representation on the msr dataset in addition to the noise effects we examine the confusion matrix of the svm based classication which yielded the best performance as using the smij features on the standardized msr dataset with n and ms window presented in table b the confusion matrix in table shows recognition rate for of the actions out of almost all of which are based on a basic single arm motion such as high arm wave horizontal arm wave forward punch tennis swing and such the most classication results for the xed temporal window approach for data segmentation the organization of the gure is the same as fig except that the horizontal axis now represents temporal window size in milliseconds taking on values from the set 200 informative joints in all of these actions are the elbow and the shoulder of the corresponding arm the proposed feature representation smij is however a coarse representation of the action based on simple measures calculated from a set of spherical joint angles extracted from the skeleton structure for instance the smij representation of waving your arm up in the air will be very similar to the smij representation of swinging your arm along your body due to this reason the rst seven actions in the msr dataset are classied as one of the more dominant actions such as draw tick or draw circle if we exclude the classication rates when we compute the overall classication performance of the smij features we see that the average classication rate is actually around which is almost double the initial classication rate for the sake of completeness we also include the confusion matrices of the svm based classication which yielded the best performance using the smij features for the standardized berkeley mhad database as with n and ms temporal window and for the database as with n and ms temporal window in tables and respectively for the database the actions grab high hop both legs throw basketball and throw are the least distinguishable actions for the smij features since the set of the most informative joints and the order of their activations along time are roughly the same similarly for the berkeley mhad database the most similar actions with respect to the smij features are the punch wave two hands and throw actions because these three actions depend on the motion of the two arms action recognition across databases in our second set of experiments we tested the performance of the aforementioned feature representations in a cross database recognition scenario where a classier is trained on one dataset and tested on another cross database validation represents a challenging task that requires examining generalization of the proposed feature representations across different conditions of the data acquisition cross database experimentation is often ignored by the community due to several open research questions recently torralba and efros analyzed in several examples of popular object recognition datasets and showed that training on specic data collections creates biased results which limit the we have proposed a very intuitive and qualitatively interpretable skeletal motion feature representation called sequence of the most informative joints smij unlike most feature representations used for human motion analysis which rely on sets of parameters that have no physical meaning the smij representation has a very specic practical interpretation i e the ordering of the joints by their informativeness and their temporal evolution for a given action more specically in the smij representation a given action sequence is divided into a number of temporal segments within each segment the joints that are deemed to be the most informative are selected the sequence of such most informative joints is then used to represent an action in this paper we extended our original work to provide more detailed description of the proposed feature representation and provided a comprehensive analysis of the recognition performance of different feature representations based on their action classication performance we showed that the intuitive and qualitatively interpretable feature representation smij performs better than the other reference feature representations i e hmij hmw and ldsp in action recognition tasks on three different datasets in two different experimental settings in addition we demonstrated the power of the smij feature representation in a cross database experiment which resulted in relatively high recognition rates one of the limitations of the smij representation that remains to be addressed is its insensitivity to discriminate different planar motions around the same joint the joint angles are computed between two connected body segments in spherical coordinates thus capturing only a coarse representation of the body conguration this limitation is apparent especially in the msr dataset as shown from our experiments the smij representation could be further extended by using an alternative joint angle description e g euler angles exponential maps and choosing the measure of informativeness o accordingly abstract recent developments in low cost cmos cameras have created the opportunity of bringing imaging capabilities to sensor networks various visual sensor platforms have been developed with the aim of integrating visual data to wireless sensor applications the objective of this article is to survey current visual sensor platforms according to in network processing and compression coding techniques together with their targeted applications characteristics of these platforms such as level of integration data processing hardware energy dissipation radios and operating systems are also explored and discussed keywords visual sensor networks embedded systems vision platforms image acquisition system integration ieee ieee embedded system interfaces introduction wireless sensor networks wsns are becoming a mature technology after a decade of intensive worldwide research and development efforts wsns primary function is to collect and disseminate critical data that characterize physical phenomena around the sensors availability of low cost cmos cameras has created the opportunity to build lowcost visual sensor network vsn platforms able to capture process and disseminate visual data collectively emerging applications such as visual surveillance and vehicle traffic monitoring can be enriched with visual data b tavli k bicakci tobb university of economics and technology ankara turkey e mail btavli etu edu tr k bicakci e mail bicakci etu edu tr r zilan j m barcelo ordinas technical university of catalonia barcelona spain r zilan e mail rzilan ac upc edu j m barcelo ordinas e mail joseb ac upc edu multimed tools appl high resolution images and videos require complex compression and coding algorithms and high bandwidth usage these requirements imply that visual sensor nodes dissipate significantly higher power than scalar sensor nodes furthermore sensors used in relaying data traffic have to be capable of buffering large number of packets to reduce the bandwidth utilization visionprocessing techniques capable of reducing the amount of traffic by intelligently manipulating the raw data can be used at the cost of allocating more computation resources main motivations of this study are two folds first we discuss and compare vsn platforms based on processing and vision computing techniques vsns are distinguished from wsns by their capability to present information rich visual data however effective use of visual data depends on the use of appropriate processing techniques there are differences in the techniques implemented or realizable in current vsn platforms because technical challenges involve many design trade offs and decisions are usually enforced by hardware limitations and energy constraints second we investigate the capabilities and limitations of vsn platforms in detail we believe that protocol algorithm and application development for vsns cannot be of practical use unless the underlying enabling technologies and platforms are well understood other surveys such as give a more general perspective on vsns we refer readers interested in specific vsn topics which are not in the scope of our survey to other relevant surveys in the literature written on various aspects of vsns general overview challenging issues multimedia streaming security cross layer design the rest of this paper is organized as follows section presents the reasons for designing special platforms for vsn applications section provides an overview of image compression video coding and vision computing techniques used in vsn platforms section presents an overview of the currently available vsn platforms section categorizes these platforms according to integration level data processing hardware energy dissipation radios operating system and software architecture hierarchy and applications we present a critical evaluation of vsn platforms and discuss open problems in section section presents conclusions of this survey platform design for vsns the main differences between wsns and vsns are in acquiring processing and transferring data visual data requires higher bandwidth usage due to the amount of data to be transmitted and higher power consumption due to the complexity of coding and vision processing algorithms and high volume data acquisition the spectrum of wsn platforms ranges from lightweight platforms e g through intermediate platforms e g yale xyz to pda class platforms e g intel stargates table presents the features of several wsn platforms lightweight platforms e g micaz and telos are highly resource constrained thus they are only suitable for simple sensing and detection tasks the yale xyz platform which is a typical example of intermediate platforms has more memory and processing resources than the lightweight platforms at the higher performance set there is pda class platforms e g stargates which are more powerful than the intermediate platforms but also consume more power vision computing techniques can reduce the amount of visual data to be transmitted at the cost of more computation to illustrate this fact let us consider an application that controls registration plates of vehicles traveling in an urban area such application is useful for the implementation of local administration policies to reduce the amount of pollution by alternating the circulation of vehicles based on the parity of their number plates this kind of a policy has been introduced for the beijing olympic games it is possible to use a multimed tools appl table comparison of wireless sensor network platforms mote microcontroller data memory storage memory radio data rate bit kb kb kbps bit kb kb kbps micaz atmega128l bit kb kb kbps tmote sky imote bit kb 024 kb kbps bit kb zeevo kbps xyz oki bit kb mb ram kb kbps stargate intel xscale bit mb mb ieee mbps kbps vsn for monitoring the vehicle plates in three different ways a capturing real time video and sending the data to the base station which requires real time video compression and large bandwidth b taking still images of the vehicles and sending the images to the base station which requires image compression and vision computing i e detection of vehicles c extraction of the plate numbers from the captured still images and sending these to the base station which requires sophisticated vision computing direct utilization of wsn platforms for vision applications is not feasible due to several facts itemized as follows lightweight wsn platforms are not capable enough in terms of computational power and memory for video acquisition and vision computing tasks hence direct use of these platforms for vsn applications is not feasible furthermore the radios used with lightweight platforms do not have enough bandwidth to support streaming video with reasonable quality intermediate wsn platforms e g xyz node have better computational capabilities however they are also equipped with low bandwidth radios e g ieee compliant thus real time video streaming is also not possible with these platforms the alternative for real time video streaming relies on the use of ieee radio networks that can support bandwidths of an order of magnitude higher than ieee pda class platforms equipped with ieee radios are suitable for real time video streaming due the highly capable processors and large memory available on board however the energy dissipations of these devices are more than an order of magnitude higher than the energy dissipations of lightweight platforms e g meerkats furthermore the energy consumption for communication also increases with the bandwidth e g consumes roughly mw and ieee radios consume more than w combination of powerful enough and energy efficient hardware with in network processing techniques is mandatory for vision based applications processing costs given as joules per instruction are approximately two to three orders of magnitude lower than communication costs that are given as joules per bit on available embedded platforms such as and yale xyz however there are numerous processing options to choose from as the vehicle plate monitoring application discussed above exemplified next we multimed tools appl will review image compression video coding and vision computing techniques with an emphasis on their use in vsn platforms compression coding and vision computing in vsn platforms vision is the most important functionality provided by vsns efficient and effective utilization of visual data depends on the intelligent use of vision computing techniques along with image compression and video coding because of energy efficiency considerations vsn platforms are designed with limited hardware capabilities hence vision computing in vsn platforms is highly constrained and very challenging in this section we provide an overview of coding and compression algorithms together with in network processing techniques in the context of vsn platforms and applications in the next section while providing a description of each vsn platform in detail we indicate targeted application for each of these platforms together with information on processing techniques used compression and coding transform coding also known as intra coding e g discrete cosine transform dct and wavelet transforms dwt is a method used for lossy compression of still images and video intra coding typically achieves poor compression for video since it does not exploit temporal correlation although it has high robustness for video compression transform coding is combined with motion compensated prediction inter coding one of the most used techniques is block based predictive motion compensation technique inter coding achieves high compression at the cost of high complexity this means that one or more frames should be stored at the encoder and the decoder furthermore predictive motion compensation suffers from sensitivity to synchronization between coder and decoder error prone wireless channels often cause packet losses affecting the integrity of data and inducing prediction mismatches thus the use of predictive motion compensated coding techniques necessitates powerful computation and ample storage capabilities which are not well suited for vsn platforms a full search block motion estimation algorithm incurs around operations per pixel per second for a fps frames per second video as reported in to illustrate the communication and computation energy dissipation terms we use the example presented in processing with qcif resolution and pixel coding would cost the energy equivalent of transmission of approximately kb and processing with cif resolution and bit pixel coding would cost the energy equivalent of transmission of approximately kb color provides multiple measurements at a single pixel of the image the bit rgb encoding uses bytes for each basic color red green blue enabling colors other schemes use bit rgb bits for each color component or bit rgb with emphasized green sensitivity ycbcr encoding typically used in jpeg and mpeg coders allows a better representation for image storage and transmission the amount of data to be sent becomes larger as the color coding technique becomes richer for example a sensor using cif resolution with kb flash memory can store bit black and the energy cost of transmitting kb of data to a distance of m assuming raleigh fading channel bpsk modulation ber and fourth power distance loss is joules which is approximately the same energy cost as of executing million instructions by a million instructions per second mips w processor multimed tools appl white frames bit color frames or 58 bit color frames there is thus a clear trade off between sensor capabilities i e buffering computation and energy and resolution coding compression and video frame rate see fig most of the cameras and imagers allow the choice of a wide range of resolutions and color encodings table shows image resolutions and encoding and table shows compression algorithms used in vsn platforms below we provide a general discussion and comparison of vsn platforms based on compression and coding techniques detailed descriptions of vsn platforms are specified in section the behavior of jpeg compression is investigated in most of the platforms for example in it is shown that by using the firefly mosaic platform the jpeg processing time varies little with the quality level but greatly with the resolution of the image other platforms like panoptes are designed with more powerful hardware intel arm mhz and able to support more complex compression algorithms panoptes designers compare a standard jpeg compression algorithm chendct with their own algorithm optimized for the panoptes platform they showed that optimized compression could be performed with significantly lower energy i e they reported only execution times but assuming similar power per unit time longer execution time implies more energy dissipation cyclops designers also report the implementation of their own jpeg compression algorithm showing that their jpeg implementation consumes only j to compress the test images while the standard integer implementation from the independent jpeg group takes j to compress the image with the same amount of peak signal to noise ratio performance cyclops panoptes meerkats firefly mosaic citric and vision mote see table use jpeg or modified jpeg compression algorithms to perform intra frame coding no data compression or coding results are reported for either mesheye or micreleye platforms there is an evident need of low complex and high efficient image compression mechanisms able to offer acceptable qos in terms of power rate distortion parameters or different aer image emulators are used other quality metrics the only prototypes that implement and report a scheme different than jpeg compression or its versions are xyz aloha and citric citric implements compressed sensing a new approach for simultaneous sensing and compression explained in this subsection xyz aloha uses aer address event representation aer only outputs a few features of interest of the visual scene by detecting intensity differences motion information these sensors are sensitive to light and motion so that only pixels that include the brightest one generate the events first and more periodically than others thus only pixels which realize a difference in the light intensity generate the events since the sensor sends metadata and not images there is a great saving in number of bits and thus in energy there are alternatives other than modifying standard jpeg compression such as distributed video coding dvc based on slepian wolf and wyner ziv multimed tools appl theorems dvc exploits the possibility to compress two statistically dependent signals using a separate encoding and a joint decoding approach which is different from the classical predictive encoding that jointly encodes and decodes this approach moves the complexity from the source node to the end node giving room to design low complexity and low power sensors at the cost of the sink node as reported in 37 sink complexity may be a drawback if high node density sensor networks are deployed in terms of how many sensors such a sink could support and what dvc compression efficiency may be achieved with low encoding complexity nevertheless the potential benefits of dvc in vsns are due to the higher coding efficiency allowing a reduction in transmission rates b low power consumption c improved error resilience d multi view correlation exploitation none of the existing vsn platforms and prototypes introduced in section has implemented and tested dvc however at the academic research level there is a growing base of information about the use of dvc in vsns for instance prism aims to bring block motion block dct to the decoder in the use of lapped biorthogonal transform lbt instead of dct dwt discrete cosine wavelet transforms and golomb multiple quantization mq methods instead of huffman coding or arithmetic coding is proposed to further reduce the compression complexity the authors propose sharing the processing tasks between nodes belonging to the same cluster the result is a low complexity and lowmemory demanding image compression algorithm that reduces hardware cost and energy consumption in a dvc scheme is proposed for vsns which exploits the correlations among video frames from adjacent nodes via robust media hashing the global motion parameters are estimated by the decoder the sink and sent to the encoders compressed sensing or compressive sampling cs 26 aims to compress data before storing the whole information to be compressed cs works on the sparseness representation of the signal instead of acquiring the whole signal the complete set of coefficients are computed the largest ones are encoded and the rest of the coefficients are discarded cs assumes data is in y ax form where a is a kxn sensing sparseness matrix k n cs guarantees that for a certain matrix a x may be reconstructed from y whenever x is compressible in some domain vector y is easy to compute even in a distributed manner has small size and does not require a prior knowledge about the data being therefore a good candidate for compressing in vsn applications distributed compressed sensing dcs exploits both intra and inter signal correlation structures and is based on the concept of joint sparseness of a signal ensemble compressive wireless sensing cws reconstructs the sensed data from noisy random projections of the sensor network data and obtains good power distortion trade offs given the lack of prior knowledge about the signal network coding can help to minimize the amount of energy required per packet furthermore the use of joint network coding and distributed source coding to send multiple correlated sources to a common receiver is also proposed cs and other related techniques are promising emerging techniques for coding and compression of data at this moment current platforms and prototypes focus on well known intra frame compression algorithms and rely on in network processing for data reduction an exception is citric platform where image compression via cs is implemented and tested in citric cs implementation different sensors measure sparse but correlated signals and send them to a sink that is able to jointly reconstruct all the signals precisely data processing in vsn is more computationally intensive than scalar sensors that makes energy dissipation for video coding compression an important feature to be analyzed and researched in scalar sensors there is a clear objective in analyzing connectivity energy consumption transmission and reception of individual sensors and global network lifetime in terms of protocol energy performance nevertheless visual sensors consume higher data multimed tools appl processing energy than scalar sensors and therefore there is a need to optimize energy consumption for data processing in individual visual sensors in this sense an analytical model is introduced to characterize the relationship between encoding power consumption and its quality performance the objective is to extend the traditional ratedistortion analysis to include energy constraints rate distortion and time distortion trade offs also are evaluated in citric where it is shown that cs time distortion tends to become zero while jpeg needs a residual computation time of ms per image for the lowest distortion performance incorporating power consumption and rate distortion analysis to video sensors is a promising research avenue since most of the current studies related to video coders in vsns are using intra and inter frame compression techniques the next subsection is dedicated to in network vision computing mechanisms vision computing techniques aimed to manipulate image content and thus allowing the transmission of a reduced amount of information are generally named as in network processing techniques within vsn domain most of these techniques come from the vision computing field vision computing can reduce the amount of data to be sent to the sink well known visual computing techniques such as object recognition or object tracking can reduce the amount of raw data to be transmitted although there are many techniques for object recognition depending on the applications general object recognition methods can be categorized as in fig object event detection and recognition can be done by using the identified features of the object object tracking can be done by localization algorithms e g active contour based feature based and model based vision computing techniques implemented in vsn platforms are summarized in table it is important to mention that most of these techniques require powerful processors and large memory resources if they are not customized for implementation in vsn platforms vision computing techniques optimized for vsn platforms are implemented tested and fig general object recognition methods multimed tools appl reported for most of the vsn platforms here similar to what we have done for compression and coding general overview of vision computing in vsn platforms is provided most of the platforms utilize background subtraction frame differencing and edge detection as the main in network processing techniques background subtraction is based on extracting the object of interest from the image and removing the background it must be robust against changes in illumination and should avoid detecting non stationary background objects moving leaves rain snow etc moving average filters smooth backgrounds with illumination changes or moving objects frame differencing detects changes between two consecutive frames differencing the current frame with a background frame averaged over past frames adds robustness against illumination changes for example meerkats cyclops mesheye micreleye and citric perform background subtraction and frame differencing as object detection mechanisms micreleye takes a fixed background frame at the beginning and then performs pixel by pixel differencing between each frame and the background frame cyclops use moving average filters to smooth background changes firefly mosaic uses a gaussian mixture model gmm to separate foreground from background gmm is able to detect objects even when the object stands still edges could be considered as boundaries between dissimilar regions in an image computation of edges are fairly cheap and recognition of an object is easy since it provides strong visual clues however edge detection can be affected by the noise in an image although there are several different methods to perform edge detection generally they can be categorized into two classes a gradient based e g sobel algorithm b laplacianbased while the first one detects the edges by looking for the maximum and minimum in the first derivative of the image the second one searches for zero crossings in the second derivative of the image to find edges cyclops supports sobel libraries to perform edge detection delay incurred by edge detection as a function of image size is investigated for xyz platform for example bit sobel edge detection lasts respectively ms ms ms and ms for and resolutions however the use of an fpga block can decrease delay at reasonable energy costs the results reported on energy dissipation and efficiency of the vision algorithms suggests that vision computing is an essential component of the vsn paradigm it is shown in that processing energy dissipation is no longer negligible and is comparable to even higher than the energy dissipation for networking meerkats performs object detection based on background subtraction that simply detects motion taken between two snapshots at different times nodes wake up periodically perform object detection and send a wakeup message to neighbors which in response start taking pictures this duty cycle method consumes more batteries than using the n tier architecture proposed in power consumption and performance of object classification tasks are reported for micreleye the authors implement three hardware and software architectures serial parallelized and optimized and demonstrated a power consumption of mw at fps for the serial implementation mw at fps for the parallel implementation and mw at fps for the optimized implementation showing that simple in network processing tasks can be performed at moderate frame rates energy consumption will be one of the main challenges in the design of future in network processing techniques exploiting spatial correlation between the images obtained by neighboring nodes can reduce power consumption collaborative image coding computes differences between neighbor camera frames and local frames then using edge detection e g sobel operator dominant features that are to be sent to the sink are computed before initiating the process it is necessary to obtain the background to later reconstruct the image where synchronization of the cameras is the critical part experiments in are multimed tools appl performed using an intel strongarm sa as processor and as transceiver an application in which after an initial training phase multiple overlapping cameras merge regions and collaborate in object activity detection is proposed for firefly mosaic platform however the trade off between the increase in signal processing for target detection and the transmission of data in light weight sensors still remains as an active research issue collaborative image coding is a promising technique for vsns however it has also drawbacks in networks with scalar sensor nodes the sensing area and the transmitting node coverage area are basically the same in visual sensor networks sensing nodes are cameras with a fov and a dov depth of view fov is the maximum volume visible from a camera while dov is the amount of distance between the nearest and farthest objects that appear in an image overlapping cameras can cover the same sensing area depending on the fov and the dov however if the transmission radius is not large enough then vsn nodes with correlated data cannot directly reach each other in this situation collaborative image coding needs specific sensor coordination algorithms to reach out of range nodes vsn platforms in this section we present an overview of nine available vsn platforms there are three criteria we use to select the vsn platforms described below first all platforms have local processing capabilities they do not transmit visual data in raw form second they all operate with battery not with power adapters third all nine platforms selected are sufficiently characterized for instance information on energy dissipation characteristics and in network processing techniques is reported due to the several options with respect to integration level as discussed in subsection there is no general architecture which all platforms comply to however we observe that all platforms share a similar set of hardware components a typical vsn platform consists of at least four building blocks a microcontroller a memory on board and or external imager and wireless transceiver built in or integrated and is powered with a limited battery resource in this section we aim to present all vsn platforms which have the aforementioned physical attributes and are well characterized e g energy dissipation characteristics are provided nevertheless in addition to the nine platforms we presented in detail we also presented several other platforms in section vsn platforms presented in this section are similar not only in their physical attributes but also in their capabilities and operation typically vsn platforms capture visual data from the environment process the data locally and then transmit processed data via its wireless transceiver to a sink either directly or over relays with multihop communication the visual data can be either still image or video the ability to process the visual data is crucial for vsns because sending raw data is usually not a viable option due to the power and bandwidth constraints vsn platforms are equipped with radios that can both transmit to and receive from the base station hence processed visual data can be transmitted to the sink either with a push model e g event detection triggers the visual data transfer or a pull model e g information is requested by the sink with a broadcast query message however depending on the application one particular model can dominate the other for example in a surveillance application push model is the dominant mode of operation detection of a moving object triggers vsn nodes to transmit data to the sink node in table we present push pull model of operation for each platform based on the test cases reported multimed tools appl in the following subsections we organize the information provided for each vsn platform under four headings and strive for providing content consistently as much as possible however the goal of consistency can not always be achieved due to unavailable information and missing details in relevant publications main structural attributes of the platforms are summarized in table compression coding and in network processing techniques used in vsn platforms together with information on targeted applications are presented in table cyclops the information for cyclops is organized under the following headings hardware architecture of cyclops is given in fig cyclops is designed to operate with mote a wsn platform from crossbow technology the imager is a cif resolution camera the mcu is an bit processor which is responsible for controlling cyclops communicating with and performing image inference in addition to limited amount of internal mcu memory there is an external sram kb as well as an external flash mb image capture is performed by the cpld complex programmable logic device software protocols cyclops firmware is written in nesc language the operating system is tinyos which provides clear interfaces and standard scheduler and services cyclops software consists of three set of components drivers libraries and the sensor application drivers are components used for interfacing the mcu with the peripheral subsystems e g cpld drivers there are two different classes of libraries primitive structural analysis libraries e g matrix manipulation libraries statistics and histogram libraries and high level algorithmic libraries e g background subtraction object detection motion vector recognition libraries the sensor fig hardware architecture of cyclops multimed tools appl 726 application orchestrates the overall operation of cyclops and provides a high level interface with the host cyclops sees the external world through the active eye layer module which is the cyclops image capture and control stack works on a tinyos networking stack released in tinyos applications and processing techniques cyclops is used in object detection and hand posture recognition applications detection of a moving object within the field of view fov is a critical task in many sensing applications for this purpose cyclops periodically captures images and reconstructs an estimate of the stationary background by using a moving average methodology the foreground object is detected by taking the difference between the instantaneous image and the estimated background experimental evaluations reported that cyclops achieves detection efficiency hand posture detection is important for human computer interaction and it is a well known pattern recognition problem cyclops converts a set of training images into feature vectors by using an orientation histogram transformation which gives robustness to illumination variations and provides translational invariance the performance of the algorithm is tested by using static hand gestures form a subset of american sign language which resulted in successful recognition notable features and or functionalities cpld is used as a lightweight frame grabber because image capture requires faster data transfer and address generation than what a lightweight processor can provide cyclops is designed for maximum energy efficiency and for this purpose any hardware block that is not needed to be active during a particular operation is automatically relaxed to its lower power state for example cpld clock is halted by the mcu when its services are not required likewise sram is kept in sleep state when the memory resources are not needed mesheye the information for mesheye is organized under the following headings hardware figure illustrates the hardware architecture of mesheye which is a single board fully integrated vision sensor platform designed with two low resolution imagers and one highresolution camera the main processor of mesheye has kb sram and kb flash memory for image buffering and storage there is an mmc sd flash memory card of mb size the wireless communication module is a radio software protocols radio implements the ieee standard mesheye does not have an operating system resource management and process scheduling are performed through fsm based approaches details of which are not provided on the other hand two data link protocols are implemented and reported the first protocol is a simple data ack protocol where each new data packet is sent upon reception of an ack from the receiver for the previous packet and in the lack of an ack retransmission occurs upon the expiration of a multimed tools appl 726 fig hardware architecture of mesheye certain timeout in the second protocol only the first and last packets of a data stream are acknowledged by the receiver the list of all lost data packets is sent together with the last ack which is then retransmitted to the receiver at the end of the transmission applications and processing techniques the design of mesheye has been pursued with distributed intelligent surveillance application in mind the vision algorithms of mesheye consist of object detection stereo matching and object acquisition phases object detection is achieved by computing the difference of each captured frame from the estimated background the difference map is then processed by thresholding smoothing and blob filtering once the region of interest roi is detected for one imager stereo matching phase begins where the difference images of two lowresolution images are matched by correlation processing to confirm that the roi detected in previous phase is a positive match once an object is detected and stereo matched the highresolution camera takes a snapshot of the roi for further processing notable features and or functionalities low resolution imagers in mesheye architecture are used for detection of a moving object within their fovs once an object is detected the high resolution camera is triggered and starts to acquire the high resolution images from the fov such a system level design considerably improves the energy efficiency of the overall architecture without sacrificing the quality of the object detection i e high resolution imager does not dissipate energy when there is no moving object yet high resolution images can be acquired rapidly upon detection of a moving object in design of mesheye off the shelf low power components are used with standard interfaces and total number of components are minimized to keep the energy dissipation as low as possible e g flash memory card kilopixel imagers and the radio are connected to the processor through a single spi interface mesheye is specifically designed for lowpower surveillance applications due to the lack of fpga or cpld based frame capturing and buffering components in mesheye design real time streaming video is not possible multimed tools appl 726 panoptes the information for panoptes is organized under the following headings hardware hardware architecture of panoptes is illustrated in fig the panoptes platform is originally developed on an applied data bitsy board which utilizes a processor with mb of on board memory a video camera is utilized for acquisition with resolutions ranging from vga to lower resolutions the communication module is an off theshelf ieee card the system is later migrated to the crossbow stargate platform panoptes use usb 0 as the primary i o interconnect with a maximum of mbps aggregate bandwidth due to the fact that at the time of implementation low powered embedded devices do not support higher bandwidth mbps usb 0 software protocols ieee protocols are used for communication linux operating system is used due to its flexibility python is used for providing adaptive sensor functionality which is an interpreted programming language the advantage of using python is that it allows both the advantage of the speed of optimized compiled code and the flexibility of a scripting language in construct in panoptes software architecture video sensing functionality is divided into several components including capture compression filtering buffering adaptation and streaming phillips web camera interface with video for linux is used for video capture a priority buffering and adaptation scheme is developed for panoptes where data packets are handled according to the priority level and timestamp they are labeled with the application software system consists of three main components the user interface the video sensor software and the video aggregation software the user interface allows basic queries to be run on the video database along with other functionalities the video sensor software detects the motion within the fov and records the motion within the fov fig hardware architecture of panoptes multimed tools appl 726 for a user defined amount of time furthermore the recorded motion is transformed into a bitmap of active motion detected pixel blocks to allow efficient database access for future queries video aggregation software orchestrates the visual surveillance system as a whole it defines and executes visual queries by using the pre processed information provided by the available software components for example by using aggregated image bitmaps for a single event the system rapidly identifies events of interest various aspects of the panoptes platform are analyzed through direct experimentation e g usb performance compression performance component interaction energy dissipation and buffering and adaption performance applications and processing techniques panoptes platform is used in an intelligent visual surveillance application called the little sister sensor network application jpeg differential jpeg and conditional replenishment compression formats are set up on the panoptes platform the performance of off the shelf jpeg compression algorithm chendct is compared with an implementation which takes advantage of intel performance primitives for the strongarm and xscale processors filtering is used to eliminate the redundant or uninteresting image frames or video sequences the implementation is based on a brute force pixel by pixel algorithm the filtering element in panoptes allows a user to specify how and what data should be filtered notable features and or functionalities panoptes is one of the earliest video sensor network platforms designed for high quality video delivery over ieee networks with a power requirement less than w since panoptes is designed with more powerful hardware it is able to support more complex compression algorithms panoptes platform achieves approximately fps using a high quality image image resolution furthermore using gray scale images reduces the image size by meerkats the information for meerkats 27 is organized under the following headings hardware figure shows the hardware architecture of meerkats the meerkats testbed consists of meerkats nodes designated as ordinary vsn nodes and a laptop acting as a data sink meerkats nodes are built on top of a crossbow stargate platform which features a cpu operating at mhz with mb flash memory and mb ram on the main board the camera can capture vga quality image frames the communication module is an orinoco gold ieee pcmcia wireless card software protocols ieee protocols are used for communication the operating system used on the stargate platform is an embedded linux system furthermore stargate comes with built in battery monitoring capability which is extremely useful for testing the energy dissipation characteristics multimed tools appl 726 fig hardware architecture of meerkats meerkats software architecture has three main components a resource manager module b visual processing module and c communication module resource manager module oversees the overall operation of the meerkats node it controls the visual sensor and the communication subsystems energy efficiency of the platform is among the responsibilities of the resource manager which orchestrates the operation of the platform on a duty cycle basis e g the node periodically wakes up acquires image and returns back to sleep or idle mode depending on the energy dissipation and qos requirements visual processing module is the nerve center of the sensing operations of meerkats the operation of visual processing module is triggered by the resource manager which simultaneously activates the webcam visual processing module processes the acquired sequence of images for possible moving objects visual processing unit returns resource manager module a set of parameters including an event detection flag the number of moving blobs and the velocities of the blobs eventually critical data is transmitted to the sink communication module is responsible for transmitting data and control traffic towards the data sink possibly through multiple hops network layer functionality is performed by the dynamic source routing dsr protocol dsr version running on meerkats nodes is ported from the pocketpc linux dsr kernel module meerkats use both udp control packets used for synchronization and signaling and tcp data packets used for image data transmission for transport layer functionality applications and processing techniques meerkats testbed is designed for detecting and tracking moving bodies within the fov the performance metric of the surveillance application is defined as the ratio of the expected number of missed events to the expected number of bodies in the network main function of meerkats visual processing module is to detect objects of interest within the fov image acquisition image understanding image processing and data compression operations are among the duties of the vision processing module if an event multimed tools appl 726 i e moving blobs is detected through a fast motion analysis algorithm the relevant portion of the visual data is jpeg compressed notable features and or functionalities meerkats platform is designed by integrating carefully selected and readily available offthe shelf components for example stargate platform is used because it features a usb connector which is necessary for interfacing the webcam used as the visual sensor two meerkats nodes may coordinate a master slave coordination scheme for energy efficiency purposes where the master periodically scans the fov and alerts the slave via the radio link if it detects a moving object firefly mosaic the information for firefly mosaic is organized under the following headings hardware figure shows the hardware architecture of firefly mosaic firefly mosaic platform is created by integrating firefly wsn platform with a vision board featuring embedded vision processor there is also an additional board for pc interface the vision board consists of a cmos camera chip a frame buffer and an mcu the imager is capable of supporting fps in cif resolution the frame buffer is an averlogic fifo chip processing of images is performed by a bit running at mhz with kb on chip ram and kb on chip flash memory the firefly wsn nodes are equipped with a radio and an atmel bit processor software protocols provides various open source image processing and vision libraries the firefly platform utilizes nano rk real time multi tasking priority driven reservation based poweraware operating system which provides the necessary means to maintain network wide tight global synchronization such level of time synchronization is of utmost importance for synchronous image capturing and communication scheduling the communication between the imaging board and networking board is based on the serial line ip protocol inter node fig hardware architecture of firefly mosaic multimed tools appl 726 communication is achieved by using tdma based link layer architecture i e rt link collision free tdma link protocol with scheduled and routing tables constructed according to the communication networking and fov based network topology it is reported that nanork in conjunction with rt link protocol can achieve sub millisecond time synchronization the network formed by firefly mosaic nodes utilize a camera network graph cng which captures the relationships between the cameras and their fovs e g cng captures the overlapping fovs of cameras to optimize the routing within the network upon construction of the cng the tdma schedule is computed by using a two step procedure applications and processing techniques the target application scenarios for firefly mosaic include the broad field of distributed vision tasks e g assisted living applications for elderly firefly mosaic platform is used in a home activity clustering application in this application the system automatically identifies the regions within an apartment where particular activities frequently occur the end result of the application is a markov model which characterizes the transition probabilities of activity regions in this application the tdma timeline consists of ms time epochs which is the time to capture and process images the frame rate achieved is fps there are five main steps in activity clustering a the first phase is the training phase where a test object is used to determine the correlations within the camera network and the cng is built b using a gmm gaussian mixture model the foreground and background are separated and locations of the occupants are recorded over time c the gmm is processed and significant local activities for each camera is identified d local activity clusters for the same region from all relevant cameras are merged by using the cng e main activity states and their transition probabilities are determined the open source library of includes jpeg compression frame differencing color tracking convolutions histogramming edge detection connected component analysis and face detector and other processing techniques notable features and or functionalities firefly mosaic is the first vsn system to integrate multiple coordinating cameras performing local processing firefly nodes include an am receiver for external tight time synchronization which facilitates capturing synchronous images as well as scheduling appropriate communications patterns micreleye the information for micreleye is organized under the following headings hardware hardware architecture of micreleye is presented in fig the processor is an atmel fpslic soc which includes an mcu a field programmable gate array fpga with k gates and kb of onboard sram kb can be used for data and kb is reserved for program storage the external memory for frame storage is a mb sram wireless capabilities of the node are provided by the integration of bluetooth transceiver which includes a radio a baseband controller and a memory block multimed tools appl 726 fig hardware architecture of micreleye software protocols micreleye has no operating system bluetooth serial port profile is used in micreleye which allows the establishment of a virtual serial port between the transceiver and a remote device the algorithm implemented in micreleye is split between the fpga and the mcu to achieve parallelism where image processing tasks involving high speed logic and high computational power e g background subtraction sub window transfer are performed at the fpga and the higher level operations e g feature extraction support vector machine operations are performed at the mcu the part of the algorithm run on the mcu is written in c applications and processing techniques the micreleye node is used for people detection where a smart camera positioned at a critical position e g main entrance of a building discriminates between the objects within its fov i e whether the object is a human being or not the main motivation is developing an intelligent system capable of understanding certain aspects of the incoming data by performing image classification algorithms the classification accuracy is validated by using well known standard data sets and the classification results obtained are close to the results obtained by the standard svm classifiers the image stream coming from the cmos imager includes both luminance and chrominance components after a complete frame is transferred to the fpga background subtraction is performed by pixel differencing with the reference frame which can be updated as needed the roi within the frame is extracted and transferred to the internal memory and the remaining higher level operations are performed by the mcu the feature vector is extracted from the roi which is normalized to 0 interval by using a highly efficient algorithm the feature vector consists of elements the averages of the rows and the columns the feature vector is fed into a state vector machine like svm like structure called ersvm which is used to recover unknown dependencies svm is a learning from examples technique and requires a set of training data to be able classify the incoming feature vectors which is provided before the classification operation starts the end result is a binary classification of whether the feature vector describes a human being or not multimed tools appl 726 notable features and or functionalities in micreleye having both an mcu and an fpga block on the same chip eliminates the energy dissipation on the capacitive loading introduced by the inter chip pcb connections the main function of the fpga is to accelerate computationally demanding vision tasks which cannot be handled by the mcu efficiently e g image capture high speed logic sram memory access management most of the image processing tasks for detection interface between the fpslic and the transceiver the finite state machine governing the overall system operation micreleye is evaluated by using three implementation strategies a serial implementation where the mcu and fpga operations stop and wait for other operation to finish b parallel implementation where fpga and mcu operate concurrently without waiting for each other c optimized implementation where several intelligent shortcuts are introduced to improve the efficiency xyz aloha the information for xyz aloha is organized under the following headings 1 hardware hardware architecture of xyz aloha platform is given in fig xyz nodes use a processor that features an bit microcontroller the processor has kb of internal ram and kb of flash there is an additional 2 mb external ram available on the node the radio transceiver of the platform is a radio three different platforms are built on top of xyz sensor nodes with a aloha image sensor b ov camera module from omnivision c software image emulator the first of these platforms is named xyz aloha camera module can capture images at vga and qvga resolutions fig hardware architecture of xyz aloha multimed tools appl 726 2 software protocols xyz platform operates on sos which is a lightweight operating system that follows an event driven design similar to tinyos unlike tinyos sos supports the use of dynamically loadable modules xyz implemented a specialized api in sos that consists of device driver sleep mode driver radio manager and frequency manager modules ieee compliant medium access control is ported to xyz which allows xyz to interoperate with other devices running ieee applications and processing techniques the reason why xyz platform is utilized in three different configurations is to evaluate different aspects of the aer concept in the first platform xyz aloha where the imager is an aer array each element pixel of an aloha array signals its x and y coordinates to the host upon the charge on it exceeds a threshold depending on the amount of charge of the pixel induced by the incident photons which is called an event the second platform is built by integrating xyz nodes with an off the shelf camera and the third platform utilizes a software emulator of aer which takes an bit grayscale input from a usb camera and outputs a queue of events to an output file the reason for creating the third platform is to create a flexible infrastructure for rapid evaluation of new aer imager designs xyz aloha performance is tested for several application scenarios including pattern recognition e g letter recognition hand gesture recognition each element of the aloha array behaves like a sigma delta modulator and the frequency of event generation is dependent on the light intensity at each pixel hence image reconstruction from the sensor outputs require significant amount of post processing histogram reconstruction is the technique chosen for reconstruction of images from aloha outputs due to its relative simplicity the results of the experiments have shown that aer shortens the computation time and simplifies the task due to the fact that it does not require multiplications using the software aer emulator three different aer imagers are emulated a motion detector an edge detector and a centroid detector notable features and or functionalities the xyz aloha platform is designed for extracting a small set of features representing certain attributes of a scene rather than acquiring full size images with a high degree of redundancy this is a unique approach possible only with custom designed address based image sensors other vsn platforms utilize a standard cmos camera capturing a full image the sensing modality associated with the xyz aloha system is called address event representation aer which measure parameters of interest from the environment at the sensor level e g location direction of motion lighting aer concept has many interesting properties such as ultra low a few w power dissipation due to the elimination of redundant data acquisition information selectivity only the parameters of interest are sensed and privacy preservation full images are difficult to reconstruct from aer data aer is a biologically inspired communication and coding model for information flow from a sensor to a receiver indeed an address event channel is a model of the transmission of neural information in biological sensory systems an aer image sensor accumulates charge in proportion with the light incident on it and when the accumulated multimed tools appl 726 charge exceeds a threshold the address of the element aer elements are connected to a bus with a matrix topology is transmitted hence they do not need to be queried to pull information instead information is pushed to the receiver by the sensors themselves in an aer array of sensor elements sensitive to light intensity brighter sensors produce events more frequently than the others the same concept applies also for other sensory data types citric the information for citric is organized under the following headings 1 hardware figure shows the hardware architecture of citric platform which is created by integrating an imaging board designed by citric team with a well known networking platform i e tmote sky tmote sky is equipped with a bit mcu kb ram and kb flash a ieee compliant radio and 1 mb external flash memory imaging platform includes a 1 megapixel cmos imager a frequency scalable pda class cpu mb flash and mb ram the imager provides images with sxga resolution and can be scaled down to resolution the processor has kb internal sram the microphone on the board is connected to the wolfson mono audio adc fig hardware architecture of citric multimed tools appl 726 2 software protocols citric team uses tmote sky which is an off the shelf mote running tinyos nesc data transmission is achieved using ieee protocol embedded linux is chosen as os of the imaging board for rapid prototyping and ease of programming and maintenance for jpeg compression ijg library provided by embedded linux os is utilized applications and processing techniques citric is designed to support the computational demands of a wide range of distributed pattern recognition applications visual surveillance is the basic scenario considered in the design of citric as a system and the overall system design is inspired by the needs of security of a perimeter administered by a central entity the performance of citric as a vsn platform is demonstrated via three representative applications image compression target tracking and camera localization in image compression application the speed of image compression is investigated furthermore rate distortion and time distortion tradeoffs between jpeg and cs schemes are measured in the single target tracking application the foreground object is separated from the estimated background by using frame differencing thresholding and median filtering bounding boxes are computed for each foreground object identified and sent to the server possibly from multiple vsn nodes in camera localization application multiple cameras track multiple mobile targets and construct the paths traversed by the targets these tracks are used to determine the location of the cameras by using an adaptive localization method notable features and or functionalities the shared computing model is introduced with citric the client server interface enables users to access the network resources concurrently the first user of a node becomes the manager of the node and can assign different tasks to it users are able to listen to managed nodes but not assign tasks until the manager user logs out of the node usability of the system and easiness to develop new applications is one of the design goals of citric vision mote the information for vision mote is organized under the following headings 1 hardware hardware architecture of vision mote is presented in fig vision mote has mb flash and mb sdram memory an unspecified cmos camera is used for visual data acquisition 2 software protocols mote is operated on linux os opencv machine vision library based on video for linux is provided for video capture and analysis communication with the base station is realized through zigbee protocol there is also an energy control api for application developers multimed tools appl 726 fig hardware architecture of vision mote applications and processing techniques vision mote is developed for water conservatory engineering applications the goal is to acquire multi view image or video information of fov easily jpeg is used as the compression algorithm processing time for jpeg coded image is reported to be ms and ms for resolution and resolution respectively 9 notable features and or functionalities the result of an experiment which measures the wireless transmission rate is reported although in theory zigbee supports transmission rates up to kbps in practice maximum transmission rate achievable is shown to be only kbps other platforms in addition to the nine aforementioned vsn platforms there are several other platforms with imaging and wireless communications capabilities such as ecam wica and videoweb however these platforms are either not sufficiently characterized or do not satisfy our criteria to be a vsn platform e g videoweb is not battery operated in this subsection we provide summarized information for these platforms ecam is a miniature platform total size of the platform is less than 1 cm3 created by integrating a vga video camera with the eco networking platform the camera module can operate as a video camera or as a still camera with built in jpeg compression capability the image sensor within the camera is omnivision the camera module supports various image resolutions gva cif 64 and it can capture up to fps network platform includes a wireless radio nordic vlsi there are many other smart camera platforms which are not specifically designed as standalone vsn platforms a comprehensive survey of smart camera platforms is presented in 714 multimed tools appl 726 which operates at 2 ghz range and can transmit up to 1 mbps data rate ecam is designed for applications that require very small form factor platforms therefore it has very limited in node processing capabilities hence it is not in the same class with the aforementioned vsn platforms wica is a wireless smart camera based on an simd single instruction multiple data processor which is a technique employed to achieve data level parallelism wica has four main components vga color image sensor simd processor a xetal family of simd processor for low level image processing suitable for parallel processing a general purpose processor for higher level operations atmel and a communications module phillips aquis grain zigbee module developed around chipcon both processors have access to a dual port ram that enables them to share a common workspace which enables both processors to collectively use the data and even pipeline the processing of data in a flexible manner the software for is written in c and they are designed in such a way that any image processing operation performed on each image line is completed within a single clock cycle such an approach is necessary to utilize the full capacity of an simd processor wica platform is used in several applications including distributed face detection wica is promising vsn platform with its unique design and its use of an simd processor rather than an fpga chip for low level image processing operations however energy dissipation characteristics of wica i e energy dissipation measurements for the whole platform are not reported thus we do not include wica among the vsn platforms we use for platform categorization videoweb is a software reconfigurable high bandwidth wireless network architecture using ip cameras and wireless bridges and routers no on camera processing is performed in videoweb approach instead tiered processing of the video data is achieved with low level processing servers e g for detecting moving objects and high level processing servers e g for face recognition multi objective optimization on video resolution and compression quality are performed to provide insights on performance trade offs the designers of videoweb choose to use power adapters considering that for long term applications batterypowered cameras are not appropriate taxonomy of vsn platforms it is possible to categorize the vsn platforms presented in section according to different attributes they possess such taxonomies provide a better understanding of the design and operation of vsn platforms 1 integration according to the level of integration vsn platforms can be divided into three categories device level integration multiple board level integration and single board level integration in the device level integration approach a vsn platform is constructed by integrating off the shelf components e g panoptes and meerkats components of the platform are all off the shelf components which are presented in figs and the panoptes node consists of an applied data bitsy board utilizing a webcam and an ieee based networking card the meerkats node is based on the crossbow stargate platform an ieee wireless card and a webcam in multiple board level integration image acquisition and the processing subsystem and radio transceiver subsystem are separately designed and integrated a video acquisition and multimed tools appl 60 726 processing board is designed as a daughter board of the networking board cyclops fig firefly mosaic fig xyz aloha fig 9 and citric fig vsn nodes are built with integration of video acquisition and processing boards with xyz firefly and tmote sky general purpose sensor platforms respectively in single board level design all functionalities of a vsn platform including the transceiver and imager are implemented on a single board e g mesheye fig micreleye fig and vision mote fig mesheye micreleye and vision mote are built specifically for visual sensor networking on a single board the central processing units are at the center of the operation of the nodes which are directly connected to the transceiver external memory and the camera 2 data processing hardware data acquisition and processing for vsn requires computation and memory resources significantly beyond those that can be provided by most of the generic sensor platforms e g mica telos some vsn nodes are equipped with relatively powerful processors and large memory components to meet the requirements of the application demands i e panoptes and meerkats however the energy dissipations of these platforms are high several watts to achieve both energy efficiency and computational capabilities powerful enough to handle imaging operations an alternative architecture for vsn platforms has emerged e g cyclops rather than using general purpose processors for all of the image related computationally involved or high frequency operations the burden of performing low level operations are carried out directly on specially designed hardware components for example in cyclops architecture specialized operations such as on demand access to high speed clocking at capture time calculating image statistics and background subtraction is performed by a cpld rather than by the main processor visual sensor platforms can benefit from reconfigurable processing engines which include microprocessor fpga and sram since inter device pcb connections are eliminated by using such processing engines energy consumption is reduced when compared to externally connected microprocessor and fpga chips furthermore the hybrid processor fpga architecture is optimal for applications that do not need the computational power provided by higher end fpgas with high energy consumption the micreleye platform is designed with such a processing engine bit microcontroller fpga with kgates and kb sram rather than utilizing a lightweight processor for high energy efficiency e g cyclops or a high performance processor with low energy efficiency e g panoptes a middle way can be followed by using a medium weight processor with moderate energy dissipation characteristics in firefly mosaic citric mesheye and visionmote 32 bit processors with relatively higher operating frequencies are used for both image capture and wireless networking functions 3 energy dissipation while the energy consumption of wsn nodes is dominated by communication this is generally not true in vsn nodes for example it was shown that communication energy dissipation constitutes and of the total energy dissipation in telos and micaz scalar sensor nodes respectively unlike in scalar sensor nodes energy dissipation in visual sensor nodes is dominated by the computation energy rather than the communication multimed tools appl 60 726 energy for example communication energy dissipation constitutes only of the total energy dissipation of micreleye vsn platform since vision involves handling large chunks of data visual sensors are typically power hungry and vision algorithms are computationally expensive therefore the hardware that is suitable for handling such computational challenges is liable to exhaust the limited energy sources of sensors quickly unless they are configured appropriately in sensor network applications vision can only play a meaningful role if the power consumption of the nodes is not significantly increased the energy dissipation characteristics of vsn platforms described in section are collected and presented in table 2 note that we present the maximum and minimum if available energy dissipation values reported for the platforms investigated we do not include more detailed energy dissipation characteristics e g energy cost of one picture transmission since such measurements are not reported for most of the platforms cyclops energy dissipation is the lowest due to the fact that cyclops is designed specifically for energy efficiency it has a lightweight bit processor and small amount of memory suitable for low speed processing and low resolution images furthermore the whole system is designed to support the capability for automatic relaxation of each subsystem to its lower power state adaptively however we note that even the most basic image processing application dissipates more energy with lower capability processors than they dissipate with moderate capability processors the xyz aloha platform power dissipation is mostly due to the xyz sensor node operation since the aloha imaging module is not a conventional imaging system and its power dissipation is extremely low on the other hand most of the power dissipation of the firefly mosaic platform is due to the imaging subsystem i e and the combined power dissipation by the radio and the processor running the radio is less than of the total power in active mode likewise in citric when the imaging board is in idle mode less than of the energy dissipation is due to the networking platform when the imaging board is active energy dissipation of the imaging board constitutes an even larger percentage of the total energy dissipation panoptes and meerkats are designed as high end platforms with powerful computational capabilities and their power dissipation figures are approximately an order of magnitude higher than the rest of the platforms it should be noted that both of these platforms are equipped with ieee radios and capable of transmitting at least an order of magnitude larger volume of data than the ieee radios used in the lower power platforms radios there are four type of radios used in the platforms we investigated a b or c bluetooth and d ieee the node which is the host for cyclops uses radio which supports a maximum of 38 kbps raw channel rate mesheye xyz xyz aloha firefly firefly mosaic tmote sky citric and vision mote are equipped with ieee zigbee compliant or which supports kbps data rate although the data rate supported by is higher than it is still not high enough to transfer reasonable quality images in real time we note that transmission rate achievable in practice is significantly lower than the theoretical maximum of kbps a jpeg compressed qcif image requires 7 kb of memory which is more than 128 byte ieee packets note that at most 3 qcif frames can be transmitted per second with a kbps channel multimed tools appl 60 726 micreleye node is equipped with a bluetooth transceiver which is chosen for its low power consumption since bluetooth devices come with several usage profiles it is easy to utilize them in embedded applications although it is possible to reach data rates up to kbps over rfcomm standard in micreleye 4 kbps data rate is utilized nevertheless the power dissipation is approximately more than power dissipation 23 both panoptes and meerkats utilize ieee transceivers since both platforms are equipped with powerful processing hardware and are capable of generating real time video ieee 4 or bluetooth transceivers cannot cope with such a large amount of data thus ieee transceivers are the best alternative if not only for real time video streaming however the power dissipation of an ieee transceiver is more than an order of magnitude larger than the power dissipation on an ieee 4 transceiver operating system and software architecture we can classify operating systems os utilized in vsn platforms into three categories a general purpose os b os specifically designed for sensor networks and c finite state machine fsm based resource management no os each category of os has its benefits and drawbacks panoptes meerkats citric and vision mote platforms use linux operating system linux provides the flexibility to alter and modify the part of the system according to the specific needs of the applications furthermore there are many available vision computing algorithms implemented in c c e g opencv library ijg library and routing protocols available for linux e g dsr routing protocol thus for ease of programming interfacing and rapid prototyping utilization of a general purpose os e g linux is highly advantageous 4 for example in panoptes platform specific functions jpeg compression routines are implemented in high level languages e g c and natively compiled and the modularized code pieces are connected via function calls by using an interpreted programming language i e python this approach eliminates the need for recompiling all the code nevertheless using a general purpose os brings extra overhead when compared to an application specific os e g tinyos vsn platforms using a general purpose os instead of an applicationspecific os are all equipped with pda class processors thus the energy dissipation for these platforms are significantly higher than the other platforms three of the vsn platforms use application specific operating systems cyclops firefly mosaic and xyz aloha platforms use tinyos nano rk and sos operating systems respectively tinyos is an event based operating system designed to support the concurrency intensive operations required by sensor network platforms with minimal hardware requirements tinyos has a synchronous execution model where a synchronous task blocks the computational resources unless it is not preempted by an interrupt handler vision computing can take extended computation times hence in cyclops there are two processors one for image processing on the imaging board and the other is for networking on the networking board nano rk is a real time resource centric operating system which provides interfaces for globally synchronized task processing this framework supports synchronous image capturing and transmission scheduling without sacrificing the overall timing constraints sos is an operating system for mote class wireless sensor networks which follows an event driven design similar to tinyos the kernel used in sos 4 there are some real time embedded operating systems that provide a full compliance posix interface e g lynxos qnx the same functionalities as linux but with less overhead however they were not used in vsn platforms presented in this paper multimed tools appl 60 726 implements messaging dynamic memory module loading and unloading along with other services unlike tinyos which compiles a static os image sos uses dynamically loaded software modules to create a system supporting dynamic addition modification and removal of network services which supports the installation and modification of programs at runtime tinyos is the most popular os for networked embedded sensor network platforms however its use may require extra development time for example in cyclops device drivers are implemented specifically for tinyos and require extra efforts sos on the other hand is not under active development and at its web site it is recommended to use one of the more actively supported alternatives on the other hand there are many studies in the literature e g contiki that aims to design an operating system for sensor nodes having lightweight mechanisms and abstractions that provide a rich enough execution environment while staying within the limitations of the constrained devices contiki os enables sensor nodes to communicate over ip both and and has been used in a variety of projects which involves scalar sensor nodes up to our best knowledge it has not been ported to any vsn platform yet micreleye and mesheye platforms does not use an operating system instead resource management and process scheduling are performed through fsm based approaches such an approach have advantages in terms of speed and energy efficiency however the user would need to program in a hardware description language making algorithm implementation and debugging a time consuming process vision computing is an established research area however utilization of high level vision algorithms requires intensive efforts on currently available vsn platforms several architectures are introduced to utilize high level vision algorithms effectively and efficiently 9 18 wisnap wireless image sensor network application platform is a matlab based application development architecture for vsn platforms wisnap is intended to accelerate algorithm development by providing standardized and easy to use apis to interface various components e g image sensor wireless radios of a vsn platform wisnap framework currently includes device libraries for agilent adcm and adns image sensors and chipcon radio sensor data library sdlib 9 is a library of well tested and reusable nesc components that capture the fundamental high level operations similar to the well known communication abstractions in tcp ip service model this library allows a convenient toolbox for rapid development of sensor applications at nesc language hierarchy the concept of hierarchy in the context of vsns can be explained best by considering a visual surveillance application where the sensors implement three tasks in multiple tiers of a hierarchical structure object detection in the first tier object recognition in the second tier and finally object tracking in the third tier a hierarchical network structure see fig enables both cost savings and energy efficiency for aforementioned tasks it is widely accepted that for optimal energy efficient operation of a scalable vsn application the network should be structured functionally similar to the hierarchical network organization proposed in which consists of three levels tiers of platforms object detection in a visual surveillance application can be achieved by low cost and low energy dissipation lightweight vsn platforms e g cyclops uses frame differencing and background subtraction for detecting moving objects xyz aloha platform utilizes aer concept for motion detection micreleye uses an svm like technique for people detection and mesheye uses frame differencing and stereo matching for moving object detection once an object is detected second tier nodes with higher computation capabilities perform object recognition multimed tools appl 60 726 fig hierarchical organization of a vsn tier 1 platform tier 2 platform tier 3 platform lbr low bandwidth radio hbr high bandwidth radio micreleye mesheye firefly mosaic citric and vision mote platforms are capable of acting as second tier nodes the third task involves higher resolution cameras which track the object send streaming video and hand over the tracking task to next set of cameras in the path of the object panoptes and meerkats platforms are suitable for third tier utilization this example poses the main computation challenges that a vsn platform has to deal with table 4 presents a comparison of vsn platforms with respect to their typical tasks in a hierarchical network organization 7 applications there are many applications envisioned for vsn platforms table 3 lists only the applications specifically targeted by each platform application scenarios include vehicle table 4 hierarchy typical tasks and corresponding vsn platforms hierarchy typical task vsn platforms tier 1 object detection cyclops xyz aloha tier 2 object recognition mesheye micreleye mesheyea firefly mosaic micreleyea citric vision mote tier 3 object tracking panoptes meerkats a can also be used as tier 1 platform multimed tools appl 60 726 tracking applications environmental observation systems security and surveillance applications emergency response systems assisted living applications and smart homes water conservatory engineering virtual reality and telepresence systems 5 and smart meeting rooms 44 by taking a holistic view we can see that all these applications can be realized by implementing three sets of tasks a object and event detection and localization b recognition of these objects and events c tracking monitoring and real time video delivery it is of no surprise that these tasks match with those already mentioned when we introduce the concept of hierarchical network organization in previous subsection panoptes and meerkats platforms are the best matches for use in the highest tier and for real time video surveillance and object tracking purposes due to their relatively high bandwidth radios i e ieee and ample processing and storage capabilities mesheye firefly mosaic micreleye citric and vision mote platforms can be used as platforms and for object and event recognition mesheye has the lowest energy dissipation among these platforms with its multiple types of cameras on the same platform it can also be used as a tier 1 platform low resolution cameras for object detection and high resolution camera for high resolution image acquisition for object recognition cyclops and xyz aloha are two platforms most suitable as platforms for object and event detection and object localization applications in a multi tiered architecture sensor nodes which do not have the ability to capture visual information can also be used for detection purposes for instance upon sensing an event with a pir sensor a low cost sensor node can trigger a platform which takes high resolution pictures and performs complex object recognition processing pir sensors are up to two orders of magnitude more energy efficient than cameras critical evaluation and open problems direct comparison of the nine vsn platforms presented is not meaningful because as it is stated in 3 and different platforms are designed with functionalities complementing each other however we can compare platforms in the same functional class energy dissipation of platforms is roughly at least an order of magnitude higher than the rest of the platforms meerkats dissipates approximately less power than panoptes when both devices are in their highest energy dissipation mode meerkats energy dissipation at the lowest energy dissipation state is approximately less than that of panoptes in its lowest energy dissipation state the difference is due to meerkats use of more recent components e g panoptes uses applied data bitsy board and later strongarm whereas meerkats uses xscale both panoptes and meerkats use linux os and they are designed with a device level integration approach among all the platforms citric has the highest computational resources and its energy dissipation is also the highest since there is no os running on mesheye it requires special effort to reprogram this platform for different applications micreleye is also a platform without an os governing the operation of the platform mesheye and micreleye are single board designs thus they are designed with a cross layer methodology firefly mosaic energy dissipation is higher than micreleye and lower than citric firefly mosaic os nano rk and link protocol rt link are custom designed both citric and firefly mosaic platforms are designed with multiple board level integration approach vision mote citric mesheye and firefly mosaic are equipped with kbps or radios although micreleye is equipped with a bluetooth radio it is used with kbps data rate multimed tools appl 60 726 cyclops has the lowest energy dissipation characteristics among all the platforms it also has the lowest data rate radio 38 4 kbps yet it has sufficient amount of computational power for tasks envisioned for xyz aloha energy dissipation is more than double the energy dissipation of cyclops however most of the energy dissipation is due to the networking board aer concept demonstrated on xyz aloha platform has potential to reduce the energy dissipation below the cyclops energy dissipation level for operations if integrated with a more energy efficient networking board each vsn platform introduced in section 4 is designed with a consideration of specific functionalities and application see tables 3 and 4 since these platforms are mainly implemented as proof of concept and not commercialized many details that could be useful for application developers such as cost information are not available while many commercial versions of wsn platforms are available none of the vsn platforms presented in this paper can be purchased from vendors if application developers aim to decrease design effort by purchasing off the shelf components they can choose among three alternatives buying a general purpose sensor platform and attaching imagers to it this is the most appropriate choice if the focus is more on system aspects but not on hardware design 3 the designers of panoptes and meerkats 3 have preferred this approach the platforms they use utilize a webcam attached to the stargate board from crossbow technology purchasing a vision sensor platform and integrating it with a networking board firefly mosaic team has designed a sensor board named firefly having a radio and integrated it with is a vision sensor platform without networking functions available from multiple international commercial vendors for a cost of approximately us buying a general purpose sensor platform and integrating it with a custom designed image acquisition and processing board e g cyclops uses 10 and citric 6 uses tmote sky there are various interesting open problems to be studied ahead we present some of these in the rest of this section the main motivation for implementing in network processing techniques and compression coding in vsn platforms is that the complexity involved with acquiring raw images and transmitting large volumes of data impose limitations on cost scalability network lifetime and robustness of the sensor networks on the other hand in the context of vsn applications it would not be correct to assume that computation is always cheaper than communication and thus preferred hence it is important to balance the tradeoff between amount of reduction achieved with computation on vsn platforms and amount of data transmitted to the base station s in a recent study using a linear programming framework it was shown that neither compressing all data nor completely avoiding data compression achieve the longest possible network lifetime dynamic data transformation is shown to achieve significantly longer network lifetimes than the lifetimes obtained with pure strategies modeling communication computation tradeoffs as optimization problems could provide insights for the energy optimal operation of vsns but these models become more useful if they are fed by elaborate and realistic performance data obtained experimentally for vsn platforms therefore we think that more energy efficient vsns could be attained by a close cooperation between practitioners and system optimizers a promising future work on this multimed tools appl 60 726 area could be investigating whether general cost models covering both computation and communication aspects could be developed or these models should be customized for each platform separately performance results of video transfer as opposed to still images were reported only for firefly mosaic and panoptes in the experiment of assisted living application with firefly mosaic the nodes run just five days from aa batteries the reported value of the power required for panoptes is even higher an order of magnitude greater than firefly mosaic due to its use of higher bandwidth and more energy hungry based networking card these results indicate that extending the lifetime of battery powered real time video applications continues to be one of the greatest challenges that lie ahead of vsn researchers in our view maintaining privacy may be critical to the success of future vsn applications in real life settings considering that camera violates privacy more than other sensors and privacy enhancing techniques like video masking may introduce extra overhead an efficient and promising strategy is to take privacy issues into consideration not as an afterthought but from the start for instance xyz aloha could satisfy both privacy and efficiency requirements by the use of address event imagers for ease of deployment and increased flexibility of vsn platforms programming should be simple enough to be used by application developers who are experts in their domains but not in technology for achieving the ultimate goal of plug and play functionality more effort is definitely required with the next generation of programming solutions we expect to see more vsns deployed in real life high efficiency video coding hevc standard is the most recent joint video project of the itu t video coding experts group vceg and the iso iec moving picture experts group mpeg standardization organizations working together in a partnership known as the joint collaborative team on video coding jct vc the first edition of the hevc standard is expected to be finalized in january resulting in an aligned text that will be published by both itu t and iso iec additional work is planned to extend the standard to support several additional application scenarios including extended range uses with enhanced precision and color format support scalable video coding and d stereo multiview video coding in iso iec the hevc standard will become mpeg h part iso iec and in itu t it is likely to become itu t recommendation h manuscript received may revised august accepted august date of publication october date of current version january this paper was recommended by associate editor h gharavi corresponding author w j han g j sullivan is with microsoft corporation redmond wa usa e mail garysull microsoft com j r ohm is with the institute of communication engineering rwth aachen university aachen germany e mail ohm ient rwth aachen de w j han is with the department of software design and management gachon university seongnam korea e mail hurumi gmail com t wiegand is with the fraunhofer institute for telecommunications heinrich hertz institute berlin germany and also with the berlin institute of technology berlin germany e mail twiegand ieee org color versions of one or more of the figures in this paper are available online at http ieeexplore ieee org digital object identifier tcsvt video coding standards have evolved primarily through the development of the well known itu t and iso iec standards the itu t produced h and h iso iec produced mpeg and mpeg visual and the two organizations jointly produced the h mpeg video and h mpeg advanced video coding avc standards the two standards that were jointly produced have had a particularly strong impact and have found their way into a wide variety of products that are increasingly prevalent in our daily lives throughout this evolution continued efforts have been made to maximize compression capability and improve other characteristics such as data loss robustness while considering the computational resources that were practical for use in products at the time of anticipated deployment of each standard the major video coding standard directly preceding the hevc project was h mpeg avc which was initially developed in the period between and and then was extended in several important ways from h mpeg avc has been an enabling technology for digital video in almost every area that was not previously covered by h mpeg video and has substantially displaced the older standard within its existing application domains it is widely used for many applications including broadcast of high definition hd tv signals over satellite cable and terrestrial transmission systems video content acquisition and editing systems camcorders security applications internet and mobile network video blu ray discs and real time conversational applications such as video chat video conferencing and telepresence systems however an increasing diversity of services the growing popularity of hd video and the emergence of beyondhd formats e g or resolution are creating even stronger needs for coding efficiency superior to h mpeg avc capabilities the need is even stronger when higher resolution is accompanied by stereo or multiview capture and display moreover the traffic caused by video applications targeting mobile devices and tablet pcs as well as the transmission needs for video on demand services are imposing severe challenges on today networks an increased desire for higher quality and resolutions is also arising in mobile applications hevc has been designed to address essentially all existing applications of h mpeg avc and to particularly focus on two key issues increased video resolution and increased use of parallel processing architectures the syntax of hevc c ieee ieee transactions on circuits and systems for video technology vol no december is generic and should also be generally suited for other applications that are not specifically mentioned above as has been the case for all past itu t and iso iec video coding standards in hevc only the bitstream structure and syntax is standardized as well as constraints on the bitstream and its mapping for the generation of decoded pictures the mapping is given by defining the semantic meaning of syntax elements and a decoding process such that every decoder conforming to the standard will produce the same output when given a bitstream that conforms to the constraints of the standard this limitation of the scope of the standard permits maximal freedom to optimize implementations in a manner appropriate to specific applications balancing compression quality implementation cost time to market and other considerations however it provides no guarantees of end toend reproduction quality as it allows even crude encoding techniques to be considered conforming to assist the industry community in learning how to use the standard the standardization effort not only includes the development of a text specification document but also reference software source code as an example of how hevc video can be encoded and decoded the draft reference software has been used as a research tool for the internal work of the committee during the design of the standard and can also be used as a general research tool and as the basis of products a standard test data suite is also being developed for testing conformance to the standard this paper is organized as follows section ii highlights some key features of the hevc coding design section iii explains the high level syntax and the overall structure of hevc coded data the hevc coding technology is then described in greater detail in section iv section v explains the profile tier and level design of hevc since writing an overview of a technology as substantial as hevc involves a significant amount of summarization the reader is referred to for any omitted details the history of the hevc standardization effort is discussed in section vi ii hevc coding design and feature highlights the hevc standard is designed to achieve multiple goals including coding efficiency ease of transport system integration and data loss resilience as well as implementability using parallel processing architectures the following subsections briefly describe the key elements of the design by which these goals are achieved and the typical encoder operation that would generate a valid bitstream more details about the associated syntax and the decoding process of the different elements are provided in sections iii and iv a video coding layer the video coding layer of hevc employs the same hybrid approach inter intrapicture prediction and d transform coding used in all video compression standards since h fig depicts the block diagram of a hybrid video encoder which could create a bitstream conforming to the hevc standard an encoding algorithm producing an hevc compliant bitstream would typically proceed as follows each picture is split into block shaped regions with the exact block partitioning being conveyed to the decoder the first picture of a video sequence and the first picture at each clean random access point into a video sequence is coded using only intrapicture prediction that uses some prediction of data spatially from region to region within the same picture but has no dependence on other pictures for all remaining pictures of a sequence or between random access points interpicture temporally predictive coding modes are typically used for most blocks the encoding process for interpicture prediction consists of choosing motion data comprising the selected reference picture and motion vector mv to be applied for predicting the samples of each block the encoder and decoder generate identical interpicture prediction signals by applying motion compensation mc using the mv and mode decision data which are transmitted as side information the residual signal of the intra or interpicture prediction which is the difference between the original block and its prediction is transformed by a linear spatial transform the transform coefficients are then scaled quantized entropy coded and transmitted together with the prediction information the encoder duplicates the decoder processing loop see gray shaded boxes in fig such that both will generate identical predictions for subsequent data therefore the quantized transform coefficients are constructed by inverse scaling and are then inverse transformed to duplicate the decoded approximation of the residual signal the residual is then added to the prediction and the result of that addition may then be fed into one or two loop filters to smooth out artifacts induced by block wise processing and quantization the final picture representation that is a duplicate of the output of the decoder is stored in a decoded picture buffer to be used for the prediction of subsequent pictures in general the order of encoding or decoding processing of pictures often differs from the order in which they arrive from the source necessitating a distinction between the decoding order i e bitstream order and the output order i e display order for a decoder video material to be encoded by hevc is generally expected to be input as progressive scan imagery either due to the source video originating in that format or resulting from deinterlacing prior to encoding no explicit coding features are present in the hevc design to support the use of interlaced scanning as interlaced scanning is no longer used for displays and is becoming substantially less common for distribution however a metadata syntax has been provided in hevc to allow an encoder to indicate that interlace scanned video has been sent by coding each field i e the even or odd numbered lines of each video frame of interlaced video as a separate picture or that it has been sent by coding each interlaced frame as an hevc coded picture this provides an efficient method of coding interlaced video without burdening decoders with a need to support a special decoding process for it in the following the various features involved in hybrid video coding using hevc are highlighted as follows coding tree units and coding tree block ctb structure the core of the coding layer in previous standards was sullivan et al overview of the hevc standard fig typical hevc video encoder with decoder modeling elements shaded in light gray the macroblock containing a block of luma samples and in the usual case of color sampling two corresponding blocks of chroma samples whereas the analogous structure in hevc is the coding tree unit ctu which has a size selected by the encoder and can be larger than a traditional macroblock the ctu consists of a luma ctb and the corresponding chroma ctbs and syntax elements the size l l of a luma ctb can be chosen as l or samples with the larger sizes typically enabling better compression hevc then supports a partitioning of the ctbs into smaller blocks using a tree structure and quadtree like signaling coding units cus and coding blocks cbs the quadtree syntax of the ctu specifies the size and positions of its luma and chroma cbs the root of the quadtree is associated with the ctu hence the size of the luma ctb is the largest supported size for a luma cb the splitting of a ctu into luma and chroma cbs is signaled jointly one luma cb and ordinarily two chroma cbs together with associated syntax form a coding unit cu a ctb may contain only one cu or may be split to form multiple cus and each cu has an associated partitioning into prediction units pus and a tree of transform units tus prediction units and prediction blocks pbs the decision whether to code a picture area using interpicture or intrapicture prediction is made at the cu level a pu partitioning structure has its root at the cu level depending on the basic prediction type decision the luma and chroma cbs can then be further split in size and predicted from luma and chroma prediction blocks pbs hevc supports variable pb sizes from down to samples tus and transform blocks the prediction residual is coded using block transforms a tu tree structure has its root at the cu level the luma cb residual may be identical to the luma transform block tb or may be further split into smaller luma tbs the same applies to the chroma tbs integer basis functions similar to those of a discrete cosine transform dct are defined for the square tb sizes and for the transform of luma intrapicture prediction residuals an integer transform derived from a form of discrete sine transform dst is alternatively specified motion vector signaling advanced motion vector prediction amvp is used including derivation of several most probable candidates based on data from adjacent pbs and the reference picture a merge mode for mv coding can also be used allowing the inheritance of mvs from temporally or spatially neighboring pbs moreover compared to h mpeg avc improved skipped and direct motion inference are also specified motion compensation quarter sample precision is used for the mvs and tap or tap filters are used for interpolation of fractional sample positions compared to six tap filtering of half sample positions followed by linear interpolation for quarter sample positions in ieee transactions on circuits and systems for video technology vol no december h mpeg avc similar to h mpeg avc multiple reference pictures are used for each pb either one or two motion vectors can be transmitted resulting either in unipredictive or bipredictive coding respectively as in h mpeg avc a scaling and offset operation may be applied to the prediction signal in a manner known as weighted prediction intrapicture prediction the decoded boundary samples of adjacent blocks are used as reference data for spatial prediction in regions where interpicture prediction is not performed intrapicture prediction supports directional modes compared to eight such modes in h mpeg avc plus planar surface fitting and dc flat prediction modes the selected intrapicture prediction modes are encoded by deriving most probable modes e g prediction directions based on those of previously decoded neighboring pbs quantization control as in h mpeg avc uniform reconstruction quantization urq is used in hevc with quantization scaling matrices supported for the various transform block sizes entropy coding context adaptive binary arithmetic coding cabac is used for entropy coding this is similar to the cabac scheme in h mpeg avc but has undergone several improvements to improve its throughput speed especially for parallel processing architectures and its compression performance and to reduce its context memory requirements in loop deblocking ltering a deblocking filter similar to the one used in h mpeg avc is operated within the interpicture prediction loop however the design is simplified in regard to its decision making and filtering processes and is made more friendly to parallel processing sample adaptive offset sao a nonlinear amplitude mapping is introduced within the interpicture prediction loop after the deblocking filter its goal is to better reconstruct the original signal amplitudes by using a look up table that is described by a few additional parameters that can be determined by histogram analysis at the encoder side b high level syntax architecture a number of design aspects new to the hevc standard improve flexibility for operation over a variety of applications and network environments and improve robustness to data losses however the high level syntax architecture used in the h mpeg avc standard has generally been retained including the following features parameter set structure parameter sets contain information that can be shared for the decoding of several regions of the decoded video the parameter set structure provides a robust mechanism for conveying data that are essential to the decoding process the concepts of sequence and picture parameter sets from h mpeg avc are augmented by a new video parameter set vps structure nal unit syntax structure each syntax structure is placed into a logical data packet called a network abstraction layer nal unit using the content of a twobyte nal unit header it is possible to readily identify the purpose of the associated payload data slices a slice is a data structure that can be decoded independently from other slices of the same picture in terms of entropy coding signal prediction and residual signal reconstruction a slice can either be an entire picture or a region of a picture one of the main purposes of slices is resynchronization in the event of data losses in the case of packetized transmission the maximum number of payload bits within a slice is typically restricted and the number of ctus in the slice is often varied to minimize the packetization overhead while keeping the size of each packet within this bound supplemental enhancement information sei and video usability information vui metadata the syntax includes support for various types of metadata known as sei and vui such data provide information about the timing of the video pictures the proper interpretation of the color space used in the video signal d stereoscopic frame packing information other display hint information and so on c parallel decoding syntax and modied slice structuring finally four new features are introduced in the hevc standard to enhance the parallel processing capability or modify the structuring of slice data for packetization purposes each of them may have benefits in particular application contexts and it is generally up to the implementer of an encoder or decoder to determine whether and how to take advantage of these features tiles the option to partition a picture into rectangular regions called tiles has been specified the main purpose of tiles is to increase the capability for parallel processing rather than provide error resilience tiles are independently decodable regions of a picture that are encoded with some shared header information tiles can additionally be used for the purpose of spatial random access to local regions of video pictures a typical tile configuration of a picture consists of segmenting the picture into rectangular regions with approximately equal numbers of ctus in each tile tiles provide parallelism at a more coarse level of granularity picture subpicture and no sophisticated synchronization of threads is necessary for their use wavefront parallel processing when wavefront parallel processing wpp is enabled a slice is divided into rows of ctus the first row is processed in an ordinary way the second row can begin to be processed after only two ctus have been processed in the first row the third row can begin to be processed after only two ctus have been processed in the second row and so on the context models of the entropy coder in each row are inferred from those in the preceding row with a two ctu processing lag wpp provides a form of processing parallelism at a rather fine level of sullivan et al overview of the hevc standard granularity i e within a slice wpp may often provide better compression performance than tiles and avoid some visual artifacts that may be induced by using tiles dependent slice segments a structure called a dependent slice segment allows data associated with a particular wavefront entry point or tile to be carried in a separate nal unit and thus potentially makes that data available to a system for fragmented packetization with lower latency than if it were all coded together in one slice a dependent slice segment for a wavefront entry point can only be decoded after at least part of the decoding process of another slice segment has been performed dependent slice segments are mainly useful in low delay encoding where other parallel tools might penalize compression performance in the following two sections a more detailed description of the key features is given iii high level syntax the high level syntax of hevc contains numerous elements that have been inherited from the nal of h mpeg avc the nal provides the ability to map the video coding layer vcl data that represent the content of the pictures onto various transport layers including rtp ip iso and h mpeg systems and provides a framework for packet loss resilience for general concepts of the nal design such as nal units parameter sets access units the byte stream format and packetized formatting please refer to nal units are classified into vcl and non vcl nal units according to whether they contain coded pictures or other associated data respectively in the hevc standard several vcl nal unit types identifying categories of pictures for decoder initialization and random access purposes are included table i lists the nal unit types and their associated meanings and type classes in the hevc standard the following subsections present a description of the new capabilities supported by the high level syntax a random access and bitstream splicing features the new design supports special features to enable random access and bitstream splicing in h mpeg avc a bitstream must always start with an idr access unit an idr access unit contains an independently coded picture i e a coded picture that can be decoded without decoding any previous pictures in the nal unit stream the presence of an idr access unit indicates that no subsequent picture in the bitstream will require reference to pictures prior to the picture that it contains in order to be decoded the idr picture is used within a coding structure known as a closed gop in which gop stands for group of pictures the new clean random access cra picture syntax specifies the use of an independently coded picture at the location of a random access point rap i e a location in a bitstream at which a decoder can begin successfully decoding pictures without needing to decode any pictures that appeared earlier in the bitstream which supports an efficient temporal coding table i nal unit types meanings and type classes type meaning slice segment of ordinary trailing picture slice segment of tsa picture slice segment of stsa picture slice segment of radl picture slice segment of rasl picture reserved for future use slice segment of bla picture slice segment of idr picture slice segment of cra picture reserved for future use video parameter set vps sequence parameter set sps picture parameter set pps access unit delimiter end of sequence end of bitstream filler data sei messages reserved for future use unspecified available for system use class vcl vcl vcl vcl vcl vcl vcl vcl vcl vcl non vcl non vcl non vcl non vcl non vcl non vcl non vcl non vcl non vcl non vcl order known as open gop operation good support of random access is critical for enabling channel switching seek operations and dynamic streaming services some pictures that follow a cra picture in decoding order and precede it in display order may contain interpicture prediction references to pictures that are not available at the decoder these nondecodable pictures must therefore be discarded by a decoder that starts its decoding process at a cra point for this purpose such nondecodable pictures are identified as random access skipped leading rasl pictures the location of splice points from different original coded bitstreams can be indicated by broken link access bla pictures a bitstream splicing operation can be performed by simply changing the nal unit type of a cra picture in one bitstream to the value that indicates a bla picture and concatenating the new bitstream at the position of a rap picture in the other bitstream a rap picture may be an idr cra or bla picture and both cra and bla pictures may be followed by rasl pictures in the bitstream depending on the particular value of the nal unit type used for a bla picture any rasl pictures associated with a bla picture must always be discarded by the decoder as they may contain references to pictures that are not actually present in the bitstream due to a splicing operation the other type of picture that can follow a rap picture in decoding order and precede it in output order is the random access decodable leading radl picture which cannot contain references to any pictures that precede the rap picture in decoding order rasl and radl pictures are collectively referred to as leading pictures lps pictures that follow a rap picture in both decoding order and output order which are known as trailing pictures cannot contain references to lps for interpicture prediction b temporal sublayering support similar to the temporal scalability feature in the h mpeg avc scalable video coding svc extension ieee transactions on circuits and systems for video technology vol no december fig example of a temporal prediction structure and the poc values decoding order and rps content for each picture hevc specifies a temporal identifier in the nal unit header which indicates a level in a hierarchical temporal prediction structure this was introduced to achieve temporal scalability without the need to parse parts of the bitstream other than the nal unit header under certain circumstances the number of decoded temporal sublayers can be adjusted during the decoding process of one coded video sequence the location of a point in the bitstream at which sublayer switching is possible to begin decoding some higher temporal layers can be indicated by the presence of temporal sublayer access tsa pictures and stepwise tsa stsa pictures at the location of a tsa picture it is possible to switch from decoding a lower temporal sublayer to decoding any higher temporal sublayer and at the location of an stsa picture it is possible to switch from decoding a lower temporal sublayer to decoding only one particular higher temporal sublayer but not the further layers above that unless they also contain stsa or tsa pictures c additional parameter sets the vps has been added as metadata to describe the overall characteristics of coded video sequences including the dependences between temporal sublayers the primary purpose of this is to enable the compatible extensibility of the standard in terms of signaling at the systems layer e g when the base layer of a future extended scalable or multiview bitstream would need to be decodable by a legacy decoder but for which additional information about the bitstream structure that is only relevant for the advanced decoder would be ignored d reference picture sets and reference picture lists for multiple reference picture management a particular set of previously decoded pictures needs to be present in the decoded picture buffer dpb for the decoding of the remainder of the pictures in the bitstream to identify these pictures a list of picture order count poc identifiers is transmitted in each slice header the set of retained reference pictures is called the reference picture set rps fig shows poc values decoding order and rpss for an example temporal prediction structure as in h mpeg avc there are two lists that are constructed as lists of pictures in the dpb and these are called reference picture list and list an index called a reference picture index is used to identify a particular picture in one of these lists for uniprediction a picture can be selected from either of these lists for biprediction two pictures are selected one from each list when a list contains only one picture the reference picture index implicitly has the value and does not need to be transmitted in the bitstream the high level syntax for identifying the rps and establishing the reference picture lists for interpicture prediction is more robust to data losses than in the prior h mpeg avc design and is more amenable to such operations as random access and trick mode operation e g fast forward smooth rewind seeking and adaptive bitstream switching a key aspect of this improvement is that the syntax is more explicit rather than depending on inferences from the stored internal state of the decoding process as it decodes the bitstream picture by picture moreover the associated syntax for these aspects of the design is actually simpler than it had been for h mpeg avc iv hevc video coding techniques as in all prior itu t and iso iec jtc video coding standards since h the hevc design follows the classic block based hybrid video coding approach as depicted in fig the basic source coding algorithm is a hybrid of interpicture prediction to exploit temporal statistical dependences intrapicture prediction to exploit spatial statistical dependences and transform coding of the prediction residual signals to further exploit spatial statistical dependences there is no single coding element in the hevc design that provides the majority of its significant improvement in compression efficiency in relation to prior video coding standards it is rather a plurality of smaller improvements that add up to the significant gain a sampled representation of pictures for representing color video signals hevc typically uses a tristimulus ycbcr color space with sampling although extension to other sampling formats is straightforward and is planned to be defined in a subsequent version this separates a color representation into three components called y cb and cr the y component is also called luma and represents brightness the two chroma components cb and cr represent the extent to which the color deviates from gray toward blue and red respectively because the human visual system is more sensitive to luma than chroma the sampling structure is typically used in which each chroma component has one fourth of the number of samples of the luma component half the number of samples in both the horizontal and vertical dimensions each sample for each component is typically represented with or b of precision and the b case is the more typical one in the remainder of this paper we focus our attention on the typical use ycbcr components with sampling and b per sample for the representation of the encoded input and decoded output video signal the video pictures are typically progressively sampled with rectangular picture sizes w h where w is the width and sullivan et al overview of the hevc standard h is the height of the picture in terms of luma samples each chroma component array with sampling is then w h given such a video signal the hevc syntax partitions the pictures further as described follows b division of the picture into coding tree units a picture is partitioned into coding tree units ctus which each contain luma ctbs and chroma ctbs a luma ctb covers a rectangular picture area of l l samples of the luma component and the corresponding chroma ctbs cover each l l samples of each of the two chroma components the value of l may be equal to or as determined by an encoded syntax element specified in the sps compared with the traditional macroblock using a fixed array size of luma samples as used by all previous itu t and iso iec jtc video coding standards since h that was standardized in hevc supports variable size ctbs selected according to needs of encoders in terms of memory and computational requirements the support of larger ctbs than in previous standards is particularly beneficial when encoding high resolution video content the luma ctb and the two chroma ctbs together with the associated syntax form a ctu the ctu is the basic processing unit used in the standard to specify the decoding process c division of the ctb into cbs the blocks specified as luma and chroma ctbs can be directly used as cbs or can be further partitioned into multiple cbs partitioning is achieved using tree structures the tree partitioning in hevc is generally applied simultaneously to both luma and chroma although exceptions apply when certain minimum sizes are reached for chroma the ctu contains a quadtree syntax that allows for splitting the cbs to a selected appropriate size based on the signal characteristics of the region that is covered by the ctb the quadtree splitting process can be iterated until the size for a luma cb reaches a minimum allowed luma cb size that is selected by the encoder using syntax in the sps and is always or larger in units of luma samples the boundaries of the picture are defined in units of the minimum allowed luma cb size as a result at the right and bottom edges of the picture some ctus may cover regions that are partly outside the boundaries of the picture this condition is detected by the decoder and the ctu quadtree is implicitly split as necessary to reduce the cb size to the point where the entire cb will fit into the picture fig modes for splitting a cb into pbs subject to certain size constraints for intrapicture predicted cbs only m m and m m are supported have their own intrapicture prediction mode the reason for allowing this split is to enable distinct intrapicture prediction mode selections for blocks as small as in size when the luma intrapicture prediction operates with blocks the chroma intrapicture prediction also uses blocks each covering the same picture region as four luma blocks the actual region size at which the intrapicture prediction operates which is distinct from the pb size at which the intrapicture prediction mode is established depends on the residual coding partitioning that is described as follows when the prediction mode is signaled as inter it is specified whether the luma and chroma cbs are split into one two or four pbs the splitting into four pbs is allowed only when the cb size is equal to the minimum allowed cb size using an equivalent type of splitting as could otherwise be performed at the cb level of the design rather than at the pb level when a cb is split into four pbs each pb covers a quadrant of the cb when a cb is split into two pbs six types of this splitting are possible the partitioning possibilities for interpicture predicted cbs are depicted in fig the upper partitions illustrate the cases of not splitting the cb of size m m of splitting the cb into two pbs of size m m or m m or splitting it into four pbs of size m m the lower four partition types in fig are referred to as asymmetric motion partitioning amp and are only allowed when m is or larger for luma one pb of the asymmetric partition has the height or width m and width or height m respectively and the other pb fills the rest of the cb by having a height or width of and width or height m each interpicture predicted pb is assigned one or two motion vectors and reference picture indices to minimize worst case memory bandwidth pbs of luma size are not allowed for interpicture prediction and pbs of luma sizes and are restricted to unipredictive coding the interpicture prediction process is further described as follows the luma and chroma pbs together with the associated prediction syntax form the pu d pbs and pus the prediction mode for the cu is signaled as being intra or inter according to whether it uses intrapicture spatial prediction or interpicture temporal prediction when the prediction mode is signaled as intra the pb size which is the block size at which the intrapicture prediction mode is established is the same as the cb size for all block sizes except for the smallest cb size that is allowed in the bitstream for the latter case a flag is present that indicates whether the cb is split into four pb quadrants that each e tree structured partitioning into transform blocks and units for residual coding a cb can be recursively partitioned into transform blocks tbs the partitioning is signaled by a residual quadtree only square cb and tb partitioning is specified where a block can be recursively split into quadrants as illustrated in fig for a given luma cb of size m m a flag signals whether it is split into four blocks of size m m if ieee transactions on circuits and systems for video technology vol no december fig subdivision of a ctb into cbs and transform block tbs solid lines indicate cb boundaries and dotted lines indicate tb boundaries a ctb with its partitioning b corresponding quadtree further splitting is possible as signaled by a maximum depth of the residual quadtree indicated in the sps each quadrant is assigned a flag that indicates whether it is split into four quadrants the leaf node blocks resulting from the residual quadtree are the transform blocks that are further processed by transform coding the encoder indicates the maximum and minimum luma tb sizes that it will use splitting is implicit when the cb size is larger than the maximum tb size not splitting is implicit when splitting would result in a luma tb size smaller than the indicated minimum the chroma tb size is half the luma tb size in each dimension except when the luma tb size is in which case a single chroma tb is used for the region covered by four luma tbs in the case of intrapicture predicted cus the decoded samples of the nearest neighboring tbs within or outside the cb are used as reference data for intrapicture prediction in contrast to previous standards the hevc design allows a tb to span across multiple pbs for interpicture predicted cus to maximize the potential coding efficiency benefits of the quadtree structured tb partitioning f slices and tiles slices are a sequence of ctus that are processed in the order of a raster scan a picture may be split into one or several slices as shown in fig a so that a picture is a collection of one or more slices slices are self contained in the sense that given the availability of the active sequence and picture parameter sets their syntax elements can be parsed from the bitstream and the values of the samples in the area of the picture that the slice represents can be correctly decoded except with regard to the effects of in loop filtering near the edges of the slice without the use of any data from other slices in the same picture this means that prediction within the picture e g intrapicture spatial signal prediction or prediction of motion vectors is not performed across slice boundaries some information from other slices may however be needed to apply the in loop filtering across slice boundaries each slice can be coded using different coding types as follows i slice a slice in which all cus of the slice are coded using only intrapicture prediction p slice in addition to the coding types of an i slice some cus of a p slice can also be coded using interpicture prediction with at most one motion compensated prediction signal per pb i e uniprediction p slices only use reference picture list b slice in addition to the coding types available in a p slice some cus of the b slice can also be coded fig subdivision of a picture into a slices and b tiles c illustration of wavefront parallel processing using interpicture prediction with at most two motioncompensated prediction signals per pb i e biprediction b slices use both reference picture list and list the main purpose of slices is resynchronization after data losses furthermore slices are often restricted to use a maximum number of bits e g for packetized transmission therefore slices may often contain a highly varying number of ctus per slice in a manner dependent on the activity in the video scene in addition to slices hevc also defines tiles which are self contained and independently decodable rectangular regions of the picture the main purpose of tiles is to enable the use of parallel processing architectures for encoding and decoding multiple tiles may share header information by being contained in the same slice alternatively a single tile may contain multiple slices a tile consists of a rectangular arranged group of ctus typically but not necessarily with all of them containing about the same number of ctus as shown in fig b to assist with the granularity of data packetization dependent slices are additionally defined finally with wpp a slice is divided into rows of ctus the decoding of each row can be begun as soon a few decisions that are needed for prediction and adaptation of the entropy coder have been made in the preceding row this supports parallel processing of rows of ctus by using several processing threads in the encoder or decoder or both an example is shown in fig c for design simplicity wpp is not allowed to be used in combination with tiles although these features could in principle work properly together g intrapicture prediction intrapicture prediction operates according to the tb size and previously decoded boundary samples from spatially neighboring tbs are used to form the prediction signal directional prediction with different directional orientations is defined for square tb sizes from up to the sullivan et al overview of the hevc standard fig modes and directional orientations for intrapicture prediction possible prediction directions are shown in fig alternatively planar prediction assuming an amplitude surface with a horizontal and vertical slope derived from the boundaries and dc prediction a flat surface with a value matching the mean value of the boundary samples can also be used for chroma the horizontal vertical planar and dc prediction modes can be explicitly signaled or the chroma prediction mode can be indicated to be the same as the luma prediction mode and as a special case to avoid redundant signaling when one of the first four choices is indicated and is the same as the luma prediction mode the intra angular mode is applied instead each cb can be coded by one of several coding types depending on the slice type similar to h mpeg avc intrapicture predictive coding is supported in all slice types hevc supports various intrapicture predictive coding methods referred to as intra angular intra planar and intra dc the following subsections present a brief further explanation of these and several techniques to be applied in common pb partitioning an intrapicture predicted cb of size m m may have one of two types of pb partitions referred to as part and part n n the first of which indicates that the cb is not split and the second indicates that the cb is split into four equal sized pbs conceptually in this notation n m however it is possible to represent the same regions that would be specified by four pbs by using four smaller cbs when the size of the current cb is larger than the minimum cu size thus the hevc design only allows the partitioning type part n n to be used when the current cb size is equal to the minimum cu size this means that the pb size is always equal to the cb size when the cb is coded using an intrapicture prediction mode and the cb size is not equal to the minimum cu size although the intrapicture prediction mode is established at the pb level the actual prediction process operates separately for each tb intra angular prediction spatial domain intrapicture prediction has previously been successfully used in h mpeg avc the intrapicture prediction of hevc similarly operates in the spatial domain but is extended significantly mainly due to the increased size of the tb and an increased number of selectable prediction directions compared to the eight prediction directions of h avc hevc supports a total of prediction directions denoted as intra angular k where k is a mode number from to the angles are intentionally designed to provide denser coverage for near horizontal and near vertical angles and coarser coverage for near diagonal angles to reflect the observed statistical prevalence of the angles and the effectiveness of the signal prediction processing when using an intra angular mode each tb is predicted directionally from spatially neighboring samples that are reconstructed but not yet filtered by the in loop filters before being used for this prediction for a tb of size n n a total of spatially neighboring samples may be used for the prediction as shown in fig when available from preceding decoding operations samples from lower left tbs can be used for prediction in hevc in addition to samples from tbs at the left above and above right of the current tb the prediction process of the intra angular modes can involve extrapolating samples from the projected reference sample location according to a given directionality to remove the need for sample by sample switching between reference row and column buffers for intra angular k with k in the range of the samples located in the above row are projected as additional samples located in the left column and with k in the range of the samples located at the left column are projected as samples located in the above row to improve the intrapicture prediction accuracy the projected reference sample location is computed with sample accuracy bilinear interpolation is used to obtain the value of the projected reference sample using two closest reference samples located at integer positions the prediction process of the intra angular modes is consistent across all block sizes and prediction directions whereas h mpeg avc uses different methods for its supported block sizes of and this design consistency is especially desirable since hevc supports a greater variety of tb sizes and a significantly increased number of prediction directions compared to h mpeg avc intra planar and intra dc prediction in addition to intra angular prediction that targets regions with strong directional edges hevc supports two alternative prediction methods intra planar and intra dc for which similar modes were specified in h mpeg avc while intra dc prediction uses an average value of reference samples for the prediction average values of two linear predictions using four corner reference samples are used in intra planar prediction ieee transactions on circuits and systems for video technology vol no december to prevent discontinuities along the block boundaries the intra planar prediction mode is supported at all block sizes in hevc while h mpeg avc supports plane prediction only when the luma pb size is and its plane prediction operates somewhat differently from the planar prediction in hevc reference sample smoothing in hevc the reference samples used for the intrapicture prediction are sometimes filtered by a three tap smoothing filter in a manner similar to what was used for intrapicture prediction in h mpeg avc hevc applies smoothing operations more adaptively according to the directionality the amount of detected discontinuity and the block size as in h avc the smoothing filter is not applied for blocks for blocks only the diagonal directions intra angular k with k or use the reference sample smoothing for blocks the reference samples are filtered for most directions except the near horizontal and near vertical directions k in the range of and for blocks all directions except the exactly horizontal k and exactly vertical k directions use the smoothing filter and when the amount of detected discontinuity exceeds a threshold bilinear interpolation from three neighboring region samples is applied to form a smooth prediction the intra planar mode also uses the smoothing filter when the block size is greater than or equal to and the smoothing is not used or useful for the intra dc case boundary value smoothing to remove discontinuities along block boundaries in three modes intra dc mode and intra angular k with k or exactly horizontal or exactly vertical the boundary samples inside the tb are replaced by filtered values when the tb size is smaller than for intra dc mode both the first row and column of samples in the tb are replaced by the output of a two tap filter fed by their original value and the adjacent reference sample in horizontal intra angular prediction the boundary samples of the first column of the tb are modified such that half of the difference between their neighbored reference sample and the top left reference sample is added this makes the prediction signal more smooth when large variations in the vertical direction are present in vertical intra angular prediction the same is applied to the first row of samples reference sample substitution the neighboring reference samples are not available at the slice or tile boundaries in addition when a loss resilience feature known as constrained intra prediction is enabled the neighboring reference samples inside any interpicture predicted pb are also considered not available in order to avoid letting potentially corrupted prior decoded picture data propagate errors into the prediction signal while only intra dc prediction mode is allowed for such cases in h mpeg avc hevc allows the use of other intrapicture prediction modes after substituting the nonavailable reference sample values with the neighboring available reference sample values mode coding hevc supports a total of intra angular prediction modes and intra planar and intra dc prediction modes for luma prediction for all block sizes due to the increased number of directions hevc considers three most probable modes mpms when coding the luma intrapicture prediction mode predictively rather than the one most probable mode considered in h mpeg avc among the three most probable modes the first two are initialized by the luma intrapicture prediction modes of the above and left pbs if those pbs are available and are coded using an intrapicture prediction mode any unavailable prediction mode is considered to be intra dc the pb above the luma ctb is always considered to be unavailable in order to avoid the need to store a line buffer of neighboring luma prediction modes when the first two most probable modes are not equal the third most probable mode is set equal to intra planar intra dc or intra angular vertical according to which of these modes in this order is not a duplicate of one of the first two modes when the first two most probable modes are the same if this first mode has the value intra planar or intra dc the second and third most probable modes are assigned as intra planar intra dc or intra angular according to which of these modes in this order are not duplicates when the first two most probable modes are the same and the first mode has an intra angular value the second and third most probable modes are chosen as the two angular prediction modes that are closest to the angle i e the value of k of the first in the case that the current luma prediction mode is one of three mpms only the mpm index is transmitted to the decoder otherwise the index of the current luma prediction mode excluding the three mpms is transmitted to the decoder by using a b fixed length code for chroma intrapicture prediction hevc allows the encoder to select one of five modes intra planar intra angular vertical intra angular horizontal intra dc and intra derived the intra derived mode specifies that the chroma prediction uses the same angular direction as the luma prediction with this scheme all angular modes specified for luma in hevc can in principle also be used in the chroma prediction and a good tradeoff is achieved between prediction accuracy and the signaling overhead the selected chroma prediction mode is coded directly without using an mpm prediction mechanism h interpicture prediction pb partitioning compared to intrapicture predicted cbs hevc supports more pb partition shapes for interpicture predicted cbs the partitioning modes of part part n and part n indicate the cases when the cb is not split split into two equal size pbs horizontally and split into two equal size pbs vertically respectively part n n specifies that the cb is split into four equal size pbs but this mode is only supported when the cb size is equal to the smallest allowed cb size in addition there are four partitioning types that support splitting the cb into two pbs having different sizes part nu part nd part nl and part nr these types are known as asymmetric motion partitions fractional sample interpolation the samples of the pb for an intrapicture predicted cb are obtained from those of a sullivan et al overview of the hevc standard table ii filter coefficients for luma fractional sample interpolation index i hfilter i qfilter i fig integer and fractional sample positions for luma interpolation corresponding block region in the reference picture identified by a reference picture index which is at a position displaced by the horizontal and vertical components of the motion vector except for the case when the motion vector has an integer value fractional sample interpolation is used to generate the prediction samples for noninteger sampling positions as in h mpeg avc hevc supports motion vectors with units of one quarter of the distance between luma samples for chroma samples the motion vector accuracy is determined according to the chroma sampling format which for sampling results in units of one eighth of the distance between chroma samples the fractional sample interpolation for luma samples in hevc uses separable application of an eight tap filter for the half sample positions and a seven tap filter for the quartersample positions this is in contrast to the process used in h mpeg avc which applies a two stage interpolation process by first generating the values of one or two neighboring samples at half sample positions using six tap filtering rounding the intermediate results and then averaging two values at integer or half sample positions hevc instead uses a single consistent separable interpolation process for generating all fractional positions without intermediate rounding operations which improves precision and simplifies the architecture of the fractional sample interpolation the interpolation precision is also improved in hevc by using longer filters i e seven tap or eight tap filtering rather than the sixtap filtering used in h mpeg avc using only seven taps rather than the eight used for half sample positions was sufficient for the quarter sample interpolation positions since the quarter sample positions are relatively close to integersample positions so the most distant sample in an eight tap interpolator would effectively be farther away than in the halfsample case where the relative distances of the integer sample positions are symmetric the actual filter tap values of the interpolation filtering kernel were partially derived from dct basis function equations in fig the positions labeled with upper case letters ai j represent the available luma samples at integer sample locations whereas the other positions labeled with lower case letters represent samples at noninteger sample locations which need to be generated by interpolation the samples labeled j j j and are derived from the samples ai j by applying the eight tap filter for half sample positions and the seven tap filter for the quarter sample positions as follows j i ai j qfilter i b j i ai j hfilter i b j i ai j qfilter i b i j qfilter j b i j hfilter j b j j qfilter j b where the constant b is the bit depth of the reference samples and typically b for most applications and the filter coefficient values are given in table ii in these formulas denotes an arithmetic right shift operation the samples labeled f k and can be derived by applying the corresponding filters to samples located at vertically adjacent j j and j positions as follows v v qfilter v v v qfilter v v v qfilter v v v hfilter v v v hfilter v v v hfilter v v v qfilter v v v qfilter v v v qfilter v the interpolation filtering is separable when b is equal to so the same values could be computed in this case by applying the vertical filtering before the horizontal filtering when implemented appropriately the motion compensation process of hevc can be performed using only b storage elements although care must be taken to do this correctly it is at this point in the process that weighted prediction is applied when selected by the encoder whereas h mpeg avc supported both temporally implicit and explicit weighted prediction in hevc only explicit weighted prediction is applied by scaling and offsetting the prediction with values sent explicitly by the encoder the bit depth of the prediction is then adjusted to the original bit depth of the reference samples in the case of uniprediction the interpolated and possibly weighted prediction value is rounded ieee transactions on circuits and systems for video technology vol no december table iii filter coefficients for chroma fractional sample interpolation index i i i i right shifted and clipped to have the original bit depth in the case of biprediction the interpolated and possibly weighted prediction values from two pbs are added first and then rounded right shifted and clipped in h mpeg avc up to three stages of rounding operations are required to obtain each prediction sample for samples located at quarter sample positions if biprediction is used the total number of rounding operations is then seven in the worst case in hevc at most two rounding operations are needed to obtain each sample located at the quarter sample positions thus five rounding operations are sufficient in the worst case when biprediction is used moreover in the most common usage where the bit depth b is b the total number of rounding operations in the worst case is further reduced to due to the lower number of rounding operations the accumulated rounding error is decreased and greater flexibility is enabled in regard to the manner of performing the necessary operations in the decoder the fractional sample interpolation process for the chroma components is similar to the one for the luma component except that the number of filter taps is and the fractional accuracy is for the usual chroma format case hevc defines a set of four tap filters for eighth sample positions as given in table iii for the case of chroma format where in h mpeg avc only two tap bilinear filtering was applied filter coefficient values denoted as i i i and i with i are used for interpolating the and fractional positions for the chroma samples respectively using symmetry for the 8th and 8th fractional positions the mirrored values of i i and i with i are used respectively merge mode motion information typically consists of the horizontal and vertical motion vector displacement values one or two reference picture indices and in the case of prediction regions in b slices an identification of which reference picture list is associated with each index hevc includes a merge mode to derive the motion information from spatially or temporally neighboring blocks it is denoted as merge mode since it forms a merged region sharing all motion information the merge mode is conceptually similar to the direct and skip modes in h mpeg avc however there are two important differences first it transmits index information to select one out of several available candidates in a manner sometimes referred to as a motion vector competition scheme it also explicitly identifies the reference picture list and reference picture index whereas the direct mode assumes that these have some predefined values fig positions of spatial candidates of motion information the set of possible candidates in the merge mode consists of spatial neighbor candidates a temporal candidate and generated candidates fig shows the positions of five spatial candidates for each candidate position the availability is checked according to the order if the block located at the position is intrapicture predicted or the position is outside of the current slice or tile it is considered as unavailable after validating the spatial candidates two kinds of redundancy are removed if the candidate position for the current pu would refer to the first pu within the same cu the position is excluded as the same merge could be achieved by a cu without splitting into prediction partitions furthermore any redundant entries where candidates have exactly the same motion information are also excluded for the temporal candidate the right bottom position just outside of the collocated pu of the reference picture is used if it is available otherwise the center position is used instead the way to choose the collocated pu is similar to that of prior standards but hevc allows more flexibility by transmitting an index to specify which reference picture list is used for the collocated reference picture one issue related to the use of the temporal candidate is the amount of the memory to store the motion information of the reference picture this is addressed by restricting the granularity for storing the temporal motion candidates to only the resolution of a luma grid even when smaller pb structures are used at the corresponding location in the reference picture in addition a pps level flag allows the encoder to disable the use of the temporal candidate which is useful for applications with error prone transmission the maximum number of merge candidates c is specified in the slice header if the number of merge candidates found including the temporal candidate is larger than c only the first c spatial candidates and the temporal candidate are retained otherwise if the number of merge candidates identified is less than c additional candidates are generated until the number is equal to c this simplifies the parsing and makes it more robust as the ability to parse the coded data is not dependent on merge candidate availability sullivan et al overview of the hevc standard for b slices additional merge candidates are generated by choosing two existing candidates according to a predefined order for reference picture list and list for example the first generated candidate uses the first merge candidate for list and the second merge candidate for list hevc specifies a total of predefined pairs of two in the following order in the already constructed merge candidate list as and among them up to five candidates can be included after removing redundant entries when the slice is a p slice or the number of merge candidates is still less than c zero motion vectors associated with reference indices from zero to the number of reference pictures minus one are used to fill any remaining entries in the merge candidate list in hevc the skip mode is treated as a special case of the merge mode when all coded block flags are equal to zero in this specific case only a skip flag and the corresponding merge index are transmitted to the decoder the b direct mode of h mpeg avc is also replaced by the merge mode since the merge mode allows all motion information to be derived from the spatial and temporal motion information of the neighboring blocks with residual coding motion vector prediction for nonmerge mode when an interpicture predicted cb is not coded in the skip or merge modes the motion vector is differentially coded using a motion vector predictor similar to the merge mode hevc allows the encoder to choose the motion vector predictor among multiple predictor candidates the difference between the predictor and the actual motion vector and the index of the candidate are transmitted to the decoder only two spatial motion candidates are chosen according to the availability among five candidates in fig the first spatial motion candidate is chosen from the set of left positions a1 and the second one from the set of above positions b1 according to their availabilities while keeping the searching order as indicated in the two sets hevc only allows a much lower number of candidates to be used in the motion vector prediction process for the nonmerge h case since the encoder can send a coded difference to change the motion vector furthermore the encoder needs to perform motion estimation which is one of the most computationally expensive operations in the encoder and complexity is reduced by allowing a small number of candidates when the reference index of the neighboring pu is not equal to that of the current pu a scaled version of the motion vector is used the neighboring motion vector is scaled according to the temporal distances between the current picture and the reference pictures indicated by the reference indices of the neighboring pu and the current pu respectively when two spatial candidates have the same motion vector components one redundant spatial candidate is excluded when the number of motion vector predictors is not equal to two and the use of temporal mv prediction is not explicitly disabled the temporal mv prediction candidate is included this means that the temporal candidate is not used at all when two spatial candidates are available finally a zero motion vector is included repeatedly until the number of motion vector prediction candidates is equal to two which guarantees that the number of motion vector predictors is two thus only a coded flag is necessary to identify which motion vector prediction is used in the case of nonmerge mode i transform scaling and quantization hevc uses transform coding of the prediction error residual in a similar manner as in prior standards the residual block is partitioned into multiple square tbs as described in section iv e the supported transform block sizes are and core transform two dimensional transforms are computed by applying d transforms in the horizontal and vertical directions the elements of the core transform matrices were derived by approximating scaled dct basis functions under considerations such as limiting the necessary dynamic range for transform computation and maximizing the precision and closeness to orthogonality when the matrix entries are specified as integer values 90 90 87 43 43 87 90 80 57 75 70 87 57 90 43 83 57 43 80 90 70 75 89 89 75 87 80 70 57 43 ieee transactions on circuits and systems for video technology vol no december for simplicity only one integer matrix for the length of points is specified and subsampled versions are used for other sizes for example the matrix for the length transform is as shown in the equation at the bottom of the previous page the matrices for the length and length transforms can be derived by using the first eight entries of rows and using the first four entries of rows respectively although the standard specifies the transform simply in terms of the value of a matrix the values of the entries in the matrix were selected to have key symmetry properties that enable fast partially factored implementations with far fewer mathematical operations than an ordinary matrix multiplication and the larger transforms can be constructed by using the smaller transforms as building blocks due to the increased size of the supported transforms limiting the dynamic range of the intermediate results from the first stage of the transformation is quite important hevc explicitly inserts a b right shift and b clipping operation after the first d inverse transform stage of the transform the vertical inverse transform stage to ensure that all intermediate values can be stored in b memory for b video decoding alternative transform for the transform block size of an alternative integer transform derived from a dst is applied to the luma residual blocks for intrapicture prediction modes with the transform matrix h 74 74 the basis functions of the dst better fit the statistical property that the residual amplitudes tend to increase as the distance from the boundary samples that are used for prediction becomes larger in terms of complexity the dst style transform is not much more computationally demanding than the dct style transform and it provides approximately bit rate reduction in intrapicture predictive coding the usage of the dst type of transform is restricted to only luma transform blocks since for other cases the additional coding efficiency improvement for including the additional transform type was found to be marginal scaling and quantization since the rows of the transform matrix are close approximations of values of uniformly scaled basis functions of the orthonormal dct the prescaling operation that is incorporated in the dequantization of h mpeg avc is not needed in hevc this avoidance of frequency specific basis function scaling is useful in reducing the intermediate memory size especially when considering that the size of the transform can be as large as for quantization hevc uses essentially the same urq scheme controlled by a quantization parameter qp as in h mpeg avc the range of the qp values is defined from to and an increase by doubles the quantization step size such that the mapping of qp values to step sizes is approximately logarithmic quantization scaling matrices are also supported to reduce the memory needed to store frequency specific scaling values only quantization matrices of sizes and are used for the larger transformations of and sizes an scaling matrix is sent and is applied by sharing values within and coefficient groups in frequency subspaces except for values at dc zero frequency positions for which distinct values are sent and applied j entropy coding hevc specifies only one entropy coding method cabac rather than two as in h mpeg avc the core algorithm of cabac is unchanged and the following subsections present several aspects of how it is used in the hevc design context modeling appropriate selection of context modeling is known to be a key factor to improve the efficiency of cabac coding in hevc the splitting depth of the coding tree or transform tree is exploited to derive the context model indices of various syntax elements in addition to the spatially neighboring ones used in h avc for example the syntax element skip flag specifying whether the cb is coded as interpicture predictively skipped and the syntax element split coding unit flag specifying whether the cb is further split are coded by using context models based on the spatially neighboring information the syntax element split transform flag specifying whether the tb is further split and three syntax elements specifying non zero transform coefficients for each color component cbf luma cbf cb and cbf cr are coded based on the splitting depth of the transform tree although the number of contexts used in hevc is substantially less than in h mpeg avc the entropy coding design actually provides better compression than would a straightforward extension of the h avc scheme moreover more extensive use is made in hevc of the bypass mode of cabac operation to increase throughput by reducing the amount of data that needs to be coded using cabac contexts dependences between coded data are also carefully considered to enable further throughput maximization adaptive coefcient scanning coefficient scanning is performed in subblocks for all tb sizes i e using only one coefficient region for the tb size and using multiple coefficient regions within larger transform blocks three coefficient scanning methods diagonal up right horizontal and vertical scans as shown in fig are selected implicitly for coding the transform coefficients of and tb sizes in intrapicture predicted regions the selection of the coefficient scanning order depends on the directionalities of the intrapicture prediction the vertical scan is used when the prediction direction is close to horizontal and the horizontal scan is used when the prediction direction is close to vertical for other prediction directions the diagonal up right scan is used for the transform coefficients in interpicture prediction modes of all block sizes and for the transform coefficients of or intrapicture prediction the diagonal up right scan is exclusively applied to subblocks of transform coefficients sullivan et al overview of the hevc standard fig three coefficient scanning methods in hevc a diagonal up right scan b horizontal scan c vertical scan coefcient coding similar to h mpeg avc hevc transmits the position of the last nonzero transform coefficient a significance map sign bits and levels for the transform coefficients however various changes for each part have been made especially for better handling of the significantly increased size of the tbs first the horizontal and vertical frequency coordinate positions of the last nonzero coefficient are coded for the tb before sending the significance maps of subblocks that indicate which other transform coefficients have nonzero values rather than sending a series of last coefficient identification flags that are interleaved with the significance map as done in h mpeg avc the significance map is derived for significance groups relating to the fixed size subblocks for all groups having at least one coefficient preceding the last coefficient position a significant group flag specifying a nonzero coefficient group is transmitted followed by coefficient significance flags for each coefficient prior to the indicated position of the last significant coefficient the context models for the significant coefficient flags are dependent on the coefficient position as well as the values of the right and the bottom significant group flags a method known as sign data hiding is used for further compression improvement the sign bits are coded conditionally based on the number and positions of coded coefficients when sign data hiding is used and there are at least two nonzero coefficients in a subblock and the difference between the scan positions of the first and the last nonzero coefficients is greater than the sign bit of the first nonzero coefficient is inferred from the parity of the sum of the coefficient amplitudes otherwise the sign bit is coded normally at the encoder side this can be implemented by selecting one coefficient with an amplitude close to the boundary of a quantization interval to be forced to use the adjacent quantization interval in cases where the parity would not otherwise indicate the correct sign of the first coefficient this allows the sign bit to be encoded at a lower cost in rate distortion terms than if it were coded separately by giving the encoder the freedom to choose which transform coefficient amplitude can be altered with the lowest rate distortion cost for each position where the corresponding significant coefficient flag is equal to one two flags specifying whether the level value is greater than one or two are coded and then the remaining level value is coded depending on those two values k in loop filters in hevc two processing steps namely a deblocking filter dbf followed by an sao filter are applied to the reconstructed samples before writing them into the decoded picture buffer in the decoder loop the dbf is intended to reduce the blocking artifacts due to block based coding the dbf is similar to the dbf of the h mpeg avc standard whereas sao is newly introduced in hevc while the dbf is only applied to the samples located at block boundaries the sao filter is applied adaptively to all samples satisfying certain conditions e g based on gradient during the development of hevc it had also been considered to operate a third processing step called the adaptive loop filter alf after the sao filter however the alf feature was not included in the final design deblocking filter the deblocking filter is applied to all samples adjacent to a pu or tu boundary except the case when the boundary is also a picture boundary or when deblocking is disabled across slice or tile boundaries which is an option that can be signaled by the encoder it should be noted that both pu and tu boundaries should be considered since pu boundaries are not always aligned with tu boundaries in some cases of interpicture predicted cbs syntax elements in the sps and slice headers control whether the deblocking filter is applied across the slice and tile boundaries unlike h mpeg avc where the deblocking filter is applied on a sample grid basis hevc only applies the deblocking filter to the edges that are aligned on an sample grid for both the luma and chroma samples this restriction reduces the worst case computational complexity without noticeable degradation of the visual quality it also improves parallel processing operation by preventing cascading interactions between nearby filtering operations the strength of the deblocking filter is controlled by the values of several syntax elements similar to the scheme in h mpeg avc but only three strengths are used rather than five given that p and q are two adjacent blocks with a common grid boundary the filter strength of is assigned when one of the blocks is intrapicture predicted otherwise the filter strength of is assigned if any of the following conditions is satisfied p or q has at least one nonzero transform coefficient the reference indices of p and q are not equal the motion vectors of p and q are not equal the difference between a motion vector component of p and q is greater than or equal to one integer sample if none of the above conditions is met the filter strength of is assigned which means that the deblocking process is not applied according to the filter strength and the average quantization parameter of p and q two thresholds t c and  are determined from predefined tables for luma samples one of three cases no filtering strong filtering and weak filtering is chosen based on  note that this decision is shared across four luma rows or columns using the first and the last rows or columns to reduce the computational complexity there are only two cases no filtering and normal filtering for chroma samples normal filtering is applied only when the filter strength is greater than one the filtering process is then performed using the control variables t c and  ieee transactions on circuits and systems for video technology vol no december table iv sample edgeidx categories in sao edge classes fig four gradient patterns used in sao sample labeled p indicates a center sample to be considered two samples labeled and specify two neighboring samples along the a horizontal sao eo class b vertical sao eo class c diagonal sao eo class and d sao eo class gradient patterns in hevc the processing order of the deblocking filter is defined as horizontal filtering for vertical edges for the entire picture first followed by vertical filtering for horizontal edges this specific order enables either multiple horizontal filtering or vertical filtering processes to be applied in parallel threads or can still be implemented on a ctb by ctb basis with only a small processing latency sao sao is a process that modifies the decoded samples by conditionally adding an offset value to each sample after the application of the deblocking filter based on values in look up tables transmitted by the encoder sao filtering is performed on a region basis based on a filtering type selected per ctb by a syntax element sao type idx a value of for sao type idx indicates that the sao filter is not applied to the ctb and the values and signal the use of the band offset and edge offset filtering types respectively in the band offset mode specified by sao type idx equal to the selected offset value directly depends on the sample amplitude in this mode the full sample amplitude range is uniformly split into segments called bands and the sample values belonging to four of these bands which are consecutive within the bands are modified by adding transmitted values denoted as band offsets which can be positive or negative the main reason for using four consecutive bands is that in the smooth areas where banding artifacts can appear the sample amplitudes in a ctb tend to be concentrated in only few of the bands in addition the design choice of using four offsets is unified with the edge offset mode of operation which also uses four offset values in the edge offset mode specified by sao type idx equal to a syntax element sao eo class with values from to signals whether a horizontal vertical or one of two diagonal gradient directions is used for the edge offset classification in the ctb fig depicts the four gradient patterns used for the respective sao eo class in this mode each sample in the ctb is classified into one of five edgeidx categories by comparing the sample value p located at some position with the values and of two samples located at neighboring positions as shown in table iv this classification is done for each sample based on decoded sample values so no additional signaling is required for the edgeidx classification depending on the edgeidx category at the sample position for edgeidx categories from to an offset value from a transmitted look up table is added to the sample value the offset values are always positive for categories and and negative for categories and thus the filter generally has a smoothing effect in the edge offset mode edgeidx condition cases not listed below p and p p and p or p and p p and p or p and p p and p meaning monotonic area local min edge edge local max thus for sao types and a total of four amplitude offset values are transmitted to the decoder for each ctb for type the sign is also encoded the offset values and related syntax elements such as sao type idx and sao eo class are determined by the encoder typically using criteria that optimize rate distortion performance the sao parameters can be indicated to be inherited from the left or above ctb using a merge flag to make the signaling efficient in summary sao is a nonlinear filtering operation which allows additional refinement of the reconstructed signal and it can enhance the signal representation in both smooth areas and around edges l special coding modes hevc defines three special coding modes which can be invoked at the cu level or the tu level in i pcm mode the prediction transform quantization and entropy coding are bypassed and the samples are directly represented by a pre defined number of bits its main purpose is to avoid excessive consumption of bits when the signal characteristics are extremely unusual and cannot be properly handled by hybrid coding e g noise like signals in lossless mode the transform quantization and other processing that affects the decoded picture sao and deblocking filters are bypassed and the residual signal from inter or intrapicture prediction is directly fed into the entropy coder using the same neighborhood contexts that would usually be applied to the quantized transform coefficients this allows mathematically lossless reconstruction which is achieved without defining any additional coding tools in transform skipping mode only the transform is bypassed this primarily improves compression for certain types of video content such as computer generated images or graphics mixed with camera view content e g scrolling text this mode can be applied to tbs of size only sao and deblocking filtering are not applied to lossless mode regions and a flag controls whether they are applied to i pcm regions v profiles tiers and levels a prole level and tier concepts profiles tiers and levels specify conformance points for implementing the standard in an interoperable way across various applications that have similar functional requirements a profile defines a set of coding tools or algorithms that can be used in generating a conforming bitstream whereas a level places sullivan et al overview of the hevc standard constraints on certain key parameters of the bitstream corresponding to decoder processing load and memory capabilities level restrictions are established in terms of maximum sample rate maximum picture size maximum bit rate minimum compression ratio and capacities of the dpb and the coded picture buffer cpb that holds compressed data prior to its decoding for data flow management purposes in the design of hevc it was determined that some applications existed that had requirements that differed only in terms of maximum bit rate and cpb capacities to resolve this issue two tiers were specified for some levels a main tier for most applications and a high tier for use in the most demanding applications a decoder conforming to a certain tier and level is required to be capable of decoding all bitstreams that conform to the same tier or the lower tier of that level or any level below it decoders conforming to a specific profile must support all features in that profile encoders are not required to make use of any particular set of features supported in a profile but are required to produce conforming bitstreams i e bitstreams that obey the specified constraints that enable them to be decoded by conforming decoders b the hevc prole and level denitions only three profiles targetting different application requirements called the main main and main still picture profiles are foreseen to be finalized by january minimizing the number of profiles provides a maximum amount of interoperability between devices and is further justified by the fact that traditionally separate services such as broadcast mobile streaming are converging to the point where most devices should become usable to support all of them the three drafted profiles consist of the coding tools and highlayer syntax described in the earlier sections of this paper while imposing the following restrictions only chroma sampling is supported when an encoder encodes a picture using multiple tiles it cannot also use wavefront parallel processing and each tile must be at least luma samples wide and luma samples tall in the main and main still picture profiles only a video precision of b per sample is supported while the main profile supports up to b per sample in the main still picture profile the entire bitstream must contain only one coded picture and thus interpicture prediction is not supported currently the definition of levels is planned to be included in the first version of the standard as shown in table v ranging from levels that support only relatively small picture sizes such as a luma picture size of sometimes called a quarter common intermediate format to picture sizes as large as often called the picture width and height are each required to be less than or equal to maxlumaps where maxlumaps is the maximum luma picture size as shown in table v to avoid the problems for decoders that could be involved with extreme picture shapes there are two tiers supported for eight of these levels level and higher the cpb capacity is equal to the table v level limits for the main profile main tier high tier max luma max luma max max min level picture size sample rate bit rate bit rate comp ratio samples samples bits bits 864 122 760 3000 588 983 228 693 386 896 760 896 547 651 095 651 800 000 maximum bit rate times for all levels except level which has a higher cpb capacity of 000 b the specified maximum dpb capacity in each level is six pictures when operating at the maximum picture size supported by the level including both the current picture and all other pictures that are retained in the decoder at any point in time for reference or output purposes when operating with a smaller picture size than the maximum size supported by the level the dpb picture storage capacity can increase to as many as pictures depending on the particular selected picture size levelspecific constraints are also specified for the maximum number of tiles used horizontally and vertically within each picture and the maximum number of tiles used per second vi history and standardization process after the finalization of the h mpeg avc high profile in mid both itu t vceg and iso iec mpeg have been trying to identify when the next major advances in coding efficiency would become ready for standardization vceg began studying potential advances in began identifying certain key technology areas ktas for study in early and developed a common kta software codebase for this paper various technologies were proposed and verified using the kta software codebase which was developed from the h mpeg avc reference software known as the joint model jm from to mpeg began exploration activities toward significant coding efficiency improvements as well organized several workshops and issued a call for evidence of such advances in april expert viewing tests were conducted to evaluate submissions of responses to the call from their respective investigations it was agreed that there were sufficient technologies with the potential to improve the coding efficiency significantly compared to the existing video coding standards the joint collaborative team on video coding jct vc was planned to be established by both groups in january and a joint call for proposals cfp on video compression technology was issued by the same ieee transactions on circuits and systems for video technology vol no december table vi table vii structure of coding tools associated with high efficiency and low complexity configurations of hm summary of coding tools of high efficiency configuration in hm and hevc functionality ctb cu structure pu structure tu structure transform intra prediction luma interpolation chroma interpolation mv prediction entropy coding deblocking filter adaptive loop filter high efciency low complexity tree structure from to square and symmetric shapes three level tree structure two level tree structure integer transforms from four to points angular modes with dc mode tap separable six tap directional bilinear spatial cu merging amvp cabac event based vlc enabled enabled disabled time to identify the initial technologies that would serve as a basis of future standardization activities at its first meeting in april the jct vc established the hevc project name studied the proposals submitted in response to the cfp and established the first version of a test model under consideration tmuc which was produced collectively from elements of several promising proposals a corresponding software codebase was implemented after this meeting the technology submitted in several of the key proposal contributions was previously discussed in a special section of the ieee transactions on circuits and systems for video technology although the tmuc showed significant coding efficiency improvements compared to prior standards it had several redundant coding tools in each functional block of the video compression system primarily due to the fact that the tmuc was a collective design from various contributions during the second jct vc meeting in july the process began of selecting the minimal necessary set of coding tools for each functional block by thoroughly testing each component of the tmuc based on the reported results of the exhaustive component testing an hevc test model version hm and the corresponding hevc working draft specification version wd were produced as outputs of the third jct vc meeting in october compared to the prior tmuc design hm was simplified greatly by removing coding tools that showed only marginal benefits relative to their computational complexity in several subsequent studies the coding tools of the hm were classified into two categories called the high efficiency and low complexity configurations two corresponding test scenarios for verifying future contributions in the jct vc were also established table vi summarizes the hm coding tools for the high efficiency and low complexity configurations from the fourth to the eleventh jct vc meetings not just coding efficiency improvements but many other aspects including computational complexity reduction unification of various coding tools and parallel friendly design were investigated and the hevc design was updated accordingly until the current status of draft standard as described in this functionality ctu structure core transform hm high efciency hevc draft tree structure from tree structure from to to square symmetric and square and symmetric asymmetric only square for intra tree structure of square tree structure of square tus tus integer transforms from integer transforms from to points to points alternative transform full factorization n a pu structure tu structure angular modes with dc mode luma interpolation tap separable chroma interpolation bilinear mv prediction amvp mc region merging spatial cu merging entropy coding cabac deblocking filter nonparallel sample adaptive offset n a adaptive loop filter multiple shapes dedicated tools for slices parallel processing intra prediction partially factorable integer dst type for angular modes with planar and dc modes tap tap separable tap separable amvp pu merging cabac parallel enabled n a slices tiles wavefronts and dependent slice segments paper was reached in this context it also turned out that the differentiation for low complexity and high efficiency was no longer necessary became possible to define the unified main profile table vii provides a summary of coding tools of the high efficiency configuration in hm and the current specification of hevc at the eighth jct vc meeting in february the draft version of hevc standard was produced which was subsequently balloted as the iso iec committee draft of the hevc standard the tenth jct vc meeting in july released the draft version for a draft international standard ballot and the finalized text for consent in itu t and final draft international standard in iso iec is expected to be produced in january future extensions of hevc which are already being explored and prepared by the jct vc parent bodies are likely to include extended range formats with increased bit depth and enhanced color component sampling scalable coding and d stereo multi view video coding the latter including the encoding of depth maps for use with advanced d displays is performed using all the possible depth levels and prediction modes to find the one with the least rate distortion rd cost using lagrange multiplier this achieves the highest coding efficiency but requires a very high computational complexity in this paper we propose a fast cu size decision algorithm for hm since the optimal depth level is highly content dependent it is not efficient to use all levels we can determine cu depth range including the minimum depth level and the maximum depth level and skip some specific depth levels rarely used in the previous frame and neighboring cus besides the proposed algorithm also introduces early termination methods based on motion homogeneity checking rd cost checking and skip mode checking to skip me on unnecessary cu sizes experimental results demonstrate that the proposed algorithm can significantly reduce computational complexity while maintaining almost the same rd performance as the original hevc encoder index terms cu size decision hevc motion estimation i introduction high efficiency video coding hevc is an ongoing standard and the current working draft is a successor to h it significantly outperforms previous standards such as mpeg mpeg part and h in terms of coding efficiency because new techniques are adopted in hevc such as the hierarchical quadtree structure of motion compensation large coding treeblock coding unit cu and block partition unit pu a treeblock is an of luma samples together with the two corresponding blocks of chroma samples whose concept is broadly analogous to that of the macroblock mb in h the cu is the basic unit of region splitting used for inter intra prediction which allows recursive subdividing into four equally sized blocks it is always square and may take a size from the size of treeblock down to the pu is the basic unit used for carrying the information related to the prediction processes in general it is not restricted to being square in shape in order to facilitate partitioning that matches the boundaries of real objects in the image intra partition and partition but cus have two types of pus inter cus have four types of pus including manuscript received november revised february and may accepted july date of publication november date of current version january this work is sponsored by shanghai rising star program and innovation program of shanghai municipal education commission and is supported by the national natural science foundation of china under grant no and the associate editor coordinating the review of this manuscript and approving it for publication was charles d chuck creusere l shen z liu w zhao and z zhang are with the key laboratory of advanced display and system application ministry of education shanghai university shanghai china e mail jsslq com zhaowenqiangfly com shu edu cn zhyzhang shu edu cn x zhang is with the school of communication and information engineering shanghai university shanghai china e mail xzhang shu edu cn color versions of one or more of the figures in this paper are available online at http ieeexplore ieee org digital object identifier tmm fig illustration of recursive cu structure and modes at each depth level and hevc encoders enable different modes including skip inter inter inter intra mode inter and intra for inter slice fig shows the architecture of tree structured cus and prediction modes at each depth level an image is first divided into treeblocks in the test model of hevc hm each treeblock can further be split into the so called cus for a cu in depth level x the procedure shown in the right part of fig will be followed and the current cu will be divided into sub cus coming in each level a cu can be split into or to depth level prediction units as shown in the left part of fig similar to the joint model of h the mode decision process in hm is performed using all the possible cu sizes and prediction modes to find the one with the least rate distortion rd cost using lagrange multiplier more detail can be found in rd cost for each cu size or prediction mode is defined as follows specifies bit cost to be considered for cu size decision and where is the average difference between the current cu mode decision is the lagrange multiplier however and the matching blocks and this try all and select the best method will result in high computational complexity and limit the use of hevc encoders in real time applications therefore fast algorithms which can reduce the complexity of cu size decision without compromising coding efficiency are very desirable for real time implementation of hevc encoders the rest of this paper is organized as follows an overview of related work on fast mode decision is provided in section ii adaptive depth range determination method and early termination methods for me on small cu size are presented in section iii simulation results and conclusions are given in sections iv and v respectively ii overview of related work on fast mode decision a number of efforts have been made to explore fast decision algorithms in intra mode prediction and inter mode prediction in pre 00 ieee ieee transactions on multimedia vol no february vious video coding standards such as h and its extension of scalable video coding svc a criterion based on the sum of approximate square difference is proposed in to reduce complexity of h intra mode decision the methods introduced in and use the rd cost distribution or skip mode detection to early terminate the inter mode decision prediction modes in are directly selected using phase correlation information with all possible large modes based on the motion dominancy and relative positioning a statistical learning based approach in is applied to the mode decision procedure in h encoders to speed up the computation the motion homogeneity and inter layer correlation are respectively utilized to skip some unnecessary prediction modes of h and svc in our previous works although those algorithms above are useful for previous video coding standards they could not be accurately adapted to the hierarchical quadtree structure of the cu with extended and variable block sizes in hm in addition the above algorithms are more favorably for the sequences with relatively smaller spatial resolutions due to the limited size of mbs in previous standards recently studies on reduction of computation complexity of hevc encoders also have been reported hm adopted a fast mode decision based on rd cost from skip mode to terminate procedures of cu splitting and mod decision an adaptive cu depth range algorithm is proposed in to reduce encoding complexity of hm and a fast cu size decision method is proposed based on coding tree pruning in meanwhile a fast cu decision algorithm at either frame level or cu level is proposed in to accelerate the encoding process the aforementioned methods are well developed for hm to achieve significant time saving however most of these methods only utilize the spatial or temporal correlation while coding information correlations among different depth levels are not fully studied there is still some room for further improvement in the cu size decision process to overcome these problems this paper proposes a fast cu size decision algorithm for hm incorporating an adaptive cu depth range determination algorithm and early termination et methods for motion estimation me on small cu size considering that the optimal cu depth level is highly content dependent it is not efficient to use a fixed cu depth range for the whole sequence therefore we can skip some specific depth levels rarely used in the previous frame and neighboring cus meanwhile there are motion vector mv rd cost and prediction mode correlations among different levels or neighboring cus by fully exploiting these correlations we propose three et methods based on motion homogeneity checking rd cost checking and skip mode checking to skip the procedure of me on unnecessary small cu sizes experimental results demonstrate that the proposed algorithm can significantly reduce the computational complexity of hm while maintaining almost the same rd performance as the original encoder iii effective cu size decision method a observation and statistical analysis hm usually allows the maximum cu size equal to and the depth level range is from to we first analyze the depth level distribution based on coding of sequences with the resolution of 576 and sequences with different motion activities are selected to test the depth level distribution among these sequences basketball and kayak are with a large global motion or a large local motion mobcal flamingo parkrun and shipcalendar are with a medium local motion test conditions are listed as follows each test sequence is encoded using the hierarchical b frame structure quantization parameter qp is set to and respectively rdo rd optimization and lcec low complexity entropy coding based on cavlc are enabled search range of me is set to table i shows the optimal depth level distribution in inter slices table i statistical analysis of depth level distribution fig temporal and spatial correlations of cus current left upper left upper the co located cu in the previously frame the result shows that and of treeblocks choose the depth level and respectively for sequences containing a large area with high activities such as kayak and basketball the possibility of selecting depth level is very low with for sequences containing a large area with low activities such as mobcal and shipcalendar more than 70 treeblocks select the depth level as the optimal levels these results show that small depth levels are always selected at treeblocks in the homogeneous region and large depth levels are selected at treeblocks with active motion or rich texture the depth range should be adaptively determined based on the treeblock property meanwhile we can see that most of treeblocks choose the first two depth levels especially for low activity sequences and treeblocks choose the optimal depth levels after cu size decision with the depth level and the depth level respectively and only about treeblocks choose the last depth level the depth level thus me on small cu sizes could be skipped in most cases without loss of coding performance b adaptive cu depth range dr determination in hm cu depth has a fixed range for a whole video sequence however small depth values are suitable for treeblocks in the homogeneous region and large depth values are reasonable for treeblocks with active motion or rich texture the exhaustive cu size decision is inefficient and cu depth range should be adaptively determined based on motion and texture properties of the current treeeblock an adaptive cu depth range at cu level is proposed in based on the maximal and minimal values of depth levels from left and upper treeblocks however this method simply utilizes the depth information from left and upper treeblocks and most of treeblocks still need to perform me on more than depth levels therefore the time saving of the method in is rather limited natural video sequences have strong spatial and temporal correlations especially in the homogeneous regions the optimal depth level of a certain treeblock is the same or very close to the depth level of its spatially adjacent blocks due to the high correlation between adjacent blocks meanwhile most treeblocks do not exhibit a wide variation from the co located block in the previous frame since successive frames are highly correlated the depth information of the co located treeblock in the previous frame affects the depth level determination process of the current treeblock thus we can make use of spatial and temporal correlations to analyze region properties and skip me on unnecessary cu sizes specifically the optimal depth level of a treeblock ieee transactions on multimedia vol no february table ii candidate depth levels for each treeblock type table iii statistical analysis of depth distribution for different treeblock is predicted using spatial neighboring treeblocks in fig and the co located treeblock in fig at the and previously coded frame as follows where is the number of treeblocks equal to is the value of depth level and is the weight determined based on correlations between the current treeblock and its neighboring treeblocks the four weights are since the co located treeblock in the normalized to have previously coded frame and neighboring treeblocks in the horizontal and vertical directions have a large correlation of the optimal cu size for the current treeblock compared to that in the diagonal direction the treeblocks are set to and the weight for weights for and treeblock is set to according to the predicted value of the optimal depth level each as follows treeblock is divided into five types of current treeblock is equal and its op when timal prediction mode at depth level is chosen with skip mode current treeblock is located in the still or homogeneous motion region and classified as type is smaller than the value of most of when neighbor treeblocks choose the optimal depth levels with the depth level current block is located in the moderate motion region and classified as type most of neighbor treeblocks when choose the optimal depth levels with the depth level cur rent block is classified as type most of neighbor treeblocks when choose the optimal depth levels with the depth level cur rent block is classified as type otherwise most of neighbor treeblocks choose the optimal depth levels with the depth level current block is located in the fast motion region and classified as type treeblock the optimal depth levels of spatial and temfor type poral neighboring treeblocks are all with the depth level and the area covered by the current treeblock and its neighboring blocks contains motionless or slow motion content thus the current treeblock treeblock it is not effionly needs me on treeblock size for type cient to perform me at depth levels and based on the above analysis the candidate depth levels that will be tested using rdo for each treeblock are summarized in table ii most of treeblocks can skip two to three tested depth levels to verify legitimacy of the adjusted depth range extensive simulations have been conducted on a set of video sequences by exploiting the exhaustive cu size decision in hm under the aforementioned test conditions in section iii a we investigate the depth level distribution for these types of treeblocks in table iii the statistical data in table iii demonstrates the rationality of the candidate depth levels determined for each type of treeblocks and it ensures that the optimal depth levels of a great majority of treeblocks the average coverage is from to will fall in their determined candidate depth levels if we perform rdo only on the corresponding candidate depth levels according to the treeblock type we can anticipate from table iii that the coding efficiency will be kept very close to the optimal one achieved using the exhaustive cu size decision the anticipation here is verified in our experimental results later c early termination et of me on unnecessary cu sizes we have analyzed in section iii a that a majority of optimal depth levels after cu size decision are determined as either or which implies that me on small cu sizes would be unnecessary in most cases so it is better to introduce a proper et strategy into the fast cu size decision algorithm there are a lot of homogeneous regions in natural video sequences which belong to the same video object such as the background it is suitable to use larger cu sizes for prediction on these homogeneous blocks with similar motions when mvs of the current cu and its spatial neighboring cus are homogeneous me on the next depth level is usually unnecessary meanwhile coding information of the current cu including rd cost and the prediction mode is strongly correlated to the co located cu in the previous frame and the parent cus in upper depth levels the main idea of proposed et methods is to use the coding information correlation among depth levels and the spatio temporal correlation among neighboring blocks to check whether the prediction mode and mvs in the current cu size can represent motion efficiently or not and whether necessary to perform me on the next depth level in the following we will detail the three et methods method motion homogeneity checking based et in natural video sequences there exist a lot of homogeneous regions belonging to the same video objects and having similar motions it is sufficient to represent motion on a coarse level for those homgeneous motion regions coding using larger cu sizes without larger residuals on the contrary it is suitable to represent motion in a finer level for the region with spatially discontinuous motion thus motion homogeneity is a good indication in choosing the best inter cu size which can be used to skip me on unnecessary cu size so as to speed the procedure of me in this paper mvs from the current cu and those from blocks covered by neighboring cus left cu upper cu left upper cu can be used to evaluate the motion homogeneity mv information of the current cu is achieved from me in the current depth level assume that the mvs of the current cu and the covered blocks of neighboring cus are denoted as the motion homogeneities of the current cu in the horizontal and vertical direction are respectively defined as where represents blocks covered by the current cu and its neighboring cus and is the total number of blocks it should be noted that when a small depth level such as depth level or is chosen for a neighboring cu and me for smaller block sizes is not performed ieee transactions on multimedia vol no february table iv accuracy of each et method it means that the neighboring cu is located in homogeneous region mvs of all blocks covered by the cu are similar to the ones at the chosen depth level in this condition the mv at the chosen depth level is used to compute the motion homogeneity when the motion the current cu is considhomogeneity is smaller than a threshold ered with homogeneous motion otherwise the cu is considered with complex motion the threshold is set to in order to tolerate one noisy mv in the motion homogenous region when the current cu is considered with homogeneous motion its motion can be efficiently represented using the current cu size therefore it is not necessary to spit the current cu into four sub cus and perform me on sub cus we can skip the me on the next depth level method rd cost checking based et spatial and temporal neighboring treeblocks or cus usually show a similar rd cost distribution and thus we can make use of the rd cost based correlation to determine the et threshold when rd cost of the current cu size is smaller than the determined threshold the me on the next depth level can be skipped rd costs of spatial neighboring cus left cu upper cu and left upper cu and the co located cu at the previously coded frame can be achieved since the procedure of mode decision has performed on these neighboring cus the threshold is calculated as follows where is the number of neighboring cus equal to is an adjust is the weights determined based on the correlations parameter and between the current cu and its neighboring cus the weights for cus and are set to and the weight for cu is set to the selection of the threshold should greatly reduce the complexity while keeping a high accuracy based on extensive experiments the threshold is set to which achieves a good and consistent performance on a variety of video sequences with different motion activities method skip mode checking based et besides the aforementioned et technique the proposed algorithm also introduces skip mode checking based et to skip checking unnecessary me on small cu sizes by utilizing the prediction mode information in the upper depth level and the current depth level in the mode decision procedure of hm choosing a small cu size usually results in a lower energy residual after motion compensation but requires a larger number of bits to signal the mvs and type of prediction the smooth and slow motion can be predicted more accurately using larger cu size when the skip mode is selected as the best prediction mode on the current cu size it indicates that the current cu is located in a region with homogeneous motion or static region the motion of the cu can be predicted well using the skip mode which results in a lower energy residual after motion compensation compared to other predic inter inter and inter tion modes such as inter thus no further processing of sub cus is necessary based on this observation the proposed algorithm checks the prediction modes from the current cu and the parent cu in the upper depth level when both of them have skip as the best mode it indicates that the motion fig flowchart of the proposed overall algorithm can be efficiently represented using the current cu size and the me on the next depth level can be skipped to verify legitimacy of the proposed three et methods extensive simulations have been conducted on a set of video sequences as listed in table iv by exploiting the exhaustive cu size decision in hm under the aforementioned test conditions in section iii a we investigate the effectiveness of the proposed three et methods table iv shows the accuracy of each et method the average accuracy of the motion homogeneity checking based et method achieves 90 with a maximum of as far as the proposed rd cost checking based et method is concerned the average accuracy is the highest with similar to the previous two et methods skip mode checking based method also achieves a high accuracy with the results shown in table iv indicate that the proposed three et methods can accurately reduce unnecessary me on small cu size d overall algorithm based on the aforementioned analysis the proposed overall algorithm incorporates the adaptive cu depth range determination algorithm and the above three et methods a flowchart of the proposed algorithm is shown in fig treeblock is determined based on it should be noted that type its prediction mode at depth level and the predicted optimal depth before performing me of a treeblock the mode level of information at depth level is not available if the treeblock and a treeblock equals to it is defined as the type its depth range is pre assigned with after performing me on treeblock that chooses skip mode as the depth level the type ieee transactions on multimedia vol no february table v results of each individual algorithm compared to hm optimal mode at depth level is confirmed as type is classified as type otherwise it iv experimental results in order to evaluate the performance of the proposed fast cu size decision approaches the adaptive cu depth range determination algorithm and three et methods are implemented on the recent hevc reference software hm encoder with low complexity profile two coding conditions are defined in the call for hevc proposals constraint set and constraint set a random access case corresponds to a broadcast scenario with a maximum gop size of and a maximum intra frame period of approximately seconds a low delay case corresponds to a low delay scenario with no picture reordering the two constraint sets restricting coding parameters and the temporal coding structure are used in all tests coding treeblock has a fixed size of 64 pixels for luma and a maximum quadtree depth of resulting in a minimum cu size of the initial search range is 64 in both horizontal and vertical directions search mode epzs and fen fast encoder decision are enabled the proposed algorithm is evaluated with qps and using test sequences recommended by jct vc joint collaborative team on video coding in four resolutions formats note that six training sequences which are utilized to verify legitimacy of the proposed algorithm in section iii are not used as the test sequences the experimental results are presented in tables v vi in which coding efficiency is measured with psnr and bit rate and computational complexity is measured with the consumed coding time bdpsnr db and bdbr are used to represent the average psnr and bitrate differences and dt is used to represent the coding time change in percentage table v shows individual evaluation results of the proposed approaches i e adaptive depth range determination adrd and early termination of unnecessary small cu size me et scume respectively the proposed two approaches can greatly reduce the coding time with similar coding efficiency for all sequences adrd can achieve about coding time saving over all sequences under both and conditions it can be also observed that a consistent gain is obtained over all sequences under both conditions the coding efficiency loss is very negligible with db db psnr drop or bitrate increase this result indicates that adrd can efficiently skip unnecessary depth levels in cu size decision as far as the et scume method is concerned and coding time has been reduced in and respectively with a maximum of in and a minimum of in racehorses 240 meanwhile the average psnr drop for all the test sequences is db and the average increase of bitrate is which is negligible the foregoing result analysis indicates that et scume can efficiently reduce the coding time while maintaining nearly the same rd performance as the original hevc encoder in the following we analyze the experimental result of the proposed overall algorithm which incorporates adrd and et scume the comparison results of the overall algorithm and a state of the art fast algorithm for hevc encoders adaptive cu depth range acudr are shown in table vi the proposed overall algorithm respectively reduces and coding time under and and achieves the better gain in coding speed compared to acudr also shown is a consistent gain in coding speed for all test sequences with the lowest gain in and the highest gain of for racehorses for slow motion sequences like for for traffic and the proposed algorithm saves more than coding time the computation reduction is particularly high because the exhaustive cu size decision procedures of a significant number of cus are reasonably skipped for high activity sequences like peopleonstreet and racehorses the proposed algorithm also can reduce about coding time on the other hand the coding efficiency loss is negligible specifically where the average psnr loss is 05 db or the average increase of bitrate is compared with acudr the proposed overall algorithm performs better on all the sequences and achieves more than coding time saving the coding efficiency losses are negligible considering the time saving it achieves the proposed overall algorithm has only db psnr loss or bitrate increment compared to acudr figs a b show a typical example of rd and time saving curves under different qps compared to hm as shown in fig the proposed approaches adrd et scume and the overall algorithm can achieve a consistent time saving over a large bitrate range with almost negligible loss in psnr and increment in bit rate ieee transactions on multimedia vol no february results of the table vi proposed overall algorithm compared with a state of the art fast algorithm fig experimental results of basketballdrive under different qp settings a rd curves of basketballdrive b time saving curves basketballdrive v conclusion this paper presents a fast cu size decision algorithm to reduce the computational complexity of the hevc encoder by exploiting two fast approaches i e adaptive depth range determination and early termination of unnecessary me on small cu sizes the proposed algorithm is implemented on the recent hevc reference software the comparative experimental results show that the proposed algorithm can significantly reduce the computational complexity of hm while maintaining almost the same rd performances as the original encoder and achieves a higher gain time saving compared to the state of the art fast algorithm the recently developed depth sensors e g the kinect sensor have provided new opportunities for human computer interaction hci although great progress has been made by leveraging the kinect sensor e g in human body tracking face recognition and human action recognition robust hand gesture recognition remains an open problem compared to the entire human body the hand is a smaller object with more complex articulations and more easily affected by segmentation errors it is thus a very challenging problem to recognize hand gestures this paper focuses on building a robust part based hand gesture recognition system using kinect sensor to handle the noisy hand shapes obtained from the kinect sensor we propose a novel distance metric finger earth mover distance femd to measure the dissimilarity between hand shapes as it only matches the finger parts while not the whole hand it can better distinguish the hand gestures of slight differences the extensive experiments demonstrate that our hand gesture recognition system is accurate a mean accuracy on a challenging gesture dataset efficient average per frame robust to hand articulations distortions and orientation or scale changes and can work in uncontrolled environments cluttered backgrounds and lighting conditions the superiority of our system is further demonstrated in two real life hci applications index terms finger earth mover distance hand gesture recognition human computer interaction kinect system i introduction h and gesture recognition is of great importance for human computer interaction hci because of its extensive applications in virtual reality sign language recognition and computer games despite lots of previous work traditional vision based hand gesture recognition methods are still far from satisfactory for real life applications because of the nature of optical sensing the quality of the captured images is sensitive to lighting conditions and cluttered backgrounds thus optical sensor based methods are usually unable to detect and track the hands robustly which largely affects the performance of hand gesture recognition to enable a more robust hand gesture recognition one effective way is to use other sensors to capture the hand gesture manuscript received july revised october accepted october a preliminary version of this paper appeared in the acm international conference on multimedia this work was supported in part by the nanyang assistant professorship sug and microsoft research gift grant the associate editor coordinating the review of this manuscript and approving it for publication was eckehard g steinbach z ren j yuan and j meng are with nanyang technological university singapore z zhang is with microsoft research redmond wa usa color versions of one or more of the figures in this paper are available online at http ieeexplore ieee org digital object identifier tmm fig the first two columns illustrate three challenging cases for hand gesture recognition using kinect sensor where the first two hands have the same gesture while the third one confuses the recognition using the skeleton representation shown in red in the third column the last two hand gestures lead to very similar skeletons thus skeleton based matching algorithm classifies them as the same gesture in the last column the part based representations are illustrated using the proposed distance metric finger earth mover distance we can classify the first two hands as the same gesture and handle the noisy hand shapes obtained by kinect sensor and motion e g through the data glove unlike optical sensors such sensors are usually more reliable and are not affected by lighting conditions or cluttered backgrounds however as it requires the user to wear a data glove and sometimes requires calibration it is inconvenient to use and may hinder the natural articulation of hand gesture also such data gloves are usually more expensive than optical sensors e g cameras as a result it is not a very popular way for hand gesture recognition thanks to the recent development of inexpensive depth cameras e g the kinect sensor new opportunities for hand gesture recognition emerge instead of wearing a data glove using the kinect sensor can also detect and segment the hands robustly thus it provides a valid base for gesture recognition in spite of many recent successes in applying the kinect sensor to articulated face recognition human body tracking and human action recognition it is still an open problem to use kinect for hand gesture recognition due to the low resolution of the kinect depth map typically of only although it works well to track a large object e g the human body it is difficult to detect and segment a small object from an image with this resolution e g a human hand which occupies a very small portion of the image with more complex articulations in such a case the segmentation of the hand is usually inaccurate thus may significantly affect the recognition step to illustrate the above problem the first column of fig shows three examples it can be seen that the contours in the second column have significant local distortions in addition to pose variations due to the low resolution and inaccuracy of the 00 ieee ieee transactions on multimedia vol no august ii related work fig the framework of our part based hand gesture recognition system kinect sensor the two fingers of the second hand are indistinguishable as they are close to each other unfortunately classic shape recognition methods such as correspondence based shape matching algorithms and skeleton matching methods cannot robustly recognize shape contour with severe distortions for example as shown in the third column of fig the red skeletons of the last two hands are very similar hence skeleton matching algorithms classify them as the same gesture clearly recognizing such noisy hand contours is challenging especially if there are many hand gestures to recognize in order to address this problem we propose a novel shape distance metric called finger earth mover distance femd femd is specifically designed for hand shapes as it only matches the fingers while not the whole hand it can better handle the noisy hand shapes obtained by kinect sensor fig shows the framework of our hand gesture recognition system we use kinect sensor as the input device which captures both the color image and its corresponding depth map with the help of depth cue we can detect the user hand robustly to the cluttered backgrounds and lighting conditions then we represent the hand shape by its finger parts which is detected by shape decomposition finally the dissimilarity between the obtained hand shape and each gesture template is measured by the proposed distance metric femd for gesture recognition to evaluate our method we build a new challenging dataset containing cases collected in uncontrolled environments tests on this dataset shows that our hand gesture recognition system not only operates accurately and efficiently a mean accuracy of in per frame but also is robust to uncontrolled environments and hand gesture variations in orientation scale articulation and shape distortions we compare our algorithm with shape contexts and skeleton path similarity in section iv and show our superiority in hand gesture recognition furthermore on top of our gesture recognition algorithm we build two real life hci demos to illustrate the effectiveness of our method in section v the main contributions of this paper are as follows we propose a part based hand gesture recognition system based on a novel distance metric finger earth mover distance femd it is robust to orientation scale articulation changes as well as local distortions of hand shapes to our best knowledge this is the first part based hand gesture recognition system using kinect sensor we demonstrate our hand gesture recognition algorithm in two hci applications the proposed system operates accurately and efficiently in uncontrolled environments it is applicable to other hci applications many vision based hand gesture recognition approaches have been proposed in the literature see for more complete reviews vision based hand gesture recognition methods can be classified into two categories the first category is statistics learning based approaches for a dynamic gesture by treating it as the output of a stochastic process the hand gesture recognition can be addressed based on statistical modeling such as pca hmms and more advanced particle filtering and condensation algorithms the second category is rule based approaches rule based approaches propose a set of pre encoded rules between input features which are applicable for both dynamic gestures and static gestures when testing an hand gesture a set of features are extracted and compared with the encoded rules the gesture with the rule that best matches the test input is outputted as the recognized gesture unfortunately all existing hand gesture recognition methods have constraints on the user or the environment which greatly hinders its widespread use in real life applications on one hand to infer the pose of the palm and angles of the joints many methods use colored markers to extract high level features such as the fingertip joint locations or some anchor points on the palm on the other hand some methods proposed to represent the hand region by edges or an ellipse 25 using skin color model however a common problem of the methods in these two categories is the inaccurate hand segmentation none of these methods operates well in cluttered environments due to the sensitivity of colored markers and skin color model to the background besides a few studies try to first fully reconstruct the hand surfaces even though the data provides valuable information that can handle problems like self occlusion an accurate real time and robust reconstruction is still very difficult furthermore the high computational cost forbids its widespread adoption fortunately recent development of depth sensors e g kinect sensor provides a robust solution to hand segmentation however due to the low resolution and inaccuracy of the depth map the obtained hand contour can be quite noisy classic shape recognition methods are not robust to severe distortions in hand shapes for instance contour based recognition approaches such as moments are not robust when the contour is polluted by local distortions skeleton based recognition methods also suffer from contour distortions because even little noise or slight variations in the contour often severely perturb the topology of its skeletal representation bai et al proposed a skeleton pruning method in which makes skeleton robust to contour noise however skeleton based methods still cannot deal with the ambiguity problem as shown in fig as the second and the third shape have more similar skeletons than that of the first and the second shape as for the correspondence based shape recognition methods such as shape contexts and inner distance they are not effective in solving the ambiguity in fig either because the correspondences of the second and the last hands have more similar contexts than the first and the second one do ren et al robust part based hand gesture recognition using kinect sensor template matching we use template matching for recognition i e the input hand is recognized as the class with which it has the minimum dissimilarity distance where fig hand detection a the rough hand segmented by depth thresholding b a more accurate hand detected with black belt the green line the initial point the red point and the center point the cyan point c its time series curve representation iii part based hand gesture recognition now we introduce our part based hand gesture recognition system fig illustrates the framework which consists of two major modules hand detection and hand gesture recognition a hand detection as shown in fig we use kinect sensor as the input device which captures the color image and the depth map at 480 resolution generally the depth information derived from kinect sensor is usable but not very accurate in details in order to segment the hand shape firstly we locate the hand position using the kinect windows sdk hand tracking function then by thresholding from the hand position with a certain depth interval a rough hand region can be obtained as shown in fig a second we require the user to wear a black belt on the gesturing hand wrist in order to more accurately segment the hand shape after detecting the black color pixels we use ransac to fit a line to locate the black belt as shown in fig b the hand shape is generally of pixel resolution with possibly severe distortions after detecting the hand shape we represent it as a time series curve as shown in fig c such a shape representation has been successfully used for the classification and clustering of shapes the time series curve records the relative distance between each contour vertex and a center point we define the center point as the point with the maximal distance after distance transform on the shape the cyan point as shown in fig b and the initial point the red point is defined according to the ransac line detected from the black belt the green line in our time series representation the horizontal axis denotes the angle between each contour vertex and the initial point relative to the center point normalized by the vertical axis denotes the euclidean distance between the contour vertices and the center point normalized by the radius of the maximal inscribed circle as shown in fig the time series curve captures nice topological properties of the hand such as the finger parts b hand gesture recognition the hand gesture recognition module in fig is the major part of our part based hand gesture recognition system with the hand shape and its time series curve we now present how to robustly recognize the hand gesture is the input hand is the template of class denotes the proposed finger earth mover distance between the input hand and each template now we introduce the finger earth mover distance finger earth mover distance in rubner et al presented a general and flexible metric called earth mover distance emd to measure the distance between signatures or histograms emd is widely used in many problems such as content based image retrieval and pattern recognition emd is a measure of the distance between two probability distributions it is named after a physical analogy that is drawn from the process of moving piles of earth spread around one set of locations into another set of holes in the same space the location of earth pile and hole denotes the mean of each cluster in the signatures the size of each earth pile or hole is the weight of cluster and the ground distance between a pile and a hole is the amount of work needed to move a unit of earth to use this transportation problem as a distance measure i e a measure of dissimilarity one seeks the least costly transportation the movement of earth that requires the least amount of work references and applied emd to shape matching and contour retrieval which represents the contour by a set of local descriptive features and computes the set of correspondences with minimum emd costs between the local features however the existing emd based contour matching algorithms have two deficiencies when applied to hand gesture recognition two hand shapes differ mainly in global features while not local features as shown in fig a and b the fingers global features are their major difference besides the large number of local features slows down the speed of contour matching therefore it is better to consider global features in contour matching emd allows for partial matching i e a signature and its subset are considered to be the same in emd measure as in fig c and d the emd distance of these two signatures is zero because the signature in fig d is a subset of fig c however in many situations partial matching is illogical such as in the case of fig a and b where the finger in fig b is a partial set of the fingers in fig a clearly they should be considered different our finger earth mover distance femd can address these two deficiencies of the contour matching methods using emd different from the emd based algorithm which considers each local feature as a cluster we represent the input hand by global features the finger clusters and we add penalty on empty holes to alleviate partial matches on global features formally let be the first hand signature with clusters where is the cluster representative and is the weight of the cluster is the second hand signature with clusters now we show how to represent a time series curve as a signature fig e and f show the time series ieee transactions on multimedia vol no august fig a b two hand shapes whose time series curves are shown in e f c d two signatures that partially match whose emd cost is e f illustration of the signature representations of time series curves fig the parts in color are the fingers detected by the proposed finger detection methods a near convex decomposition b thresholding decomposition curves of the hands in fig a and b respectively where each finger corresponds to a segment of the curve we define each cluster of a signature as the finger segment of the time series curve the representative of each cluster is defined as the angle interval between the endpoints of each segment where and the weight of a cluster is defined as the normalized area within the finger segment is the ground distance matrix of signature and where is the ground distance from cluster to is defined as the minimum moving distance for interval to totally overlap with i e totally overlap with otherwise for two signatures and their femd distance is defined as the least work needed to move the earth piles plus the penalty on the empty hole that is not filled with earth in section iv c as we can see are constants for the two signatures to compute the femd we need to compute the value of is defined by minimizing the work needed to move all the earth piles we follow the definition of the flow matrix in emd as we also intend to find the minimum work needed to move the earth piles the first constraint restricts the moving flow to one direction from earth piles to the holes the last constraint forces the maximum amount of earth possible to be moved we will demonstrate the superiority of femd over emd for contour matching in section iv c c finger detection where is the normalization factor is the flow from cluster to cluster which constitutes the flow matrix parameter modulates the importance between the first and the second terms we will investigate the effects of in order to measure the femd distance between hand shapes we need to represent the hand shape as a signature with each finger as a cluster namely to detect the finger parts from the hand shape in fig we propose two finger detection methods to obtain the finger parts from the hand shapes now we introduce these two algorithms near convex decomposition we note that the fingers have a common geometric property they are near convex parts ren et al robust part based hand gesture recognition using kinect sensor of the hand shape therefore we adjust the minimum nearconvex decomposition mncd proposed in to a finger detection method which is illustrated in fig a the goal of the first term in the objective function is to reduce the redundant parts that are not fingers and the second term is to improve the visual naturalness of the decomposition parameter balances the influence between the first and the second term we will investigate the effects of in section iv c thresholding decomposition although near convex decomposition algorithm can detect the finger parts accurately it is generally complexly formulated and cannot be solved in real time thus we propose an alternative finger detection methods that are more efficient named thresholding decomposition as shown in fig b as mentioned before the time series curve reveals a hand topological information well as shown in fig each finger corresponds to a peak in the curve therefore we can apply the height information in time series curve to decompose the fingers specifically we define a finger as a segment in the timeseries curve whose height is greater than a threshold in this way we can detect the fingers fast however choosing a good height threshold is essential we will investigate the effects of in section iv c fig our system is robust to cluttered backgrounds a the hand that is cluttered by background can be detected accurately b the hand that is cluttered by face can be detected accurately iv evaluations a dataset we collect a new hand gesture dataset using kinect sensor http www ntu edu sg home renzhou handgesture htm our dataset is collected from subjects and it contains gestures for number to each subject performs different poses for the same gesture thus in total our dataset has each of which consists of a color image and the corresponding depth map our dataset is a very challenging real life dataset which is collected in cluttered backgrounds besides for each gesture the subject poses with variations in hand orientation scale articulation etc b performance evaluation all experiments were done on a intel core quad ghz cpu with gb of ram now we evaluate the performance of our system from the following aspects robustness to cluttered backgrounds our hand gesture recognition system is robust to cluttered backgrounds because the hand shape is detected using the depth information thus the backgrounds can be easily removed fig a illustrates an example when the hand is cluttered by the background which is hard for other hand gesture recognition methods that use colored markers to detect the hand in fig b it shows a difficult case for the skin color based hand gesture recognition approaches where the hand is cluttered by the user face fig our method is robust to orientation and scale changes a the hands with orientation changes and their time series curves b the hands with scale changes and their time series curves however our hand segmentation is very accurate using kinect sensor as shown in the right column of fig robustness to distortions and hand variations in orientation scale articulation in real life environment a hand can have variations on orientation scale and articulation besides because of the limited resolution of the depth map the hand shapes are always distorted or ambiguous however we can demonstrate that the proposed dissimilarity distance metric finger earth mover distance femd is not only robust to the orientation and scale changes of the hand but also insensitive to distortions and articulations fig a shows hands with different orientations as we can see the initial point the red point on the figure and the center point the blue point are relatively fixed in these shapes thus the time series curves of these hands the second row in fig a are similar and their distances are very small in ieee transactions on multimedia vol no august table i the mean accuracy and the mean running time of shape contexts skeleton matching and our methods our part based hand gesture recognition system using femd outperforms the traditional shape matching algorithms fig two pairs of confusing gestures in experiment i a gesture and b gesture and fig our system is insensitive to the distortions and articulation fig b there are hands of different size because the timeseries curve and the femd distance are normalized they are correctly recognized as the same gesture hence we can conclude that femd is robust to orientation and scale changes furthermore our hand gesture recognition method is robust to the articulations and distortions brought by imperfect hand segmentation since the proposed femd distance metric uses global features fingers to measure the dissimilarity local distortions are tolerable as for articulations fig shows an example the leftmost column shows hand images of the same gesture the middle column shows the corresponding hand shapes and the rightmost column shows their time series curves as we can see the hand shapes in fig c and d are heavily distorted however as illustrated in the rightmost column of fig by detecting the finger parts the yellow regions we represent each shape as a signature whose clusters are the finger parts particularly the signatures of fig a and b have clusters and the signatures of fig c and d only have cluster from section iii we can estimate that and the ground distance according to the definition we know that the femd distances among the therefore our femd metric is insensitive to distortions and articulations accuracy and efficiency in order to evaluate the accuracy and efficiency of our system two experiments are conducted on the new dataset experiment i uses thresholding decomposition as discussed in section iii to detect the finger parts for femd measurement and experiment ii uses near convex decomposition as illustrated in section iii for femd finger detection experiment i thresholding decomposition femd in experiment i we fix the height threshold and the femd parameter fig is the confusion matrix of experiment i the mean accuracy is as it shows the two most confused gesture categories are gesture and gesture and fig shows two confused cases of these categories because the thumb is shorter and smaller if decomposing the hands only by a height thresholding important finger regions may be lost in some cases as shown in fig the thumbs are not well decomposed as a result the femd distances of these two cases are very small which confuse the recognition however thresholding decomposition is fast besides due to the few number of extracted global features femd operates efficiently table i gives the mean running time of a hand recognition procedure in experiment i it should be noted that our femd algorithm is mainly rewritten and optimized in c rather than just using matlab as in our previous work which leads to the better results than as we see thresholding decomposition based femd runs in real time experiment ii near convex decomposition femd in order to more accurately decompose the fingers from the hands we conduct another experiment which detects the fingers using near convex decomposition here we fixed the nearconvex decomposition parameter and the femd parameter fig shows some finger detection results of our near convex decomposition algorithm as we see the finger ren et al robust part based hand gesture recognition using kinect sensor fig the confusion matrix of experiment i fig the confusion matrix of hand gesture recognition using shape context a is the result of recognition computed without bending cost and b is the result computed with bending cost fig the confusion matrix of experiment ii fig finger detection results of experiment ii using near convex decomposition algorithm parts detection results are more accurate than thresholding decomposition fig shows the confusion matrix of experiment ii there are no seriously confused categories in the fourth row of table i the mean accuracy and the mean running time of experiment ii are given the mean accuracy of experiment ii is higher than that of experiment i owing to the more accurate finger decomposition on the other hand the speed of experiment ii is slower than that of experiment i because of the more complex finger detection algorithm comparison with other methods femd is a part based hand matching metric we compare it with the traditional correspondence based matching algorithm shape context and the skeleton based matching algorithm path similarity their mean accuracies and mean running times are given in fig the confusion matrix of hand gesture recognition using skeleton matching table i we pre segment the hand shape using the same method as ours in section iii a fig illustrates the confusion matrixes of shape context from both fig a and b we find that the most confusing classes are gesture and the reason is that the fingers are more easily distorted in these classes making them indistinguishable which we have discussed before in figs and fig shows some confusing cases for shape context where shapes are locally distorted from the first two rows of table i we notice that considering the bending cost of tps transformation worsens the recognition ieee transactions on multimedia vol no august fig some confusing cases for shape context where shapes are locally distorted fig some confusing cases for skeleton matching where very different shapes have similar skeletons performance compared to shape contexts computed without the bending cost the reason is that in order to be rotation invariant shape context needs to treat the tangent vector at each point as the positive axis for the log polar histogram frame however since our shape is binary a small variation on the shape could cause severe change of the tangent vectors at points on the shape thus adding tps bending cost worsens the performance fig shows the confusion matrix of skeleton matching we first prune the noisy skeleton using the method proposed in and match them using path similarity proposed in from the figure we notice that many gestures are severely confused such as between gesture and gesture and the reason is that in those cases their skeletons have very similar global structure as shown in fig very different hand gestures in a b have very similar skeletons thus skeleton matching algorithms are unable to differentiate these classes c parameter sensitivity in this section we evaluate important parameters the height threshold in thresholding decomposition finger detection method section iii the parameter in near convex decomposition section iii and the parameter in femd formulation section iii the results are shown in fig in thresholding decomposition determines the radius of the decomposing circle see fig b if is too small i e the fingers cannot be well decomposed and if is too large i e essential finger regions will be lost fig a shows that we can obtain the best result if setting around in finger detection using near convex decomposition balances the impact of the visual naturalness and the number of parts as shown in fig b if we only minimize the visual naturalness term i e we will obtain noisy parts that affect the femd measure besides the curve drops fast after because if minimizing the parts number too much while ignoring the visual naturalness we may obtain parts that are not fingers in the femd measure modulates importance between the earth moving work and the empty hole penalty fig c shows that if either only considering i e or only considering i e femd cannot measure correct dissimilarity between hand shapes this curve also justifies fig parameter sensitivity on the emd metric and when femd becomes that femd is better than emd the special case when for dissimilarity measure between hand shapes v applications lately there has been a great emphasis on human computer interaction hci research to create easy to use interfaces by facilitating natural communication and manipulation skills of humans among different human body parts the hand is the most effective interaction tool because of its dexterity adopting hand gesture as an interface in hci will not only allow the deployment of a wide range of applications in sophisticated computing environments such as virtual reality systems and interactive gaming platforms but also benefit our daily life such as providing aids for the hearing impaired and maintaining absolute sterility in health care environments using touchless interfaces via gestures now we propose to use the hand gesture as an interface and introduce two real life hci applications on top of our hand gesture recognition system arithmetic computation and rock paper scissors game it should be noticed that in the demo system we perform frame based hand gesture recognition using the femd distance metric based on thresholding decomposition finger detection method taking both accuracy and efficiency into consideration as shown in table i it runs in real time and achieves comparable accuracy as that of femd metric based on finger detection using near convex decomposition a arithmetic computation arithmetic computation is an interesting hci application instead of interacting with the computer by the keyboard or mouse we input arithmetic commands to the computer via hand gestures as shown in fig hand gestures are defined to represent commands namely number and operator respectively by recognizing each input gesture as a command the computer can perform arithmetic computations instructed by the ren et al robust part based hand gesture recognition using kinect sensor fig arithmetic computation fig the gesture commands in our arithmetic computation system user two examples are shown in fig the key frames are shown as well b rock paper scissors game rock paper scissors is a traditional game the rule is rock breaks scissors scissors cut paper and paper wraps rock in this demo we build a rock paper scissors game system played between a human and a computer the computer randomly chooses a weapon and the user gesture is recognized by our system according to the game rule our system can decide who is the winner fig shows two examples this two demos have been demonstrated in acm multimedia etc it runs accurately in real time it is feasible to build more interesting kinect demos on top of our hand gesture recognition system the hand gesture dataset we collected with kinect sensor and the technical demo video showing these two hci applications are available at http eeeweba ntu edu sg computervision people home renzhou vi conclusion and future work hand gesture recognition for real life applications is very challenging because of its requirements on the robustness accuracy and efficiency in this paper we presented a robust part based hand gesture recognition system using the kinect sensor a novel distance metric finger earth mover distance femd is used for dissimilarity measure which represents the hand shape as a signature with each finger part as a cluster and penalizes the empty finger holes extensive experiments on a challenging gesture dataset validate that our part based hand gesture recognition system is accurate and efficient more specifically our femd based hand gesture recognition system achieves mean accuracy and runs in per frame fig rock paper scissors game when using the thresholding decomposition finger detection method and it achieves better accuracy of when using a more accurate finger detection method however at the cost of efficiency taking both accuracy and efficiency into consideration we use thresholding decomposition for finger detection in our real time demo system one major contribution of our paper is the distance metric based on part based representation traditional distance measures such as shape contexts distance and path similarity is not robust to local distortions and shape variations since their representations i e shape contexts and skeleton are not consistent in the case of hand variations or severe local distortions the proposed femd distance metric is based on a part based representation which represents a hand shape as a signature with each finger part as a cluster such a representation enables the computation on the global features thus it is robust to local distortions and it is robust to articulation orientation scale changes as discussed in section iv another contribution of this paper is the real life hci applications we built on top of our hand gesture recognition system it shows that with hand gesture recognition technique we can mimic the communications between human and involve hand gesture as a natural and intuitive way to interact with machines consequently we can benefit our daily life in many aspects such as providing aids for the hearing impaired and maintaining absolute sterility in health care environments using touchless interfaces via gestures our future research will focus on exploring a more efficient part based representation to handle the problem shown in fig and the efficiency drawback of near convex decomposition based finger detection method and we will further develop interesting hci applications of our hand gesture recognition system matconvnet is a matlab toolbox implementing convolutional neural networks cnn for computer vision applications since the breakthrough work of cnns have had a major impact in computer vision and image understanding in particular essentially replacing traditional image representations such as the ones implemented in our own vlfeat open source library while most cnns are obtained by composing simple linear and non linear filtering operations such as convolution and rectification their implementation is far from trivial the reason is that cnns need to be learned from vast amounts of data often millions of images requiring very efficient implementations as most cnn libraries matconvnet achieves this by using a variety of optimisations and chiefly by supporting computations on gpus numerous other machine learning deep learning and cnn open source libraries exist to cite some of the most popular ones cudaconvnet torch theano and many of these libraries are well supported with dozens of active contributors and large user bases therefore why creating yet another library permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page copyrights for components of this work owned by others than the author must be honored abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee request permissions from permissions acm org mm october brisbane australia copyright is held by the owner author publication rights licensed to acm acm 00 doi http dx doi org 2807412 https code google com p cuda convnet http cilvr nyu edu doku php id code start http deeplearning net software theano http caffe berkeleyvision org the key motivation for developing matconvnet was to provide an environment particularly friendly and efficient for researchers to use in their investigations matconvnet achieves this by its deep integration in the matlab environment which is one of the most popular development environments in computer vision research as well as in many other areas in particular matconvnet exposes as simple matlab commands cnn building blocks such as convolution normalisation and pooling section these can then be combined and extended with ease to create cnn architectures while many of such blocks use optimised cpu and gpu implementations written in c and cuda section matlab native support for gpu computation means that it is often possible to write new blocks in matlab directly while maintaining computational efficiency compared to writing new cnn components using lower level languages this is an important simplification that can significantly accelerate testing new ideas using matlab also provides a bridge towards other areas for instance matconvnet was recently used by the university of arizona in planetary science as summarised in this nvidia blogpost matconvnet can learn large cnn models such alexnet and the very deep networks of from millions of images pre trained versions of several of these powerful models can be downloaded from the matconvnet home page section while powerful matconvnet remains simple to use and install the implementation is fully selfcontained requiring only matlab and a compatible c compiler using the gpu code requires the freely available cuda devkit and a suitable nvidia gpu as demonstrated in figure and section it is possible to download compile and install matconvnet using three matlab commands several fully functional examples demonstrating how small and large networks can be learned are included importantly several standard pre trained network can be immediately downloaded and used in applications a manual with a complete technical description of the toolbox is maintained along with the toolbox these features make matconvnet useful in an educational context too matconvnet is open source released under a bsd like license it can be downloaded from http www vlfeat org matconvnet as well as from github the key command in this example is a wrapper that takes as input the cnn net and the preprocessed image and produces as output a structure res of results this particular wrapper can be used to model networks that have a simple structure namely a chain of operations examining the code of edit in matlab we note that the wrapper transforms the data sequentially applying a number of matlab functions as specified by the network configuration these function discussed in detail in section are called building blocks and constitute the backbone of matconvnet while most blocks implement simple operations what makes them non trivial is their efficiency section as well as support for backpropagation section to allow learning cnns next we demonstrate how to use one of such building blocks directly for the sake of the example consider convolving an image with a bank of linear filters start by reading an image in matlab say using im single imread peppers png obtaining a h w d array im where d is the number of colour channels in the image then create a bank of k random filters of size using f randn single finally convolve the image with the filters by using the command y x f this results in an array y with k channels one for each of the k filters in the bank while users are encouraged to make use of the blocks directly to create new architectures matconvnet provides wrappers such as for standard cnn architectures such as alexnet or network in network furthermore the library provides numerous examples in the examples subdirectory including code to learn a variety models on the mnist cifar and imagenet datasets all these examples use the examples training code which is an implementation of stochastic gradient descent section while this training code is perfectly serviceable and quite flexible it remains in the examples subdirectory as it is somewhat problem specific users are welcome to implement their optimisers getting started matconvnet is simple to install and use figure provides a complete example that classifies an image using a latest generation deep convolutional neural network the example includes downloading matconvnet compiling the package downloading a pre trained cnn model and evaluating the latter on one of matlab stock images overview of the available blocks in order to understand the design of the building blocks it is necessary to first review the fundamentals of cnns on the outset a convolutional neural network cnn is a function f mapping data x for example an image to an output vector y the function f fl is the composition of a sequence of simpler functions fl which we call computational blocks or layers let xl be the outputs of each layer in the network and let x denote the network input each output xl fl xl wl is computed from the previous output xl by applying the function fl with parameters wl the data flowing through the network has a spatial structure namely xl rhl wl dl is a array whose first two dimensions are interpreted as spatial coordinates it therefore represents a feature field a fourth non singleton dimension in the array allows processing batches of images in parallel which is important for efficiency the network is called convolutional because the while from a user perspective matconvnet currently relies on matlab the library is being developed with a clean separation between matlab code and the c and cuda core therefore in the future the library may be extended to allow processing convolutional networks independently of matlab http devblogs nvidia com parallelforall deep learning image understanding planetary science http www vlfeat org matconvnet matconvnet manual pdf an example laboratory experience based on matconvnet can be downloaded from http www robots ox ac uk vgg practicals cnn index html building blocks at the core of matconvnet there is a library of cnn building blocks such as the convolution operator seen above this section discusses in detail these building blocks and their design http ww github com matconvnet functions fl act as local and translation invariant operators i e non linear filters matconvnet includes a variety of building blocks contained in the matlab directory such as convolution convolution transpose or deconvolution max and average pooling relu activation sigmoid activation softmax operator classification log loss batch normalization spatial normalization cross channel normalization or p distance the library of blocks is sufficiently extensive that many interesting stateof the art network can be implemented and learned using the toolbox or even ported from other toolboxes such as caffe cnns are used as classifiers or regressors in the example of figure the output y f x is a vector of probabilities one for each of a 000 possible image labels dog cat trilobite if y is the true label of image x we can measure the cnn performance by a loss function y y r which assigns a penalty to classification errors the cnn parameters can then be tuned or learned to minimise this loss averaged over a large dataset of labelled example images learning generally uses a variant of stochastic gradient descent sgd while this is an efficient method for this type of problems networks may contain several million parameters and need to be trained on millions of images thus efficiency is a paramount in matconvnet design as further discussed in section sgd requires also to compute the cnn derivatives as explained in the next section dropout top val dropout top val bnorm top val bnorm top val epoch figure training alexnet on imagenet ilsvrc dropout vs batch normalisation x w to convolve input x and obtain output y in the backward mode one calls dzdx dzdw x w dzdy to understand this syntax imagine that f is connected to the rest of the network denoted z terminating in a scalar loss x f y z z r w here dzdy is the derivative dz dy of the downstream subnetwork z dzdx the derivative d z f dx of the upstream subnetwork i e prefixed by f and dzdx and dzdw the corresponding derivatives w r t x and w as explained above dzdx dzdw and dzdy have the same dimension of x w and y in this manner the computation of larger jacobians is encapsulated in the function call and never carried explicitly another way of looking at this is that instead of computing a derivative such as dy dw one always computes a projection of the type hdz dy dy dwi backpropagation the fundamental operation to learn a network is computing the derivative of the loss with respect to the network parameters as this is required for gradient descent this is obtained using an algorithm called backpropagation which is an application of the chain rule for derivatives d y f x wl dwl d y fl fl xl dfl xl wl dx dwl l documentation and examples there are three main sources of information about matconvnet first the website contains descriptions of all the functions and several examples and tutorials second there is a pdf manual containing a great deal of technical details about the toolbox including detailed mathematical descriptions of the building blocks third matconvnet ships with several examples section most examples are fully self contained for example in order to run the mnist example it suffices to point matlab to the matconvnet root directory and type addpath examples followed by due to the problem size the imagenet ilsvrc example requires some more preparation including downloading and preprocessing the images using the bundled script utils preprocess imagenet sh several advanced examples are included as well for example figure illustrates the top and top validation errors as a model similar to alexnet is trained using either standard dropout regularisation or the recent batch normalisation technique of the latter is shown to converge in about one third of the where for notational simplicity that data and parameters are identified with vectors which is always possible up to stacking note that the formula involves computing the derivative of parts of the network with respect to the data these are obtained in a similar manner and require computing the derivative one level up overall derivatives are computed by backtracking from the last layer output to the first input since the loss function output is scalar the dimension of all the intermediate derivatives is the same as the corresponding parameter for example d y fl fl dx l has hl wl dl components equal to the number of elements of xl compare this to a jacobian such as dfl dx l that has hl wl dl hl wl dl components instead the key in implementing backpropagation efficiently is to store only the smaller derivatives leaving the intermediate calculations of the larger jacobians implicit this is best illustrated with an example consider a layer f such as the convolution operator implemented by in the so called forward mode one calls the function as y see also http www robots ox ac uk vgg practicals cnn index html model alexnet vgg f vgg m vgg s vgg vd vgg vd epochs passes through the training data required by the former the matconvnet website contains also numerous pretrained models i e large cnns trained on imagenet ilsvrc that can be downloaded and used as a starting point for many other problems these include alexnet vgg s vgg m vgg s and vgg vd and vgg vd the example code of figure shows how one such models can be used in a few lines of matlab code cpu gpu 15 cudnn table imagenet training speed images num gpus vgg vd speed speed efficiency is very important for working with cnns matconvnet supports using nvidia gpus as it includes cuda implementations of all algorithms or relies on matlab cuda support to use the gpu provided that suitable hardware is available and the toolbox has been compiled with gpu support one simply converts the arguments to gpuarrays in matlab as in y gpuarray x gpuarray w in this manner switching between cpu and gpu is fully transparent note that matconvnet can also make use of the nvidia cudnn library which significant speed and space benefits next we evaluate the performance of matconvnet when training large architectures on the imagenet ilsvrc challenge data the test machine is a dell server with two intel xeon cpu clocked at ghz each cpu has eight cores gb of ram and four nvidia titan black gpus only one of which is used unless otherwise noted experiments use matconvnet cudnn and matlab the data is preprocessed to avoid rescaling images on the fly in matlab and stored in a ram disk for faster access the code uses the command to read large batches of jpeg images from disk in a number of separate threads the driver examples m is used in all experiments we train the models discussed in section on imagenet ilsvrc table reports the training speed as number of images per second processed by stochastic gradient descent alexnet trains at about images with cudnn which is about faster than the vanilla gpu implementation using cublas and more than times faster than using the cpus furthermore we note that despite matlab overhead the implementation speed is comparable to caffe they report images with cudnn and a titan a slightly slower gpu than the titan black used here note also that as the model grows in size the size of a sgd batch must be decreased to fit in the gpu memory increasing the overhead impact somewhat table reports the speed on vgg vd a very large model using multiple gpus in this case the batch size is set to images these are further divided in sub batches of images each to fit in the gpu memory the latter are then distributed among one to four gpus on the same machine while there is a substantial communication overhead training speed increases from images to addressing this overhead is one of the medium term goals of the library batch sz 128 table multiple gpu speed images in matlab and allows easy experimentation with novel ideas matconvnet is already sufficient for advanced research in deep learning despite being introduced less than a year ago it is already citied times in arxiv papers and has been used in several papers published at the recent cvpr conference as cnns are a rapidly moving target matconvnet is developing fast so far there have been ad interim releases incrementally adding new features to the toolbox several new features including support for dags will be included in the upcoming releases starting in august the goal is to ensure that matconvnet will stay current for the next several years of research in deep learning acknowledgments we kindly thank nvidia supporting this project by providing us with top of the line gpus and mathworks for ongoing discussion on how to improve the library andrea vedaldi is partially supported by the erc stg and karel lenc by a dta of the university of oxford the compression capability of several generations of video coding standards is compared by means of peak signal to noise ratio psnr and subjective testing results a unied approach is applied to the analysis of designs including h mpeg video h mpeg visual h mpeg advanced video coding avc and high efciency video coding hevc the results of subjective tests for wvga and hd sequences indicate that hevc encoders can achieve equivalent subjective reproduction quality as encoders that conform to h mpeg avc when using approximately 50 less bit rate on average the hevc design is shown to be especially effective for low bit rates high resolution video content and low delay communication applications the measured subjective improvement somewhat exceeds the improvement measured by the psnr metric index terms advanced video coding avc h high efciency video coding hevc jct vc mpeg mpeg standards vceg video compression i introduction t he primary goal of most digital video coding standards has been to optimize coding efficiency coding efficiency is the ability to minimize the bit rate necessary for representation of video content to reach a given level of video quality or as alternatively formulated to maximize the video quality achievable within a given available bit rate the goal of this paper is to analyze the coding efficiency that can be achieved by use of the emerging high efficiency manuscript received may revised august accepted august date of publication october date of current version january this paper was recommended by associate editor h gharavi j r ohm is with the institute of communication engineering rwth aachen university aachen germany e mail ohm ient rwthaachen de g j sullivan is with microsoft corporation redmond wa usa e mail garys ieee org h schwarz is with the fraunhofer institute for telecommunications heinrich hertz institute berlin germany e mail heiko schwarz hhi fraunhofer de t k tan is with m sphere consulting pte ltd singapore and also with ntt docomo inc tokyo japan e mail tktan msph com t wiegand is with the fraunhofer institute for telecommunications heinrich hertz institute berlin germany and also with the berlin institute of technology berlin germany e mail twiegand ieee org color versions of one or more of the figures in this paper are available online at http ieeexplore ieee org digital object identifier tcsvt 2221192 video coding hevc standard relative to the coding efficiency characteristics of its major predecessors including h mpeg video h mpeg visual and h mpeg advanced video coding avc when designing a video coding standard for broad use the standard is designed in order to give the developers of encoders and decoders as much freedom as possible to customize their implementations this freedom is essential to enable a standard to be adapted to a wide variety of platform architectures application environments and computing resource constraints this freedom is constrained by the need to achieve interoperability i e to ensure that a video signal encoded by each vendor products can be reliably decoded by others this is ordinarily achieved by limiting the scope of the standard to two areas cp fig specifying the format of the data to be produced by a conforming encoder and constraining some characteristics of that data such as its maximum bit rate and maximum frame rate without specifying any aspects of how an encoder would process input video to produce the encoded data leaving all preprocessing and algorithmic decision making processes outside the scope of the standard specifying or bounding the approximation of the decoded results to be produced by a conforming decoder in response to a complete and error free input from a conforming encoder prior to any further operations to be performed on the decoded video providing substantial freedom over the internal processing steps of the decoding process and leaving all postprocessing loss error recovery and display processing outside the scope as well this intentional limitation of scope complicates the analysis of coding efficiency for video coding standards as most of the elements that affect the end to end quality characteristics are outside the scope of the standard in this paper the emerging hevc design is analyzed using a systematic approach that is largely similar in spirit to that previously applied to the analysis of the first version of h mpeg avc in a major emphasis in this analysis is the application of a disciplined and uniform approach for optimization of each of c ieee 00 ieee transactions on circuits and systems for video technology vol no december the video encoders additionally a greater emphasis is placed on subjective video quality analysis than what was applied in as the most important measure of video quality is the subjective perception of quality as experienced by human observers the paper is organized as follows section ii describes the syntax features of the investigated video coding standards and highlights the main coding tools that contribute to the coding efficiency improvement from one standard generation to the next the uniform encoding approach that is used for all standards discussed in this paper is described in section iii in section iv the current performance of the hevc reference implementation is investigated in terms of toolwise analysis and in comparison with previous standards as assessed by objective quality measurement particularly peak signal to noise ratio psnr section v provides results of the subjective quality testing of hevc in comparison to the previous best performing standard h mpeg avc ii syntax overview the basic design of all major video coding standards since h in follows the so called block based hybrid video coding approach each block of a picture is either intrapicture coded also known as coded in an intra coding mode without referring to other pictures of the video sequence or it is temporally predicted i e inter picture coded also known as coded in an inter coding mode where the prediction signal is formed by a displaced block of an already coded picture the latter technique is also referred to as motion compensated prediction and represents the key concept for utilizing the large amount of temporal redundancy in video sequences the prediction error signal or the complete intra coded block is processed using transform coding for exploiting spatial redundancy the transform coefficients that are obtained by applying a decorrelating linear or approximately linear transform to the input signal are quantized and then entropy coded together with side information such as coding modes and motion parameters although all considered standards follow the same basic design they differ in various aspects which finally results in a significantly improved coding efficiency from one generation of standard to the next in the following we provide an overview of the main syntax features for the considered standards the description is limited to coding tools for progressive scan video that are relevant for the comparison in this paper for further details the reader is referred to the draft hevc standard the prior standards and corresponding books 15 and overview articles in order to specify conformance points facilitating interoperability for different application areas each standard defines particular profiles a profile specifies a set of coding tools that can be employed in generating conforming bitstreams we concentrate on the profiles that provide the best coding efficiency for progressive scanned bit per sample video with the chroma sampling format as the encoding of interlacedscan video high bit depths and non material has not been in the central focus of the hevc project for developing the first version of the standard a itu t rec h iso iec mpeg video h mpeg video was developed as an official joint project of itu t and iso iec jtc it was finalized in and is still widely used for digital television and the dvdvideo optical disc format similarly as for its predecessors h and mpeg video each picture of a video sequence is partitioned into macroblocks mbs which consist of a luma block and in the chroma sampling format two associated chroma blocks the standard defines three picture types i p and b pictures i and p pictures are always coded in display output order in i pictures all mbs are coded in intra coding mode without referencing other pictures in the video sequence an mb in a p picture can be either transmitted in intra or in inter mode for the inter mode the last previously coded i or p picture is used as reference picture the displacement of an inter mb in a p picture relative to the reference picture is specified by a half sample precision motion vector the prediction signal at half sample locations is obtained by bilinear interpolation in general the motion vector is differentially coded using the motion vector of the mb to the left as a predictor the standard includes syntax features that allow a particularly efficient signaling of zero valued motion vectors in h mpeg video b pictures have the property that they are coded after but displayed before the previously coded i or p picture for a b picture two reference pictures can be employed the i p picture that precedes the b picture in display order and the i p picture that succeeds it when only one motion vector is used for motion compensation of an mb the chosen reference picture is indicated by the coding mode b pictures also provide an additional coding mode for which the prediction signal is obtained by averaging prediction signals from both reference pictures for this mode which is referred to as the biprediction or bidirectional prediction mode two motion vectors are transmitted consecutive runs of inter mbs in b pictures that use the same motion parameters as the mb to their left and do not include a prediction error signal can be indicated by a particularly efficient syntax for transform coding of intra mbs and the prediction errors of inter mbs a discrete cosine transform dct is applied to blocks of samples the dct coefficients are represented using a scalar quantizer for intra mbs the reconstruction values are uniformly distributed while for inter mbs the distance between zero and the first nonzero reconstruction values is increased to three halves of the quantization step size the intra dc coefficients are differentially coded using the intra dc coefficient of the block to their left if available as their predicted value for perceptual optimization the standard supports the usage of quantization weighting matrices by which effectively different quantization step sizes can be used for different transform coefficient frequencies the transform coefficients of a block are scanned in a zig zag manner and transmitted using d run level variable length coding vlc two vlc tables are specified for quantized transform ohm et al comparison of the coding efficiency of video coding standards coefficients also known as transform coefficient levels one table is used for inter mbs for intra mbs the employed table can be selected at the picture level the most widely implemented profile of h mpeg video is the main profile mp it supports video coding with the chroma sampling format and includes all tools that significantly contribute to coding efficiency the main profile is used for the comparisons in this paper b itu t recommendation h the first version of itu t rec h defines syntax features that are very similar to those of h mpeg video but it includes some changes that make it more efficient for low delay low bit rate coding the coding of motion vectors has been improved by using the component wise median of the motion vectors of three neighboring previously decoded blocks as the motion vector predictor the transform coefficient levels are coded using a d run level last vlc with tables optimized for lower bit rates the first version of h contains four annexes annexes d through g that specify additional coding options among which annexes d and f are frequently used for improving coding efficiency the usage of annex d allows motion vectors to point outside the reference picture a key feature that is not permitted in h mpeg video annex f introduces a coding mode for p pictures the inter mode in which four motion vectors are transmitted for an mb each for an subblock it further specifies the usage of overlapped block motion compensation the second and third versions of h which are often called h and h respectively add several optional coding features in the form of annexes annex i improves the intra coding by supporting a prediction of intra ac coefficients defining alternative scan patterns for horizontally and vertically predicted blocks and adding a specialized quantization and vlc for intra coefficients annex j specifies a deblocking filter that is applied inside the motion compensation loop annex o adds scalability support which includes a specification of b pictures roughly similar to those in h mpeg video some limitations of version in terms of quantization are removed by annex t which also improves the chroma fidelity by specifying a smaller quantization step size for chroma coefficients than for luma coefficients annex u introduces the concept of multiple reference pictures with this feature motion compensated prediction is not restricted to use just the last decoded i p picture or for coded b pictures using annex o the last two i p pictures as a reference picture instead multiple decoded reference pictures are inserted into a picture buffer and can be used for inter prediction for each motion vector a reference picture index is transmitted which indicates the employed reference picture for the corresponding block the other annexes in h and h mainly provide additional functionalities such as the specification of features for improved error resilience the h profiles that provide the best coding efficiency are the conversational high compression chc profile and the high latency profile hlp the chc profile includes most of the optional features annexes d f i j t and u that provide enhanced coding efficiency for low delay applications the hlp adds the support of b pictures as defined in annex o to the coding efficiency tools of the chc profile and is targeted for applications that allow a higher coding delay c iso iec mpeg visual mpeg visual also known as part of the mpeg suite is backward compatible to h in the sense that each conforming mpeg decoder must be capable of decoding h baseline bitstreams i e bitstreams that use no h optional annex features similarly as for annex f in h the inter prediction in mpeg visual can be done with or blocks while the first version of mpeg visual only supports motion compensation with half sample precision motion vectors and bilinear interpolation similar to h mpeg video and h version added support for quarter sample precision motion vectors the luma prediction signal at half sample locations is generated using an tap interpolation filter for generating the quarter sample positions bilinear interpolation of the integer and half sample positions is used the chroma prediction signal is generated by bilinear interpolation motion vectors are differentially coded using a component wise median prediction and are allowed to point outside the reference picture mpeg visual supports b pictures in some profiles but it does not support the feature of multiple reference pictures except on a slice basis for loss resilience purposes and it does not specify a deblocking filter inside the motion compensation loop the transform coding in mpeg visual is basically similar to that of h mpeg video and h however two different quantization methods are supported the first quantization method which is sometimes referred to as mpeg style quantization supports quantization weighting matrices similarly to h mpeg video with the second quantization method which is called h style quantization the same quantization step size is used for all transform coefficients with the exception of the dc coefficient in intra blocks the transform coefficient levels are coded using a d runlevel last vlc code as in h similarly as in annex i of h mpeg visual also supports the prediction of ac coefficients in intra blocks as well as alternative scan patterns for horizontally and vertically predicted intra blocks and the usage of a separate vlc table for intra coefficients for the comparisons in this paper we used the advanced simple profile asp of mpeg visual which includes all relevant coding tools we generally enabled quarter sample precision motion vectors mpeg asp additionally includes global motion compensation due to the limited benefits experienced in practice and the complexity and general difficulty of estimating global motion fields suitable for improving the coding efficiency this feature is rarely supported in encoder implementations and is also not used in our comparison d itu t rec h iso iec mpeg avc h mpeg avc is the second video coding standard that was jointly developed by itu t vceg and iso iec mpeg it still uses the concept of mbs but contains many additional features one of the most obvious ieee transactions on circuits and systems for video technology vol no december differences from older standards is its increased flexibility for inter coding for the purpose of motion compensated prediction an mb can be partitioned into square and rectangular block shapes with sizes ranging from to luma samples h mpeg avc also supports multiple reference pictures similarly to annex u of h motion vectors are associated with a reference picture index for specifying the employed reference picture the motion vectors are transmitted using quarter sample precision relative to the luma sampling grid luma prediction values at half sample locations are generated using a tap interpolation filter and prediction values at quarter sample locations are obtained by averaging two values at integer and half sample positions weighted prediction can be applied using a scaling and offset for the prediction signal for the chroma components a bilinear interpolation is applied in general motion vectors are predicted by the component wise median of the motion vectors of three neighboring previously decoded blocks for and blocks the predictor is given by the motion vector of a single already decoded neighboring block where the chosen neighboring block depends on the location of the block inside an mb in contrast to prior coding standards the concept of b pictures is generalized and the picture coding type is decoupled from the coding order and the usage as a reference picture instead of i p and b pictures the standard actually specifies i p and b slices a picture can contain slices of different types and a picture can be used as a reference for inter prediction of subsequent pictures independently of its slice coding types this generalization allowed the usage of prediction structures such as hierarchical b pictures that show improved coding efficiency compared to the ibbp coding typically used for h mpeg video h mpeg avc also includes a modified design for intra coding while in previous standards some of the dct coefficients can be predicted from neighboring intra blocks the intra prediction in h mpeg avc is done in the spatial domain by referring to neighboring samples of previously decoded blocks the luma signal of an mb can be either predicted as a single block or it can be partitioned into or blocks with each block being predicted separately for and blocks nine prediction modes specifying different prediction directions are supported in the intra mode and for the chroma components four intra prediction modes are specified for transform coding h mpeg avc specifies a and an transform while chroma blocks are always coded using the transform the transform size for the luma component can be selected on an mb basis for intra mbs the transform size is coupled to the employed intra prediction block size an additional hadamard transform is applied to the four dc coefficients of each chroma component for the intra mode a similar second level hadamard transform is also applied to the dc coefficients of the luma signal in contrast to previous standards the inverse transforms are specified by exact integer operations so that in errorfree environments the reconstructed pictures in the encoder and decoder are always exactly the same the transform coefficients are represented using a uniform reconstruction quantizer that is without the extra wide dead zone that is found in older standards similar to h mpeg video and mpeg visual h mpeg avc also supports the usage of quantization weighting matrices the transform coefficient levels of a block are generally scanned in a zig zag fashion for entropy coding of all mb syntax elements h mpeg avc specifies two methods the first entropy coding method which is known as context adaptive variable length coding cavlc uses a single codeword set for all syntax elements except the transform coefficient levels the approach for coding the transform coefficients basically uses the concept of run level coding as in prior standards however the efficiency is improved by switching between vlc tables depending on the values of previously transmitted syntax elements the second entropy coding method specifies context adaptive binary arithmetic coding cabac by which the coding efficiency is improved relative to cavlc the statistics of previously coded symbols are used for estimating conditional probabilities for binary symbols which are transmitted using arithmetic coding inter symbol dependencies are exploited by switching between several estimated probability models based on previously decoded symbols in neighboring blocks similar to annex j of h h mpeg avc includes a deblocking filter inside the motion compensation loop the strength of the filtering is adaptively controlled by the values of several syntax elements the high profile hp of h mpeg avc includes all tools that contribute to the coding efficiency for bit persample video in format and is used for the comparison in this paper because of its limited benefit for typical video test sequences and the difficulty of optimizing its parameters the weighted prediction feature is not applied in the testing e hevc draft of october high efficiency video coding hevc is the name of the current joint standardization project of itu t vceg and iso iec mpeg currently under development in a collaboration known as the joint collaborative team on video coding jct vc it is planned to finalize the standard in early in the following a brief overview of the main changes relative to h mpeg avc is provided for a more detailed description the reader is referred to the overview in in hevc a picture is partitioned into coding tree blocks ctbs the size of the ctbs can be chosen by the encoder according to its architectural characteristics and the needs of its application environment which may impose limitations such as encoder decoder delay constraints and memory requirements a luma ctb covers a rectangular picture area of n n samples of the luma component and the corresponding chroma ctbs cover each n n samples of each of the two chroma components the value of n is signaled inside the bitstream and can be or 64 the luma ctb and the two chroma ctbs together with the associated syntax form a coding tree unit ctu the ctu is the basic processing unit of the standard to specify the decoding process conceptually corresponding to an mb in prior standards the blocks specified as luma and chroma ctbs can be further partitioned into multiple coding blocks cbs the ohm et al comparison of the coding efficiency of video coding standards ctu contains a quadtree syntax that allows for splitting into blocks of variable size considering the characteristics of the region that is covered by the ctb the size of the cb can range from the same size as the ctb to a minimum size luma samples or larger that is specified by a syntax element conveyed to the decoder the luma cb and the chroma cbs together with the associated syntax form a coding unit cu for each cu a prediction mode is signaled which can be either an intra or inter mode when intra prediction is chosen one of spatial intra prediction modes is signaled for the luma cb when the luma cb has the indicated smallest allowable size it is also possible to signal one intra prediction mode for each of its four square subblocks for both chroma cbs a single intra prediction mode is selected it specifies using the same prediction mode that was used for luma or using a horizontal vertical planar left downward diagonal or dc prediction mode the intra prediction mode is applied separately for each transform block tb for inter coded cus the luma and chroma cbs correspond to one two or four luma and chroma pbs the smallest luma pb size is or samples the luma and chroma pbs together with the associated syntax form a prediction unit pu each pu contains one or two motion vectors for unipredictive or bipredictive coding respectively all pbs of a cb can have the same size or when asymmetric motion partitioning amp is used a luma cb of size n n can also be split into two luma pbs where one of the luma pbs covers n n or n n samples and the other luma pb covers the remaining n n or n n area of the cb the amp splitting is also applied to chroma cbs accordingly similar to h mpeg avc hevc supports quartersample precision motion vectors the luma prediction signal for all fractional sample locations is generated by separable or tap filters depending on the subsample shift for chroma tap interpolation filters are applied hevc also supports multiple reference pictures and the concepts of i p and b slices are basically unchanged from h mpeg avc weighted prediction is also supported in a similar manner the coding of motion parameters has been substantially improved compared to prior standards hevc supports a socalled merge mode in which no motion parameters are coded instead a candidate list of motion parameters is derived for the corresponding pu in general the candidate list includes the motion parameters of spatially neighboring blocks as well as temporally predicted motion parameters that are derived based on the motion data of a co located block in a reference picture the chosen set of motion parameters is signaled by transmitting an index into the candidate list the usage of large block sizes for motion compensation and the merge mode allow a very efficient signaling of motion data for large consistently displaced picture areas if a pu is not coded using the merge mode the associated reference indices and motion vector prediction differences are transmitted prediction is done using the advanced motion vector prediction amvp algorithm in amvp for each motion vector a candidate list is constructed which can include the motion vectors of neighboring blocks with the same reference index as well as a temporally predicted motion vector the motion vector is coded by transmitting an index into the candidate list for specifying the chosen predictor and coding a difference vector for coding the inter or intra prediction residual signal of a luma cb the cb is either represented as a single luma tb or is split into four equal sized luma tbs if the luma cb is split each resulting luma tb can be further split into four smaller luma tbs the same splitting applies to the chroma cb except that chroma tbs are not further split and the scheme is called the residual quadtree rqt with the luma and chroma tbs and associated syntax forming a transform unit tu for each tu the luma and chroma tbs are each transformed using a d transform maximum and minimum tb sizes are selected by the encoder all tbs are square with block sizes of or similarly as in h mpeg avc the inverse transforms are specified by exact integer operations in general the transforms represent integer approximations of a dct for luma intra tbs of size an alternative transform representing an approximation of a discrete sine transform is used all slice data syntax elements are entropy coded using cabac which is similar to the cabac coding in h mpeg avc however the coding of transform coefficient levels has been improved by using a more sophisticated context selection scheme which is particularly efficient for larger transform sizes besides a deblocking filter the hevc design includes a sample adaptive offset sao operation inside the motion compensation loop sao classifies the reconstructed samples into different categories depending on sample amplitude or edge characteristics and reduces the distortion by adding a separate offset for each class of samples the hevc draft specifies a single profile the main profile mp it includes all coding tools as described above and supports the coding of bit per sample video in chroma format for some comparisons in this paper we used a modified configuration where some coding tools are disabled as for h mpeg avc the weighted prediction feature of hevc has not been used for the simulations in this paper two additional profiles the main profile for bitper sample video and the main still picture profile for still picture coding using only intra coding tools were included in the subsequent hevc draft but are not considered in our investigation iii encoder control since all video coding standards of itu t and iso iec jtc specify only the bitstream syntax and the decoding process they do not guarantee any particular coding efficiency in this paper all encoders are operated using the same encoding techniques where the main focus of the comparison is on investigating the coding efficiency that is achievable by the bitstream syntax encoding constraints such as real time operation or error robustness are not taken into account in order to keep the paper self contained we briefly review the lagrangian encoding technique used in this paper the task of an encoder control for a particular coding standard is to determine the values of the syntax elements and thus ieee transactions on circuits and systems for video technology vol no december the bitstream b for a given input sequence in a way that the distortion d between the input sequence and its reconstruction s b is minimized subject to a set of constraints which usually includes constraints for the average and maximum bit rate and the maximum coding delay let bc be the set of all conforming bitstreams that obey the given set of constraints for any particular distortion measure d s s the optimal bitstream in the rate distortion sense is given by b arg min d s s b b bc due to the huge parameter space and encoding delay it is impossible to directly apply the minimization in instead the overall minimization problem is split into a series of smaller minimization problems by partly neglecting spatial and temporal interdependencies between coding decisions let sk be a set of source samples such as a video picture or a block of a video picture and let p pk be a vector of coding decisions or syntax element values out of a set pk of coding options for the set of source samples sk the problem of finding the coding decisions p that minimize a distortion measure dk p d sk sk between the original samples sk and their reconstructions sk sk p subject to a rate constraint rc can be formulated as min dk p p pk subject to rk p rc where rk p represents the number of bits that are required for signaling the coding decisions p in the bitstream other constraints such as the maximum coding delay or the minimum interval between random access points can be considered by selecting appropriate prediction structures and coding options the constrained minimization problem in can be reformulated as an unconstrained minimization min dk p  rk p p pk where  denotes the so called lagrange multiplier if a set of source samples sk can be partitioned into a number of subsets sk i in a way that the associated coding decisions pi are independent of each other and an additive distortion measure dk i pi is used the minimization problem in can be written as min dk i pi  rk i pi i pi pk i the optimal solution of can be obtained by independently selecting the coding options pi for the subsets sk i although most coding decisions in a video encoder cannot be considered independent for a practical applicability of the lagrangian encoder control it is required to split the overall optimization problem into a set of feasible decisions while the past decisions are taken into account by determining the distortion and rate terms based on already coded samples the impact of a decision on future samples and coding decisions is ignored the concept of the described lagrangian encoder control is applied for mode decision motion estimation and quantiza tion the used distortion measures d are defined as si si  i b with  for the sum of absolute differences sad and  for the sum of squared differences ssd si and si represent the original and reconstructed samples respectively of a considered block b except for motion estimation we use ssd as the distortion measure for all coding decisions hence all encoders are basically optimized with respect to the mean squared error mse or psnr the subjective quality of the reconstructed video which is the ultimate video quality measure is not directly taken into account during encoding nonetheless this encoder control method usually also provides a good tradeoff between subjective quality and bit rate a mode decision the minimization of a lagrangian cost function for mode decision was proposed in the investigated video coding standards provide different coding modes c for coding a block of samples sk such as an mb or a cu the coding modes may represent intra or inter prediction modes or partitions for motion compensated prediction or transform coding given the set ck of applicable coding modes for a block of samples sk the selected coding mode is chosen according to c arg min dk c  rk c c ck where the distortion term dk c represents the ssd between the original block sk and its reconstruction sk that is obtained by coding the block sk with the mode c the term rk c represents the number of bits or an estimate thereof that are required for representing the block sk using the coding mode c for the given bitstream syntax it includes the bits required for signaling the coding mode and the associated side information e g motion vectors reference indices intra prediction modes and coding modes for subblocks of sk as well as the bits required for transmitting the transform coefficient levels representing the residual signal a coding mode is often associated with additional parameters such as coding modes for subblocks motion parameters and transform coefficient levels while coding modes for subblocks are determined in advance according to motion parameters and transform coefficient levels are chosen as described in sections iii b and iii c respectively for calculating the distortion and rate terms for the different coding modes decisions for already coded blocks of samples are taken into account e g by considering the correct predictors or context models for the investigated encoders the described mode decision process is used for the following the decision on whether an mb or a cu is coded using intra or inter prediction the determination of intra prediction modes the selection of a subdivision for a block or cu into subblocks for inter prediction the selection of the transform size or transform subdivision for an mb or cu ohm et al comparison of the coding efficiency of video coding standards the subdivision of a cu into smaller cus for hevc a similar process is used for determining the sao parameters in hevc b motion estimation the minimization of a lagrangian cost function for motion estimation was proposed in given a reference picture list r and a candidate set m of motion vectors the motion parameters for a block sk which consist of a displacement or motion vector m mx my and if applicable a reference index r are determined according to r m arg min dk r m m rk r m r r m m the rate term rk r m represents an estimate of the number of bits that is required for transmitting the motion parameters for determining the rate term the motion vector predictor for the current block or for hevc one of the possible predictors is taken into account for each candidate reference index r the motion search first proceeds over a defined set of integer sample precision displacement vectors for this stage the distortion dk r m is measured as the sad between the block sk and the displaced reference block in the reference pictures indicated by the reference index r for the integer sample precision search all encoders use the same fast motion estimation strategy the one implemented in the hm reference software given the selected integer sample precision displacement vector the eight surrounding half sample precision displacement vectors are evaluated then for the coding standards supporting quarter sample precision motion vectors the half sample refinement is followed by a quarter sample refinement in which the eight quarter sample precision vectors that surround the selected half sample precision motion vector are tested the distortion measure that is used for the subsample refinements is the sad in the hadamard domain the difference between the original block sk and its motion compensated prediction signal given by r and m is transformed using a block wise or hadamard transform and the distortion is obtained by summing up the absolute transform coefficients as has been experimentally found the usage of the sad in the hadamard domain usually improves the coding efficiency in comparison to using the sad in the sample domain 25 due to its computationally demanding calculation the hadamarddomain measurement is only used for the subsample refinement in hevc the motion vector predictor for a block is not fixed but can be chosen out of a set of candidate predictors the used predictor is determined by minimizing the number of bits required for coding the motion vector m finally given the selected motion vector for each reference index r the used reference index is selected according to where the sad in the hadamard domain is used as the distortion measure for bipredictively coded blocks two motion vectors and reference indices need to be determined the initial motion parameters for each reference list are determined independently by minimizing the cost measure in this is followed by an iterative refinement step in which one motion vector is held constant and for the other motion vector a refinement search is carried out for this iterative refinement the distortions are calculated based on the prediction signal that is obtained by biprediction the decision whether a block is coded using one or two motion vectors is also based on a lagrangian function similar to where the sad in the hadamard domain is used as distortion measure and the rate term includes all bits required for coding the motion parameters due to the different distortion measure the lagrange multiplier m that is used for determining the motion parameters is different from the lagrange multiplier  used in mode decision in and the simple relationship m  between those parameters is suggested which is also used for the investigations in this paper c quantization in classical scalar quantization fixed thresholds are used for determining the quantization index of an input quantity but since the syntax for transmitting the transform coefficient levels in image and video coding uses interdependencies between the transform coefficient levels of a block the rate distortion efficiency can be improved by taking into account the number of bits required for transmitting the transform coefficient levels an approach for determining transform coefficient levels based on a minimization of a lagrangian function has been proposed in for h mpeg video in and similar concepts for a rate distortion optimized quantization are described for h mpeg avc the general idea is to select the vector of transform coefficient levels l for a tb t according to l arg min d l  r l l ln where ln represents the vector space of the n transform coefficient levels and d l and r l denote the distortion and the number of bits associated with the selection l for the considered tb as distortion measure we use ssd since the transforms specified in the investigated standards have orthogonal basis functions if neglecting rounding effects the ssd can be directly calculated in the transform domain d l i d li it is of course infeasible to perform the minimization over the entire product space ln however it is possible to apply a suitable decision process by which none or only some minor interdependencies are neglected the actual quantization process is highly dependent on the bitstream syntax as an example we briefly describe the quantization for hevc in the following in hevc a tb is represented by a flag indicating whether the block contains nonzero transform coefficient levels the location of the last nonzero level in scanning order a flag for subblocks indicating whether the subblock contains nonzero levels and syntax elements for representing the actual levels the quantization process basically consists of the following ordered steps for each scanning position i the selected level li is determined assuming that the scanning position lies in a nonzero subblock and i is less than or equal to ieee transactions on circuits and systems for video technology vol no december the last scanning position this decision is based on minimization of the function d li  ri li where d li represents the normalized squared error for the considered transform coefficient and ri li denotes the number of bits that would be required for transmitting the level li for reducing complexity the set of tested levels can be limited e g to the two levels that would be obtained by a mathematically correct rounding and a rounding toward zero of the original transform coefficient divided by the quantization step size for each subblock the rate distortion cost for the determined levels is compared with the rate distortion cost that is obtained when all levels of the subblock are set equal to zero if the latter cost is smaller all levels of the subblock are set equal to zero finally the flag indicating whether the block contains nonzero levels and the position of the last nonzero level are determined by calculating the rate distortion cost that is obtained when all levels of the tb are set equal to zero and the rate distortion costs that are obtained when all levels that precede a particular nonzero level are set equal to zero the setting that yields the minimum ratedistortion costs determines the chosen set of transform coefficient levels d quantization parameters and lagrange multipliers for all results presented in this paper the quantization parameter qp and the lagrange multiplier  are held constant for all mbs or cus of a video picture the lagrange multiplier is set according to   where q denotes the quantization step size which is controlled by the quantization parameter qp cp given the quantization parameter qpi for i pictures the quantization parameters for all other pictures and the factors  are set using a deterministic approach the actual chosen values depend on the used prediction structure and have been found in an experimental way iv performance measurement of the hevc reference codec implementation a description of criteria the bjntegaard measurement method for calculating objective differences between rate distortion curves was used as evaluation criterion in this section the average differences in bit rate between two curves measured in percent are reported here in the original measurement method separate rate distortion curves for the luma and chroma components were used hence resulting in three different average bit rate differences one for each of the components separating these measurements is not ideal and is sometimes confusing as tradeoffs between the performance of the luma and chroma components are not taken into account in the used method the rate distortion curves of the combined luma and chroma components are used the combined psnr psnryuv is first calculated as the weighted sum of the psnr per picture of the individual components psnry psnru and psnrv psnryuv psnry psnru psnrv where psnry psnru and psnrv are each computed as psnr log10 mse where b is the number of bits per sample of the video signal to be coded and the mse is the ssd divided by the number of samples in the signal the psnr measurements per video sequence are computed by averaging the per picture measurements using the bit rate and the combined psnryuv as the input to the bjntegaard measurement method gives a single average difference in bit rate that at least partially takes into account the tradeoffs between luma and chroma component fidelity b results about the benet of some representative tools in general it is difficult to fairly assess the benefit of a video compression algorithm on a tool by tool basis as the adequate design is reflected by an appropriate combination of tools for example introduction of larger block structures has impact on motion vector compression particularly in the case of homogeneous motion but should be accompanied by incorporation of larger transform structures as well therefore the subsequent paragraphs are intended to give some idea about the benefits of some representative elements when switched on in the hevc design compared to a configuration which would be more similar to h mpeg avc in the hevc specification there are several syntax elements that allow various tools to be configured or enabled among these are parameters that specify the minimum and maximum cb size tb size and transform hierarchy depth there are also flags to turn tools such as temporal motion vector prediction tmvp amp sao and transform skip ts on or off by setting these parameters the contribution of these tools to the coding performance improvements of hevc can be gauged for the following experiments the test sequences from classes a to e specified in the appendix and the coding conditions as defined in were used hevc test model software hm 8 was used for these specific experiments two coding structures were investigated one suitable for entertainment applications with random access support and one for interactive applications with low delay constraints the following tables show the effects of constraining or turning off tools defined in the hevc mp in doing so there will be an increase in bit rate which is an indication of the benefit that the tool brings the reported percentage difference in the encoding and decoding time is an indication of the amount of processing that is needed by the tool note that this is not suggested to be a reliable measure of the complexity of the tool in an optimized hardware or software based encoder or decoder but may provide some rough indication table i compares the effects of setting the maximum coding block size for luma to or samples versus the ohm et al comparison of the coding efficiency of video coding standards table i table iii difference in bit rate for equal psnr relative to hevc mp when smaller maximum coding block sizes were used instead of 64 64 coding blocks difference in bit rate for equal psnr relative to hevc mp when smaller maximum rqt depths were used instead of a depth of class a class b class c class d class e overall enc time dec time entertainment applications maximum cu size 16 16 8 8 8 11 160 interactive applications maximum cu size 32 16 16 83 58 table ii difference in bit rate for equal psnr relative to hevc mp when smaller maximum transform block sizes are used instead of 32 32 transform blocks class a class b class c class d class e overall enc time dec time entertainment applications maximum transform size 16 16 8 8 87 interactive applications maximum transform size 16 16 8 8 7 7 8 7 90 64 64 maximum size allowed in the hevc mp these results show that although the encoder spends less time searching and deciding on the cb sizes there is a significant penalty in coding efficiency when the maximum block size is limited to 32 32 or 16 16 samples it can also be seen that the benefit of larger block sizes is more significant for the higher resolution sequences as well as for sequences with sparse content such as the class e sequences an interesting effect on the decoder side is that when larger block sizes are used the decoding time is reduced as smaller block sizes require more decoding time in the hm implementation table ii compares the effects of setting the maximum tb size to 8 8 and 16 16 versus the 32 32 maximum size allowed in hevc mp the results show the same trend as constraining the maximum coding block sizes however the percentage bit rate penalty is smaller since constraining the maximum coding block size also indirectly constrains the maximum transform size while the converse is not true the amount of the reduced penalty shows that there are some benefits from using larger cus that are not simply due to the larger transforms it is however noted that constraining the transform size has a more significant effect on the chroma components than the luma component hevc allows the tb size in a cu to be selected independently of the prediction block size with few exceptions class a class b class c class d class e overall enc time dec time entertainment applications max rqt depth 8 3 3 89 81 interactive applications max rqt depth 5 5 5 3 8 3 this is controlled through the rqt which has a selectable depth table iii compares the effects of setting the maximum transform hierarchy depth to and instead of 3 the value used in the common test conditions 32 it shows that some savings in the encoding decision time can be made for a modest penalty in coding efficiency for all classes of test sequences however there is no significant impact on the decoding time table iv shows the effects of turning off tmvp sao amp and ts in the hevc mp the resulting bit rate increase is measured by averaging over all classes of sequences tested bit rate increases of 5 and were measured when disabling tmvp and sao respectively for the entertainment application scenario for the interactive application scenario the disabling of tmvp or sao tool yielded a bit rate increase of 5 it should be noted that sao has a larger impact on the subjective quality than on the psnr neither of these tools has a significant impact on encoding or decoding time when the amp tool is disabled bit rate increases of 9 and were measured for the entertainment and interactive applications scenario respectively the significant increase in encoding time can be attributed to the additional motion search and decision that is needed for amp disabling the ts tool does not change the coding efficiency it should however be noted that the ts tool is most effective for content such as computer screen capture and overlays for such content disabling of the ts tool shows bit rate increases of 7 3 and 6 3 for the entertainment and interactive application scenarios respectively results for other tools of hevc that yield improvements relative to h mpeg avc including merge mode intra prediction and motion interpolation filter are not provided here for more information see c results in comparison to previous standards for comparing the coding efficiency of hevc with that of prior video coding standards we performed coding experiments for the two different scenarios of entertainment and interactive applications the encoding strategy described in section iii has been used for all investigated standards for hevc the described encoder control is the same as the one implemented in the hm 8 reference software ieee transactions on circuits and systems for video technology vol no december table iv table v difference in bit rate for equal psnr relative to hevc mp when the tmvp sao amp and ts tools are turned off average bit rate savings for equal psnr for interactive applications entertainment applications interactive applications tools disabled in mp tools disabled in mp tmvp sao amp ts tmvp sao amp ts class a 6 6 class b 7 5 6 class c 7 8 9 class d 7 5 9 3 0 0 class e 3 3 7 0 1 overall 5 1 6 0 9 0 0 5 5 1 0 0 enc time 87 101 101 dec time 98 98 99 bit rate savings relative to h mpeg h mpeg mpeg avc hp chc asp h mp hevc mp 3 9 3 80 1 h mpeg avc hp 8 54 1 0 h chc mpeg asp 27 8 so this software has been used unmodified for the other standards we integrated the described encoder control into older encoder implementations the following codecs have been used as basis the mpeg software simulation group software version 1 for h mpeg video the h codec of the university of british columbia signal processing and multimedia group see a fraunhofer hhi implementation of mpeg visual and the jsvm version 9 1 for h mpeg avc all encoders use the same strategies for mode decision motion estimation and quantization these encoders show significantly improved coding efficiency relative to publicly available reference implementations or the encoder versions that were used in for hevc all coding tools specified in the draft hevc mp are enabled for the other tested video coding standards we selected the profiles and coding tools that provide the best coding efficiency for the investigated scenarios the chosen profiles are the h mpeg 2 mp the h chc profile for the interactive scenario and the h hlp for the entertainment scenario the mpeg asp and the h mpeg avc hp each test sequence was coded at different bit rates for h mpeg avc and hevc the quantization parameter qpi for i pictures was varied in the range from to inclusive for h mpeg 2 video h and mpeg visual the quantization parameters for i pictures were chosen in a way that the resulting quantization step sizes are approximately the same as for h mpeg avc and hevc the quantization parameters for non i pictures are set relative to qpi using a deterministic approach that is basically the same for all tested video coding standards in order to calculate bit rate savings for one codec relative to another the rate distortion curves were interpolated in the logarithmic domain using cubic splines with the not a knot condition at the border points average bit rate savings are calculated by numerical integration with equal sized subintervals 1 interactive applications the first experiment addresses interactive video applications such as video conferencing we 1 the jm encoder or the modified jm 2 which was used for the comparison in section v provides very similar coding efficiency to our modified jsvm version but differs in some details from the hm encoder control encoding selected six test sequences with typical video conferencing content which are the sequences of classes e and e listed in the appendix since interactive applications require a low coding delay all pictures were coded in display order where only the first picture is coded as an i picture and all subsequent pictures are temporally predicted only from reference pictures in the past in display order for h mpeg 2 mp and mpeg 4 asp the temporally predicted pictures were coded as p pictures ippp coding structure and the quantization step size for the p pictures was increased by about relative to that for i pictures the syntax of h h mpeg 4 avc and hevc supports low delay coding structures that usually provide an improved coding efficiency here we used dyadic low delay hierarchical prediction structures with groups of four pictures see while for h chc and h mpeg 4 avc hp all pictures are coded with p slices for hevc mp all pictures are coded with b slices for h mpeg 4 avc hp and hevc mp which both support low delay coding with p or b slices we selected the slice coding type that provided the best coding efficiency p slices for h mpeg 4 avc hp and b slices for hevc mp the quantization step size for the p or b pictures of the lowest hierarchy level is increased by about relative to that for i pictures and it is further increased by about from one hierarchy level to the next for h chc h mpeg 4 avc hp and hevc mp the same four previously coded pictures are used as active reference pictures except for h mpeg 2 mp which does not support slices that cover more than one mb row all pictures are coded as a single slice for h mpeg 2 mp one slice per mb row is used inverse transform mismatches for h mpeg 2 mp h chc and mpeg 4 asp are avoided since the used decoders implement exactly the same transform as the corresponding encoder in practice where this cannot be guaranteed the psnr values and subjective quality for these standards would be reduced and intra mbs would need to be inserted periodically in order to limit the mismatch accumulation in fig 1 rate distortion curves are depicted for two selected sequences in which the psnryuv as defined in section iv a is plotted as a function of the average bit rate this figure additionally shows plots that illustrate the bit rate savings of hevc mp relative to h mpeg 2 mp h chc mpeg 4 asp and h mpeg 4 avc hp as a function of the psnryuv in the diagrams the psnryuv is denoted as yuv psnr the average bit rate savings between the differ ohm et al comparison of the coding efficiency of video coding standards fig 1 1679 selected rate distortion curves and bit rate savings plots for interactive applications ent codecs which are computed over the entire test set and the investigated quality range are summarized in table v these results indicate that the emerging hevc standard clearly outperforms its predecessors in terms of coding efficiency for interactive applications the bit rate savings for the low bitrate range are generally somewhat higher than the average savings given in table v which becomes evident from the plots in the right column of fig 1 d entertainment applications besides interactive applications one of the most promising application areas for hevc is the coding of high resolution video with entertainment quality for analyzing the potential of hevc in this application area we have selected a set of five full hd and four wvga test sequences which are listed as class b and c sequences in the appendix in contrast to our first experiment the delay constraints are relaxed for this application scenario for h mpeg 4 avc hp and hevc mp we used dyadic high delay hierarchical prediction structures see with groups of eight pictures where all pictures are coded as b pictures except at random access refresh points where i pictures are used this prediction structure is characterized by a structural delay of eight pictures and has been shown to provide an improved coding efficiency compared to ibbp coding similarly as for the first experiment the quantization step size is increased by about qp increase by 1 from one hierarchy level to the next and the quantization step size for the b pictures of the lowest hierarchy level is increased by relative to that of the i pictures the same four active reference pictures are used for h mpeg 4 avc hp and hevc mp h mpeg 2 mp h hlp and mpeg 4 asp do not support hierarchical prediction structures here we used a coding structure where three b pictures are inserted between each two successive p pictures the usage of three b pictures ensures that the i pictures are inserted at the same locations as for the h mpeg 4 avc hp and hevc mp configurations and it slightly improves the coding efficiency in comparison to the typical coding structure with two b pictures the quantization step sizes were increased by about from i to p pictures and from p to b pictures for h hlp four active reference pictures are used for both the p and b pictures for all tested codecs i pictures are inserted in regular time intervals of about 1 second at exactly the same time instances such frequent periodic intra refreshes are typical in entertainment quality applications in order to enable fast random access e g for channel switching in order to enable clean random access pictures that follow an i picture in both coding and display order are not allowed to reference any picture that precedes the i picture in either coding or display order however pictures that follow the i picture in coding order but precede it in display order are generally allowed to use pictures that precede the i picture in coding order as reference pictures for motion compensated prediction this structure is sometimes referred to as open gop where a gop is a group of pictures that begins with an i picture the diagrams in fig 2 show rate distortion curves and bit rate saving plots for two typical examples of the tested ieee transactions on circuits and systems for video technology vol no december fig 2 selected rate distortion curves and bit rate savings plots for entertainment applications table vi average bit rate savings for equal psnr for entertainment applications bit rate savings relative to h mpeg 4 mpeg 4 h mpeg 2 avc hp asp hlp h mp hevc mp 35 4 7 1 70 8 h mpeg 4 avc hp 5 6 4 mpeg 4 asp 3 9 7 h hlp 16 2 encoding sequences the bit rate savings results averaged over the entire set of test sequences and the examined quality range are summarized in table vi as for the previous case hevc provides significant gains in term of coding efficiency relative to the older video coding standards as can be seen in the plots in fig 2 the coding efficiency gains for the lower bitrate range are again generally higher than the average results reported in table vi v preliminary investigation of the hevc reference implementation compared to h mpeg 4 avc using subjective quality a laboratory and test setup the laboratory for the subjective assessment was set up following itu r rec bt except for the section on the displays and video server a 50 inch panasonic professional plasma display th was used in its fig 3 dsis basic test cell native resolution of pixels the video display board was a panasonic dual link hd sdi input module the uncompressed video recorder player was a udr by keisoku giken co ltd controlled using a dell precision double stimulus impairment scale dsis as defined in the hevc call for proposals 38 was used for the evaluation of the quality rather than of the impairment hence a quality rating scale made of 11 levels was adopted ranging from 0 lowest quality to 10 highest quality the structure of the basic test cell of the dsis method consists of two consecutive presentations of the sequence under test first the original version of the video sequence is displayed followed immediately by the decoded sequence then a message is shown for 5 seconds asking the viewers to vote see fig 3 the presentation of the video clips is preceded by a mid level gray screen for a duration of one second each test session comprised tests on a single test sequence and lasted approximately 8 minutes a total of nine test sequences listed as class b and c in the appendix were used in the subjective assessment the total number of test subjects ohm et al comparison of the coding efficiency of video coding standards fig 4 mean opinion score mos for test sequences plotted against bit rate fig 5 bit rate savings as a function of subjective quality was the test subjects were divided into groups of four in each test session seated in a row a viewing distance of was used in all tests where h is the height of the video on the plasma display table vii average bit rate savings for entertainment application scenario based on subjective assessment results sequences b codecs tested and coding conditions in the subjective assessment the test sequences for h mpeg 4 avc hp were encoded using the jm 18 2 codec with the encoder modifications as described in the test sequences for the hevc mp were encoded using the hm 5 0 software it should be noted that the hevc mp configuration by the time of hm 5 0 was slightly worse in performance than hm 8 0 and also did not include amp the same random access coding structure was used in all test sequences quantization parameter qp values of 37 and were selected for the hevc mp for h mpeg 4 avc hp qp values of 27 30 and were chosen it was confirmed in a visual prescreening that these settings resulted in decoded sequences of roughly comparable subjective quality and the bit rate reductions for the hevc mp encodings ranged from to on average relative to the corresponding h mpeg 4 avc hp bit rates bq terrace basketball drive park scene cactus bq mall basketball drill party scene race horses average bit rate savings of hevc mp relative to h mpeg 4 avc hp 1 6 2 7 50 2 6 9 29 8 7 3 was also calculated and represented as vertical error bars on the graphs as can be seen from the example corresponding points have largely overlapping confidence intervals indicating that the quality of the sequences would be measured within these intervals again with probability this confirms that the test sequences encoded with hevc at an average of lower bit rate than the h mpeg 4 avc hp encodings achieved approximately the same subjective quality c results fig 4 shows the result of the formal subjective assessment the mos values were computed from the votes provided by the subjects for each test point the confidence interval d further processing of the results the subjective test results were further analyzed to obtain a finer and more precise measure of the coding performance ieee transactions on circuits and systems for video technology vol no december 2012 table viii test sequences used in the comparisons guideline how complete should postconditions be agile vs heavy analysis contracts may not be useful this question is discussed in a subsequent section but assuming some are useful generating a complete and detailed set of postconditions for all system operations is not likelyor necessary in the spirit of agile modeling treat their creation as an initial best guess with the understanding they will not be complete and that perfect complete specifications are rarely possible or believable but understanding that light analysis is realistic and skillful doesn t mean to abandon a little investigation before programmingthat the other extreme of misunderstanding example enteritem postconditions the following section dissects the motivation for the postconditions of the enteritem system operation instance creation and deletion after the itemid and quantity of an item have been entered what new object should have been created a saleslineitem thus a saleslineitem instance sli was created instance creation note the naming of the instance this name will simplify references to the new instance in other post condition statements attribute modification after the itemid and quantity of an item have been entered by the cashier what attributes of new or existing objects should have been modified the quantity of the saleslineitem should have become equal to the quantity parameter thus sli quantity became quantity attribute modification associations formed and broken after the itemid and quantity of an item have been entered by the cashier what associations between new or existing objects should have been formed or broken the new saleslineitem should have been related to its sale and related to its productdescription thus sli was associated with the current sale association formed sli was associated with a productdescription based on itemid match association formed note the informal indication that it forms a relationship with a productdescriptionthe one whose itemid matches the parameter more fancy and formal language approaches are possible such as using the object constraint language ocl recommendation keep it plain and simple guideline should we update the domain model it common during the creation of the contracts to discover the need to record new conceptual classes attributes or associations in the domain model do not be limited to the prior definition of the domain model enhance it as you make new discoveries while thinking through operation contracts guideline when are contracts useful in the up the use cases are the main repository of requirements for the project they may provide most or all of the detail necessary to know what to do in the design in which case contracts are not helpful however there are situations where the details and complexity of required state changes are awkward or too detailed to capture in use cases for example consider an airline reservation system and the system operation addnewreservation the complexity is very high regarding all the domain objects that must be changed created and associated these fine grained details can be written up in the use case but it will make it extremely detailed for example noting each attribute in all the objects that must change observe that the postcondition format offers and encourages a very precise analytical language that supports detailed thoroughness if developers can comfortably understand what to do without them then avoid writing contracts this case study shows more contracts than are necessaryfor education in practice most of the details they record are obviously inferable from the use case text on the other hand obvious is a slippery concept guideline how to create and write contracts apply the following advice to create contracts identify system operations from the ssds for system operations that are complex and perhaps subtle in their results or which are not clear in the use case construct a contract to describe the postconditions use the following categories instance creation and deletion attribute modification associations formed and broken writing contracts as mentioned write the postconditions in a declarative passive past tense form was to emphasize the observation of a change rather than a design of how it is going to be achieved for example better a saleslineitem was created worse create a saleslineitem remember to establish an association between existing objects or those newly created for example it is not enough that a new saleslineitem instance is created when the enteritem operation occurs after the operation is complete it should also be true that the newly created instance was associated with sale thus the saleslineitem was associated with the sale association formed what the most common mistake the most common problem is forgetting to include the forming of associations particularly when new instances are created it is very likely that associations to several objects need be established don t forget example nextgen pos contracts system operations of the process sale use case contract makenewsale operation makenewsale cross references use cases process sale preconditions none postconditions a sale instance was created instance creation was associated with a register association formed attributes of were initialized note the vague description in the last postcondition if understandable it fine on a project all these particular postconditions are so obvious from the use case that the makenewsale contract should probably not be written recall one of the guiding principles of healthy process and the up keep it as light as possible and avoid all artifacts unless they really add value contract enteritem operation enteritem itemid itemid quantity integer cross references use cases process sale preconditions there is a sale underway postconditions a saleslineitem instance sli was created instance creation sli was associated with the current sale association formed sli quantity became quantity attribute modification sli was associated with a productdescription based on itemid match association formed contract endsale operation endsale cross references use cases process sale preconditions there is a sale underway postconditions sale iscomplete became true attribute modification contract makepayment operation makepayment amount money cross references use cases process sale preconditions there is a sale underway postconditions a payment instance p was created instance creation p amounttendered became amount attribute modification p was associated with the current sale association formed the current sale was associated with the store association formed to add it to the historical log of completed sales changes to the pos domain model there is at least one point suggested by these contracts that is not yet represented in the domain model completion of item entry to the sale the endsale specification modifies it and it is probably a good idea later during design work for the makepayment operation to test it to disallow payments until a sale is complete meaning no more items to add one way to represent this information is with an iscomplete attribute in the sale there are alternatives especially considered during design work one technique is called the state pattern another is the use of session objects that track the state of a session and disallow out of order operations this will be explored later example monopoly contracts i ll use this case study to emphasize that many analysis artifacts aren t always needed including contracts the up encourages avoiding creating an artifact unless it addresses a risk or solves a real problem people who know the rules of the game from experience as a child or teenager most people it seems can implement it without looking at many written details applying uml operations contracts and the ocl what the relationship between contracts in this chapter and the uml the uml formally defines operations to quote an operation is a specification of a transformation or query that an object may be called to execute for example the elements of an interface are operations in uml terms an operation is an abstraction not an implementation by contrast a method in the uml is an implementation of an operation to quote a method is the implementation of an operation it specifies the algorithm or procedure associated with an operation in the uml metamodel an operation has a signature name and parameters and most importantly in this context is associated with a set of uml constraint objects classified as preconditions and postconditions that specify the semantics of the operation to summarize the uml defines operation semantics via constraints which are specifiable in the pre and post condition style note that as emphasized in this chapter a uml operation specification can not show an algorithm or solution but only the state changes or effects of the operation in addition to using contracts to specify public operations of the entire system that is system operations contracts can be applied to operations at any level of granularity the public operations or interface of a subsystem a component an abstract class and so forth for example operations can be defined for a single software class such as a stack the coarse grained operations discussed in this chapter belong to a system class representing the overall system as a black box component but in the uml operations can belong to any class or interface all with pre and post conditions operation contracts expressed with the ocl the pre and post condition format in this chapter is informal natural languageperfectly acceptable in the uml and desirable to be easily understood but also associated with the uml is a formal rigorous language called the object constraint language ocl which can be used to express constraints of uml operations the ocl defines an official format for specifying pre and postconditions for operations as demonstrated in this fragment system makenewsale pre statements in ocl post further ocl details are beyond the scope of this introduction process operation contracts within the up a pre and postcondition contract is a well known style to specify an operation in the uml in the uml operations exists at many levels from system down to fine grained classes such as sale operation contracts for the system level are part of the use case model although they were not formally highlighted in the original rup or up documentation their inclusion in this model was verified with the rup authors private communication phases inception contracts are not motivated during inceptionthey are too detailed elaboration if used at all most contracts will be written during elaboration when most use cases are written only write contracts for the most complex and subtle system operations history operation contracts come out of the formal specifications area in computer science originally from the prolific tony hoare hoare was working in industry in the mid to develop an algol compiler and read bertrand russell introduction to mathematical philosophy which introduced him to the idea of axiomatic theory and assertions he realized that computer programs could be expressed with assertions pre and post conditions relative to the results that were expected at the launch and termination of a program in he joined academia and his idea spread along with other researchers theories of formal specifications in at the ibm lab in vienna a pl compiler was being developed and the researchers desired an unambiguous formal specification of the language out of this need vdlthe vienna definition languagewas born by peter lucas vdl borrowed the pre and post condition assertion form earlier explored by hoare and russel vdl eventually evolved into the language used within the vienna definition method vdm a method that applied operation contract formal specifications and rigorous proof theory in the bertrand meyernot surprisingly yet another compiler writer the oo language eiffel started to promote the use of pre and post condition assertions as first class elements within his eiffel language to be applied to ooa d he contributed to a much wider awareness of formal specifications and operation contracts in his popular book object oriented software construction in which he also proposed the approach as a method called design by contract dbc in dbc contracts are written for operations of fine grained software class operations not specifically the public operations of the overall system in addition dbc promotes an invariant section common in thorough contract specifications invariants define things that must not change state before and after the operation has executed invariants have not been used in this chapter for the sake of simplicity in the early grady booch briefly discussed applying contracts to object operations in his booch method also derek coleman and colleagues at hp labs borrowed the operation contract idea and applied it to ooa and domain modeling making it part of the influential ooa d fusion method coleman programming language support for contracts some languages such as eiffel have first class support for invariants and pre and post conditions using attributes javadoc tags or pre compilers similar facilities can be provided in java and c for example recommended resources many examples of ooa oriented system operation contracts can be found in object oriented development the fusion method by coleman et al object oriented software construction by meyer shows many program level contract examples in eiffel within the uml operation contracts can also be specified more rigorously in the object constraint language ocl for which warmer and kleppe the object constraint language precise modeling with uml is recommended chapter requirements to designiteratively hardware n the parts of a computer system that can be kicked anonymous introduction so far the case studies have emphasized analysis of the requirements and objects if following the up guidelines perhaps of the requirements were investigated in inception and a slightly deeper investigation was started in this first iteration of elaboration the following chapters are a shift in emphasis toward designing a solution for this iteration in terms of collaborating software objects view full size image iteratively do the right thing do the thing right the requirements and object oriented analysis has focused on learning to do the right thing that is understanding some of the outstanding goals for the case studies and related rules and constraints by contrast the following design work will stress do the thing right that is skillfully designing a solution to satisfy the requirements for this iteration in iterative development a transition from primarily a requirements or analysis focus to primarily a design and implementation focus will occur in each iteration early iterations will spend relatively more time on analysis activities as the vision and specifications start to stabilize based on early programming test and feedback in later iterations it is common that analysis lessens there more focus on just building the solution provoking early change it is natural and healthy to discover and change some requirements during the design and implementation work especially in the early iterations iterative and evolutionary methods embrace change although we try to provoke that inevitable change in early iterations so that we have a more stable goal and estimate and schedule for the later iterations early programming tests and demos help provoke the inevitable changes early on take note this simple idea lies at the heart of why iterative development works the discovery of changing specifications will both clarify the purpose of the design work of this iteration and refine the requirements understanding for future iterations over the course of these early elaboration iterations the requirements discovery should stabilize so that by the end of elaboration perhaps of the requirements are reliably defineddefined and refined as a result of feedback early programming and testing rather than speculation as occurs in a waterfall method didn t all that analysis and modeling take weeks to do after many chapters of detailed discussion it must surely seem like the prior modeling would take weeks of effort not so when one is comfortable with the skills of use case writing domain modeling and so forth the duration to do all the actual modeling that has been explored so far is realistically just a few hours or days however that does not mean that only a few days have passed since the start of the project many other activities such as proof of concept programming finding resources people software planning setting up the environment and so on could consume a few weeks of preparation chapter logical architecture and uml package diagrams hamlet introduction first to set the expectation level this is a very short introduction to the topic of logical architecture a fairly large topic learn more starting on p now that we have transitioned from analysis oriented work to software design let start with the large scale at this level the design of a typical oo system is based on several architectural layers such as a ui layer an application logic or domain layer and so forth this chapter briefly explores a logical layered architecture and related uml notation view full size image up artifact influence emphasizing the logical architecture la is shown in figure uml package diagrams may illustrate the la as part of the design modeland also be summarized as a view in the software architecture document the prime input is the architectural forces captured in the supplementary specification the la defines the packages within which software classes are defined figure sample up artifact influence view full size image bus lnl t s od ling requl re ml n u visioil s plemootary pa ck ig o i of lhe log ea l e rchftec tl llll i ta stanc ilaw dnl mqd i i s t m ch das gn i teracm n diagmms ta dy namle view i regisl cr produclcatalog i i i i uanli ll ii i i i i e m d l i i i oosisa igr a ultic ewl regl ter makanewsalao cntc rl tcm ii i i i i i example figure shows a partial layered logical architecture drawn with uml package diagram notation figure layers shown with uml package diagram notation what is the logical architecture and layers the logical architecture is the large scale organization of the software classes into packages or namespaces subsystems and layers it called the logical architecture because there no decision about how these elements are deployed across different operating system processes or across physical computers in a network these latter decisions are part of the deployment architecture a layer is a very coarse grained grouping of classes packages or subsystems that has cohesive responsibility for a major aspect of the system also layers are organized such that higher layers such as the ui layer call upon services of lower layers but not normally vice versa typically layers in an oo system include user interface application logic and domain objects software objects representing domain concepts for example a software class sale that fulfill application requirements such as calculating a sale total technical services general purpose objects and subsystems that provide supporting technical services such as interfacing with a database or error logging these services are usually application independent and reusable across several systems in a strict layered architecture a layer only calls upon the services of the layer directly below it this design is common in network protocol stacks but not in information systems which usually have a relaxed layered architecture in which a higher layer calls upon several lower layers for example the ui layer may call upon its directly subordinate application logic layer and also upon elements of a lower technical service layer for logging and so forth a logical architecture doesn t have to be organized in layers but it very common and hence introduced at this time what layers are the focus in the case studies to reiterate a point made when the case studies were introduced case studies p exploring design of the other layers such as the ui layer will focus on the design of their interface to the application logic layer the discussion on p explains but in brief why the other layers tend to be very technology dependent for example very specific to java or net and in any case the oo design lessons learned in the context of the application logic domain layer are applicable to all other layers or components what is software architecture i touched on the logical and deployment architectures so now is a good time to introduce a definition for software architecture here one an architecture is the set of significant decisions about the organization of a software system the selection of the structural elements and their interfaces by which the system is composed together with their behavior as specified in the collaborations among those elements the composition of these structural and behavioral elements into progressively larger subsystems and the architectural style that guides this organizationthese elements and their interfaces their collaborations and their composition regardless of the definition and there are many the common theme in all software architecture definitions is that it has to do with the large scalethe big ideas in the motivations constraints organization patterns responsibilities and connections of a system or a system of systems applying uml package diagrams uml package diagrams are often used to illustrate the logical architecture of a systemthe layers subsystems packages in the java sense etc a layer can be modeled as a uml package for example the ui layer modeled as a package named ui a uml package diagram provides a way to group elements a uml package can group anything classes other packages use cases and so on nesting packages is very common a uml package is a more general concept than simply a java package or net namespace though a uml package can represent thoseand more the package name may be placed on the tab if the package shows inner members or on the main folder if not it is common to want to show dependency a coupling between packages so that developers can see the large scale coupling in the system the uml dependency line is used for this a dashed arrowed line with the arrow pointing towards the depended on package a uml package represents a namespace so that for example a date class may be defined in two packages if you need to provide fully qualified names the uml notation is for example java util date in the case that there was an outer package named java with a nested package named util with a date class the uml provides alternate notations to illustrate outer and inner nested packages sometimes it is awkward to draw an outer package box around inner packages alternatives are shown in figure figure alternate uml approaches to show package nesting using embedded packages uml fully qualified names and the circle cross symbol view full size image uml tools reverse engineer package diagrams from code during early development we may sketch a uml package diagram and then organize our code according to these package sketches over time the code base grows and we spend more time programming and less on modeling or uml diagrams at that point a great use for a uml case tool is to reverse engineer the source code and generate a package diagram automatically this practice is enhanced if we use the naming conventions on p suggested for code packages guideline design with layers the essential ideas of using layers are simple organize the large scale logical structure of a system into discrete layers of distinct related responsibilities with a clean cohesive separation of concerns such that the lower layers are low level and general services and the higher layers are more application specific collaboration and coupling is from higher to lower layers lower to higher layer coupling is avoided some more design issues are covered later starting on p the idea is described as the layers pattern in and produces a layered architecture it has been applied and written about so often as a pattern that the pattern almanac lists over patterns that are variants of or related to the layers pattern using layers helps address several problems source code changes are rippling throughout the systemmany parts of the systems are highly coupled application logic is intertwined with the user interface so it cannot be reused with a different interface or distributed to another processing node potentially general technical services or business logic is intertwined with more application specific logic so it cannot be reused distributed to another node or easily replaced with a different implementation there is high coupling across different areas of concern it is thus difficult to divide the work along clear boundaries for different developers the purpose and number of layers varies across applications and application domains information systems operating systems and so forth applied to information systems typical layers are illustrated and explained in figure figure common layers in an information system logical architecture view full size image the width of the package is used to communicate range of applicability in this diagram but this is not a general uml practice aka means also known as the application layer in figure is discussed on p benefits of using layers in general there is a separation of concerns a separation of high from low level services and of application specific from general services this reduces coupling and dependencies improves cohesion increases reuse potential and increases clarity related complexity is encapsulated and decomposable some layers can be replaced with new implementations this is generally not possible for lower level technical service or foundation layers e g java util but may be possible for ui application and domain layers lower layers contain reusable functions some layers primarily the domain and technical services can be distributed development by teams is aided because of the logical segmentation guideline cohesive responsibilities maintain a separation of concerns the responsibilities of the objects in a layer should be strongly related to each other and should not be mixed with responsibilities of other layers for example objects in the ui layer should focus on ui work such as creating windows and widgets capturing mouse and keyboard events and so forth objects in the application logic or domain layer should focus on application logic such as calculating a sales total or taxes or moving a piece on a game board ui objects should not do application logic for example a java swing jframe window object should not contain logic to calculate taxes or move a game piece and on the other hand application logic classes should not trap ui mouse or keyboard events that would violate a clear separation of concerns and maintaining high cohesionbasic architectural principles high cohesion p later chapters will explore these important principles plus the model view separation principle in greater detail model view p code mapping code organization to layers and uml packages most popular oo languages java c c python provide support for packages called namespaces in c and c here an example using java for mapping uml packages to code the layers and packages illustrated in figure can map to java package names as follows notice that the layer name is used as a section of the java package name ui layer com mycompany nextgen ui swing com mycompany nextgen ui web domain layer packages specific to the nextgen project com mycompany nextgen domain sales com mycompany nextgen domain payments technical services layer our home grown persistence database access layer com mycompany service persistence third party org apache org apache soap rpc foundation layer foundation packages that our team creates com mycompany util notice that to support cross project reuse we avoided using a specific application qualifier nextgen in the package names unless necessary the ui packages are related to the nextgen pos application so they are qualified with the application name com mycompany nextgen ui but the utilities we write could be shared across many projects hence the package name com mycompany utils not com mycompany nextgen utils uml tools reverse engineer package diagrams from code as mentioned earlier a great use for a uml case tool is to reverse engineer the source code and generate a package diagram automatically this practice is enhanced if you use the recommended naming conventions in code for example if you include the partial name ui in all packages for the ui layer then the uml case tool will automatically group and nest sub packages under a ui package and you can see the layered architecture in both code and package diagram definition domain layer vs application logic layer domain objects a typical software system has ui logic and application logic such as gui widget creation and tax calculations now here a key question how do we design the application logic with objects we could create one class called xyz and put all the methods for all the required logic in that one class it could technically work though be a nightmare to understand and maintain but it isn t the recommended approach in the spirit of oo thinking so what is the recommended approach answer to create software objects with names and information similar to the real world domain and assign application logic responsibilities to them for example in the real world of pos there are sales and payments so in software we create a sale and payment class and give them application logic responsibilities this kind of software object is called a domain object it represents a thing in the problem domain space and has related application or business logic for example a sale object being able to calculate its total designing objects this way leads to the application logic layer being more accurately called the domain layer of the architecturethe layer that contains domain objects to handle application logic work what the relationship between the domain layer and domain model this is another key point there a relationship between the domain model and the domain layer we look to the domain model which is a visualization of noteworthy domain concepts for inspiration for the names of classes in the domain layer see figure figure domain layer and domain model relationship view full size image the domain layer is part of the software and the domain model is part of the conceptual perspective analysisthey aren t the same thing but by creating a domain layer with inspiration from the domain model we achieve a lower representational gap between the real world domain and our software design for example a sale in the up domain model helps inspire us to consider creating a software sale class in the domain layer of the up design model definition tiers layers and partitions the original notion of a tier in architecture was a logical layer not a physical node but the word has become widely used to mean a physical processing node or cluster of nodes such as the client tier the client computer this book will avoid the term for clarity but bear this in mind when reading architecture literature the layers of an architecture are said to represent the vertical slices while partitions represent a horizontal division of relatively parallel subsystems of a layer for example the technical services layer may be divided into partitions such as security and reporting figure figure layers and partitions guideline don t show external resources as the bottom layer most systems rely on external resources or services such as a mysql inventory database and a novell ldap naming and directory service these are physical implementation components not a layer in the logical architecture showing external resources such as a particular database in a layer below the foundation layer for example mixes up the logical view and the deployment views of the architecture rather in terms of the logical architecture and its layers access to a particular set of persistent data such as inventory data can be viewed as a sub domain of the domain layerthe inventory sub domain and the general services that provide access to databases may be viewed as a technical service partitionthe persistence service see figure figure mixing views of the architecture view full size image w lfs e l llb c r dd jj a logical r ta of ia llllmltl l cil llllla or rvieiii la ii 1iubooma ns bsliradtli g implcmeill ion de i lom svd imi lllllllml e li i i i e guideline the model view separation principle what kind of visibility should other packages have to the ui layer how should non window classes communicate with windows in this context model is a synonym for the domain layer of objects it an old oo term from the late view is a synonym for ui objects such as windows web pages applets and reports domain layer object p the model view separation principle states that model domain objects should not have direct knowledge of view ui objects at least as view objects so for example a register or sale object should not directly send a message to a gui window object processsaleframe asking it to display something change color close and so forth this is a key principle in the pattern model view controller mvc mvc was originally a small scale smalltalk pattern and related data objects models gui widgets views and mouse and keyboard event handlers controllers more recently the term mvc has been coopted by the distributed design community to also apply on a large scale architectural level the model is the domain layer the view is the ui layer and the controllers are the workflow objects in the application layer a legitimate relaxation of this principle is the observer pattern where the domain objects send messages to ui objects viewed only in terms of an interface such as propertylistener a common java interface for this situation then the domain object doesn t know that the ui object is a ui objectit doesn t know its concrete window class it only knows the object as something that implements the propertylistener interface observer p a further part of this principle is that the domain classes encapsulate the information and behavior related to application logic the window classes are relatively thin they are responsible for input and output and catching gui events but do not maintain application data or directly provide application logic for example a java jframe window should not have a method that does a tax calculation a web jsp page should not contain logic to calculate the tax these ui elements should delegate to non ui elements for such responsibilities the motivation for model view separation includes to support cohesive model definitions that focus on the domain processes rather than on user interfaces to allow separate development of the model and user interface layers to minimize the impact of requirements changes in the interface upon the domain layer to allow new views to be easily connected to an existing domain layer without affecting the domain layer to allow multiple simultaneous views on the same model object such as both a tabular and business chart view of sales information to allow execution of the model layer independent of the user interface layer such as in a message processing or batch mode system to allow easy porting of the model layer to another user interface framework what the connection between ssds system operations and layers during analysis work we sketched some ssds for use case scenarios we identified input events from external actors into the system calling upon system operations such as makenewsale and enteritem the ssds illustrate these system operations but hide the specific ui objects nevertheless normally it will be objects in the ui layer of the system that capture these system operation requests usually with a rich client gui or web page in a well designed layered architecture that supports high cohesion and a separation of concerns the ui layer objects will then forwardor delegatethe request from the ui layer onto the domain layer for handling now here the key point for example in java swing perhaps a gui window class called processsaleframe in the ui layer will pick up the mouse and keyboard events requesting to enter an item and then the processsaleframe object will send an enteritem message on to a software object in the domain layer such as register to perform the application logic see figure figure system operations in the ssds and in terms of layers view full size image l l i i sy ste m ii i e te m id n m ta j le w n i i i i i wsalilo entelltemo emlsa le l p i i i lhe opei eliqn handhki by the y l em i n a n s d reprffill ltie operal orn i oo lhe p jloo or layer rom th ui tayer i i icniiw i example nextgen logical architecture and package diagram figure hints at the simple logical architecture for this iteration things get more interesting in later iterations for example see many examples of the nextgen logical architecture and package diagrams starting on p example monopoly logical architecture the monopoly architecture is a simple layered designui domain and services there is nothing novel to illustrate so the nextgen case study is used for the architectural examples recommended resources there a wealth of literature on layered architectures both in print and on the web a series of patterns in pattern languages of program design volume first address the topic in pattern form although layered architectures have been used and written about since at least the volume continues with further layers related patterns pattern oriented software architecture volume provides a good treatment of the layers pattern chapter on to object design i do not like this word bomb it is not a bomb it is a device that is exploding ambassador jacques le blanc on nuclear weapons introduction how do developers design objects here are three ways code design while coding java c ideally with power tools such as refactorings from mental model to code draw then code drawing some uml on a whiteboard or uml case tool then switching to with a text strong ide e g eclipse or visual studio only draw somehow the tool generates everything from diagrams many a dead tool vendor has washed onto the shores of this steep island only draw is a misnomer as this still involves a text programming language attached to uml graphic elements view full size image of course there are other ways to design with other languages if we use draw then code the most popular approach with uml the drawing overhead should be worth the effort this chapter introduces object design and lightweight drawing before coding suggesting ways to make it pay off what a next generation language a one view is that it one that raises the level of the coding symbols from bits to text to perhaps icons or even gestures packing more functionality into each symbol another view is that a is more declarative and goal specifying rather than procedural although already exhibit this other design techniques p agile modeling and lightweight uml drawing some aims of agile modeling are to reduce drawing overhead and model to understand and communicate rather than to documentthough documenting is easy with digital photos try the simple agile modeling approach practices include using lots of whiteboards ten in a room not two or special white plastic static cling sheets that work like whiteboards covering large wall areas using markers digital cameras and printers to capture uml as sketch one of the three ways to apply uml agile modeling p three ways to apply uml p agile modeling also includes modeling with others creating several models in parallel for example five minutes on a wall of interaction diagrams then five minutes on a wall of related class diagrams how big is the area you d like to draw in with your eyes and hands fifteen by two meters or by cm more monitor size most people prefer big but cheap virtual reality uml tools don t exist yet the simple alternative is lots of white static cling sheets or whiteboards reflecting the xp agile principle do the simplest thing that could possibly work more tips it easy to upload digital photos of wall drawings to an internal wiki see www twiki org that captures your project information popular brands of white plastic static cling sheets north america and avery write on cling sheets europe legamaster magic chart i like this roll style it makes it easy to unroll a long sheet of cling plastic uml case tools please don t misinterpret my suggestion of wall sketching and agile modeling as implying that uml case tools aren t also useful both can add value these tools range from expensive to free and open source and each year improve in usefulness each year best choice changes so i won t make a stale suggestion but many developers find it useful to code awhile in their favorite ide then press a button reverse engineer the code and see a uml big picture graphical view of their design also note how much time spent drawing uml before coding if agile modeling then before each subsequent modeling session reverse engineer the growing code base into uml diagrams print them out perhaps on large plotter paper and refer to them during the sketching session designing objects what are static and dynamic modeling there are two kinds of object models dynamic and static dynamic models such as uml interaction diagrams sequence diagrams or communication diagrams help design the logic the behavior of the code or the method bodies they tend to be the more interesting difficult important diagrams to create static models such as uml class diagrams help design the definition of packages class names attributes and method signatures but not method bodies see figure figure static and dynamic uml diagrams for object modeling there a relationship between static and dynamic modeling and the agile modeling practice of create models in parallel spend a short period of time on interaction diagrams dynamics then switch to a wall of related class diagrams statics dynamic object modeling people new to uml tend to think that the important diagram is the static view class diagram but in fact most of the challenging interesting useful design work happens while drawing the uml dynamic view interaction diagrams it during dynamic object modeling such as drawing sequence diagrams that the rubber hits the road in terms of really thinking through the exact details of what objects need to exist and how they collaborate via messages and methods therefore this book starts by introducing dynamic object modeling with interaction diagrams interaction diagrams p note that it especially during dynamic modeling that we apply responsibility driven design and the grasp principles the subsequent chapters focus on these key topics of the bookand key skills in oo design rdd and grasp p there are other dynamic tools in the uml kit including state machine diagrams p and activity diagrams p static object modeling the most common static object modeling is with uml class diagrams after first covering dynamic modeling with interaction diagrams i introduce the details note though that if the developers are applying the agile modeling practice of create several models in parallel they will be drawing both interaction and class diagrams concurrently class diagrams p other support in the uml for static modeling includes package diagrams p and deployment diagrams p the importance of object design skill over uml notation skill the following chapters explore detailed object design while applying uml diagrams it been said before but is important to stress what important is knowing how to think and design in objects and apply object design best practice patterns which is a very different and much more valuable skill than knowing uml notation while drawing a uml object diagram we need to answer key questions what are the responsibilities of the object who does it collaborate with what design patterns should be applied far more important than knowing the difference between uml and notation therefore the emphasis of the following chapters is on these principles and patterns in object design other object design techniques crc cards people prefer different design methods because of familiarity and quite significantly because of different cognitive styles don t assume that icons and pictures are better than text for everyone or vice versa a popular text oriented modeling technique is class responsibility collaboration crc cards created by the agile influential minds of kent beck and ward cunningham also founders of the ideas of xp and design patterns crc cards are paper index cards on which one writes the responsibilities and collaborators of classes each card represents one class a crc modeling session involves a group sitting around a table discussing and writing on the cards as they play what if scenarios with the objects considering what they must do and what other objects they must collaborate with see figure and figure figure template for a crc card figure four sample crc cards this minimized example is only meant to show the typical level of detail rather than the specific text view full size image g j l w m r tlill t i um j c _ ol ii i i i i i ic ae e i chapter uml interaction diagrams cats are smarter than dogs you can t get eight cats to pull a sled through snow jeff valdez introduction the uml includes interaction diagrams to illustrate how objects interact via messages they are used for dynamic object modeling there are two common types sequence and communication interaction diagrams this chapter introduces the notationview it as a reference to skim throughwhile subsequent chapters focus on a more important question what are key principles in oo design in the following chapters interaction diagrams are applied to help explain and demonstrate object design hence it useful to at least skim these examples before moving on view full size image sequence and communication diagrams the term interaction diagram is a generalization of two more specialized uml diagram types sequence diagrams communication diagrams both can express similar interactions a related diagram is the interaction overview diagram it provides a big picture overview of how a set of interaction diagrams are related in terms of logic and process flow however it new to uml and so it too early to tell if it will be practically useful sequence diagrams are the more notationally rich of the two types but communication diagrams have their use as well especially for wall sketching throughout the book both types will be used to emphasize the flexibility in choice sequence diagrams illustrate interactions in a kind of fence format in which each new object is added to the right as shown in figure figure sequence diagram what might this represent in code probably that class a has a method named doone and an attribute of type b also that class b has methods named dotwo and dothree perhaps the partial definition of class a is code mapping or generation rules will vary depending on the oo language public class a private b myb new b public void doone myb dotwo myb dothree communication diagrams illustrate object interactions in a graph or network format in which objects can be placed anywhere on the diagram the essence of their wall sketching advantage as shown in figure figure communication diagram what are the strengths and weaknesses of sequence vs communication diagrams each diagram type has advantages and modelers have idiosyncratic preferencethere isn t an absolutely correct choice however uml tools usually emphasize sequence diagrams because of their greater notational power sequence diagrams have some advantages over communication diagrams perhaps first and foremost the uml specification is more sequence diagram centricmore thought and effort has been put into the notation and semantics thus tool support is better and more notation options are available also it is easier to see the call flow sequence with sequence diagramssimply read top to bottom with communication diagrams we must read the sequence numbers such as and hence sequence diagrams are excellent for documentation or to easily read a reverse engineered call flow sequence generated from source code with a uml tool but on the other hand communication diagrams have advantages when applying uml as sketch to draw on walls an agile modeling practice because they are much more space efficient this is because the boxes can be easily placed or erased anywherehorizontal or vertical consequently as well modifying wall sketches is easier with communication diagramsit is simple during creative high change oo design work to erase a box at one location draw a new one elsewhere and sketch a line to it in contrast new objects in a sequence diagrams must always be added to the right edge which is limiting as it quickly consumes and exhausts right edge space on a page or wall free space in the vertical dimension is not efficiently used developers doing sequence diagrams on walls rapidly feel the drawing pain when contrasted with communication diagrams three ways to use uml p likewise when drawing diagrams that are to be published on narrow pages like this book communication diagrams have the advantage over sequence diagrams of allowing vertical expansion for new objectsmuch more can be packed into a small visual space type strengths weaknesses sequence clearly shows sequence or time ordering of messages large set of detailed notation options forced to extend to the right when adding new objects consumes horizontal space communication space economicalflexibility to add new objects in two dimensions more difficult to see sequence of messages fewer notation options example sequence diagram makepayment the sequence diagram shown in figure is read as follows the message makepayment is sent to an instance of a register the sender is not identified the register instance sends the makepayment message to a sale instance the sale instance creates an instance of a payment figure sequence diagram view full size image from reading figure what might be some related code for the sale class and its makepayment method public class sale private payment payment public void makepayment money cashtendered payment new payment cashtendered example communication diagram makepayment figure communication diagram view full size image the communication diagram shown in figure has the same intent as the prior sequence diagram novice uml modelers don t pay enough attention to interaction diagrams most uml novices are aware of class diagrams and usually think they are the only important diagram in oo design not true although the static view class diagrams are indeed useful the dynamic view interaction diagramsor more precisely acts of dynamic interaction modelingare incredibly valuable why because it when we have to think through the concrete details of what messages to send and to whom and in what order that the rubber hits the road in terms of thinking through the true oo design details common uml interaction diagram notation illustrating participants with lifeline boxes in the uml the boxes you ve seen in the prior sample interaction diagrams are called lifeline boxes their precise uml definition is subtle but informally they represent the participants in the interactionrelated parts defined in the context of some structure diagram such as a class diagram it is not precisely accurate to say that a lifeline box equals an instance of a class but informally and practically the participants will often be interpreted as such therefore in this text i ll often write something like the lifeline representing a sale instance as a convenient shorthand see figure for common cases of notation figure lifeline boxes to show participants in interactions view full size image basic message expression syntax interaction diagrams show messages between objects the uml has a standard syntax for these message expressions an alternate syntax such as c or java is acceptableand supported by uml tools return message parameter parametertype returntype parentheses are usually excluded if there are no parameters though still legal type information may be excluded if obvious or unimportant for example initialize code initialize d getproductdescription id d getproductdescription id itemid d getproductdescription id itemid productdescription singleton objects in the world of oo design patterns there is one that is especially common called the singleton pattern it is explained later but an implication of the pattern is that there is only one instance of a class instantiatednever two in other words it is a singleton instance in a uml interaction diagram sequence or communication such an object is marked with a in the upper right corner of the lifeline box it implies that the singleton pattern is used to gain visibility to the objectthe meaning of that won t be clear at this time but will be upon reading its description on p see figure figure singletons in interaction diagrams view full size image singleton p basic sequence diagram notation lifeline boxes and lifelines in contrast to communication diagrams in sequence diagrams the lifeline boxes include a vertical line extending below themthese are the actual lifelines although virtually all uml examples show the lifeline as dashed because of uml influence in fact the uml specification says it may be solid or dashed lifeline boxes p messages each typical synchronous message between objects is represented with a message expression on a filled arrowed solid line between the vertical lifelines see figure the time ordering is organized from top to bottom of lifelines an open message arrow means an asynchronous message in an interaction diagram figure messages and focus of control with execution specification bar in the example of figure the starting message is called a found message in the uml shown with an opening solid ball it implies the sender will not be specified is not known or that the message is coming from a random source however by convention a team or tool may ignore showing this and instead use a regular message line without the ball intending by convention it is a found message therefore many of the book examples won t bother with the found message notation focus of control and execution specification bars as illustrated in figure sequence diagrams may also show the focus of control informally in a regular blocking call the operation is on the call stack using an execution specification bar previously called an activation bar or simply an activation in uml the bar is optional guideline drawing the bar is more common and often automatic when using a uml case tool and less common when wall sketching illustrating reply or returns there are two ways to show the return result from a message using the message syntax returnvar message parameter using a reply or return message line at the end of an activation bar both are common in practice i prefer the first approach when sketching as it less effort if the reply line is used the line is normally labelled with an arbitrary description of the returning value see figure figure two ways to show a return result from a message messages to self or this you can show a message being sent from an object to itself by using a nested activation bar see figure figure messages to this creation of instances object creation notation is shown in figure note the uml mandated dashed line the arrow is filled if it a regular synchronous message such as implying invoking a java constructor or open stick arrow if an asynchronous call the message name create is not requiredanything is legalbut it a uml idiom i see no value in requiring a dashed line but it in the spec many author examples use a solid line as early draft versions of the spec did as well figure instance creation and object lifelines view full size image the typical interpretation in languages such as java or c of a create message on a dashed line with a filled arrow is invoke the new operator and call the constructor object lifelines and object destruction in some circumstances it is desirable to show explicit destruction of an object for example when using c which does not have automatic garbage collection or when you want to especially indicate an object is no longer usable such as a closed database connection the uml lifeline notation provides a way to express this destruction see figure figure object destruction diagram frames in uml sequence diagrams to support conditional and looping constructs among many other things the uml uses frames frames are regions or fragments of the diagrams they have an operator or label such as loop and a guard conditional clause see figure also called diagram frames or interaction frames the boolean test guard should be placed over the lifeline to which it belongs figure example uml frame view full size image the following table summarizes some common frame operators frame operator meaning alt alternative fragment for mutual exclusion conditional logic expressed in the guards loop loop fragment while guard is true can also write loop n to indicate looping n times there is discussion that the specification will be enhanced to define a for loop such as loop i opt optional fragment that executes if guard is true par parallel fragments that execute in parallel region critical region within which only one thread can run looping the loop frame notation to show looping is shown in figure conditional messages an opt frame is placed around one or more messages notice that the guard is placed over the related lifeline see figure figure a conditional message conditional messages in uml x stylestill useful the uml x notation to show a single conditional message is heavyweight requiring an entire opt frame box around one message see figure the older uml x notation for single conditional messages in sequence diagrams is not legal in uml but so simple that especially when sketching it will probably be popular for years to come see figure figure a conditional message in uml x notationa simple style guideline use uml style only for simple single messages when sketching mutually exclusive conditional messages an alt frame is placed around the mutually exclusive alternatives see figure figure mutually exclusive conditional messages iteration over a collection a common algorithm is to iterate over all members of a collection such as a list or map sending the same message to each often some kind of iterator object is ultimately used such as an implementation of java util iterator or a c standard library iterator although in the sequence diagram that low level mechanism need not be shown in the interest of brevity or abstraction at the time of this writing the uml specification did not and may never have an official idiom for this case two alternatives are shownreviewed with the leader of the uml interaction specificationin figure and figure figure iteration over a collection using relatively explicit notation view full size image figure iteration over a collection leaving things more implicit note the selector expression lineitems i in the lifeline of figure the selector expression is used to select one object from a group lifeline participants should represent one object not a collection in java for example the following code listing is a possible implementation that maps the explicit use of the incrementing variable i in figure to an idiomatic solution in java using its enhanced for statement c has the same public class sale private list saleslineitem lineitems new arraylist saleslineitem public money gettotal money total new money money subtotal null for saleslineitem lineitem lineitems subtotal lineitem getsubtotal total add subtotal return total another variation is shown in figure the intent is the same but details are excluded a team or tool could agree on this simple style by convention to imply iteration over all the collection elements i use this style later in the book nesting of frames frames can be nested see figure figure nesting of frames how to relate interaction diagrams figure illustrates probably better than words an interaction occurrence also called an interaction use is a reference to an interaction within another interaction it is useful for example when you want to simplify a diagram and factor out a portion into another diagram or there is a reusable interaction occurrence uml tools take advantage of them because of their usefulness in relating and linking diagrams figure example interaction occurrence sd and ref frames view full size image they are created with two related frames a frame around an entire sequence diagram labeled with the tag sd and a name such as authenticateuser interaction occurrences and ref frames can also be used for communication diagrams a frame tagged ref called a reference that refers to another named sequence diagram it is the actual interaction occurrence interaction overview diagrams also contain a set of reference frames interaction occurrences these diagrams organized references into a larger structure of logic and process flow guideline any sequence diagram can be surrounded with an sd frame to name it frame and name one when you want to refer to it using a ref frame messages to classes to invoke static or class methods you can show class or static method calls by using a lifeline box label that indicates the receiving object is a class or more precisely an instance of a metaclass see figure figure invoking class or static methods showing a class object as an instance of a metaclass what do i mean for example in java and smalltalk all classes are conceptually or literally instances of class class in net classes are instances of class type the classes class and type are metaclasses which means their instances are themselves classes a specific class such as class calendar is itself an instance of class class thus class calendar is an instance of a metaclass it may help to drink some beer before trying to understand this in code a likely implementation is public class foo public void dox static method call on class calendar locale locales calendar getavailablelocales polymorphic messages and cases polymorphism is fundamental to oo design how to show it in a sequence diagram that a common uml question one approach is to use multiple sequence diagramsone that shows the polymorphic message to the abstract superclass or interface object and then separate sequence diagrams detailing each polymorphic case each starting with a found polymorphic message figure illustrates figure an approach to modeling polymorphic cases in sequence diagrams view full size image asynchronous and synchronous calls an asynchronous message call does not wait for a response it doesn t block they are used in multi threaded environments such as net and java so that new threads of execution can be created and initiated in java for example you may think of the thread start or runnable run called by thread start message as the asynchronous starting point to initiate execution on a new thread the uml notation for asynchronous calls is a stick arrow message regular synchronous blocking calls are shown with a filled arrow see figure figure asynchronous calls and active objects view full size image an object such as the clock in figure is also known as an active objecteach instance runs on and controls its own thread of execution in the uml it may be shown with double vertical lines on the left and right sides of the lifeline box the same notation is used for an active class whose instances are active objects active class p in java a likely implementation for figure follows notice that the thread object in the code is excluded from the uml diagram because it is simply a consistent overhead mechanism to realize an asynchronous call in java public class clockstarter public void startclock thread t new thread new clock t start asynchronous call to the run method on the clock system runfinalization example follow on message objects should implement the runnable interface in java to be used on new threads public class clock implements runnable public void run while true loop forever on own thread basic communication diagram notation links a link is a connection path between two objects it indicates some form of navigation and visibility between the objects is possible see figure more formally a link is an instance of an association for example there is a linkor path of navigationfrom a register to a sale along which messages may flow such as the makepayment message figure link lines messages each message between objects is represented with a message expression and small arrow indicating the direction of the message many messages may flow along this link figure a sequence number is added to show the sequential order of messages in the current thread of control figure messages messages to self or this a message can be sent from an object to itself figure this is illustrated by a link to itself with messages flowing along the link figure messages to this creation of instances any message can be used to create an instance but the convention in the uml is to use a message named create for this purpose some use new see figure if another less obvious message name is used the message may be annotated with a uml stereotype like so create the create message may include parameters indicating the passing of initial values this indicates for example a constructor call with parameters in java furthermore the uml tagged value new may optionally be added to the lifeline box to highlight the creation tagged values are a flexible extension mechanism in the uml to add semantically meaningful information to a uml element figure instance creation view full size image message number sequencing the order of messages is illustrated with sequence numbers as shown in figure the numbering scheme is the first message is not numbered thus is unnumbered actually a starting number is legal but it makes all subsequent numbering more awkward creating another level of number nesting deeper than otherwise necessary the order and nesting of subsequent messages is shown with a legal numbering scheme in which nested messages have a number appended to them you denote nesting by prepending the incoming message number to the outgoing message number figure sequence numbering figure shows a more complex case figure complex sequence numbering view full size image conditional messages you show a conditional message figure by following a sequence number with a conditional clause in square brackets similar to an iteration clause the message is only sent if the clause evaluates to true figure conditional message mutually exclusive conditional paths the example in figure illustrates the sequence numbers with mutually exclusive conditional paths figure mutually exclusive messages view full size image in this case we must modify the sequence expressions with a conditional path letter the first letter used is a by convention figure states that either or could execute after both are sequence number since either could be the first internal message note that subsequent nested messages are still consistently prepended with their outer message sequence thus is nested message within iteration or looping iteration notation is shown in figure if the details of the iteration clause are not important to the modeler a simple can be used figure iteration iteration over a collection a common algorithm is to iterate over all members of a collection such as a list or map sending the same message to each in communication diagrams this could be summarized as shown in figure although there is no official uml convention figure iteration over a collection view full size image messages to a classes to invoke static class methods see the discussion of metaclasses in the sequence diagram case on p to understand the purpose of the example in figure figure messages to a class object static method invocation polymorphic messages and cases refer to figure for the related context class hierarchy and example for sequence diagrams as in the sequence diagram case multiple communication diagrams can be used to show each concrete polymorphic case figure 34 figure 34 an approach to modeling polymorphic cases in communication diagrams view full size image asynchronous and synchronous calls as in sequence diagrams asynchronous calls are shown with a stick arrow synchronous calls with a filled arrow see figure figure asynchronous call in a communication diagram ru n finalizat ioo clockstarter system cliass o r ii i i i i i icniiw i chapter uml class diagrams to iterate is human to recurse divine anonymous introduction the uml includes class diagrams to illustrate classes interfaces and their associations they are used for static object modeling we ve already introduced and used this uml diagram while domain modeling applying class diagrams in a conceptual perspective this chapter summarizes more of the notation irrespective of the perspective conceptual or software as with the prior interaction diagram chapter this is a reference subsequent chapters focus on a more important question what are key principles in oo design those chapters apply uml interaction and class diagrams to help explain and demonstrate object design hence it useful to first skim this chapter but there no need to memorize all these low level details view full size image applying uml common class diagram notation much of the high frequency class diagram notation can be summarized and understood in one figure most elements in figure are optional e g visibility parameters compartments modelers draw show or hide them depending on context and the needs of the reader or uml tool figure common uml class diagram notation view full size image for example this chapter summarizes uml association class notation but doesn t explain the ooa d modeling context likewise with many of the notation elements applying association classes p definition design class diagram as we ve explored the same uml diagram can be used in multiple perspectives figure in a conceptual perspective the class diagram can be used to visualize a domain model for discussion we also need a unique term to clarify when the class diagram is used in a software or design perspective a common modeling term for this purpose is design class diagram dcd which i ll use regularly in later chapters in the up the set of all dcds form part of the design model other parts of the design model include uml interaction and package diagrams figure uml class diagrams in two perspectives view full size image definition classifier a uml classifier is a model element that describes behavioral and structure features classifiers can also be specialized they are a generalization of many of the elements of the uml including classes interfaces use cases and actors in class diagrams the two most common classifiers are regular classes and interfaces ways to show uml attributes attribute text and association lines attributes of a classifier also called structural properties in the uml are shown several ways often shortened to property with the disadvantage of causing ambiguity versus the more general definition of a uml property p attribute text notation such as currentsale sale association line notation both together figure shows these notations being used to indicate that a register object has an attribute a reference to one sale object figure attribute text versus association line notation for a uml attribute view full size image the full format of the attribute text notation is visibility name type multiplicity default property string also the uml allows any other programming language syntax to be used for the attribute declaration as long as the reader or tool are notified as indicated in figure visibility marks include public private and so forth guideline attributes are usually assumed private if no visibility is given notice in figure that this attribute as association line has the following style a navigability arrow pointing from the source register to target sale object indicating a register object has an attribute of one sale a multiplicity at the target end but not the source end use the multiplicity notation described on p a rolename currentsale only at the target end to show the attribute name no association name guideline when showing attributes as associations follow this style in dcds which is suggested by the uml specification it is true that the uml metamodel also allows multiplicity and rolenames at the source end e g the register end in figure and also an association name but they are not usually useful in the context of a dcd guideline on the other hand when using class diagrams for a domain model do show association names but avoid navigation arrows as a domain model is not a software perspective see figure figure idioms in association notation usage in different perspectives view full size image note that this is not a new kind of association notation it the same uml notation for associations explored while applying class diagrams to domain modeling on p this is an elaboration of the notation for use in the context of a software perspective dcd guideline when to use attribute text versus association lines for attributes this question was first explored in the context of domain modeling on p to review a data type refers to objects for which unique identity is not important common data types are primitive oriented types such as boolean date or datetime number character string text time address color geometrics point rectangle phone number social security number universal product code upc sku zip or postal codes enumerated types guideline use the attribute text notation for data type objects and the association line notation for others both are semantically equal but showing an association line to another class box in the diagram as in figure gives visual emphasisit catches the eye emphasizing the connection between the class of objects on the diagram see figure for contrasting examples figure applying the guidelines to show attributes in two notations view full size image again these different styles exist only in the uml surface notation in code they boil down to the same thingthe register class of figure has three attributes for example in java public class register private int id private sale currentsale private store location the uml notation for an association end as discussed the end of an association can have a navigability arrow it can also include an optional rolename officially an association end name to indicate the attribute name and of course the association end may also show a multiplicity value as explored earlier on p such as or notice in figure that the rolename currentsale is used to indicate the attribute name and as shown in figure a property string such as ordered or ordered list is possible ordered is a uml defined keyword that implies the elements of the collection are the suspense builds ordered another related keyword is unique implying a set of unique elements figure two ways to show a collection attribute in the uml view full size image the keyword list illustrates that the uml also supports user defined keywords i define list to mean the collection attribute lineitems will be implemented with an object implementing the list interface how to show collection attributes with attribute text and association lines suppose that a sale software object holds a list an interface for a kind of collection of many saleslineitem objects for example in java public class sale private list saleslineitem lineitems new arraylist saleslineitem figure shows two ways to illustrate a collection attribute in class diagrams notice also the optional use of property strings such as ordered note symbols notes comments constraints and method bodies note symbols can be used on any uml diagram but are especially common on class diagrams a uml note symbol is displayed as a dog eared rectangle with a dashed line to the annotated element they ve already been used throughout the book for example figure a note symbol may represent several things such as a uml note or comment which by definition have no semantic impact a uml constraint in which case it must be encased in braces see figure a method bodythe implementation of a uml operation see figure figure how to show a method body in a class diagram view full size image operations and methods operations one of the compartments of the uml class box shows the signatures of operations see figure for many examples at the time of this writing the full official format of the operation syntax is visibility name parameter list property string notice there is no return type element an obvious problem but purposefully injected into the uml specification for inscrutable reasons there is a chance that the specification will revert to a ish syntax which in any event many authors show and uml tools will continue to support visibility name parameter list return type property string guideline assume the version that includes a return type guideline operations are usually assumed public if no visibility is shown the property string contains arbitrary additional information such as exceptions that may be raised if the operation is abstract and so forth in addition to the official uml operation syntax the uml allows the operation signature to be written in any programming language such as java assuming the reader or tool is notified for example both expressions are possible getplayer name string player exception ioexception public player getplayer string name throws ioexception an operation is not a method a uml operation is a declaration with a name parameters return type exceptions list and possibly a set of constraints of pre and post conditions but it isn t an implementationrather methods are implementations when we explored operation contracts p in uml terms we were exploring the definition of constraints for uml operations as was discussed on p how to show methods in class diagrams a uml method is the implementation of an operation if constraints are defined the method must satisfy them a method may be illustrated several ways including in interaction diagrams by the details and sequence of messages in class diagrams with a uml note symbol stereotyped with method both styles will be used in subsequent chapters figure applies a uml note symbol to define the method body notice subtly that when we use a uml note to show a method we are mixing static and dynamic views in the same diagram the method body which defines dynamic behavior adds a dynamic element to the static class diagram note that this style is good for book or document diagrams and tool generated output but perhaps too fussy or stylized for sketching or tool input tools may provide a popup window to simply enter the code for a method operation issues in dcds the create operation the create message in an interaction diagram is normally interpreted as the invocation of the new operator and a constructor call in languages such as java and c in a dcd this create message will usually be mapped to a constructor definition using the rules of the languagesuch as the constructor name equal to the class name java c c figure shows an example with the superclassfoo constructor stereotyped constructor so that its category is clear operations to access attributes accessing operations retrieve or set attributes such as getprice and setprice these operations are often excluded or filtered from the class diagram because of the high noise to value ratio they generate for n attributes there may be uninteresting getter and setter operations most uml tools support filtering their display and it especially common to ignore them while wall sketching keywords a uml keyword is a textual adornment to categorize a model element for example the keyword to categorize that a classifier box is an interface is shocking surprise interface figure illustrates the interface keyword the actor keyword was used on p to replace the human stick figure actor icon with a class box to model computer system or robotic actors guideline when sketching umlwhen we want speed ease and creative flowmodelers often simplify keywords to something like interface or i most keywords are shown in guillemet but some are shown in curly braces such as abstract which is a constraint containing the abstract keyword in general when a uml element says it can have a property string such as a uml operation and uml association end havesome of the property string terms will be keywords and some may be user defined terms used in the curly brace format note that in uml guillemet were only used for stereotypes in uml guillemets are used for both keywords and stereotypes constraints p figure illustrates both the interface and abstract keywords a few sample predefined uml keywords include there are many keywords refer to the uml specification for details keyword meaning example usage actor classifier is an actor in class diagram above classifier name interface classifier is an interface in class diagram above classifier name abstract abstract element can t be instantiated in class diagrams after classifier name or operation name ordered a set of objects have some imposed order in class diagrams at an association end stereotypes profiles and tags as with keywords stereotypes are shown with guillemets symbols such as authorship but they are not keywords which can be confusing a stereotype represents a refinement of an existing modeling concept and is defined within a uml profileinformally a collection of related stereotypes tags and constraints to specialize the use of the uml for a specific domain or platform such as a uml profile for project management or for data modeling guillemets are special single character brackets most widely known by their use in french typography to indicate a quote typographically challenged tool vendors often substitute two angle brackets for the more elegant the uml predefines many stereotypes such as destroy used on sequence diagrams and also allows user defined ones thus stereotypes provide an extension mechanism in the uml see the uml specification for example figure shows a stereotype declaration and its use the stereotype declares a set of tags using the attribute syntax when an element such as the square class is marked with a stereotype all the tags apply to the element and can be assigned values figure stereotype declaration and use view full size image uml properties and property strings in the uml a property is a named value denoting a characteristic of an element a property has semantic impact some properties are predefined in the uml such as visibilitya property of an operation others can be user defined properties of elements may be presented in many ways but a textual approach is to use the uml property string value2 format such as abstract visibility public some properties are shown without a value such as abstract this usually implies a boolean property shorthand for abstract true note that abstract is both an example of a constraint and a property string constraint p generalization abstract classes abstract operations generalization in the uml is shown with a solid line and fat triangular arrow from the subclass to superclass see figure what does it mean in the uml to quote generalization a taxonomic relationship between a more general classifier and a more specific classifier each instance of the specific classifier is also an indirect instance of the general classifier thus the specific classifier indirectly has features of the more general classifier is this the same as oo programming language oopl inheritance it depends in a domain model conceptual perspective class diagram the answer is no rather it implies the superclass is a superset and the subclass is a subset on the other hand in a dcd software perspective class diagram it implies oopl inheritance from the superclass to subclass as shown in figure abstract classes and operations can be shown either with an abstract tag useful when sketching uml or by italicizing the name easy to support in a uml tool the opposite case final classes and operations that can t be overridden in subclasses are shown with the leaf tag dependency dependency lines may be used on any diagram but are especially common on class and package diagrams the uml includes a general dependency relationship that indicates that a client element of any kind including classes packages use cases and so on has knowledge of another supplier element and that a change in the supplier could affect the client that a broad relationship dependency is illustrated with a dashed arrow line from the client to supplier dependency can be viewed as another version of coupling a traditional term in software development when an element is coupled to or depends on another there are many kinds of dependency here are some common types in terms of objects and class diagrams having an attribute of the supplier type sending a message to a supplier the visibility to the supplier could be an attribute a parameter variable a local variable a global variable or class visibility invoking static or class methods receiving a parameter of the supplier type the supplier is a superclass or interface all of these could be shown with a dependency line in the uml but some of these types already have special lines that suggest the dependency for example there a special uml line to show the superclass one to show implementation of an interface and one for attributes the attribute as association line so for those cases it is not useful to use the dependency line for example in figure a sale has some kind of dependency on saleslineitems by virtue of the association line since there already an association line between these two elements adding a second dashed arrow dependency line is redundant therefore when to show a dependency guideline in class diagrams use the dependency line to depict global parameter variable local variable and static method when a call is made to a static method of another class dependency between objects for example the following java code shows an updatepricefor method in the sale class public class sale public void updatepricefor productdescription description money baseprice description getprice the updatepricefor method receives a productdescription parameter object and then sends it a getprice message therefore the sale object has parameter visibility to the productdescription and message sending coupling and thus a dependency on the productdescription if the latter class changed the sale class could be affected this dependency can be shown in a class diagram figure figure showing dependency another example the following java code shows a dox method in the foo class public class foo public void dox system runfinalization the dox method invokes a static method on the system class therefore the foo object has a static method dependency on the system class this dependency can be shown in a class diagram figure figure showing dependency dependency labels to show the type of dependency or to help a tool with code generation the dependency line can be labeled with keywords or stereotypes see figure see the uml specification for many predefined dependency labels figure optional dependency labels in the uml view full size image interfaces the uml provides several ways to show interface implementation providing an interface to clients and interface dependency a required interface in the uml interface implementation is formally called interface realization see figure figure different notations to show interfaces in uml view full size image the socket notation is new to uml it useful to indicate class x requires uses interface y without drawing a line pointing to interface y composition over aggregation aggregation is a vague kind of association in the uml that loosely suggests whole part relationships as do many ordinary associations it has no meaningful distinct semantics in the uml versus a plain association but the term is defined in the uml why to quote rumbaugh one of the original and key uml creators in spite of the few semantics attached to aggregation everybody thinks it is necessary for different reasons think of it as a modeling placebo guideline therefore following the advice of uml creators don t bother to use aggregation in the uml rather use composition when appropriate composition also known as composite aggregation is a strong kind of whole part aggregation and is useful to show in some models a composition relationship implies that an instance of the part such as a square belongs to only one composite instance such as one board at a time the part must always belong to a composite no free floating fingers and the composite is responsible for the creation and deletion of its partseither by itself creating deleting the parts or by collaborating with other objects related to this constraint is that if the composite is destroyed its parts must either be destroyed or attached to another compositeno free floating fingers allowed for example if a physical paper monopoly game board is destroyed we think of the squares as being destroyed as well a conceptual perspective likewise if a software board object is destroyed its software square objects are destroyed in a dcd software perspective the uml notation for composition is a filled diamond on an association line at the composite end of the line see figure figure composition in the uml view full size image guideline the association name in composition is always implicitly some variation of has part therefore don t bother to explicitly name the association constraints constraints may be used on most uml diagrams but are especially common on class diagrams a uml constraint is a restriction or condition on a uml element it is visualized in text between braces for example size the text may be natural language or anything else such as uml formal specification language the object constraint language ocl see figure figure constraints qualified association a qualified association has a qualifier that is used to select an object or objects from a larger set of related objects based upon the qualifier key informally in a software perspective it suggests looking things up by a key such as objects in a hashmap for example if a productcatalog contains many productdescriptions and each one can be selected by an itemid then the uml notation in figure can be used to depict this figure qualified associations in the uml there one subtle point about qualified associations the change in multiplicity for example as contrasted in figure a vs b qualification reduces the multiplicity at the target end of the association usually down from many to one because it implies the selection of usually one instance from a larger set association class an association class allows you treat an association itself as a class and model it with attributes operations and other features for example if a company employs many persons modeled with an employs association you can model the association itself as the employment class with attributes such as startdate in the uml it is illustrated with a dashed line from the association to the association class see figure figure association classes in the uml singleton classes in the world of oo design patterns there is one that is especially common called the singleton pattern it is explained later but an implication of the pattern is that there is only one instance of a class instantiatednever two in other words it is a singleton instance in a uml diagram such a class can be marked with a in the upper right corner of the name compartment see figure figure showing a singleton view full size image singleton p template classes and interfaces many languages java c support templatized types also known with shades of variant meanings as templates parameterized types and generics they are most commonly used for the element type of collection classes such as the elements of lists and maps for example in java suppose that a board software object holds a list an interface for a kind of collection of many squares and the concrete class that implements the list interface is an arraylist motivations for template classes include increased type safety and performance public class board private list square squares new arraylist square notice that the list interface and the arraylist class that implements the list interface are parameterized with the element type square how to show template classes and interfaces in the uml figure illustrates figure templates in the uml view full size image user defined compartments in addition to common predefined compartments class compartments such as name attributes and operations user defined compartments can be added to a class box figure shows an example figure compartments active class an active object runs on and controls its own thread of execution not surprisingly the class of an active object is an active class in the uml it may be shown with double vertical lines on the left and right sides of the class box figure figure active classes in the uml active object p what the relationship between interaction and class diagrams when we draw interaction diagrams a set of classes and their methods emerge from the creative design process of dynamic object modeling for example if we started with the trivial for explanation makepayment sequence diagram in figure we see that a register and sale class definition in a class diagram can be obviously derived figure the influence of interaction diagrams on class diagrams view full size image thus from interaction diagrams the definitions of class diagrams can be generated this suggests a linear ordering of drawing interaction diagrams before class diagrams but in practice especially when following the agile modeling practice of models in parallel these complementary dynamic and static views are drawn concurrently for example minutes on one then on the other guideline a good uml tool should automatically support changes in one diagram being reflected in the other if wall sketching use one wall for interaction diagrams and an adjacent wall for class diagrams chapter grasp designing objects with responsibilities understanding responsibilities is key to good object oriented design martin fowler this chapter and the next contribute significantly to an understanding of core oo design ood ood is sometimes taught as some variation of the following after identifying your requirements and creating a domain model then add methods to the appropriate classes and define the messaging between the objects to fulfill the requirements ouch such vague advice doesn t help us because deep principles and issues are involved deciding what methods belong where and how objects should interact carries consequences and should be undertaken seriously mastering oodand this is its intricate charminvolves a large set of soft principles with many degrees of freedom it isn t magicthe patterns can be named important explained and applied examples help practice helps and this small step helps after studying these case studies try recreating from memory the monopoly solution on walls with partners and apply the principles such as information expert view full size image uml versus design principles since the uml is simply a standard visual modeling language knowing its details doesn t teach you how to think in objectsthat a theme of this book the uml is sometimes described as a design tool but that not quite right uml and silver bullet thinking p the critical design tool for software development is a mind well educated in design principles it is not the uml or any other technology object design example inputs activities and outputs this section summarizes a big picture example of design in an iterative method what been done prior activities e g workshop and artifacts how do things relate influence of prior artifacts e g use cases on oo design how much design modeling to do and how what the output especially i d like you to understand how the analysis artifacts relate to object design what are inputs to object design let start with process inputs assume we are developers working on the pos nextgen project and the following scenario is true the first two day requirements workshop is finished the chief architect and business agree to implement and test some scenarios of process sale in the first three week timeboxed iteration three of the twenty use casesthose that are the most architecturally significant and of high business valuehave been analyzed in detail including of course the process sale use case the up recommends as typical with iterative methods analyzing only of the requirements in detail before starting to program other artifacts have been started supplementary specification glossary and domain model programming experiments have resolved the show stopper technical questions such as whether a java swing ui will work on a touch screen the chief architect has drawn some ideas for the large scale logical architecture using uml package diagrams this is part of the up design model what are the artifact inputs and their relationship to object design they are summarized in figure and in the following table other artifact inputs could include design documents for an existing system being modified it also useful to reverse engineer existing code into uml package diagrams to see the large scale logical structure and some class and sequence diagrams figure artifact relationships emphasizing influence on oo design view full size image the use case text defines the visible behavior that the software objects must ultimately supportobjects are designed to realize implement the use cases in the up this oo design is called not surprisingly the use case realization the supplementary specification defines the non functional goals such as internalization our objects must satisfy the system sequence diagrams identify the system operation messages which are the starting messages on our interaction diagrams of collaborating objects the glossary clarifies details of parameters or data coming in from the ui layer data being passed to the database and detailed item specific logic or validation requirements such as the legal formats and validation for product upcs universal product codes the operation contracts may complement the use case text to clarify what the software objects must achieve in a system operation the post conditions define detailed achievements the domain model suggests some names and attributes of software domain objects in the domain layer of the software architecture not all of these artifacts are necessary recall that in the up all elements are optional possibly created to reduce some risk what are activities of object design we re ready to take off our analyst hats and put on our designer modeler hats given one or more of these inputs developers start immediately coding ideally with test first development start some uml modeling for the object design or start with another modeling technique such as crc cards all of these approaches are skillful depending on context and person test first p in the uml case the real point is not the uml but visual modelingusing a language that allows us to explore more visually than we can with just raw text in this case for example we draw both interaction diagrams and complementary class diagrams dynamic and static modeling during one modeling day and most importantly during the drawing and coding activity we apply various oo design principles such as grasp and the gang of four gof design patterns the overall approach to doing the oo design modeling will be based on the metaphor of responsibility driven design rdd thinking about how to assign responsibilities to collaborating objects grasp p gof p rdd p on the modeling day perhaps the team works in small groups for hours either at the walls or with software modeling tools doing different kinds of modeling for the difficult creative parts of the design this could include ui oo and database modeling with uml drawings prototyping tools sketches and so forth during uml drawing we adopt the realistic attitude also promoted in agile modeling that we are drawing the models primarily to understand and communicate not to document of course we expect some of the uml diagrams to be useful input to the definition or automated code generation with a uml tool of the code on tuesdaystill early in the three week timeboxed iterationthe team stops modeling and puts on programmer hats to avoid a waterfall mentality of over modeling before programming what are the outputs figure illustrates some inputs and their relationship to the output of a uml interaction and class diagram notice that we may refer to these analysis inputs during design for example re reading the use case text or operation contracts scanning the domain model and reviewing the supplementary specification what been created during the modeling day for example specifically for object design uml interaction class and package diagrams for the difficult parts of the design that we wished to explore before coding ui sketches and prototypes database models with uml data modeling profile notation p report sketches and prototypes responsibilities and responsibility driven design a popular way of thinking about the design of software objects and also larger scale components is in terms of responsibilities roles and collaborations this is part of a larger approach called responsibility driven design or rdd thinking in terms of responsibilities can apply at any scale of softwarefrom a small object to a system of systems in rdd we think of software objects as having responsibilitiesan abstraction of what they do the uml defines a responsibility as a contract or obligation of a classifier responsibilities are related to the obligations or behavior of an object in terms of its role basically these responsibilities are of the following two types doing and knowing doing responsibilities of an object include doing something itself such as creating an object or doing a calculation initiating action in other objects controlling and coordinating activities in other objects knowing responsibilities of an object include knowing about private encapsulated data knowing about related objects knowing about things it can derive or calculate responsibilities are assigned to classes of objects during object design for example i may declare that a sale is responsible for creating saleslineitems a doing or a sale is responsible for knowing its total a knowing guideline for software domain objects the domain model because of the attributes and associations it illustrates often inspires the relevant responsibilities related to knowing for example if the domain model sale class has a time attribute it natural by the goal of low representational gap that a software sale class knows its time low representational gap p the translation of responsibilities into classes and methods is influenced by the granularity of the responsibility big responsibilities take hundreds of classes and methods little responsibilities might take one method for example the responsibility to provide access to relational databases may involve two hundred classes and thousands of methods packaged in a subsystem by contrast the responsibility to create a sale may involve only one method in one class a responsibility is not the same thing as a methodit an abstractionbut methods fulfill responsibilities rdd also includes the idea of collaboration responsibilities are implemented by means of methods that either act alone or collaborate with other methods and objects for example the sale class might define one or more methods to know its total say a method named gettotal to fulfill that responsibility the sale may collaborate with other objects such as sending a getsubtotal message to each saleslineitem object asking for its subtotal key point grasp names and describes some basic principles to assign responsibilities so it useful to knowto support rdd grasp a methodical approach to basic oo design grasp a learning aid for oo design with responsibilities it is possible to name and explain the detailed principles and reasoning required to grasp basic object design assigning responsibilities to objects the grasp principles or patterns are a learning aid to help you understand essential object design and apply design reasoning in a methodical rational explainable way this approach to understanding and using design principles is based on patterns of assigning responsibilities this chapterand several othersuses grasp as a tool to help master the basics of ood and understanding responsibility assignment in object design so grasp is relevant but on the other hand it just a learning aid to structure and name the principlesonce you grasp the fundamentals the specific grasp terms information expert creator aren t important what the connection between responsibilities grasp and uml diagrams you can think about assigning responsibilities to objects while coding or while modeling within the uml drawing interaction diagrams becomes the occasion for considering these responsibilities realized as methods figure indicates that sale objects have been given a responsibility to create payments which is concretely invoked with a makepayment message and handled with a corresponding makepayment method furthermore the fulfillment of this responsibility requires collaboration to create the payment object and invoke its constructor figure responsibilities and methods are related therefore when we draw a uml interaction diagram we are deciding on responsibility assignments this chapter emphasizes fundamental principlesexpressed in graspto guide choices about assigning responsibilities thus you can apply the grasp principles while drawing uml interaction diagrams and also while coding what are patterns experienced oo developers and other software developers build up a repertoire of both general principles and idiomatic solutions that guide them in the creation of software these principles and idioms if codified in a structured format describing the problem and solution and named may be called patterns for example here is a sample pattern pattern name information expert problem what is a basic principle by which to assign responsibilities to objects solution assign a responsibility to the class that has the information needed to fulfill it in oo design a pattern is a named description of a problem and solution that can be applied to new contexts ideally a pattern advises us on how to apply its solution in varying circumstances and considers the forces and trade offs many patterns given a specific category of problem guide the assignment of responsibilities to objects patterns have namesimportant software development is a young field young fields lack well established names for their principlesand that makes communication and education difficult patterns have names such as information expert and abstract factory naming a pattern design idea or principle has the following advantages it supports chunking and incorporating that concept into our understanding and memory it facilitates communication when a pattern is named and widely publishedand we all agree to use the namewe can discuss a complex design idea in shorter sentences or shorter diagrams a virtue of abstraction consider the following discussion between two software developers using a vocabulary of pattern names jill hey jack for the persistence subsystem let expose the services with a facade we ll use an abstract factory for mappers and proxies for lazy materialization jack what the hell did you just say jill here read this new pattern is an oxymoron new pattern should be considered an oxymoron if it describes a new idea the very term pattern suggests a long repeating thing the point of design patterns is not to express new design ideas quite the oppositegreat patterns attempt to codify existing tried and true knowledge idioms and principles the more honed old and widely used the better consequently the grasp patterns don t state new ideas they name and codify widely used basic principles to an oo design expert the grasp patternsby idea if not by namewill appear fundamental and familiar that the point the gang of four design patterns book the idea of named patterns in software comes from kent beck also of extreme programming fame in the mid however was a major milestone in the history of patterns oo design and software design books the massive selling and hugely influential book design patterns was published authored by gamma helm johnson and vlissides the book considered the bible of design pattern books describes patterns for oo design with names such as strategy and adapter these patterns authored by four people are therefore called the gang of four or gof design patterns the notion of patterns originated with the building architectural patterns of christopher alexander patterns for software originated in the with kent beck who became aware of alexander pattern work in architecture and then were developed by beck with ward cunningham at tektronix publishers list the publication date as but it was released october also a subtle joke related to mid chinese politics following mao death however design patterns isn t an introductory book it assumes significant prior oo design and programming knowledge and most code examples are in c laterintermediatechapters of this book especially chapter p chapter 35 p and chapter p introduce many of the most frequently used gof design patterns and apply them to our case studies also see contents by major topics on page ix it is a key goal of this text to learn both grasp and essential gof patterns is grasp a set of patterns or principles grasp defines nine basic oo design principles or basic building blocks in design some have asked doesn t grasp describe principles rather than patterns one answer is in the words of the gang of four authors from the preface of their influential design patterns book one person pattern is another person primitive building block rather than focusing on labels this text focuses on the pragmatic value of using the pattern style as an excellent learning aid for naming presenting and remembering basic classic design ideas where are we now so far this chapter has summarized the background for oo design the iterative process backgroundprior artifacts how do they relate to oo design models how much time should we spend design modeling rdd as a metaphor for object designa community of collaborating responsible objects patterns as a way to name and explain oo design ideasgrasp for basic patterns of assigning responsibilities and gof for more advanced design ideas patterns can be applied during modeling and during coding uml for oo design visual modeling during which time both grasp and gof patterns can be applied with that understood it time to focus on some details of object design a short example of object design with grasp following sections explore grasp in more detail but let start with a shorter example to see the big ideas applied to the monopoly case study there are nine grasp patterns this example applies the following subset creator information expert low coupling controller high cohesion all the grasp patterns are summarized on the inside front cover of this book creator problem who creates the square object one of the first problems you have to consider in oo design is who creates object x this is a doing responsibility for example in the monopoly case study who creates a square software object now any object can create a square but what would many oo developers choose and why how about having a dog object i e some arbitrary class be the creator no we can feel it in our bones why becauseand this is the critical pointit doesn t appeal to our mental model of the domain dog doesn t support low representational gap lrg between how we think of the domain and a straightforward correspondence with software objects i ve done this problem with literally thousands of developers and virtually every one from india to the usa will say make the board object create the squares interesting it reflects an intuition that oo software developers often exceptions are explored later want containers to create the things contained such as boards creating squares by the way why we are defining software classes with the names square and board rather than the names and answer by lrg this connects the up domain model to the up design model or our mental model of the domain to its realization in the domain layer of the software architecture with that as background here the definition of the creator pattern alternate creation patterns such as concrete factory and abstract factory are discussed later name creator problem who creates an a solution this can be viewed as advice assign class b the responsibility to create an instance of class a if one of these is true the more the better b contains or compositely aggregates a b records a b closely uses a b has the initializing data for a notice this has to do with responsibility assignment let see how to apply creator first a subtle but important point in applying creator and other grasp patterns b and a refer to software objects not domain model objects we first try to apply creator by looking for existing software objects that satisfy the role of b but what if we are just starting the oo design and we have not yet defined any software classes in this case by lrg look to the domain model for inspiration thus for the square creation problem since no software classes are yet defined we look at the domain model in figure and see that a board contains squares that a conceptual perspective not a software one but of course we can mirror it in the design model so that a software board object contains software square objects and then consistent with lrg and the creator advice the board will create squares also squares will always be a part of one board and board manages their creation and destruction thus they are in a composite aggregation association with the board figure monopoly iteration domain model recall that an agile modeling practice is to create parallel complementary dynamic and static object models therefore i ve drawn both a partial sequence diagram and class diagram to reflect this design decision in which i ve applied a grasp pattern while drawing uml diagrams see figure and figure notice in figure that when the board is created it creates a square for brevity in this example i ll ignore the side issue of drawing the loop to create all squares figure applying the creator pattern in a dynamic model figure in a dcd of the design model board has a composite aggregation association with squares we are applying creator in a static model information expert problem who knows about a square object given a key the pattern information expert often abbreviated to expert is one of the most basic responsibility assignment principles in object design suppose objects need to be able to reference a particular square given its name who should be responsible for knowing a square given a key of course this is a knowing responsibility but expert also applies to doing as with creator any object can be responsible but what would many oo developers choose and why as with the creator problem most oo developers choose the board object it seems sort of trivially obvious to assign this responsibility to a board but it is instructive to deconstruct why and to learn to apply this principle in more subtle cases later examples will get more subtle information expert explains why the board is chosen name information expert problem what is a basic principle by which to assign responsibilities to objects solution advice assign a responsibility to the class that has the information needed to fulfill it a responsibility needs information for its fulfillmentinformation about other objects an object own state the world around an object information the object can derive and so forth in this case to be able to retrieve and present any one squaregiven its namesome object must know have the information about all the squares we previously decided as shown in figure that a software board will aggregate all the square objects therefore board has the information necessary to fulfill this responsibility figure illustrates applying expert in the context of drawing figure applying expert the next grasp principle low coupling explains why expert is a useful core principle of oo design low coupling question why board over dog expert guides us to assign the responsibility to know a particular square given a unique name to the board object because the board knows about all the squares it has the informationit is the information expert but why does expert give this advice the answer is found in the principle of low coupling briefly and informally coupling is a measure of how strongly one element is connected to has knowledge of or depends on other elements if there is coupling or dependency then when the depended upon element changes the dependant may be affected for example a subclass is strongly coupled to a superclass an object a that calls on the operations of object b has coupling to b services the low coupling principle applies to many dimensions of software development it really one of the cardinal goals in building software in terms of object design and responsibilities we can describe the advice as follows name low coupling problem how to reduce the impact of change solution advice assign responsibilities so that unnecessary coupling remains low use this principle to evaluate alternatives we use low coupling to evaluate existing designs or to evaluate the choice between new alternativesall other things being equal we should prefer a design whose coupling is lower than the alternatives for example as we ve decided in figure a board object contains many squares why not assign getsquare to dog i e some arbitrary other class consider the impact in terms of low coupling if a dog has getsquare as shown in the uml sketch in figure it must collaborate with the board to get the collection of all the squares in the board they are probably stored in a map collection object which allows retrieval by a key then the dog can access and return one particular square by the key name figure evaluating the effect of coupling on this design view full size image but let evaluate the total coupling with this poor dog design versus our original design where board does getsquare in the dog case the dog and the board must both know about square objects two objects have coupling to square in the board case only board must know about square objects one object has coupling to square thus the overall coupling is lower with the board design and all other things being equal it is better than the dog design in terms of supporting the goal of low coupling at a higher goal level why is low coupling desirable in other words why would we want to reduce the impact of change because low coupling tends to reduce the time effort and defects in modifying software that a short answer but one with big implications in building and maintaining software applying uml please note a few uml elements in the sequence diagram in figure the return value variable sqs from the getallsquares message is also used to name the lifeline object in sqs map square e g a collection of type map that holds square objects referencing a return value variable in a lifeline box to send it messages is common the variable in the starting getsquare message and the variable in the later get message refer to the same object the message expression get name square indicates that the type of is a reference to a square instance controller a simple layered architecture has a ui layer and a domain layer among others actors such as the human observer in the monopoly game generate ui events such as clicking on a button with a mouse to play the game the ui software objects in java for example a jframe window and a jbutton button must then react to the mouse click event and ultimately cause the game to play from the model view separation principle we know the ui objects should not contain application or business logic such as calculating a player move therefore once the ui objects pick up the mouse event they need to delegate forward the task to another object the request to domain objects in the domain layer model view separation p the controller pattern answers this simple question what first object after or beyond the ui layer should receive the message from the ui layer to tie this back to system sequence diagrams as a review of figure shows the key system operation is playgame somehow the human observer generates a playgame request probably by clicking on a gui button labeled play game and the system responds figure ssd for the monopoly game note the playgame operation figure illustrates a finer grained look at what going on assuming a java swing gui jframe window and jbutton button clicking on a jbutton sends an actionperformed message to some object often to the jframe window itself as we see in figure thenand this is the key pointthe jframe window must adapt that actionperformed message into something more semantically meaningful such as a playgame message to correspond to the ssd analysis and delegate the playgame message to a domain object in the domain layer similar objects messages and collaboration patterns apply to net python etc figure who is the controller for the playgame system operation view full size image thus controller deals with a basic question in oo design how to connect the ui layer to the application logic layer should the board be the first object to receive the playgame message from the ui layer or something else in some ooa d methods the name controller was given to the application logic object that received and controlled coordinated handling the request the controller pattern offers the following advice name controller problem what first object beyond the ui layer receives and coordinates controls a system operation solution advice assign the responsibility to an object representing one of these choices represents the overall system a root object a device that the software is running within or a major subsystem these are all variations of a facade controller represents a use case scenario within which the system operation occurs a use case or session controller let consider these options option represents the overall system or a root object such as an object called monopolygame option represents a device that the software is running withinthis option appertains to specialized hardware devices such as a phone or a bank cash machine e g software class phone or bankcashmachine it doesn t apply in this case option represents the use case or session the use case that the playgame system operation occurs within is called play monopoly game thus a software class such as playmonopolygamehandler appending handler or session is an idiom in oo design when this version is used option class monopolygame is reasonable if there are only a few system operations more on the trade offs when we discuss high cohesion therefore figure illustrates the design decision based on controller figure applying the controller patternusing monopolygame connecting the ui layer to the domain layer of software objects high cohesion based on the controller decision we are now at the design point shown in the sequence diagram to the right the detailed design discussion of what comes nextconsistently and methodically applying graspis explored in a following chapter but right now we have two contrasting design approaches worth considering illustrated in figure figure contrasting the level of cohesion in different designs view full size image notice in the left hand version that the monopolygame object itself does all the work and in the right hand version it delegates and coordinates the work for the playgame request in software design a basic quality known as cohesion informally measures how functionally related the operations of a software element are and also measures how much work a software element is doing as a simple contrasting example an object big with methods and source lines of code sloc is doing a lot more than an object small with methods and source lines and if the methods of big are covering many different areas of responsibility such as database access and random number generation then big has less focus or functional cohesion than small in summary both the amount of code and the relatedness of the code are an indicator of an object cohesion to be clear bad cohesion low cohesion doesn t just imply an object does work only by itself indeed a low cohesion object with sloc probably collaborates with many other objects now here a key point all that interaction tends to also create bad high coupling bad cohesion and bad coupling often go hand in hand in terms of the contrasting designs in figure the left hand version of monopolygame has worse cohesion than the right hand version since the left hand version is making the monopolygame object itself do all the work rather than delegating and distributing work among objects this leads to the principle of high cohesion which is used to evaluate different design choices all other things being equal prefer a design with higher cohesion name high cohesion problem how to keep objects focused understandable and manageable and as a side effect support low coupling solution advice assign responsibilities so that cohesion remains high use this to evaluate alternatives we can say that the right hand design better supports high cohesion than the left hand version applying grasp to object design grasp stands for general responsibility assignment software patterns the name was chosen to suggest the importance of grasping these principles to successfully design object oriented software technically one should write gras patterns rather than grasp patterns but the latter sounds better all nine grasp patterns are summarized on the inside front cover of this book understanding and being able to apply the ideas behind graspwhile coding or while drawing interaction and class diagramsenables developers new to object technology needs to master these basic principles as quickly as possible they form a foundation for designing oo systems there are nine grasp patterns creator controller pure fabrication information expert high cohesion indirection low coupling polymorphism protected variations the remainder of this chapter reexamines the first five in more detail the remaining four are introduced in chapter starting on p creator problem who should be responsible for creating a new instance of some class the creation of objects is one of the most common activities in an object oriented system consequently it is useful to have a general principle for the assignment of creation responsibilities assigned well the design can support low coupling increased clarity encapsulation and reusability solution assign class b the responsibility to create an instance of class a if one of these is true the more the better other creation patterns such as concrete factory and abstract factory are explored later b contains or compositely aggregates a b records a b closely uses a b has the initializing data for a that will be passed to a when it is created thus b is an expert with respect to creating a b is a creator of a objects if more than one option applies usually prefer a class b which aggregates or contains class a example in the nextgen pos application who should be responsible for creating a saleslineitem instance by creator we should look for a class that aggregates contains and so on saleslineitem instances consider the partial domain model in figure figure partial domain model since a sale contains in fact aggregates many saleslineitem objects the creator pattern suggests that sale is a good candidate to have the responsibility of creating saleslineitem instances this leads to the design of object interactions shown in figure figure creating a saleslineitem view full size image this assignment of responsibilities requires that a makelineitem method be defined in sale once again the context in which we considered and decided on these responsibilities was while drawing an interaction diagram the method section of a class diagram can then summarize the responsibility assignment results concretely realized as methods discussion creator guides the assigning of responsibilities related to the creation of objects a very common task the basic intent of the creator pattern is to find a creator that needs to be connected to the created object in any event choosing it as the creator supports low coupling composite aggregates part container contains content and recorder records recorded are all very common relationships between classes in a class diagram creator suggests that the enclosing container or recorder class is a good candidate for the responsibility of creating the thing contained or recorded of course this is only a guideline note that we turned to the concept of composition in considering the creator pattern a composite object is an excellent candidate to make its parts composite aggregation p sometimes you identify a creator by looking for the class that has the initializing data that will be passed in during creation this is actually an example of the expert pattern initializing data is passed in during creation via some kind of initialization method such as a java constructor that has parameters for example assume that a payment instance when created needs to be initialized with the sale total since sale knows the total sale is a candidate creator of the payment contraindications often creation requires significant complexity such as using recycled instances for performance conditionally creating an instance from one of a family of similar classes based upon some external property value and so forth in these cases it is advisable to delegate creation to a helper class called a concrete factory or an abstract factory rather than use the class suggested by creator factories are discussed starting on p benefits low coupling is supported which implies lower maintenance dependencies and higher opportunities for reuse coupling is probably not increased because the created class is likely already visible to the creator class due to the existing associations that motivated its choice as creator related patterns or principles low coupling concrete factory and abstract factory whole part describes a pattern to define aggregate objects that support encapsulation of components information expert or expert problem what is a general principle of assigning responsibilities to objects a design model may define hundreds or thousands of software classes and an application may require hundreds or thousands of responsibilities to be fulfilled during object design when the interactions between objects are defined we make choices about the assignment of responsibilities to software classes if we ve chosen well systems tend to be easier to understand maintain and extend and our choices afford more opportunity to reuse components in future applications solution assign a responsibility to the information expertthe class that has the information necessary to fulfill the responsibility example in the nextgen pos application some class needs to know the grand total of a sale by this advice the statement is who should be responsible for knowing the grand total of a sale by information expert we should look for that class of objects that has the information needed to determine the total now we come to a key question do we look in the domain model or the design model to analyze the classes that have the information needed the domain model illustrates conceptual classes of the real world domain the design model illustrates software classes answer if there are relevant classes in the design model look there first otherwise look in the domain model and attempt to use or expand its representations to inspire the creation of corresponding design classes for example assume we are just starting design work and there is no or a minimal design model therefore we look to the domain model for information experts perhaps the real world sale is one then we add a software class to the design model similarly called sale and give it the responsibility of knowing its total expressed with the method named gettotal this approach supports low representational gap in which the software design of objects appeals to our concepts of how the real domain is organized to examine this case in detail consider the partial domain model in figure figure associations of sale what information do we need to determine the grand total we need to know about all the saleslineitem instances of a sale and the sum of their subtotals a sale instance contains these therefore by the guideline of information expert sale is a suitable class of object for this responsibility it is an information expert for the work as mentioned it is in the context of the creation of interaction diagrams that these questions of responsibility often arise imagine we are starting to work through the drawing of diagrams in order to assign responsibilities to objects a partial interaction diagram and class diagram in figure illustrate some decisions figure partial interaction and class diagrams we are not done yet what information do we need to determine the line item subtotal saleslineitem quantity and productdescription price the saleslineitem knows its quantity and its associated productdescription therefore by expert saleslineitem should determine the subtotal it is the information expert each of the saleslineitems and sum the results this design is shown in figure figure calculating the sale total view full size image to fulfill the responsibility of knowing and answering its subtotal a saleslineitem has to know the product price the productdescription is an information expert on answering its price therefore saleslineitem sends it a message asking for the product price the design is shown in figure figure calculating the sale total view full size image in conclusion to fulfill the responsibility of knowing and answering the sale total we assigned design class responsibility sale knows sale total saleslineitem knows line item subtotal productdescription knows product price we considered and decided on these responsibilities in the context of drawing an interaction diagram we could then summarize the methods in the method section of a class diagram the principle by which we assigned each responsibility was information expertplacing it with the object that had the information needed to fulfill it discussion information expert is frequently used in the assignment of responsibilities it is a basic guiding principle used continuously in object design expert is not meant to be an obscure or fancy idea it expresses the common intuition that objects do things related to the information they have notice that the fulfillment of a responsibility often requires information that is spread across different classes of objects this implies that many partial information experts will collaborate in the task for example the sales total problem ultimately required the collaboration of three classes of objects whenever information is spread across different objects they will need to interact via messages to share the work expert usually leads to designs where a software object does those operations that are normally done to the inanimate real world thing it represents peter coad calls this the do it myself strategy for example in the real world without the use of electro mechanical aids a sale does not tell you its total it is an inanimate thing someone calculates the total of the sale but in object oriented software land all software objects are alive or animated and they can take on responsibilities and do things fundamentally they do things related to the information they know i call this the animation principle in object design it is like being in a cartoon where everything is alive the information expert patternlike many things in object technologyhas a real world analogy we commonly give responsibility to individuals who have the information necessary to fulfill a task for example in a business who should be responsible for creating a profit and loss statement the person who has access to all the information necessary to create itperhaps the chief financial officer and just as software objects collaborate because the information is spread around so it is with people the company chief financial officer may ask accountants to generate reports on credits and debits contraindications in some situations a solution suggested by expert is undesirable usually because of problems in coupling and cohesion these principles are discussed later in this chapter for example who should be responsible for saving a sale in a database certainly much of the information to be saved is in the sale object and thus expert could argue that the responsibility lies in the sale class and by logical extension of this decision each class would have its own services to save itself in a database but acting on that reasoning leads to problems in cohesion coupling and duplication for example the sale class must now contain logic related to database handling such as that related to sql and jdbc java database connectivity the class no longer lower its cohesion the class must be coupled to the technical database services of another subsystem such as jdbc services rather than just being coupled to other objects in the domain layer of software objects so its coupling increases and it is likely that similar database logic would be duplicated in many persistent classes all these problems indicate violation of a basic architectural principle design for a separation of major system concerns keep application logic in one place such as the domain software objects keep database logic in another place such as a separate persistence services subsystem and so forth rather than intermingling different system concerns in the same component see chapter for a discussion of separation of concerns supporting a separation of major concerns improves coupling and cohesion in a design thus even though by expert we could find some justification for putting the responsibility for database services in the sale class for other reasons usually cohesion and coupling we d end up with a poor design benefits information encapsulation is maintained since objects use their own information to fulfill tasks this usually supports low coupling which leads to more robust and maintainable systems low coupling is also a grasp pattern that is discussed in a following section behavior is distributed across the classes that have the required information thus encouraging more cohesive lightweight class definitions that are easier to understand and maintain high cohesion is usually supported another pattern discussed later related patterns or principles low coupling high cohesion also known as similar to place responsibilities with data that which knows does do it myself put services with the attributes they work on low coupling problem how to support low dependency low change impact and increased reuse coupling is a measure of how strongly one element is connected to has knowledge of or relies on other elements an element with low or weak coupling is not dependent on too many other elements too many is context dependent but we examine it anyway these elements include classes subsystems systems and so on a class with high or strong coupling relies on many other classes such classes may be undesirable some suffer from the following problems forced local changes because of changes in related classes harder to understand in isolation harder to reuse because its use requires the additional presence of the classes on which it is dependent solution assign a responsibility so that coupling remains low use this principle to evaluate alternatives example consider the following partial class diagram from a nextgen case study assume we need to create a payment instance and associate it with the sale what class should be responsible for this since a register records a payment in the real world domain the creator pattern suggests register as a candidate for creating the payment the register instance could then send an addpayment message to the sale passing along the new payment as a parameter a possible partial interaction diagram reflecting this is shown in figure figure register creates payment this assignment of responsibilities couples the register class to knowledge of the payment class applying uml note that the payment instance is explicitly named p so that in message it can be referenced as a parameter figure shows an alternative solution to creating the payment and associating it with the sale figure sale creates payment which design based on assignment of responsibilities supports low coupling in both cases we assume the sale must eventually be coupled to knowledge of a payment design in which the register creates the payment adds coupling of register to payment design in which the sale does the creation of a payment does not increase the coupling purely from the point of view of coupling prefer design because it maintains overall lower coupling this example illustrates how two patternslow coupling and creatormay suggest different solutions discussion low coupling is a principle to keep in mind during all design decisions it is an underlying goal to continually consider it is an evaluative principle that you apply while evaluating all design decisions in object oriented languages such as c java and c common forms of coupling from typex to typey include the following typex has an attribute data member or instance variable that refers to a typey instance or typey itself a typex object calls on services of a typey object typex has a method that references an instance of typey or typey itself by any means these typically include a parameter or local variable of type typey or the object returned from a message being an instance of typey typex is a direct or indirect subclass of typey typey is an interface and typex implements that interface low coupling encourages you to assign a responsibility so that its placement does not increase the coupling to a level that leads to the negative results that high coupling can produce low coupling supports the design of classes that are more independent which reduces the impact of change it can t be considered in isolation from other patterns such as expert and high cohesion but rather needs to be included as one of several design principles that influence a choice in assigning a responsibility a subclass is strongly coupled to its superclass consider carefully any decision to derive from a superclass since it is such a strong form of coupling for example suppose that objects must be stored persistently in a relational or object database in this case you could follow the relatively common design practice of creating an abstract superclass called persistentobject from which other classes derive the disadvantage of this subclassing is that it highly couples domain objects to a particular technical service and mixes different architectural concerns whereas the advantage is automatic inheritance of persistence behavior you cannot obtain an absolute measure of when coupling is too high what is important is that you can gauge the current degree of coupling and assess whether increasing it will lead to problems in general classes that are inherently generic in nature and with a high probability for reuse should have especially low coupling the extreme case of low coupling is no coupling between classes this case offends against a central metaphor of object technology a system of connected objects that communicate via messages low coupling taken to excess yields a poor designone with a few incohesive bloated and complex active objects that do all the work and with many passive zero coupled objects that act as simple data repositories some moderate degree of coupling between classes is normal and necessary for creating an object oriented system in which tasks are fulfilled by a collaboration between connected objects contraindications high coupling to stable elements and to pervasive elements is seldom a problem for example a application can safely couple itself to the java libraries java util and so on because they are stable and widespread pick your battles it is not high coupling per se that is the problem it is high coupling to elements that are unstable in some dimension such as their interface implementation or mere presence this is an important point as designers we can add flexibility encapsulate details and implementations and in general design for lower coupling in many areas of the system but if we put effort into future proofing or lowering the coupling when we have no realistic motivation this is not time well spent you must pick your battles in lowering coupling and encapsulating things focus on the points of realistic high instability or evolution for example in the nextgen project we know that different third party tax calculators with unique interfaces need to be connected to the system therefore designing for low coupling at this variation point is practical benefits not affected by changes in other components simple to understand in isolation convenient to reuse background coupling and cohesion are truly fundamental principles in design and should be appreciated and applied as such by all software developers larry constantine also a founder of structured design in the and a current advocate of more attention to usability engineering was primarily responsible in the for identifying and communicating coupling and cohesion as critical principles related patterns protected variation controller problem what first object beyond the ui layer receives and coordinates controls a system operation system operations were first explored during the analysis of ssd these are the major input events upon our system for example when a cashier using a pos terminal presses the end sale button he is generating a system event indicating the sale has ended similarly when a writer using a word processor presses the spell check button he is generating a system event indicating perform a spell check a controller is the first object beyond the ui layer that is responsible for receiving or handling a system operation message solution assign the responsibility to a class representing one of the following choices represents the overall system a root object a device that the software is running within or a major subsystemthese are all variations of a facade controller represents a use case scenario within which the system event occurs often named usecasename handler usecasename coordinator or usecasename session use case or session controller use the same controller class for all system events in the same use case scenario informally a session is an instance of a conversation with an actor sessions can be of any length but are often organized in terms of use cases use case sessions corollary note that window view and document classes are not on this list such classes should not fulfill the tasks associated with system events they typically receive these events and delegate them to a controller example some get a better sense of applying this pattern with code examples look ahead in the implementation section on p for java examples of both rich client and web uis the nextgen application contains several system operations as illustrated in figure this model shows the system itself as a class which is legal and sometimes useful when modeling figure some system operations of the nextgen pos application during analysis system operations may be assigned to the class system in some analysis model to indicate they are system operations however this does not mean that a software class named system fulfills them during design rather during design a controller class is assigned the responsibility for system operations see figure figure what object should be the controller for enteritem view full size image who should be the controller for system events such as enteritem and endsale by the controller pattern here are some choices represents the overall system root object device or subsystem represents a receiver or handler of all system events of a use case scenario register possystem processsalehandler processsalesession note that in the domain of pos a register called a pos terminal is a specialized device with software running in it in terms of interaction diagrams one of the examples in figure could be useful figure controller choices the choice of which of these classes is the most appropriate controller is influenced by other factors which the following section explores during design the system operations identified during system behavior analysis are assigned to one or more controller classes such as register as shown in figure figure allocation of system operations view full size image discussion some get a better sense of applying this pattern with code examples look ahead in the implementation section on p for examples in java for both rich client and web uis simply this is a delegation pattern in accordance with the understanding that the ui layer shouldn t contain application logic ui layer objects must delegate work requests to another layer when the other layer is the domain layer the controller pattern summarizes common choices that you as an oo developer make for the domain object delegate that receives the work requests systems receive external input events typically involving a gui operated by a person other mediums of input include external messages such as in a call processing telecommunications switch or signals from sensors such as in process control systems in all cases you must choose a handler for these events turn to the controller pattern for guidance toward generally accepted suitable choices as illustrated in figure the controller is a kind of facade into the domain layer from the ui layer you will often want to use the same controller class for all the system events of one use case so that the controller can maintain information about the state of the use case such information is useful for example to identify out of sequence system events for example a makepayment operation before an endsale operation different controllers may be used for different use cases a common defect in the design of controllers results from over assignment of responsibility a controller then suffers from bad low cohesion violating the principle of high cohesion please see the issues and solutions section for elaboration the first category of controller is a facade controller representing the overall system device or a subsystem the idea is to choose some class name that suggests a cover or facade over the other layers of the application and that provides the main point of service calls from the ui layer down to other layers the facade could be an abstraction of the overall physical unit such as a register telecommswitch phone or robot a class representing the entire software system such as possystem or any other concept which the designer chooses to represent the overall system or a subsystem even for example chessgame if it was game software various terms are used for a physical pos unit including register point of sale terminal post and so forth over time register has come to embody the notion of both a physical unit and the logical abstraction of the thing that registers sales and payments facade controllers are suitable when there are not too many system events or when the user interface ui cannot redirect system event messages to alternating controllers such as in a message processing system if you choose a use case controller then you will have a different controller for each use case note that this kind of controller is not a domain object it is an artificial construct to support the system a pure fabrication in terms of the grasp patterns for example if the nextgen application contains use cases such as process sale and handle returns then there may be a processsalehandler class and so forth when should you choose a use case controller consider it an alternative when placing the responsibilities in a facade controller leads to designs with low cohesion or high coupling typically when the facade controller is becoming bloated with excessive responsibilities a use case controller is a good choice when there are many system events across different processes it factors their handling into manageable separate classes and also provides a basis for knowing and reasoning about the state of the current scenario in progress in the up and jacobson older objectory method there are the optional concepts of boundary control and entity classes boundary objects are abstractions of the interfaces entity objects are the application independent and typically persistent domain software objects and control objects are use case handlers as described in this controller pattern a important corollary of the controller pattern is that ui objects for example window or button objects and the ui layer should not have responsibility for fulfilling system events in other words system operations should be handled in the application logic or domain layers of objects rather than in the ui layer of a system see the issues and solutions section for an example web uis and server side application of controller please see p for a server side example using java strutsa popular framework a similar delegation approach can be used in asp net and webforms the code behind file that contains event handlers for web browser button clicks will obtain a reference to a domain controller object e g a register object in the pos case study and then delegate the request for work this is in contrast to the common fragile style of asp net programming in which developers insert application logic handling in the code behind file thus mixing application logic into the ui layer server side web ui frameworks such as struts embody the concept of the web mvc model view controller pattern the controller in web mvc differs from this grasp controller the former is part of the ui layer and controls the ui interaction and page flow the grasp controller is part of the domain layer and controls or coordinates the handling of the work request essentially unaware of what ui technology is being used e g a web ui a swing ui also common with server side designs when java technologies are used is delegation from the web ui layer e g from a struts action class to an enterprise javabeans ejb session object variant of the controller patternan object representing a user session or use case scenariocovers this case in this case the ejb session object may itself delegate farther on to the domain layer of objects and again you can apply the controller pattern to choose a suitable receiver in the pure domain layer all that said the appropriate handling of server side systems operations is strongly influenced by the chosen server technical frameworks and continues to be a moving target but the underlying principle of model view separation can and does still apply even with a rich client ui e g a swing ui that interacts with a server the controller pattern still applies the client side ui forwards the request to the local client side controller and the controller forwards all or part of the request handling to remote services this design lowers the coupling of the ui to remote services and makes it easier for example to provide the services either locally or remotely through the indirection of the client side controller benefits increased potential for reuse and pluggable interfaces these benefits ensure that application logic is not handled in the interface layer the responsibilities of a controller could technically be handled in an interface object but such a design implies that program code and the fulfillment of application logic would be embedded in interface or window objects an interface as controller design reduces the opportunity to reuse logic in future applications since logic that is bound to a particular interface for example window like objects is seldom applicable in other applications by contrast delegating a system operation responsibility to a controller supports the reuse of the logic in future applications and since the application logic is not bound to the interface layer it can be replaced with a different interface opportunity to reason about the state of the use case sometimes we must ensure that system operations occur in a legal sequence or we want to be able to reason about the current state of activity and operations within the use case that is underway for example we may have to guarantee that the makepayment operation cannot occur until the endsale operation has occurred if so we need to capture this state information somewhere the controller is one reasonable choice especially if we use the same controller throughout the use case as recommended implementation the following examples use java technologies for two common cases a rich client in java swing and a web ui with struts on the server a servlet engine please note that you should apply a similar approach in net winforms and asp net webforms a good practice in well designed net often ignored by ms programmers who violate the model view separation principle is to not insert application logic code in the event handlers or in the code behind files those are both part of the ui layer rather in the net event handlers or code behind files simply obtain a reference to a domain object e g a register object and delegate to it implementation with java swing rich client ui this section assumes you are familiar with basic swing the code contains comments to explain the key points a few comments notice at that the processsalejframe window has a reference to the domain controller object the register at i define the handler for the button click at i show the key messagesending the enteritem message to the controller in the domain layer package com craiglarman nextgen ui swing imports in java a jframe is a typical window public class processsalejframe extends jframe the window has a reference to the controller domain object private register register the window is passed the register on creation public processsalejframe register register this button is clicked to perform the system operation enteritem private jbutton this is the important method here i show the message from the ui layer to domain layer private jbutton does the button exist if null return else button needs to be initialized new jbutton settext enter item this is the key section in java this is how you define a click handler for a button addactionlistener new actionlistener public void actionperformed actionevent e transformer is a utility class to transform strings to other data types because the jtextfield gui widgets have strings itemid id transformer toitemid gettext int qty transformer toint gettext here we cross the boundary from the ui layer to the domain layer delegate to the controller this is the key statement register enteritem id qty end of the addactionlistener call return end of method end of class implementation with java struts client browser and webui this section assumes you are familiar with basic struts notice at that to obtain a reference to the register domain object on the server side the action object must dig into the servlet context at i show the key messagesending the enteritem message to the domain controller object in the domain layer package com craiglarman nextgen ui web imports in struts an action object is associated with a web browser button click and invoked on the server when the button is clicked public class enteritemaction extends action this is the method invoked on the server when the button is clicked on the client browser public actionforward execute actionmapping mapping actionform form httpservletrequest request httpservletresponse response throws exception the server has a repository object that holds references to several things including the pos register object repository repository repository getservlet getservletcontext getattribute constants register register repository getregister extract the itemid and qty from the web form string txtid saleform form getitemid string txtqty saleform form getquantity transformer is a utility class to transform strings to other data types itemid id transformer toitemid txtid int qty transformer toint txtqty here we cross the boundary from the ui layer to the domain layer delegate to the domain controller this is the key statement register enteritem id qty end of method end of class bloated controllers issues and solutions poorly designed a controller class will have low cohesionunfocused and handling too many areas of responsibility this is called a bloated controller signs of bloating are there is only a single controller class receiving all system events in the system and there are many of them this sometimes happens if a facade controller is chosen the controller itself performs many of the tasks necessary to fulfill the system event without delegating the work this usually involves a violation of information expert and high cohesion a controller has many attributes and it maintains significant information about the system or domain which should have been distributed to other objects or it duplicates information found elsewhere among the cures for a bloated controller are these two add more controllersa system does not have to need only one instead of facade controllers employ use case controllers for example consider an application with many system events such as an airline reservation system it may contain the following controllers use case controllers makereservationhandler managescheduleshandler managefareshandler design the controller so that it primarily delegates the fulfillment of each system operation responsibility to other objects ui layer does not handle system events to reiterate an important corollary of the controller pattern is that ui objects for example window objects and the ui layer should not have responsibility for handling system events as an example consider a design in java that uses a jframe to display the information assume the nextgen application has a window that displays sale information and captures cashier operations using the controller pattern figure illustrates an acceptable relationship between the jframe and the controller and other objects in a portion of the pos system with simplifications figure desirable coupling of ui layer to domain layer view full size image notice that the salejframe classpart of the ui layerdelegates the enteritem request to the register object it did not get involved in processing the operation or deciding how to handle it the window only delegated it to another layer assigning the responsibility for system operations to objects in the application or domain layer by using the controller pattern rather than the ui layer can increase reuse potential if a ui layer object like the salejframe handles a system operation that represents part of a business process then business process logic would be contained in an interface for example window like object the opportunity for reuse of the business logic then diminishes because of its coupling to a particular interface and application consequently the design in figure is undesirable figure less desirable coupling of interface layer to domain layer view full size image placing system operation responsibility in a domain object controller makes it easier to reuse the program logic supporting the associated business process in future applications it also makes it easier to unplug the ui layer and use a different ui framework or technology or to run the system in an offline batch mode message handling systems and the command pattern some applications are message handling systems or servers that receive requests from other processes a telecommunications switch is a common example in such systems the design of the interface and controller is somewhat different the details are explored in a later chapter but in essence a common solution is to use the command pattern and command processor pattern introduced in chapter related patterns command in a message handling system each message may be represented and handled by a separate command object facade a facade controller is a kind of facade layers this is a posa pattern placing domain logic in the domain layer rather than the presentation layer is part of the layers pattern pure fabrication this grasp pattern is an arbitrary creation of the designer not a software class whose name is inspired by the domain model a use case controller is a kind of pure fabrication high cohesion problem how to keep objects focused understandable and manageable and as a side effect support low coupling in terms of object design cohesion or more specifically functional cohesion is a measure of how strongly related and focused the responsibilities of an element are an element with highly related responsibilities that does not do a tremendous amount of work has high cohesion these elements include classes subsystems and so on solution assign a responsibility so that cohesion remains high use this to evaluate alternatives a class with low cohesion does many unrelated things or does too much work such classes are undesirable they suffer from the following problems hard to comprehend hard to reuse hard to maintain delicate constantly affected by change low cohesion classes often represent a very large grain of abstraction or have taken on responsibilities that should have been delegated to other objects example let take another look at the example problem used in the low coupling pattern and analyze it for high cohesion assume we have a need to create a cash payment instance and associate it with the sale what class should be responsible for this since register records a payment in the real world domain the creator pattern suggests register as a candidate for creating the payment the register instance could then send an addpayment message to the sale passing along the new payment as a parameter as shown in figure figure register creates payment this assignment of responsibilities places the responsibility for making a payment in the register the register is taking on part of the responsibility for fulfilling the makepayment system operation in this isolated example this is acceptable but if we continue to make the register class responsible for doing some or most of the work related to more and more system operations it will become increasingly burdened with tasks and become incohesive imagine fifty system operations all received by register if register did the work related to each it would become a bloated incohesive object the point is not that this single payment creation task in itself makes the register incohesive but as part of a larger picture of overall responsibility assignment it may suggest a trend toward low cohesion and most important in terms of developing skills as object designers regardless of the final design choice is the valuable achievement that at least we know to consider the impact on cohesion by contrast as shown in figure 27 the second design delegates the payment creation responsibility to the sale supports higher cohesion in the register figure 27 sale creates payment since the second design supports both high cohesion and low coupling it is desirable discussion like low coupling high cohesion is a principle to keep in mind during all design decisions it is an underlying goal to continually consider it is an evaluative principle that a designer applies while evaluating all design decisions grady booch describes high functional cohesion as existing when the elements of a component such as a class all work together to provide some well bounded behavior here are some scenarios that illustrate varying degrees of functional cohesion very low cohesion a class is solely responsible for many things in very different functional areas assume the existence of a class called rdb rpc interface which is completely responsible for interacting with relational databases and for handling remote procedure calls these are two vastly different functional areas and each requires lots of supporting code the responsibilities should be split into a family of classes related to rdb access and a family related to rpc support low cohesion a class has sole responsibility for a complex task in one functional area assume the existence of a class called rdbinterface which is completely responsible for interacting with relational databases the methods of the class are all related but there are lots of them and a tremendous amount of supporting code there may be hundreds or thousands of methods the class should split into a family of lightweight classes sharing the work to provide rdb access high cohesion a class has moderate responsibilities in one functional area and collaborates with other classes to fulfill tasks assume the existence of a class called rdbinterface that is only partially responsible for interacting with relational databases it interacts with a dozen other classes related to rdb access in order to retrieve and save objects moderate cohesion a class has lightweight and sole responsibilities in a few different areas that are logically related to the class concept but not to each other assume the existence of a class called company that is completely responsible for a knowing its employees and b knowing its financial information these two areas are not strongly related to each other although both are logically related to the concept of a company in addition the total number of public methods is small as is the amount of supporting code as a rule of thumb a class with high cohesion has a relatively small number of methods with highly related functionality and does not do too much work it collaborates with other objects to share the effort if the task is large a class with high cohesion is advantageous because it is relatively easy to maintain understand and reuse the high degree of related functionality combined with a small number of operations also simplifies maintenance and enhancements the fine grain of highly related functionality also supports increased reuse potential the high cohesion patternlike many things in object technologyhas a real world analogy it is a common observation that if a person takes on too many unrelated responsibilitiesespecially ones that should properly be delegated to othersthen the person is not effective this is observed in some managers who have not learned how to delegate these people suffer from low cohesion they are ready to become unglued another classic principle modular design coupling and cohesion are old principles in software design designing with objects does not imply ignoring well established fundamentals another of thesewhich is strongly related to coupling and cohesionis to promote modular design to quote modularity is the property of a system that has been decomposed into a set of cohesive and loosely coupled modules we promote a modular design by creating methods and classes with high cohesion at the basic object level we achieve modularity by designing each method with a clear single purpose and by grouping a related set of concerns into a class cohesion and coupling yin and yang bad cohesion usually begets bad coupling and vice versa i call cohesion and coupling the yin and yang of software engineering because of their interdependent influence for example consider a gui widget class that represents and paints a widget saves data to a database and invokes remote object services not only is it profoundly incohesive but it is coupled to many and disparate elements contraindications in a few cases accepting lower cohesion is justified one case is the grouping of responsibilities or code into one class or component to simplify maintenance by one personalthough be warned that such grouping may also worsen maintenance but suppose an application contains embedded sql statements that by other good design principles should be distributed across ten classes such as ten database mapper classes now commonly only one or two sql experts know how to best define and maintain this sql even if dozens of object oriented oo programmers work on the project few oo programmers may have strong sql skills suppose the sql expert is not even a comfortable oo programmer the software architect may decide to group all the sql statements into one class rdboperations so that it is easy for the sql expert to work on the sql in one location another case for components with lower cohesion is with distributed server objects because of overhead and performance implications associated with remote objects and remote communication it is sometimes desirable to create fewer and larger less cohesive server objects that provide an interface for many operations this approach is also related to the pattern called coarse grained remote interface in that pattern the remote operations are made more coarse grained so that they can to do or request more work in remote operation calls to alleviate the performance penalty of remote calls over a network as a simple example instead of a remote object with three fine grained operations setname setsalary and sethiredate there is one remote operation setdata which receives a set of data this results in fewer remote calls and better performance benefits clarity and ease of comprehension of the design is increased maintenance and enhancements are simplified low coupling is often supported reuse of fine grained highly related functionality is increased because a cohesive class can be used for a very specific purpose recommended resources the metaphor of rdd especially emebged from the influential object work in smalltalk at tektronix in portland from kent beck ward cunningham rebecca wirfs brock and others designing object oriented software is the landmark text and is as relevant today as when it was written wirfs brock has more recently released another rdd text object design roles responsibilities and collaborations two other recommended texts emphasizing fundamental object design principles are object oriented design heuristics by riel and object models by coad chapter object design examples with grasp to invent you need a good imagination and a pile of junk thomas edison introduction this chapter applies oo design principles and the uml to the case studies to show larger examples of reasonably designed objects with responsibilities and collaborations please note that the grasp patterns by name are not important they re just a learning aid that helps us think methodically about basic oo design view full size image the no magic zone this chapter invites you to learn through detailed explanations how an oo developer might reason while designing by principles in fact over a short time of practice these principles become ingrained and some of the decision making happens almost at a subconscious level but first i wish to exhaustively illustrate that no magic is needed in object design no unjustifiable decisions are necessaryassignment of responsibilities and the choice of collaborations can be rationally explained and learned oo software design really can be more science than art though there is plenty of room for creativity and elegant design what is a use case realization the last chapter on basic oo design principles looked at little fragments of design problems in contrast this chapter demonstrates the larger picture of designing the domain objects for an entire use case scenario you will see larger scale collaborations and more complex uml diagrams recall as explained on p that the case studies focus on the domain layer not the ui or service layers which are nevertheless important to quote a use case realization describes how a particular use case is realized within the design model in terms of collaborating objects rup more precisely a designer can describe the design of one or more scenarios of a use case each of these is called a use case realization though non standard perhaps better called a scenario realization use case realization is a up term used to remind us of the connection between the requirements expressed as use cases and the object design that satisfies the requirements uml diagrams are a common language to illustrate use case realizations and as we explored in the prior chapter we can apply principles and patterns of object design such as information expert and low coupling during this use case realization design work to review figure illustrates the relationship between some up artifacts emphasizing the use case model and the design modeluse case realizations figure artifact relationships emphasizing use case realization view full size image some relevant artifact influence points include the following the use case suggests the system operations that are shown in ssds the system operations become the starting messages entering the controllers for domain layer interaction diagrams see figure this is a key point often missed by those new to ooa d modeling figure communication diagrams and system operation handling view full size image domain layer interaction diagrams illustrate how objects interact to fulfill the required tasksthe use case realization artifact comments ssds system operations interaction diagrams and use case realizations in the current nextgen pos iteration we are considering scenarios and system operations identified on the ssds of the process sale use case makenewsale enteritem endsale makepayment if we use communication diagrams to illustrate the use case realizations we will draw a different communication diagram to show the handling of each system operation message of course the same is true for sequence diagrams for example see figure and figure figure sequence diagrams and system operation handling view full size image use cases and use case realizations naturally use cases are a prime input to use case realizations the use case text and related requirements expressed in the supplementary specifications glossary ui prototypes report prototypes and so forth all inform developers what needs to be built but bear in mind that written requirements are imperfectoften very imperfect involve the customer frequently the above section gives the impression that documents are the critical requirements input to doing software design and development truly though it is hard to beat the ongoing participation of customers in evaluating demos discussing requirements and tests prioritizing and so forth one of the principles of agile methods is business people and developers must work together daily throughout the project a very worthy goal operation contracts and use case realizations as discussed use case realizations could be designed directly from the use case text or from one domain knowledge for some complex system operations contracts may have been written that add more analysis detail for example contract enteritem operation enteritem itemid itemid quantity integer cross references use cases process sale preconditions there is a sale underway postconditions a saleslineitem instance sli was created instance creation in conjunction with contemplating the use case text for each contract we work through the postcondition state changes and design message interactions to satisfy the requirements for example given this partial enteritem system operation we diagram a partial interaction that satisfies the state change of saleslineitem instance creation as shown in figure figure partial interaction diagram satisfies a contract postcondition view full size image the domain model and use case realizations in the interaction diagrams the domain model inspires some of the software objects such as a sale conceptual class and sale software class the existing domain modelas with all analysis artifactswon t be perfect you should expect errors and omissions you will discover new concepts that were previously missed ignore concepts that were previously identified and do likewise with associations and attributes must you limit the design classes in the design model to classes with names inspired from the domain model not at all it normal to discover new conceptual classes during design work that were missed during earlier domain analysis and to make up software classes whose names and purpose are completely unrelated to the domain model what next the remainder of this chapter is organized as follows a relatively detailed discussion of the design of the nextgen pos likewise for the monopoly case study starting on p applying uml and patterns to these case studies let get into the details use case realizations for the nextgen iteration the following sections explore the choices and decisions made during the design of a use case realization with objects based on the grasp patterns i intentionally detail explanations to show that there no magic in oo designit based on justifiable principles initialization and the start up use case the start up use case realization is the design context in which to consider creating most of the root or long lived objects see p for some of the design details based on this guideline we will explore the process sale use case realization before the supporting start up design how to design makenewsale the makenewsale system operation occurs when a cashier initiates a request to start a new sale after a customer has arrived with things to buy the use case may have been sufficient to decide what was necessary but for this case study we wrote contracts for all the system operations to demonstrate the approach contract makenewsale operation makenewsale cross references use cases process sale preconditions none postconditions a sale instance was created instance creation was associated with the register association formed attributes of were initialized choosing the controller class our first design choice involves choosing the controller for the system operation message enteritem by the controller pattern here are some choices represents the overall system root object a specialized device or a major subsystem represents a receiver or handler of all system events of a use case scenario store a kind of root object because we think of most of the other domain objects as within the store register a specialized device that the software runs on also called a posterminal possystem a name suggesting the overall system processsalehandler constructed from the pattern use case name handler or session processsalesession choosing a device object facade controller like register is satisfactory if there are only a few system operations and if the facade controller is not taking on too many responsibilities in other words if it is not becoming incohesive choosing a use case controller is suitable when we have many system operations and we wish to distribute responsibilities in order to keep each controller class lightweight and focused in other words cohesive in this case register suffices since there are only a few system operations thus based on the controller pattern the interaction diagram shown in figure begins by sending the system operation makenewsale message to a register software object figure applying the grasp controller pattern creating a new sale we must create a software sale object and the grasp creator pattern suggests assigning the responsibility for creation to a class that aggregates contains or records the object to be created analyzing the domain model reveals that a register may be thought of as recording a sale indeed the word register in business has for hundreds of years meant the thing that recorded or registered account transactions such as sales thus register is a reasonable candidate for creating a sale note how this supports a low representational gap lrg and by having the register create the sale we can easily associate the register with it over time so that during future operations within the session the register will have a reference to the current sale instance in addition to the above when the sale is created it must create an empty collection such as a java list to record all the future saleslineitem instances that will be added this collection will be contained within and maintained by the sale instance which implies by creator that the sale is a good candidate for creating the collection therefore the register creates the sale and the sale creates an empty collection represented by a multiobject in the interaction diagram hence the interaction diagram in figure illustrates the design figure sale and the collection creation view full size image conclusion the design was not difficult but the point of its careful explanation in terms of controller and creator was to illustrate that the details of a design can be rationally and methodically decided and explained in terms of principles and patterns such as grasp how to design enteritem the enteritem system operation occurs when a cashier enters the itemid and optionally the quantity of something to be purchased here is the complete contract contract enteritem operation enteritem itemid itemid quantity integer cross references use cases process sale preconditions there is an underway sale postconditions a saleslineitem instance sli was created instance creation sli was associated with the current sale association formed sli quantity became quantity attribute modification sli was associated with a productdescription based on itemid match association formed we now construct an interaction diagram to satisfy the postconditions of enteritem using the grasp patterns to help with the design decisions choosing the controller class our first choice involves handling the responsibility for the system operation message enteritem based on the controller pattern as for makenewsale we will continue to use register as a controller display item description and price because of a principle of model view separation it is not the responsibility of non gui objects such as a register or sale to get involved in output tasks therefore although the use case states that the description and price are displayed after this operation we ignore the design at this time all that is required with respect to responsibilities for the display of information is that the information is known which it is in this case creating a new saleslineitem the enteritem contract postconditions indicate the creation initialization and association of a saleslineitem analysis of the domain model reveals that a sale contains saleslineitem objects taking inspiration from the domain we determine that a software sale may similarly contain software saleslineitem hence by creator a software sale is an appropriate candidate to create a saleslineitem we can associate the sale with the newly created saleslineitem by storing the new instance in its collection of line items the postconditions indicate that the new saleslineitem needs a quantity when created therefore the register must pass it along to the sale which must pass it along as a parameter in the create message in java that would be implemented as a constructor call with a parameter therefore by creator a makelineitem message is sent to a sale for it to create a saleslineitem the sale creates a saleslineitem and then stores the new instance in its permanent collection the parameters to the makelineitem message include the quantity so that the saleslineitem can record it and the productdescription that matches the itemid finding a productdescription the saleslineitem needs to be associated with the productdescription that matches the incoming itemid this implies that we must retrieve a product description based on an itemid match before considering how to achieve the lookup we want to consider who should be responsible for it thus a first step is to restate the problem who should be responsible for knowing a productdescription based on an itemid match this is neither a creation problem nor one of choosing a controller for a system event now we see our first application of information expert in the design in many cases the expert pattern is the principal one to apply information expert suggests that the object that has the information required to fulfill the responsibility should do it who knows about all the productdescription objects analyzing the domain model reveals that the productcatalog logically contains all the productdescriptions once again taking inspiration from the domain we design software classes with similar organization a software productcatalog will contain software productdescriptions with that decided then by information expert productcatalog is a good candidate for this lookup responsibility since it knows all the productdescription objects the lookup can be implemented for example with a method called getproductdescription abbreviated as getproductdesc in some of the diagrams the name of access methods is idiomatic to each language java always uses the object getfoo form c tends to use object foo and c uses object foo which hides like eiffel and ada whether access is by a method call or is direct access of a public attribute visibility to a productcatalog who should send the getproductdescription message to the productcatalog to ask for a productdescription it is reasonable to assume that a long life register and a productcatalog instance were created during the initial start up use case and that the register object is permanently connected to the productcatalog object with that assumption which we might record on a task list of things to ensure in the design when we get to designing the initialization we know that the register can send the getproductdescription message to the productcatalog this implies another concept in object design visibility visibility is the ability of one object to see or have a reference to another object since we assume that the register has a permanent connectionor referenceto the productcatalog it has visibility to it and hence can send it messages such as getproductdescription a following chapter explores the question of visibility more closely the final design given the above discussion the interaction diagram in figure and the dcd in figure dynamic and static views reflects the decisions regarding the assignment of responsibilities and how objects should interact mark the considerable reflection on the grasp patterns that brought us to this design the design of object interactions and responsibility assignment requires some deliberation figure the enteritem interaction diagram dynamic view view full size image figure 8 partial dcd related to the enteritem design static view view full size image yet once these principles are deeply grasped the decisions often come quickly almost subconsciously retrieving productdescriptions from a database in the final version of the nextgen pos application it is unlikely that all the productdescriptions will be in memory they will most likely be stored in a relational database and retrieved on demand some may be locally cached for performance or fault tolerance reasons however in the interest of simplicity we defer for now the issues surrounding retrieval from a database and assume that all the productdescriptions are in memory chapter explores the topic of database access of persistent objects which is a larger topic influenced by the choice of technologies such as java or net how to design endsale the endsale system operation occurs when a cashier presses a button indicating the end of entering line items into a sale another name could have been enditementry here is the contract contract endsale operation endsale cross references use cases process sale preconditions there is an underway sale postconditions sale iscomplete became true attribute modification choosing the controller class our first choice involves handling the responsibility for the system operation message endsale based on the controller grasp pattern as for enteritem we will continue to use register as a controller setting the sale iscomplete attribute the contract postconditions state sale iscomplete became true attribute modification as always expert should be the first pattern considered unless the problem is a controller or creation problem which it is not who should be responsible for setting the iscomplete attribute of the sale to true by expert it should be the sale itself since it owns and maintains the iscomplete attribute thus the register will send a becomecomplete message to the sale to set it to true see figure that style is especially a smalltalk idiom probably in java setcomplete true figure completion of item entry view full size image calculating the sale total consider this fragment of the process sale use case main success scenario customer arrives cashier tells system to create a new sale cashier enters item identifier system records sale line item and cashier repeats steps until indicates done system presents total with taxes calculated in step a total is presented or displayed because of the model view separation principle we should not concern ourselves with the design of how the sale total will be displayed but we must ensure that the total is known note that no design class currently knows the sale total so we need to create a design of object interactions that satisfies this requirement as always information expert should be a pattern to consider unless the problem is a controller or creation problem which it is not you have probably figured out by expert that the sale itself should be responsible for knowing its total but to make crystal clear the reasoning process to find an expert follow the analysis of this simple example state the responsibility who should be responsible for knowing the sale total summarize the information required the sale total is the sum of the subtotals of all the sales line items sales line item subtotal line item quantity product description price list the information required to fulfill this responsibility and the classes that know this information information required for sale total information expert productdescription price productdescription saleslineitem quantity saleslineitem all the saleslineitems in the current sale sale next we analyze the reasoning process in more detail who should be responsible for calculating the sale total by expert it should be the sale itself since it knows about all the saleslineitem instances whose subtotals must be summed to calculate the sale total therefore sale will have the responsibility of knowing its total implemented as a gettotal method for a sale to calculate its total it needs the subtotal for each saleslineitem who should be responsible for calculating the saleslineitem subtotal by expert it should be the saleslineitem itself since it knows the quantity and the productdescription it is associated with therefore saleslineitem will have the responsibility of knowing its subtotal implemented as a getsubtotal method for the saleslineitem to calculate its subtotal it needs the price of the productdescription who should be responsible for providing the productdescription price by expert it should be the productdescription itself since it encapsulates the price as an attribute therefore productdescription will have the responsibility of knowing its price implemented as a getprice operation the sale gettotal design given the above discussion let us construct an interaction diagram that illustrates what happens when a sale is sent a gettotal message the first message in this diagram is gettotal but observe that the gettotal message is not a system operation message such as enteritem or makenewsale this leads to the following observation the interaction diagram is shown in figure first the gettotal message is sent to a sale instance the sale then sends a getsubtotal message to each related saleslineitem instance the saleslineitem in turn sends a getprice message to its associated productdescriptions figure sale gettotal interaction diagram view full size image since arithmetic is not usually illustrated via messages we can illustrate the details of the calculations by attaching algorithms or constraints to the diagram that defines the calculations who will send the gettotal message to the sale most likely it will be an object in the ui layer such as a java jframe observe in figure the use of the method note symbol style in uml figure showing a method in a note symbol view full size image figure 11 showing a method in a note symbol view full size image how to design makepayment the makepayment system operation occurs when a cashier enters the amount of cash tendered for payment here is the complete contract contract makepayment operation makepayment amount money cross references use cases process sale preconditions there is an underway sale postconditions a payment instance p was created instance creation p amounttendered became amount attribute modification p was associated with the current sale association formed the current sale was associated with the store association formed to add it to the historical log of completed sales we construct a design to satisfy the postconditions of makepayment creating the payment one of the contract postconditions states a payment instance p was created instance creation this is a creation responsibility so we consider the creator grasp pattern who records aggregates most closely uses or contains a payment there is some appeal in stating that a register logically records a payment because in the real domain a register records account information this motivates register candidacy by the goal of reducing the representational gap in the software design additionally we can reasonably expect that sale software will closely use a payment thus it too may be a candidate another way to find a creator is to use the expert pattern in terms of who the information expert is with respect to initializing datathe amount tendered in this case the register is the controller that receives the system operation makepayment message so it will initially have the amount tendered consequently the register is again a candidate in summary there are two candidates register sale now this leads to a key design idea consider some of the implications of these choices in terms of the high cohesion and low coupling grasp patterns if we choose the sale to create the payment the work or responsibilities of the register is lighterleading to a simpler register definition also the register does not need to know about the existence of a payment instance because it can be recorded indirectly via the saleleading to lower coupling in the register this leads to the design shown in figure figure register makepayment interaction diagram view full size image this interaction diagram satisfies the postconditions of the contract the payment has been created associated with the sale and its amounttendered has been set logging a sale once complete the requirements state that the sale should be placed in an historical log as always information expert should be an early pattern considered unless the problem is a controller or creation problem which it is not and the responsibility should be stated who is responsible for knowing all the logged sales and doing the logging by the goal of low representational gap in the software design in relation to our concepts of the domain we can reasonably expect a store to know all the logged sales since they are strongly related to its finances other alternatives include classic accounting concepts such as a salesledger using a salesledger object makes sense as the design grows and the store becomes incohesive see figure figure 14 who should be responsible for knowing the completed sales view full size image note also that the postconditions of the contract indicate relating the sale to the store this is an example of postconditions not being what we want to actually achieve in the design perhaps we didn t think of a salesledger earlier but now that we have we choose to use it instead of a store if this were the case we would ideally add salesledger to the domain model as well since a sales ledger is a concept in the real world domain this kind of discovery and change during design work is to be expected in this case we stick with the original plan of using the store see figure figure 15 logging a completed sale view full size image calculating the balance the process sale use case implies that the balance due from a payment be printed on a receipt and displayed somehow because of the model view separation principle we should not concern ourselves with how the balance will be displayed or printed but we must ensure that it is known note that no class currently knows the balance so we need to create a design of object interactions that satisfies this requirement as always information expert should be considered unless the problem is a controller or creation problem which it is not and the responsibility should be stated who is responsible for knowing the balance to calculate the balance we need the sale total and payment cash tendered therefore sale and payment are partial experts on solving this problem if the payment is primarily responsible for knowing the balance it needs visibility to the sale to ask the sale for its total since it does not currently know about the sale this approach would increase the overall coupling in the designit would not support the low coupling pattern in contrast if the sale is primarily responsible for knowing the balance it needs visibility to the payment to ask it for its cash tendered since the sale already has visibility to the paymentas its creatorthis approach does not increase the overall coupling and is therefore a preferable design consequently the interaction diagram in figure provides a solution for knowing the balance figure sale getbalance interaction diagram view full size image the final nextgen dcd for iteration in accordance with the design decisions in this chapter figure illustrates a static view dcd of the emerging design for the domain layer reflecting the use case realizations for the chosen scenarios of process sale in iteration figure a more complete dcd reflecting most design decisions view full size image of course we still have more oo design workeither while coding or while modelingto do in other layers include the ui layer and services layers how to connect the ui layer to the domain layer common designs by which objects in the ui layer obtain visibility to objects in the domain layer include the following an initializer object for example a factory object called from the application starting method e g the java main method creates both a ui and a domain object and passes the domain object to the ui a ui object retrieves the domain object from a well known source such as a factory object that is responsible for creating domain objects once the ui object has a connection to the register instance the facade controller in this design it can forward system event messages such as the enteritem and endsale message to it see figure figure connecting the ui and domain layers view full size image in the case of the enteritem message we want the window to show the running total after each entry design solutions are add a gettotal method to the register the ui sends the gettotal message to the register which delegates to the sale this has the possible advantage of maintaining lower coupling from the ui to the domain layerthe ui only knows of the register object but it starts to expand the interface of the register object making it less cohesive a ui asks for a reference to the current sale object and then when it requires the total or any other information related to the sale it directly sends messages to the sale this design increases the coupling from the ui to the domain layer however as we explored in the low coupling grasp pattern discussion higher coupling in and of itself is not a problem rather coupling to unstable things is a real problem assume we decide the sale is a stable object that will be an integral part of the designwhich is reasonable then coupling to the sale is not a major problem as illustrated in figure this design follows the second approach figure connecting the ui and domain layers view full size image initialization and the start up use case when to create the initialization design most if not all systems have either an implicit or explicit start up use case and some initial system operation related to the starting up of the application although abstractly a startup system operation is the earliest one to execute delay the development of an interaction diagram for it until after all other system operations have been considered this practice ensures that information has been discovered concerning the initialization activities required to support the later system operation interaction diagrams how do applications start up the startup or initialize system operation of a start up use case abstractly represents the initialization phase of execution when an application is launched to understand how to design an interaction diagram for this operation you must first understand the contexts in which initialization can occur how an application starts and initializes depends on the programming language and operating system in all cases a common design idiom is to create an initial domain object or a set of peer initial domain objects that are the first software domain objects created this creation may happen explicitly in the starting main method or in a factory object called from the main method often the initial domain object assuming the singular case once created is responsible for the creation of its direct child domain objects for example a store chosen as the initial domain object may be responsible for the creation of a register object in a java application for example the main method may create the initial domain object or delegate the work to a factory object that creates it public class main public static void main string args store is the initial domain object the store creates some other domain objects store store new store register register store getregister processsalejframe frame new processsalejframe register choosing the initial domain object what should the class of the initial domain object be high cohesion and low coupling considerations influence the choice between these alternatives in this application we chose the store as the initial object store create design the tasks of creation and initialization derive from the needs of the prior design work such as the design for handling enteritem and so on by reflecting on the prior interaction designs we identify the following initialization work create a store register productcatalog and productdescriptions associate the productcatalog with productdescriptions associate store with productcatalog associate store with register associate register with productcatalog figure shows the design we chose the store to create the productcatalog and register by the creator pattern likewise we chose productcatalog to create the productdescriptions recall that this approach to creating the specifications is temporary in the final design we will materialize them from a database as needed figure creation of the initial domain object and subsequent objects view full size image applying uml observe that the creation of all the productdescription instances and their addition to a container happens in a repeating section indicated by the following the sequence numbers an interesting deviation between modeling the real world domain and the design is illustrated in the fact that the software store object only creates one register object a real store may house many real registers or pos terminals however we are considering a software design not real life in our current requirements our software store only needs to create a single instance of a software register use case realizations for the monopoly iteration first an education point please don t dismiss this case study because it isn t a business application the logic especially in later iterations becomes quite complex with rich oo design problems to solve the core object design principles that it illustratesapplying information expert evaluating the coupling and cohesion of alternativesare relevant to object design in all domains we are designing a simplified version of monopoly in iteration for a scenario of the use case play monopoly game it has two system operations initialize or startup and playgame following our guideline we will ignore initialization design until the last step and focus first on the main system operationsonly playgame in this case iteration requirements p also to support the goal of low representational gap lrg we look again at figure which shows the domain model we turn to it for inspiration as we design the domain layer of the design model figure iteration domain model for monopoly how to design playgame the playgame system operation occurs when the human game observer performs some ui gesture such as clicking a play game button to request the game to play as a simulation while the observer watches the output we didn t write a detailed use case or an operation contract for this case study as most people know the rules our focus is the design issues not the requirements choosing the controller class our first design choice involves selecting the controller for the system operation message playgame that comes from the ui layer into the domain layer by the controller pattern here are some choices represents the overall system root object a specialized device or a major subsystem represents a receiver or handler of all system events of a use case scenario monopolygame a kind of root object we think of most of the other domain objects as contained within the monopolygame abbreviated mgame in most of the uml sketches monopolygamesystem a name suggesting the overall system playmonopolygamehandler constructed from the pattern use case name handler playmonopolygamesession choosing a root object facade controller like monopolygame mgame in figure is satisfactory if there are only a few system operations there are only two in this use case and if the facade controller is not taking on too many responsibilities in other words if it is not becoming incohesive figure applying controller to the playgame system operation the game loop algorithm before discussing oo design choices we prepare by considering the basic algorithm of the simulation first some terminology turn a player rolling the dice and moving the piece round all the players taking one turn now the game loop for n rounds for each player p p takes a turn recall that the iteration version does not have a winner so the simulation simply runs for n rounds who is responsible for controlling the game loop reviewing the algorithm the first responsibility is game loop controllooping for n rounds and having a turn played for each player this is a doing responsibility and is not a creation or controller problem so naturally expert should be considered applying expert means asking what information is needed for the responsibility here the analysis information needed who has the information the current round count no object has it yet but by lrg assigning this to the monopolygame object is justifiable all the players so that each can be used in taking a turn taking inspiration from the domain model monopolygame is a good candidate therefore by expert monopolygame is a justifiable choice to control the game loop and coordinate the playing of each round figure illustrates in uml notice the use of a private internal playround helper method it accomplishes at least two goals it factors the play single round logic into a helper method it is good to organize cohesive chunks of behavior into small separate methods good oo method design encourages small methods with a single purpose this supports high cohesion at the method level the name playround is inspired by domain vocabularythat desirable it improves comprehension figure game loop who takes a turn taking a turn involves rolling the dice and moving a piece to the square indicated by the total of the dice face values what object should be responsible for taking the turn of a player this is a doing responsibility again expert applies now a naive reaction might be to say a player object should take the turn because in the real world a human player takes a turn howeverand this is a key pointoo designs are not one to one simulations of how a real domain works especially with respect to how people behave if you applied the wrong guideline put responsibilities in software objects as they are assigned to people then for example in the pos domain a cashier software object would do almost everything a violation of high cohesion and low coupling big fat objects rather object designs distribute responsibilities among many objects by the principle of information expert among many others therefore we should not choose a player object just because a human player takes a turn yet as we shall see player turns out to be a good choice for taking a turn but the justification will be by expert not inspiration from how humans behave applying expert means asking what information is needed for the responsibility here the analysis information needed who has the information current location of the player to know the starting point of a move taking inspiration from the domain model a piece knows its square and a player knows its piece therefore a player software object could know its location by lrg the two die objects to roll them and calculate their total taking inspiration from the domain model monopolygame is a candidate since we think of the dice as being part of the game information needed who has the information all the squaresthe square organization to be able to move to the correct new square by lrg board is a good candidate now this is an interesting problem there are three partial information experts for the take a turn responsibility player monopolygame and board what interesting about this problem is how to resolve itthe evaluations and trade offs an oo developer may consider here the first guideline to solve the problem guideline when there are multiple partial information experts to choose from place the responsibility in the dominant information expertthe object with the majority of the information this tends to best support low coupling unfortunately in this case are all rather equal each with about one third of the informationno dominant expert so here another guideline to try guideline when there are alternative design choices consider the coupling and cohesion impact of each and choose the best ok that can be applied monopolygame is already doing some work so giving it more work impacts its cohesion especially when contrasted with a player and board object which are not doing anything yet but we still have a two way tie with these objects so here another guideline guideline when there is no clear winner from the alternatives other guidelines consider probable future evolution of the software objects and the impact in terms of information expert cohesion and coupling for example in iteration taking a turn doesn t involve much information however consider the complete set of game rules in a later iteration then taking a turn can involve buying a property that the player lands on if the player has enough money or if its color fits in with the player color strategy what object would be expected to know a player cash total answer a player by lrg what object would be expected to know a player color strategy answer a player by lrg as it involves a player current holdings of properties thus in the end by these guidelines player turns out to be a good candidate justified by expert when we consider the full game rules based on the above figure illustrates the emerging dynamic design and static design figure 24 player takes a turn by expert view full size image applying uml notice the approach to indicating that the taketurn message is sent to each player in a collection named players taking a turn taking a turn means calculating a random number total between and the range of two dice calculating the new square location moving the player piece from an old location to a new square location first the random number problem by lrg we ll create a die object with a facevalue attribute calculating a new random facevalue involves changing information in the die so by expert die should be able to roll itself generate a new random value using domain vocabulary and answer its facevalue second the new square location problem by lrg it reasonable that a board knows all its squares then by expert a board will be responsible for knowing a new square location given an old square location and some offset the dice total third the piece movement problem by lrg it reasonable for a player to know its piece and a piece its square location or even for a player to directly know its square location then by expert a piece will set its new location but it may receive that new location from its owner the player who coordinates all this the above three steps need to be coordinated by some object since the player is responsible for taking a turn the player should coordinate the problem of visibility however that the player coordinates these steps implies its collaboration with the die board and piece objects and this implies a visibility needthe player must have an object reference to those objects since the player will need visibility to the die board and piece objects each and every turn we can usefully initialize the player during startup with permanent references to those objects the final design of playgame based on the above design decisions the emerging dynamic design is as shown in figure and the static design as in figure 26 notice that each message each allocation of responsibility was methodically and rationally motivated by the grasp principles as you come to master these principles you will be able to reason through a design and evaluate existing ones in terms of coupling cohesion expert and so forth figure dynamic design for playgame view full size image figure 26 static design for playgame view full size image applying uml notice in figure 25 that i show two sequence diagrams in the top the taketurn message to a player is not expanded then in the bottom diagram i expand the taketurn message this is a common sketching style so that each wall diagram is not too large the two diagrams are related informally more formally i could use uml sd and ref frames see p which would be easy and appropriate in a uml tool but for wall sketching informality suffices notice again with the roll and getfacevalue messages to a die object the convention of drawing a loop frame around messages to a collection selection object to indicate collection over each element in a collection notice the parameter fvtot in the getsquare message i am informally suggesting this is the total of all the die facevalues this kind of informality is appropriate when we apply uml as sketch assuming the audience understands the context the command query separation principle notice in figure 25 that the message to roll the die is followed by a second getfacevalue to retrieve its new facevalue in particular the roll method is voidit has no return value for example style used in the official solution public void roll facevalue random num generation public int getfacevalue return facevalue why not make roll non void and combine these two functions so that the roll method returns the new facevalue as follows style why is this poor public int roll facevalue random num generation return facevalue you can find many examples of code that follow style but it is considered undesirable because it violates the command query separation principle cqs a classic oo design principle for methods this principle states that every method should either be a command method that performs an action updating coordinating often has side effects such as changing the state of objects and is void no return value or a query that returns data to the caller and has no side effectsit should not permanently change the state of any objects butand this is the key pointa method should not be both the roll method is a commandit has the side effect of changing the state of the die facevalue therefore it should not also return the new facevalue as then the method also becomes a kind of query and violates the must be void rule motivation why bother cqs is widely considered desirable in computer science theory because with it you can more easily reason about a program state without simultaneously modifying that state and it makes designs simpler to understand and anticipate for example if an application consistently follows cqs you know that a query or getter method isn t going to modify anything and a command isn t going to return anything simple pattern this often turns out to be nice to rely on as the alternative can be a nasty surpriseviolating the principle of least surprise in software development consider this contrived but explosive counter example in which a query method violates cqs missile m new missile looks harmless to me string name m getname public class missile public string getname launch launch missile return name end of class initialization and the start up use case the initialize system operation occurs at least abstractly in a start up use case for this design we must first choose a suitable root object that will be the creator of some other objects for example monopolygame is itself a good candidate root object by creator the monopolygame can justifiably create the board and players for exampleand the board can justifiably create the squares for example we could show the details of the dynamic design with uml interaction diagrams but i ll use this case as an opportunity to show a uml dependency line stereotyped with create in a class diagram figure 27 illustrates a static view diagram that suggests the creation logic i ignore the fine details of the interactions in fact that probably suitable because from this uml sketch we the developers who drew this can pretty easily figure out the creation details while coding figure 27 creation dependencies 6 process iterative and evolutionary object design i ve made many suggestions about iterative and evolutionary object design for use case realizations over the last few chapters including on to object design on page object design example inputs activities and outputs on page the essential point keep it light and short move quickly to code and test and don t try to detail everything in uml models model the creative difficult parts of the design figure 28 offers suggestions on the time and space for doing this work figure 28 sample process and setting context view full size image object design within the up to again consider the up as the example iterative method use case realizations are part of the up design model inception the design model and use case realizations will not usually be started until elaboration because they involve detailed design decisions which are premature during inception elaboration during this phase use case realizations may be created for the most architecturally significant or risky scenarios of the design however uml diagramming will not be done for every scenario and not necessarily in complete and fine grained detail the idea is to do interaction diagrams for the key use case realizations that benefit from some forethought and exploration of alternatives focusing on the major design decisions construction use case realizations are created for remaining design problems table summarizes table sample up artifacts and timing start r refine discipline artifact incep elab const trans iteration en cn business modeling domain model requirements use case model ssds r supplementary specification r glossary r design design model r sw architecture document data model r summary designing object interactions and assigning responsibilities is at the heart of object design these choices can have a profound impact on the extensibility clarity and maintainability of an object software system plus on the degree and quality of reusable components there are principles by which the choices of responsibility assignment can be made the grasp patterns summarize some of the most general and common ones used by object oriented designers chapter designing for visibility a mathematician is a device for turning coffee into theorems paul erds introduction visibility is the ability of one object to see or have reference to another this chapter explores this basic but necessary design issue those new to object design sometimes don t think about and design to achieve necessary visibility view full size image visibility between objects the designs created for the system operations enteritem and so on illustrate messages between objects for a sender object to send a message to a receiver object the sender must be visible to the receiverthe sender must have some kind of reference or pointer to the receiver object for example the getproductdescription message sent from a register to a productcatalog implies that the productcatalog instance is visible to the register instance as shown in figure figure visibility from the register to productcatalog is required in this and subsequent code examples language simplifications may be made for the sake of brevity and clarity when creating a design of interacting objects it is necessary to ensure that the necessary visibility is present to support message interaction what is visibility in common usage visibility is the ability of an object to see or have a reference to another object more generally it is related to the issue of scope is one resource such as an instance within the scope of another there are four common ways that visibility can be achieved from object a to object b attribute visibility b is an attribute of a parameter visibility b is a parameter of a method of a local visibility b is a non parameter local object in a method of a global visibility b is in some way globally visible the motivation to consider visibility is this for example to create an interaction diagram in which a message is sent from a register instance to a productcatalog instance the register must have visibility to the productcatalog a typical visibility solution is that a reference to the productcatalog instance is maintained as an attribute of the register attribute visibility attribute visibility from a to b exists when b is an attribute of a it is a relatively permanent visibility because it persists as long as a and b exist this is a very common form of visibility in object oriented systems to illustrate in a java class definition for register a register instance may have attribute visibility to a productcatalog since it is an attribute java instance variable of the register public class register private productcatalog catalog this visibility is required because in the enteritem diagram shown in figure a register needs to send the getproductdescription message to a productcatalog figure attribute visibility view full size image parameter visibility parameter visibility from a to b exists when b is passed as a parameter to a method of a it is a relatively temporary visibility because it persists only within the scope of the method after attribute visibility it is the second most common form of visibility in object oriented systems to illustrate when the makelineitem message is sent to a sale instance a productdescription instance is passed as a parameter within the scope of the makelineitem method the sale has parameter visibility to a productdescription see figure figure parameter visibility view full size image it is common to transform parameter visibility into attribute visibility when the sale creates a new saleslineitem it passes the productdescription in to its initializing method in c or java this would be its constructor within the initializing method the parameter is assigned to an attribute thus establishing attribute visibility figure figure parameter to attribute visibility view full size image local visibility local visibility from a to b exists when b is declared as a local object within a method of a it is a relatively temporary visibility because it persists only within the scope of the method after parameter visibility it is the third most common form of visibility in object oriented systems two common means by which local visibility is achieved are create a new local instance and assign it to a local variable assign the returning object from a method invocation to a local variable as with parameter visibility it is common to transform locally declared visibility into attribute visibility an example of the second variation assigning the returning object to a local variable can be found in the enteritem method of class register figure figure local visibility a subtle version on the second variation is when the method does not explicitly declare a variable but one implicitly exists as the result of a returning object from a method invocation for example there is implicit local visibility to the foo object returned via the getfoo call anobject getfoo dobar global visibility global visibility from a to b exists when b is global to a it is a relatively permanent visibility because it persists as long as a and b exist it is the least common form of visibility in object oriented systems one way to achieve global visibility is to assign an instance to a global variable which is possible in some languages such as c but not others such as java the preferred method to achieve global visibility is to use the singleton pattern which is discussed in a later chapter chapter mapping designs to code beware of bugs in the above code i have only proved it correct not tried it donald knuth introduction with the completion of interaction diagrams and dcds for the current iteration of the case studies there more than enough thought and detail to cut some code for the domain layer of objects the uml artifacts created during the design workthe interaction diagrams and dcdswill be used as input to the code generation process in up terms there exists an implementation model this is all the implementation artifacts such as the source code database definitions jsp xml html pages and so forth thus the code being created in this chapter can be considered part of the up implementation model view full size image language samples java is used for the examples because of its widespread use and familiarity however this is not meant to imply a special endorsement of java c visual basic c smalltalk python and many more languages are amenable to the object design principles and mapping to code presented in this case study programming and iterative evolutionary development the prior design modeling should not be taken to imply that there is no prototyping or design while programming modern development tools provide an excellent environment to quickly explore and refactor alternate approaches and some often lots design while programming is worthwhile the creation of code in an oo languagesuch as java or c is not part of ooa dit an end goal the artifacts created in the design model provide some of the information necessary to generate the code a strength of use cases plus ooa d plus oo programming is that they provide an end to end roadmap from requirements through to code the various artifacts feed into later artifacts in a traceable and useful manner ultimately culminating in a running application this is not to suggest that the road will be smooth or can simply be mechanically followedthere are many variables but having a roadmap provides a starting point for experimentation and discussion creativity and change during implementation some decision making and creative work was accomplished during design work it will be seen during the following discussion that the generation of the code in these examples a relatively mechanical translation process however in general the programming work is not a trivial code generation stepquite the opposite realistically the results generated during design modeling are an incomplete first step during programming and testing myriad changes will be made and detailed problems will be uncovered and resolved done well the ideas and understanding not the diagrams or documents generated during oo design modeling will provide a great base that scales up with elegance and robustness to meet the new problems encountered during programming but expect and plan for lots of change and deviation from the design during programming that a keyand pragmaticattitude in iterative and evolutionary methods mapping designs to code implementation in an object oriented language requires writing source code for class and interface definitions method definitions the following sections discuss their generation in java as a typical case the discussion is more or less independent of using a uml tool for code generation or working from some wall sketches creating class definitions from dcds at the very least dcds depict the class or interface name superclasses operation signatures and attributes of a class this is sufficient to create a basic class definition in an oo language if the dcd was drawn in a uml tool it can generate the basic class definition from the diagrams defining a class with method signatures and attributes from the dcd a mapping to the attribute definitions java fields and method signatures for the java definition of saleslineitem is straightforward as shown in figure figure saleslineitem in java view full size image note the addition in the source code of the java constructor saleslineitem it is derived from the create desc qty message sent to a saleslineitem in the enteritem interaction diagram this indicates in java that a constructor supporting these parameters is required the create method is often excluded from the class diagram because of its commonality and multiple interpretations depending on the target language creating methods from interaction diagrams the sequence of the messages in an interaction diagram translates to a series of statements in the method definitions the enteritem interaction diagram in figure illustrates the java definition of the enteritem method for this example we will explore the implementation of the register and its enteritem method a java definition of the register class is shown in figure figure the enteritem interaction diagram view full size image the enteritem message is sent to a register instance therefore the enteritem method is defined in class register public void enteritem itemid itemid int qty message a getproductdescription message is sent to the productcatalog to retrieve a productdescription productdescription desc catalog getproductdescription itemid message the makelineitem message is sent to the sale currentsale makelineitem desc qty in summary each sequenced message within a method as shown on the interaction diagram is mapped to a statement in the java method the complete enteritem method and its relationship to the interaction diagram is shown in figure 4 the register enteritem method figure the register class view full size image figure 4 the enteritem method view full size image collection classes in code one to many relationships are common for example a sale must maintain visibility to a group of many saleslineitem instances as shown in figure in oo programming languages these relationships are usually implemented with the introduction of a collection object such as a list or map or even a simple array figure 5 adding a collection view full size image for example the java libraries contain collection classes such as arraylist and hashmap which implement the list and map interfaces respectively using arraylist the sale class can define an attribute that maintains an ordered list of saleslineitem instances the choice of collection class is of course influenced by the requirements key based lookup requires the use of a map a growing ordered list requires a list and so on as a small point note that the lineitems attribute is declared in terms of its interface guideline if an object implements an interface declare the variable in terms of the interface not the concrete class for example in figure 5 the definition for the lineitems attribute demonstrates this guideline private list lineitems new arraylist 6 exceptions and error handling exception handling has been ignored so far in the development of a solution this was intentional to focus on the basic questions of responsibility assignment and object design however in application development it wise to consider the large scale exception handling strategies during design modeling as they have a large scale architectural impact and certainly during implementation briefly in terms of the uml exceptions can be indicated in the property strings of messages and operation declarations see p for example 7 defining the sale makelineitem method as a final example the makelineitem method of class sale can also be written by inspecting the enteritem collaboration diagram an abridged version of the interaction diagram with the accompanying java method is shown in figure 6 figure 6 sale makelineitem method view full size image 8 order of implementation classes need to be implemented and ideally fully unit tested from least coupled to most coupled see figure 7 for example possible first classes to implement are either payment or productdescription next are classes only dependent on the prior implementationsproductcatalog or saleslineitem figure 7 possible order of class implementation and testing view full size image 9 test driven or test first development an excellent practice promoted by the extreme programming xp method and applicable to the up and other iterative methods as most xp practices are is test driven development tdd or test first development in this practice unit testing code is written before the code to be tested and the developer writes unit testing code for all production code the basic rhythm is to write a little test code then write a little production code make it pass the test then write some more test code and so forth this is explore in more detail in a following chapter tdd p 10 summary of mapping designs to code as demonstrated there is a translation process from uml class diagrams to class definitions and from interaction diagrams to method bodies there is still lots of room for creativity evolution and exploration during programming work 11 introduction to the nextgen pos program solution this section presents a sample domain layer of classes in java for this iteration the code generation is largely derived from the design class diagrams and interaction diagrams defined in the design work based on the principles of mapping designs to code as previously explored comments excluded on purpose in the interest of brevity as the code is simple class payment all classes are probably in a package named something like package com foo nextgen domain public class payment private money amount public payment money cashtendered amount cashtendered public money getamount return amount class productcatalog public class productcatalog private map itemid productdescription descriptions new hashmap itemid productdescription public productcatalog sample data itemid new itemid itemid new itemid money price new money productdescription desc desc new productdescription price product descriptions put desc desc new productdescription price product descriptions put desc public productdescription getproductdescription itemid id return descriptions get id class register public class register private productcatalog catalog private sale currentsale public register productcatalog catalog this catalog catalog public void endsale currentsale becomecomplete public void enteritem itemid id int quantity productdescription desc catalog getproductdescription id currentsale makelineitem desc quantity public void makenewsale currentsale new sale public void makepayment money cashtendered currentsale makepayment cashtendered class productdescription public class productdescription private itemid id private money price private string description public productdescription itemid id money price string description this id id this price price this description description public itemid getitemid return id public money getprice return price public string getdescription return description class sale public class sale private list saleslineitem lineitems new arraylist saleslineitem private date date new date private boolean iscomplete false private payment payment public money getbalance return payment getamount minus gettotal public void becomecomplete iscomplete true public boolean iscomplete return iscomplete public void makelineitem productdescription desc int quantity lineitems add new saleslineitem desc quantity public money gettotal money total new money money subtotal null for saleslineitem lineitem lineitems subtotal lineitem getsubtotal total add subtotal return total public void makepayment money cashtendered payment new payment cashtendered class saleslineitem public class saleslineitem private int quantity private productdescription description public saleslineitem productdescription desc int quantity this description desc this quantity quantity public money getsubtotal return description getprice times quantity class store public class store private productcatalog catalog new productcatalog private register register new register catalog public register getregister return register 12 introduction to the monopoly program solution this section presents a sample domain layer of classes in java for this iteration iteration will lead to refinements and improvements in this code and design comments excluded on purpose in the interest of brevity as the code is simple class square all classes are probably in a package named something like package com foo monopoly domain public class square private string name private square nextsquare private int index public square string name int index this name name this index index public void setnextsquare square nextsquare public square getnextsquare return nextsquare public string getname return name public int getindex return index class piece public class piece private square location public piece square location this location location public square getlocation return location public void setlocation square location this location location class die public class die public static final int max 6 private int facevalue public die roll public void roll facevalue int math random max public int getfacevalue return facevalue class board public class board private static final int size private list squares new arraylist size public board buildsquares linksquares public square getsquare square start int distance int endindex start getindex distance size return square squares get endindex public square getstartsquare return square squares get private void buildsquares for int i i size i build i private void build int i square new square square i i squares add private void linksquares for int i i size i link i square first square squares get square last square squares get size last setnextsquare first private void link int i square current square squares get i square next square squares get i current setnextsquare next class player public class player private string name private piece piece private board board private die dice public player string name die dice board board this name name this dice dice this board board piece new piece board getstartsquare public void taketurn roll dice int rolltotal for int i i dice length i dice i roll rolltotal dice i getfacevalue square newloc board getsquare piece getlocation rolltotal piece setlocation newloc public square getlocation return piece getlocation public string getname return name class monopolygame public class monopolygame private static final int private static final int private list players new arraylist private board board new board private die dice new die new die public monopolygame player p p new player horse dice board players add p p new player car dice board players add p public void playgame for int i i i playround public list getplayers return players private void playround for iterator iter players iterator iter hasnext player player player iter next player taketurn chapter test driven development and refactoring logic is the art of going wrong with confidence joseph wood krutch introduction extreme programming xp promoted an important testing practice writing the tests first it also promoted continuously refactoring code to improve its qualityless duplication increased clarity and so forth modern tools support both practices and many oo developers swear by their value view full size image test driven development an excellent practice promoted by the iterative and agile xp method and applicable to the up as most xp practices are is test driven development tdd it is also known as test first development tdd covers more than just unit testing testing individual components but this introduction will focus on its application to unit testing individual classes in oo unit testing tdd style test code is written before the class to be tested and the developer writes unit testing code for nearly all production code the basic rhythm is to write a little test code then write a little production code make it pass the test then write some more test code and so forth key point the test is written first imagining the code to be tested is written advantages include the unit tests actually get written human or at least programmer nature is such that avoidance of writing unit tests is very common if left as an afterthought programmer satisfaction leading to more consistent test writing this is more important than it sounds for sustainable enjoyable testing work if following the traditional style a developer first writes the production code informally debugs it and then as an afterthought is expected to add unit tests it doesn t feel satisfying this is test last development also known as just this one time i ll skip writing the test development it human psychology however if the test is written first we feel a worthwhile challenge and question in front of us can i write code to pass this test and then after the code is cut to pass the tests there is some feeling of accomplishmentmeeting a goal and a very useful goalan executable repeatable test the psychological aspects of development can t be ignoredprogramming is a human endeavor clarification of detailed interface and behavior this sounds subtle but it turns out in practice to be a major value of tdd consider your state of mind if you write the test for an object first as you write the test code you must imagine that the object code exists for example if in your test code you write sale makelineitem description to test the makelineitem method which doesn t exist yet you must think through the details of the public view of the methodits name return value parameters and behavior that reflection improves or clarifies the detailed design provable repeatable automated verification obviously having hundreds or thousands of unit tests that build up over the weeks provides some meaningful verification of correctness and because they can be run automatically it easy over time as the test base builds from 10 tests to tests to tests the early more painful investment in writing tests starts to really feel like it paying off as the size of the application grows the confidence to change things in tdd there will eventually be hundreds or thousands of unit tests and a unit test class for each production class when a developer needs to change existing codewritten by themselves or othersthere is a unit test suite that can be run providing immediate feedback if the change caused an error a popular free open source tool to automatically re build the application and run all unit tests is cruisecontrol find it on the web the most popular unit testing framework is the xunit family for many languages available at www junit org for java the popular version is junit there also an nunit for net and so forth junit is integrated into most of the popular java ides such as eclipse www eclipse org the xunit family and junit was started by kent beck creator of xp and eric gamma one of the gang of four design pattern authors and the chief architect of the popular eclipse ide example suppose we are using junit and tdd to create the sale class before programming the sale class we write a unit testing method in a saletest class that does the following create a salethe thing to be tested also known as the fixture add some line items to it with the makelineitem method the makelineitem method is the public method we wish to test 3 ask for the total and verify that it is the expected value using the asserttrue method junit will indicate a failure if any asserttrue statement does not evaluate to true each testing method follows this pattern create the fixture 2 do something to it some operation that you want to test 3 evaluate that the results are as expected a key point to note is that we do not write all the unit tests for sale first rather we write only one test method implement the solution in class sale to make it pass and then repeat to use junit you must create a test class that extends the junit testcase class your test class inherits various unit testing behaviors in junit you create a separate testing method for each sale method that you want to test in general you will write unit testing methods perhaps several for each public method of the sale class exceptions include trivial and usually auto generated get and set methods to test method dofoo it is an idiom to name the testing method testdofoo for example public class saletest extends testcase test the sale makelineitem method public void testmakelineitem step 1 create the fixture this is the object to test it is an idiom to name it fixture it is often defined as an instance field rather than a local variable sale fixture new sale set up supporting objects for the test money total new money 7 5 money price new money 2 5 itemid id new itemid 1 productdescription desc new productdescription id price product 1 step 2 execute the method to test note we write this code imagining there is a makelineitem method this act of imagination as we write the test tends to improve or clarify our understanding of the detailed interface to to the object thus tdd has the side benefit of clarifying the detailed object design test makelineitem sale makelineitem desc 1 sale makelineitem desc 2 step 3 evaluate the results there could be many asserttrue statements for a complex evaluation verify the total is 7 5 asserttrue sale gettotal equals total only after this testmakelineitem test method is written do we then write the sale makelineitem method to pass this test hence the term test driven or test first development ide support for tdd and xunit most ides have built in support for some xunit tool for example eclipse supports junit junit includes a visual cueif all the tests pass when executed it displays a green bar this gave rise to the tdd mantra keep the bar green to keep the code clean figure 1 illustrates figure 1 support for tdd and junit in a popular ide eclipse view full size image 2 refactoring refactoring is a structured disciplined method to rewrite or restructure existing code without changing its external behavior applying small transformation steps combined with re executing tests each step continuously refactoring code is another xp practice and applicable to all iterative methods including the up 3 3 ralph johnson one of the gang of four design pattern authors and bill opdyke first discussed refactoring in kent beck xp creator along with martin fowler are two other refactoring pioneers the essence of refactoring is applying small behavior preserving transformations each called a refactoring one at a time after each transformation the unit tests are re executed to prove that the refactoring did not cause a regression failure therefore there a relationship between refactoring and tddall those unit tests support the refactoring process each refactoring is small but a series of transformationseach followed by executing the unit tests againcan of course produce a major restructuring of the code and design for the better all the while ensuring the behavior remains the same what are the activities and goals refactoring they are simply the activities and goals of good programming remove duplicate code improve clarity make long methods shorter remove the use of hard coded literal constants and more code that been well refactored is short tight clear and without duplicationit looks like the work of a master programmer code that doesn t have these qualities smells bad or has code smells in other words there is a poor design code smells is a metaphor in refactoringthey are hints that something may be wrong in the code the name code smell was chosen to suggest that when we look into the smelly code it might turn out to be alright and not need improvement that in contrast to code stenchtruly putrid code crying out for clean up some code smells include duplicated code big method class with many instance variables class with lots of code strikingly similar subclasses little or no use of interfaces in the design high coupling between many objects and so many other ways bad code is written 4 4 see the original and major oo patterns xp and refactoring wiki com cgi wiki for many wiki pages on code smells and refactoring fascinating site the remedy to smelly code are the refactorings like patterns refactorings have names such as extract method there are about named refactorings here a sample to get a sense of them refactoring description extract method transform a long method into a shorter one by factoring out a portion into a private helper method extract constant replace a literal constant with a constant variable introduce explaining variable specialization of extract local variable put the result of the expression or parts of the expression in a temporary variable with a name that explains the purpose replace constructor call with factory method in java for example replace using the new operator and constructor call with invoking a helper method that creates the object hiding the details example this example demonstrates the common extract method refactoring notice in the figure 2 listing that the taketurn method in the player class has an initial section of code that rolls the dice and calculates the total in a loop this code is itself a distinct cohesive unit of behavior we can make the taketurn method shorter clearer and better supporting high cohesion by extracting that code into a private helper method called rolldice notice that the rolltotal value is required in taketurn so this helper method must return the cmpt developing object oriented systems catalogue description object oriented programming the use of modeling abstractions patterns and guis to design and build a good oo system unit testing to ensure that it works applications of the techniques to interactive systems prerequisite cmpt or and credit units of level calculus or stat or equivalent note students with credit for cmpt may not take this course for credit lectures tu th 50pm arts tutorials m 20am spinks w 20pm spinks tu 20pm spinks website moodle instructor information course objectives cmpt exposes students to object oriented programming concepts with the following objectives learn how to program in the object oriented programming language java learn the basics of guis graphics and concurrent programming in java learn the techniques of unit testing learn the basic principles of designing and building a large software system student evaluation grading scheme assignments midterm exam final exam total assignments the course assignments are structured around developing a large object oriented software system in a sequence of stages the tentative assignment schedule is as follows assignment due sept finding object types and their features assignment due sept getting started with java part a assignment due oct getting started with java part b assignment due oct inheritance and data structures assignment due oct three layer architecture assignment due nov graphic user interfaces assignment due dec model view controller architecture criteria that must be met to pass students must achieve an total mark of or greater to pass the course attendance expectation regular attendance is expected final exam scheduling the registrar schedules all final examinations including deferred and supplemental exams students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information required text big java late objects edi tion by cay s horstmann wi ley recommended texts data structures and software development in an object ori ented domain java edition by jean paul tremblay and grant a cheston prentice hall required excerpts will be posted to moodle program development in java abstraction specification and object oriented design edition by barbara liskov addison wesley core java volume i funda mentals edition by cay s horstmann prentice hall lecture schedule topic subtopics introduction overview of course objectives and evaluation java basics syntax types arrays compiling using eclipse style objects and classes classes interfaces constructors members methods inheritance and interfaces subclasses this super object polymorphism testing basics notation black box testing special case testing boundary value testing regression testing bottom up vs top down testing exception handling exceptions throw try catch review linear data structures java collections set map arraylist iterators stack queue more java typing java generics static variables and methods object oriented design decomposition abstraction locality cohesion coupling informa tion hiding classification of classes two layer architecture com mand pattern data model design manual design reviews graphical user interfaces frames events and listeners components layouts awt swing graphics jpanel paintcomponent shapes text images multi threading basic parallel execution threads race conditions deadlocks thread safety synchronization animation animation via timer class and events animation via threads model view controller ball and socket uml observer pattern model view controller ar chitecture game example more testing equivalence class testing white box testing testing loops and re cursion gray box and object oriented testing files and streams text input scanner reader binary input datainputstream objectinputstream serialization output printstream dataout putstream objectoutputstream printwriter java generics generic types usage best practices policies late assignments assignments are due fridays at late assignments may be submitted by the following monday before but will be deducted assignments submitted later than this will receive a mark of zero missed assignments missed assignments submitted later than monday at following the assignment deadline will be given a grade of zero missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible a doctor note is required for misses due to illness arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the appli cation must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the course outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations subsection of the calendar university of saskatchewan calendar academic courses policy course syllabus website design and development catalog description introduction to design concepts and issues in the development of usable applications on the world wide web including visual design concepts the user centered iterative design process and development technologies that enable application development for the web prerequisite cmpt or cmpt note cmpt cannot be used towards requirements for a b sc in computer science but may be used as an open elective cmpt cannot be taken after cmpt or cmpt but may be taken concurrently course objectives at the end of the course the acquired knowledge and skills will enable the student to build a simple web site that organizes information effectively identify an organization for information based on its inherent structure chronological alphabetic etc use cascading style sheets to create style standards for a web site create a navigational framework that matches the content and genre of the site explain separation of concerns as it applies to the design and implementation of a web site describe the issues involved in developing a web interface summarize the need and issues involved in web site implementation and integration explain why accessibility issues are an important consideration in web page development design and implement a web interface compare contrast graphic media file format characteristics such as color depth compression and codec explain and compare media file formats including lossy vs lossless compression color palettes streaming formats and codecs explain and compare the inter operability of formats student evaluation grading scheme assignments class project midterm exam final exam total assignments assignments covering html css javascript and design issues project and milestones integrating technology and design to be done in teams criteria that must be met to pass completion of assignments and projects attendance expectation students are expected to attend classes and labs final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information required text connolly r hoar r fundamentals of web development pearson recommended text douglas k van duyne james a landay jason i hong the design of sites patterns for creating winning web sites edition prentice hall recommended sources a good source of information and instruction on html css and javascript is the website accessible at http www com proposed lecture schedule the class has been modified every effort will be made to follow the schedule but it is difficult to predict to the day week major items topics reference sept introduction to the www ch sept intro to html o markup o html history o request response ch sept assignment detailed html o tags o attributes ch sept intro to css o including css o basic css examples o attributes ch sept assignment intro to css o css selectors o box model ch oct html tables ch oct thanksgiving holiday oct midterm html forms midterm ch oct assignment javascript ch oct javascript ch nov assignment advanced css o floating layouts ch nov fall midterm break nov advanced css o responsive design o frameworks ch nov web media o color models o image formats o canvas ch dec class project various topics and review course overview labs will offer the student a combination of short exercises as well as opportunities to work on assignments and term projects the course will give the student a high level understanding of the architecture of the internet and of the design and delivery of content primary from the client side the activity that occurs on the client user computer but to some extent from the server side as well the student will gain a deeper understand of the nature of interactivity on the internet certain responses are delivered on the user machine others require access to web resources students should be able to sophisticated software tools like wordpress for the purpose of customization such tools generally prevent or discourage the users from changing the internals but they nonetheless often require that the user have an understanding of things such as css which the user embeds in appropriate parts of the installation rather than modifying the installation so that the product remains robust as updates happen the student will gain experience working with a team policies late assignments assignments submitted after the due date are subject to a penalty please note late assignments delay posting of solutions missed assignments by arrangement with the instructor missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the application must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses http www arts usask ca students transition tips php catalogue description web programming involves the development or coding of web based applications that typically follow the client server architecture model this course will focus on the concepts technologies and tools needed for the development of web centric applications special emphasis will be given to client server programming scripting integration of existing application and high level web services choices e g use of soap and rest course objectives students will understand the concept of web technology languages e g javascript php students will become familiar with key concepts in designing web services client server systems rest soap students will become familiar with back end design and storage issues including rdbms and nosql systems students will be briefly introduced to cloud computing and its impact on web programming and api usage student evaluation grading scheme participation assignments pop quiz project final exam total there will be three assignments in the class the first is due at the end of january the second is due end of february and the third is due end of march projects will be marked during the last week of classes criteria that must be met to pass completion of assignments and projects attendance expectation you are expected to attend all lectures and labs final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information required text deitel deitel internet world wide web how to program edition prentice hall isbn recommended texts jon duckett web design with html css javascript and jquery set edition wiley isbn robin nixon learning php mysql javascript css a step by step guide to creating dynamic websites edition o reilly isbn recommended sources good sources of information and instruction on a sql and javascript is the website accessible at http www com b web based frameworks accessible at http jquery com http phonegap com and http mediaqueri es lecture schedule week major items topics slides references week introduction to www html chapter week week responsive web o css o media queries o content adaptation chapter week javascript chapter week assignment dom ajax xml json week php i http client and server tomcat week php ii rdbms o mysql week nosql week web services o rest o soap week assignment jquery week mobile web frameworks i o jquerymobile week frameworks ii o node js o angularjs week assignment cloud computing week project inspection and marking course overview the course will tentatively cover the following topics and scripting languages responsive and adaptive web design web based frameworks for cross platform application design client server architectures caching proxies sql nosql web services cloud computing and the adoption of web based apis policies recording of lectures students should seek approval from the instructor late assignments discuss with the instructor but there will be penalty missed assignments discuss with the instructor missed examinations students who miss an exam should contact the instructor as soon as possible if it is known in advance that an exam will be missed the instructor should be contacted before the exam a student who is absent from a final examination due to medical compassionate or other valid reasons may apply to the college of arts and science undergraduate student office for a deferred exam application must be made within three business days of the missed examination and be accompanied by supporting documents principles and techniques for developing software combined with the practical experience of creating a mid size software system as a member of a software development team includes teamwork projects planning and process users and requirements use cases modeling quality software architecture testing gui design design principles patterns and implementation ethics professionalism topics include course overview and project description projects teamwork planning process risk agility quality technology users and requirements use cases design software modelling gui design implementation interactive systems design and implementation design principles patterns and implementation advanced design design patterns refactoring inspection and testing software architecture project presentations software maintenance and evolution revised hmr feb professionalism and ethics course review prerequisites cmpt and note as of september course prerequisites will be cmpt and september cmpt will be replaced by and students with credit for do not need to take learning outcomes by the completion of this course students will be expected to learn the following objectives the students are expected to learn of how to think a real world problem as a software development problem the students are expected to learn of how to analyze a software development problem and then design the problem in software development context using different modeling languages e g uml the students should be capable of providing in depth analysis of the subject problem for the purpose of better design once a tentative design is reached the students are then expected to apply advanced software engineering techniques and principles both to improve the analysis and the design as outlined in the course syllabus the students are expected to apply all those from step and to mid size projects and implement first prototypes based on further advanced techniques of software design and analysis e g design patterns and refactoring techniques the students then refine their implementations to make better software systems the students then learn advanced techniques of software testing and apply to their developed projects the students are expected to learn of how to work in a group even in different challenging circumstances such as scheduling issues of the group mates the students are expected to learn the usability issues of the developed software and then they also refine the usability of the software systems they developed information on literal descriptors for grading at the university of saskatchewan can be found at http students usask ca current academics grades grading system php please note there are different literal descriptors for undergraduate and graduate students more information on the academic courses policy on course delivery examinations and assessment of student learning can be found at http policies usask ca policies academic affairs academic courses php the university of saskatchewan learning charter is intended to define aspirations about the learning experience that the university aims to provide and the roles to be played in realizing these aspirations by students instructors and the institution a copy of the learning charter can be found at http www usask ca learningcharter pdf university of saskatchewan grading system for undergraduate courses exceptional a superior performance with consistent evidence of a comprehensive incisive grasp of the subject matter an ability to make insightful critical evaluation of the material given an exceptional capacity for original creative and or logical thinking an excellent ability to organize to analyze to synthesize to integrate ideas and to express thoughts fluently excellent an excellent performance with strong evidence of a comprehensive grasp of the subject matter an ability to make sound critical evaluation of the material given a very good capacity for original creative and or logical thinking an excellent ability to organize to analyze to synthesize to integrate ideas and to express thoughts fluently good a good performance with evidence of a substantial knowledge of the subject matter a good understanding of the relevant issues and a good familiarity with the relevant literature and techniques some capacity for original creative and or logical thinking a good ability to organize to analyze and to examine the subject material in a critical and constructive manner satisfactory a generally satisfactory and intellectually adequate performance with evidence of an acceptable basic grasp of the subject material a fair understanding of the relevant issues a general familiarity with the relevant literature and techniques an ability to develop solutions to moderately difficult problems related to the subject material a moderate ability to examine the material in a critical and analytical manner minimal pass a barely acceptable performance with evidence of a familiarity with the subject material some evidence that analytical skills have been developed some understanding of relevant issues some familiarity with the relevant literature and techniques attempts to solve moderately difficult problems related to the subject material and to examine the course overview principles and techniques for developing software combined with the practical experience of creating a mid size software system as a member of a software development team includes teamwork projects planning and process users and requirements use cases modeling quality software architecture testing gui design design principles patterns and implementation ethics professionalism the major workload in this course is a group project accomplished in self selected teams of four people or less your team registers a company with the instructor with the goal of creating a software product for the course it will be important to choose an appropriately scaled project that is neither too big or too small the students are advised to talk to the instructor if they are not sure whether their project is of reasonable size the tutorial leader and makers will also comment on the project size discussions during the lectures will help establish your project scope the milestones and the final project submission are the responsibility of the entire group but grades will be assigned individually based on project results and individual contributions as a result grades may vary between team members a work plan outlining the tasks assigned to each team member will be a part of each milestone submission teamwork is also a part of the project and a portion of your grade will be assigned based on how well your team works together the milestones focus on key deliverables for that stage in the project but in order to complete the project in a timely fashion it will be important to work on deliverables from later stages early in the term class schedule tentative and might vary a bit over the term but we will mostly follow this below week module readings evaluation due date sep introduction the course goals content workload and expectations the course project sep software process the nature of software and software engineering software failures sep software projects software process models software projects teams and planning problem definition and problem scope sep analysis basic cost estimation requirements and use cases business and domain modelling oct design model based development introduction to design patterns design principles oct design mapping designs to implementation design principles and patterns oct design advanced design patterns oct testing quality assurance basic testing methods nov human computer interaction usability engineering gui development quality assurance basic testing methods nov reading week no lectures nov maintenance software architecture the importance of maintenance refactoring and re engineering software architecture nov project presentation and etc student project presentations professionalism and ethics dec project presentations and exam overview exam overview providing solutions on difficult exam related issues and so on student project presentations final exam midterm and final examination scheduling midterm and final examinations must be written on the date scheduled final examinations may be scheduled at any time during the examination period insert first and last day of current exam period students should therefore avoid making prior travel employment or other commitments for this period if a student is unable to write an exam through no fault of his or her own for medical or other valid reasons documentation must be provided and an opportunity to write the missed exam may be given students are encouraged to review all examination policies and procedures http students usask ca academics exams php instructor information contact information chanchal roy thorvaldson chanchal roy usask ca office hours there are no office hours however i am available anytime you ask for please email me if you have any questions or to arrange a time to meet i give highest priority to meet my students and to talk about the concerns they have because of the diversity of material and topics covered there are two recommended texts for cmpt one required and one recommended each of the books listed is an good reference for future workbut neither is required mcconnell s software project survival guide redmond wash microsoft press isbn available through the u of s library demarco t and lister t peopleware productive projects and teams dorset house edition isbn available through the u of s library course contents the course follows up but greatly expands upon the software engineering issues discussed in cmpt while there will be continued and refined emphasis on the technical side of software engineering the focus for this course is on the critical process and human side of software engineering coordination and management of the software development process project planning and estimation risks and risk analysis team building contract design and negotiation testing and software quality assurance productivity behavioral considerations software configuration management deployment and maintenance training and help software process standards and software process improvement the completion of a significant group project and the management and documentation of this project and the staged delivery of quality products are an essential part of the class assignments and class participation will also contribute significantly to students grades all students must be properly registered in order to attend lectures and receive credit for this course project as described more fully in the final section of this document the project will be carried out in large project groups there will be several stages in the development of the project note that several of the stages are on going throughout the project and some may be done in parallel planning new iterations in the development of the project including adding new requirements these iterations should be risk driven taken into account value delivered to users and salient uncertainties the students required to offer deliverables at least times during the term to help keep you on course we recommend but do not absolutely require more frequent deliverables associated with sprints of or weeks in duration submitting a fully documented rigorously tested and critically reviewed version of the system for each deliverable the feature sets associated with each such deliverable are less important than its reliability and conformance to stated requirements handing in project documentation at significant milestones as required to provide information to the instructor and tutor as to the state of the project clear documentation of requirements simple prototypes for the ui mostly on paper and for areas of particular risk initial and ongoing contact with clients users to validate the requirements and receive feedback on product progress where this is not possible due to continued unavailability of or lack of communication from a client for more than one iteration of the project please notify the instructor immediately meeting multiple times during the term with senior management instructor in a series of briefings to interact with them on the state of the project please note that a possible consequence of such meetings could be changed or new requirements added during the project aggressive up front and ongoing risk management including through an accountable risk officer this should include risk scanning on a periodic basis at least once for each deliverable creation and updating of the identity and status of clear binary mini milestones for the product each such mini milestone should indicate that a task is either complete or not complete use of a version control system e g git subversion if need be for use by with all parties it is essential that this system be used in a way that fosters not just individual version control but team wide coordination the version control system will be available for ad hoc review by the instructor tutor and will be used for the following a code check ins such check ins will refer to design specification and the defect database b an updated top risks enumeration c the project schedule d task assignments to individuals e mini milestone list and status f build smoke test status maintenance of a continuous build regimen including automated smoke testing there should be a a clearly defined build team may rotate b an infrastructure such as with maven teamcity jenkins hudson etc to provide feedback to the team on the state of the build and the broader state of the project c access of the instructor ta to the status reports appointment of a dedicated project manager to help coordinate the project widespread use of assertions in your code adherence to test driven development where tests are written prior to the code use of a project wiki atlassian confluence is available at https wiki usask ca github and other systems will often maintain their own wiki functionality maintenance of a defect database e g jira http www atlassian com software jira fossil google code and or issue tracking system e g trac a code coverage testing framework such as cobertura emma a logging framework e g java logging api optional but recommended use of a style checking system such as checkstyle http checkstyle sourceforge net to be run together with continuous integration project builds a series of peer deskchecks and inspections on artifacts each person in the team must have at least one artifact that is the subject of an inspection documentation on such inspections must be turned in at semester end at least one formal inspection must take place and be documented per deliverable see below note that inspections are not limited to code and can begin almost immediately on a project including one the following a risk inventory b requirements documents c testing plans d test cases e design documents clear planning and documentation of tests ideally prior to implementation including an appointed testing team this should include a use of a test matrix b planning for testability planning not just the testing steps but also the interfaces hooks and scaffolding harnesses needed to drive the system through key elements of its functionality systems that cannot be tested at an integration system or acceptance level because of technical challenges will be viewed highly unfavourably c where possible diagrams showing path coverage whether at the level of testing higher level features or functional blocks in the entire system or at the code level rigorously testing the product and documentation of that testing in the form of a series of documented maintained peer reviewed tests while not all of these tests need to be automated indeed some should not be they all need to be described and those that are automated need to have results checked students are encouraged but not required to develop most tests prior to creation of the software itself use of a testing framework such as junit nunit etc is required for at least the final deliverable an estimate of the fraction of undiagnosed defects using one of the methodologies discussed in class and corresponding refinements to the qa process given the different skillsets involved we require that teams maintain a set of dedicated testers who will devote themselves to testing projects these would include but are not limited to the following a planning test strategies test cases b selecting and familiarizing themselves with testing tools c identifying test platforms d ensure that test driven development is being practiced e establishing appropriate logging f helping maintain the continuous integration test suite organizing bug parties holding such parties with two different subsets of the team form a natural place to using the pooling method covered in class to estimate the count of undiagnosed defects use of a mocking framework e g jmock mockito an activity log should be kept throughout the project to help provide the necessary information to compile the estimates of individual effort adherence to a documented process for governing requirements changes use of a three strike system that involves the use of virtual yellow and red cards to indicate escalated levels of warnings for individuals individuals with red cards must talk to the instructor immediately to avoid high marking penalties preparing for the deployment and maintenance of the software thorough user documentation and an approach to user training workload the anticipated due dates and weighting for submissions for project related deadlines and other assignments are as below all dates indicate a deadline of midnight at the close of the specified day project related deadlines deliverable detail mark due date infrastructure presentation and presentation description of chosen project process components issue tracking wiki pages continuous integration smoke test and status reporting mechanisms version control structure tools e g for mocking gui testing logging persistence etc jan incremental deliverable presentation on incremental deliverable reflects work planned for incremental deliverable fully working version of system with initi requirements test cases and matrix al feb presentation presentation on incremental deliverable reflects work accomplishe since incremental deliverable and work planned for incremental deliverable d feb incremental deliverable fully working version of system with modifications to support best practices and plans for incremental deliverable identifies work to b accomplished by incremental deliverable e feb presentation presentation on incremental deliverable reflects work accomplish since incremental deliverable and work planned for incremental deliverable feb incremental deliverable fully working version of system with added features mar presentation update on incremental deliverable reflects work accomplished sin incremental deliverable and no need to work planned for increment deliverable al mar mitted other deadlines deliverable detail mark due date pop quizzes throughout term in class final exam closed book tbd participation classroom participation in addition to the above deliverables a significant fraction of students grades will be based on participation in recognition of differences in communication styles and interests among students this participation score will reflect interaction in lecture tutorials in project meetings at the beginning of class and in office hours while it is understood that occasional absence from lectures is unfortunately sometimes necessary for health or other extraordinary considerations students who are absent from class will be missing both course material of importance to functioning productively in the project and the team meetings to be held in the first minutes of class please see below students who are chronically absent from class sessions will naturally be at a great disadvantage when it comes to learning and the marks that reflect it but can expect both to receive very poor scores for participation and will also be viewed as contributing less to the team project project participation participation in the team project is mandatory for all students students deemed to have inadequately contributed to the mandatory course project will receive an escalating pair of warnings beginning with the receipt of a yellow virtual warning card from the instructor it is the obligation of a student who receives a yellow warning card to make immediate efforts to arrange for a meeting with the professor or teaching assistant to discuss and resolve their situation failure by the student to adequately resolve the underlying concerns regarding participation can be expected to lead to the issuance of a red warning card from the instructor students receiving such a red card are at imminent risk of failure of the course and it is incumbent open them to act immediately to make a meeting with the instructor to address these concerns and to make all efforts to contribute to the project failure of a student to respond to either of these warnings will be taken as an indication that they agree that they are failing to contribute adequately to the project peer review of student artifacts students must have at least one artifact undergo a peer deskcheck review either with or without the author present for each deliverable these artifacts may be pieces of code but could also be other substantial components for the project including design elements including class diagrams requirements documents a risk management plan tests a testing plan etc the reviewer and author must sign off on a form that indicates the results of the review including brief description of defects found or rework to be performed when the review is complete these reports must be submitted with the deliverable in addition each team is repsonsible for conducting at least one formal inspection during each deliverable each student is required to be present as a reviewer or other participant in at least two such inspections during the term these inspections must be documented and signed off on by the inspection team students whose artifacts are not inspected for the term or who do not participate in at least two peer reviews may have signifcant marks deducted as may projects that fail to have at least one formal inspection per deliverable we suggest the use of the doodle scheduler www doodle ch for scheduling peer reviews please note that some of the most critical of these meetings must be held prior to the first milestone with the results reflected in that milestone and milestone presentation management plan review risk management review test plan design changes finally there may be additional management meetings to be held at a mutual agreeable time for students instructor and tas topic plan lecture slides will be provided via the course website when possible but are not guaranteed for all classes there will also be some guest lectures scattered through the term and frequent meetings with project management there will also be tutorials as particular topics warrant see below a substantial portion of the class grade will reflect student participation so students should come to tutorial as well as lecture prepared to discuss the material recently presented that will be being presented in that day a preliminary lecture schedule is included below the schedule has been designed to provide students with knowledge and skills that will enhance project success students are strongly advised to apply the techniques covered within class within their projects please note that the schedule below is tentative and we expect that it will be revised throughout the term updated versions will be posted on the moodle site in cases of larger changes the class may be notified via email during some sessions of this semester class in which the instructor online videos are readily available we will be experimenting with a flipped classroom style of instruction that has been positively used by the instructor in other classes for such sessions students will be required to review relevant material including a video prior to each session and most classroom time will be devoted to activities that take key advantage of the shared classroom experience these include but are not limited to assisting students with applying and adapting the ideas captured in the videos to their projects providing feedback on project requirements design code and other issues and addressing student questions and assisting students with exercises such as in refactoring test and testability planning the instructor videos from the version of cmpt may be particularly helpful date topic jan introduction and overview best practices greatest hits jan best practices risk management risk driven incremental delivery with mini milestones jan best practices assertions exceptions return codes jan best practices daily build smoke test continuous integration jan essential testing test driven development test matrices testability overview of test coverage estimating undiagnosed defects jan best practices qa processes defect reporting traceability triage and testing status reports jan best practices estimation decomposition and estimation final comments jan best practices scheduling feb best practices meetings reviews inspections and audits feb best practices clean coding value objecs side effects feb best practices clean coding liskov substitution principle part part feb best practices clean coding mixins skeletal implementations dangers of subclassing safe subclassing and wrapper classes feb vacation feb feb test case design feb test design tips mar test design mar debugging mar debugging mar guest lecture tbd mar testing mar project control mar human capital team building mar managing teams behavioral considerations i mar systems thinking project dynamics apr systems thinking project dynamics apr pressure efficiency and effectiveness tutorials tutorial periods are to be used for team meetings project presentations and formal inspections date topic jan team acquaintenance jan team discussion jan team meeting jan infrastructure presentation feb formal inspection inform instructor ta of material location by previous day feb presentation feb presentation mar formal inspection inform instructor of location for each team in case he wants to attend mar presentation mar formal inspection inform instructor of location for each team in case he wants to attend mar presentation mar formal inspection inform instructor of location for each team in case he wants to attend apr presentation marking the term project pop quizzes class participation and final exam collectively account for of the grade the grade of the course will be assigned on an individual and team basis for the term project the work is done by a set of students working together as a development team the team grade of the term project is obtained from the term project documents from the term project grade each member will get an individual term project grade depending on her his efforts and contributions as evaluated by her his peers in the group each individual will receive a grade equal to the term project grade times a multiplier this multiplier can be lower or greater than one the average of the individual grades will be equal to the team grade specifically to establish individual contributions a peer evaluation is performed during the final project phase this evaluations will ask each team member to distribute a financial bonus among the team members provide a recommendation for each member of the team evaluate the effect of each member to the morale of the team evaluate the contribution of the team member to the term project and assign a title to each member of the team including the person filling out the form there is the expectation that such evaluation will remain confidential it is also expected that team members will behave professionally and honestly while filling the evaluation no consultation is permitted with other team members when or before filling out the evaluation working effectively in a team is a precondition to get a good grade but in the case that circumstances on the team create a difficult environment individuals who contribute very strongly to the team can still secure a good mark nonetheless it bears emaphasis that because this coefficient influences the points accrued to the student from across the entire group project a student level of involvement in the project may have a substantial impact on their final grade per day may be deducted from late term project phases up to a maximum of seven days term project phases received more than seven days late may not receive any credit under certain circumstances extensions may be granted please contact the instructor or tutor prior to the due date if an extension is required failure to complete the assigned course work will result in failure of the course in addition to the above the course includes a final exam failure to write the final exam will result in failure of the course the final exam will include both a take home and an in class portion students are expected to adhere to university of saskatchewan academic honesty policy more information can be found at http www usask ca honesty pdf pdf project description project overview cmpt is a course about the management of the software development process participation in a real world project is important in helping students internalize course material the project in cmpt will involve the incremental design and implementation of one of a specified set of projects with defined stakeholders the cmpt project will be carried out in randomly selected larger member groups effective management of such a large group to achieve stated project objectives will be critical to the success of the project after the group has been put together the first order of business will be to decide which project to develop deciding this will require consulting with the stakeholder for a project in which you are interested and working towards an agreement with that stakeholder as to their willingness to work with the team if several teams are interested in the same project the stakeholder can choose with which subset of teams possibly none at all they wish to work with based on the professionalism of those teams in the initial meetings their time availability etc a plan will have to be devised and put into effect to achieve the stated objectives it is required that the plan will include at least elaboration cycles each delivering a robust and stable version of the software and incorporating increasingly sophisticated functionality as will be discussed in class you will wish to prioritize these iterations such that they prioritize the early delivery of value the resolution of pronounced risks build team morale and spread team knowledge etc as the project proceeds the plan will guide activities but will always be subject to updating as circumstances change so it will likely be quite specific only for the immediately approaching development cycles and fairly general for later iterations in the end the final project will be a high quality working system fully documented and tested and measured complete with a deployment plan although it won t actually have to be deployed the emphasis for this deliverables will be on quality rather than on quantity of features each deliverable should be as close to industrial strength as you can make it and include adequate documentation to permit extension by another team if so required the higher quality that is expected will require extra levels of care and planning for example each person in the class will be required to have all significant artifacts peer reviewed see below and we expect formal defect tracking and defect removal estimation please scope your projects accordingly in addition to outside arrangements made amongst themselves students should plan on holding brief time boxed project scrum meetings in the first minutes of each scheduled lecture period unless otherwise noted to make best use of these meetings we suggest that students refer to the following reference http www martinfowler com articles itsnotjuststandingup html tutorial periods are to be used for team meetings and for formal inspections see schedule workload there will be three presentations and four project major milestones during the course and regular meetings with management the requirements for each of these are now outlined milestone unless special arrangements have been made with the instructor the system should be in a working and stable state for this and other milestones initial system with source code repository daily build and smoke tests a mini milestone list list of risks risk report requirements documentation of team personnel roles which may include more than one role per individual documentation of any reviews conducted so far e g on initial requirements or design this should also mention meetings with stakeholders a report of testing results should be included including a defect reports for defects found the deliverable should comment on any existing or planned tests and any testability investments that currently exist or that are planned in the near future many of these testability investments could be hooks mechanisms to allow tests to examine state which would otherwise be difficult to observed or logging mechanisms efforts to secure separation of concerns use of specifications design by contract and other elements that foster testability you should also give a sense as to how the system is to be tested not just manual mechanism and support for any mechanisms test harnesses that are currently used to drive the system through key elements of its functionality or that will be pursued prior to milestone a statement of requirements to incorporate in the next deliverables should be clearly spelled out paper or simple gui prototypes should be presented to describe user oriented functionality for the incremental deliverable the deliverable should also include a detailed plan for achieving the next milestone objectives this plan should include a design and architecture a project cpm schedule outlining steps mini milestones in carrying out the project and a critical path through this network the plan may have sub plans corresponding to sub goals established by the various work units into which the project has been sub divided an initial estimate of the size of the software for incremental deliverable and the effort required to produce it should also be made the first milestone should include an activity report summarizing work undertaken by each group member to date and the activity log giving time estimates presentation the first presentation will provide a project overview to management and other class members of milestone and should include a demonstration of the system as it is currently present as well as a demonstration of status report information milestone in milestone all project artifacts created or modified since the first milestone should be handed in including updated planning analysis and design artifacts as well as new artifacts updated risk analysis including results of new scanning the version control strategy the test strategy and testing done to date including test results and the latest measurements of software quality e g results of code reviews code and interfaces should also be included in a manner similar to what was done for milestone you should indicate in detail what is planned to accomplish between now and milestone it is important for this presentation that you summarize not only what has been accomplished for the work products but also for refining the development process what you have learned or changed in the development or management process presentation the second presentation will provide a summary of milestone and a demonstration of the milestone deliverable milestone milestone is similar to milestone in contents and should include detailed discussion of the lessons learned from the project presentation the third presentation will provide a summary of milestone and a discussion of what is planned for milestone milestone final milestone milestone is similar to milestone in contents except that no summary of work to be accomplished prior to the next milestone is included milestone should however include detailed discussion of the lessons learned from the project presentation this presentation will provide a summary of milestone and a demonstration of the final completed system inspections each member of a team is expected to have at least one significant artifacts they create undergo peer review via a peer deskcheck per milestone artifacts to be reviewed include requirements specifications design specifications tests code bug fixes etc this review will require signoff by both parties involved plus an indication of any issues that were identified that require changes and subsequent peer review each person must also have at least one artifact inspected via a formal process described in class and in the readings during the course of the term this process will involve pre inspection review of documents by team members rigorous note taking and follow up some important issues group work your group should divide the tasks in the project appropriately so that each member can work independently or in a smaller sub group much of the time however effective group interactions will still be mandatory to complete the project successfully as in cmpt only more so you are encouraged to communicate electronically using your mechanism of choice in a private area each lecture meeting will provide an opportunity to meet during the first minutes of class 05 but you should all also physically meet separately at least once a week to make sure that everybody is on the same page an agenda for each such meeting should be prepared beforehand and minutes of the meeting should be kept if group problems e g caused by personality conflicts slackers etc start to occur they should be nipped in the bud before they develop into real obstacles to success you should try to solve the problems yourselves perhaps at the regular meetings as a last resort you should approach the higher management tutor and if absolutely necessary the professor remember that managing such a large group has its own special disadvantages too many cats all going their own direction but also its own special advantages one problem worker doesn t usually bring down the whole group while experience suggests that some members of the class may not participate fully at all times throughout the semester they do so at their own peril and it is important that the project members welcome those individuals when and if they choose to make an effort to contribute as noted below the final exam will survey individuals in your team as to the roles played and levels of effort expended by yourself and your teammates presentation timing each of the presentations will be scheduled for a full hour there should be minutes at most for set up minutes for the presentation and a minute question period not all members of the team need to take part in any given presentation but over the term everybody should stand in front of the class at least once hand in at each project milestone and in the final project all appropriate artifacts should be handed in using moodle the project should be appropriately packaged so each component is clear there should be some sort of overview document and or index to help the marker to understand the various parts of the project and in the final project this should also contain instructions for running the system critically these should involve a readme file in the main folder to direct the reader to the appropriate starting point marking the marking of term work in the course will be based upon meeting the objectives set above for each presentation milestone and the overall project we will be looking for realistic outlook sensible and responsive planning comprehensible documentation aimed at practical communication for future teams that will extend this project appropriate communicational use of the analysis and design techniques explored in earlier computer science courses in particular cmpt and in the later presentations and milestones also a solid risk analysis thorough testing and measurement good deployment planning including training and help material these artifacts should all be continuously updated as the project proceeds the system developed at each milestone will be judged for its robustness maintainability usability utility and adherence to requirements although all members of the group will share in the overall group mark for each presentation at each milestone and in the final project it is possible that group members who do not contribute equally will be assigned only a portion of the group mark each of you will be asked on the final exam to indicate in both a qualitative and quantitative fashion the contribution of each member of the group contributed during the year this information will be combined with the activity tables handed in during the course and appropriate adjustments will be made catalogue description covers advanced software engineering principles and techniques includes software architecture software evolution reverse engineering design recovery refactoring software comprehension software analysis domain specific techniques requirements and specification advanced design and modeling techniques formal methods and the business of software course objectives and course contents covers advanced software engineering principles and techniques includes software architecture software evolution reverse engineering design recovery refactoring software comprehension software analysis domain specific techniques requirements and specification advanced design and modeling techniques formal methods and the business of software this course builds on the understanding of software engineering presented in cmpt and to a lesser degree cmpt the focus is on techniques to help foster quality software engineering the major topics covered will range from introductory but latest software engineering concepts to advanced software quality assurance and maintenance principles and techniques the major topics are source transformation for advanced software engineering the students will learn the importance of source transformation systems and their applications in software engineering in particular the students will learn the txl source transformation language and then will apply txl in several advanced software engineering activities such as source to source transformation e g c language system to java language systems or vice versa pretty printing standard formatting of one code improving the quality of the software systems including the performance support for better software testing software maintenance activities including clone detection analysis and refactoring the students will also learn the associated state of the art tools design patterns design patterns are one of the important aspect of quality software and thus for quality software development the course will extensively talked about all those advanced design patterns from gang of four design patterns in particular the course will discuss the following design patterns singleton adapter facade observer builder bridge chain of responsibility factory abstract factory interpreter state strategy prototype mediator memento command flyweight visitor proxy composite and decorator design patterns development anti patterns as of the design patterns anti patterns in particular development anti patterns are equally important for quality software development in this course we will also discuss in details a few of the development anti patterns such as spaghetti code mushroom management stovepipe enterprise cover your assets vendor lock in design by committee warm bodies jumble wolf ticket swiss army knife and so on software quality assurance software quality assurance is the primary theme of this course and the students will learn the details of software quality what is it why do we need it how to measure it what are the different methods of providing quality assurance the students will learn the details of different testing methods including black box white box grey box and mutation testing methods code smells and refactoring given that software quality is the primary goal of this course the students will also learn the details of different code smells including the number one code smell the software clones and then they will learn the different refactoring methods of how to remove those code smells including code clones software change this part of the lecture deals with software change which is a foundation of most of the software engineering processes we will first explain the classification of changes followed by the explanation of the requirements their analysis and the product backlog software change consists of several phases and the first one is the selection of a specific change request from product backlog this is followed by the phase of concept location where the programmers find what specific software module needs to be changed then impact analysis decides on the strategy and extent of the change actualization implements the new functionality and incorporates it into the old code refactoring reorganizes software so that the change is easier prefactoring or cleans up the mess that the change may have created postfactoring verification validates the correctness of the change change conclusion creates a new baseline prepares software for the next change and possibly releases the latest version to the users this part of the lecture establishes a foundation on which the next part software processes builds software process this part of the lecture presents the most common software processes it explains what a software process is and what the forms are model enactment performance and plan then it deals with the iterative process of a solitary programmer sip the team agile iterative process aip directed iterative process dip and centralized iterative process cip these processes are primarily applicable to the stages of software evolution servicing but they also apply also to initial implementation and reengineering this part then covers initial development and the final stages of software lifespan and hence it presents an overview of processes applicable to all stages of software lifespan advanced software engineering topics in addition to the above topics this course will also discuss on some other advanced software engineering topics as follows a subtyping subclassing and liskov substitute principle open close principle b the map reduce framework c garbage collection d multithreading in java e architectural styles f jini g dynamic proxies in java h remote method invocation java rmi i actor model j java idl k evolutionary coupling student evaluation grading scheme participation grad urd assignments grad urd midterm exam grad urd final exam urd project grad total assignments and due date assignment change requests for software maintenance group marks the students will be given a set of change requests based on the requests the students are expected to do concept location impact analysis prefactoring refactoring and actual implementation with due consideration of verification tentative due date feb assignment research paper presentations with critical analysis group marks the students read a research paper in the area of the advanced software engineering make a critical review and present the paper in the class tentative due date feb assignment tutorial on advanced topics of software engineering individual marks the students choose an advanced topic in the area of advanced software engineering and make a tutorial report based on the report the students make slides and present the tutorial in the class tentative due date march assignment making pretty printer for c programs using txl group marks the students use the source transformation language txl and then make a grammar for toy c language for parsing and facilitate source transformation and analysis tentative due date march assignment c to pascal or pascal to c programs translator using txl group marks using the txl grammars for c and pascal languages the students build source transformation systems that will translate c programs to pascal or pascal programs to c tentative due date march mid terms the mid term for the class is scheduled for february in the lecture criteria that must be met to pass the students should obtain pass marks both on the assignments and exams in order to pass the course attendance expectation attendance is not mandatory however there is up to marks including bonus on class participation and discussion final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information required text readings from a variety of sources will be provided as pdfs on the course website also the following texts j r cordy excerpts from the txl cookbook generative and transformational techniques in software engineering lecture notes in computer science january pp vaclav rajlich software engineering the contemporary practice crc press nov pages lecture schedule and course overview week of topics week monday january introduction to software engineering and software change week monday january software change concept location impact analysis prerefactoring week monday january refactoring actualization week monday january software verification software quality student presentations week monday february change based software processes student presentations week monday february case studies on software change process student presentations week monday february family day winter mid term break no classes week monday february txl source transformations and analysis student presentations week monday march txl source transformations and analysis student presentations week monday march design patterns advanced software engineering topics student presentations week monday march design anti patterns advanced software engineering topics student presentations week monday march formal methods student presentations week monday march domain specific techniques student presentations week monday april final exam overview student presentations policies recording of lectures i usually record the lectures students are also welcome to record the lectures if they wish late assignments for any late assignments please contact the instructor depending on the reasons we may reach an agreement without any penalties missed assignments please talk to the instructor there might be scope for alternative assignments the til chairmarks til tiny imperative language is a very small imperative language with as signments conditionals and loops designed by eelco visser and james cordy as a basis for small illustrative example transformations all of the example ap plications in the txl cookbook work on til or extended dialects of it figure shows two examples of basic til programs the til chairmarks are a small set of benchmark transformation and analysis tasks based on til they are called chairmarks because they are too file factors til find factors of a given number var n write input n please read n write the factors of n are var f f while n do while n f f n do write f n n f end f f file multiples til first multiples of numbers through for i to do for j to do write i j end end end fig example til programs small to be called benchmarks these tasks form the basis of our cookbook and the examples in this tutorial are txl solutions to some of the problems posed in the chairmarks the til chairmark problems are split into six cat egories parsing restructuring optimization static and dynamic analysis and language implementation in this tutorial we only have room for one or two spe cic problems from each category in each case a specic concrete problem is proposed and a txl solution is demonstrated introducing the corresponding txl solution paradigms and additional language features as we go along we begin with the most important category parsing parsing problems every transformation or analysis task begins with the creation or selection of a txl grammar for the source language the form of the language grammar has a big inuence on the ease of writing transformation rules in these rst problems we explore the creation of language grammars pretty printers and syntactic extensions using the parsing aspect of txl only with no transformations the grammars we create here will serve as the basis of the transformation and analysis problems in the following sections in many cases a txl grammar is already available for the language you want to process on the txl website it is important to remember that the purpose of the txl grammar for an input language is not in general to serve as a syntax checker unless of course that is what we are implementing we can normally assume that inputs to be transformed are well formed this allows us to craft grammars that are simpler and more abstract than the true language grammar for example allowing all statements uniformly everywhere in the language even if some are semantically valid only in certain contexts such as the return statement in pascal which is valid only inside procedures in general such uniformity in the grammar makes analyzing and transforming the language forms easier in the case of til the language is simple enough that such simplication of the grammar is unnecessary basic parser syntax checker in this rst problem our purpose is only to create a grammar for the language we plan to process in this case til figure shows a basic txl grammar le til grm for til the main nonterminal of a txl grammar must always be called program and there must be a nonterminal denition for program somewhere in the grammar implementing a parser and syntax checker using this grammar is straightforward simply including the grammar in a txl program that does nothing but match its input figure le tilparser txl paradigm the grammar is the parser txl grammars are in some sense misnamed they are not really grammars in the usual bnf specication sense to be processed and analyzed by a parser generator such as sdf or bison rather a txl grammar is a directly interpreted recursive descent parser written in grammatical style thus in txl the grammar is really a program for parsing file til grm txl grammar for tiny imperative language when pretty printing we parse and output comments controlled by this pragma pragma comment keywords of til a reserved word language keys var if then else while do for read write end end keys compound tokens to be recognized as a single lexical unit compounds end compounds commenting convention for til comments are ignored unless comment is set comments end comments direct txl encoding of the til grammar nl in and ex on the right are optional pretty printing cues define program statement end define define statement declaration end define untyped variables define declaration var name nl end define define name expression nl end define define if expression then in nl statement ex opt end nl end define define else in nl statement ex end define define while expression do in nl statement ex end nl end define define for name expression to expression do in nl statement ex end nl end define define read name nl end define define write expression nl end define define only ever present if comment is set nl comment nl end define traditional priority expression grammar define expression comparison expression logop comparison end define define logop and or end define define comparison term comparison eqop term end define define eqop end define define term factor term addop factor end define define addop end define define factor primary factor mulop primary end define define mulop end define define primary name literal expression end define define literal integernumber stringlit end define define name id end define fig til grammar in txl file tilparser txl txl parser for tiny imperative language all txl parsers are automatically also pretty printers if the grammar includes the optional formatting cues as in this case use the til grammar include til grm no need to do anything except recognize the input since the grammar includes the output formatting cues function main match program program end function fig til parser and pretty printer the input language where the input is source text and the output is a parse tree when crafting txl grammars one needs to be aware of this fact and think at least partly like a programmer rather than a language specier the creation of a txl grammar begins with the specication of the lexical forms tokens or terminal symbols of the language using txl regular expres sion pattern notation several common lexical forms are built in to txl notably id which matches c style identiers number which matches c style integer and oat constants stringlit which matches double quoted c style string lit erals and charlit which matches single quoted c style character literals the til grammar uses only the default tokens id integernumber and stringlit as its terminal symbols thus avoiding dening any token patterns of its own integernumber is a built in renement of number to non oating point forms more commonly it would be necessary to dene at least some of the lexical forms of the input language explicitly using txl tokens statements the til keywords are specied in the grammar using the keys statement which tells txl that the given words are reserved and not to be mistaken for identiers the compounds section tells us that the til symbols and are to be treated as single tokens and the comments section tells txl that til comments begin with and go to the end of line comments are by default ignored and removed from the parsed input and do not appear in the parse tree or output however they can be preserved see section paradigm use sequences not recursions the fact that txl grammars are actually parsing programs has a strong inuence on the expression of language forms for example in general it is better to express sequences of statements or expressions directly as sequences x or equivalently repeat x rather than right or left recursive productions this is both because the parser will be more ecient and because the txl pattern matching engine is optimized for searching sequences thus forms such as this one which is often seen in bnf grammars should be converted to sequences in this case statement in txl statements statement statements statement paradigm join similar forms in order to avoid backtracking multiple similar forms are typically joined together into one in txl grammars for example when expressed in traditional bnf the til grammar shows two forms for the if statement with and without an else clause as separate cases if expression then statement end if expression then statement else statement end while we could have coded this directly into the txl grammar because it is directly interpreted when instances of the second form were parsed txl would have to begin parsing the rst form until it failed then backtrack and start over trying the second form to match the input when many such similar forms are in the grammar this backtracking can become expensive and it is better to avoid it by programming the grammar more eciently in txl in this case both forms are subsumed into one by separating and making the else clause optional in the txl dene for if statement figure paradigm encode precedence and associativity directly in the grammar as in all direct top down parsing methods left recursive nonterminal forms can be a particular problem and should in general be avoided however sometimes as with left associative operators direct left recursion is required and txl will recognize and optimize such direct left recursions an example of this can be seen in the expression grammar for til figure which encodes precedence and associativity of operators directly in the grammar using a traditional prece dence chain rather than separate precedence and associativity into separate disambiguation rules txl normally includes them in the grammar in this way figure shows a txl program using the til grammar that simply parses input programs and the result of running it on the example program multi ples til of figure using the command txl xml multiples til tilparser txl is shown in figure the xml output shows the internal xml form of the parse tree of the input program pretty printing the next problem we tackle is creating a pretty printer for the input language in this case til pretty printing is a natural application of source transformation systems since they all have to create source output of some kind paradigm using formatting cues to control output format txl is designed for pretty printing and output formatting is controlled by inserting formatting cues for indent in exdent ex and new line nl into the grammar these cues look like nonterminal symbols but have no eect on input parsing their linux txl multiples til tilparser txl xml program repeat statement statement for name id i id name expression primary literal integernumber integernumber literal primary expression to expression primary literal integernumber integernumber literal primary expression do repeat statement statement for name id j id name expression primary literal integernumber integernumber literal primary expression to expression primary literal integernumber integernumber literal primary expression do repeat statement statement write expression expression primary name id i id name primary expression op op expression primary name id j id name primary expression expression statement repeat statement end statement repeat statement end statement repeat statement program fig example xml parse tree output of til parser only role is to specify how output is to be formatted for example in the til grammar of figure the denition for while statement uses in nl following the while clause specifying that subsequent lines should be indented and that a new line should begin following the clause the ex after the statements in the body species that subsequent lines should no longer be indented and the nl following the end of the loop species that a new line should begin following the while statement paradigm preserving comments in output by default txl ignores comments specied using the comments section as shown in figure where til comments are specied as from to end of line in order to preserve comments in output we must tell txl that we wish to do that using the comment command line argument or the equivalent pragma directive pragma comment once we have done that comments become rst class tokens and the grammar must allow comments anywhere they may appear for well formed input code this is not dicult but in general it is tricky and can require signicant tuning it is a weakness of txl that it has no other good way to preserve comments in the til case we have only end of line comments and we will assume that they are used only at the statement level if we observe other cases they can be added to the grammar this is specied in the grammar with the statement form comment statement which has no eect when comment is o because no comment token will be available to parse comment statement is dened to put a new line before each comment in order to separate it in the output define nl comment nl end define figure shows the result of pretty printing multiples til using the parsing pro gram of figure language extensions language extensions dialects and embedded dsls are a common application of source transformation systems the next problem involves implementing a number of syntactic extensions to the til grammar syntactic extension is one of the things txl was explicitly designed for and the paradigm is straightforward figure shows four small language extensions to til the addition of begin end statements the addition of arrays the addition of functions and the addition of modules i e anonymous or singleton classes new grammatical forms to kens and keywords are dened using the usual tokens keys and dene statements of txl as for example with the begin statement denition in the begin end extension of til and the addition of the function keyword in the function ex tension of til both in figure paradigm extension of grammatical forms new forms are integrated into the ex isting language grammar using redenitions of existing forms such as statement in the begin end dialect of til txl redefine statement is explicitly designed to support language modications and extensions in the begin end extension we can see the use of redene to add a new statement form to an existing language redefine statement refers to all existing forms add alternative for our new form end redefine linux cat examples multiples til output first multiples of numbers through for i to do for j to do output each multiple write i j end end linux txl comment multiples til tilparser txl output first multiples of numbers through for i to do for j to do output each multiple write i j end end fig example output of the til pretty printer file tilbeginend grm txl grammar overrides for begin end extension of the tiny imperative language add begin end statements redefine statement existing forms adds new form end redefine define begin in nl statement ex end nl end define file tilfunctions grm txl grammar overrides for functions extension of the tiny imperative language add functions using grammar overrides redefine declaration existing new form end redefine redefine statement existing end redefine keys function end keys define function name name opt in nl statement ex end nl nl end define define opt name expression nl end define define name end define define name end define file tilarrays grm txl grammar overrides for array extension of the tiny imperative language add arrays using grammar overrides redefine declaration var name opt subscript nl end redefine redefine primary name opt subscript end redefine redefine name opt subscript expression nl end redefine define subscript expression end define file tilmodules grm txl grammar overrides for module extension of the tiny imperative language add modules using grammar overrides requires functions extension redefine declaration existing forms add new form end redefine keys module public end keys define module name in nl statement ex end nl nl end define redefine opt public end redefine fig txl overrides for four dialects of til such a grammar modication is called a grammar override in txl since it overrides or replaces the original denition with the new one the notation in this example is not an elision it is an actual part of the txl syntax it refers to all of the previously dened alternative forms for the nonterminal type in this case statement and is a shorthand for repeating them all in the redene it also makes the redenition largely independent of the base grammar so if the denition of statement in til changes the language extension will not require any change it will just inherit the new denition because txl grammars are actually directly interpreted programs for pars ing any ambiguities of the extension with existing forms are automatically re solved the rst dened alternative that matches an input will always be the one recognized so even if the base language changes such that some or all of the language extension forms are subsumed by the base language grammar the extension will continue to be valid paradigm grammar overrides files language extension and dialect les are normally stored in a separate grammar le such a grammar modication le is called a grammar overrides le and is included in the txl program following the include of the base grammar so that it can refer to the base grammar dened grammatical types include til grm include tilbeginend grm for example while the til begin end extension is independent of the grammat ical forms of til other than the statement form it is extending in the arrays extension of figure expression and name refer to existing grammatical types of til paradigm preferential ordering of grammatical forms in the begin end exten sion the new form is listed as the last alternative indicating a simple extension that adds to the existing language when the new forms should be used in prefer ence to existing ones as in the arrays example the new form is given as the rst alternative and the existing alternatives are listed below as in the declaration and primary redenitions in the arrays extension of til redefine declaration var name opt subscript nl end redefine redefine primary name opt subscript end redefine because grammatical types are interpreted directly as a parsing program this means that any input that matches will be parsed as the new form even if existing old forms would have matched it so for example every var declaration in the arrays extension including those without a subscript e g var x will be parsed with an optional subscript in the extended language even though the base grammar for til already has a form for it similarly every name reference which appears as a primary in the extension will be parsed with an opt subscript even though there is an existing name form for primary pretty printing cues for extended forms are specied in redene statements in the usual way by adding nl in and ex output formatting nonterminals to the denitions of the new forms as in the new declaration form above paradigm replacement of grammatical forms grammar type redenitions can also completely replace the original form in the base grammar for example the assignment statement form of the arrays extension of til ignores the denition in the base grammar and denes its own form which completely replaces it this means that every occurrence of an assignment statement in the extended language must match the form dened in the dialect redefine name opt subscript expression nl end redefine paradigm composition of dialects and extensions language extensions and dialects can be composed and combined to create more sophisticated dialects for example the module anonymous class extension of til shown in figure is itself an extension of the function extension extensions are combined by including their grammars in the txl program in dependency order for example include til grm include tilarrays grm include tilfunctions grm include tilmodules grm paradigm modification of existing forms extended forms need not be com pletely separate alternatives or replacements when used directly in a redene rather than as an alternative the notation still refers to all of the origi nal forms of the nonterminal modied by the additional syntax around it for example in the module extension of til figure the function declaration form is extended to have an optional public keyword preceding it in this way the module dialect does not depend on what a function denition looks like only that it exists figure shows an example of a program written in the modular til language dialect described by the composition of the arrays functions and modules grammar overrides in figure robust parsing robust parsing is a general term for grammars and parsers that are insensi tive to minor syntax errors and or sections of code that are unexplained by the input language grammar robust parsing is very important in production pro gram analysis and transformation systems since production software languages are often poorly documented making it dicult to get an accurate grammar because language compilers and interpreters often allow forms not ocially in the language denition and because languages often have dialects or local cus tom extensions in practice for all of these reasons it is important that analysis and transformation tools such as those implemented in txl be able to handle exceptions to the grammar so that systems can be at least partially processed paradigm fall through forms the basic paradigm for robust parsing in txl is to explicitly allow for unexplained input as a dialect of the grammar that adds a last uninterpreted alternative to the grammatical forms for which there may be such unocial or custom variants for example we may want to allow for statements that are not in the ocial grammar by making them the last alternative when we are expecting a statement file primes mtil this program determines the primes up to maxprimes using the sieve method var maxprimes var maxfactor maxprimes maxfactor maxprimes div var prime var notprime prime notprime module flags var flagvector maxprimes public function flagset f tf flagvector f tf end public function flagget f tf tf flagvector f end end everything begins as prime var i i while i maxprimes do flagset i prime i i end fig part of an example program in the til arrays functions modules dialect figure shows the grammar overrides for a dialect of til that allows for unexplained statement forms the key idea is that all statements in til end with a semicolon so if we have a form ending in a semicolon that does not match any of the known forms it must be an unknown statement form because alternatives in txl grammars are tried in order we can encode this by adding the unknown case as the last form for statement redefine statement existing forms for statement fall through if not recognized end redefine paradigm uninterpreted forms when parsing all other alternatives are tested after which we fall through to the unknown statement form unknown state ment is any sequence of input items that are not semicolons not semicolon ended with a semicolon this ensures that we don t accidentally accept uninter preted input over a statement boundary the not semicolon nonterminal type is the key to ushing uninterpreted input and uses a standard txl paradigm for ushing input token or key token is a special txl built in type that matches any input token that is not a keyword of the grammar and key is a special built in type that matches any keyword thus the following denition describes a type that will accept any single item from the input define token any input token that is not a keyword key any keyword end define paradigm guarded forms in the robust til dialect we must be careful not to throw away a semicolon and thus we have guarded token or key with a nonterminal guard in the not semicolon denition not indicates that if the next input token is a semicolon then we should not accept it as a token or key define not any item except semicolon end define not x is a generalized grammatical guard that can be used to limit what can be matched by the form following it to those inputs that cannot be recognized as an x which can be any grammatical type its semantics are simple if an x can be parsed at the current point in the input then the following form is not tested otherwise it is in either case not x does not itself consume any input file tilrobust grm txl grammar overrides for robust parsing extension of tiny imperative language allow for unrecognized statement forms redefine statement refers to all existing forms for statement add fall through if we don t recognize a statement end redefine define nl end define define not any input item that is not a semicolon end define define token any input token that is not a keyword key any keyword end define fig txl overrides for robust statement parsing in til island grammars island grammars address a related problem to robust parsing the problem of embedded code we wish to process in a sea of other text we don t want to process for example we may want to analyze only the embedded c code examples in the chapters of a textbook or a set of html pages or only the exec sql blocks in a large set of cobol programs the basic strategy for island grammars in txl is to invert the robust parsing strategy we treat the input as a sequence of meaningful things islands and unmeaningful things water figure the meaningful things are for example til programs and the unmeaningful things are any sequence of input items not beginning with a til program file islands grm generic grammar for parsing documents with embedded islands the input is a sequence of interesting islands and uninteresting water redefine program end redefine define island water end define water is any input that is not an island define water end define define any item that does not begin an island not island end define define token any token not a keyword key any keyword end define file tilislands txl txl program for parsing documents with embedded til programs begin with the til grammar include til grm and the generic island grammar include islands grm in this case the islands are til programs define island end define define statement end define we can now target rules at embedded til island but in this case we just delete the non til to yield code only rule main replace water water rest by rest end rule fig txl generic island grammar left and an island parser for embedded til programs based on it right paradigm preferential island parsing figure shows a generic txl gram mar for implementing island grammars to parse documents such as this one recognizing the embedded islands such as til programs and ignoring the rest of the text such as this paragraph as usual the trick is that the rst alter native island is preferred and the second water is tried only if the rst fails parameterized generic grammars such as this one are frequently used in txl to encode reusable parsing paradigms such as island grammars the generic island grammar is used by dening island the interesting form in the txl program that includes the generic grammar the second half of fig ure is a txl program that uses the generic island grammar to make an island grammar for embedded til programs in documents such as this tutorial island is dened as til program which uses the included til grammar statement form the analysis or transformation rules can then target the island forms only ignoring the uninterpreted water in this case the program simply replaces all occurrences of water by the empty sequence leaving only the embedded til programs in the output agile parsing agile parsing refers to the use of grammar tuning on an individual analysis or transformation task basis by using the parser to change the parse to better isolate the parts of the program of interest or make them more amenable to the particular transformation or analysis we can greatly simplify the rules necessary to perform the task paradigm transformation specific forms agile parsing is implemented in txl using grammar overrides redenes in exactly the same way as we have done for language extensions and dialects in essence we create a special dialect grammar for the language in support of the particular task the remainder of this paper consists of a sampling of example problems in various applications of source transformation highlighting the txl paradigms that are used in each solution restructuring problems once we have crafted grammars for our input languages we can begin using them to support the real work the transformation and analysis tasks that support software understanding maintenance renovation migration and evolution the exibility of the txl parser is a key to its application in many domains for example we exploit agile parsing in many solutions but the real work is in the transformation and analysis rules in the remaining problems from the txl cookbook we concentrate on source code transformation and analysis problems in three categories restructuring problems optimization problems and static and dynamic analysis problems in each category we will look at a set of small but real challenges each couched in terms of til and its extensions we only have space for a few representative examples in each category chosen not because they are the most useful but because they introduce new recipes and paradigms as we have seen a txl grammar is not really a grammar rather it is a functional program for parsing the input which gives us direct control over the parse yielding both exibility and generality similarly a txl transformation rule set is not really a term rewriting system rather the rules form a func tional program for transforming the input with similar direct control over tree traversal and strategy again yielding exibility and generality we begin with problems in basic program restructuring the heart of appli cations in refactoring and code improvement as with our parsing examples all of our example problems are based on the tiny imperative language til and its extensions we will use the grammars and parsing techniques we developed in section to support all our solutions paradigm programmed functional control transformations and analyses are coded in txl using rules and functions the basic dierence between the two is that rules repeatedly search for and transform instances of their pattern until no more can be found whereas functions transform exactly one instance of their pattern txl is a functional language and the transformation is driven by the application of one rule or function the main rule to the parse tree of the entire input all other rules and functions must be explicitly invoked either in the main rule or in other rules invoked by it in contrast to pure term rewriting systems this functional style gives the pro grammer ne grained programmed control over the application of transformation rules on an invocation by invocation basis and tree traversals and strategies can be customized to each task of course the downside of this exibility and control is that you must do so the price we pay in txl for detailed programmability as we shall see in practice the common traversals and strategies are simply txl coding paradigms which we can learn quickly and reuse as need be it is these functional paradigms that we will be exploiting in our solutions paradigm transformation scopes the result of a txl rule or function invo cation is a transformed copy of the scope parse tree it is directly applied to in txl scopes of application are explicitly programmed rules are not global but transform only the subtree they are applied to the result of a rule application is semantically a completely separate copy from the scope itself the original txl variable bound to the scope is unchanged by a rule invocation on it and retains its original value parse tree as in all functional languages for example if the txl variable x is bound to the number the rule invocation x yields but does not change x which retains its original value feature reduction applications in code analysis and transformation often begin by normalizing the code to reduce the number of features in the code to be analyzed in order to expose basic semantics and reduce the number of cases to analyze figure is a simple example of such a feature reduction transformation the elimination of til for loops by translation to an equivalent while the transformation has only one rule main which searches for sequences of statement beginning with a for statement and replaces the for with an equivalent while statement while small this simple example introduces us to a number of txl paradigms it may surprise you to see that the rule is targeted at the type statement a sequence of statements rather than just statement since it is a single for statement that we are replacing the reason for this is that we need to replace the for loop with not one statement but several the initialization of the iteration variable the declaration and computation of the upper limit and the while loop itself if we had tried to replace a single statement with this sequence we would get a syntax error in the replacement because txl rules are constrained to preserve grammatical type in order to guarantee a well formed result a sequence of statements statement is not an instance of the type statement and thus a replacement of several statements would violate the type constraint paradigm raising the scope of application this situation is an example of a general paradigm in txl transforming a pattern that is further up the parse tree than what we really want to match in order to be able to create a result that is signicantly dierent the saying in txl is if you can t create the replacement you want target further up the tree in this specic case we need to create several statement out of one so we must target the statement sequence statement of which the for statement is a part note that the statements following the for are also captured in the pattern morestatements and preserved in the result this is a part of the paradigm if we had not allowed for these the pattern could match only sequences containing file tilfortowhile txl convert tiny imperative language for statements to while form based on the til grammar include til grm preserve comments in output pragma comment rule to convert every for statement rule main capture each for statement in its statement sequence context so that we can replace it with multiple statements replace statement for id id expression to expression do statements statement end morestatements statement need a unique new identifier for the upper bound construct upperid id id upper construct the iterator construct iteratestatement statement id id replace the whole thing by end rule var id id var upperid upperid while id upperid do statements iteratestatement end morestatements fig txl transformation to convert for statements to while statements exactly one statement that is the last statement of a sequence there is no cost to copying these from the pattern to the result since like many functional languages txl optimizes ow through copies paradigm explicit patterns the pattern for the for loop is fully explicated that is it matches all of its parts right away rather than just a for statement which we could then take apart similarly the replacement contains all the parts of the result explicitly rather than constructing a while statement and replacing it whole this example like way of expressing rules is a txl style making the pattern and replacement show as much as possible of the form of the actual intended pattern and result target code rather than the constructed terms txl uses the same parser i e the txl grammar you specify to parse patterns and replacements in rules as it does for input thus it constructs all of the intermediate terms for you this means that there is no cost to explicating details in a pattern and it would be no more ecient to have a pattern searching for a for statement only than for the entire pattern we have coded in the main rule because the parsed pattern is in fact a for statement anyway given this preference for an example like style it may also surprise you to see that the iteration statement iteratestatement is separately constructed and appended to the sequence of statements in the body of the loop rather than appearing in the replacement directly the reason for this is the denition of sequence in txl the sequence type x has a recursive denition deriving x x or empty thus although a statement at the head of a sequence statement statement as in the pattern of this rule is a valid statement a statement at the end statement statement is not therefore the txl sequence append built in function is provided to allow for this and the rule uses it to append the new statement to those in the loop body it may also surprise you to see that the literal identiers and keywords in both the pattern and the replacement have been quoted using a single quote in all cases while this is not necessary except for the txl keyword end txl programmers often choose to quote literal identiers to remind the reader that they are not txl variable references but part of the output text paradigm generating unique new identifiers the rule uses two built in func tions and to generate a unique new identier for the introduced upper bound variable in the construct of upperid a new identier is constructed from the original for iteration variable name id to which the literal identier upper is appended with underscore using the built in function to form a new iden tier for example if id is i then we have i upper the new identier is then made globally unique using the unique built in function which appends a number to it to create a new identier unused anywhere else in the input for example i this rst example did its transformations in place let look at one that moves things around a bit declarations to global one of the standard challenges for transformation tools is the ability to move things about and in particular to make transformations at an outer level that depend on things deeply embedded in an inner level and vice versa in the next two examples we will look at each of these kinds of problems in turn in the rst problem we are simply going to move all declarations in the til program to the global scope even though til declarations seem to be able to appear anywhere according to the til grammar their meaning is apparently global since no scope rules are dened in this transformation we make the true meaning of embedded declarations explicit by promoting all declarations to one global list at the beginning of the program the simplest solution to this problem figure uses two common paradigms of txl type extraction and type filtering the basic strategy is shown in the main rule which has three steps construct a copy of all the declarations in the program as a sequence construct a copy of the program with all declarations removed and concatenate the one to the other to form the result file tiltoglobal txl make all til declarations global based on the til base grammar include til grm preserve comments in output pragma comment the main rule in this case a function applies only once function main replace program program statement extract all statements then filter for declarations only construct declarations statement program removenondeclarations make a copy of the program with all declarations removed construct programsansdeclarations statement program removedeclarations the result consists of the declarations concatenated with the non declarations by rule removedeclarations rule to remove every declaration at every level from statements replace statement declaration declaration followingstatements statement by followingstatements end rule rule removenondeclarations rule to remove all statements that are not declarations from statements replace statement nondeclaration statement followingstatements statement check the statement isn t a declaration deconstruct not nondeclaration declaration if so take it out by followingstatements end rule declarations programsansdeclarations end function fig txl transformation to move all declarations to the global scope extracting all the declarations from the program is done in two steps using the extract  built in rule to get a sequence of all the statements of the program and then removing all those that are not declarations construct declarations statement program removenondeclarations paradigm extracting all instances of a type the extract built in function  is applied to a scope of type t for any type t and takes as parameter a bound variable v of any type the rule constructs a sequence containing a copy of every occurrence of an item of type t in v and replaces its scope with the result in our case a sequence containing a copy of every statement in the program is constructed extract ignores its original scope so it is normally empty to begin with in this case we have used the empty variable a special txl variable denoting an empty item as the scope of the rule this is the usual way that extract is used paradigm filtering all instances of a type the second step in this construct uses the subrule removenondeclarations to remove all non declarations from the constructed sequence of all statements the constructor could have ex tracted all declaration directly but this would cause problems later when we tried to concatenate them to the beginning of the program the subrule uses a common ltering paradigm in txl looking for any occurrence of a sequence of statements beginning with a statement that is not a declaration and replacing it with the sequence without the beginning statement the rule continues until it can nd no remaining instances in its scope paradigm negative patterns determining that a statement is not a declaration involves another common paradigm in txl a negated deconstructor a normal deconstructor simply matches a bound variable to a pattern for example deconstruct statement assignment which succeeds and binds assignment if the statement to which statement is bound consists entirely of an assignment statement in this case however we are interested in statements that are not a decla ration so we use deconstruct not to say that our match succeeds only if the deconstructor fails that is the statement bound to nondeclaration is not a declaration although it has a pattern a deconstruct not does not bind any pat tern variables since to succeed it must not match its pattern thus any variable names in the negated deconstructor pattern are irrelevant and in this case we have explicitly indicated that by using the anonymous name in the pattern deconstruct not nondeclaration declaration the same ltering paradigm is used in the second constructor of the main rule to remove all declarations from the copy of the program used in the result of the rule this general removal paradigm can be used with any simple complex or guarded pattern to remove items matching any criterion from a scope finally the replacement of the rule simply appends the copy of the program without declarations to the extracted declarations yielding a result with all declarations at the beginning of the program 3 declarations to local the other half of the movement challenge is the ability to make transformations on an inner level that depend on things from an outer level one such problem is localization in which things at an outer level are to be gathered and moved to an inner level it can be used to support clustering of related methods refactoring to infer methods creation of inferred classes and so on in this next problem we assume that til is a scoped language rather than unscoped the idea is to nd all declarations of variables that are articially global and localize them as much as possible to the deepest inner scope in which they are used in some sense it is the inverse of the previous problem figure shows a txl solution to this problem the main rule for this transformation uses two steps immediatize and localize the immedia tizedeclarations rule moves declarations as far down in their scope as possible to immediately before the rst statement that uses their declared variable for example if we have the scope shown on the left below a then the rst step immediatizedeclarations will yield the intermediate result in the middle b the second step localizedeclarations then looks for compound statements file tiltolocal txl move all declarations in a til program to their most local location based on the til base grammar include til grm preserve comments pragma comment transformation to move all declarations to their most local location immediately before their first use in the innermost block they can be rule main this rule pattern matches its result so it has no natural termination point replace program program program so we add an explicit fixed point guard after each application of the two transformations we check to see that something more was actually done construct newprogram program program immediatizedeclarations localizedeclarations deconstruct not newprogram program by newprogram end rule rule immediatizedeclarations move declarations past statements that don t depend on them use a one pass traversal replace statement var v id statement statement morestatements statement we can move the declaration past a statement if the statement does not refer to the declared variable deconstruct not id statement v by statement var v morestatements end rule rule localizedeclarations move declarations outside a structured statement inside if following statements do not depend on the declared variable again use a one pass traversal replace statement declaration declaration compoundstatement statement morestatements statement check that it is some kind of compound statement one with a statement list inside deconstruct statement compoundstatement statement check that the following statements don t depend on the declaration deconstruct id declaration v id deconstruct not id morestatements v alright we can move it in another solution might use agile parsing to abstract all these similar cases into one by compoundstatement injectdeclarationwhile declaration injectdeclarationfor declaration injectdeclarationifthen declaration injectdeclarationifelse declaration morestatements end rule function injectdeclarationwhile declaration declaration there is no legal way that the while expn can depend on the declaration since there are no assignments between the declaration and the expn replace statement while expn expression do statements statement end by while expn do declaration statements end end function other injection rules similar fig txl transformation to localize all declarations into which an immediately preceding declaration can be moved and moves the declaration var x in the example inside yielding the result c on the right var y var x var y read y var y read y read y y y y y y y var x if y then if y then if y then var x x y x y x y write x write x write x end end end a b c paradigm transformation to a fixed point because declarations may be more than one level too global the process must be repeated on the result until a xed point is reached this is encoded in the main rule which is an instance of the standard xed point paradigm for txl rules although its only purpose is to invoke the other rules main is a rule rather than a function because we expect it to continue to look for more opportunities to transform its result after each application but unless we check that some thing was actually done on each application the rule will never halt since its replacement newprogram is a program and therefore matches its pattern to terminate the rule we use a deconstructor as an explicit xed point test deconstruct not newprogram program the deconstructor simply tests whether the set of rules has changed anything on each repeated application that is if the newprogram is exactly the same as the matched program if nothing has changed we are by denition at a xed point this rule is a complete generic paradigm for xed point application of any rule set only the set of rules applied in the constructor changes paradigm dependency sorting the rule immediatizedeclarations works by iteratively moving declarations over statements that do not depend on them in essence this is a dependency sort of the code the rule continues to move dec larations down until every declaration is immediately before the rst statement that uses its declared variable this could be done more eciently by moving declarations directly but our purpose here is to demonstrate as many paradigms as possible in the clearest and simplest way dependency sorting in this way is a common paradigm in txl and we will see it again in other solutions paradigm deep pattern match the dependency test uses another common paradigm in txl a deep deconstruct this is similar to the negated deconstruct used in the previous problem but this time we are not just interested in whether statement does not match something we are interested in whether it does not contain something deep deconstructs test for containment by specifying the type of the pattern they are looking for inside the bound variable and a pattern of that type to nd in this case we are looking to see if there is an instance of an identier type id exactly like the declared one bound to v paradigm one pass rules the immediatizedeclarations rule also demon strates another paradigm of txl the one pass rule if there are two dec larations in a row this rule will contually move them over one another never coming to a xed point for this reason the rule is marked as one pass using replace this means that the scope should be searched in linear fashion for instances of the pattern and replacements should not be directly reexamined for instances of the pattern in this case if we move a declaration over another we don t try to move the other over it again because we move on to the next sequence of statements in one pass rather than recursive fashion the second rule in this transformation localizedeclarations looks for in stances of a declaration that has been moved to immediately before a compound statement such as if while for and checks to see whether it can be moved inside the statement scope the rule uses all of the paradigms outlined above it is one pass replace so that it does not try the same case twice and it uses deep pattern matching both to get the declared identier v from the declaration and to check that the following statements morestatements do not depend on the declaration we want to move inside by searching for uses of v in them a new use of deconstruct in this rule is the deep deconstruct of compound statement which is simply used to check that we actually have an inner scope in the statement in which to move the declaration paradigm multiple transformation cases the replacement of this rule demon strates another paradigm the programming of cases in txl there are several dierent compound statements into which we can move the declaration while statements for statements then clauses and else clauses each one is slightly dierent and so they have dierent patterns and replacements in txl such multiple cases use one function for each case all applied to the same scope in essence this is the paradigm for case selection or if then else in txl application of one function for each case only one of the functions will match any particular instance of compoundstatement and the others that do not match will leave the scope untouched txl functions and rules are total that is they have a dened result the identify transformation when they do not match paradigm context dependent transformation rules in each case the declara tion to be inserted into the compoundstatement is passed into the function for the case using a rule parameter rule parameters allow us to carry context from outer scopes into rules that transform inner scopes and this is the paradigm for context dependent transformation in txl in this case we pass the declaration from the outer scope into the rule that transforms the inner scope the context carried in can be arbitrarily large or complex for example if the inner transformation rule wanted to change small things inside its scope but depended on global things we could pass a copy of the entire program into the rule outer context can also be passed arbitrarily deeply into subrules so if a small change deeply inside a sub sub subrule depended on something in the outer scope we could pass a copy all the way in 4 goto elimination the agship of all restructuring problems is goto elimination the inference of structured code such as while loops and nested if then else from spaghetti coded goto statements in legacy languages such as cobol in this example we imagine a dialect of til that has goto statements and infer equivalent while statements where possible figure gives the grammar for a dialect of til that adds goto statements and labels so that we can write programs like the one shown on the left below a our goal is to recognize and transform loop equivalent goto structures into their while loop form like the result b on the right factor an input number var n var f write input n please read n write the factors of n are f outer loop over potential factors factors if n then goto endfactors end inner loop over multiple instances of same factor multiples if n f f n then goto endmultiples end write f n n f goto multiples endmultiples f f goto factors endfactors factor an input number var n var f write input n please read n write the factors of n are f outer loop over potential factors while n do inner loop over multiple instances of same factor while n f f n do write f n n f end f f end a b an example txl solution to the problem of recognizing and transforming while equivalent goto structures is shown in figure the basic strategy is to catalogue the patterns of use we observe encode them as patterns and use one rule per pattern to replace them with their equivalent loop structures in practice we would rst run a goto normalization feature reduction transformation to reduce the number of cases the program presently recognizes two cases forward while structures which begin with an if statement guarding a goto and end with a goto back to the if statement and backward whiles which begin with a labelled statement and end with an if statement guarding a goto branching back to it by now most of the txl code will be looking pretty familiar however this example has two new paradigms to teach us the rst is the match of the pattern in the rule transformforwardwhile ideally we are looking for a pattern of the form replace statement label if c expression then goto label end statements statement goto follow statement rest statement paradigm matching a subsequence the trailing rest statement is necessary since we are matching a subsequence of an entire sequence if the pattern were to end without including the trailing sequence i e without rest then it would only match when the pattern appeared as the last statements in the sequence of statements which is not what we intend file tilgotos grm dialect of til that adds goto statements redefine statement end redefine define label statement end define define goto label nl end define allow for trailing labels define nl end define define label id end define add missing not operator to til redefine primary primary end redefine case structures of the form loop if cond then goto endloop end loopstatements goto loop endloop trailingstatements rule transformforwardwhile we re looking for a labelled if guarding a goto it could be the head of a loop replace statement label if c expression then goto label end rest statement if we have a goto back to the labelled if we have a guarded loop i e a while the skipping makes sure we look only in this statement sequence not deeper skipping statement deconstruct rest goto follow statement finalrest statement the body of the loop is the statements after the if and before the goto back construct loopbody statement rest truncategoto by file tilgotoelim txl goto elimination in til programs recognize and resolve while equivalent goto structures using the goto dialect of basic til include til grm include tilgotos grm preserve comments in this transformation pragma comment main program just applies the rules for cases we know how to transform function main replace program p program by p transformforwardwhile transformbackwardwhile end function while c do loopbody end follow finalrest end rule rule transformbackwardwhile similar to above for backward case end rule utility rule used by all cases function truncategoto label label skipping statement replace statement goto follow statement finalrest statement by nothing end function fig txl dialect grammar to add goto statements and labels to til and trans formation to eliminate gotos showing rst case only what is not so obvious is why we could not simply write the pattern above directly in the rule the reason again has to do with the denition of x which as we recall is recursively dened as x x or empty the pattern above is trying to match statement statement statement statement statement which can t be parsed using that denition no matter how we group it paradigm matching a gapped subsequence the txl paradigm to match such gapped sequences is the one used in the transformforwardwhile rule in it we rst match the head of the pattern we are looking for that is the leading if statement and the statements following it we then search in the statements following it for the trailing pattern the goto back and the ending forward label the trick of the paradigm is that we must not look inside the statements of the sequence because we want the trailing pattern to be in the same statement sequence this is achieved using a skipping deep deconstruct skipping statement deconstruct rest goto follow statement finalrest statement this deconstructor says that we only have a match if we can nd the goto back and the ending forward label without looking inside any of the statements in the sequence that is if they are both at the same level in the statement sequence itself skipping t limits a search to the parse tree nodes above any embedded t in our case above any statements so that the goto back is in the same sequence as the heading if statement completing the pattern we are looking for paradigm truncating the tail of a sequence the other new paradigm this example shows us is the truncation of a trailing subsequence achieved by the function truncategoto which removes everything from the goto on from the statements following the initial if statement the trick in this function is to look for the pattern heading the trailing subsequence we want to truncate and replacing it and the following items by an empty sequence once again we use the skipping notation since we don t want to accidentally match a similar instance in a deeper statement optimization problems source transformation tools are often used in source code optimization tasks of various kinds and txl is no exception in this section we attack some traditional source code optimizations observing the txl paradigms that support these kinds of tasks once again our examples are based on the tiny imperative language til and its extensions statement level code motion the rst example problem is on the border between restructuring and optimiza tion moving invariant assignments and computations out of while loops in the rst solution we simply look for assignment statements in while loops that are independent of the loop that is that don t change over the iterations of the loop for example in this loop the assignment to x does not depend on the loop and can be moved out var j var x var y var z j x z while j do y y j x z z j j end figure shows a solution to this problem for til programs the key to the solution is the function looplift which given a while loop and an assignment statement in it checks to see whether the assigned expression of the assignment contains only variables that are not assigned in the loop and that the assigned variable of the assignment is assigned exactly once in the loop if both these conditions are met then the assignment is moved out by putting a copy of it before the loop and deleting it from the loop the function uses a number of txl paradigms it begins by deconstructing the assignment statement it is passed to get its parts then uses the extract paradigm to get all of the variable references in the assigned expression both of these paradigms we have seen before the interesting new paradigm is the guarding of the transformation using where clauses we can only lift the assignment out if all the identifiers in its expression are not assigned in the loop where not loop assigns each idsinexpression and x itself is assigned only once deconstruct body x expression rest statement where not rest assigns x and the effect of it does not wrap around the loop construct precontext statement body deleteassignmentandrest x where not precontext refers x paradigm guarding a transformation with a complex condition where clauses guard the pattern match of a rule or function with conditions that are tested by a subrule or set of subrules if the where clause is positive i e has no not modier then the subrule must match its pattern for the rule to proceed if it is a where not as in these cases then it must not match its pattern paradigm condition rules the subrules used in a where clause are of a special kind called condition rules which have only a pattern and no replacement the pattern may be simple as in the assigns and refers subrules of this example which simply check to see if their parameter occurs in the context of their scope or they may be complex involving other deconstructors where clauses and sub rules in either case a condition subrule simply matches its pattern or not and the where clause using it succeeds or not depending on whether it matches if multiple subrules are used in the condition the where clause succeeds if any one of them matches and fails only if all do not match or conversely for where not succeeds only if none match file tilcodemotion txl lift independent til assignments outside of while loops based on the til grammar include til grm lift all independent assignments out of loops rule main find every loop replace statement while expn expression do body statement end rest statement get all the top level assignments in it construct allassignments statement body deletenonassignments make a copy of the loop to work on construct liftedloop statement while expn do body end only proceed if there are assignments left that can be lifted out the looplift form tests if the looplift rule can be matched each allassignments tests this for any of the top level internal assignments where liftedloop looplift body each allassignments if the above guard succeeds some can be moved out so go ahead and move them replacing the original loop with the result by liftedloop looplift body each allassignments rest end rule attempt to lift a given assignment outside the loop function looplift body statement assignment statement deconstruct assignment x id e expression extract a list of all the identifiers used in the expression construct idsinexpression id e replace the loop and its contents replace statement loop statement we can only lift the assignment out if all the identifiers in its expression are not assigned in the loop where not loop assigns each idsinexpression and x itself is assigned only once deconstruct body x expression rest statement where not rest assigns x and the effect of it does not wrap around the loop construct precontext statement body deleteassignmentandrest x where not precontext refers x now lift out the assignment by assignment loop deleteassignment assignment end function utility rules used above delete a given assignment from a scope function deleteassignment assignment statement replace statement assignment rest statement by rest end function delete all non assignments in a scope rule deletenonassignments replace statement s statement rest statement deconstruct not s by rest end rule delete everything in a scope from the first assignment to x on function deleteassignmentandrest x id replace statement x e expression rest statement by nada end function does a scope assign to the identifier function assigns id id match id expn expression end function does a scope refer to the identifier function refers id id match id id end function fig txl transformation to lift independent assignments out of while loops paradigm each element of a sequence the rst where condition in the looplift function also uses another paradigm the each modier where not loop assigns each idsinexpression each takes a sequence of type x for any type x and calls the subrule once with each element of the sequence as parameter so for example if idsinex pression is bound to the sequence of identiers a b c then where not loop assigns each idsinexpression means where not loop assigns a assigns b assigns c and the guard succeeds only if none of the assigns calls matches its pattern this is a common txl paradigm for checking multiple items at once the main rule in this example simply nds every while loop extracts all the assignment statements in it by making a copy of the statements in the loop body and deleting those that are not assignments and then calls looplift with each to try to move each of them outside the loop rather than use the xed point paradigm this main rule uses a where clause as a guard to check whether there are any assignments to move in advance to do this it actually uses the looplift function itself to check by converting it to a condition using where liftedloop looplift body each allassignments paradigm using a transformation rule as a condition looplift means that looplift should not do any replacement rather it should act as a condition rule simply checking whether its complex pattern matches or not thus the where clause above simply checks whether looplift will succeed for any of the assignments and the rule only proceeds if at least one will match 2 common subexpression elimination common subexpression elimination is a traditional optimization transformation that searches for repeated subexpressions whose value cannot have changed be tween two instances the idea is to introduce a new temporary variable to hold the value of the subexpression and replace all instances with a reference to the temporary for example if the input contains the code on the left a below then the output should be the code b shown on the right var a var b var a var b read a b a a var i i c a a a read a var t t a a b t var i i c t b a txl solution to this problem for til programs is shown in figure the solution uses a number of new paradigms for us to look at to begin the program uses agile parsing to modify the til grammar in two ways paradigm grammatical form abstraction first it overrides the denition of statement to gather all compound statements into one statement type this file tilcommonsubexp txl recognize and optimize common subexpressions based on the til base grammar include til grm preserve comments pragma comment override to abstract compound statements redefine statement end redefine define for_statement end define allow statements to be attributed so we don t mistake one we ve generated for one we need to process redefine statement statement attr new end redefine main rule rule main replace statement statement ss statement don t process statements we generated deconstruct not attr new new we re looking for an expression deconstruct expression e expression that is nontrivial deconstruct not e primary and repeated deconstruct expression ss e see if we can abstract it checks if variables assigned between where ss replaceexpncopies e t if so generate a new temp name construct t id temp declare it assign it the expression and replace instances with it by recursively replace copies of a given expression with a given temp variable id provided the variables used in the expression are not assigned in between function replaceexpncopies statement e expression t id construct eids id e if the previous statement did not assign any of the variables in the expression where not assigns each eids then we can continue to substitute the temporary variable for the expression in the next statement replace statement s statement ss statement as long as it isn t a compound statement that internally assigs one of the variables in the expression where not all s assignsone eids iscompoundstatement by s replaceexpn e t ss replaceexpncopies s e t end function check to see if a statement assigns any of a list of variables function assignsone eids id match statement s statement where s assigns each eids end function function assigns id id match statement id expression end function function iscompoundstatement match statement end function rule replaceexpn e expression t id replace expression e by t end rule var t new t e new replaceexpn e t ss replaceexpncopies e t end rule fig txl transformation to recognize and optimize common subexpressions redenition takes advantage of txl programmed parsing to prefer that if while and for statements be parsed as compound statement the original forms are still in the denition of statement denoted by but since our new form appears rst all of them will be parsed as compound statement this paradigm is often used to gather forms so that we can use one rule to target all of the forms at once rather than having several rules for the dierent grammatical types paradigm marking using attributes the second technique used here is gram mar attributes denoted by the attr modier txl grammar attributes denote optional parts of the grammar that will not appear in the unparsed output text they can be of any grammatical type including complex types with lots of infor mation in them in this case the attribute is simply the identier new and it is added to allow us to mark statements that are inserted by the transformation so that we don t mistake them for a statement to be processed marking things that have been generated or already processed using attributes is a common technique in txl and is often the easiest way to distinguish things that have been processed from those that have not the new attributed form is recursive allowing any statement to be marked as new the main rule nds any statement containing a nontrivial expression de termined by deconstructing it to ensure that it is not simply a primary it then deconstructs the following statements to determine if the expression is re peated in them if so then it uses the conditional guard paradigm to check that the repetition will be legally transformable replaceexpncopies a new unique temporary name of the form is then created using the unique iden tier paradigm and nally statements are generated to declare and assign the expression to the new temporary this is where the new attribute comes in by marking the newly generated statements with the new attribute we are sure that they will not be matched by the main rule and reprocessed the remainder of the replacement copies the original statement and following statements substituting the new temporary name for the expression in the original statement replaceexpn e t and any subsequent uses in following statements replaceexpncopies e t paradigm tail recursive continuation rule replaceexpncopies figure introduces us to another new paradigm continuing a transformation through a sequence as long as some condition holds in this case we can continue to sub stitute the temporary name for the common expression as long as the variables in the expression are not assigned to in txl such situations are encoded as a tail recursive function which pro cesses each statement one by one checking that the conditions still hold until it fails and terminates the recursion in each recursion we pass the previous state ment as parameter and rst check that it has not assigned any of the identiers used in the expression again using the where not each paradigm of the previous problem we then match the next statement in the sequence and check that it is not a compound statement that assigns any of the identiers in the expression paradigm guarding with multiple conditions this check uses a new paradigm where not all as we ve seen in previous paradigms where clauses normally check whether any of the condition rules matches when all is specied the check is whether all of the condition rules match thus the where clause here where not all s assignsone eids iscompoundstatement checks whether it is both the case that one of the identiers used in the expression is assigned by the statement and that the statement is a compound statement in which case our simple algorithm choose to give up and stop if the check succeeds and either the statement is not a compound statement or does not assign any of the variables in the original expression then instances of the expression are substituted in the matched statement and we recursively move on to the next one 3 constant folding constant folding or optimizing by recognizing and precomputing compile time known subexpressions is another traditional optimization technique in essence the solution is a partial evaluation of the program replacing named constants by their values and interpreting resulting operations on those values thus a constant folding algorithm must have rules to evaluate much of the expression sublanguage of the target language the solution for til figure is in two parts recognition and substitution of constant assignments to variables that are not destroyed and interpretation of constant subexpressions of course these two processes interact because sub stitution of constant values for variables yields more constant subexpressions to evaluate and evaluation of constant subexpressions yields more constant values for variables so in the main rule we see the now familiar paradigm for a xed point continuing until neither rule changes anything the propagateconstants rule handles the rst half of the problem searching for assignments of constant values to variables e g x that are not destroyed by a subsequent assignment in the same statement sequence the two deep deconstructs of rest are the key to the rule the rst one ensures that the following statements do not subsequently assign to the variable destroying its constant value the second one makes sure that there is a reference to the variable to substitute when both conditions are met the value is substituted for all references to the variable in the following statements the second half of the transformation is the interpretation of constant subex pressions possibly created by the rst half substituting constant variable values the rule foldconstantexpressions simply applies a set of rules each of which knows how to evaluate an operator with constant operands in addition to the simple cases a number of special cases such as multiplying any expression by zero are also handled foldconstantexpressions continues applying the set of evaluation rules until none of them changes anything and a xed point is reached file tilconst txl constant propagation and folding for til begin with the til base grammar include til grm preserve comments in this transformation pragma comment main function rule main replace program p program construct newp program p propagateconstants foldconstantexpressions deconstruct not newp p by newp end rule constant folding find and evaluate constant expressions rule foldconstantexpressions replace expression e expression construct newe expression e generic folding of pure constant expressions resolveaddition resolvesubtraction resolvemultiplication resolvedivision other special cases resolvemultiply1left resolveparentheses continue until we don t find anything to fold deconstruct not newe constant propagation find each constant assignment to a variable and if it is not assigned again then replace references with the constant by end rule e newe rule propagateconstants replace statement var id const literal rest statement deconstruct not statement rest var expression deconstruct primary rest var utility rules to do the arithmetic rule resolveaddition replace expression integernumber integernumber by end rule by end rule var const rest replaceexpn var const rule resolvesubtraction replace expression integernumber integernumber rule replaceexpn var id const literal replace primary var by end rule by end rule const other operator folding rules fig txl transformation to fold constant subexpressions 4 statement folding our last optimization example is statement folding the elimination of statements that cannot be reached because the conditions that guard them are known at compile time for example when an if condition is known to be true or false in practice constant folding and statement folding go together constant folding precomputes conditional expressions some of which are then known to be true or false allowing for statement folding these problems are closely related to condi tional compilation transformations to implement preprocessors and conditional compilation are essentially the same as constant and statement folding figure shows a txl solution to the statement folding problem for til if and while statements with known conditions in this case the main rule is a fig txl transformation to fold known if statements function since none of the rules changes anything that may create new instances of the others and thus the xed point paradigm is not needed paradigm handling optional parts in the false if condition case rule foldfal seifstatements there is a new paradigm used to get the falsestatements from the else clause of the if statement beginning with an empty sequence using the empty variable a separate function is used to get the falsestatements from the else clause the reason for this construction is that the else statement is optional there may not be one so beginning with the assumption there is none i e the empty sequence we used the getelsestatements function to both check if there is one by deconstructing the elseclause and if so to replace the empty sequence by the falsestatements paradigm creating output comments both cases illustrate another txl paradigm the creation of target language comments besides explicitly marking identiers intended to be literal output quoting of items in txl marks some thing to be lexically interpreted in the target language rather than txl thus a target language comment can be created in a txl replacement simply by pre quoting it figure rule foldfalseifstmts this can be handy when marking sections of code that have been transformed in output now a manager object is printed as manager name salary hireday bonus the tostring method is ubiquitous for an important reason whenever an object is concatenated with a string by the operator the java compiler automatically invokes the tostring method to obtain a string representation of the object for example click here to view code image point p new point string message the current position is p automatically invokes p tostring tip instead of writing x tostring you can write x this statement concatenates the empty string with the string representation of x that is exactly x tostring unlike tostring this statement even works if x is of primitive type if x is any object and you call system out println x then the println method simply calls x tostring and prints the resulting string the object class defines the tostring method to print the class name and the hash code of the object for example the call system out println system out produces an output that looks like this java io printstream the reason is that the implementor of the printstream class didn t bother to override the tostring method caution annoyingly arrays inherit the tostring method from object with the added twist that the array type is printed in an archaic format for example click here to view code image int luckynumbers string luckynumbers yields the string i the prefix i denotes an array of integers the remedy is to call the static arrays tostring method instead the code string arrays tostring luckynumbers yields the string to correctly print multidimensional arrays that is arrays of arrays use arrays deeptostring the tostring method is a great tool for logging many classes in the standard class library define the tostring method so that you can get useful information about the state of an object this is particularly useful in logging messages like this system out println current position position as we explain in chapter an even better solution is to use an object of the logger class and call logger global info current position position tip we strongly recommend that you add a tostring method to each class that you write you as well as other programmers who use your classes will be grateful for the logging support the program in listing implements the equals hashcode and tostring methods for the classes employee listing and manager listing listing equals equalstest java click here to view code image package equals this program demonstrates the equals method version author cay horstmann public class equalstest public static void main string args employee new employee alice adams employee employee new employee alice adams employee bob new employee bob brandson system out println alice2 system out println system out println equals equals system out println equals bob equals bob system out println bob tostring bob manager carl new manager carl cracker manager boss new manager carl cracker boss setbonus system out println boss tostring boss system out println carl equals boss carl equals boss system out println hashcode hashcode system out println hashcode hashcode system out println bob hashcode bob hashcode system out println carl hashcode carl hashcode listing equals employee java click here to view code image package equals import java util date import java util gregoriancalendar import java util objects public class employee private string name private double salary private date hireday public employee string n double int year int month int day name n salary gregoriancalendar calendar new gregoriancalendar year month day hireday calendar gettime public string getname return name public double getsalary return salary public date gethireday return hireday public void raisesalary double bypercent double raise salary bypercent salary raise public boolean equals object otherobject a quick test to see if the objects are identical if this otherobject return true must return false if the explicit parameter is null if otherobject null return false if the classes don t match they can t be equal if getclass otherobject getclass return false now we know otherobject is a non null employee employee other employee otherobject test whether the fields have identical values return objects equals name other name salary other salary objects equals hireday other hireday public int hashcode return objects hash name salary hireday public string tostring return getclass getname name name salary salary hireday hireday listing equals manager java click here to view code image package equals public class manager extends employee private double bonus public manager string n double int year int month int day super n year month day bonus public double getsalary double basesalary super getsalary return basesalary bonus public void setbonus double b bonus b public boolean equals object otherobject if super equals otherobject return false manager other manager otherobject super equals checked that this and other belong to the same class return bonus other bonus public int hashcode return super hashcode new double bonus hashcode public string tostring return super tostring bonus bonus java lang object class getclass returns a class object that contains information about the object as you will see later in this chapter java has a runtime representation for classes that is encapsulated in the class class boolean equals object otherobject compares two objects for equality returns true if the objects point to the same area of memory and false otherwise you should override this method in your own classes string tostring returns a string that represents the value of this object you should override this method in your own classes java lang class string getname returns the name of this class class getsuperclass returns the superclass of this class as a class object generic array lists in many programming languages in particular in c you have to fix the sizes of all arrays at compile time programmers hate this because it forces them into uncomfortable trade offs how many employees will be in a department surely no more than what if there is a humongous department with employees do we want to waste entries for every department with just employees in java the situation is much better you can set the size of an array at runtime int actualsize employee staff new employee actualsize of course this code does not completely solve the problem of dynamically modifying arrays at runtime once you set the array size you cannot change it easily instead in java you can deal with this common situation by using another java class called arraylist the arraylist class is similar to an array but it automatically adjusts its capacity as you add and remove elements without any additional code arraylist is a generic class with a type parameter to specify the type of the element objects that the array list holds you append a class name enclosed in angle brackets such as arraylist employee you will see in chapter how to define your own generic class but you don t need to know any of those technicalities to use the arraylist type here we declare and construct an array list that holds employee objects arraylist employee staff new arraylist employee it is a bit tedious that the type parameter employee is used on both sides as of java you can omit the type parameter on the right hand side arraylist employee staff new arraylist this is called the diamond syntax because the empty brackets resemble a diamond use the diamond syntax together with the new operator the compiler checks what happens to the new value if it is assigned to a variable passed into a method or returned from a method then the compiler checks the generic type of the variable parameter or method it then places that type into the in our example the new arraylist is assigned to a variable of type arraylist employee therefore the generic type is employee note before java se there were no generic classes instead there was a single arraylist class a one size fits all collection that holds elements of type object you can still use arraylist without a suffix it is considered a raw type with the type parameter erased note in even older versions of java programmers used the vector class for dynamic arrays however the arraylist class is more efficient and there is no longer any good reason to use the vector class use the add method to add new elements to an array list for example here is how you populate an array list with employee objects click here to view code image staff add new employee harry hacker staff add new employee tony tester the array list manages an internal array of object references eventually that array will run out of space this is where array lists work their magic if you call add and the internal array is full the array list automatically creates a bigger array and copies all the objects from the smaller to the bigger array if you already know or have a good guess how many elements you want to store call the ensurecapacity method before filling the array list staff ensurecapacity that call allocates an internal array of objects then the first calls to add will not involve any costly reallocation you can also pass an initial capacity to the arraylist constructor arraylist employee staff new arraylist caution allocating an array list as new arraylist capacity is is not the same as allocating a new array as new employee size is there is an important distinction between the capacity of an array list and the size of an array if you allocate an array with entries then the array has slots ready for use an array list with a capacity of elements has the potential of holding elements and in fact more than at the cost of additional reallocations but at the beginning even after its initial construction an array list holds no elements at all the size method returns the actual number of elements in the array list for example staff size returns the current number of elements in the staff array list this is the equivalent of a length for an array a once you are reasonably sure that the array list is at its permanent size you can call the trimtosize method this method adjusts the size of the memory block to use exactly as much storage space as is required to hold the current number of elements the garbage collector will reclaim any excess memory once you trim the size of an array list adding new elements will move the block again which takes time you should only use trimtosize when you are sure you won t add any more elements to the array list c note the arraylist class is similar to the c vector template both arraylist and vector are generic types but the c vector template overloads the operator for convenient element access java does not have operator overloading so it must use explicit method calls instead moreover c vectors are copied by value if a and b are two vectors then the assignment a b makes a into a new vector with the same length as b and all elements are copied from b to a the same assignment in java makes both a and b refer to the same array list java util arraylist t arraylist t constructs an empty array list arraylist t int initialcapacity constructs an empty array list with the specified capacity boolean add t obj appends an element at the end of the array list always returns true int size returns the number of elements currently stored in the array list of course this is never larger than the array list capacity void ensurecapacity int capacity ensures that the array list has the capacity to store the given number of elements without reallocating its internal storage array void trimtosize reduces the storage capacity of the array list to its current size accessing array list elements unfortunately nothing comes for free the automatic growth convenience that array lists give requires a more complicated syntax for accessing the elements the reason is that the arraylist class is not a part of the java programming language it is just a utility class programmed by someone and supplied in the standard library instead of using the pleasant syntax to access or change the element of an array you use the get and set methods for example to set the ith element you use staff set i harry this is equivalent to a i harry for an array a as with arrays the index values are zero based caution do not call list set i x until the size of the array list is larger than i for example the following code is wrong click here to view code image arraylist employee list new arraylist capacity size list set x no element yet use the add method instead of set to fill up an array and use set only to replace a previously added element to get an array list element use employee e staff get i this is equivalent to employee e a i note when there were no generic classes the get method of the raw arraylist class had no choice but to return an object consequently callers of get had to cast the returned value to the desired type employee e employee staff get i the raw arraylist is also a bit dangerous its add and set methods accept objects of any type a call staff set i new date compiles without so much as a warning and you run into grief only when you retrieve the object and try to cast it if you use an arraylist employee instead the compiler will detect this error you can sometimes get the best of both worlds flexible growth and convenient element access with the following trick first make an array list and add all the elements arraylist x list new arraylist while x list add x when you are done use the toarray method to copy the elements into an array x a new x list size list toarray a sometimes you need to add elements in the middle of an array list use the add method with an index parameter int n staff size staff add n e the elements at locations n and above are shifted up to make room for the new entry if the new size of the array list after the insertion exceeds the capacity the array list reallocates its storage array similarly you can remove an element from the middle of an array list employee e staff remove n the elements located above it are copied down and the size of the array is reduced by one inserting and removing elements is not terribly efficient it is probably not worth worrying about for small array lists but if you store many elements and frequently insert and remove in the middle of a collection consider using a linked list instead we explain how to program with linked lists in chapter you can use the for each loop to traverse the contents of an array list for employee e staff do something with e this loop has the same effect as for int i i staff size i employee e staff get i do something with e listing is a modification of the employeetest program of chapter the employee array is replaced by an arraylist employee note the following changes you don t have to specify the array size you use add to add as many elements as you like you use size instead of length to count the number of elements you use a get i instead of a i to access an element listing arraylist arraylisttest java click here to view code image package arraylist import java util this program demonstrates the arraylist class version author cay horstmann public class arraylisttest public static void main string args fill the staff array list with three employee objects arraylist employee staff new arraylist staff add new employee carl cracker staff add new employee harry hacker staff add new employee tony tester raise everyone salary by for employee e staff e raisesalary print out information about all employee objects for employee e staff system out println name e getname salary e getsalary hireday e gethireday java util arraylist t void set int index t obj puts a value in the array list at the specified index overwriting the previous contents t get int index gets the value stored at a specified index void add int index t obj shifts up elements to insert an element t remove int index removes an element and shifts down all elements above it the removed element is returned compatibility between typed and raw array lists in your own code you will always want to use type parameters for added safety in this section you will see how to interoperate with legacy code that does not use type parameters suppose you have the following legacy class click here to view code image public class employeedb public void update arraylist list public arraylist find string query you can pass a typed array list to the update method without any casts arraylist employee staff employeedb update staff the staff object is simply passed to the update method caution even though you get no error or warning from the compiler this call is not completely safe the update method might add elements into the array list that are not of type employee when these elements are retrieved an exception occurs this sounds scary but if you think about it the behavior is simply as it was before generics were added to java the integrity of the virtual machine is never jeopardized in this situation you do not lose security but you also do not benefit from the compile time checks conversely when you assign a raw arraylist to a typed one you get a warning arraylist employee result employeedb find query yields warning note to see the text of the warning compile with the option xlint unchecked using a cast does not make the warning go away click here to view code image arraylist employee result arraylist employee employeedb find query yields another warning instead you get a different warning telling you that the cast is misleading this is the consequence of a somewhat unfortunate limitation of generic types in java for compatibility the compiler translates all typed array lists into raw arraylist objects after checking that the type rules were not violated in a running program all array lists are the same there are no type parameters in the virtual machine thus the casts arraylist and arraylist employee carry out identical runtime checks there isn t much you can do about that situation when you interact with legacy code study the compiler warnings and satisfy yourself that the warnings are not serious once you are satisfied you can tag the variable that receives the cast with the suppresswarnings unchecked annotation like this click here to view code image suppresswarnings unchecked arraylist employee result arraylist employee employeedb find query yields another warning object wrappers and autoboxing occasionally you need to convert a primitive type like int to an object all primitive types have class counterparts for example a class integer corresponds to the primitive type int these kinds of classes are usually called wrappers the wrapper classes have obvious names integer long float double short byte character void and boolean the first six inherit from the common superclass number the wrapper classes are immutable you cannot change a wrapped value after the wrapper has been constructed they are also final so you cannot subclass them suppose we want an array list of integers unfortunately the type parameter inside the angle brackets cannot be a primitive type it is not possible to form an arraylist int here the integer wrapper class comes in it is ok to declare an array list of integer objects arraylist integer list new arraylist caution an arraylist integer is far less efficient than an int array because each value is separately wrapped inside an object you would only want to use this construct for small collections when programmer convenience is more important than efficiency another java se innovation makes it easy to add and get array elements the call list add is automatically translated to list add integer valueof this conversion is called autoboxing note you might think that autowrapping would be more consistent but the boxing metaphor was taken from c conversely when you assign an integer object to an int value it is automatically unboxed that is the compiler translates int n list get i into int n list get i intvalue automatic boxing and unboxing even works with arithmetic expressions for example you can apply the increment operator to a wrapper reference integer n n the compiler automatically inserts instructions to unbox the object increment the resulting value and box it back in most cases you get the illusion that the primitive types and their wrappers are one and the same there is just one point in which they differ considerably identity as you know the operator applied to wrapper objects only tests whether the objects have identical memory locations the following comparison would therefore probably fail integer a integer b if a b however a java implementation may if it chooses wrap commonly occurring values into identical objects and thus the comparison might succeed this ambiguity is not what you want the remedy is to call the equals method when comparing wrapper objects note the autoboxing specification requires that boolean byte char short and int between and are wrapped into fixed objects for example if a and b had been initialized with in the preceding example then the comparison would have had to succeed finally let us emphasize that boxing and unboxing is a courtesy of the compiler not the virtual machine the compiler inserts the necessary calls when it generates the bytecodes of a class the virtual machine simply executes those bytecodes you will often see the number wrappers for another reason the designers of java found the wrappers a convenient place to put certain basic methods such as those for converting strings of digits to numbers to convert a string to an integer use the following statement int x integer parseint this has nothing to do with integer objects parseint is a static method but the integer class was a good place to put it the api notes show some of the more important methods of the integer class the other number classes implement corresponding methods caution some people think that the wrapper classes can be used to implement methods that can modify numeric parameters however that is not correct recall from chapter that it is impossible to write a java method that increments an integer parameter because parameters to java methods are always passed by value click here to view code image public static void triple int x won t work x x modifies local variable could we overcome this by using an integer instead of an int click here to view code image public static void triple integer x won t work the problem is that integer objects are immutable the information contained inside the wrapper can t change you cannot use these wrapper classes to create a method that modifies numeric parameters if you do want to write a method to change numeric parameters you can use one of the holder types defined in the org omg corba package intholder booleanholder and so on each holder type has a public field value through which you can access the stored value public static void triple intholder x x value x value java lang integer int intvalue returns the value of this integer object as an int overrides the intvalue method in the number class static string tostring int i returns a new string object representing the number i in base static string tostring int i int radix lets you return a representation of the number i in the base specified by the radix parameter static int parseint string static int parseint string int radix returns the integer whose digits are contained in the string the string must represent an integer in base for the first method or in the base given by the radix parameter for the second method static integer valueof string static integer valueof string int radix returns a new integer object initialized to the integer whose digits are contained in the string the string must represent an integer in base for the first method or in the base given by the radix parameter for the second method java text numberformat number parse string returns the numeric value assuming the specified string represents a number methods with a variable number of parameters before java se every java method had a fixed number of parameters however it is now possible to provide methods that can be called with a variable number of parameters these are sometimes called varargs methods you have already seen such a method printf for example the calls system out printf d n and system out printf d n widgets both call the same method even though one call has two parameters and the other has three the printf method is defined like this click here to view code image public class printstream public printstream printf string fmt object args return format fmt args here the ellipsis is part of the java code it denotes that the method can receive an arbitrary number of objects in addition to the fmt parameter the printf method actually receives two parameters the format string and an object array that holds all other parameters if the caller supplies integers or other primitive type values autoboxing turns them into objects it now faces the unenviable task of scanning the fmt string and matching up the ith format specifier with the value args i in other words for the implementor of printf the object parameter type is exactly the same as object the compiler needs to transform each call to printf bundling the parameters into an array and autoboxing as necessary system out printf d new object new integer n widgets you can define your own methods with variable parameters and you can specify any type for the parameters even a primitive type here is a simple example a function that computes the maximum of a variable number of values click here to view code image public static double max double values double largest double for double v values if v largest largest v return largest simply call the function like this double m max the compiler passes a new double to the max function note it is legal to pass an array as the last parameter of a method with variable parameters for example system out printf d new object new integer widgets therefore you can redefine an existing function whose last parameter is an array to a method with variable parameters without breaking any existing code for example messageformat format was enhanced in this way in java se if you like you can even declare the main method as public static void main string args enumeration classes you saw in chapter how to define enumerated types here is a typical example public enum size small medium large the type defined by this declaration is actually a class the class has exactly four instances it is not possible to construct new objects therefore you never need to use equals for values of enumerated types simply use to compare them you can if you like add constructors methods and fields to an enumerated type of course the constructors are only invoked when the enumerated constants are constructed here is an example click here to view code image public enum size small s medium m large l xl private string abbreviation private size string abbreviation this abbreviation abbreviation public string getabbreviation return abbreviation all enumerated types are subclasses of the class enum they inherit a number of methods from that class the most useful one is tostring which returns the name of the enumerated constant for example size small tostring returns the string small the converse of tostring is the static valueof method for example the statement size enum valueof size class small sets to size small each enumerated type has a static values method that returns an array of all values of the enumeration for example the call size values size values returns the array with elements size small size medium size large and size t he ordinal method yields the position of an enumerated constant in the enum declaration counting from zero for example size medium ordinal returns the short program in listing demonstrates how to work with enumerated types note the enum class has a type parameter that we have ignored for simplicity for example the enumerated type size actually extends enum size the type parameter is used in the compareto method we discuss the compareto method in chapter and type parameters in chapter java lang enum e static enum valueof class enumclass string name returns the enumerated constant of the given class with the given name string tostring returns the name of this enumerated constant int ordinal returns the zero based position of this enumerated constant in the enum declaration int compareto e other returns a negative integer if this enumerated constant comes before other zero if this other and a positive integer otherwise the ordering of the constants is given by the enum declaration listing enums enumtest java click here to view code image package enums import java util this program demonstrates enumerated types version author cay horstmann public class enumtest public static void main string args scanner in new scanner system in system out print enter a size small medium large string input in next touppercase size size enum valueof size class input system out println size size system out println abbreviation size getabbreviation if size size system out println good job you paid attention to the enum size small s medium m large l xl private size string abbreviation this abbreviation abbreviation public string getabbreviation return abbreviation private string abbreviation reflection t he reflection library gives you a very rich and elaborate toolset to write programs that manipulate java code dynamically this feature is heavily used in javabeans the component architecture for java see volume ii for more on javabeans using reflection java can support tools like those to which users of visual basic have grown accustomed in particular when new classes are added at design time or runtime rapid application development tools can dynamically inquire about the capabilities of these classes a program that can analyze the capabilities of classes is called reflective the reflection mechanism is extremely powerful as the next sections show you can use it to analyze the capabilities of classes at runtime inspect objects at runtime for example to write a single tostring method that works for all classes implement generic array manipulation code and take advantage of method objects that work just like function pointers in languages such as c reflection is a powerful and complex mechanism however it is of interest mainly to tool builders not application programmers if you are interested in programming applications rather than tools for other java programmers you can safely skip the remainder of this chapter and return to it later the class class while your program is running the java runtime system always maintains what is called runtime type identification on all objects this information keeps track of the class to which each object belongs runtime type information is used by the virtual machine to select the correct methods to execute however you can also access this information by working with a special java class the class that holds this information is called somewhat confusingly class the getclass method in the object class returns an instance of class type employee e class cl e getclass just like an employee object describes the properties of a particular employee a class object describes the properties of a particular class probably the most commonly used method of class is getname this returns the name of the class for example the statement system out println e getclass getname e getname prints employee harry hacker if e is an employee or manager harry hacker if e is a manager if the class is in a package the package name is part of the class name click here to view code image date d new date class cl d getclass string name cl getname name is set to java util date you can obtain a class object corresponding to a class name by using the static forname method string classname java util date class cl class forname classname use this method if the class name is stored in a string that varies at runtime this works if classname is the name of a class or interface otherwise the forname method throws a checked exception see section a primer on catching exceptions on p for how to supply an exception handler whenever you use this method tip at startup the class containing your main method is loaded it loads all classes that it needs each of those loaded classes loads the classes that it needs and so on that can take a long time for a big application frustrating the user you can give users of your program the illusion of a faster start with the following trick make sure that the class containing the main method does not explicitly refer to other classes in it display a splash screen then manually force the loading of other classes by calling class forname a third method for obtaining an object of type class is a convenient shorthand if t is any java type then t class is the matching class object for example click here to view code image class date class if you import java util class int class class double class note that a class object really describes a type which may or may not be a class for example int is not a class but int class is nevertheless an object of type class note as of java se the class class is parameterized for example employee class is of type class employee we are not dwelling on this issue because it would further complicate an already abstract concept for most practical purposes you can ignore the type parameter and work with the raw class type see chapter for more information on this issue caution for historical reasons the getname method returns somewhat strange names for array types double class getname returns ljava lang double int class getname returns i the virtual machine manages a unique class object for each type therefore you can use the operator to compare class objects for example if e getclass employee class another example of a useful method is one that lets you create an instance of a class on the fly this method is called naturally enough newinstance for example e getclass newinstance creates a new instance of the same class type as e the newinstance method calls the no argument constructor to initialize the newly created object an exception is thrown if the class does not have a no argument constructor a combination of forname and newinstance lets you create an object from a class name stored in a string string java util date object m class forname newinstance note if you need to provide parameters for the constructor of a class you want to create by name in this manner you can t use the above statements instead you must use the newinstance method in the constructor class c note the newinstance method corresponds to the idiom of a virtual constructor in c however virtual constructors in c are not a language feature but just an idiom that needs to be supported by a specialized library the class class is similar to the class in c and the getclass method is equivalent to the typeid operator the java class is quite a bit more versatile than though the c can only reveal a string with the name of the type not create new objects of that type a primer on catching exceptions we cover exception handling fully in chapter but in the meantime you will occasionally encounter methods that threaten to throw exceptions when an error occurs at runtime a program can throw an exception throwing an exception is more flexible than terminating the program because you can provide a handler that catches the exception and deals with it if you don t provide a handler the program still terminates and prints a message to the console giving the type of the exception you may have already seen exception reports when you accidentally used a null reference or overstepped the bounds of an array there are two kinds of exceptions unchecked exceptions and checked exceptions with checked exceptions the compiler checks that you provide a handler however many common exceptions such as accessing a null reference are unchecked the compiler does not check whether you provided a handler for these errors after all you should spend your mental energy on avoiding these mistakes rather than coding handlers for them but not all errors are avoidable if an exception can occur despite your best efforts then the compiler insists that you provide a handler the class forname method is an example of a method that throws a checked exception in chapter you will see several exception handling strategies for now we just show you the simplest handler implementation place one or more statements that might throw checked exceptions inside a try block then provide the handler code in the catch clause try statements that might throw exceptions c atch exception e handler action here is an example click here to view code image try string name get class name class cl class forname name might throw exception do something with cl c atch exception e e printstacktrace if the class name doesn t exist the remainder of the code in the try block is skipped and the program enters the catch clause here we print a stack trace by using the printstacktrace method of the throwable class throwable is the superclass of the exception class if none of the methods in the try block throws an exception the handler code in the catch clause is skipped you only need to supply an exception handler for checked exceptions it is easy to find out which methods throw checked exceptions the compiler will complain whenever you call a method that threatens to throw a checked exception and you don t supply a handler java lang class static class forname string classname returns the class object representing the class with name classname object newinstance returns a new instance of this class java lang reflect constructor object newinstance object args constructs a new instance of the constructor declaring class java lang throwable void printstacktrace prints the throwable object and the stack trace to the standard error stream using reflection to analyze the capabilities of classes here is a brief overview of the most important parts of the reflection mechanism for letting you examine the structure of a class the three classes field method and constructor in the java lang reflect package describe the fields methods and constructors of a class respectively all three classes have a method called getname that returns the name of the item the field class has a method gettype that returns an object again of type class that describes the field type the method and constructor classes have methods to report the types of the parameters and the method class also reports the return type all three of these classes also have a method called getmodifiers that returns an integer with various bits turned on and off that describes the modifiers used such as public and static you can then use the static methods in the modifier class in the java lang reflect package to analyze the integer that getmodifiers returns use methods like ispublic isprivate or isfinal in the modifier class to tell whether a method or constructor was public private or final all you have to do is have the appropriate method in the modifier class work on the integer that getmodifiers returns you can also use the modifier tostring method to print the modifiers the getfields getmethods and getconstructors methods of the class class return arrays of the public fields methods and constructors that the class supports this includes public members of superclasses the getdeclaredfields getdeclaredmethods and getdeclaredconstructors methods of the class class return arrays consisting of all fields methods and constructors that are declared in the class this includes private package and protected members but not members of superclasses listing shows you how to print out all information about a class the program prompts you for the name of a class and writes out the signatures of all methods and constructors as well as the names of all instance fields of a class for example if you enter java lang double the program prints click here to view code image public class java lang double extends java lang number public java lang double java lang string public java lang double double public int hashcode public int compareto java lang object public int compareto java lang double public boolean equals java lang object public java lang string tostring public static java lang string tostring double public static java lang double valueof java lang string public static boolean isnan double public boolean isnan public static boolean isinfinite double public boolean isinfinite public byte bytevalue public short shortvalue public int intvalue public long longvalue public float floatvalue public double doublevalue public static double parsedouble java lang string public static native long doubletolongbits double public static native long doubletorawlongbits double public static native double longbitstodouble long public static final double public static final double public static final double nan public static final double public static final double public static final java lang class type private double value private static final long serialversionuid listing reflection reflectiontest java click here to view code image package reflection import java util import java lang reflect this program uses reflection to print all features of a class version author cay horstmann public class reflectiontest public static void main string args read class name from command line args or user input string name if args length name args else scanner in new scanner system in system out println enter class name e g java util date name in next try print class name and superclass name if object class cl class forname name class supercl cl getsuperclass string modifiers modifier tostring cl getmodifiers if modifiers length system out print modifiers system out print class name if supercl null supercl object class system out print extends supercl getname system out print n n printconstructors cl system out println printmethods cl system out println printfields cl system out println catch classnotfoundexception e e printstacktrace system exit prints all constructors of a class param cl a class public static void printconstructors class cl constructor constructors cl getdeclaredconstructors for constructor c constructors string name c getname system out print string modifiers modifier tostring c getmodifiers if modifiers length system out print modifiers system out print name print parameter types class paramtypes c getparametertypes for int j j paramtypes length j if j system out print system out print paramtypes j getname system out println prints all methods of a class param cl a class public static void printmethods class cl method methods cl getdeclaredmethods for method m methods class rettype m getreturntype string name m getname system out print print modifiers return type and method name string modifiers modifier tostring m getmodifiers if modifiers length system out print modifiers system out print rettype getname name print parameter types class paramtypes m getparametertypes for int j j paramtypes length j if j system out print system out print paramtypes j getname system out println prints all fields of a class param cl a class public static void printfields class cl field fields cl getdeclaredfields for field f fields class type f gettype string name f getname system out print string modifiers modifier tostring f getmodifiers if modifiers length system out print modifiers system out println type getname name what is remarkable about this program is that it can analyze any class that the java interpreter can load not just the classes that were available when the program was compiled we will use this program in the next chapter to peek inside the inner classes that the java compiler generates automatically java lang class field getfields field getdeclaredfields getfields returns an array containing field objects for the public fields of this class or its superclasses getdeclaredfield returns an array of field objects for all fields of this class the methods return an array of length if there are no such fields or if the class object represents a primitive or array type method getmethods method getdeclaredmethods returns an array containing method objects getmethods returns public methods and includes inherited methods getdeclaredmethods returns all methods of this class or interface but does not include inherited methods constructor getconstructors constructor getdeclaredconstructors returns an array containing constructor objects that give you all the public constructors for getconstructors or all constructors for getdeclaredconstructors of the class represented by this class object java lang reflect field java lang reflect method java lang reflect constructor class getdeclaringclass returns the class object for the class that defines this constructor method or field class getexceptiontypes in constructor and method classes returns an array of class objects that represent the types of the exceptions thrown by the method int getmodifiers returns an integer that describes the modifiers of this constructor method or field use the methods in the modifier class to analyze the return value string getname returns a string that is the name of the constructor method or field class getparametertypes in constructor and method classes returns an array of class objects that represent the types of the parameters class getreturntype in method classes returns a class object that represents the return type java lang reflect modifier static string tostring int modifiers returns a string with the modifiers that correspond to the bits set in modifiers static boolean isabstract int modifiers static boolean isfinal int modifiers static boolean isinterface int modifiers static boolean isnative int modifiers static boolean isprivate int modifiers static boolean isprotected int modifiers static boolean ispublic int modifiers static boolean isstatic int modifiers static boolean isstrict int modifiers static boolean issynchronized int modifiers static boolean isvolatile int modifiers tests the bit in the modifiers value that corresponds to the modifier in the method name using reflection to analyze objects at runtime in the preceding section we saw how we can find out the names and types of the data fields of any object get the corresponding class object call getdeclaredfields on the class object in this section we will go one step further and actually look at the contents of the fields of course it is easy to look at the contents of a specific field of an object whose name and type are known when you write a program but reflection lets you look at fields of objects that were not known at compile time the key method to achieve this examination is the get method in the field class if f is an object of type field for example one obtained from getdeclaredfields and obj is an object of the class of which f is a field then f get obj returns an object whose value is the current value of the field of obj this is all a bit abstract so let run through an example click here to view code image employee harry new employee harry hacker class cl harry getclass the class object representing employee field f cl getdeclaredfield name the name field of the employee class object v f get harry the value of the name field of the harry object i e the string object harry hacker actually there is a problem with this code since the name field is a private field the get method will throw an illegalaccessexception you can only use get to get the values of accessible fields the security mechanism of java lets you find out what fields an object has but it won t let you read the values of those fields unless you have access permission the default behavior of the reflection mechanism is to respect java access control however if a java program is not controlled by a security manager that disallows it you can override access control to do this invoke the setaccessible method on a field method or constructor object for example f setaccessible true now ok to call f get harry the setaccessible method is a method of the accessibleobject class the common super class of the field method and constructor classes this feature is provided for debuggers persistent storage and similar mechanisms we use it for a generic tostring method later in this section there is another issue with the get method that we need to deal with the name field is a string and so it is not a problem to return the value as an object but suppose we want to look at the salary field that is a double and in java number types are not objects to handle this you can either use the getdouble method of the field class or you can call get whereby the reflection mechanism automatically wraps the field value into the appropriate wrapper class in this case double of course you can also set the values that you can get the call f set obj value sets the field represented by f of the object obj to the new value listing shows how to write a generic tostring method that works for any class it uses getdeclaredfields to obtain all data fields it then uses the setaccessible convenience method to make all fields accessible for each field it obtains the name and the value listing turns each value into a string by recursively invoking tostring click here to view code image class objectanalyzer public string tostring object obj class cl obj getclass string r cl getname inspect the fields of this class and all superclasses do r field fields cl getdeclaredfields accessibleobject setaccessible fields true get the names and values of all fields for field f fields if modifier isstatic f getmodifiers if r endswith r r f getname try object val f get obj r tostring val catch exception e e printstacktrace r cl cl getsuperclass while cl null return r the complete code in listing needs to address a couple of complexities cycles of references could cause an infinite recursion therefore the objectanalyzer listing keeps track of objects that were already visited also to peek inside arrays you need a different approach you ll learn about the details in the next section you can use this tostring method to peek inside any object for example the call click here to view code image arraylist integer squares new arraylist for int i i i squares add i i system out println new objectanalyzer tostring squares yields the printout click here to view code image java util arraylist elementdata class java lang object java lang integer value java lang integer value java lang integer value java lang integer value java lang integer value null null null null null size modcount you can use this generic tostring method to implement the tostring methods of your own classes like this public string tostring return new objectanalyzer tostring this this is a hassle free method for supplying a tostring method that you may find useful in your own programs listing objectanalyzer objectanalyzertest java click here to view code image package objectanalyzer import java util arraylist this program uses reflection to spy on objects version author cay horstmann public class objectanalyzertest public static void main string args arraylist integer squares new arraylist for int i i i squares add i i system out println new objectanalyzer tostring squares listing objectanalyzer objectanalyzer java click here to view code image package objectanalyzer import java lang reflect accessibleobject import java lang reflect array import java lang reflect field import java lang reflect modifier import java util arraylist public class objectanalyzer private arraylist object visited new arraylist converts an object to a string representation that lists all fields param obj an object return a string with the object class name and all field names and values public string tostring object obj if obj null return null if visited contains obj return visited add obj class cl obj getclass if cl string class return string obj if cl isarray string r cl getcomponenttype for int i i array getlength obj i if i r object val array get obj i if cl getcomponenttype isprimitive r val else r tostring val return r string r cl getname inspect the fields of this class and all superclasses do r field fields cl getdeclaredfields accessibleobject setaccessible fields true get the names and values of all fields for field f fields if modifier isstatic f getmodifiers if r endswith r r f getname try class t f gettype object val f get obj if t isprimitive r val else r tostring val catch exception e e printstacktrace r cl cl getsuperclass while cl null return r java lang reflect accessibleobject void setaccessible boolean flag sets the accessibility flag for this reflection object a value of true indicates that java language access checking is suppressed and that the private properties of the object can be queried and set boolean isaccessible gets the value of the accessibility flag for this reflection object static void setaccessible accessibleobject array boolean flag is a convenience method to set the accessibility flag for an array of objects java lang class field getfield string name field getfields gets the public field with the given name or an array of all fields field getdeclaredfield string name field getdeclaredfields gets the field that is declared in this class with the given name or an array of all fields java lang reflect field object get object obj gets the value of the field described by this field object in the object obj void set object obj object newvalue sets the field described by this field object in the object obj to a new value using reflection to write generic array code the array class in the java lang reflect package allows you to create arrays dynamically this is used for example in the implementation of the copyof method in the arrays class recall how this method can be used to grow an array that has become full employee a new employee array is full a arrays copyof a a length how can one write such a generic method it helps that an employee array can be converted to an object array that sounds promising here is a first attempt click here to view code image public static object badcopyof object a int newlength not useful object newarray new object newlength system arraycopy a newarray math min a length newlength return newarray however there is a problem with actually using the resulting array the type of array that this code returns is an array of objects object because we created the array using the line of code new object newlength an array of objects cannot be cast to an array of employees employee the virtual machine would generate a classcastexception at runtime the point is as we mentioned earlier that a java array remembers the type of its entries that is the element type used in the new expression that created it it is legal to cast an employee temporarily to an object array and then cast it back but an array that started its life as an object array can never be cast into an employee array to write this kind of generic array code we need to be able to make a new array of the same type as the original array for this we need the methods of the array class in the java lang reflect package the key is the static newinstance method of the array class that constructs a new array you must supply the type for the entries and the desired length as parameters to this method object newarray array newinstance componenttype newlength to actually carry this out we need to get the length and the component type of the new array we obtain the length by calling array getlength a the static getlength method of the array class returns the length of an array to get the component type of the new array first get the class object of a confirm that it is indeed an array use the getcomponenttype method of the class class which is defined only for class objects that represent arrays to find the right type for the array why is getlength a method of array but getcomponenttype a method of class we don t know the distribution of the reflection methods seems a bit ad hoc at times here the code click here to view code image public static object goodcopyof object a int newlength class cl a getclass if cl isarray return null class componenttype cl getcomponenttype int length array getlength a object newarray array newinstance componenttype newlength system arraycopy a newarray math min length newlength return newarray note that this copyof method can be used to grow arrays of any type not just arrays of objects int a a int goodcopyof a to make this possible the parameter of goodcopyof is declared to be of type object not an array of objects object the integer array type int can be converted to an object but not to an array of objects listing shows both methods in action note that the cast of the return value of badcopyof will throw an exception java lang reflect array static object get object array int index static xxx getxxx object array int index xxx is one of the primitive types boolean byte char double float int long or short these methods return the value of the given array that is stored at the given index static void set object array int index object newvalue static set xxx object array int index xxx newvalue xxx is one of the primitive types boolean byte char double float int long or short these methods store a new value into the given array at the given index static int getlength object array returns the length of the given array static object newinstance class componenttype int length static object newinstance class componenttype int lengths returns a new array of the given component type with the given dimensions listing arrays copyoftest java click here to view code image package arrays import java lang reflect import java util this program demonstrates the use of reflection for manipulating arrays version author cay horstmann public class copyoftest public static void main string args int a a int goodcopyof a system out println arrays tostring a string b tom dick harry b string goodcopyof b system out println arrays tostring b system out println the following call will generate an exception b string badcopyof b this method attempts to grow an array by allocating a new array and copying all elements param a the array to grow param newlength the new length return a larger array that contains all elements of a however the returned array has type object not the same type as a public static object badcopyof object a int newlength not useful object newarray new object newlength system arraycopy a newarray math min a length newlength return newarray this method grows an array by allocating a new array of the same type and copying all elements param a the array to grow this can be an object array or a primitive type array return a larger array that contains all elements of a public static object goodcopyof object a int newlength class cl a getclass if cl isarray return null class componenttype cl getcomponenttype int length array getlength a object newarray array newinstance componenttype newlength system arraycopy a newarray math min length newlength return newarray invoking arbitrary methods in c and c you can execute an arbitrary function through a function pointer on the surface java does not have method pointers i e ways of giving the location of a method to another method so that the second method can invoke it later in fact the designers of java have said that method pointers are dangerous and error prone and that java interfaces discussed in the next chapter are a superior solution however the reflection mechanism allows you to call arbitrary methods note among the nonstandard language extensions that microsoft added to its java derivatives j and c is another method pointer type called a delegate that is different from the method class that we discuss in this section however inner classes which we will introduce in the next chapter are a more useful construct than delegates recall that you can inspect a field of an object with the get method of the field class similarly the method class has an invoke method that lets you call the method that is wrapped in the current method object the signature for the invoke method is object invoke object obj object args the first parameter is the implicit parameter and the remaining objects provide the explicit parameters for a static method the first parameter is ignored you can set it to null for example if represents the getname method of the employee class the following code shows how you can call it string n string invoke harry if the return type is a primitive type the invoke method will return the wrapper type instead for example suppose that represents the getsalary method of the employee class then the returned object is actually a double and you must cast it accordingly use automatic unboxing to turn it into a double double double invoke harry how do you obtain a method object you can of course call getdeclaredmethods and search through the returned array of method objects until you find the method you want or you can call the getmethod method of the class class this is similar to the getfield method that takes a string with the field name and returns a field object however there may be several methods with the same name so you need to be careful that you get the right one for that reason you must also supply the parameter types of the desired method the signature of getmethod is method getmethod string name class parametertypes for example here is how you can get method pointers to the getname and raisesalary methods of the employee class click here to view code image method employee class getmethod getname method employee class getmethod raisesalary double class now that you have seen the rules for using method objects let put them to work listing is a program that prints a table of values for a mathematical function such as math sqrt or math sin the printout looks like this click here to view code image public static native double java lang math sqrt double 4142 0000 0000 0000 0000 0000 0000 0000 0000 0000 the code for printing a table is of course independent of the actual function that is being tabulated click here to view code image double dx to from n for double x from x to x dx double y double f invoke null x system out printf n x y here f is an object of type method the first parameter of invoke is null because we are calling a static method to tabulate the math sqrt function we set f to math class getmethod sqrt double class that is the method of the math class that has the name sqrt and a single parameter of type double listing shows the complete code of the generic tabulator and a couple of test runs listing methods methodtabletest java click here to view code image package methods import java lang reflect this program shows how to invoke methods through reflection version author cay horstmann public class methodtabletest public static void main string args throws exception get method pointers to the square and sqrt methods method square methodtabletest class getmethod square double class method sqrt math class getmethod sqrt double class print tables of x and y values printtable square printtable sqrt returns the square of a number param x a number return x squared public static double square double x return x x prints a table with x and y values for a method param from the lower bound for the x values param to the upper bound for the x values param n the number of rows in the table param f a method with a double parameter and double return value public static void printtable double from double to int n method f print out the method as table header system out println f double dx to from n for double x from x to x dx try double y double f invoke null x system out printf 4f 4f n x y catch exception e e printstacktrace as this example clearly shows you can do anything with method objects that you can do with function pointers in c or delegates in c just as in c this style of programming is usually quite inconvenient and always error prone what happens if you invoke a method with the wrong parameters the invoke method throws an exception also the parameters and return values of invoke are necessarily of type object that means you must cast back and forth a lot as a result the compiler is deprived of the chance to check your code so errors surface only during testing when they are more tedious to find and fix moreover code that uses reflection to get at method pointers is significantly slower than code that simply calls methods directly for that reason we suggest that you use method objects in your own programs only when absolutely necessary using interfaces and inner classes the subject of the next chapter is almost always a better idea in particular we echo the developers of java and suggest not using method objects for callback functions using interfaces for the callbacks see the next chapter as well leads to code that runs faster and is a lot more maintainable java lang reflect method public object invoke object implicitparameter object explicitparameters invokes the method described by this object passing the given parameters and returning the value that the method returns for static methods pass null as the implicit parameter pass primitive type values by using wrappers primitive type return values must be unwrapped design hints for inheritance we want to end this chapter with some hints that we have found useful when using inheritance place common operations and fields in the superclass this is why we put the name field into the person class instead of replicating it in the employee and student classes don t use protected fields some programmers think it is a good idea to define most instance fields as protected just in case so that subclasses can access these fields if they need to however the protected mechanism doesn t give much protection for two reasons first the set of subclasses is unbounded anyone can form a subclass of your classes and then write code that directly accesses protected instance fields thereby breaking encapsulation and second in the java programming language all classes in the same package have access to protected fields whether or not they are subclasses however protected methods can be useful to indicate methods that are not ready for general use and should be redefined in subclasses use inheritance to model the is a relationship inheritance is a handy code saver but sometimes people overuse it for example suppose we need a contractor class contractors have names and hire dates but they do not have salaries instead they are paid by the hour and they do not stay around long enough to get a raise there is the temptation to form a subclass contractor from employee and add an hourlywage field class contractor extends employee private double hourlywage this is not a good idea however because now each contractor object has both a salary and hourly wage field it will cause you no end of grief when you implement methods for printing paychecks or tax forms you will end up writing more code than you would have written by not inheriting in the first place the contractor employee relationship fails the is a test a contractor is not a special case of an employee don t use inheritance unless all inherited methods make sense suppose we want to write a holiday class surely every holiday is a day and days can be expressed as instances of the gregoriancalendar class so we can use inheritance class holiday extends gregoriancalendar unfortunately the set of holidays is not closed under the inherited operations one of the public methods of gregoriancalendar is add and add can turn holidays into nonholidays holiday christmas christmas add calendar therefore inheritance is not appropriate in this example don t change the expected behavior when you override a method the substitution principle applies not just to syntax but more importantly to behavior when you override a method you should not unreasonably change its behavior the compiler can t help you it cannot check whether your redefinitions make sense for example you can fix the issue of the add method in the holiday class by redefining add perhaps to do nothing or to throw an exception or to move on to the next holiday however such a fix violates the substitution principle the sequence of statements int x get calendar x add calendar int x get calendar system out println should have the expected behavior no matter whether x is of type gregoriancalendar or holiday of course therein lies the rub reasonable and unreasonable people can argue at length about what the expected behavior is for example some authors argue that the substitution principle requires manager equals to ignore the bonus field because employee equals ignores it these discussions are pointless if they occur in a vacuum ultimately what matters is that you do not circumvent the intent of the original design when you override methods in subclasses use polymorphism not type information whenever you find code of the form if x is of type x else if x is of type x think polymorphism do and represent a common concept if so make the concept a method of a common superclass or interface of both types then you can simply call x action and have the dynamic dispatch mechanism inherent in polymorphism launch the correct action code using polymorphic methods or interface implementations is much easier to maintain and extend than code that uses multiple type tests don t overuse reflection the reflection mechanism lets you write programs with amazing generality by detecting fields and methods at runtime this capability can be extremely useful for systems programming but it is usually not appropriate in applications reflection is fragile with it the compiler cannot help you find programming errors any errors are found at runtime and result in exceptions you have now seen how java supports the fundamentals of object oriented programming classes inheritance and polymorphism in the next chapter we will tackle two advanced topics that are very important for using java effectively interfaces and inner classes chapter interfaces and inner classes in this chapter interfaces object cloning interfaces and callbacks inner classes proxies you have now seen all the basic tools for object oriented programming in java this chapter shows you several advanced techniques that are commonly used despite their less obvious nature you will need to master them to complete your java tool chest the first technique called interfaces is a way of describing what classes should do without specifying how they should do it a class can implement one or more interfaces you can then use objects of these implementing classes whenever conformance to the interface is required after we cover interfaces we take up cloning an object or deep copying as it is sometimes called a clone of an object is a new object that has the same state as the original in particular you can modify the clone without affecting the original next we move on to the mechanism of inner classes inner classes are technically somewhat complex they are defined inside other classes and their methods can access the fields of the surrounding class inner classes are useful when you design collections of cooperating classes in particular inner classes enable you to write concise professional looking code to handle gui events this chapter concludes with a discussion of proxies objects that implement arbitrary interfaces a proxy is a very specialized construct that is useful for building system level tools you can safely skip that section on first reading interfaces in the java programming language an interface is not a class but a set of requirements for the classes that want to conform to the interface typically the supplier of some service states if your class conforms to a particular interface then i ll perform the service let look at a concrete example the sort method of the arrays class promises to sort an array of objects but under one condition the objects must belong to classes that implement the comparable interface here is what the comparable interface looks like public interface comparable int compareto object other this means that any class that implements the comparable interface is required to have a compareto method and the method must take an object parameter and return an integer note as of java se the comparable interface has been enhanced to be a generic type click here to view code image public interface comparable t int compareto t other parameter has type t for example a class that implements comparable employee must supply a method int compareto employee other you can still use the raw comparable type without a type parameter but then you have to manually cast the parameter of the compareto method to the desired type all methods of an interface are automatically public for that reason it is not necessary to supply the keyword public when declaring a method in an interface of course there is an additional requirement that the interface cannot spell out when calling x compareto y the compareto method must actually be able to compare the two objects and return an indication whether x or y is larger the method is supposed to return a negative number if x is smaller than y zero if they are equal and a positive number otherwise this particular interface has a single method some interfaces have multiple methods as you will see later interfaces can also define constants what is more important however is what interfaces cannot supply interfaces never have instance fields and the methods are never implemented in the interface supplying instance fields and method implementations is the job of the classes that implement the interface you can think of an interface as being similar to an abstract class with no instance fields however there are some differences between these two concepts we look at them later in some detail now suppose we want to use the sort method of the arrays class to sort an array of employee objects then the employee class must implement the comparable interface to make a class implement an interface you carry out two steps you declare that your class intends to implement the given interface you supply definitions for all methods in the interface to declare that a class implements an interface use the implements keyword class employee implements comparable of course now the employee class needs to supply the compareto method let suppose that we want to compare employees by their salary here is an implementation of the compareto method click here to view code image public int compareto object otherobject employee other employee otherobject return double compare salary other salary here we use the static double compare method that returns a negative if the first argument is less than the second argument if they are equal and a positive value otherwise caution in the interface declaration the compareto method was not declared public because all methods in an interface are automatically public however when implementing the interface you must declare the method as public otherwise the compiler assumes that the method has package visibility the default for a class the compiler then complains that you re trying to supply a weaker access privilege as of java se we can do a little better we ll implement the comparable employee interface type instead click here to view code image class employee implements comparable employee public int compareto employee other return double compare salary other salary note that the unsightly cast of the object parameter has gone away tip the compareto method of the comparable interface returns an integer if the objects are not equal it does not matter what negative or positive value you return this flexibility can be useful when you are comparing integer fields for example suppose each employee has a unique integer id and you want to sort by the employee id number then you can simply return id other id that value will be some negative value if the first id number is less than the other if they are the same id and some positive value otherwise however there is one caveat the range of the integers must be small enough so that the subtraction does not overflow if you know that the ids are not negative or that their absolute value is at most integer you are safe of course the subtraction trick doesn t work for floating point numbers the difference salary other salary can round to if the salaries are close together but not identical the call double compare x y simply returns if x y or if x now you saw what a class must do to avail itself of the sorting service it must implement a compareto method that eminently reasonable there needs to be some way for the sort method to compare objects but why can t the employee class simply provide a compareto method without implementing the comparable interface the reason for interfaces is that the java programming language is strongly typed when making a method call the compiler needs to be able to check that the method actually exists somewhere in the sort method will be statements like this if a i compareto a j rearrange a i and a j the compiler must know that a i actually has a compareto method if a is an array of comparable objects then the existence of the method is assured because every class that implements the comparable interface must supply the method note you would expect that the sort method in the arrays class is defined to accept a comparable array so that the compiler can complain if anyone ever calls sort with an array whose element type doesn t implement the comparable interface sadly that is not the case instead the sort method accepts an object array and uses a clumsy cast click here to view code image approach used in the standard library not recommended if comparable a i compareto a j rearrange a i and a j if a i does not belong to a class that implements the comparable interface the virtual machine throws an exception listing presents the full code for sorting an array of instances of the class employee listing for sorting an employee array listing interfaces employeesorttest java click here to view code image package interfaces import java util this program demonstrates the use of the comparable interface version author cay horstmann public class employeesorttest public static void main string args employee staff new employee staff new employee harry hacker staff new employee carl cracker staff new employee tony tester arrays sort staff print out information about all employee objects for employee e staff system out println name e getname salary e getsalary listing interfaces employee java click here to view code image package interfaces public class employee implements comparable employee private string name private double salary public employee string n double name n salary public string getname return name public double getsalary return salary public void raisesalary double bypercent double raise salary bypercent salary raise compares employees by salary param other another employee object return a negative value if this employee has a lower salary than otherobject if the salaries are the same a positive value otherwise public int compareto employee other return double compare salary other salary java lang comparable t int compareto t other compares this object with other and returns a negative integer if this object is less than other zero if they are equal and a positive integer otherwise java util arrays static void sort object a sorts the elements in the array a using a tuned mergesort algorithm all elements in the array must belong to classes that implement the comparable interface and they must all be comparable to each other java lang integer static int compare int x int y returns a negative integer if x y zero if x and y are equal and a positive integer otherwise java lang double static int compare double x double y returns a negative integer if x y zero if x and y are equal and a positive integer otherwise note according to the language standard the implementor must ensure sgn x compareto y sgn y compareto x for all x and y this implies that x compareto y must throw an exception if y compareto x throws an exception here sgn is the sign of a number sgn n is if n is negative if n equals and if n is positive in plain english if you flip the parameters of compareto the sign but not necessarily the actual value of the result must also flip as with the equals method problems can arise when inheritance comes into play si nc e manager extends employee it implements comparable employee and not comparable manager if manager chooses to override compareto it must be prepared to compare managers to employees it can t simply cast an employee to a manager click here to view code image class manager extends employee public int compareto employee other manager othermanager manager other no that violates the antisymmetry rule if x is an employee and y is a manager then the call x compareto y doesn t throw an exception it simply compares x and y as employees but the reverse y compareto x throws a classcastexception this is the same situation as with the equals method that we discussed in chapter and the remedy is the same there are two distinct scenarios if subclasses have different notions of comparison then you should outlaw comparison of objects that belong to different classes each compareto method should start out with the test if getclass other getclass throw new classcastexception if there is a common algorithm for comparing subclass objects simply provide a single compareto method in the superclass and declare it as final for example suppose that you want managers to be better than regular employees regardless of the salary what about other subclasses such as executive and secretary if you need to establish a pecking order supply a method such as rank in the employee class have each subclass override rank and implement a single compareto method that takes the rank values into account properties of interfaces interfaces are not classes in particular you can never use the new operator to instantiate an interface x new comparable error however even though you can t construct interface objects you can still declare interface variables comparable x ok an interface variable must refer to an object of a class that implements the interface x new employee ok provided employee implements comparable next just as you use instanceof to check whether an object is of a specific class you can use instanceof to check whether an object implements an interface if anobject instanceof comparable just as you can build hierarchies of classes you can extend interfaces this allows for multiple chains of interfaces that go from a greater degree of generality to a greater degree of specialization for example suppose you had an interface called moveable public interface moveable void move double x double y then you could imagine an interface called powered that extends it click here to view code image public interface powered extends moveable double milespergallon although you cannot put instance fields or static methods in an interface you can supply constants in them for example click here to view code image public interface powered extends moveable double milespergallon double a public static final constant just as methods in an interface are automatically public fields are always public static final note it is legal to tag interface methods as public and fields as public static final some programmers do that either out of habit or for greater clarity however the java language specification recommends that the redundant keywords not be supplied and we follow that recommendation some interfaces define just constants and no methods for example the standard library contains an interface swingconstants that defines constants north south horizontal and so on any class that chooses to implement the swingconstants interface automatically inherits these constants its methods can simply refer to north rather than the more cumbersome swingconstants north however this use of interfaces seems rather degenerate and we do not recommend it while each class can have only one superclass classes can implement multiple interfaces this gives you the maximum amount of flexibility in defining a class behavior for example the java programming language has an important interface built into it called cloneable we will discuss this interface in detail in the next section if your class implements cloneable the clone method in the object class will make an exact copy of your class objects suppose therefore you want cloneability and comparability then you simply implement both interfaces class employee implements cloneable comparable use commas to separate the interfaces that describe the characteristics that you want to supply interfaces and abstract classes if you read the section about abstract classes in chapter you may wonder why the designers of the java programming language bothered with introducing the concept of interfaces why can t comparable simply be an abstract class click here to view code image abstract class comparable why not public abstract int compareto object other the employee class would then simply extend this abstract class and supply the compareto method click here to view code image class employee extends comparable why not public int compareto object other there is unfortunately a major problem with using an abstract base class to express a generic property a class can only extend a single class suppose that the employee class already extends a different class say person then it can t extend a second class class employee extends person comparable error but each class can implement as many interfaces as it likes class employee extends person implements comparable ok other programming languages in particular c allow a class to have more than one superclass this feature is called multiple inheritance the designers of java chose not to support multiple inheritance because it makes the language either very complex as in c or less efficient as in eiffel instead interfaces afford most of the benefits of multiple inheritance while avoiding the complexities and inefficiencies c note c has multiple inheritance and all the complications that come with it such as virtual base classes dominance rules and transverse pointer casts few c programmers use multiple inheritance and some say it should never be used other programmers recommend using multiple inheritance only for the mix in style of inheritance in the mix in style a primary base class describes the parent object and additional base classes the so called mix ins may supply auxiliary characteristics that style is similar to a java class with a single base class and additional interfaces however in c mix ins can add default behavior whereas java interfaces cannot object cloning when you make a copy of a variable the original and the copy are references to the same object see figure this means a change to either variable also affects the other click here to view code image employee original new employee john public employee copy original copy raisesalary oops also changed original figure copying and cloning if you would like copy to be a new object that begins its life being identical to original but whose state can diverge over time use the clone method click here to view code image employee copy original clone copy raisesalary ok original unchanged but it isn t quite so simple the clone method is a protected method of object which means that your code cannot simply call it only the employee class can clone employee objects there is a reason for this restriction think about the way in which the object class can implement clone it knows nothing about the object at all so it can make only a field by field copy if all data fields in the object are numbers or other basic types copying the fields is just fine but if the object contains references to subobjects then copying the field gives you another reference to the same subobject so the original and the cloned objects still share some information to visualize that phenomenon let consider the employee class that was introduced in chapter figure shows what happens when you use the clone method of the object class to clone such an employee object as you can see the default cloning operation is shallow it doesn t clone objects that are referenced inside other objects figure a shallow copy does it matter if the copy is shallow it depends if the subobject shared between the original and the shallow clone is immutable then the sharing is safe this certainly happens if the subobject belongs to an immutable class such as string alternatively the subobject may simply remain constant throughout the lifetime of the object with no mutators touching it and no methods yielding a reference to it quite frequently however subobjects are mutable and you must redefine the clone method to make a deep copy that clones the subobjects as well in our example the hireday field is a date which is mutable for every class you need to decide whether the default clone method is good enough the default clone method can be patched up by calling clone on the mutable subobjects and clone should not be attempted the third option is actually the default to choose either the first or the second option a class must implement the cloneable interface and redefine the clone method with the public access modifier note the clone method is declared protected in the object class so that your code can t simply call anobject clone but aren t protected methods accessible from any subclass and isn t every class a subclass of object fortunately the rules for protected access are more subtle see chapter a subclass can call a protected clone method only to clone its own objects you must redefine clone to be public to allow objects to be cloned by any method in this case the appearance of the cloneable interface has nothing to do with the normal use of interfaces in particular it does not specify the clone method that method is inherited from the object class the interface merely serves as a tag indicating that the class designer understands the cloning process objects are so paranoid about cloning that they generate a checked exception if an object requests cloning but does not implement that interface note the cloneable interface is one of a handful of tagging interfaces that java provides some programmers call them marker interfaces recall that the usual purpose of an interface such as comparable is to ensure that a class implements a particular method or set of methods a tagging interface has no methods its only purpose is to allow the use of instanceof in a type inquiry if obj instanceof cloneable we recommend that you do not use tagging interfaces in your own programs even if the default shallow copy implementation of clone is adequate you still need to implement the cloneable interface redefine clone to be public and call super clone here is an example click here to view code image class employee implements cloneable raise visibility level to public change return type public employee clone throws clonenotsupportedexception return employee super clone note before java se the clone method always had return type object nowadays the covariant return type feature lets you specify the correct return type for your clone methods the clone method that you just saw adds no functionality to the shallow copy provided by object clone it merely makes the method public to make a deep copy you have to work harder and clone the mutable instance fields here is an example of a clone method that creates a deep copy click here to view code image class employee implements cloneable public employee clone throws clonenotsupportedexception call object clone employee cloned employee super clone clone mutable fields cloned hireday date hireday clone return cloned the clone method of the object class threatens to throw a clonenotsupportedexception it does that whenever clone is invoked on an object whose class does not implement the cloneable interface of course the employee and date classes implement the cloneable interface so the exception won t be thrown however the compiler does not know that therefore we declared the exception public employee clone throws clonenotsupportedexception would it be better to catch the exception instead click here to view code image public employee clone try return employee super clone catch clonenotsupportedexception e return null this won t happen since we are cloneable this is appropriate for final classes otherwise it is a good idea to leave the throws specifier in place that gives subclasses the option of throwing a clonenotsupportedexception if they can t support cloning you have to be careful about cloning of subclasses for example once you have defined the clone method for the employee class anyone can use it to clone manager objects can the employee clone method do the job it depends on the fields of the manager class in our case there is no problem because the bonus field has primitive type but manager might have acquired fields that require a deep copy or are not cloneable there is no guarantee that the implementor of the subclass has fixed clone to do the right thing for that reason the clone method is declared as protected in the object class but you don t have that luxury if you want users of your classes to invoke clone should you implement clone in your own classes if your clients need to make deep copies then you probably should some authors feel that you should avoid clone altogether and instead implement another method for the same purpose we agree that clone is rather awkward but you ll run into the same issues if you shift the responsibility to another method at any rate cloning is less common than you may think less than percent of the classes in the standard library implement clone the program in listing clones an instance of the class employee listing then invokes two mutators the raisesalary method changes the value of the salary field whereas the sethireday method changes the state of the hireday field neither mutation affects the original object because clone has been defined to make a deep copy listing clone clonetest java click here to view code image package clone this program demonstrates cloning version author cay horstmann public class clonetest public static void main string args try employee original new employee john q public original sethireday employee copy original clone copy raisesalary copy sethireday system out println original original system out println copy copy catch clonenotsupportedexception e e printstacktrace listing clone employee java click here to view code image package clone import java util date import java util gregoriancalendar public class employee implements cloneable private string name private double salary private date hireday public employee string n double name n salary hireday new date public employee clone throws clonenotsupportedexception call object clone employee cloned employee super clone clone mutable fields cloned hireday date hireday clone return cloned set the hire day to a given date param year the year of the hire day param month the month of the hire day param day the day of the hire day public void sethireday int year int month int day date newhireday new gregoriancalendar year month day gettime example of instance field mutation hireday settime newhireday gettime public void raisesalary double bypercent double raise salary bypercent salary raise public string tostring return employee name name salary salary hireday hireday note all array types have a clone method that is public not protected you can use it to make a new array that contains copies of all elements for example click here to view code image int luckynumbers int cloned luckynumbers clone cloned doesn t change luckynumbers note chapter of volume ii shows an alternate mechanism for cloning objects using the object serialization feature of java that mechanism is easy to implement and safe but not very efficient interfaces and callbacks a common pattern in programming is the callback pattern in this pattern you want to specify the action that should occur whenever a particular event happens for example you may want a particular action to occur when a button is clicked or a menu item is selected however as you have not yet seen how to implement user interfaces we will consider a similar but simpler situation t he javax swing package contains a timer class that is useful if you want to be notified whenever a time interval has elapsed for example if a part of your program contains a clock then you can ask to be notified every second so that you can update the clock face when you construct a timer you set the time interval and you tell it what it should do whenever the time interval has elapsed how do you tell the timer what it should do in many programming languages you supply the name of a function that the timer should call periodically however the classes in the java standard library take an object oriented approach you pass an object of some class the timer then calls one of the methods on that object passing an object is more flexible than passing a function because the object can carry additional information of course the timer needs to know what method to call the timer requires that you specify an object of a class that implements the actionlistener interface of the java awt event package here is that interface click here to view code image public interface actionlistener void actionperformed actionevent event the timer calls the actionperformed method when the time interval has expired c note as you saw in chapter java does have the equivalent of function pointers namely method objects however they are difficult to use slower and cannot be checked for type safety at compile time whenever you would use a function pointer in c you should consider using an interface in java suppose you want to print a message at the tone the time is followed by a beep once every seconds you would define a class that implements the actionlistener interface you would then place whatever statements you want to have executed inside the actionperformed method click here to view code image class timeprinter implements actionlistener public void actionperformed actionevent event date now new date system out println at the tone the time is now toolkit getdefaulttoolkit beep note the actionevent parameter of the actionperformed method this parameter gives information about the event such as the source object that generated it see chapter for more information however detailed information about the event is not important in this program and you can safely ignore the parameter next you construct an object of this class and pass it to the timer constructor click here to view code image actionlistener listener new timeprinter timer t new timer listener the first parameter of the timer constructor is the time interval that must elapse between notifications measured in milliseconds we want to be notified every seconds the second parameter is the listener object finally you start the timer t start every seconds a message like at the tone the time is thu apr pdt is displayed followed by a beep listing puts the timer and its action listener to work after the timer is started the program puts up a message dialog and waits for the user to click the ok button to stop while the program waits for the user the current time is displayed at second intervals be patient when running the program the quit program dialog box appears right away but the first timer message is displayed after seconds note that the program imports the javax swing timer class by name in addition to importing javax swing and java util this breaks the ambiguity between javax swing timer and java util timer an unrelated class for scheduling background tasks listing timer timertest java click here to view code image package timer version author cay horstmann import java awt import java awt event import java util import javax swing import javax swing timer to resolve conflict with java util timer public class timertest public static void main string args actionlistener listener new timeprinter construct a timer that calls the listener once every seconds timer t new timer listener t start joptionpane showmessagedialog null quit program system exit class timeprinter implements actionlistener public void actionperformed actionevent event date now new date system out println at the tone the time is now toolkit getdefaulttoolkit beep javax swing joptionpane static void showmessagedialog component parent object message displays a dialog box with a message prompt and an ok button the dialog is centered over the parent component if parent is null the dialog is centered on the screen javax swing timer timer int interval actionlistener listener constructs a timer that notifies listener whenever interval milliseconds have elapsed void start starts the timer once started the timer calls actionperformed on its listeners void stop stops the timer once stopped the timer no longer calls actionperformed on its listeners java awt toolkit static toolkit getdefaulttoolkit gets the default toolkit a toolkit contains information about the gui environment void beep emits a beep sound inner classes an inner class is a class that is defined inside another class why would you want to do that there are three reasons inner class methods can access the data from the scope in which they are defined including the data that would otherwise be private inner classes can be hidden from other classes in the same package anonymous inner classes are handy when you want to define callbacks without writing a lot of code we will break up this rather complex topic into several steps starting on page you will see a simple inner class that accesses an instance field of its outer class on page we cover the special syntax rules for inner classes starting on page we peek inside inner classes to see how they are translated into regular classes squeamish readers may want to skip that section starting on page we discuss local inner classes that can access local variables of the enclosing scope starting on page we introduce anonymous inner classes and show how they are commonly used to implement callbacks finally starting on page you will see how static inner classes can be used for nested helper classes c note c has nested classes a nested class is contained inside the scope of the enclosing class here is a typical example a linked list class defines a class to hold the links and a class to define an iterator position click here to view code image class linkedlist p ublic class iterator a nested class public void insert int x int erase private class link a nested class public link next int data the nesting is a relationship between classes not objects a linkedlist object does not have subobjects of type iterator or link there are two benefits name control and access control the name iterator is nested inside the linkedlist class so it is known externally as linkedlist iterator and cannot conflict with another class called iterator in java this benefit is not as important because java packages give the same kind of name control note that the link class is in the private part of the linkedlist class it is completely hidden from all other code for that reason it is safe to make its data fields public they can be accessed by the methods of the linkedlist class which has a legitimate need to access them and they are not visible elsewhere in java this kind of control was not possible until inner classes were introduced however the java inner classes have an additional feature that makes them richer and more useful than nested classes in c an object that comes from an inner class has an implicit reference to the outer class object that instantiated it through this pointer it gains access to the total state of the outer object you will see the details of the java mechanism later in this chapter in java static inner classes do not have this added pointer they are the java analog to nested classes in c use of an inner class to access object state the syntax for inner classes is rather complex for that reason we present a simple but somewhat artificial example to demonstrate the use of inner classes we refactor the timertest example and extract a talkingclock class a talking clock is constructed with two parameters the interval between announcements and a flag to turn beeps on or off click here to view code image public class talkingclock private int interval private boolean beep public talkingclock int interval boolean beep public void start public class timeprinter implements actionlistener an inner class note that the timeprinter class is now located inside the talkingclock class this does not mean that every talkingclock has a timeprinter instance field as you will see the timeprinter objects are constructed by methods of the talkingclock class here is the timeprinter class in greater detail note that the actionperformed method checks the beep flag before emitting a beep click here to view code image public class timeprinter implements actionlistener public void actionperformed actionevent event date now new date system out println at the tone the time is now if beep toolkit getdefaulttoolkit beep something surprising is going on the timeprinter class has no instance field or variable named beep instead beep refers to the field of the talkingclock object that created this timeprinter this is quite innovative traditionally a method could refer to the data fields of the object invoking the method an inner class method gets to access both its own data fields and those of the outer object creating it for this to work an object of an inner class always gets an implicit reference to the object that created it see figure figure an inner class object has a reference to an outer class object this reference is invisible in the definition of the inner class however to illuminate the concept let us call the reference to the outer object outer then the actionperformed method is equivalent to the following click here to view code image public void actionperformed actionevent event date now new date system out println at the tone the time is now if outer beep toolkit getdefaulttoolkit beep the outer class reference is set in the constructor the compiler modifies all inner class constructors adding a parameter for the outer class reference the timeprinter class defines no constructors therefore the compiler synthesizes a no argument constructor generating code like this click here to view code image public timeprinter talkingclock clock automatically generated code outer clock again please note outer is not a java keyword we just use it to illustrate the mechanism involved in an inner class when a timeprinter object is constructed in the start method the compiler passes the this reference to the current talking clock into the constructor actionlistener listener new timeprinter this parameter automatically added listing shows the complete program that tests the inner class have another look at the access control had the timeprinter class been a regular class it would have needed to access the beep flag through a public method of the talkingclock class using an inner class is an improvement there is no need to provide accessors that are of interest only to one other class note we could have declared the timeprinter class as private then only talkingclock methods would be able to construct timeprinter objects only inner classes can be private regular classes always have either package or public visibility listing innerclass innerclasstest java click here to view code image package innerclass import java awt import java awt event import java util import javax swing import javax swing timer this program demonstrates the use of inner classes version 02 author cay horstmann public class innerclasstest public static void main string args talkingclock clock new talkingclock true clock start keep program running until user selects ok joptionpane showmessagedialog null quit program system exit a clock that prints the time in regular intervals class talkingclock private int interval private boolean beep constructs a talking clock param interval the interval between messages in milliseconds param beep true if the clock should beep public talkingclock int interval boolean beep this interval interval this beep beep starts the clock public void start actionlistener listener new timeprinter timer t new timer interval listener t start public class timeprinter implements actionlistener public void actionperformed actionevent event date now new date system out println at the tone the time is now if beep toolkit getdefaulttoolkit beep special syntax rules for inner classes in the preceding section we explained the outer class reference of an inner class by calling it outer actually the proper syntax for the outer reference is a bit more complex the expression outerclass this denotes the outer class reference for example you can write the actionperformed method of the timeprinter inner class as click here to view code image public void actionperformed actionevent event if talkingclock this beep toolkit getdefaulttoolkit beep conversely you can write the inner object constructor more explicitly using the syntax outerobject new innerclass construction parameters for example actionlistener listener this new timeprinter here the outer class reference of the newly constructed timeprinter object is set to the this reference of the method that creates the inner class object this is the most common case as always the this qualifier is redundant however it is also possible to set the outer class reference to another object by explicitly naming it for example since timeprinter is a public inner class you can construct a timeprinter for any talking clock click here to view code image talkingclock jabberer new talkingclock true talkingclock timeprinter listener jabberer new timeprinter note that you refer to an inner class as outerclass innerclass when it occurs outside the scope of the outer class are inner classes useful actually necessary secure when inner classes were added to the java language in java many programmers considered them a major new feature that was out of character with the java philosophy of being simpler than c the inner class syntax is undeniably complex it gets more complex as we study anonymous inner classes later in this chapter it is not obvious how inner classes interact with other features of the language such as access control and security by adding a feature that was elegant and interesting rather than needed has java started down the road to ruin which has afflicted so many other languages while we won t try to answer this question completely it is worth noting that inner classes are a phenomenon of the compiler not the virtual machine inner classes are translated into regular class files with dollar signs delimiting outer and inner class names and the virtual machine does not have any special knowledge about them for example the timeprinter class inside the talkingclock class is translated to a class file talkingclock timeprinter class to see this at work try the following experiment run the reflectiontest program of chapter and give it the class talkingclock timeprinter to reflect upon alternatively simply use the javap utility javap private classname note if you use unix remember to escape the character if you supply the class name on the command line that is run the reflectiontest or javap program as java reflection reflectiontest innerclass talkingclock timeprinter or javap private innerclass talkingclock timeprinter you will get the following printout click here to view code image public class talkingclock timeprinter public talkingclock timeprinter talkingclock public void actionperformed java awt event actionevent final talkingclock this you can plainly see that the compiler has generated an additional instance field this for the reference to the outer class the name this is synthesized by the compiler you cannot refer to it in your code you can also see the talkingclock parameter for the constructor if the compiler can automatically do this transformation couldn t you simply program the same mechanism by hand let try it we would make timeprinter a regular class outside the talkingclock class when constructing a timeprinter object we pass it the this reference of the object that is creating it click here to view code image class talkingclock public void start actionlistener listener new timeprinter this timer t new timer interval listener t start class timeprinter implements actionlistener private talkingclock outer public timeprinter talkingclock clock outer clock now let us look at the actionperformed method it needs to access outer beep if outer beep error here we run into a problem the inner class can access the private data of the outer class but our external timeprinter class cannot thus inner classes are genuinely more powerful than regular classes because they have more access privileges you may well wonder how inner classes manage to acquire those added access privileges if they are translated to regular classes with funny names the virtual machine knows nothing at all about them to solve this mystery let again use the reflectiontest program to spy on the talkingclock class click here to view code image class talkingclock private int interval private boolean beep public talkingclock int boolean static boolean access talkingclock public void start notice the static access method that the compiler added to the outer class it returns the beep field of the object that is passed as a parameter the method name might be slightly different such as access depending on your compiler the inner class methods call that method the statement if beep in the actionperformed method of the timeprinter class effectively makes the following call if access outer is this a security risk you bet it is it is an easy matter for someone else to invoke the access method to read the private beep field of course access is not a legal name for a java method however hackers who are familiar with the structure of class files can easily produce a class file with virtual machine instructions to call that method for example by using a hex editor since the secret access methods have package visibility the attack code would need to be placed inside the same package as the class under attack to summarize if an inner class accesses a private data field then it is possible to access that data field through other classes added to the package of the outer class but to do so requires skill and determination a programmer cannot accidentally obtain access but must intentionally build or modify a class file for that purpose note the synthesized constructors and methods can get quite convoluted skip this note if you are squeamish suppose we turn timeprinter into a private inner class there are no private classes in the virtual machine so the compiler produces the next best thing a package visible class with a private constructor private talkingclock timeprinter talkingclock of course nobody can call that constructor so there is a second package visible constructor talkingclock timeprinter talkingclock talkingclock that calls the first one the complier translates the constructor call in the start method of the talkingclock class to new talkingclock timeprinter this null local inner classes if you look carefully at the code of the talkingclock example you will find that you need the name of the type timeprinter only once when you create an object of that type in the start method in a situation like this you can define the class locally in a single method click here to view code image public void start class timeprinter implements actionlistener public void actionperformed actionevent event date now new date system out println at the tone the time is now if beep toolkit getdefaulttoolkit beep actionlistener listener new timeprinter timer t new timer interval listener t start local classes are never declared with an access specifier that is public or private their scope is always restricted to the block in which they are declared local classes have one great advantage they are completely hidden from the outside world not even other code in the talkingclock class can access them no method except start has any knowledge of the timeprinter class accessing final variables from outer methods local classes have another advantage over other inner classes not only can they access the fields of their outer classes they can even access local variables however those local variables must be declared final here is a typical example let move the interval and beep parameters from the talkingclock constructor to the start method click here to view code image public void start int interval final boolean beep class timeprinter implements actionlistener public void actionperformed actionevent event date now new date system out println at the tone the time is now if beep toolkit getdefaulttoolkit beep actionlistener listener new timeprinter timer t new timer interval listener t start note that the talkingclock class no longer needs to store a beep instance field it simply refers to the beep parameter variable of the start method maybe this should not be so surprising the line if beep is after all ultimately inside the start method so why shouldn t it have access to the value of the beep variable to see why there is a subtle issue here let consider the flow of control more closely the start method is called the object variable listener is initialized by a call to the constructor of the inner class timeprinter the listener reference is passed to the timer constructor the timer is started and the start method exits at this point the beep parameter variable of the start method no longer exists a second later the actionperformed method executes if beep for the code in the actionperformed method to work the timeprinter class must have copied the beep field as a local variable of the start method before the beep parameter value went away that is indeed exactly what happens in our example the compiler synthesizes the name talkingclock for the local inner class if you use the reflectiontest program again to spy on the talkingclock class you will get the following output click here to view code image class talkingclock talkingclock talkingclock boolean public void actionperformed java awt event actionevent final boolean val beep final talkingclock this note the boolean parameter to the constructor and the val beep instance variable when an object is created the value beep is passed into the constructor and stored in the val beep field the compiler detects access of local variables makes matching instance fields for each one of them and copies the local variables into the constructor so that the instance fields can be initialized from the programmer point of view local variable access is quite pleasant it makes your inner classes simpler by reducing the instance fields that you need to program explicitly as we already mentioned the methods of a local class can refer only to local variables that are declared final for that reason the beep parameter was declared final in our example a local variable that is declared final cannot be modified after it has been initialized thus it is guaranteed that the local variable and the copy that is made inside the local class will always have the same value note you have seen final variables used for constants such as public static final double t he final keyword can be applied to local variables instance variables and static variables in all cases it means the same thing you can assign to this variable once after it has been created afterwards you cannot change the value it is final however you don t have to initialize a final variable when you define it for example the final parameter variable beep is initialized once after its creation when the start method is called if the method is called multiple times each call has its own newly created beep parameter the val beep instance variable that you can see in the talkingclock inner class is set once in the inner class constructor a final variable that isn t initialized when it is defined is often called a blank final variable the final restriction is somewhat inconvenient suppose for example that you want to update a counter in the enclosing scope here we want to count how often the compareto method is called during sorting click here to view code image int counter date dates new date for int i i dates length i dates i new date public int compareto date other counter error return super compareto other arrays sort dates system out println counter comparisons you can t declare counter as final because you clearly need to update it you can t replace it with an integer because integer objects are immutable the remedy is to use an array of length click here to view code image final int counter new int for int i i dates length i dates i new date public int compareto date other counter return super compareto other the array variable is still declared as final but that merely means that you can t have it refer to a different array you are free to mutate the array elements when inner classes were first invented the prototype compiler automatically made this transformation for all local variables that were modified in the inner class however some programmers were fearful of having the compiler produce heap objects behind their backs and the final restriction was adopted instead it is possible that a future version of the java language will revise this decision anonymous inner classes when using local inner classes you can often go a step further if you want to make only a single object of this class you don t even need to give the class a name such a class is called an anonymous inner class click here to view code image public void start int interval final boolean beep actionlistener listener new actionlistener public void actionperformed actionevent event date now new date system out println at the tone the time is now if beep toolkit getdefaulttoolkit beep timer t new timer interval listener t start this syntax is very cryptic indeed what it means is this create a new object of a class that implements the actionlistener interface where the required method actionperformed is the one defined inside the braces in general the syntax is click here to view code image new supertype construction parameters inner class methods and data here supertype can be an interface such as actionlistener then the inner class implements that interface supertype can also be a class then the inner class extends that class an anonymous inner class cannot have constructors because the name of a constructor must be the same as the name of a class and the class has no name instead the construction parameters are given to the superclass constructor in particular whenever an inner class implements an interface it cannot have any construction parameters nevertheless you must supply a set of parentheses as in new interfacetype methods and data you have to look carefully to see the difference between the construction of a new object of a class and the construction of an object of an anonymous inner class extending that class click here to view code image person queen new person mary a person object person count new person dracula an object of an inner class extending person if the closing parenthesis of the construction parameter list is followed by an opening brace then an anonymous inner class is being defined are anonymous inner classes a great idea or are they a great way of writing obfuscated code probably a bit of both when the code for an inner class is short just a few lines of simple code then anonymous inner classes can save typing time but it is exactly such time saving features that lead you down the slippery slope to obfuscated java code contests listing contains the complete source code for the talking clock program with an anonymous inner class if you compare this program with listing you will find that in this case the solution with the anonymous inner class is quite a bit shorter and hopefully with a bit of practice as easy to comprehend listing anonymousinnerclass anonymousinnerclasstest java click here to view code image package anonymousinnerclass import java awt import java awt event import java util import javax swing import javax swing timer this program demonstrates anonymous inner classes version 02 author cay horstmann public class anonymousinnerclasstest public static void main string args talkingclock clock new talkingclock clock start true keep program running until user selects ok joptionpane showmessagedialog null quit program system exit a clock that prints the time in regular intervals class talkingclock starts the clock param interval the interval between messages in milliseconds param beep true if the clock should beep public void start int interval final boolean beep actionlistener listener new actionlistener public void actionperformed actionevent event date now new date system out println at the tone the time is now if beep toolkit getdefaulttoolkit beep timer t new timer interval listener t start note the following trick called double brace initialization takes advantage of the inner class syntax suppose you want to construct an array list and pass it to a method click here to view code image arraylist string friends new arraylist favorites add harry favorites add tony invite friends if you don t need the array list again it would be nice to make it anonymous but then how can you add the elements here is how invite new arraylist string add harry add tony note the double braces the outer braces make an anonymous subclass of arraylist the inner braces are an object construction block see chapter caution it is often convenient to make an anonymous subclass that is almost but not quite like its superclass but you need to be careful with the equals method in chapter we recommended that your equals methods use a test if getclass other getclass return false an anonymous subclass will fail this test tip when you produce logging or debugging messages you often want to include the name of the current class such as system err println something awful happened in getclass but that fails in a static method after all the call to getclass calls this getclass and a static method has no this use the following expression instead new object getclass getenclosingclass gets class of static method here new object makes an anonymous object of an anonymous subclass of object and getenclosingclass gets its enclosing class that is the class containing the static method static inner classes occasionally you may want to use an inner class simply to hide one class inside another but you don t need the inner class to have a reference to the outer class object you can suppress the generation of that reference by declaring the inner class static here is a typical example of where you would want to do this consider the task of computing the minimum and maximum value in an array of course you write one method to compute the minimum and another method to compute the maximum when you call both methods the array is traversed twice it would be more efficient to traverse the array only once computing both the minimum and the maximum simultaneously double min double double max double for double v values if min v min v if max v max v however the method must return two numbers we can achieve that by defining a class pair that holds two values click here to view code image class pair private double first private double second public pair double f double first f second public double getfirst return first public double getsecond return second the minmax method can then return an object of type pair click here to view code image class arrayalg public static pair minmax double values return new pair min max the caller of the method uses the getfirst and getsecond methods to retrieve the answers click here to view code image pair p arrayalg minmax d system out println min p getfirst system out println max p getsecond of course the name pair is an exceedingly common name and in a large project it is quite possible that some other programmer had the same bright idea but made a pair class that contains a pair of strings we can solve this potential name clash by making pair a public inner class inside arrayalg then the class will be known to the public as arrayalg pair arrayalg pair p arrayalg minmax d however unlike the inner classes that we used in previous examples we do not want to have a reference to any other object inside a pair object that reference can be suppressed by declaring the inner class static class arrayalg public static class pair of course only inner classes can be declared static a static inner class is exactly like any other inner class except that an object of a static inner class does not have a reference to the outer class object that generated it in our example we must use a static inner class because the inner class object is constructed inside a static method public static pair minmax double d return new pair min max had the pair class not been declared as static the compiler would have complained that there was no implicit object of type arrayalg available to initialize the inner class object note use a static inner class whenever the inner class does not need to access an outer class object some programmers use the term nested class to describe static inner classes note inner classes that are declared inside an interface are automatically static and public listing contains the complete source code of the arrayalg class and the nested pair class listing staticinnerclass staticinnerclasstest java click here to view code image package staticinnerclass this program demonstrates the use of static inner classes version 01 02 author cay horstmann public class staticinnerclasstest public static void main string args double d new double for int i i d length i d i math random arrayalg pair p arrayalg minmax d system out println min p getfirst system out println max p getsecond class arrayalg a pair of floating point numbers public static class pair private double first private double second constructs a pair from two floating point numbers param f the first number param the second number public pair double f double first f second returns the first number of the pair return the first number public double getfirst return first returns the second number of the pair return the second number public double getsecond return second computes both the minimum and the maximum of an array param values an array of floating point numbers return a pair whose first element is the minimum and whose second element is the maximum public static pair minmax double values double min double double max double for double v values if min v min v if max v max v return new pair min max proxies in the final section of this chapter we discuss proxies you can use a proxy to create at runtime new classes that implement a given set of interfaces proxies are only necessary when you don t yet know at compile time which interfaces you need to implement this is not a common situation for application programmers and you should feel free to skip this section if you are not interested in advanced wizardry however for certain systems programming applications the flexibility that proxies offer can be very important suppose you want to construct an object of a class that implements one or more interfaces whose exact nature you may not know at compile time this is a difficult problem to construct an actual class you can simply use the newinstance method or use reflection to find a constructor but you can t instantiate an interface you need to define a new class in a running program to overcome this problem some programs generate code place it into a file invoke the compiler and then load the resulting class file naturally this is slow and it also requires deployment of the compiler together with the program the proxy mechanism is a better solution the proxy class can create brand new classes at runtime such a proxy class implements the interfaces that you specify in particular the proxy class has the following methods all methods required by the specified interfaces and all methods defined in the object class tostring equals and so on however you cannot define new code for these methods at runtime instead you must supply an invocation handler an invocation handler is an object of any class that implements the invocationhandler interface that interface has a single method object invoke object proxy method method object args whenever a method is called on the proxy object the invoke method of the invocation handler gets called with the method object and parameters of the original call the invocation handler must then figure out how to handle the call to create a proxy object use the newproxyinstance method of the proxy class the method has three parameters a class loader as part of the java security model different class loaders can be used for system classes classes that are downloaded from the internet and so on we will discuss class loaders in chapter of volume ii for now we specify null to use the default class loader an array of class objects one for each interface to be implemented an invocation handler there are two remaining questions how do we define the handler and what can we do with the resulting proxy object the answers depend of course on the problem that we want to solve with the proxy mechanism proxies can be used for many purposes such as routing method calls to remote servers associating user interface events with actions in a running program tracing method calls for debugging purposes in our example program we use proxies and invocation handlers to trace method calls we define a tracehandler wrapper class that stores a wrapped object its invoke method simply prints the name and parameters of the method to be called and then calls the method with the wrapped object as the implicit parameter click here to view code image class tracehandler implements invocationhandler private object target public tracehandler object t target t public object invoke object proxy method m object args throws throwable print method name and parameters invoke actual method return m invoke target args here is how you construct a proxy object that causes the tracing behavior whenever one of its methods is called click here to view code image object value construct wrapper invocationhandler handler new tracehandler value construct proxy for one or more interfaces class interfaces new class comparable class object proxy proxy newproxyinstance null interfaces handler now whenever a method from one of the interfaces is called on proxy the method name and parameters are printed out and the method is then invoked on value in the program shown in listing we use proxy objects to trace a binary search we fill an array with proxies to the integers then we invoke the binarysearch method of the arrays class to search for a random integer in the array finally we print the matching element click here to view code image object elements new object fill elements with proxies for the integers for int i i elements length i integer value i elements i proxy newproxyinstance proxy for value construct a random integer integer key new random nextint elements length search for the key int result arrays binarysearch elements key print match if found if result system out println elements result the integer class implements the comparable interface the proxy objects belong to a class that is defined at runtime it has a name such as that class also implements the comparable interface however its compareto method calls the invoke method of the proxy object handler note as you saw earlier in this chapter the integer class actually implements comparable integer however at runtime all generic types are erased and the proxy is constructed with the class object for the raw comparable class the binarysearch method makes calls like this if elements i compareto key since we filled the array with proxy objects the compareto calls call the invoke method of the tracehandler class that method prints the method name and parameters and then invokes compareto on the wrapped integer object finally at the end of the sample program we call system out println elements result the println method calls tostring on the proxy object and that call is also redirected to the invocation handler here is the complete trace of a program run compareto compareto compareto compareto compareto compareto compareto 288 tostring you can see how the binary search algorithm homes in on the key by cutting the search interval in half in every step note that the tostring method is proxied even though it does not belong to the comparable interface as you will see in the next section certain object methods are always proxied listing proxy proxytest java click here to view code image package proxy import java lang reflect import java util this program demonstrates the use of proxies version author cay horstmann public class proxytest public static void main string args object elements new object fill elements with proxies for the integers for int i i elements length i integer value i invocationhandler handler new tracehandler value object proxy proxy newproxyinstance null new class comparable class handler elements i proxy construct a random integer integer key new random nextint elements length search for the key int result arrays binarysearch elements key print match if found if result system out println elements result an invocation handler that prints out the method name and parameters then invokes the original method class tracehandler implements invocationhandler private object target constructs a tracehandler param t the implicit parameter of the method call public tracehandler object t target t public object invoke object proxy method m object args throws throwable print implicit argument system out print target print method name system out print m getname print explicit arguments if args null for int i i args length i system out print args i if i args length system out print system out println invoke actual method return m invoke target args properties of proxy classes now that you have seen proxy classes in action let go over some of their properties remember that proxy classes are created on the fly in a running program however once they are created they are regular classes just like any other classes in the virtual machine all proxy classes extend the class proxy a proxy class has only one instance field the invocation handler which is defined in the proxy superclass any additional data required to carry out the proxy objects tasks must be stored in the invocation handler for example when we proxied comparable objects in the program shown in listing the tracehandler wrapped the actual objects all proxy classes override the tostring equals and hashcode methods of the object class like all proxy methods these methods simply call invoke on the invocation handler the other methods of the object class such as clone and getclass are not redefined the names of proxy classes are not defined the proxy class in oracle virtual machine generates class names that begin with the string proxy there is only one proxy class for a particular class loader and ordered set of interfaces that is if you call the newproxyinstance method twice with the same class loader and interface array you get two objects of the same class you can also obtain that class with the getproxyclass method class proxyclass proxy getproxyclass null interfaces a proxy class is always public and final if all interfaces that the proxy class implements are public the proxy class does not belong to any particular package otherwise all non public interfaces must belong to the same package and the proxy class will also belong to that package you can test whether a particular class object represents a proxy class by calling the isproxyclass method of the proxy class java lang reflect invocationhandler object invoke object proxy method method object args define this method to contain the action that you want carried out whenever a method was invoked on the proxy object java lang reflect proxy static class getproxyclass classloader loader class interfaces returns the proxy class that implements the given interfaces static object newproxyinstance classloader loader class interfaces invocationhandler handler constructs a new instance of the proxy class that implements the given interfaces all methods call the invoke method of the given handler object static boolean isproxyclass class c returns true if c is a proxy class this ends our final chapter on the fundamentals of the java programming language interfaces and inner classes are concepts that you will encounter frequently however as we already mentioned proxies are an advanced technique that is of interest mainly to tool builders not application programmers you are now ready to go on to learn about graphics and user interfaces starting with chapter chapter graphics programming in this chapter introducing swing creating a frame positioning a frame displaying information in a component working with shapes using color using special fonts for text displaying images to this point you have seen only how to write programs that take input from the keyboard fuss with it and display the results on a console screen this is not what most users want now modern programs don t work this way and neither do web pages this chapter starts you on the road to writing java programs that use a graphical user interface gui in particular you will learn how to write programs that size and locate windows on the screen display text with multiple fonts in a window display images and so on this gives you a useful valuable repertoire of skills that you will put to good use in subsequent chapters as you write interesting programs the next two chapters show you how to process events such as keystrokes and mouse clicks and how to add interface elements such as menus and buttons to your applications when you finish these three chapters you will know the essentials of writing graphical applications for more sophisticated graphics programming techniques we refer you to volume ii if on the other hand you intend to use java for server side programming only and are not interested in writing gui programming you can safely skip these chapters introducing swing when java was introduced it contained a class library which sun called the abstract window toolkit awt for basic gui programming the basic awt library deals with user interface elements by delegating their creation and behavior to the native gui toolkit on each target platform windows solaris macintosh and so on for example if you used the original awt to put a text box on a java window an underlying peer text box actually handled the text input the resulting program could then in theory run on any of these platforms with the look and feel of the target platform hence sun trademarked slogan write once run anywhere the peer based approach worked well for simple applications but it soon became apparent that it was fiendishly difficult to write a high quality portable graphics library depending on native user interface elements user interface elements such as menus scrollbars and text fields can have subtle differences in behavior on different platforms it was hard therefore to give users a consistent and predictable experience with this approach moreover some graphical environments such as motif do not have as rich a collection of user interface components as does windows or the macintosh this in turn further limits a portable library based on a lowest common denominator approach as a result gui applications built with the awt simply did not look as nice as native windows or macintosh applications nor did they have the kind of functionality that users of those platforms had come to expect more depressingly there were different bugs in the awt user interface library on the different platforms developers complained that they had to test their applications on each platform a practice derisively called write once debug everywhere in netscape created a gui library they called the ifc internet foundation classes that used an entirely different approach user interface elements such as buttons menus and so on were painted onto blank windows the only functionality required from the underlying windowing system was a way to put up windows and to paint on the window thus netscape ifc widgets looked and behaved the same no matter which platform the program ran on sun worked with netscape to perfect this approach creating a user interface library with the code name swing swing was available as an extension to java and became a part of the standard library in java se since as duke ellington said it don t mean a thing if it ain t got that swing swing is now the official name for the non peer based gui toolkit swing is part of the java foundation classes jfc the full jfc is vast and contains far more than the swing gui toolkit besides the swing components it also has an accessibility api a api and a drag and drop api note swing is not a complete replacement for the awt it is built on top of the awt architecture swing simply gives you more capable user interface components whenever you write a swing program you use the foundations of the awt in particular event handling from now on we say swing when we mean the painted user interface classes and we say awt when we mean the underlying mechanisms of the windowing toolkit such as event handling of course swing based user interface elements will be somewhat slower to appear on the user screen than the peer based components used by the awt in our experience on any reasonably modern machine the speed difference shouldn t be a problem on the other hand the reasons to choose swing are overwhelming swing has a rich and convenient set of user interface elements swing has few dependencies on the underlying platform it is therefore less prone to platformspecific bugs swing gives a consistent user experience across platforms still the third plus is also a potential drawback if the user interface elements look the same on all platforms they look different from the native controls so users will be less familiar with them swing solves this problem in a very elegant way programmers writing swing programs can give the program a specific look and feel for example figures and show the same program running with the windows and the gtk look and feel figure the windows look and feel of swing figure the gtk look and feel of swing furthermore sun developed a platform independent look and feel that was called metal until the marketing folks renamed it into java look and feel however most programmers continue to use the term metal and we will do the same in this book some people criticized metal for being stodgy and the look was freshened up for the java se release see figure now the metal look supports multiple themes minor variations in colors and fonts the default theme is called ocean figure the ocean theme of the metal look and feel in java se sun improved the support for the native look and feel for windows and gtk a swing application will now pick up the color scheme customizations and faithfully render the throbbing buttons and scrollbars that have become fashionable java offers a new look and feel called nimbus figure but it is not available by default nimbus uses vector drawings not bitmaps and is therefore independent of the screen resolution figure the nimbus look and feel some users prefer their java applications to use the native look and feel of their platforms others like metal or a third party look and feel as you will see in chapter it is very easy to let your users choose their favorite look and feel note although we won t have space in this book to tell you how to do it java programmers can extend an existing look and feel or even design a totally new one this is a tedious process that involves specifying how each swing component is painted some developers have done just that especially when porting java to nontraditional platforms such as kiosk terminals or handheld devices see www javootoo com for a collection of interesting lookand feel implementations java se introduced a look and feel called synth that makes this process easier in synth you can define a new look and feel by providing image files and xml descriptors without doing any programming tip the napkin look and feel http napkinlaf sourceforge net gives a hand drawn appearance to all user interface elements this is very useful when you show prototypes to your customers sending a clear message that you re not giving them a finished product note most java user interface programming is nowadays done in swing with one notable exception the eclipse integrated development environment uses a graphics toolkit called swt that is similar to the awt mapping to the native components on various platforms you can find articles describing swt at www eclipse org articles oracle is developing an alternate technology called javafx that may at some point become a replacement for swing we do not discuss javafx in this book see www oracle com technetwork java javafx overview for more information if you have programmed microsoft windows applications with visual basic or c you know about the ease of use that comes with the graphical layout tools and resource editors these products provide these tools let you design the visual appearance of your application and then they generate much often all of the gui code for you gui builders are available for java programming but we feel that in order to use these tools effectively you should know how to build a user interface manually the remainder of this chapter shows you the basics of displaying windows and painting their contents creating a frame a top level window that is a window that is not contained inside another window is called a frame in java the awt library has a class called frame for this top level the swing version of this class is called jframe and extends the frame class the jframe is one of the few swing components that is not painted on a canvas thus the decorations buttons title bar icons and so on are drawn by the user windowing system not by swing caution most swing component classes start with a j jbutton jframe and so on there are classes such as button and frame but they are awt components if you accidentally omit a j your program may still compile and run but the mixture of swing and awt components can lead to visual and behavioral inconsistencies in this section we will go over the most common methods for working with a swing jframe listing lists a simple program that displays an empty frame on the screen as illustrated in figure figure the simplest visible frame let work through this program line by line the swing classes are placed in the javax swing package the package name javax indicates a java extension package not a core package for historical reasons swing is considered an extension however it is present in every java se implementation since version listing simpleframe simpleframetest java click here to view code image package simpleframe import java awt import javax swing version author cay horstmann public class simpleframetest public static void main string args eventqueue invokelater new runnable 16 public void run simpleframe frame new simpleframe frame setdefaultcloseoperation jframe frame setvisible true class simpleframe extends jframe private static final int private static final int public simpleframe setsize by default a frame has a rather useless size of pixels we define a subclass simpleframe whose constructor sets the size to pixels this is the only difference between a simpleframe and a jframe in the main method of the simpleframetest class we construct a simpleframe object and make it visible there are two technical issues that we need to address in every swing program first all swing components must be configured from the event dispatch thread the thread of control that passes events such as mouse clicks and keystrokes to the user interface components the following code fragment is used to execute statements in the event dispatch thread eventqueue invokelater new runnable public void run statements we discuss the details in chapter for now you should simply consider it a magic incantation that is used to start a swing program note you will see many swing programs that do not initialize the user interface in the event dispatch thread it used to be perfectly acceptable to carry out the initialization in the main thread sadly as swing components got more complex the developers of the jdk were no longer able to guarantee the safety of that approach the probability of an error is extremely low but you would not want to be one of the unlucky few who encounter an intermittent problem it is better to do the right thing even if the code looks rather mysterious next we define what should happen when the user closes the application frame for this particular program we want the program to exit to select this behavior we use the statement frame setdefaultcloseoperation jframe in other programs with multiple frames you would not want the program to exit just because the user closes one of the frames by default a frame is hidden when the user closes it but the program does not terminate it might have been nice if the program terminated once the last frame becomes invisible but that is not how swing works simply constructing a frame does not automatically display it frames start their life invisible that gives the programmer the chance to add components into the frame before showing it for the first time to show the frame the main method calls the setvisible method of the frame note before java se it was possible to use the show method that the jframe class inherits from the superclass window the window class has a superclass component that also has a show method the component show method was deprecated in java se you are supposed to call setvisible true instead if you want to show a component however until java se the window show method was not deprecated in fact it was quite useful making the window visible and bringing it to the front sadly that benefit was lost on the deprecation police and java se deprecated the show method for windows as well after scheduling the initialization statements the main method exits note that exiting main does not terminate the program just the main thread the event dispatch thread keeps the program alive until it is terminated either by closing the frame or by calling the system exit method the running program is shown in figure on p it is a truly boring top level window as you can see in the figure the title bar and the surrounding decorations such as resize corners are drawn by the operating system and not the swing library if you run the same program in windows gtk and the mac the frame decorations will be different the swing library draws everything inside the frame in this program it just fills the frame with a default background color note as of java se you can turn off all frame decorations by calling frame setundecorated true positioning a frame the jframe class itself has only a few methods for changing how frames look of course through the magic of inheritance most of the methods for working with the size and position of a frame come from the various superclasses of jframe here are some of the most important methods the setlocation and setbounds methods for setting the position of the frame the seticonimage method which tells the windowing system which icon to display in the title bar task switcher window and so on the settitle method for changing the text in the title bar the setresizable method which takes a boolean to determine if a frame will be resizeable by the user figure illustrates the inheritance hierarchy for the jframe class figure inheritance hierarchy for the frame and component classes in awt and swing tip the api notes for this section give what we think are the most important methods for giving frames the proper look and feel some of these methods are defined in the jframe class others come from the various superclasses of jframe at some point you may need to search the api docs to see if there are methods for some special purpose unfortunately that is a bit tedious to do with inherited methods for example the tofront method is applicable to objects of type jframe but since it simply inherited from the window class the jframe documentation doesn t explain it if you feel that there should be a method to do something and it isn t mentioned in the documentation for the class you are working with try looking at the api documentation for the methods of the superclasses of that class the top of each api page has hyperlinks to the superclasses and inherited methods are listed below the method summary for the new and overridden methods as the api notes indicate the component class which is the ancestor of all gui objects and the window class which is the superclass of the frame class are where you need to look for the methods to resize and reshape frames for example the setlocation method in the component class is one way to reposition a component if you make the call setlocation x y the top left corner is located x pixels across and y pixels down where is the top left corner of the screen similarly the setbounds method in component lets you resize and relocate a component in particular a jframe in one step as setbounds x y width height alternatively you can give the windowing system control over window placement if you call setlocationbyplatform true before displaying the window the windowing system picks the location but not the size typically with a slight offset from the last window note for a frame the coordinates of the setlocation and setbounds are taken relative to the whole screen as you will see in chapter for other components inside a container the measurements are taken relative to the container frame properties many methods of component classes come in getter setter pairs such as the following methods of the frame class public string gettitle public void settitle string title such a getter setter pair is called a property a property has a name and a type the name is obtained by changing the first letter after the get or set to lowercase for example the frame class has a property with name title and type string conceptually title is a property of the frame when we set the property we expect the title to change on the user screen when we get the property we expect to get back the value that we have set we do not know or care how the frame class implements this property perhaps it simply uses its peer frame to store the title perhaps it has an instance field private string title not required for property if the class does have a matching instance field we don t know or care how the getter and setter methods are implemented perhaps they just read and write the instance field perhaps they do more such as notifying the windowing system whenever the title changes there is one exception to the get set convention for properties of type boolean the getter starts with is for example the following two methods define the locationbyplatform property click here to view code image public boolean islocationbyplatform public void setlocationbyplatform boolean b we will look at properties in much greater detail in chapter of volume ii note many programming languages in particular visual basic and c have built in support for properties it is possible that a future version of java may also add a language construct for properties determining a good frame size remember if you don t explicitly size a frame all frames will default to by pixels to keep our example programs simple we resize the frames to a size that we hope works acceptably on most displays however in a professional application you should check the resolution of the user screen and write code that resizes the frames accordingly a window that looks nice on a laptop screen will look like a postage stamp on a high resolution screen to find out the screen size use the following steps call the static getdefaulttoolkit method of the toolkit class to get the toolkit object the toolkit class is a dumping ground for a variety of methods interfacing with the native windowing system then call the getscreensize method which returns the screen size as a dimension object a dimension object simultaneously stores a width and a height in public instance variables width and height here is the code click here to view code image toolkit kit toolkit getdefaulttoolkit dimension screensize kit getscreensize int screenwidth screensize width int screenheight screensize height we use of these values for the frame size and tell the windowing system to position the frame click here to view code image setsize screenwidth screenheight setlocationbyplatform true we also supply an icon the imageicon class is convenient for loading images here is how you use it click here to view code image image img new imageicon icon gif getimage seticonimage img depending on your operating system you can see the icon in various places for example in windows the icon is displayed in the top left corner of the window and you can see it in the list of active tasks when you press alt tab listing is the complete program when you run the program pay attention to the core java icon here are a few additional tips for dealing with frames if your frame contains only standard components such as buttons and text fields you can simply call the pack method to set the frame size the frame will be set to the smallest size that contains all components it is quite common to set the main frame of a program to the maximum size as of java se you can simply maximize a frame by calling frame setextendedstate frame it is also a good idea to remember how the user positions and sizes the frame of your application and restore those bounds when you start the application again you will see in chapter how to use the preferences api for this purpose listing sizedframe sizedframetest java click here to view code image package sizedframe import java awt import javax swing version 04 author cay horstmann public class sizedframetest public static void main string args eventqueue invokelater new runnable 16 public void run 18 jframe frame new sizedframe frame settitle sizedframe frame setdefaultcloseoperation jframe frame setvisible true class sizedframe extends jframe public sizedframe get screen dimensions toolkit kit toolkit getdefaulttoolkit dimension screensize kit getscreensize int screenheight screensize height int screenwidth screensize width set frame width height and let platform pick screen location setsize screenwidth screenheight setlocationbyplatform true set frame icon image img new imageicon icon gif getimage seticonimage img if you write an application that takes advantage of multiple display screens use the graphicsenvironment and graphicsdevice classes to find the dimensions of the display screens the graphicsdevice class also lets you execute your application in full screen mode java awt component boolean isvisible void setvisible boolean b gets or sets the visible property components are initially visible with the exception of top level components such as jframe void setsize int width int height resizes the component to the specified width and height void setlocation int x int y moves the component to a new location the x and y coordinates use the coordinates of the container if the component is not a top level component or the coordinates of the screen if the component is top level for example a jframe void setbounds int x int y int width int height moves and resizes this component dimension getsize void setsize dimension d gets or sets the size property of this component java awt window void tofront shows this window on top of any other windows void toback moves this window to the back of the stack of windows on the desktop and rearranges all other visible windows accordingly boolean islocationbyplatform void setlocationbyplatform boolean b gets or sets the locationbyplatform property when the property is set before this window is displayed the platform picks a suitable location java awt frame boolean isresizable void setresizable boolean b gets or sets the resizable property when the property is set the user can resize the frame string gettitle void settitle string gets or sets the title property that determines the text in the title bar for the frame image geticonimage void seticonimage image image gets or sets the iconimage property that determines the icon for the frame the windowing system may display the icon as part of the frame decoration or in other locations boolean isundecorated void setundecorated boolean b gets or sets the undecorated property when the property is set the frame is displayed without decorations such as a title bar or close button this method must be called before the frame is displayed int getextendedstate void setextendedstate int state gets or sets the extended window state the state is one of frame normal frame iconified frame frame frame java awt toolkit static toolkit getdefaulttoolkit returns the default toolkit dimension getscreensize gets the size of the user screen javax swing imageicon imageicon string filename constructs an icon whose image is stored in a file image getimage gets the image of this icon displaying information in a component in this section we will show you how to display information inside a frame for example instead of displaying not a hello world program in text mode in a console window as we did in chapter we display the message in a frame as shown in figure figure a frame that displays information you could draw the message string directly onto a frame but that is not considered good programming practice in java frames are really designed to be containers for components such as a menu bar and other user interface elements you normally draw on another component which you add to the frame the structure of a jframe is surprisingly complex look at figure which shows the makeup of a jframe as you can see four panes are layered in a jframe the root pane layered pane and glass pane are of no interest to us they are required to organize the menu bar and content pane and to implement the look and feel the part that most concerns swing programmers is the content pane when designing a frame you add components into the content pane using code such as the following click here to view code image container contentpane frame getcontentpane component c contentpane add c figure internal structure of a jframe up to java se the add method of the jframe class was defined to throw an exception with the message do not use jframe add use jframe getcontentpane add instead as of java se the jframe add method has given up trying to reeducate programmers and simply calls add on the content pane thus you can simply use the call frame add c in our case we want to add a single component to the frame onto which we will draw our message to draw on a component you define a class that extends jcomponent and override the paintcomponent method in that class t h e paintcomponent method takes one parameter of type graphics a graphics object remembers a collection of settings for drawing images and text such as the font you set or the current color all drawing in java must go through a graphics object it has methods that draw patterns images and text note the graphics parameter is similar to a device context in windows or a graphics context in programming here how to make a component onto which you can draw class mycomponent extends jcomponent public void paintcomponent graphics g code for drawing each time a window needs to be redrawn no matter what the reason the event handler notifies the component this causes the paintcomponent methods of all components to be executed never call the paintcomponent method yourself it is called automatically whenever a part of your application needs to be redrawn and you should not interfere with this automatic process what sorts of actions trigger this automatic response for example painting occurs when the user increases the size of the window or minimizes and then restores the window if the user popped up another window that covered an existing window and then made the overlaid window disappear the window that was covered is now corrupted and will need to be repainted the graphics system does not save the pixels underneath and of course when the window is displayed for the first time it needs to process the code that specifies how and where it should draw the initial elements tip if you need to force repainting of the screen call the repaint method instead of paintcomponent the repaint method will cause paintcomponent to be called for all components with a properly configured graphics object as you saw in the code fragment above the paintcomponent method takes a single parameter of type graphics measurement on a graphics object for screen display is done in pixels the coordinate denotes the top left corner of the component on whose surface you are drawing displaying text is considered a special kind of drawing the graphics class has a drawstring method that has the following syntax g drawstring text x y in our case we want to draw the string not a hello world program in our original window roughly one quarter of the way across and halfway down although we don t yet know how to measure the size of the string we ll start the string at coordinates this means the first character in the string will start at a position pixels to the right and pixels down actually it is the baseline for the text that is pixels down see page for more on how text is measured thus our paintcomponent method looks like this click here to view code image class nothelloworldcomponent extends jcomponent public static final int public static final int public void paintcomponent graphics g g drawstring not a hello world program finally a component should tell its users how big it would like to be override the getpreferredsize method and return an object of the dimension class with the preferred width and height click here to view code image class nothelloworldcomponent extends jcomponent private static final int private static final int public dimension getpreferredsize return new dimension when you fill a frame with one or more components and you simply want to use their preferred size call the pack method instead of the setsize method click here to view code image class nothelloworldframe extends jframe public nothelloworldframe add new nothelloworldcomponent pack listing shows the complete code note instead of extending jcomponent some programmers prefer to extend the jpanel class a jpanel is intended to be a container that can contain other components but it is also possible to paint on it there is just one difference a panel is opaque which means that it is responsible for painting all pixels within its bounds the easiest way to achieve that is to paint the panel with the background color by calling super paintcomponent in the paintcomponent method of each panel subclass class nothelloworldpanel extends jpanel public void paintcomponent graphics g super paintcomponent g code for drawing listing nothelloworld nothelloworld java click here to view code image package nothelloworld import javax swing import java awt version author cay horstmann public class nothelloworld public static void main string args eventqueue invokelater new runnable 16 public void run 18 jframe frame new nothelloworldframe frame settitle nothelloworld frame setdefaultcloseoperation jframe frame setvisible true a frame that contains a message panel class nothelloworldframe extends jframe public nothelloworldframe add new nothelloworldcomponent pack a component that displays a message class nothelloworldcomponent extends jcomponent public static final int public static final int private static final int private static final int public void paintcomponent graphics g g drawstring not a hello world program public dimension getpreferredsize return new dimension javax swing jframe container getcontentpane returns the content pane object for this jframe component add component c adds and returns the given component to the content pane of this frame before java se this method threw an exception java awt component void repaint causes a repaint of the component as soon as possible dimension getpreferredsize is the method to override to return the preferred size of this component javax swing jcomponent void paintcomponent graphics g is the method to override to describe how your component needs to be painted java awt window void pack resizes this window taking into account the preferred sizes of its components working with shapes starting with java the graphics class has methods to draw lines rectangles ellipses and so on but those drawing operations are very limited for example you cannot vary the line thickness and cannot rotate the shapes java se introduced the java library which implements a powerful set of graphical operations in this chapter we only look at the basics of the java library see chapter in volume ii for more information on the advanced features to draw shapes in the java library you need to obtain an object of the class this class is a subclass of the graphics class ever since java se methods such as paintcomponent automatically receive an object of the class simply use a cast as follows public void paintcomponent graphics g g the java library organizes geometric shapes in an object oriented fashion in particular there are classes to represent lines rectangles and ellipses these classes all implement the shape interface note the java library supports more complex shapes in particular arcs quadratic and cubic curves and general paths see chapter of volume ii for more information to draw a shape you first create an object of a class that implements the shape interface and then call the draw method of the class for example rect draw rect note before the java library appeared programmers used methods of the graphics class such as drawrectangle to draw shapes superficially the old style method calls look a bit simpler however by using the java library you keep your options open you can later enhance your drawings with some of the many tools that the java library supplies using the java shape classes introduces some complexity unlike the draw methods which used integer pixel coordinates java shapes use floating point coordinates in many cases that is a great convenience because it allows you to specify your shapes in coordinates that are meaningful to you such as millimeters or inches and then translate them to pixels the java library uses singleprecision float quantities for many of its internal floating point calculations single precision is sufficient after all the ultimate purpose of the geometric computations is to set pixels on the screen or printer as long as any roundoff errors stay within one pixel the visual outcome is not affected furthermore float computations are faster on some platforms and float values require half the storage of double values however manipulating float values is sometimes inconvenient for the programmer because java is adamant about requiring casts when converting double values into float values for example consider the following statement float f error this statement does not compile because the constant has type double and the compiler is nervous about loss of precision the remedy is to add an f suffix to the floating point constant float f ok now consider this statement r float f r getwidth error this statement does not compile either for the same reason the getwidth method returns a double this time the remedy is to provide a cast float f float r getwidth ok these suffixes and casts are a bit of a pain so the designers of the library decided to supply two versions of each shape class one with float coordinates for frugal programmers and one with double coordinates for the lazy ones in this book we fall into the second camp and use double coordinates whenever we can the library designers chose a curious and initially confusing method for packaging these choices consider the class this is an abstract class with two concrete subclasses which are also static inner classes float double figure shows the inheritance diagram figure rectangle classes it is best to ignore the fact that the two concrete classes are static inner classes that is just a gimmick to avoid names such as and for more information on static inner classes see chapter when you construct a float object you supply the coordinates as float numbers for a double object you supply them as double numbers click here to view code image float floatrect new float 0f double doublerect new double actually since both float and double extend the common class and the methods in the subclasses simply override those in the superclass there is no benefit in remembering the exact shape type you can simply use variables to hold the rectangle references click here to view code image floatrect new float 0f 0f 0f doublerect new double that is you only need to use the pesky inner classes when you construct the shape objects the construction parameters denote the top left corner width and height of the rectangle note actually the float class has one additional method that is not inherited from namely setrect float x float y float h float w you lose that method if you store the float reference in a variable but it is not a big loss the class has a setrect method with double parameters the methods use double parameters and return values for example the getwidth method returns a double value even if the width is stored as a float in a float object tip simply use the double shape classes to avoid dealing with float values altogether however if you are constructing thousands of shape objects consider using the float classes to conserve memory what we just discussed for the classes holds for the other shape classes as well furthermore there is a class with subclasses float and double here is how to make a point object p new double tip t he class is very useful it is more object oriented to work with objects than with separate x and y values many constructors and methods accept parameters we suggest that you use objects when you can they usually make geometric computations easier to understand the classes and both inherit from the common superclass rectangularshape admittedly ellipses are not rectangular but they have a bounding rectangle see figure figure the bounding rectangle of an ellipse the rectangularshape class defines over methods that are common to these shapes among them such useful methods as getwidth getheight getcenterx and getcentery but sadly at the time of this writing not a getcenter method that would return the center as a object finally a couple of legacy classes from java have been fitted into the shape class hierarchy the rectangle and point classes which store a rectangle and a point with integer coordinates extend the and classes figure shows the relationships between the shape classes however the double and float subclasses are omitted legacy classes are marked with a gray fill figure relationships between the shape classes and objects are simple to construct you need to specify the x and y coordinates of the top left corner and the width and height for ellipses these refer to the bounding rectangle for example e new double constructs an ellipse that is bounded by a rectangle with the top left corner at width of and height of however sometimes you don t have the top left corner readily available it is quite common to have two diagonal corner points of a rectangle but perhaps they aren t the top left and bottom right corners you can t simply construct a rectangle as rect new double px py qx px qy py error if p isn t the top left corner one or both of the coordinate differences will be negative and the rectangle will come out empty in that case first create a blank rectangle and use the setframefromdiagonal method as follows rect new double rect setframefromdiagonal px py qx qy or even better if you have the corner points as objects p and q use rect setframefromdiagonal p q when constructing an ellipse you usually know the center width and height but not the corner points of the bounding rectangle which don t even lie on the ellipse the setframefromcenter method uses the center point but it still requires one of the four corner points thus you will usually end up constructing an ellipse as follows ellipse new double centerx width centery height width height to construct a line you supply the start and end points either as objects or as pairs of numbers line new double start end or line new double startx starty endx endy the program in listing draws a rectangle the ellipse that is enclosed in the rectangle a diagonal of the rectangle and a circle that has the same center as the rectangle figure shows the result figure 12 drawing geometric shapes listing draw drawtest java click here to view code image package draw import java awt import java awt geom import javax swing version 04 author cay horstmann public class drawtest 12 public static void main string args eventqueue invokelater new runnable 16 public void run 18 jframe frame new drawframe frame settitle drawtest frame setdefaultcloseoperation jframe frame setvisible true a frame that contains a panel with drawings class drawframe extends jframe public drawframe add new drawcomponent pack a component that displays rectangles and ellipses class drawcomponent extends jcomponent private static final int private static final int public void paintcomponent graphics g g draw a rectangle double leftx double topy double width 200 double height rect new double leftx topy width height draw rect draw the enclosed ellipse ellipse new double ellipse setframe rect draw ellipse draw a diagonal line draw new double leftx topy leftx width topy height draw a circle with the same center double centerx rect getcenterx double centery rect getcentery double radius circle new double circle setframefromcenter centerx centery centerx radius centery radius draw circle public dimension getpreferredsize return new dimension java awt geom rectangularshape double getcenterx double getcentery double getminx double getminy double getmaxx double getmaxy returns the center minimum or maximum x or y value of the enclosing rectangle double getwidth double getheight returns the width or height of the enclosing rectangle double getx double gety returns the x or y coordinate of the top left corner of the enclosing rectangle java awt geom double double double x double y double w double h constructs a rectangle with the given top left corner width and height java awt geom float float float x float y float w float h constructs a rectangle with the given top left corner width and height java awt geom double ellipse2d double double x double y double w double h constructs an ellipse whose bounding rectangle has the given top left corner width and height java awt geom double double double x double y constructs a point with the given coordinates java awt geom double double start end double double startx double starty double endx double endy constructs a line with the given start and end points using color t he setpaint method of the class lets you select a color that is used for all subsequent drawing operations on the graphics context for example setpaint color red drawstring warning you can fill the interiors of closed shapes such as rectangles or ellipses with a color simply call fill instead of draw rect setpaint color red fill rect fills rect with red color to draw in multiple colors select a color draw or fill then select another color and draw or fill again note the fill method paints one fewer pixel to the right and the bottom for example if you draw a new double 20 then the drawing includes the pixels with x and y 20 if you fill the same rectangle those pixels are not painted define colors with the color class the java awt color class offers predefined constants for the following standard colors black blue cyan gray green magenta orange pink red white yellow note before java se color constant names were lowercase such as color red this is odd because the standard coding convention is to write constants in uppercase you can now write the standard color names in uppercase or for backward compatibility lowercase you can specify a custom color by creating a color object by its red green and blue components using a scale of 255 that is one byte for the redness blueness and greenness call the color constructor like this color int redness int greenness int blueness here is an example of setting a custom color click here to view code image setpaint new color 128 a dull blue green drawstring welcome 125 note in addition to solid colors you can select more complex paint settings such as varying hues or images see the advanced awt chapter in volume ii for more details if you use a graphics object instead of a object you need to use the setcolor method to set colors to set the background color use the setbackground method of the component class an ancestor of jcomponent mycomponent p new mycomponent p setbackground color pink there is also a setforeground method it specifies the default color that is used for drawing on the component tip t he brighter and darker methods of the color class produce as their names suggest either brighter or darker versions of the current color using the brighter method is also a good way to highlight an item actually brighter is just a little bit brighter to make a color really stand out apply it three times c brighter brighter brighter java gives you predefined names for many more colors in its systemcolor class the constants in this class encapsulate the colors used for various elements of the user system for example p setbackground systemcolor window sets the background color of the component to the default used by all windows on the user desktop the background is filled in whenever the window is repainted using the colors in the systemcolor class is particularly useful when you want to draw user interface elements so that the colors match those already found on the user desktop table lists the system color names and their meanings table system colors java awt color color int r int g int b creates a color object java awt graphics color getcolor void setcolor color c gets or sets the current color all subsequent graphics operations will use the new color java awt paint getpaint void setpaint paint p gets or sets the paint property of this graphics context the color class implements the paint interface therefore you can use this method to set the paint attribute to a solid color void fill shape fills the shape with the current paint java awt component color getbackground void setbackground color c gets or sets the background color color getforeground void setforeground color c gets or sets the foreground color using special fonts for text the not a hello world program at the beginning of this chapter displayed a string in the default font often you will want to show your text in a different font you can specify a font by its font face name a font face name is composed of a font family name such as helvetica and an optional suffix such as bold for example the font faces helvetica and helvetica bold are both considered to be part of the family named helvetica to find out which fonts are available on a particular computer call the getavailablefontfamilynames method of the graphicsenvironment class the method returns an array of strings containing the names of all available fonts to obtain an instance of the graphicsenvironment class that describes the graphics environment of the user system use the static getlocalgraphicsenvironment method the following program prints the names of all fonts on your system import java awt public class listfonts public static void main string args string fontnames graphicsenvironment getlocalgraphicsenvironment getavailablefontfamilynames for string fontname fontnames system out println fontname on one system the list starts out like this abadi mt condensed light arial arial black arial narrow arioso baskerville binner gothic and goes on for another seventy or so fonts font face names can be trademarked and font designs can be copyrighted in some jurisdictions thus the distribution of fonts often involves royalty payments to a font foundry of course just as there are inexpensive imitations of famous perfumes there are lookalikes for name brand fonts for example the helvetica imitation that is shipped with windows is called arial to establish a common baseline the awt defines five logical font names sansserif serif monospaced dialog dialoginput these names are always mapped to some fonts that actually exist on the client machine for example on a windows system sansserif is mapped to arial in addition the oracle jdk always includes three font families named lucida sans lucida bright and lucida sans typewriter to draw characters in a font you must first create an object of the class font specify the font face name the font style and the point size here is an example of how you construct a font object font new font sansserif font bold the third argument is the point size points are commonly used in typography to indicate the size of a font there are points per inch you can use a logical font name in place of the font face name in the font constructor specify the style plain bold italic or bold italic by setting the second font constructor argument to one of the following values font plain font bold font italic font bold font italic note the mapping from logical to physical font names is defined in the fontconfig properties file in the jre lib subdirectory of the java installation see http docs oracle com javase docs technotes guides intl fontconfig html for information on this file you can read font files in truetype opentype or postscript type formats you need an input stream for the font typically from a file or url see chapter of volume ii for more information on streams then call the static font createfont method click here to view code image url url new url http www fonts com wingbats ttf inputstream in url openstream font font createfont font in the font is plain with a font size of point use the derivefont method to get a font of the desired size font f derivefont 0f caution there are two overloaded versions of the derivefont method one of them with a float parameter sets the font size the other with an int parameter sets the font style thus derivefont sets the style and not the size the result is an italic font because it happens that the binary representation of has the italic bit but not the bold bit set the java fonts contain the usual ascii characters as well as symbols for example if you print the character in the dialog font you get a character only the symbols defined in the unicode character set are available here the code that displays the string hello world in the standard sans serif font on your system using point bold type click here to view code image font new font sansserif font bold setfont string message hello world drawstring message 100 next let center the string in its component instead of drawing it at an arbitrary position we need to know the width and height of the string in pixels these dimensions depend on three factors the font used in our case sans serif bold 14 point the string in our case hello world and the device on which the font is drawn in our case the user screen to obtain an object that represents the font characteristics of the screen device call the getfontrendercontext method of the class it returns an object of the fontrendercontext class simply pass that object to the getstringbounds method of the font class click here to view code image fontrendercontext context getfontrendercontext bounds f getstringbounds message context the getstringbounds method returns a rectangle that encloses the string to interpret the dimensions of that rectangle you should know some basic typesetting terms see figure the baseline is the imaginary line where for example the bottom of a character like e rests the ascent is the distance from the baseline to the top of an ascender which is the upper part of a letter like b or k or an uppercase character the descent is the distance from the baseline to a descender which is the lower portion of a letter like p or g figure typesetting terms illustrated leading is the space between the descent of one line and the ascent of the next line the term has its origin from the strips of lead that typesetters used to separate lines the height of a font is the distance between successive baselines which is the same as descent leading ascent the width of the rectangle that the getstringbounds method returns is the horizontal extent of the string the height of the rectangle is the sum of ascent descent and leading the rectangle has its origin at the baseline of the string the top y coordinate of the rectangle is negative thus you can obtain string width height and ascent as follows click here to view code image double stringwidth bounds getwidth double stringheight bounds getheight double ascent bounds gety if you need to know the descent or leading use the getlinemetrics method of the font class that method returns an object of the linemetrics class which has methods to obtain the descent and leading click here to view code image linemetrics metrics f getlinemetrics message context float descent metrics getdescent float leading metrics getleading the following code uses all this information to center a string in its surrounding component click here to view code image fontrendercontext context getfontrendercontext bounds f getstringbounds message context x y top left corner of text double x getwidth bounds getwidth double y getheight bounds getheight add ascent to y to reach the baseline double ascent bounds gety double basey y ascent drawstring message int x int basey to understand the centering consider that getwidth returns the width of the component a portion of that width namely bounds getwidth is occupied by the message string the remainder should be equally distributed on both sides therefore the blank space on each side is half the difference the same reasoning applies to the height note when you need to compute layout dimensions outside the paintcomponent method you can t obtain the font render context from the object instead call the getfontmetrics method of the jcomponent class and then call getfontrendercontext fontrendercontext context getfontmetrics f getfontrendercontext to show that the positioning is accurate the sample program also draws the baseline and the bounding rectangle figure 14 shows the screen display listing is the program listing figure 14 drawing the baseline and string bounds listing font fonttest java click here to view code image package font import java awt import java awt font import java awt geom import javax swing version 04 14 author cay horstmann 12 public class fonttest 14 public static void main string args 16 eventqueue invokelater new runnable 18 public void run 20 jframe frame new fontframe frame settitle fonttest frame setdefaultcloseoperation jframe frame setvisible true 25 a frame with a text message component class fontframe extends jframe public fontframe add new fontcomponent pack a component that shows a centered message in a box class fontcomponent extends jcomponent private static final int private static final int 200 public void paintcomponent graphics g g string message hello world font f new font serif font bold setfont f measure the size of the message fontrendercontext context getfontrendercontext bounds f getstringbounds message context set x y top left corner of text double x getwidth bounds getwidth double y getheight bounds getheight add ascent to y to reach the baseline double ascent bounds gety double basey y ascent draw the message drawstring message int x int basey setpaint color draw the baseline draw new double x basey x bounds getwidth basey draw the enclosing rectangle rect new double x y bounds getwidth bounds getheight g2 draw rect public dimension getpreferredsize return new dimension java awt font font string name int style int size creates a new font object string getfontname gets the font face name such as helvetica bold string getfamily gets the font family name such as helvetica string getname gets the logical name such as sansserif if the font was created with a logical font name otherwise gets the font face name getstringbounds string fontrendercontext context returns a rectangle that encloses the string the origin of the rectangle falls on the baseline the top y coordinate of the rectangle equals the negative of the ascent the height of the rectangle equals the sum of ascent descent and leading the width equals the string width linemetrics getlinemetrics string fontrendercontext context returns a line metrics object to determine the extent of the string font derivefont int style font derivefont float size font derivefont int style float size returns a new font that is equal to this font except that it has the given size and style java awt font linemetrics float getascent gets the font ascent the distance from the baseline to the tops of uppercase characters float getdescent gets the font descent the distance from the baseline to the bottoms of descenders float getleading gets the font leading the space between the bottom of one line of text and the top of the next line float getheight gets the total height of the font the distance between the two baselines of text descent leading ascent java awt graphics font getfont void setfont font font gets or sets the current font that font will be used for subsequent text drawing operations void drawstring string str int x int y draws a string in the current font and color java awt fontrendercontext getfontrendercontext gets a font render context that specifies font characteristics in this graphics context void drawstring string str float x float y draws a string in the current font and color javax swing jcomponent fontmetrics getfontmetrics font f gets the font metrics for the given font the fontmetrics class is a precursor to the linemetrics class java awt fontmetrics fontrendercontext getfontrendercontext gets a font render context for the font displaying images you have already seen how to build up simple drawings by painting lines and shapes complex images such as photographs are usually generated externally for example with a scanner or special image manipulation software as you will see in volume ii it is also possible to produce an image pixel by pixel once images are stored in local files or someplace on the internet you can read them into a java application and display them on graphics objects there are many ways of reading images here we use the imageicon class that you already saw image image new imageicon filename getimage now the variable image contains a reference to an object that encapsulates the image data you can display the image with the drawimage method of the graphics class public void paintcomponent graphics g g drawimage image x y null listing takes this a little bit further and tiles the window with the graphics image the result looks like the screen shown in figure we do the tiling in the paintcomponent method we first draw one copy of the image in the top left corner and then use the copyarea call to copy it into the entire window click here to view code image for int i 0 i imagewidth getwidth i for int j 0 j imageheight getheight j if i j 0 g copyarea 0 0 imagewidth imageheight i imagewidth j imageheight figure 15 window with tiled graphics image listing shows the full source code of the image display program listing image imagetest java click here to view code image package image import java awt import javax swing version 33 04 14 author cay horstmann public class imagetest 12 public static void main string args 14 eventqueue invokelater new runnable 15 16 public void run 18 jframe frame new imageframe frame settitle imagetest 20 frame setdefaultcloseoperation jframe frame setvisible true 25 a frame with an image component class imageframe extends jframe 31 public imageframe 33 add new imagecomponent pack 40 a component that displays a tiled image class imagecomponent extends jcomponent private static final int 45 private static final int 200 private image image 48 public imagecomponent image new imageicon blue ball gif getimage public void paintcomponent graphics g 55 if image null return int imagewidth image getwidth this int imageheight image getheight this draw the image in the upper left corner g drawimage image 0 0 null tile the image across the component for int i 0 i imagewidth getwidth i for int j 0 j imageheight getheight j if i j 0 69 g copyarea 0 0 imagewidth imageheight i imagewidth j imageheight public dimension getpreferredsize return new dimension java awt graphics 0 boolean drawimage image img int x int y imageobserver observer draws an unscaled image note this call may return before the image is drawn boolean drawimage image img int x int y int width int height imageobserver observer draws a scaled image the system scales the image to fit into a region with the given width and height note this call may return before the image is drawn void copyarea int x int y int width int height int dx int dy copies an area of the screen this concludes our introduction to java graphics programming for more advanced techniques refer to the discussion of graphics and image manipulation in volume ii in the next chapter you will learn how your programs can react to user input chapter event handling in this chapter basics of event handling actions mouse events the awt event hierarchy event handling is of fundamental importance to programs with a graphical user interface to implement user interfaces you must master the way in which java handles events this chapter explains how the java awt event model works you will see how to capture events from user interface components and input devices we will also show you how to work with actions which represent a more structured approach for processing action events basics of event handling any operating environment that supports guis constantly monitors events such as keystrokes or mouse clicks the operating environment reports these events to the programs that are running each program then decides what if anything to do in response to these events in languages like visual basic the correspondence between events and code is obvious one writes code for each specific event of interest and places the code in what is usually called an event procedure for example a visual basic button named helpbutton would have a event procedure associated with it the code in this procedure executes whenever that button is clicked each visual basic gui component responds to a fixed set of events and it is impossible to change the events to which it responds on the other hand if you use a language like raw c to do event driven programming you need to write the code that constantly checks the event queue for what the operating environment is reporting this is usually done by encasing your code in a loop with a massive switch statement this technique is obviously rather ugly and in any case much more difficult to code its advantage is that the events you can respond to are not as limited as in the languages which like visual basic go to great lengths to hide the event queue from the programmer the java programming environment takes an approach somewhere in between the visual basic and the raw c in terms of power and the resulting complexity within the limits of the events that the awt knows about you completely control how events are transmitted from the event sources such as buttons or scrollbars to event listeners you can designate any object to be an event listener in practice you pick an object that can conveniently carry out the desired response to the event this event delegation model gives you much more flexibility than is possible with visual basic in which the listener is predetermined event sources have methods that allow you to register event listeners with them when an event happens to the source the source sends a notification of that event to all the listener objects that were registered for that event as one would expect in an object oriented language like java the information about the event is encapsulated in an event object in java all event objects ultimately derive from the class java util eventobject of course there are subclasses for each event type such as actionevent and windowevent different event sources can produce different kinds of events for example a button can send actionevent objects whereas a window can send windowevent objects to sum up here an overview of how event handling in the awt works a listener object is an instance of a class that implements a special interface called naturally enough a listener interface an event source is an object that can register listener objects and send them event objects the event source sends out event objects to all registered listeners when that event occurs the listener objects will then use the information in the event object to determine their reaction to the event figure shows the relationship between the event handling classes and interfaces figure relationship between event sources and listeners here is an example for specifying a listener actionlistener listener jbutton button new jbutton ok button addactionlistener listener now the listener object is notified whenever an action event occurs in the button for buttons as you might expect an action event is a button click to implement the actionlistener interface the listener class must have a method called actionperformed that receives an actionevent object as a parameter class mylistener implements actionlistener public void actionperformed actionevent event reaction to button click goes here whenever the user clicks the button the jbutton object creates an actionevent object and calls listener actionperformed event passing that event object an event source such as a button can have multiple listeners in that case the button calls the actionperformed methods of all listeners whenever the user clicks the button figure shows the interaction between the event source event listener and event object figure event notification example handling a button click as a way of getting comfortable with the event delegation model let work through all the details needed for the simple example of responding to a button click for this example we will show a panel populated with three buttons three listener objects are added as action listeners to the buttons with this scenario each time a user clicks on any of the buttons on the panel the associated listener object receives an actionevent that indicates a button click in our sample program the listener object will then change the background color of the panel before we can show you the program that listens to button clicks we first need to explain how to create buttons and how to add them to a panel for more on gui elements see chapter to create a button specify a label string an icon or both in the button constructor here are two examples click here to view code image jbutton yellowbutton new jbutton yellow jbutton bluebutton new jbutton new imageicon blue ball gif call the add method to add the buttons to a panel click here to view code image jbutton yellowbutton new jbutton yellow jbutton bluebutton new jbutton blue jbutton redbutton new jbutton red buttonpanel add yellowbutton buttonpanel add bluebutton buttonpanel add redbutton figure shows the result figure 3 a panel filled with buttons next we need to add code that listens to these buttons this requires classes that implement the actionlistener interface which as we just mentioned has one method actionperformed whose signature looks like this public void actionperformed actionevent event note the actionlistener interface we used in the button example is not restricted to button clicks it is used in many separate situations when an item is selected from a list box with a double click when a menu item is selected when the enter key is clicked in a text field when a certain amount of time has elapsed for a timer component you will see more details in this chapter and the next the way to use the actionlistener interface is the same in all situations the actionperformed method which is the only method in actionlistener takes an object of type actionevent as a parameter this event object gives you information about the event that happened when a button is clicked we want the background color of the panel to change to a particular color we store the desired color in our listener class click here to view code image class coloraction implements actionlistener private color backgroundcolor public coloraction color c backgroundcolor c public void actionperformed actionevent event set panel background color we then construct one object for each color and set the objects as the button listeners click here to view code image coloraction yellowaction new coloraction color yellow coloraction blueaction new coloraction color blue coloraction redaction new coloraction color red yellowbutton addactionlistener yellowaction bluebutton addactionlistener blueaction redbutton addactionlistener redaction for example if a user clicks on the button marked yellow the actionperformed method of the yellowaction object is called its backgroundcolor instance field is set to color yellow and it can now proceed to set the panel background color just one issue remains the coloraction object doesn t have access to the buttonpanel variable you can solve this problem in two ways you can store the panel in the coloraction object and set it in the coloraction constructor or more conveniently you can make coloraction into an inner class of the buttonframe class its methods can then access the outer panel automatically for more information on inner classes see chapter we follow the latter approach here is how you place the coloraction class inside the buttonframe class click here to view code image class buttonframe extends jframe private jpanel buttonpanel private class coloraction implements actionlistener private color backgroundcolor public void actionperformed actionevent event buttonpanel setbackground backgroundcolor look closely at the actionperformed method the coloraction class doesn t have a buttonpanel field but the outer buttonframe class does this situation is very common event listener objects usually need to carry out some action that affects other objects you can often strategically place the listener class inside the class whose state the listener should modify listing contains the complete frame class whenever you click one of the buttons the appropriate action listener changes the background color of the panel javax swing jbutton jbutton string label jbutton icon icon jbutton string label icon icon constructs a button the label string can be plain text or starting with java se 3 html for example html b ok b html java awt container 0 component add component c adds the component c to this container listing button buttonframe java click here to view code image package button 3 import java awt import java awt event import javax swing a frame with a button panel public class buttonframe extends jframe 12 private jpanel buttonpanel private static final int 14 private static final int 200 15 16 public buttonframe 18 setsize 20 create buttons jbutton yellowbutton new jbutton yellow jbutton bluebutton new jbutton blue 23 jbutton redbutton new jbutton red 25 buttonpanel new jpanel 27 add buttons to panel buttonpanel add yellowbutton buttonpanel add bluebutton buttonpanel add redbutton 31 add panel to frame 33 add buttonpanel create button actions coloraction yellowaction new coloraction color yellow coloraction blueaction new coloraction color blue coloraction redaction new coloraction color red 40 associate actions with buttons yellowbutton addactionlistener yellowaction 42 bluebutton addactionlistener blueaction redbutton addactionlistener redaction 45 an action listener that sets the panel background color 48 private class coloraction implements actionlistener 51 private color backgroundcolor public coloraction color c 55 backgroundcolor c public void actionperformed actionevent event buttonpanel setbackground backgroundcolor becoming comfortable with inner classes some people dislike inner classes because they feel that a proliferation of classes and objects makes their programs slower let have a look at that claim you don t need a new class for every user interface component in our example all three buttons share the same listener class of course each of them has a separate listener object but these objects aren t large they each contain a color value and a reference to the panel and the traditional solution with if else statements also references the same color objects that the action listeners store just as local variables not as instance fields here is a good example of how anonymous inner classes can actually simplify your code if you look at the code of listing you will note that each button requires the same treatment construct the button with a label string add the button to the panel 3 construct an action listener with the appropriate color add that action listener let implement a helper method to simplify these tasks click here to view code image public void makebutton string name color backgroundcolor jbutton button new jbutton name buttonpanel add button coloraction action new coloraction backgroundcolor button addactionlistener action then we simply call makebutton yellow color yellow makebutton blue color blue makebutton red color red now you can make a further simplification note that the coloraction class is only needed once in the makebutton method therefore you can make it into an anonymous class click here to view code image public void makebutton string name final color backgroundcolor jbutton button new jbutton name buttonpanel add button button addactionlistener new actionlistener public void actionperformed actionevent event buttonpanel setbackground backgroundcolor the action listener code has become quite a bit simpler the actionperformed method simply refers to the parameter variable backgroundcolor as with all local variables that are accessed in the inner class the parameter needs to be declared as final no explicit constructor is needed as you saw in chapter 6 the inner class mechanism automatically generates a constructor that stores all local final variables that are used in one of the methods of the inner class tip anonymous inner classes can look confusing but you can get used to deciphering them if you train your eyes to glaze over the routine code like this click here to view code image button addactionlistener new actionlistener public void actionperformed actionevent event buttonpanel setbackground backgroundcolor here the button action sets the background color as long as the event handler consists of just a few statements we think this can be quite readable particularly if you don t worry about the inner class mechanics note you are completely free to designate any object of a class that implements the actionlistener interface as a button listener we prefer to use objects of a new class expressly created for carrying out the desired button actions however some programmers are not comfortable with inner classes and choose a different strategy they make the container of the event sources implement the actionlistener interface then the container sets itself as the listener like this yellowbutton addactionlistener this bluebutton addactionlistener this redbutton addactionlistener this now the three buttons no longer have individual listeners they share a single listener object namely the button frame therefore the actionperformed method must figure out which button was clicked click here to view code image class buttonframe extends jframe implements actionlistener public void actionperformed actionevent event object source event getsource if source yellowbutton else if source bluebutton else if source redbutton else as you can see this gets quite messy and we do not recommend it java util eventobject object getsource returns a reference to the object where the event occurred java awt event actionevent string getactioncommand returns the command string associated with this action event if the action event originated from a button the command string equals the button label unless it has been changed with the setactioncommand method 3 creating listeners containing a single method call you can specify simple event listeners without programming inner classes for example suppose you have a button labeled load whose event handler contains a single method call frame loaddata of course you can use an anonymous inner class click here to view code image loadbutton addactionlistener new actionlistener public void actionperformed actionevent event frame loaddata but the eventhandler class can create such a listener automatically with the call eventhandler create actionlistener class frame loaddata of course you still need to install the handler click here to view code image loadbutton addactionlistener eventhandler create actionlistener class frame loaddata if the listener calls a method with a single parameter that can be obtained from the event parameter you can use another form of the create method for example the call eventhandler create actionlistener class frame loaddata source text is equivalent to click here to view code image new actionlistener public void actionperformed actionevent event frame loaddata jtextfield event getsource gettext the property names source and text turn into method calls getsource and gettext caution the second argument in the call to eventhandler create must belong to a public class otherwise the reflection mechanism will not be able to locate and invoke the target method java beans eventhandler static object create class listenerinterface object target string action static object create class listenerinterface object target string action string eventproperty static object create class listenerinterface object target string action string eventproperty string listenermethod constructs an object of a proxy class that implements the given interface either the named method or all methods of the interface carry out the given action on the target object the action can be a method name or a property of the target if it is a property its setter method is executed for example an action text is turned into a call of the settext method the event property consists of one or more dot separated property names the first property is read from the parameter of the listener method the second property is read from the resulting object and so on the final result becomes the parameter of the action for example the property source text is turned into calls to the getsource and gettext methods example changing the look and feel by default swing programs use the metal look and feel there are two ways to change to a different look and feel the first is to supply a file swing properties in the jre lib subdirectory of your java installation in that file set the property swing defaultlaf to the class name of the look and feel that you want for example swing defaultlaf com sun java swing plaf motif motiflookandfeel note that the metal look and feel is located in the javax swing package the other look and feel packages are located in the com sun java package and need not be present in every java implementation currently for copyright reasons the windows and macintosh look and feel packages are only shipped with the windows and macintosh versions of the java runtime environment tip lines starting with a character are ignored in property files so you can supply several look and feel selections in the swing properties file and move around the to select one of them click here to view code image swing defaultlaf javax swing plaf metal metallookandfeel swing defaultlaf com sun java swing plaf motif motiflookandfeel swing defaultlaf com sun java swing plaf windows windowslookandfeel you must restart your program to switch the look and feel in this way a swing program reads the swing properties file only once at startup the second way is to change the look and feel dynamically call the static uimanager setlookandfeel method and give it the name of the look and feel class that you want then call the static method swingutilities updatecomponenttreeui to refresh the entire set of components you need to supply one component to that method it will find all others the uimanager setlookandfeel method may throw a number of exceptions when it can t find the lookand feel that you request or when there is an error loading it as always we ask you to gloss over the exception handling code and wait until chapter for a full explanation here is an example showing how you can switch to the motif look and feel in your program click here to view code image string plaf com sun java swing plaf motif motiflookandfeel try uimanager setlookandfeel plaf swingutilities updatecomponenttreeui panel c atch exception e e printstacktrace to enumerate all installed look and feel implementations call uimanager lookandfeelinfo infos uimanager getinstalledlookandfeels then you can get the name and class name for each look and feel as string name infos i getname string classname infos i getclassname listing is a complete program that demonstrates how to switch the look and feel see figure the program is similar to listing following the advice of the preceding section we use a helper method makebutton and an anonymous inner class to specify the button action namely to switch the look and feel figure switching the look and feel listing plaf plafframe java click here to view code image package plaf 3 import java awt event import javax swing 6 a frame with a button panel for changing look and feel public class plafframe extends jframe 10 private jpanel buttonpanel 12 public plafframe 14 buttonpanel new jpanel 15 16 uimanager lookandfeelinfo infos uimanager getinstalledlookandfeels for uimanager lookandfeelinfo info infos 18 makebutton info getname info getclassname 19 20 add buttonpanel pack 23 25 makes a button to change the pluggable look and feel param name the button name 27 param plafname the name of the look and feel class 29 void makebutton string name final string plafname 31 add button to panel 33 jbutton button new jbutton name buttonpanel add button 36 set button action button addactionlistener new actionlistener 40 public void actionperformed actionevent event 42 button action switch to the new look and feel try 45 uimanager setlookandfeel plafname swingutilities updatecomponenttreeui plafframe this pack 48 49 catch exception e 51 e printstacktrace 53 55 there is one fine point to this program the actionperformed method of the inner action listener class needs to pass the this reference of the outer plafframe class to the updatecomponenttreeui method recall from chapter 6 that the outer object this pointer must be prefixed by the outer class name swingutilities updatecomponenttreeui plafpanel this javax swing uimanager static uimanager lookandfeelinfo getinstalledlookandfeels gets an array of objects that describe the installed look and feel implementations static setlookandfeel string classname sets the current look and feel using the given class name such as javax swing plaf metal metallookandfeel javax swing uimanager lookandfeelinfo string getname returns the display name for the look and feel string getclassname returns the name of the implementation class for the look and feel adapter classes not all events are as simple to handle as button clicks in a non toy program you will want to monitor when the user tries to close the main frame because you don t want your users to lose unsaved work when the user closes the frame you want to put up a dialog and exit the program only when the user agrees when the user tries to close a window the jframe object is the source of a windowevent if you want to catch that event you must have an appropriate listener object and add it to the frame list of window listeners windowlistener listener frame addwindowlistener listener the window listener must be an object of a class that implements the windowlistener interface there are actually seven methods in the windowlistener interface the frame calls them as the responses to seven distinct events that could happen to a window the names are self explanatory except that iconified is usually called minimized under windows here is the complete windowlistener interface public interface windowlistener void windowopened windowevent e void windowclosing windowevent e void windowclosed windowevent e void windowiconified windowevent e void windowdeiconified windowevent e void windowactivated windowevent e void windowdeactivated windowevent e note to find out whether a window has been maximized install a windowstatelistener see the api notes on page for details as is always the case in java any class that implements an interface must implement all its methods in this case that means implementing seven methods recall that we are only interested in one of these seven methods namely the windowclosing method of course we can define a class that implements the interface add a call to system exit 0 in the windowclosing method and write do nothing functions for the other six methods click here to view code image class terminator implements windowlistener public void windowclosing windowevent e if user agrees system exit 0 public void windowopened windowevent e public void windowclosed windowevent e public void windowiconified windowevent e public void windowdeiconified windowevent e public void windowactivated windowevent e public void windowdeactivated windowevent e typing code for six methods that don t do anything is the kind of tedious busy work that nobody likes to simplify this task each of the awt listener interfaces that have more than one method comes with a companion adapter class that implements all the methods in the interface but does nothing with them for example the windowadapter class has seven do nothing methods this means the adapter class automatically satisfies the technical requirements that java imposes for implementing the associated listener interface you can extend the adapter class to specify the desired reactions to some but not all of the event types in the interface an interface such as actionlistener that has only a single method does not need an adapter class let us make use of the window adapter we can extend the windowadapter class inherit six of the do nothing methods and override the windowclosing method class terminator extends windowadapter public void windowclosing windowevent e if user agrees system exit 0 now you can register an object of type terminator as the event listener windowlistener listener new terminator frame addwindowlistener listener whenever the frame generates a window event it passes it to the listener object by calling one of its seven methods see figure 5 six of those methods do nothing the windowclosing method calls system exit 0 terminating the application figure 5 a window listener caution if you misspell the name of a method when extending an adapter class the compiler won t catch your error for example if you define a method windowisclosing in a windowadapter class you will get a class with eight methods and the windowclosing method will do nothing creating a listener class that extends the windowadapter is an improvement but we can go even further there is no need to give a name to the listener object simply write frame addwindowlistener new terminator but why stop there we can make the listener class into an anonymous inner class of the frame frame addwindowlistener new windowadapter public void windowclosing windowevent e if user agrees system exit 0 this code does the following defines a class without a name that extends the windowadapter class adds a windowclosing method to that anonymous class as before this method exits the program inherits the remaining six do nothing methods from windowadapter creates an object of this class that object does not have a name either passes that object to the addwindowlistener method again the syntax for using anonymous inner classes takes some getting used to the payoff is that the resulting code is as short as possible java awt event windowlistener void windowopened windowevent e is called after the window has been opened void windowclosing windowevent e is called when the user has issued a window manager command to close the window note that the window will close only if its hide or dispose method is called void windowclosed windowevent e is called after the window has closed void windowiconified windowevent e is called after the window has been iconified void windowdeiconified windowevent e is called after the window has been deiconified void windowactivated windowevent e is called after the window has become active only a frame or dialog can be active typically the window manager decorates the active window for example by highlighting the title bar void windowdeactivated windowevent e is called after the window has become deactivated java awt event windowstatelistener void windowstatechanged windowevent event is called after the window has been maximized iconified or restored to normal size java awt event windowevent int getnewstate int getoldstate 4 return the new and old state of a window in a window state change event the returned integer is one of the following values frame normal frame iconified frame frame frame actions it is common to have multiple ways to activate the same command the user can choose a certain function through a menu a keystroke or a button on a toolbar this is easy to achieve in the awt event model link all events to the same listener for example suppose blueaction is an action listener whose actionperformed method changes the background color to blue you can attach the same object as a listener to several event sources a toolbar button labeled blue a menu item labeled blue a keystroke ctrl b the color change command will now be handled in a uniform way no matter whether it was caused by a button click a menu selection or a key press the swing package provides a very useful mechanism to encapsulate commands and to attach them to multiple event sources the action interface an action is an object that encapsulates a description of the command as a text string and an optional icon and parameters that are necessary to carry out the command such as the requested color in our example the action interface has the following methods click here to view code image void actionperformed actionevent event void setenabled boolean b boolean isenabled void putvalue string key object value object getvalue string key void addpropertychangelistener propertychangelistener listener void removepropertychangelistener propertychangelistener listener the first method is the familiar method in the actionlistener interface in fact the action interface extends the actionlistener interface therefore you can use an action object whenever an actionlistener object is expected the next two methods let you enable or disable the action and check whether the action is currently enabled when an action is attached to a menu or toolbar and the action is disabled the option is grayed out the putvalue and getvalue methods let you store and retrieve arbitrary name value pairs in the action object a couple of important predefined strings namely action name and action store action names and icons into an action object click here to view code image action putvalue action name blue action putvalue action new imageicon blue ball gif table shows all predefined action table names table predefined action table names if the action object is added to a menu or toolbar the name and icon are automatically retrieved and displayed in the menu item or toolbar button the value turns into a tooltip the final two methods of the action interface allow other objects in particular menus or toolbars that trigger the action to be notified when the properties of the action object change for example if a menu is added as a property change listener of an action object and the action object is subsequently disabled the menu is called and can gray out the action name property change listeners are a general construct that is a part of the javabeans component model you can find out more about beans and their properties in volume ii note that action is an interface not a class any class implementing this interface must implement the seven methods we just discussed fortunately a friendly soul has provided a class abstractaction that implements all methods except for actionperformed that class takes care of storing all name value pairs and managing the property change listeners you simply extend abstractaction and supply an actionperformed method let build an action object that can execute color change commands we store the name of the command an icon and the desired color we store the color in the table of name value pairs that the abstractaction class provides here is the code for the coloraction class the constructor sets the name value pairs and the actionperformed method carries out the color change action click here to view code image public class coloraction extends abstractaction public coloraction string name icon icon color c putvalue action name name putvalue action icon putvalue color c putvalue action set panel color to name tolowercase public void actionperformed actionevent event color c color getvalue color buttonpanel setbackground c our test program creates three objects of this class such as action blueaction new coloraction blue new imageicon blue ball gif color blue next let associate this action with a button that is easy because we can use a jbutton constructor that takes an action object jbutton bluebutton new jbutton blueaction that constructor reads the name and icon from the action sets the short description as the tooltip and sets the action as the listener you can see the icons and a tooltip in figure 6 figure 6 buttons display the icons from the action objects as we demonstrate in the next chapter it is just as easy to add the same action to a menu finally we want to add the action objects to keystrokes so that an action is carried out when the user types a keyboard command to associate actions with keystrokes you first need to generate objects of the keystroke class this is a convenience class that encapsulates the description of a key to generate a keystroke object you don t call a constructor but instead use the static getkeystroke method of the keystroke class keystroke ctrlbkey keystroke getkeystroke ctrl b to understand the next step you need to understand the concept of keyboard focus a user interface can have many buttons menus scrollbars and other components when you hit a key it is sent to the component that has focus that component is usually but not always visually distinguished for example in the java look and feel a button with focus has a thin rectangular border around the button text you can use the tab key to move the focus between components when you press the space bar the button with focus is clicked other keys carry out different actions for example the arrow keys can move a scrollbar however in our case we do not want to send the keystroke to the component that has focus otherwise each of the buttons would need to know how to handle the ctrl y ctrl b and ctrl r keys this is a common problem and the swing designers came up with a convenient solution every jcomponent has three input maps each mapping keystroke objects to associated actions the three input maps correspond to three different conditions see table table input map conditions keystroke processing checks these maps in the following order check the map of the component with input focus if the keystroke exists execute the corresponding action if the action is enabled stop processing starting from the component with input focus check the maps of its parent components as soon as a map with the keystroke is found execute the corresponding action if the action is enabled stop processing 3 look at all visible and enabled components in the window with input focus that have this keystroke registered in a map give these components in the order of their keystroke registration a chance to execute the corresponding action as soon as the first enabled action is executed stop processing this part of the process is somewhat fragile if a keystroke appears in more than one map to obtain an input map from the component use the getinputmap method here is an example inputmap imap panel getinputmap jcomponent the condition means that this map is consulted when the current component has the keyboard focus in our situation that isn t the map we want one of the buttons not the panel has the input focus either of the other two map choices works fine for inserting the color change keystrokes we use in our example program the inputmap doesn t directly map keystroke objects to action objects instead it maps to arbitrary objects and a second map implemented by the actionmap class maps objects to actions that makes it easier to share the same actions among keystrokes that come from different input maps thus each component has three input maps and one action map to tie them together you need to come up with names for the actions here is how you can tie a key to an action click here to view code image imap put keystroke getkeystroke ctrl y panel yellow actionmap amap panel getactionmap amap put panel yellow yellowaction it is customary to use the string none for a do nothing action that makes it easy to deactivate a key imap put keystroke getkeystroke ctrl c none caution the jdk documentation suggests using the action name as the action key we don t think that is a good idea the action name is displayed on buttons and menu items thus it can change at the whim of the ui designer and may be translated into multiple languages such unstable strings are poor choices for lookup keys so we recommend that you come up with action names that are independent of the displayed names to summarize here is what you do to carry out the same action in response to a button a menu item or a keystroke implement a class that extends the abstractaction class you may be able to use the same class for multiple related actions construct an object of the action class 3 construct a button or menu item from the action object the constructor will read the label text and icon from the action object 4 for actions that can be triggered by keystrokes you have to carry out additional steps first locate the top level component of the window such as a panel that contains all other components 5 then get the input map of the top level component make a keystroke object for the desired keystroke make an action key object such as a string that describes your action add the pair keystroke action key into the input map 6 finally get the action map of the top level component add the pair action key action object into the map listing 3 shows the complete code of the program that maps both buttons and keystrokes to action objects try it out both clicking the buttons and pressing ctrl y ctrl b or ctrl r changes the panel color listing 3 action actionframe java click here to view code image package action 3 import java awt 4 import java awt event 5 import javax swing 6 a frame with a panel that demonstrates color change actions 10 public class actionframe extends jframe 12 private jpanel buttonpanel 13 private static final int 14 private static final int 200 15 16 public actionframe 18 setsize 19 20 buttonpanel new jpanel define actions 23 action yellowaction new coloraction yellow new imageicon yellow ball gif color yellow 25 action blueaction new coloraction blue new imageicon blueball gif color blue 26 action redaction new coloraction red new imageicon redball gif color red 27 28 add buttons for these actions 29 buttonpanel add new jbutton yellowaction buttonpanel add new jbutton blueaction 31 buttonpanel add new jbutton redaction 32 33 add panel to frame add buttonpanel 35 36 associate the y b and r keys with names inputmap imap buttonpanel getinputmap jcomponent 38 imap put keystroke getkeystroke ctrl y panel yellow imap put keystroke getkeystroke ctrl b panel blue 40 imap put keystroke getkeystroke ctrl r panel red 42 associate the names with actions actionmap amap buttonpanel getactionmap 44 amap put panel yellow yellowaction 45 amap put panel blue blueaction 46 amap put panel red redaction 47 48 49 public class coloraction extends abstractaction 50 51 constructs a color action 53 param name the name to show on the button param icon the icon to display on the button 55 param c the background color 57 public coloraction string name icon icon color c 58 putvalue action name name putvalue action icon putvalue action set panel color to name tolowercase putvalue color c 64 public void actionperformed actionevent event color c color getvalue color buttonpanel setbackground c 69 javax swing action boolean isenabled void setenabled boolean b gets or sets the enabled property of this action void putvalue string key object value places a name value pair inside the action object object getvalue string key returns the value of a stored name value pair javax swing keystroke static keystroke getkeystroke string description constructs a keystroke from a human readable description a sequence of whitespacedelimited strings the description starts with zero or more modifiers shift control ctrl meta alt altgraph and ends with either the string typed followed by a onecharacter string for example typed a or an optional event specifier pressed or released with pressed being the default followed by a key code the key code when prefixed with should correspond to a keyevent constant for example insert corresponds to keyevent javax swing jcomponent 2 actionmap getactionmap 3 returns the map that associates action map keys which can be arbitrary objects with action objects inputmap getinputmap int flag 3 gets the input map that maps key strokes to action map keys 3 mouse events you do not need to handle mouse events explicitly if you just want the user to be able to click on a button or menu these mouse operations are handled internally by the various components in the user interface however if you want to enable the user to draw with the mouse you will need to trap mouse move click and drag events in this section we will show you a simple graphics editor application that allows the user to place move and erase squares on a canvas see figure 7 figure 7 a mouse test program when the user clicks a mouse button three listener methods are called mousepressed when the mouse is first pressed mousereleased when the mouse is released and finally mouseclicked if you are only interested in complete clicks you can ignore the first two methods by using the getx and gety methods on the mouseevent argument you can obtain the x and y coordinates of the mouse pointer when the mouse was clicked to distinguish between single double and triple clicks use the getclickcount method some user interface designers inflict mouse click and keyboard modifier combinations such as ctrl shift click on their users we find this practice reprehensible but if you disagree you will find that checking for mouse buttons and keyboard modifiers is a mess use bit masks to test which modifiers have been set in the original api two of the button masks equal two keyboard modifier masks namely meta_mask this was done so that users with a one button mouse could simulate the other mouse buttons by holding down modifier keys instead however as of java se 4 a different approach is recommended there are now masks shift_down_mask meta_down_mask the getmodifiersex method accurately reports the mouse buttons and keyboard modifiers of a mouse event note that tests for the right nonprimary mouse button under windows for example you can use code like this to detect whether the right mouse button is down click here to view code image if event getmodifiersex inputevent 0 code for right click in our sample program we supply both a mousepressed and a mouseclicked methods when you click onto a pixel that is not inside any of the squares that have been drawn a new square is added we implemented this in the mousepressed method so that the user receives immediate feedback and does not have to wait until the mouse button is released when a user double clicks inside an existing square it is erased we implemented this in the mouseclicked method because we need the click count click here to view code image public void mousepressed mouseevent event current find event getpoint if current null not inside a square add event getpoint public void mouseclicked mouseevent event current find event getpoint if current null event getclickcount 2 remove current as the mouse moves over a window the window receives a steady stream of mouse movement events note that there are separate mouselistener and mousemotionlistener interfaces this is done for efficiency there are a lot of mouse events as the user moves the mouse around and a listener that just cares about mouse clicks will not be bothered with unwanted mouse moves our test application traps mouse motion events to change the cursor to a different shape a cross hair when it is over a square this is done with the getpredefinedcursor method of the cursor class table 3 lists the constants to use with this method along with what the cursors look like under windows table 3 sample cursor shapes here is the mousemoved method of the mousemotionlistener in our example program click here to view code image public void mousemoved mouseevent event if find event getpoint null setcursor cursor getdefaultcursor else setcursor cursor getpredefinedcursor cursor note you can also define your own cursor types through the use of the createcustomcursor method in the toolkit class click here to view code image toolkit tk toolkit getdefaulttoolkit image img tk getimage dynamite gif cursor dynamitecursor tk createcustomcursor img new point 10 10 dynamite stick the first parameter of the createcustomcursor points to the cursor image the second parameter gives the offset of the hot spot of the cursor the third parameter is a string that describes the cursor this string can be used for accessibility support for example a screen reader program can read the cursor shape description to a user who is visually impaired or who simply is not facing the screen if the user presses a mouse button while the mouse is in motion mousedragged calls are generated instead of mousemoved calls our test application lets a user drag the square under the cursor we simply update the currently dragged rectangle to be centered under the mouse position then we repaint the canvas to show the new mouse position click here to view code image public void mousedragged mouseevent event if current null int x event getx int y event gety current setframe x sidelength 2 y sidelength 2 sidelength sidelength repaint note the mousemoved method is only called as long as the mouse stays inside the component however the mousedragged method keeps getting called even when the mouse is being dragged outside the component there are two other mouse event methods mouseentered and mouseexited these methods are called when the mouse enters or exits a component finally we explain how to listen to mouse events mouse clicks are reported through the mouseclicked procedure which is part of the mouselistener interface many applications are only interested in mouse clicks and not in mouse moves with the mouse move events occuring so frequently the mouse move and drag events are defined in a separate interface called mousemotionlistener in our program we are interested in both types of mouse events we define two inner classes mousehandler and mousemotionhandler the mousehandler class extends the mouseadapter class because it defines only two of the five mouselistener methods the mousemotionhandler implements the mousemotionlistener and defines both methods of that interface listing 4 is the program listing listing 4 mouse mouseframe java click here to view code image package mouse 2 3 import javax swing 4 5 6 a frame containing a panel for testing mouse operations 7 public class mouseframe extends jframe 10 public mouseframe 11 12 add new mousecomponent 13 pack 14 15 listing 5 mouse mousecomponent java click here to view code image package mouse 2 3 import java awt 4 import java awt event 5 import java awt geom 6 import java util 7 import javax swing 10 a component with mouse operations for adding and removing squares 11 12 public class mousecomponent extends jcomponent 13 14 private static final int sidelength 10 15 private arraylist squares 16 private current 17 18 public mousecomponent 19 20 squares new arraylist current null 22 23 addmouselistener new mousehandler 24 addmousemotionlistener new mousemotionhandler 25 26 27 public void paintcomponent graphics g 28 29 g2 g 30 31 draw all squares 32 for r squares 33 g2 draw r 34 35 36 finds the first square containing a point param p a point 38 return the first square that contains p 39 40 public find p 41 42 for r squares 43 44 if r contains p return r 45 46 return null 47 48 49 50 adds a square to the collection 51 param p the center of the square 53 public void add p 54 55 double x p getx 56 double y p gety 57 58 current new double x sidelength 2 y sidelength 2 sidelength 59 sidelength 60 squares add current repaint 62 63 64 65 removes a square from the collection 66 param the square to remove 67 68 public void remove 69 70 if null return 71 if current current null 72 squares remove 73 repaint 74 75 the square containing the mouse cursor private class mousehandler extends mouseadapter 78 public void mousepressed mouseevent event add a new square if the cursor isn t inside a square current find event getpoint if current null add event getpoint public void mouseclicked mouseevent event remove the current square if double clicked current find event getpoint if current null event getclickcount 2 remove current 92 private class mousemotionhandler implements mousemotionlistener public void mousemoved mouseevent event 96 set the mouse cursor to cross hairs if it is inside a rectangle if find event getpoint null setcursor cursor getdefaultcursor else setcursor cursor getpredefinedcursor cursor 100 102 public void mousedragged mouseevent event 104 if current null 106 int x event getx int y event gety drag the current rectangle to center it at x y current setframe x sidelength 2 y sidelength 2 sidelength sidelength repaint 115 java awt event mouseevent int getx int gety point getpoint returns the x horizontal and y vertical coordinate of the point where the event happened measured from the top left corner of the component that is the event source int getclickcount returns the number of consecutive mouse clicks associated with this event the time interval for what constitutes consecutive is system dependent java awt event inputevent 1 int getmodifiersex 1 4 returns the extended or down modifiers for this event use the following mask values to test the returned value shift_down_mask meta_down_mask static string getmodifiersextext int modifiers 1 4 returns a string such as shift describing the extended or down modifiers in the given flag set java awt toolkit 1 0 public cursor createcustomcursor image image point hotspot string name 1 2 creates a new custom cursor object java awt component 1 0 public void setcursor cursor cursor 1 1 sets the cursor image to the specified cursor 4 the awt event hierarchy having given you a taste of how event handling works we finish this chapter with an overview of the awt event handling architecture as we briefly mentioned earlier event handling in java is object oriented with all events descending from the eventobject class in the java util package the common superclass is not called event because that is the name of the event class in the old event model although the old model is now deprecated its classes are still a part of the java library the eventobject class has a subclass awtevent which is the parent of all awt event classes figure shows the inheritance diagram of the awt events figure inheritance diagram of awt event classes some of the swing components generate event objects of yet more event types these directly extend eventobject not awtevent the event objects encapsulate information about the event that the event source communicates to its listeners when necessary you can then analyze the event objects that were passed to the listener object as we did in the button example with the getsource and getactioncommand methods some of the awt event classes are of no practical use for the java programmer for example the awt inserts paintevent objects into the event queue but these objects are not delivered to listeners java programmers don t listen to paint events instead they override the paintcomponent method to control repainting the awt also generates a number of events that are needed only by systems programmers to provide input systems for ideographic languages automated testing robots and so on we do not discuss these specialized event types 8 4 1 semantic and low level events the awt makes a useful distinction between low level and semantic events a semantic event is one that expresses what the user is doing such as clicking that button an actionevent is a semantic event low level events are those events that make this possible in the case of a button click this is a mouse down a series of mouse moves and a mouse up but only if the mouse up is inside the button area or it might be a keystroke which happens if the user selects the button with the tab key and then activates it with the space bar similarly adjusting a scrollbar is a semantic event but dragging the mouse is a low level event here are the most commonly used semantic event classes in the java awt event package actionevent for a button click a menu selection selecting a list item or enter typed in a text field adjustmentevent the user adjusted a scrollbar itemevent the user made a selection from a set of checkbox or list items five low level event classes are commonly used keyevent a key was pressed or released mouseevent the mouse button was pressed released moved or dragged mousewheelevent the mouse wheel was rotated focusevent a component got focus or lost focus windowevent the window state changed the following interfaces listen to these events click here to view code image actionlistener mousemotionlistener adjustmentlistener mousewheellistener focuslistener windowlistener itemlistener windowfocuslistener keylistener windowstatelistener mouselistener several of the awt listener interfaces namely those that have more than one method come with a companion adapter class that implements all the methods in the interface to do nothing the other interfaces have only a single method each so there is no benefit in having adapter classes for these interfaces here are the commonly used adapter classes click here to view code image focusadapter mousemotionadapter keyadapter windowadapter mouseadapter table 8 4 shows the most important awt listener interfaces events and event sources table 8 4 event handling summary t h e javax swing event package contains additional events that are specific to swing components we cover some of them in the next chapter this concludes our discussion of awt event handling the next chapter shows you how to put together the most common swing components along with a detailed coverage of the events they generate chapter user interface components with swing in this chapter swing and the model view controller design pattern introduction to layout management text input choice components menus sophisticated layout management dialog boxes the last chapter was written primarily to show you how to use the event model in java in the process you took the first steps toward learning how to build a graphical user interface this chapter shows you the most important tools you ll need to build more full featured guis we start out with a tour of the architectural underpinnings of swing knowing what goes on under the hood is important in understanding how to use some of the more advanced components effectively we then show you how to use the most common user interface components in swing such as text fields radio buttons and menus next you will learn how to use the nifty layout manager features of java to arrange these components in a window regardless of the look and feel of a particular user interface finally you ll see how to implement dialog boxes in swing this chapter covers the basic swing components such as text components buttons and sliders these are the essential user interface components that you will need most frequently we will cover advanced swing components in volume ii 1 swing and the model view controller design pattern as promised we start this chapter with a description of the architecture of swing components we first discuss the concept of design patterns and then look at the model view controller pattern that has greatly influenced the design of the swing framework 9 1 1 design patterns when solving a problem you don t usually figure out a solution from first principles instead you are likely to be guided by your past experience or you may ask other experts for advice on what has worked for them design patterns are a method for presenting this expertise in a structured way in recent years software engineers have begun to assemble catalogs of such patterns the pioneers in this area were inspired by the architectural design patterns of the architect christopher alexander in his book the timeless way of building oxford university press alexander gives a catalog of patterns for designing public and private living spaces here is a typical example window place everybody loves window seats bay windows and big windows with low sills and comfortable chairs drawn up to them a room which does not have a place like this seldom allows you to feel comfortable or perfectly at ease if the room contains no window which is a place a person in the room will be torn between two forces 1 he wants to sit down and be comfortable and 2 he is drawn toward the light obviously if the comfortable places those places in the room where you most want to sit are away from the windows there is no way of overcoming this conflict therefore in every room where you spend any length of time during the day make at least one window into a window place figure 9 1 figure 9 1 a window place each pattern in alexander s catalog as well as those in the catalogs of software patterns follows a particular format the pattern first describes a context a situation that gives rise to a design problem then the problem is explained usually as a set of conflicting forces finally the solution shows a configuration that balances these forces in the window place pattern the context is a room in which you spend any length of time during the day the conflicting forces are that you want to sit down and be comfortable and that you are drawn to the light the solution is to make a window place in the model view controller pattern which we will describe in the next section the context is a user interface system that presents information and receives user input there are several forces there may be multiple visual representations of the same data that need to be updated together the visual representations may change for example to accommodate various look and feel standards the interaction mechanisms may change for example to support voice commands the solution is to distribute responsibilities into three separate interacting components the model the view and the controller the model view controller pattern is not the only pattern used in the design of awt and swing here are several additonal examples containers and components are examples of the composite pattern the scroll pane is a decorator layout managers follow the strategy pattern one important aspect of design patterns is that they become part of the culture programmers all over the world know what you mean when you talk about the model view controller pattern or the decorator pattern thus patterns become an efficient way of talking about design problems you will find a formal description of numerous useful software patterns in the seminal book of the pattern movement design patterns elements of reusable object oriented software by erich gamma et al addison wesley we also highly recommend the excellent book a system of patterns by frank buschmann et al john wiley sons which we find less seminal and more approachable everything you wanted to know about css but were afraid to ask kind of like the first two versions but way way better box model your layouts will work a lot better when you know what this is and what it does calculating height and width height formula top margin top border top padding height bottom padding bottom border bottom margin width formula left margin left border left padding width right padding right border right margin shorthand trbl top right bottom left use whenever you can properties for setting heights and widths width height min width max width min height max height set width of a content area width absolute width width relative width width auto width based on its containing block set height of a content area height absolute height height relative height width auto height based on its containing block set min width min height of a content area min width no less that wide max width no more than wide min height no less than wide max height no more than wide margin margin top margin bottom margin left margin right set margin on a single side of an element margin left margin right margin top margin bottom set margin on multiple sides margin top bottom left right margin top right bottom and left margin top right left bottom padding padding top padding bottom padding left padding right set padding on a single side of an element padding left padding right padding top padding bottom set padding on multiple sides padding top bottom left right padding top right bottom and left padding top right left bottom border border side border width border style border color border side width style color border border solid fff border left width border top style solid border bottom color red border width border width all sides border width top bottom left right border width top right left bottom border width top right bottom right border color border color dedede border color black gray black top bottom gray l r border style border style dashed border style solid none top and bottom solid l r none border width style and color border bottom width border right style dashed border left color gray border radius border radius applies radius to all corners border radius topleft topright lowerright lowerleft border left bottom radius border right top radius box shadow box shadow horizoffset vertoffset blurradius spread color box shadow 4px red background background color background image background repeat background attachment background position background shorthand background color image repeat attachment position background transparent background red url path to img no repeat top center background blue url path to img repeat x separating props background color yellow background repeat repeat y background repeat background repeat repeat repeats both directions background repeat repeat x horizontally y is vert background repeat no repeat doesn t repeat background attachment background attachment scroll img moves as you scroll background attachment fixed img doesn t scroll css layouts you will need to get good with this float float left float right clear clear left clear right clear both float float left float right float none float initial clear clear left clear right clear both float left float right clear both column properties column count column gap column rule column span in use article moz column count webkit column count column count moz column gap webkit column gap column gap moz column rule solid black webkit column rule solid black column rule solid black position properties position top bottom left right z index position values relative flex static initial fixed absolute flex if regular layout is based on both block and inline flow directions the flex layout is based on flex flow directions parent display flex or inline flex flex direction flex wrap flex flow justify content align items align content child order flex flow flex grow flex shrink flex basis flex align self the fivequel html structure let just hop right in and take it apart i tend to do that a lot let break it down awesome image formatting aside html is fairly easy to pick up the head element the head element cont d needs to have a title titles are very important for seo as short and descriptive as possible links to external style sheets meta tags html elements html consists of tags that contain content block level elements will take all of the horizontal space it can some examples heading tags paragraph tags p divs div section section just to name a few block level elements cont d block level elements text headings should show the structure and importance of a page content always use the tag for the most important text other block level text elements pre used for wrapping text typically for logging blockquote defaults to indent maybe use italics address formats text similar to mailing address format will only take up as much space as the content of the tag inline text elements i b sub sup br inline content elements abbr cite code span em probably the most important thing to remember when writing html use the right tag for the right job header content in a header tag footer content in a footer tag nav elements in a nav tag etc used to indicate the structure of a page header footer main section article nav aside main tag is the anchor tag a href attribute points to an asset link to a file in the same folder href products html link to file in a subfolder or parent folder href company services html link to a file based on the root folder href orders cart html link to another site href http www google com target unordered list ul ordered list ol list item li components of an img tag src points to an asset alt text description of image height width jpeg joint photographic experts group bit color gif graphic interchange format bit color allows for animations png portable network graphics can preserve transparency best for the internet css cascading style sheets introduction a little bit of history versions and basics spec created in created in almost completely implemented css provides the means to change the presentation of html documents not technically html but can be embedded in html versions and basics style sheets are semantic mechanisms for specifying style information style sheets allow you to impose a standard style on a whole doc or a collection of docs style is specified for a tag by the values of its properties levels of css css would be a lame video game with only levels but those levels are very important levels of styles inline defines a specific occurrence of a tag and apply only to that tag document level apply to the whole document in which they appear external can be applied to any number of documents levels of styles when more than one style sheet applies to a specific tag in a document the lowest level style sheet has precedence in a sense the browser searches for a style property spec starting with inline until if finds one or there isn t one levels of styles inline styles appear in the tag itself document level styles appear in the head of the document external styles exist in another place written as text files with mime type text css a link tag usually imports the style asset selectors probably the most important concept in css selectors simple the selector is a tag name or a list of tag names separated by commas or class has a name that attached to a tag class ex p narrow or p wide followed by property value lists selectors cont d generic a generic class can be defined if you want a style to apply to more than one type of tag generic classes must be named and the name must begin with a ex sale class sale sale selectors cont d id allows the application of a style to one specific element form unique id property value list ex selectors cont d contextual descendant ul li applies to li inside of ul child li a applies to a when it a child of li p em applies to em when it the child of that a child of p selectors cont d child cont d p first child p last child p only child for specific children p empty for no children those are pseudo classes btw selectors cont d pseudo classes pseudo classes are styles that apply when something happens rather than because the target element simply exists names begin with colons hover applies when cursor goes over an element focus applies when an element has focus link applies when link has not been selected visited applies when a link has been previously selected props and vals about different props in approx categories fonts lists alignment of text margins colors backgrounds borders just to name a few props and vals units of measurement pixels px inches in points pt picas pc height of the letter m em relative height of the letter m rem visual height vh visual width vw above are relative to the size of the viewport props and vals no space between number and unit specification percentages just number followed by a url values url protocol server pathname colors rgb rgba color name hex value hsl hsla props are inherited by all nested tags unless overridden font properties font family can have font names helvetica arial etc can have generic names serif sans serif etc browsers have different generic defaults font names more than word should use font properties font size can use px pt em rem etc and em or rem are the best choices can use words like small large smaller etc font properties font variants font style italic underline none etc font weight bold bolder light normal etc shorthand font bolder arial helvetica always use shorthand properties when possible list properties list style type bullet can be a disc square circle or image can be put on the ul or the li on the li it just applies to that item on ol can be used to change the sequence values decimal upper alpha lower alpha upper roman etc text alignment text indent allows for text indentation uses px or text align aligns lines or blocks of text left center justify float allows text to wrap around other elements left right none notes on floats can be a little tricky you have to clear other elements sometimes to preserve the flow of the document clear clears the float of an adjacent element left right both if we have an element we want on the right with text flowing on its left we use the default text align value left for the text and the right value for float on the element we want on the right box model very important to familiarize yourself with this borders controls whether an element has a border and if so what does it look like border style none dotted dashed double solid border width thin thick medium or px value border color you get guesses border css can apply to any or all of the borders box model margins and padding margins space between a border of an element and its neighboring element can specify top left right or bottom can combine with just margin use shorthand trbl or trouble top right bottom left box model margins and padding padding the amount of space between the content of an element and its contents controlled with top left bottom and right or you can use just padding background images property is called background image have to control the position and how it will or will not repeat is using bg img you have to specify all control for the repeat position color if any etc use shorthand with just the background prop conflicts many of the properties we have discussed apply to whole elements use divs spans or other semantic elements to operate on individual components of the content remember that style sheets cascade conflicts conflicts in css can occur when there are two or more values that correspond to more than one element conflicts between different levels of style sheets within one style sheet inheritance can cause conflicts prop values can come from style sheets written by one author the browser user and the browser defaults conflicts conflict resolution precedence rules for the different levels of style sheets source of the prop val specificity of the selector used to set the prop val can indicate importance important use sparingly and only when you need to external sheets are the best way to go you have more control and you don t have to worry about conflicts coming from other sources conflicts all available specs from all sources are sorted by origin and weight using the following rules which are given in precedence order important declarations with user origin important declarations with author origin normal declarations with author origin normal declarations with user origin any declarations with browser or other useragent origin some semi advanced ideas the css we ve just spoken about is just the tip of the iceberg i could go on for about more classes about this alone shocking i know think about these display inline inline block block flex etc position relative fixed static absolute flex an amazing way to lay out pages the fivequel the time the charm everything you ever wanted to know about html but were afraid to ask semantic html simply put it means using the right tag for the right job remember this seriously html is rendered on the client side whenever a document is requested original intent was to be able to be displayed on any number of computers html is a markup language originally used something called sgml or standard generalized markup language recent past versions that don t really matter any more c mon it html html same as but a little better xhtml defined with xml not sgml xhtml modular version of xhtml was cool and all but it handled errors about the same way as i do when i fly poorly so xhtml was just rendered as plain ol html in the took over development of html and tossed xhtml right out the window xhtml had one thing on html and that was a more strict syntax when writing then there was a boon to devkind the advent of basically it all the best parts of previous versions in the took over development of html and tossed xhtml right out the window xhtml had one thing on html and that was a more strict syntax when writing then there was a boon to devkind the advent of basically it all the best parts of previous versions while our book does a great job of discussing the need to knows about html someone who hates me must have written the html section please follow the style guide and try to do it the way that i do hopefully you ll think me for it one day that and flash the book does everything well but flash and some of the html html is a markup language markup languages use something called tags in order to define how certain elements should be displayed once there were these things called books editors and authors used markup to create page layouts html tags are fairly intuitive in what they mean tag format opening tag tag closing tag tag content will go between the opening and closing tags some tags won t have content tags can also be nested inside of each other just make sure you close what you open not all tags have an open and close some are self closing input type text you may see it like this input type text some self closers need it and some don t most don t the tag and its content together are known as an element this is important all tags can have an attribute attributes will go after the opening tag name div class red text id maindiv div onclick some js here data something data just to name a few attributes can be used to select an element identify it bind it to something all sorts of things attributes should be lower case as above comments are created with browsers will ignore anything inside a comment block browsers will also ignore unrecognized tags one that may not exist in the spec line breaks multiple spaces tabs and a few other things tags are really just suggestions to the browser even the recognized ones all docs must begin with the following doctype html the following tags are required html html head head title title body body the html tag is always the root element of the document meta tags are used for all sorts of other info provides additional info with attrs not content setting the charset meta charset utf text p p article article span span br breaks a new line inside text blocks not semantic to put text in say a plain div or a nav element headings six sizes to gauge importance h4 by default and use a larger font that the default uses the default font size and use smaller font sizes blockquotes blockquote browsers will indent this and sometimes italicize looks like a block of quote as it would appear in text fonts best to use css when possible but for now there this emphasis em em bold b b monospace code code can make text appear like code just to name a few superscripts and subscripts sup sub character entities a safe way to display certain characters in html amp will render as copy will render a copyright symbol will render a one fourth fraction nbsp will render a non breaking space many many more can be put into any tag and reduce the need to use images images gif graphic interchange format bit color colors jpeg joint photographic experts group bit color million colors both of the above use compression but jpeg compression is better png portable network graphics best choice and will probably replace the above preserves transparency image attributes alt used when a non graphical browser is used browsers with images turned off describes the image src used to point to a local or remote file you should use these attrs on all images you use image attributes alt used when a non graphical browser is used browsers with images turned off describes the image src used to point to a local or remote file you should use these attrs on all images you use different tags can be used for links a home a anchor tag can go to a new url with href can also go to another document text is clickable can point to other places in the doc as well link used to point to a local or remote file typically for style sheets or other meta elements link tags should stay in the head unordered lists ul ul will always contain list items li li puts a dot or other symbol in front of items can hold any number of tags or text ordered lists ol ol will also always contain list items li li will put numbers in front of items can also hold any number of content nested lists ul ul put this in place of a li li indents content nav items nav nav contains ul ul and li li tags typically consists of links semantic way of handling navigational elements only to be used for tabular data i can t stress that enough some sites want me to be sad and make the whole thing in tables doesn t translate well to mobile trust me can have rows and headers inside of a table table you ll see caption tags after tables denoting a title rows defined with tr tr tags row headings use the th th tags contents of a data cell use the td td tags can use a colspan attr to define cell widths same for rows html forms are awesome and versatile common way to get data from the client to the server action attr tells the form where it sending data defaults to current url if not defined method attr defines a verb to use get post etc defaults to get if none is provided uses elements called inputs to provide data type text num tel email radio checkbox password inputs need a name attr to identify them to the server some inputs can have a default attr added to them radio buttons need to have the same name select elements not an input per se allows you to present the end user with multiple options requires a name when in a form can be used in other places not just in forms filled with option option tags options can be grouped with the optgroup optgroup textareas like a regular text input but larger can limit character input also with regular inputs good for comments and such header elements hgroup container for header info header header container for header content content is usually links maybe a search bar etc footer elements can contain links info scripts etc section article watch out with safari aside and nav are some examples of other org elements audio elements native audio handling no plugins needed needs a src added text to the tag will let user know if not supported video elements native video handling no plugins needed can specify controls also needs a src same alert will happen if not supported an intro to the intronet err internet me most days i work but let keep that between us origins of the net way less radioactive spider bites than one would hope origins arpanet late and early network reliability for arpa funded research organizations bitnet csnet late early email and file transfer for other institutions nsfnet initially connected five supercomputer centers soon became the network for all by the early nsfnet eventually became known as the internet but what is it though a world wide network of computer networks at the lowest level since all connections use tcp ip cp ip hides the differences among devices connected to the internet internet protocol ip addresses internet protocol ip addresses every node has a unique numeric address form bit binary number new standard has bits organizations are assigned groups of ips for their computers you can enter an ip address directly it a little tough to remember off top of your head though at least for me it is domain names form host name domain names first domain is the smallest last is the largest ex docs google com fully qualified domain name the host name and all of the domain names it not fully qualified if it doesn t have all the parts dns servers convert fully qualified domain names to ips turns those funky numbers into human readable names the world wide web tim berners lee at cern proposed the web in purpose to allow scientists to have access to many databases of scientific work through their own computers document form hypertext pages documents resources let just call them documents hypermedia more than just text images sound etc the world wide web cont d web or internet the web uses one of the protocols http that runs on the internet there are several others telnet mailto etc the delineation between the web and the internet these days is very thin browsers mosaic ncsa univ of illinois in early first to use a gui led to explosion of web use browsers are clients always initiate servers react although sometimes servers require responses most requests are for existing documents using hypertext transfer protocol http browsers cont d but some requests are for program execution with the output being returned as a document html forms or functions firing off depending upon input chrome ie ewwww firefox opera some other browsers are out there but these are the main ones that are most widely supported servers provide responses to browser requests either existing documents or dynamically built ones all communications between browsers and servers use hypertext transfer protocol http web servers run as background processes in the operating system monitor a communications port on the host accepting http messages when they appear web servers have two main directories document root servable documents server root server system software document root is accessed indirectly by clients its actual location is set by the server configuration file requests are mapped to the actual location proxy servers dedicated computer or system running on a computer that acts as an intermediary between an endpoint device such as a computer and another server from which a user or client is requesting a service basically routing a request from your device to another device before fulfilling the request apache open source fast reliable began as the ncsa server httpd maintained by editing its configuration file very commonly used config file usually called htaccess allows for all sorts of customization take file extensions off of a url redirect under certain circumstances allow for direct file downloads etc general form scheme object address the scheme is often a communications protocol such as telnet or ftp or http for the http protocol the object address is fully qualified domain name doc path for the file protocol only the doc path is needed files downloads doc pdf host name may include a port number as in zeppo is the default so this is silly urls cannot include spaces or any of a collection of other special characters semicolons colons the doc path may be abbreviated as a partial path the rest is furnished by the server configuration docs downloads items files doc pdf would map to something like downloads doc pdf on the client side if the doc path ends with a slash it means it is a directory originally developed for email used to specify to the browser the form of a file returned by the server attached by the server to the beginning of the document type specifications form type subtype examples text plain text html image gif image jpeg server gets type from the requested file name suffix html implies text html browser gets the type explicitly from the server the hypertext transfer protocol http as it is known to its friends the protocol used by all web communications request phase form http method domain part of url http ver header fields blank line message body an example of the first line of a request get cs pitt edu degrees html http most commonly used methods get fetch a document post execute the document using the data in body head fetch just the header of the document put store a new document on the server delete remove a document from the server four categories of header fields general request response and entity common request fields accept text plain accept text if date common response fields content length content type text html can communicate with http without a browser response phase form status line response header fields blank line response body status line format http version status code explanation example http ok current version is status code is a three digit number first digit specifies the general status 1 informational success redirection client error server error the header field content type is required an example of a complete response header http 1 1 ok date tue september 20 gmt server apache 2 centos last modified tues may 38 gmt etag accept ranges bytes content length connection close content type text html charset utf both request headers and response headers must be followed by a blank line there are many kinds of security problems with the internet and the web shocking i know one fundamental problem is getting data between a browser and a server without it being intercepted or corrupted in the process hacks on hacks on hacks security issues for a communication between a browser and a server privacy integrity authentication the basic tool to support privacy and integrity is encryption originally a single key was used for both encryption and decryption which requires the sender of an encrypted document to somehow transmit the key to the receiver solution diffie and hellman public key encryption use a public private key pair everyone uses your public key to encrypt messages sent to you you decrypt them with your matching private key it works because it is virtually impossible to compute the private from a given public key rsa is the most widely used public key algorithm another security problem destruction of data on computers connected to the internet viruses and worms malware ransomware yet another common security problem denial of service dos distributed denial of service ddos created by flooding a web server with requests servers and interwebs and junk it important to know how a lot of this stuff works before you start to develop cs midterm review structure short answers t f and fill in the blanks code tracing code writing will be asked to write a simple web server app review php code examples please brush up on basic html at least to the extent covered in class php differences with java php has copy mode and interpret mode special semantics of single quoted double quoted strings dynamically typed language what does this mean what are the implications why do you need and operators php differences with java cont d semantics and implementation of php arrays hash semantics direct access sequential access semantics internal implementation passing by reference passing by value what are the classes of values in php scoping rules local global and static php differences with java cont d oop the this variable magic methods serialization regular expressions character classes matching sequences anchors subpatterns greedy vs lazy matching php facilities for web programming getting input from web client get post in what form data is passed inside http request what situations one should be used over the other how to access input from php script file i o can use low level fread fwrite but also file use of flock to synchronize accesses to files php facilities for web programming cont d http is a stateless protocol no state transferred between each page load maintaining state using cookies and sessions where cookies sessions are stored how they are communicated between web server and web client to maintain state between page loads structure of web server http response message and how it affects usage cs final review structure short answers t f and fill in the blanks code tracing code writing will be asked to write a simple web app using ajax and dom accesses jquery allowed review javascript jquery xml code examples database review quiz basic concepts schema keys joins database security encrypting sensitive information in databases sql injection sql queries php mysql api learn how to write code using both web security authorization authentication https javascript basic language characteristics distributed in source form interpreted on browser dynamic typing similar to php duck typing different from php javascript objects javascript arrays inheritance through prototypes javascript cont d document object model tree structure access modification event handling ajax asynchronous javascript and xml implementation using asynchronous call backs consistency issues between client and server xml json learn how to code using ajax javascript cont d jquery library the function returns a jquery object for further method calls document ready function registers given function as call back for ready event selector returns jquery object representing element collection each iterator dom access modification event methods ajax convenience functions load get post xml extensible markup language a metalanguage with which you can define other markup languages mathml rss xslt philosophy separate out content from formatting enables content to be processed in myriad ways enables end consumer of data to visualize it various ways according to preference document model specification defines the markup language you are using well formed vs valid specification through dtd document type definition xml cont d stylesheets css cascading style sheet css inheritance xsl extensible stylesheet language combination of xslt xpath xsl fo xslt processor interprets xsl namespace tags and leaves the rest e g html code alone applying templates using xpath selectors xsl in conjunction with ajax for on the fly translations learn how to read and use stylesheets in code client side storage storage using cookies document cookie and jquery wrapper plugin downsides in usability scalability alternative solutions with sessionstorage temporary browser session data localstorage persistent data in chapter i gave a very brief introduction to php arrays just enough for a little taste of their power in this chapter i ll show you many more things that you can do with arrays some of which if you have ever used a strongly typed language such as c may surprise you with their elegance and simplicity arrays are an example of what has made php so popular not only do they remove the tedium of writing code to deal with complicated data structures they also provide nu merous ways to access data while remaining amazingly fast basic access we ve already looked at arrays as if they were clusters of matchboxes glued together another way to think of an array is like a string of beads with the beads representing variables that can be numeric string or even other arrays they are like bead strings because each element has its own location and with the exception of the first and last ones each has other elements on either side some arrays are referenced by numeric indices others allow alphanumeric identifiers built in functions let you sort them add or remove sections and walk through them to handle each item through a special kind of loop and by placing one or more arrays inside another you can create arrays of two three or any number of dimensions numerically indexed arrays let assume that you ve been tasked with creating a simple website for a local office supply company and you re currently working on the section devoted to paper one way to manage the various items of stock in this category would be to place them in a numeric array you can see the simplest way of doing so in example example adding items to an array php paper copier paper inkjet paper laser paper photo paper in this example each time you assign a value to the array paper the first empty location within that array is used to store the value and a pointer internal to php is incremented to point to the next free location ready for future insertions the familiar function which prints out the contents of a variable array or object is used to verify that the array has been correctly populated it prints out the following array copier inkjet laser photo the previous code could also have been written as shown in example where the exact location of each item within the array is specified but as you can see that approach requires extra typing and makes your code harder to maintain if you want to insert or remove supplies from the array so unless you wish to specify a different order it usually better to simply let php handle the actual location numbers example adding items to an array using explicit locations php paper copier paper inkjet paper laser paper photo paper the output from these examples is identical but you are not likely to use in a developed website so example shows how you might print out the various types of paper the website offers using a for loop example adding items to an array and retrieving them php paper copier paper inkjet paper laser paper photo for j j j echo j paper j br this example prints out the following copier inkjet laser photo so far you ve seen a couple of ways in which you can add items to an array and one way of referencing them but php offers many more which i ll get to shortly but first we ll look at another type of array associative arrays keeping track of array elements by index works just fine but can require extra work in terms of remembering which number refers to which product it can also make code hard for other programmers to follow this is where associative arrays come into their own using them you can reference the items in an array by name rather than by number example expands on the previous code by giving each element in the array an identifying name and a longer more ex planatory string value example adding items to an associative array and retrieving them php paper copier copier multipurpose paper inkjet inkjet printer paper laser laser printer paper photo photographic paper echo paper laser in place of a number which doesn t convey any useful information aside from the position of the item in the array each item now has a unique name that you can use to reference it elsewhere as with the echo statement which simply prints out laser printer the names copier inkjet etc are called indexes or keys and the items assigned to them such as laser printer are called values this very powerful feature of php is often used when you are extracting information from xml and html for example an html parser such as those used by a search engine could place all the elements of a web page into an associative array whose names reflect the page structure html title my web page html body body of web page the program would also probably break down all the links found within a page into another array and all the headings and subheadings into another when you use asso ciative rather than numeric arrays the code to refer to all of these items is easy to write and debug assignment using the array keyword so far you ve seen how to assign values to arrays by just adding new items one at a time whether you specify keys specify numeric identifiers or let php assign numeric iden tifiers implicitly this is a long winded approach a more compact and faster assignment method uses the array keyword example shows both a numeric and an associative array assigned using this method example adding items to an array using the array keyword php array copier inkjet laser photo echo element br array copier copier multipurpose inkjet inkjet printer laser laser printer photo photographic paper echo element inkjet br the first half of this snippet assigns the old shortened product descriptions to the array there are four items so they will occupy slots through therefore the echo statement prints out the following element laser the second half assigns associative identifiers and accompanying longer product de scriptions to the array using the format index value the use of is similar to the regular assignment operator except that you are assigning a value to an index and not to a variable the index is then inextricably linked with that value unless it is as signed a new value the echo command therefore prints out element inkjet printer you can verify that and are different types of array because both of the following commands when appended to the code will cause an undefined index or undefined offset error as the array identifier for each is incorrect echo inkjet undefined index echo undefined offset the foreach as loop the creators of php have gone to great lengths to make the language easy to use so not content with the loop structures already provided they added another one especially for arrays the foreach as loop using it you can step through all the items in an array one at a time and do something with them the process starts with the first item and ends with the last one so you don t even have to know how many items there are in an array example shows how foreach as can be used to rewrite example example walking through a numeric array using foreach as php paper array copier inkjet laser photo j foreach paper as item echo j item br j when php encounters a foreach statement it takes the first item of the array and places it in the variable following the as keyword and each time control flow returns to the foreach the next array element is placed in the as keyword in this case the variable item is set to each of the four values in turn in the array paper once all values have been used execution of the loop ends the output from this code is exactly the same as example now let see how foreach works with an associative array by taking a look at example which is a rewrite of the second half of example example walking through an associative array using foreach as php paper array copier copier multipurpose inkjet inkjet printer laser laser printer photo photographic paper foreach paper as item description echo item description br remember that associative arrays do not require numeric indexes so the variable j is not used in this example instead each item of the array paper is fed into the key value pair of variables item and description from which they are printed out the dis played result of this code is as follows copier copier multipurpose inkjet inkjet printer laser laser printer photo photographic paper as an alternative syntax to foreach as you can use the list function in conjunc tion with the each function as in example example walking through an associative array using each and list php paper array copier copier multipurpose inkjet inkjet printer laser laser printer photo photographic paper while list item description each paper echo item description br in this example a while loop is set up and will continue looping until the each function returns a value of false the each function acts like foreach it returns an array con taining a key value pair from the array paper and then moves its built in pointer to the next pair in that array when there are no more pairs to return each returns false the list function takes an array as its argument in this case the key value pair returned by the function each and then assigns the values of the array to the variables listed within parentheses you can see how list works a little more clearly in example where an array is created out of the two strings alice and bob and then passed to the list function which assigns those strings as values to the variables a and b example using the list function php list a b array alice bob echo a a b b the output from this code is a alice b bob so you can take your pick when walking through arrays use foreach as to create a loop that extracts values to the variable following the as or use the each function and create your own looping system multidimensional arrays a simple design feature in php array syntax makes it possible to create arrays of more than one dimension in fact they can be as many dimensions as you like although it a rare application that goes further than three that feature makes it possible to include an entire array as a part of another one and to be able to keep doing so just like the old rhyme big fleas have little fleas upon their backs to bite em little fleas have lesser fleas add flea ad infinitum let look at how this works by taking the associative array in the previous example and extending it see example example creating a multidimensional associative array php products array paper array copier copier multipurpose inkjet inkjet printer laser laser printer photo photographic paper pens array ball ball point hilite highlighters marker markers misc array tape sticky tape glue adhesives clips paperclips echo pre foreach products as section items foreach items as key value echo section t key t value br echo pre to make things clearer now that the code is starting to grow i ve renamed some of the elements for example because the previous array paper is now just a subsection of a larger array the main array is now called products within this array there are three items paper pens and misc each of which contains another array with key value pairs if necessary these subarrays could have contained even further arrays for example under ball there might be many different types and colors of ballpoint pens available in the online store but for now i ve restricted the code to a depth of just two once the array data has been assigned i use a pair of nested foreach as loops to print out the various values the outer loop extracts the main sections from the top level of the array and the inner loop extracts the key value pairs for the categories within each section as long as you remember that each level of the array works the same way it a key value pair you can easily write code to access any element at any level the echo statement makes use of the php escape character t which outputs a tab although tabs are not normally significant to the web browser i let them be used for layout by using the pre pre tags which tell the web browser to format the text as preformatted and monospaced and not to ignore whitespace characters such as tabs and line feeds the output from this code looks like the following paper copier copier multipurpose paper inkjet inkjet printer paper laser laser printer paper photo photographic paper pens ball ball point pens hilite highlighters pens marker markers misc tape sticky tape misc glue adhesives misc clips paperclips you can directly access a particular element of the array using square brackets like this echo products misc glue which outputs the value adhesives you can also create numeric multidimensional arrays that are accessed directly by in dexes rather than by alphanumeric identifiers example creates the board for a chess game with the pieces in their starting positions example creating a multidimensional numeric array php chessboard array array r n b q k b n r array p p p p p p p p array array array array array p p p p p p p p array r n b q k b n r echo pre foreach chessboard as row foreach row as piece echo piece echo br echo pre in this example the lowercase letters represent black pieces and the uppercase white the key is r rook n knight b bishop k king q queen and p pawn again a pair of nested foreach as loops walks through the array and displays its contents the outer loop processes each row into the variable row which itself is an array because the chessboard array uses a subarray for each row this loop has two statements within it so curly braces enclose them the inner loop then processes each square in a row outputting the character piece stored in it followed by a space to square up the printout this loop has a single statement so curly braces are not required to enclose it the pre and pre tags ensure that the output displays correctly like this r n b q k b n r p p p p p p p p p p p p p p p p r n b q k b n r you can also directly access any element within this array using square brackets like this echo chessboard this statement outputs the uppercase letter q the eighth element down and the fourth along remembering that array indexes start at not using array functions you ve already seen the list and each functions but php comes with numerous other functions for handling arrays the full list is at http tinyurl com arraysinphp however some of these functions are so fundamental that it worth taking the time to look at them here arrays and variables share the same namespace this means that you cannot have a string variable called fred and an array also called fred if you re in doubt and your code needs to check whether a variable is an array you can use the function like this echo fred is an array is not an array note that if fred has not yet been assigned a value an undefined variable message will be generated count although the each function and foreach as loop structure are excellent ways to walk through an array contents sometimes you need to know exactly how many ele ments there are in your array particularly if you will be referencing them directly to count all the elements in the top level of an array use a command such as the following echo count fred should you wish to know how many elements there are altogether in a multidimensional array you can use a statement such as echo count fred the second parameter is optional and sets the mode to use it should be either a to limit counting to only the top level or to force recursive counting of all subarray elements too sort sorting is so common that php provides a built in function in its simplest form you would use it like this sort fred unlike some other functions sort will act directly on the supplied array rather than returning a new array of sorted elements instead it returns true on success and false on error and also supports a few flags but the main two that you might wish to use force sorting to be made either numerically or as strings like this sort fred sort fred you can also sort an array in reverse order using the rsort function like this rsort fred rsort fred shuffle there may be times when you need the elements of an array to be put in random order such as when you re creating a game of playing cards shuffle cards like sort shuffle acts directly on the supplied array and returns true on success or false on error explode this is a very useful function with which you can take a string containing several items separated by a single character or string of characters and then place each of these items into an array one handy example is to split up a sentence into an array containing all its words as in example example exploding a string into an array using spaces php temp explode this is a sentence with seven words temp this example prints out the following on a single line when viewed in a browser array this is a sentence with seven words the first parameter the delimiter need not be a space or even a single character example shows a slight variation example exploding a string delimited with into an array php temp explode a sentence with asterisks temp the code in example prints out the following array a sentence with asterisks extract sometimes it can be convenient to turn the key value pairs from an array into php variables one such time might be when you are processing the or vari ables as sent to a php script by a form when a form is submitted over the web the web server unpacks the variables into a global array for the php script if the variables were sent using the get method they will be placed in an associative array called if they were sent using post they will be placed in an associative array called you could of course walk through such associative arrays in the manner shown in the examples so far however sometimes you just want to store the values sent into variables for later use in this case you can have php do the job automatically for you extract so for example if the query string parameter q is sent to a php script along with the associated value hi there a new variable called q will be created and assigned that value be careful with this approach though because if any extracted variables conflict with ones that you have already defined your existing values will be overwritten to avoid this possibility you can use one of the many additional parameters available to this function like this extract fromget in this case all the new variables will begin with the given prefix string followed by an underscore so q will become i strongly recommend that you use this version of the function when handling the and arrays or any other array whose keys could be controlled by the user because malicious users could submit keys chosen deliberately to overwrite commonly used variable names and compromise your website compact there are also times when you want to use compact the inverse of extract to create an array from variables and their values example shows how you might use this function example using the compact function php fname doctor sname who planet gallifrey system gridlock constellation kasterborous contact compact fname sname planet system constellation contact the result of running example is array fname doctor sname who planet gallifrey system gridlock constellation kasterborous note how compact requires the variable names to be supplied in quotes not preceded by a symbol this is because compact is looking for a list of variable names another use of this function is for debugging when you wish to quickly view several variables and their values as in example example using compact to help with debugging php j temp hello address old street age compact explode j temp address age this works by using the explode function to extract all the words from the string into an array which is then passed to the compact function which in turn returns an array to which finally shows its contents if you copy and paste the line of code you only need to alter the variables named there for a quick printout of a group of variables values in this example the output is array j temp hello address old street age reset when the foreach as construct or the each function walks through an array it keeps an internal php pointer that makes a note of which element of the array it should return next if your code ever needs to return to the start of an array you can issue reset which also returns the value of that element examples of how to use this function are reset fred throw away return value item reset fred keep first element of the array in item end as with reset you can move php internal array pointer to the final element in an array using the end function which also returns the value of the element and can be used as in these examples end fred item end fred this chapter concludes your basic introduction to php and you should now be able to write quite complex programs using the skills you have learned in the next chapter we ll look at using php for common practical tasks questions what is the difference between a numeric and an associative array what is the main benefit of the array keyword what is the difference between foreach and each how can you create a multidimensional array how can you determine the number of elements in an array what is the purpose of the explode function how can you set php internal pointer into an array back to the first element of the array see chapter answers on page in appendix a for the answers to these questions chapter practical php previous chapters went over the elements of the php language this chapter builds on your new programming skills to teach you some common but important practical tasks you will learn the best ways to manage string handling to achieve clear and concise code that displays in web browsers exactly how you want it to including advanced date and time management you ll also find out how to create and otherwise modify files in cluding those uploaded by users using printf you ve already seen the print and echo functions which simply output text to the browser but a much more powerful function printf controls the format of the output by letting you put special formatting characters in a string for each formatting char acter printf expects you to pass an argument that it will display using that format for instance the following example uses the d conversion specifier to display the value in decimal printf there are d items in your basket if you replace the d with b the value would be displayed in binary table shows the conversion specifiers supported table the printf conversion specifiers display a character no arg is required b display arg as a binary integer c display ascii character for the arg d display arg as a signed decimal integer e display arg using scientific notation f display arg as floating point o display arg as an octal integer display arg as a string u display arg as an unsigned decimal v display arg in lowercase hexadecimal x display arg in uppercase hexadecimal you can have as many specifiers as you like in a printf function as long as you pass a matching number of arguments and as long as each specifier is prefaced by a symbol therefore the following code is valid and will output my name is simon i m years old which is in hexadecimal printf my name is i m d years old which is x in hexadecimal simon if you leave out any arguments you will receive a parse error informing you that a right bracket was unexpectedly encountered a more practical example of printf sets colors in html using decimal for example suppose you know you want a color that has a triplet value of red green and blue but don t want to convert this to hexadecimal yourself an easy solution is printf span style color x x x hello span check the format of the color specification between the apostrophes carefully first comes the pound or hash sign expected by the color specification then come three x format specifiers one for each of your numbers the resulting output from this command is span style color hello span usually you ll find it convenient to use variables or expressions as arguments to printf for instance if you stored values for your colors in the three variables r g and b you could create a darker color with printf span style color x x x hello span r g b precision setting not only can you specify a conversion type you can also set the precision of the displayed result for example amounts of currency are usually displayed with only two digits of precision however after a calculation a value may have a greater precision than this such as which results in to ensure that such values are correctly stored internally but displayed with only two digits of precision you can insert the string between the symbol and the conversion specifier printf the result is the output from this command is the result is but you actually have even more control than that because you can also specify whether to pad output with either zeros or spaces by prefacing the specifier with certain values example shows four possible combinations example precision setting php echo pre enables viewing of the spaces pad to spaces printf the result is n pad to spaces fill with zeros printf the result is n pad to spaces decimal places precision printf the result is n pad to spaces decimal places precision fill with zeros printf the result is n pad to spaces decimal places precision fill with symbol printf the result is n the output from this example looks like this the result is the result is the result is the result is the result is the way it works is simple if you go from right to left see table notice that the rightmost character is the conversion specifier in this case it is f for floating point just before the conversion specifier if there is a period and a number together then the precision of the output is specified as the value of the number regardless of whether there a precision specifier if there is a number then that represents the amount of characters to which the output should be padded in the previous example this is characters if the output is already equal to or greater than the padding length then this argument is ignored the leftmost parameter allowed after the symbol is a which is ignored unless a padding value has been set in which case the output is padded with zeros instead of spaces if a pad character other than zero or a space is required you can use any one of your choice as long as you preface it with a single quotation mark like this on the left is the symbol which starts the conversion table conversion specifier components start conversion pad character number of pad characters display precision conversion specifier examples f f f string padding you can also pad strings to required lengths as you can with numbers select different padding characters and even choose between left and right justification example shows various examples example string padding php echo pre enables viewing of the spaces h rasmus printf n h standard string output printf n h right justify with spaces to width printf n h left justify with spaces printf n h zero padding printf n n h use the custom padding character d rasmus lerdorf the original creator of php printf n d right justify cutoff of characters printf n d left justify cutoff of characters printf n d left justify pad cutoff chars note how for purposes of layout in a web page i ve used the pre html tag to preserve all the spaces and the n newline character after each of the lines to be displayed the output from this example is as follows rasmus rasmus rasmus rasmus rasmus l rasmus lerdo rasmus ler when you are specifying a padding value if a string is already of equal or greater length than that value it will be ignored unless a cutoff value is given that shortens the string back to less than the padding value table shows a breakdown of the components available to string conversion specifiers table string conversion specifier components start conversion left right padding justify character number of pad characters cutoff conversion specifier examples using rasmus rasmus rasmus rasm using sprintf often you don t want to output the result of a conversion but need it to use elsewhere in your code this is where the sprintf function comes in with it you can send the output to another variable rather than to the browser you might use it simply to make a conversion as in the following example which returns the hexadecimal string value for the rgb color group in hexstring hexstring sprintf x x x or you may wish to store output ready to display later on out sprintf the result is 123 42 echo out date and time functions to keep track of the date and time php uses standard unix timestamps which are simply the number of seconds since the start of january to determine the current timestamp you can use the time function echo time because the value is stored as seconds to obtain the timestamp for this time next week you would use the following which adds days times hours times minutes times seconds to the returned value echo time if you wish to create a timestamp for a given date you can use the mktime function its output is the timestamp for the first second of the first minute of the first hour of the first day of the year echo mktime the parameters to pass are in order from left to right the number of the hour the number of the minute the number of seconds the number of the month the number of the day the year or with php on bit signed systems you may ask why you are limited to the years through well it because the original developers of unix chose the start of the year as the base date that no programmer should need to go before luckily because as of version php supports systems using a signed bit integer for the timestamp dates from to are allowed on them however that introduces a problem even worse than the original because the unix designers also decided that nobody would be using unix after about years or so and there fore believed they could get away with storing the timestamp as a bit value which will run out on january this will create what has come to be known as the bug much like the mil lennium bug which was caused by storing years as two digit values and which also had to be fixed php introduced the datetime class in version to overcome this issue but it will work only on bit architecture to display the date use the date function which supports a plethora of formatting options enabling you to display the date any way you could wish the format is as follows date format timestamp the parameter format should be a string containing formatting specifiers as detailed in table and timestamp should be a unix timestamp for the complete list of specifiers see http php net manual en function date php the following command will output the current date and time in the format thursday july 38pm echo date l f js y g ia time table the major date function format specifiers day specifiers d day of month two digits with leading zeros to d day of the week three letters mon to sun j day of the month no leading zeros to l day of week full names sunday to saturday n day of week numeric monday to sunday to s suffix for day of month useful with specifier j st nd rd or th w day of week numeric sunday to saturday to z day of year to week specifier w week number of year to month specifiers f month name january to december m month number with leading zeros to m month name three letters jan to dec n month number no leading zeros to t number of days in given month to year specifiers l leap year yes no y year digits to y year digits to time specifiers a before or after midday lowercase am or pm a before or after midday uppercase am or pm g hour of day hour format no leading zeros to g hour of day hour format no leading zeros to h hour of day hour format with leading zeros to h hour of day hour format with leading zeros to i minutes with leading zeros to seconds with leading zeros to date constants there are a number of useful constants that you can use with the date command to return the date in specific formats for example date returns the current date and time in the valid format for an rss feed some of the more commonly used constants are this is the format for atom feeds the php format is y m d th i sp and ex ample output is this is the format for cookies set from a web server or javascript the php format is l d m y h i t and example output is thursday aug utc this is the format for rss feeds the php format is d d m y h i o and example output is thu aug utc this is the format for the world wide web consortium the php format is y m d th i sp and example output is the complete list can be found at http php net manual en class datetime php using checkdate you ve seen how to display a valid date in a variety of formats but how can you check whether a user has submitted a valid date to your program the answer is to pass the month day and year to the checkdate function which returns a value of true if the date is valid or false if it is not for example if february of any year is input it will always be an invalid date example shows code that you could use for this as it stands it will find the given date invalid example checking for the validity of a date php month september only has days day 31st year if checkdate month day year echo date is valid else echo date is invalid file handling powerful as it is mysql is not the only or necessarily the best way to store all data on a web server sometimes it can be quicker and more convenient to directly access files on the hard disk cases in which you might need to do this are modifying images such as uploaded user avatars or log files that you wish to process first though a note about file naming if you are writing code that may be used on various php installations there is no way of knowing whether these systems are case sensitive for example windows and mac os x filenames are not case sensitive but linux and unix ones are therefore you should always assume that the system is case sensitive and stick to a convention such as all lowercase filenames checking whether a file exists to determine whether a file already exists you can use the function which returns either true or false and is used like this if testfile txt echo file exists creating a file at this point testfile txt doesn t exist so let create it and write a few lines to it type example and save it as testfile php example creating a simple text file php testfile php fh fopen testfile txt w or die failed to create file text line line line fwrite fh text or die could not write to file fclose fh echo file testfile txt written successfully when you run this in a browser all being well you will receive the message file test file txt written successfully if you receive an error message your hard disk may be full or more likely you may not have permission to create or write to the file in which case you should modify the attributes of the destination folder according to your operating system otherwise the file testfile txt should now be residing in the same folder in which you saved the testfile php program try opening the file in a text or program editor the contents will look like this line line line this simple example shows the sequence that all file handling takes always start by opening the file you do this through a call to fopen then you can call other functions here we write to the file fwrite but you can also read from an existing file fread or fgets and do other things finish by closing the file fclose although the program does this for you when it ends you should clean up after yourself by closing the file when you re finished every open file requires a file resource so that php can access and manage it the preceding example sets the variable fh which i chose to stand for file handle to the value returned by the fopen function thereafter each file handling function that ac cesses the opened file such as fwrite or fclose must be passed fh as a parameter to identify the file being accessed don t worry about the content of the fh variable it a number php uses to refer to internal information about the file you just pass the variable to other functions upon failure false will be returned by fopen the previous example shows a simple way to capture and respond to the failure it calls the die function to end the program and give the user an error message a web application would never abort in this crude way you would create a web page with an error message instead but this is fine for our testing purposes notice the second parameter to the fopen call it is simply the character w which tells the function to open the file for writing the function creates the file if it doesn t already exist be careful when playing around with these functions if the file already exists the w mode parameter causes the fopen call to delete the old contents even if you don t write anything new there are several different mode parameters that can be used here as detailed in table table the supported fopen modes r read from file start open for reading only place the file pointer at the beginning of the file return false if the file doesn t already exist r read from file start and allow writing open for reading and writing place the file pointer at the beginning of the file return false if the file doesn t already exist w write from file start and truncate file w write from file start truncate file and allow reading open for writing only place the file pointer at the beginning of the file and truncate the file to zero length if the file doesn t exist attempt to create it open for reading and writing place the file pointer at the beginning of the file and truncate the file to zero length if the file doesn t exist attempt to create it a append to file end open for writing only place the file pointer at the end of the file if the file doesn t exist attempt to create it a append to file end and allow reading reading from files open for reading and writing place the file pointer at the end of the file if the file doesn t exist attempt to create it the easiest way to read from a text file is to grab a whole line through fgets think of the final as standing for string as in example example reading a file with fgets php fh fopen testfile txt r or die file does not exist or you lack permission to open it line fgets fh fclose fh echo line if you created the file as shown in example you ll get the first line line or you can retrieve multiple lines or portions of lines through the fread function as in example example reading a file with fread php fh fopen testfile txt r or die file does not exist or you lack permission to open it text fread fh fclose fh echo text i ve requested three characters in the fread call so the program displays the following lin the fread function is commonly used with binary data but if you use it on text data that spans more than one line remember to count newline characters copying files let try out the php copy function to create a clone of testfile txt type in example and save it as copyfile php and then call up the program in your browser example copying a file php copyfile php copy testfile txt txt or die could not copy file echo file successfully copied to txt if you check your folder again you ll see that you now have the new file txt in it by the way if you don t want your programs to exit on a failed copy attempt you could try the alternate syntax in example example alternate syntax for copying a file php php if copy testfile txt txt echo could not copy file else echo file successfully copied to txt moving a file to move a file rename it with the rename function as in example example moving a file php movefile php if rename txt new echo could not rename file else echo file successfully renamed to new you can use the rename function on directories too to avoid any warning messages if the original file doesn t exist you can call the function first to check deleting a file deleting a file is just a matter of using the unlink function to remove it from the file system as in example example deleting a file php deletefile php if unlink new echo could not delete file else echo file new successfully deleted whenever you access files on your hard disk directly you must also always ensure that it is impossible for your filesystem to be compro mised for example if you are deleting a file based on user input you must make absolutely certain it is a file that can be safely de leted and that the user is allowed to delete it as with moving a file a warning message will be displayed if the file doesn t exist which you can avoid by using to first check for its existence before calling unlink updating files often you will want to add more data to a saved file which you can do in many ways you can use one of the append write modes see table or you can simply open a file for reading and writing with one of the other modes that supports writing and move the file pointer to the correct place within the file that you wish to write to or read from the file pointer is the position within a file at which the next file access will take place whether it a read or a write it is not the same as the file handle as stored in the variable fh in example which contains details about the file being accessed you can see this in action by typing example and saving it as update php then call it up in your browser example updating a file php update php fh fopen testfile txt r or die failed to open file text fgets fh fseek fh fwrite fh text or die could not write to file fclose fh echo file testfile txt successfully updated this program opens testfile txt for both reading and writing by setting the mode with r which puts the file pointer right at the start it then uses the fgets function to read in a single line from the file up to the first line feed after that the fseek function is called to move the file pointer right to the file end at which point the line of text that was extracted from the start of the file stored in text is then appended to file end and the file is closed the resulting file now looks like this line line line line the first line has successfully been copied and then appended to the file end as used here in addition to the fh file handle the fseek function was passed two other parameters and tells the function to move the file pointer to the end of the file and tells it how many positions it should then be moved backward from that point in the case of example a value of is used because the pointer is required to remain at the file end there are two other seek options available to the fseek function and the option tells the function to set the file pointer to the exact position given by the preceding parameter thus the following example moves the file pointer to position fseek fh sets the file pointer to the current position plus the value of the given offset therefore if the file pointer is currently at position the following call will move it to position fseek fh although this is not recommended unless you have very specific reasons for it it is even possible to use text files such as this but with fixed line lengths as simple flat file databases your program can then use fseek to move back and forth within such a file to retrieve update and add new records you can also delete records by overwriting them with zero characters and so on locking files for multiple accesses web programs are often called by many users at the same time if more than one person tries to write to a file simultaneously it can become corrupted and if one person writes to it while another is reading from it the file is all right but the person reading it can get odd results to handle simultaneous users you must use the file locking flock function this function queues up all other requests to access a file until your program releases the lock so whenever your programs use write access on files that may be accessed concurrently by multiple users you should also add file locking to them as in example which is an updated version of example example updating a file with file locking php fh fopen testfile txt r or die failed to open file text fgets fh if flock fh fseek fh seek_end fwrite fh text or die could not write to file flock fh fclose fh echo file testfile txt successfully updated there is a trick to file locking to preserve the best possible response time for your website visitors perform it directly before a change you make to a file and then unlock it im mediately afterward having a file locked for any longer than this will slow down your application unnecessarily this is why the calls to flock in example are directly before and after the fwrite call the first call to flock sets an exclusive file lock on the file referred to by fh using the parameter flock fh from this point onward no other processes can write to or even read from the file until you release the lock by using the parameter like this flock fh as soon as the lock is released other processes are again allowed access to the file this is one reason why you should reseek to the point you wish to access in a file each time you need to read or write data because another process could have changed the file since the last access however did you notice that the call to request an exclusive lock is nested as part of an if statement this is because flock is not supported on all systems thus it is wise to check whether you successfully secured a lock just in case one could not be obtained something else you must consider is that flock is what is known as an advisory lock this means that it locks out only other processes that call the function if you have any code that goes right in and modifies files without implementing flock file locking it will always override the locking and could wreak havoc on your files by the way implementing file locking and then accidentally leaving it out in one section of code can lead to an extremely hard to locate bug flock will not work on nfs and many other networked filesystems also when using a multithreaded server like isapi you may not be able to rely on flock to protect files against other php scripts run ning in parallel threads of the same server instance additionally flock is not supported on any system using the old fat filesystem such as older versions of windows reading an entire file a handy function for reading in an entire file without having to use file handles is it very easy to use as you can see in example example using php echo pre enables display of line feeds echo testfile txt echo pre terminates pre tag but the function is actually a lot more useful than that because you can also use it to fetch a file from a server across the internet as in example which requests the html from the o reilly home page and then displays it as if the user had surfed to the page itself the result will be similar to figure example grabbing the o reilly home page php echo http oreilly com figure the o reilly home page grabbed with uploading files uploading files to a web server is a subject that seems daunting to many people but it actually couldn t be much easier all you need to do to upload a file from a form is choose a special type of encoding called multipart form data and your browser will handle the rest to see how this works type the program in example and save it as upload php when you run it you ll see a form in your browser that lets you upload a file of your choice example image uploader upload php php upload php echo html head title php form upload title head body form method post action upload php enctype multipart form data select file input type file name filename size input type submit value upload form if name filename name filename name echo uploaded image name br img src name echo body html let examine this program a section at a time the first line of the multiline echo statement starts an html document displays the title and then starts the document body next we come to the form that selects the post method of form submission sets the target for posted data to the program upload php the program itself and tells the web browser that the data posted should be encoded via the content type of multipart form data with the form set up the next lines display the prompt select file and then request two inputs the first request is for a file it uses an input type of file a name of filename and an input field with a width of characters the second requested input is just a submit button that is given the label upload which replaces the default button text of submit query and then the form is closed this short program shows a common technique in web programming in which a single program is called twice once when the user first visits a page and again when the user presses the submit button the php code to receive the uploaded data is fairly simple because all uploaded files are placed into the associative system array therefore a quick check to see whether contains anything is sufficient to determine whether the user has up loaded a file this is done with the statement if the first time the user visits the page before uploading a file is empty so the program skips this block of code when the user uploads a file the program runs again and discovers an element in the array once the program realizes that a file was uploaded the actual name as read from the uploading computer is retrieved and placed into the variable name now all that nec essary is to move the file from the temporary location in which php stored the uploaded file to a more permanent one we do this using the function passing it the original name of the file with which it is saved to the current directory finally the uploaded image is displayed within an img tag and the result should look like figure if you run this program and then receive warning messages such as permission denied for the function call then you may not have the correct permissions set for the folder in which the program is running figure uploading an image as form data using five things are stored in the array when a file is uploaded as shown in table where file is the file upload field name supplied by the submitting form table the contents of the array file name the name of the uploaded file e g smiley jpg file type the content type of the file e g image jpeg file size the file size in bytes file the name of the temporary file stored on the server file error the error code resulting from the file upload content types used to be known as mime multipurpose internet mail extension types but because their use later expanded to the whole internet now they are often called internet media types table shows some of the more frequently used types that turn up in file type table some common internet media content types application pdf image gif multipart form data text xml application zip image jpeg text css video mpeg audio mpeg image png text html video audio x wav image tiff text plain video quicktime validation i hope it now goes without saying although i ll do so anyway that form data validation is of the utmost importance due to the possibility of users attempting to hack into your server in addition to maliciously formed input data some of the things you also have to check are whether a file was actually received and if so whether the right type of data was sent taking all this into account example php is a rewrite of upload php example a more secure version of upload php php php echo html head title php form upload title head body form method post action php enctype multipart form data select a jpg gif png or tif file input type file name filename size input type submit value upload form if name filename name switch filename type case image jpeg ext jpg break case image gif ext gif break case image png ext png break case image tiff ext tif break default ext break if ext n image ext filename n echo uploaded image name as n br echo img src n else echo name is not an accepted image file else echo no image has been uploaded echo body html the non html section of code has been expanded from the half dozen lines of example to more than lines starting at if as with the previous version this if line checks whether any data was actually posted but there is now a matching else near the bottom of the program that echoes a message to screen when nothing has been uploaded within the if statement the variable name is assigned the value of the filename as retrieved from the uploading computer just as before but this time we won t rely on the user having sent us valid data instead a switch statement is used to check the uploaded content type against the four types of image this program supports if a match is made the variable ext is set to the three letter file extension for that type should no match be found the file uploaded was not of an accepted type and the variable ext is set to the empty string the next section of code then checks the variable ext to see whether it contains a string and if so creates a new filename called n with the base name image and the extension stored in ext this means that the program is in full control over the name of the file to be created as it can be only one of image jpg image gif image png or image tif safe in the knowledge that the program has not been compromised the rest of the php code is much the same as in the previous version it moves the uploaded temporary image to its new location and then displays it while also displaying the old and new image names don t worry about having to delete the temporary file that php cre ates during the upload process because if the file has not been moved or renamed it will be automatically removed when the program exits after the if statement there is a matching else which is executed only if an unsupported image type was uploaded in which case it displays an appropriate error message when you write your own file uploading routines i strongly advise you to use a similar approach and have pre chosen names and locations for uploaded files that way no attempts to add pathnames and other malicious data to the variables you use can get through if this means that more than one user could end up having a file uploaded with the same name you could prefix such files with their user name or save them to individually created folders for each user but if you must use a supplied filename you should sanitize it by allowing only alpha numeric characters and the period which you can do with the following command using a regular expression see chapter to perform a search and replace on name name a za name this leaves only the characters a z a z and periods in the string name and strips out everything else even better to ensure that your program will work on all systems regardless of whether they are case sensitive or case insensitive you should probably use the following com mand instead which changes all uppercase characters to lowercase at the same time name strtolower a za name sometimes you may encounter the media type of image pjpeg which indicates a progressive jpeg but you can safely add this to your code as an alias of image jpeg like this case image pjpeg case image jpeg ext jpg break system calls sometimes php will not have the function you need to perform a certain action but the operating system it is running on may in such cases you can use the exec system call to do the job for example to quickly view the contents of the current directory you can use a program such as example if you are on a windows system it will run as is using the windows dir command on linux unix or mac os x comment out or remove the first line and uncomment the second to use the ls system command you may wish to type this program save it as exec php and call it up in your browser example executing a system command php exec php cmd dir windows cmd ls linux unix mac exec escapeshellcmd cmd output status if status echo exec command failed else echo pre foreach output as line echo htmlspecialchars line n echo pre the htmlspecialchars function is called to turn any special characters returned by the system into ones that html can understand and properly display neatening the output depending on the system you are using the result of running this program will look something like this from a windows dir command volume in drive c is hard disk volume serial number is directory of c program files zend htdocs dir dir chars php 08 favicon ico index html 02 info php 110 test htm 04 182 test php file bytes dir 448 bytes free exec takes three arguments the command itself in the previous case cmd an array in which the system will put the output from the command in the previous case output a variable to contain the returned status of the call in the previous case status if you wish you can omit the output and status parameters but you will not know the output created by the call or even whether it completed successfully you should also note the use of the escapeshellcmd function it is a good habit to always use this when issuing an exec call because it sanitizes the command string preventing the execution of arbitrary commands should you supply user input to the call the system calling functions are typically disabled on shared web hosts as they pose a security risk you should always try to solve your problems within php if you can and go to the system directly only if it is really necessary also going to the system is relatively slow and you need to code two implementations if your application is expected to run on both windows and linux unix systems xhtml or because xhtml documents need to be well formed you can parse them using standard xml parsers unlike html which requires a lenient html specific parser for this reason xhtml never really caught on and when the time came to devise a new stan dard the world wide web consortium chose to support rather than the newer standard has some of the features of both and xhtml but is much simpler to use and less strict to validate and happily there is now just a single document type you need to place at the head of an document instead of the variety of strict tran sitional and frameset types previously required namely doctype html just the simple word html is sufficient to tell the browser that your web page is designed for and because all the latest versions of the most popular browsers have been supporting most of the specification since or so this document type is generally the only one you need unless you choose to cater to older browsers for all intents and purposes when writing html documents web developers can safely ignore the old xhtml document types and syntax such as using br instead of the simpler br tag but if you find yourself having to cater to a very old browser or an unusual application that relies on xhtml then you can get more information on how to do that at http xhtml com questions which printf conversion specifier would you use to display a floating point number what printf statement could be used to take the input string happy birthday and output the string happy to send the output from printf to a variable instead of to a browser what alternative function would you use how would you create a unix timestamp for a m on may which file access mode would you use with fopen to open a file in write and read mode with the file truncated and the file pointer at the start what is the php command for deleting the file file txt which php function is used to read in an entire file in one go even from across the web which php superglobal variable holds the details on uploaded files which php function enables the running of system commands which of the following tag styles is preferred in hr or hr see chapter answers on page in appendix a for the answers to these questions chapter introduction to mysql with well over million installations mysql is probably the most popular database management system for web servers developed in the mid it now a mature technology that powers many of today most visited internet destinations one reason for its success must be the fact that like php it free to use but it also extremely powerful and exceptionally fast it can run on even the most basic of hard ware and it hardly puts a dent in system resources mysql is also highly scalable which means that it can grow with your website for the latest benchmarks see http mysql com why mysql benchmarks mysql basics a database is a structured collection of records or data stored in a computer system and organized in such a way that it can be quickly searched and information can be rapidly retrieved the sql in mysql stands for structured query language this language is loosely based on english and also used in other databases such as oracle and microsoft sql server it is designed to allow simple requests from a database via commands such as select title from publications where author charles dickens a mysql database contains one or more tables each of which contains records or rows within these rows are various columns or fields that contain the data itself table shows the contents of an example database of five publications detailing the author title type and year of publication table example of a simple database author title type year mark twain the adventures of tom sawyer fiction jane austen pride and prejudice fiction charles darwin the origin of species non fiction charles dickens the old curiosity shop fiction william shakespeare romeo and juliet play each row in the table is the same as a row in a mysql table and each element within a row is the same as a mysql field to uniquely identify this database i ll refer to it as the publications database in the examples that follow and as you will have observed all these publications are consid ered to be classics of literature so i ll call the table within the database that holds the details classics summary of database terms the main terms you need to acquaint yourself with for now are database the overall container for a collection of mysql data table a subcontainer within a database that stores the actual data row a single record within a table which may contain several fields column the name of a field within a row i should note that i m not trying to reproduce the precise terminology used in academic literature about relational databases but just to provide simple everyday terms to help you quickly grasp basic concepts and get started with a database accessing mysql via the command line there are three main ways in which you can interact with mysql using a command line via a web interface such as phpmyadmin and through a programming language like php we ll start doing the third of these in chapter but for now let look at the first two starting the command line interface the following sections describe relevant instructions for windows os x and linux windows users if you installed the zend server free edition wamp as explained in chapter you will be able to access the mysql executable from one of the following directories the first on bit computers and the second on bit machines c program files zend bin c program files zend bin if you installed zend server in a place other than program files or program files you will need to use that directory instead by default the initial mysql user will be root and will not have had a password set seeing as this is a development server that only you should be able to access we won t worry about creating one yet so to enter mysql command line interface select start run enter cmd into the run box and press return this will call up a windows command prompt from there enter one of the following making any appropriate changes as just discussed c program files zend bin mysql u root c program files zend bin mysql u root note the quotation marks surrounding the path and filename these are present because the name contains spaces which the command prompt doesn t correctly interpret and the quotation marks group the parts of the filename into a single string for the command pro gram to understand this command tells mysql to log you in as user root without a password you will now be logged into mysql and can start entering commands so to be sure everything is working as it should be enter the following the results should look similar to the output shown in figure show databases figure accessing mysql from a windows command prompt if this has not worked and you get an error make sure that you have correctly installed mysql along with zend server as described in chapter otherwise you are ready to move on to the next section using the command line interface on page os x users to proceed with this chapter you should have installed zend server as detailed in chapter you should also have the web server already running and the mysql server started to enter the mysql command line interface start the terminal program which should be available in finder utilities then call up the mysql program which will have been installed in the directory usr local zend mysql bin by default the initial mysql user is root and it will have a password of root too so to start the program type the following usr local zend mysql bin mysql u root this command tells mysql to log you in as user root and not to request your password to verify that all is well type the following the result should look like the output shown in figure show databases figure accessing mysql from the os x terminal program if you receive an error such as can t connect to local mysql server through socket you haven t started up the mysql server so make sure you followed the advice in chapter about configuring mysql to start when os x starts you should now be ready to move on to the next section using the command line interface on page linux users on a system running a unix like operating system such as linux you will almost cer tainly already have php and mysql installed and running and you will be able to enter the examples in the next section but first you should type the following to log into your mysql system mysql u root p this tells mysql to log you in as the user root and to request your password if you have a password enter it otherwise just press return once you are logged in type the following to test the program you should see something like figure in response show databases figure accessing mysql using linux if this procedure fails at any point refer to the section installing a lamp on linux on page in chapter to ensure that you have mysql properly installed otherwise you should now be ready to move on to the next section using the command line interface on page mysql on a remote server if you are accessing mysql on a remote server you should telnet or preferably for security use ssh into the remote machine which will probably be a linux freebsd unix type of box once in there you might find that things are a little different de pending on how the system administrator has set the server up especially if it a shared hosting server therefore you need to ensure that you have been given access to mysql and that you have your username and password armed with these you can then type the following where username is the name supplied mysql u username p enter your password when prompted you can then try the following command which should result in something like figure show databases there may be other databases already created and the test database may not be there bear in mind also that system administrators have ultimate control over everything and that you can encounter some unexpected setups for example you may find that you are required to preface all database names that you create with a unique identifying string to ensure that you do not conflict with databases created by other users therefore if you have any problems talk with your system administrator who will get you sorted out just let the sysadmin know that you need a username and password you should also ask for the ability to create new databases or at a minimum to have at least one database created for you ready to use you can then create all the tables you require within that database using the command line interface from here on out it makes no difference whether you are using windows mac os x or linux to access mysql directly as all the commands used and errors you may receive are identical the semicolon let start with the basics did you notice the semicolon at the end of the show databases command that you typed the semicolon is used by mysql to separate or end commands if you forget to enter it mysql will issue a prompt and wait for you to do so the required semicolon was made part of the syntax to let you enter multiple line commands which can be convenient because some commands get quite long it also allows you to issue more than one command at a time by placing a semicolon after each one the interpreter gets them all in a batch when you press the enter or return key and executes them in order it very common to receive a mysql prompt instead of the results of your command it means that you forgot the final semicolon just enter the semicolon and press the enter key to get what you want there are six different prompts that mysql may present you with see table so you will always know where you are during a multiline input table mysql six command prompts mysql prompt meaning mysql ready and waiting for a command waiting for the next line of a command waiting for the next line of a string started with a single quote waiting for the next line of a string started with a double quote waiting for the next line of a string started with a backtick waiting for the next line of a comment started with canceling a command if you are partway through entering a command and decide you don t wish to execute it after all whatever you do don t press control c that will close the program instead you can enter c and press return example shows how to use it example canceling a line of input meaningless gibberish to mysql c when you enter that line mysql will ignore everything you typed and issue a new prompt without the c it would have displayed an error message be careful though if you have opened a string or comment close it first before using the c or mysql will think the c is just part of the string example shows the right way to do this example canceling input from inside a string this is meaningless gibberish to mysql c also note that using c after a semicolon will not work as it is then a new statement mysql commands you ve already seen the show command which lists tables databases and many other items the commands you ll probably use most often are listed in table table a selection of common mysql commands alter alter a database or table backup backup a table c cancel input create create a database delete delete a row from a table describe describe a table columns drop delete a database or table exit ctrl c exit grant change user privileges help h display help insert insert data lock lock table quit q same as exit rename rename a table show list details about an object source execute a file status display the current status truncate empty a table unlock unlock table update update an existing record use use a database i ll cover most of these as we proceed but first you need to remember a couple of points about mysql commands sql commands and keywords are case insensitive create create and create all mean the same thing however for the sake of clarity the recommended style is to use uppercase table names are case sensitive on linux and os x but case insensitive on win dows so for portability purposes you should always choose a case and stick to it the recommended style is to use lowercase for tables creating a database if you are working on a remote server and have only a single user account and access to a single database that was created for you move on to the section creating a table on page otherwise get the ball rolling by issuing the following command to create a new database called publications create database publications a successful command will return a message that doesn t mean much yet query ok row affected sec but will make sense soon now that you ve created the database you want to work with it so issue use publications you should now see the message database changed and will then be set to proceed with the following examples creating users now that you ve seen how easy it is to use mysql and created your first database it time to look at how you create users as you probably won t want to grant your php scripts root access to mysql it could cause a real headache should you get hacked to create a user issue the grant command which takes the following form don t type this in it not an actual working command grant privileges on database object to username hostname identified by password all this should be pretty straightforward with the possible exception of the data base object part which refers to the database itself and the objects it contains such as tables see table table example parameters for the grant command all databases and all their objects database only the database called database and all its objects database object only the database called database and its object called object so let create a user who can access just the new publications database and all its objects by entering the following replacing the username jim and the password mypasswd with ones of your choosing grant all on publications to jim localhost identified by mypasswd what this does is allow the user jim localhost full access to the publications database using the password mypasswd you can test whether this step has worked by entering quit to exit and then rerunning mysql the way you did before but instead of entering u root p type u jim p or whatever username you created see table for the correct command for your operating system modify it as necessary if the mysql client program is installed in a different directory on your system table starting mysql and logging in as jim localhost windows c program files zend bin mysql u jim p mac os x applications mamp library bin mysql u jim p linux mysql u jim p all you have to do now is enter your password when prompted and you will be logged in by the way if you prefer you can place your password immediately following the p without any spaces to avoid having to enter it when prompted but this is considered a poor practice because if other people are logged into your system there may be ways for them to look at the command you entered and find out your password you can grant only privileges that you already have and you must also have the privilege to issue grant commands there is a whole range of privileges you can choose to grant if you are not granting all priv ileges for further details visit http tinyurl com mysqlgrant which also covers the revoke command which can remove privileges once granted also be aware that if you create a new user but do not specify an identified by clause the user will have no password a situation that is very insecure and should be avoided creating a table at this point you should now be logged into mysql with all privileges granted for the database publications or a database that was created for you so you re ready to create your first table make sure the correct database is in use by typing the following re placing publications with the name of your database if it is different use publications now enter the commands in example one line at a time example creating a table called classics create table classics author varchar title varchar type varchar year char engine myisam you could also issue this command on a single line like this create table classics author varchar title varchar type varchar year char engine myisam but mysql commands can be long and complicated so i recom mend one line per instruction until you are comfortable with longer lines mysql should then issue the response query ok rows affected along with how long it took to execute the command if you see an error message instead check your syntax carefully every parenthesis and comma counts and typing errors are easy to make in case you are wondering the engine myisam tells mysql the type of database engine to use for this table to check whether your new table has been created type describe classics all being well you will see the sequence of commands and responses shown in example where you should particularly note the table format displayed example a mysql session creating and checking a new table mysql use publications database changed mysql create table classics author varchar title varchar type varchar year char engine myisam query ok rows affected sec mysql describe classics field type null key default extra author varchar yes null title varchar yes null type varchar yes null year char yes null rows in set sec the describe command is an invaluable debugging aid when you need to ensure that you have correctly created a mysql table you can also use it to remind yourself about a table field or column names and the types of data in each one let look at each of the headings in detail field the name of each field or column within a table type the type of data being stored in the field null whether a field is allowed to contain a value of null key mysql supports keys or indexes which are quick ways to look up and search for data the key heading shows what type of key if any has been applied default the default value that will be assigned to the field if no value is specified when a new row is created extra additional information such as whether a field is set to auto increment data types in example you may have noticed that three of the table fields were given the data type of varchar and one was given the type char the term varchar stands for variable length character string and the command takes a numeric value that tells mysql the maximum length allowed for a string stored in this field this data type is very useful as mysql can then plan the size of databases and perform lookups and searches more easily the downside is that if you ever attempt to assign a string value longer than the length allowed it will be truncated to the maximum length declared in the table definition the year field however has more predictable values so instead of varchar we use the more efficient char data type the parameter of allows for four bytes of data supporting all years from to a byte comprises bits and can have the values through which are to in decimal you could of course just store two digit values for the year but if your data is going to still be needed in the following century or may otherwise wrap around it will have to be sanitized first much like the millennium bug that would have caused dates be ginning on january to be treated as on many of the world biggest com puter installations the reason i didn t use the year data type in the classics table is because it supports only the year and years through this is because mysql stores the year in a single byte for reasons of effi ciency but it also means that only years are available and the publication years of the titles in the classics table are well before this both char and varchar accept text strings and impose a limit on the size of the field the difference is that every string in a char field has the specified size if you put in a smaller string it is padded with spaces a varchar field does not pad the text it lets the size of the field vary to fit the text that is inserted but varchar requires a small amount of overhead to keep track of the size of each value so char is slightly more efficient if the sizes are similar in all records whereas varchar is more efficient if sizes can vary a lot and get large in addition the overhead causes access to varchar data to be slightly slower than to char data the char data type table lists the char data types all these types offer a parameter that sets the maxi mum or exact length of the string allowed in the field as the table shows each type has a built in maximum number of bytes it can occupy table mysql char data types char n exactly n char hello uses bytes char goodbye uses bytes varchar n up to n varchar morning uses bytes varchar night uses bytes the binary data type the binary data type is used for storing strings of full bytes that do not have an associated character set for example you might use the binary data type to store a gif image see table table mysql binary data types binary n or byte n exactly n as char but contains binary data varbinary n up to n as varchar but for binary data the text and varchar data types the differences between text and varchar are small prior to version mysql would remove leading and trailing spaces from varchar fields text fields cannot have default values mysql indexes only the first n characters of a text column you specify n when you create the index what this means is that varchar is the better and faster data type to use if you need to search the entire contents of a field if you will never search more than a certain number of leading characters in a field you should probably use a text data type see table table mysql text data types tinytext n up to n treated as a string with a character set text n up to n treated as a string with a character set mediumtext n up to n treated as a string with a character set longtext n up to n treated as a string with a character set the blob data type the term blob stands for binary large object and therefore as you would think the blob data type is most useful for binary data in excess of bytes in size the main other difference between the blob and binary data types is that blobs cannot have default values see table table mysql blob data types tinyblob n up to n treated as binary data no character set blob n up to n treated as binary data no character set mediumblob n up to n treated as binary data no character set longblob n up to n treated as binary data no character set numeric data types mysql supports various numeric data types from a single byte up to double precision floating point numbers although the most memory that a numeric field can use up is bytes you are well advised to choose the smallest data type that will adequately handle the largest value you expect your databases will be small and quickly accessible table lists the numeric data types supported by mysql and the ranges of values they can contain in case you are not acquainted with the terms a signed number is one with a possible range from a minus value through to a positive one and an unsigned one has a value ranging from to a positive one they can both hold the same number of values just picture a signed number as being shifted halfway to the left so that half its values are negative and half are positive note that floating point values of any precision may only be signed table mysql numeric data types tinyint smallint mediumint int or integer bigint float n a 38 n a double or real n a 308 n a to specify whether a data type is signed or unsigned use the unsigned qualifier the following example creates a table called tablename with a field in it called fieldname of the data type unsigned integer create table tablename fieldname int unsigned when creating a numeric field you can also pass an optional number as a parameter like this create table tablename fieldname int but you must remember that unlike binary and char data types this parameter does not indicate the number of bytes of storage to use it may seem counterintuitive but what the number actually represents is the display width of the data in the field when it is retrieved it is commonly used with the zerofill qualifier like this create table tablename fieldname int zerofill what this does is cause any numbers with a width of less than four characters to be padded with one or more zeros sufficient to make the display width of the field four characters long when a field is already of the specified width or greater no padding takes place date and time the main remaining data types supported by mysql relate to the date and time and can be seen in table table mysql date and time data types data type time date format datetime date timestamp time year only years and the datetime and timestamp data types display the same way the main difference is that timestamp has a very narrow range from the years through whereas datetime will hold just about any date you re likely to specify unless you re interested in ancient history or science fiction timestamp is useful however because you can let mysql set the value for you if you don t specify the value when adding a row the current time is automatically inserted you can also have mysql update a timestamp column each time you change a row the data type sometimes you need to ensure that every row in your database is guaranteed to be unique you could do this in your program by carefully checking the data you enter and making sure that there is at least one value that differs in any two rows but this approach is error prone and works only in certain circumstances in the classics table for instance an author may appear multiple times likewise the year of publication will also be frequently duplicated and so on it would be hard to guarantee that you have no du plicate rows the general solution is to use an extra column just for this purpose in a while we ll look at using a publication isbn international standard book number but first i d like to introduce the data type as its name implies a column given this data type will set the value of its contents to that of the column entry in the previously inserted row plus example shows how to add a new column called id to the table classics with auto incrementing example adding the auto incrementing column id alter table classics add id int unsigned not null key this is your introduction to the alter command which is very similar to the create command alter operates on an existing table and can add change or delete columns our example adds a column named id with the following characteristics int unsigned makes the column take an integer large enough for you to store more than billion records in the table not null ensures that every column has a value many programmers use null in a field to indicate that the field doesn t have any value but that would allow duplicates which would violate the whole reason for this column existence so we disallow null values causes mysql to set a unique value for this column in every row as described earlier we don t really have control over the value that this column will take in each row but we don t care all we care about is that we are guaranteed a unique value key an auto increment column is useful as a key because you will tend to search for rows based on this column as explained in the section indexes on page each entry in the column id will now have a unique number with the first starting at and the others counting upward from there and whenever a new row is inserted its id column will automatically be given the next number in sequence rather than applying the column retroactively you could have included it by issuing the create command in slightly different format in that case the command in example would be replaced with example check the final line in particular example adding the auto incrementing id column at table creation create table classics author varchar title varchar type varchar year char id int unsigned not null key engine myisam if you wish to check whether the column has been added use the following command to view the table columns and data types describe classics now that we ve finished with it the id column is no longer needed so if you created it using example you should now remove the column using the command in example example removing the id column alter table classics drop id adding data to a table to add data to a table use the insert command let see this in action by populating the table classics with the data from table using one form of the insert command repeatedly example example populating the classics table insert into classics author title type year values mark twain the adventures of tom sawyer fiction insert into classics author title type year values jane austen pride and prejudice fiction insert into classics author title type year values charles darwin the origin of species non fiction insert into classics author title type year values charles dickens the old curiosity shop fiction insert into classics author title type year values william shakespeare romeo and juliet play after every second line you should see a query ok message once all lines have been entered type the following command which will display the table contents the result should look like figure select from classics don t worry about the select command for now we ll come to it in the section querying a mysql database on page suffice it to say that as typed it will display all the data you just entered figure populating the classics table and viewing its contents let go back and look at how we used the insert command the first part insert into classics tells mysql where to insert the following data then within parentheses the four column names are listed author title type and year all separated by com mas this tells mysql that these are the fields into which the data is to be inserted the second line of each insert command contains the keyword values followed by four strings within parentheses and separated by commas this supplies mysql with the four values to be inserted into the four columns previously specified as always my choice of where to break the lines was arbitrary each item of data will be inserted into the corresponding column in a one to one cor respondence if you accidentally listed the columns in a different order from the data the data would go into the wrong columns and the number of columns must match the number of data items renaming a table renaming a table like any other change to the structure or meta information about a table is achieved via the alter command so for example to change the name of table classics to use the following command alter table classics rename if you tried that command you should revert the table name by entering the following so that later examples in this chapter will work as printed alter table rename classics changing the data type of a column changing a column data type also makes use of the alter command this time in conjunction with the modify keyword so to change the data type of column year from char to smallint which requires only two bytes of storage and so will save disk space enter the following alter table classics modify year smallint when you do this if the conversion of data type makes sense to mysql it will auto matically change the data while keeping the meaning in this case it will change each string to a comparable integer and so on as the string is recognizable as referring to an integer adding a new column let suppose that you have created a table and populated it with plenty of data only to discover you need an additional column not to worry here how to add the new column pages which will be used to store the number of pages in a publication alter table classics add pages smallint unsigned this adds the new column with the name pages using the unsigned smallint data type sufficient to hold a value of up to 535 hopefully that more than enough for any book ever published and if you ask mysql to describe the updated table using the describe command as follows you will see the change has been made see figure describe classics figure adding the new pages column and viewing the table renaming a column looking again at figure you may decide that having a column named type can be confusing because that is the name used by mysql to identify data types again no problem let change its name to category like this alter table classics change type category varchar note the addition of varchar on the end of this command that because the change keyword requires the data type to be specified even if you don t intend to change it and varchar was the data type specified when that column was initially created as type removing a column actually upon reflection you might decide that the page count column pages isn t ac tually all that useful for this particular database so here how to remove that column using the drop keyword alter table classics drop pages remember that drop is irreversible and you should always use it with caution because you could inadvertently delete entire tables and even databases with it if you are not careful deleting a table deleting a table is very easy indeed but because i don t want you to have to reenter all the data for the classics table let quickly create a new table verify its existence and then delete it by typing the commands in example the result of these four com mands should look like figure example creating viewing and deleting a table create table disposable trash int describe disposable drop table disposable show tables figure creating viewing and deleting a table indexes as things stand the table classics works and can be searched without problem by mysql until it grows to more than a couple of hundred rows that is at that point database accesses will get slower and slower with every new row added because mysql has to search through every row whenever a query is issued this is like searching through every book in a library whenever you need to look something up of course you don t have to search libraries that way because they have either a card index system or most likely a database of their own and the same goes for mysql because at the expense of a slight overhead in memory and disk space you can create a card index for a table that mysql will use to conduct lightning fast searches creating an index the way to achieve fast searches is to add an index either when creating a table or at any time afterward but the decision is not so simple for example there are different index types such as a regular index primary key and fulltext also you must decide which columns require an index a judgment that requires you to predict whether you will be searching any of the data in that column indexes can also get complicated because you can combine multiple columns in one index and even when you ve decided that you still have the option of reducing index size by limiting the amount of each column to be indexed if we imagine the searches that may be made on the classics table it becomes apparent that all of the columns may need to be searched however if the pages column created in the section adding a new column on page had not been deleted it would prob ably not have needed an index as most people would be unlikely to search for books by the number of pages they have anyway go ahead and add an index to each of the columns using the commands in example example adding indexes to the classics table alter table classics add index author alter table classics add index title alter table classics add index category alter table classics add index year describe classics the first two commands create indexes on both the author and title columns limiting each index to only the first characters for instance when mysql indexes the fol lowing title the adventures of tom sawyer it will actually store in the index only the first characters the adventures of to this is done to minimize the size of the index and to optimize database access speed i chose because it likely to be sufficient to ensure uniqueness for most strings in these columns if mysql finds two indexes with the same contents it will have to waste time going to the table itself and checking the column that was indexed to find out which rows really matched with the category column currently only the first character is required to identify a string as unique f for fiction n for non fiction and p for play but i chose an index of four characters to allow for future category types that may be unique only after four characters you can also re index this column later when you have a more complete set of categories and finally i set no limit to the year column index because it an integer not a string the results of issuing these commands and a describe command to confirm that they worked can be seen in figure which shows the key mul for each column this key means that multiple occurrences of a value may occur within that column which is exactly what we want as authors may appear many times the same book title could be used by multiple authors and so on figure adding indexes to the classics table using create index an alternative to using alter table to add an index is to use the create index com mand they are equivalent except that create index cannot be used to create a primary key see the section primary keys on page the format of this command is shown in the second line of example example these two commands are equivalent alter table classics add index author create index author on classics author adding indexes when creating tables you don t have to wait until after creating a table to add indexes in fact doing so can be time consuming as adding an index to a large table can take a very long time there fore let look at a command that creates the table classics with indexes already in place example is a reworking of example in which the indexes are created at the same time as the table note that to incorporate the modifications made in this chapter this version uses the new column name category instead of type and sets the data type of year to smallint instead of char if you want to try it out without first deleting your current classics table change the word classics in line to something else like then drop after you have finished with it example creating the table classics with indexes create table classics author varchar title varchar category varchar year smallint index author index title index category index year engine myisam primary keys so far you ve created the table classics and ensured that mysql can search it quickly by adding indexes but there still something missing all the publications in the table can be searched but there is no single unique key for each publication to enable instant accessing of a row the importance of having a key with a unique value for each row will come up when we start to combine data from different tables the section the data type on page briefly introduced the idea of a primary key when creating the auto incrementing column id which could have been used as a primary key for this table however i wanted to reserve that task for a more appropriate column the internationally recognized isbn number so let go ahead and create a new column for this key now bearing in mind that isbns are characters long you might think that the following command would do the job alter table classics add isbn char primary key but it doesn t if you try it you ll get the error duplicate entry for key the reason is that the table is already populated with some data and this command is trying to add a column with the value null to each row which is not allowed as all values must be unique in any column having a primary key index however if there were no data already in the table this command would work just fine as would adding the primary key index upon table creation in our current situation we have to be a bit sneaky and create the new column without an index populate it with data and then add the index retrospectively using the com mands in example luckily each of the years is unique in the current set of data so we can use the year column to identify each row for updating note that this example uses the update and where keywords which are explained in more detail in the section querying a mysql database on page example populating the isbn column with data and using a primary key alter table classics add isbn char update classics set isbn where year update classics set isbn where year update classics set isbn where year update classics set isbn where year update classics set isbn where year alter table classics add primary key isbn describe classics once you have typed these commands the results should look like figure note that the keywords primary key replace the keyword index in the alter table syntax compare examples and figure retrospectively adding a primary key to the classics table to have created a primary key when the table classics was created you could have used the commands in example again rename classics in line to something else if you wish to try this example for yourself and then delete the test table afterward example creating the table classics with a primary key create table classics author varchar title varchar category varchar year smallint isbn char index author index title index category index year primary key isbn engine myisam creating a fulltext index unlike a regular index mysql fulltext allows super fast searches of entire columns of text it stores every word in every data string in a special index that you can search using natural language in a similar manner to using a search engine actually it not strictly true that mysql stores all the words in a fulltext index because it has a built in list of more than words that it chooses to ignore because they are so common that they aren t very helpful for searching anyway this list called stopwords in cludes the as is of and so on the list helps mysql run much more quickly when performing a fulltext search and keeps database sizes down appendix c contains the full list of stopwords here are some things that you should know about fulltext indexes fulltext indexes can be used only with myisam tables the type used by mysql default storage engine mysql supports at least different storage engines if you need to convert a table to myisam you can usually use the mysql command alter table tablename engine myisam fulltext indexes can be created for char varchar and text columns only a fulltext index definition can be given in the create table statement when a table is created or added later using alter table or create index for large data sets it is much faster to load your data into a table that has no fulltext index and then create the index than to load data into a table that has an existing fulltext index to create a fulltext index apply it to one or more records as in example which adds a fulltext index to the pair of columns author and title in the table classics this index is in addition to the ones already created and does not affect them example adding a fulltext index to the table classics alter table classics add fulltext author title you can now perform fulltext searches across this pair of columns this feature could really come into its own if you could now add the entire text of these publications to the database particularly as they re out of copyright protection and they would be fully searchable see the section match against on page for a description of searches using fulltext if you find that mysql is running slower than you think it should be when accessing your database the problem is usually related to your indexes either you don t have an index where you need one or the indexes are not optimally designed tweaking a table indexes will often solve such a problem performance is beyond the scope of this book but in chapter i give you a few tips so you know what to look for querying a mysql database so far we ve created a mysql database and tables populated them with data and added indexes to make them fast to search now it time to look at how these searches are performed and the various commands and qualifiers available select as you saw in figure the select command is used to extract data from a table in that section i used its simplest form to select all data and display it something you will never want to do on anything but the smallest tables because all the data will scroll by at an unreadable pace let now examine select in more detail the basic syntax is select something from tablename the something can be an asterisk as you saw before which means every column or you can choose to select only certain columns for instance example shows how to select just the author and title and just the title and isbn the result of typing these commands can be seen in figure example two different select statements select author title from classics select title isbn from classics figure the output from two different select statements select count another replacement for the something parameter is count which can be used in many ways in example it displays the number of rows in the table by passing as a parameter which means all rows as you d expect the result returned is as there are five publications in the table example counting rows select count from classics select distinct this qualifier and its synonym distinctrow allows you to weed out multiple entries when they contain the same data for instance suppose that you want a list of all authors in the table if you select just the author column from a table containing multiple books by the same author you ll normally see a long list with same author names over and over but by adding the distinct keyword you can show each author just once so let test that out by adding another row that repeats one of our existing authors example example duplicating data insert into classics author title category year isbn values charles dickens little dorrit fiction now that charles dickens appears twice in the table we can compare the results of using select with and without the distinct qualifier example and figure show that the simple select lists dickens twice and the command with the distinct qualifier shows him only once example with and without the distinct qualifier select author from classics select distinct author from classics figure selecting data with and without distinct delete when you need to remove a row from a table use the delete command its syntax is similar to the select command and allows you to narrow down the exact row or rows to delete using qualifiers such as where and limit now that you ve seen the effects of the distinct qualifier if you entered example you should remove little dorrit by entering the commands in example example removing the new entry delete from classics where title little dorrit this example issues a delete command for all rows whose title column contains the string little dorrit the where keyword is very powerful and important to enter correctly an error could lead a command to the wrong rows or have no effect in cases where nothing matches the where clause so now we ll spend some time on that clause which is the heart and soul of sql where the where keyword enables you to narrow down queries by returning only those where a certain expression is true example returns only the rows where the column exactly matches the string little dorrit using the equality operator example shows a couple more examples of using where with example using the where keyword select author title from classics where author mark twain select author title from classics where isbn given our current table the two commands in example display the same results but we could easily add more books by mark twain in which case the first line would display all titles he wrote and the second line would continue because we know the isbn is unique to display the adventures of tom sawyer in other words searches using a unique key are more predictable and you ll see further evidence later of the value of unique and primary keys you can also do pattern matching for your searches using the like qualifier which allows searches on parts of strings this qualifier should be used with a character before or after some text when placed before a keyword means anything before and after a keyword it means anything after example performs three different queries one for the start of a string one for the end and one for anywhere in a string you can see the results of these commands in figure example using the like qualifier select author title from classics where author like charles select author title from classics where title like species select author title from classics where title like and figure using where with the like qualifier the first command outputs the publications by both charles darwin and charles dickens because the like qualifier was set to return anything matching the string charles followed by any other text then just the origin of species is returned because it the only row whose column ends with the string species last both pride and prejudice and romeo and juliet are returned because they both matched the string and anywhere in the column the will also match if there is nothing in the position it occupies in other words it can match an empty string limit the limit qualifier enables you to choose how many rows to return in a query and where in the table to start returning them when passed a single parameter it tells mysql to start at the beginning of the results and just return the number of rows given in that parameter if you pass it two parameters the first indicates the offset from the start of the results where mysql should start the display and the second indicates how many to return you can think of the first parameter as saying skip this number of results at the start example includes three commands the first returns the first three rows from the table the second returns two rows starting at position skipping the first row the last command returns a single row starting at position skipping the first three rows figure shows the results of issuing these three commands example limiting the number of results returned select author title from classics limit select author title from classics limit select author title from classics limit figure restricting the rows returned with limit be careful with the limit keyword because offsets start at but the number of rows to return starts at so limit means return three rows starting from the second row match against the match against construct can be used on columns that have been given a fulltext index see the section creating a fulltext index on page with it you can make natural language searches as you would in an internet search engine unlike the use of where or where like match against lets you enter multiple words in a search query and checks them against all words in the fulltext columns fulltext indexes are case insensitive so it makes no difference what case is used in your queries assuming that you have added a fulltext index to the author and title columns enter the three queries shown in example the first asks for any of these columns that contain the word and to be returned because and is a stopword mysql will ignore it and the query will always produce an empty set no matter what is stored in the columns the second query asks for any rows that contain both of the words old and shop anywhere in them in any order to be returned and the last query applies the same kind of search for the words tom and sawyer figure shows the results of these queries example using match against on fulltext indexes select author title from classics where match author title against and select author title from classics where match author title against old shop select author title from classics where match author title against tom sawyer figure using match against on a fulltext index match against in boolean mode if you wish to give your match against queries even more power use boolean mode this changes the effect of the standard fulltext query so that it searches for any combination of search words instead of requiring all search words to be in the text the presence of a single word in a column causes the search to return the row boolean mode also allows you to preface search words with a or sign to indicate whether they must be included or excluded if normal boolean mode says any of these words will do a plus sign means this word must be present otherwise don t return the row a minus sign means this word must not be present its presence disqualifies the row from being returned example illustrates boolean mode through two queries the first asks for all rows containing the word charles and not the word species to be returned the second uses double quotes to request that all rows containing the exact phrase origin of be returned figure shows the results of these queries example using match against in boolean mode select author title from classics where match author title against charles species in boolean mode select author title from classics where match author title against origin of in boolean mode figure using match against in boolean mode as you would expect the first request returns only the old curiosity shop by charles dickens because any rows containing the word species have been excluded so charles darwin publication is ignored there is something of interest to note in the second query the stop word of is part of the search string but is still used by the search because the double quotation marks override stopwords update set this construct allows you to update the contents of a field if you wish to change the contents of one or more fields you need to first narrow in on just the field or fields to be changed in much the same way you use the select command example shows the use of update set in two different ways you can see the results in figure 15 example using update set update classics set author mark twain samuel langhorne clemens where author mark twain update classics set category classic fiction where category fiction figure 15 updating columns in the classics table in the first query mark twain real name of samuel langhorne clemens was appended to his pen name in brackets which affected only one row the second query however affected three rows because it changed all occurrences of the word fiction in the cate gory column to the term classic fiction when performing an update you can also make use of the qualifiers you have already seen such as limit and the following order by and group by keywords order by order by sorts returned results by one or more columns in ascending or descending order example shows two such queries the results of which can be seen in figure example using order by select author title from classics order by author select author title from classics order by title desc figure sorting the results of requests as you can see the first query returns the publications by author in ascending alpha betical order the default and the second returns them by title in descending order if you wanted to sort all the rows by author and then by descending year of publication to view the most recent first you would issue the following query select author title year from classics order by author year desc this shows that each ascending and descending qualifier applies to a single column the desc keyword applies only to the preceding column year because you allow au thor to use the default sort order it is sorted in ascending order you could also have explicitly specified ascending order for that column with the same results select author title year from classics order by author asc year desc group by in a similar fashion to order by you can group results returned from queries using group by which is good for retrieving information about a group of data for example if you want to know how many publications there are of each category in the classics table you can issue the following query select category count author from classics group by category which returns the following output category count author classic fiction non fiction play rows in set sec joining tables together it is quite normal to maintain multiple tables within a database each holding a different type of information for example consider the case of a customers table that needs to be able to be cross referenced with publications purchased from the classics table enter the commands in example to create this new table and populate it with three customers and their purchases figure shows the result example creating and populating the customers table create table customers name varchar isbn varchar primary key isbn engine myisam insert into customers name isbn values joe bloggs insert into customers name isbn values mary smith insert into customers name isbn values jack wilson select from customers figure creating the customers table there also a shortcut for inserting multiple rows of data as in ex ample in which you can replace the three separate insert into queries with a single one listing the data to be inserted separated by commas like this insert into customers name isbn values joe bloggs mary smith jack wilson of course in a proper table containing customers details there would also be addresses phone numbers email addresses and so on but they aren t necessary for this explan ation while creating the new table you should have noticed that it has something in common with the classics table a column called isbn because it has the same meaning in both tables an isbn refers to a book and always the same book we can use this column to tie the two tables together into a single query as in example example joining two tables into a single select select name author title from customers classics where customers isbn classics isbn the result of this operation is the following name author title joe bloggs charles dickens the old curiosity shop mary smith jane austen pride and prejudice jack wilson charles darwin the origin of species rows in set sec see how this query has neatly tied both tables together to show the publications pur chased from the classics table by the people in the customers table natural join using natural join you can save yourself some typing and make queries a little clearer this kind of join takes two tables and automatically joins columns that have the same name so to achieve the same results as from example you would enter select name author title from customers natural join classics join on if you wish to specify the column on which to join two tables use the join on construct as follows to achieve results identical to those of example select name author title from customers join classics on customers isbn classics isbn using as you can also save yourself some typing and improve query readability by creating aliases using the as keyword follow a table name with as and the alias to use the following code therefore is also identical in action to example aliases can be particularly useful when you have long queries that reference the same table names many times select name author title from customers as cust classics as class where cust isbn class isbn the result of this operation is the following name author title joe bloggs charles dickens the old curiosity shop mary smith jane austen pride and prejudice jack wilson charles darwin the origin of species rows in set sec using logical operators you can also use the logical operators and or and not in your mysql where queries to further narrow down your selections example shows one instance of each but you can mix and match them in any way you need example using logical operators select author title from classics where author like charles and author like darwin select author title from classics where author like mark twain or author like samuel langhorne clemens select author title from classics where author like charles and author not like darwin i ve chosen the first query because charles darwin might be listed in some rows by his full name charles robert darwin thus the query returns publications as long as the author column starts with charles and ends with darwin the second query searches for publications written using either mark twain pen name or his real name samuel langhorne clemens the third query returns publications written by authors with the first name charles but not the surname darwin mysql functions you might wonder why anyone would want to use mysql functions when php comes with a whole bunch of powerful functions of its own the answer is very simple the mysql functions work on the data right there in the database if you were to use php you would first have to extract raw data from mysql manipulate it and then perform the database query you first wanted having functions built into mysql substantially reduces the time needed for perform ing complex queries as well as their complexity if you wish to learn more about the available string and date time functions you can visit the following urls http tinyurl com mysqlstrings http tinyurl com mysqldates however to get you started appendix d describes a subset containing the most useful of these functions accessing mysql via phpmyadmin although to use mysql you have to learn these main commands and how they work once you understand them it can be much quicker and simpler to use a program such as phpmyadmin to manage your databases and tables however you will need to install phpmyadmin before you can use it to do this call up the zend ui by entering the following into your browser and log in as shown in figure http localhost zendserver figure 18 the zend dashboard now click on the left and right arrows to the right of the deploy sample apps section until you see the phpmyadmin logo and click it to initiate the download then click next when you re finished click next again after you have viewed the readme information to call up the application details screen see figure figure configuring phpmyadmin for zend here you should probably accept the defaults for display name and virtual host but will need to specify a directory name for phpmyadmin in order to keep it away from your document root files i have entered the name phpmyadmin all in lowercase so that i won t have to enter any capital letters whenever i type the url to call it up continue clicking next and accepting any license agreements until you get to the screen in figure here you should select the use http apache basic authentication checkbox and supply a login and password the default login is dbadmin but i have chosen to use simply admin your login and password are up to you unless you have configured them differently you can probably leave the ip port database user and password as displayed figure entering phpmyadmin user parameters now click next review the summary displayed and when ready click the deploy but ton after a few seconds you should see that the application was successfully deployed at which point you ll be ready to access phpmyadmin by entering the following into your browser http localhost phpmyadmin this will bring up the dialog shown in figure where you should enter your user name and password before clicking the log in button figure logging into phpmyadmin your browser should now look like figure and you re ready to use phpmyadmin in place of the mysql command line figure the phpmyadmin main screen full details on this installation process are on the zend website at the following shortened url http tinyurl com installpma using phpmyadmin in the lefthand pane of the main phpmyadmin screen you can click on the drop down menu that says databases to select any database you wish to work with this will open the database and display its tables from here you can perform all the main operations such as creating new databases adding tables creating indexes and much more to read the supporting documentation for phpmyadmin visit https docs phpmyadmin net if you worked with me through the examples in this chapter congratulations it been quite a long journey you ve come all the way from learning how to create a mysql database through issuing complex queries that combine multiple tables to using boolean operators and leveraging mysql various qualifiers in the next chapter we ll start looking at how to approach efficient database design advanced sql techniques and mysql functions and transactions questions what is the purpose of the semicolon in mysql queries which command would you use to view the available databases or tables how would you create a new mysql user on the local host called newuser with a password of newpass and with access to everything in the database newdatabase how can you view the structure of a table what is the purpose of a mysql index what benefit does a fulltext index provide what is a stopword both select distinct and group by cause the display to show only one output row for each value in a column even if multiple rows contain that value what are the main differences between select distinct and group by using the select where construct how would you return only rows contain ing the word langhorne somewhere in the author column of the classics table used in this chapter what needs to be defined in two tables to make it possible for you to join them together see chapter answers on page in appendix a for the answers to these questions chapter mastering mysql chapter provided you with a good grounding in the practice of using relational databases with structured query language you ve learned about creating databases and the tables they comprise as well as inserting looking up changing and deleting data with that knowledge under your belt we now need to look at how to design databases for maximum speed and efficiency for example how do you decide what data to place in which table well over the years a number of guidelines have been developed that if you follow them ensure your databases will be efficient and capable of growing as you feed them more and more data database design it very important that you design a database correctly before you start to create it otherwise you are almost certainly going to have to go back and change it by splitting up some tables merging others and moving various columns about in order to achieve sensible relationships that mysql can easily use sitting down with a sheet of paper and a pencil and writing down a selection of the queries that you think you and your users are likely to ask is an excellent starting point in the case of an online bookstore database some of your questions could be how many authors books and customers are in the database which author wrote a certain book which books were written by a certain author what is the most expensive book what is the best selling book which books have not sold this year which books did a certain customer buy which books have been purchased together of course there are many more queries that you could make on such a database but even this small sample will begin to give you insights into how to lay out your tables for example books and isbns can probably be combined into one table because they are closely linked we ll examine some of the subtleties later in contrast books and customers should be in separate tables because their connection is very loose a cus tomer can buy any book and even multiple copies of a book yet a book can be bought by many customers and be ignored by still more potential customers when you plan to do a lot of searches on something it can often benefit by having its own table and when couplings between things are loose it best to put them in separate tables taking into account those simple rules of thumb we can guess we ll need at least three tables to accommodate all these queries authors there will be lots of searches for authors many of whom have collaborated on titles and many of whom will be featured in collections listing all the information about each author together linked to that author will produce optimal results for searches hence an authors table books many books appear in different editions sometimes they change publisher and sometimes they have the same titles as other unrelated books so the links between books and authors are complicated enough to call for a separate table customers it even more clear why customers should get their own table as they are free to purchase any book by any author primary keys the keys to relational databases using the power of relational databases we can define information for each author book and customer in just one place obviously what interests us is the links between them such as who wrote each book and who purchased it but we can store that information just by making links between the three tables i ll show you the basic prin ciples and then it just takes practice for it to feel natural the magic involves giving every author a unique identifier do the same for every book and for every customer we saw the means of doing that in the previous chapter the primary key for a book it makes sense to use the isbn although you then have to deal with multiple editions that have different isbns for authors and customers you can just assign arbitrary keys which the feature that you saw in the last chapter makes easy in short every table will be designed around some object that you re likely to search for a lot an author book or customer in this case and that object will have a primary key don t choose a key that could possibly have the same value for different objects the isbn is a rare case for which an industry has provided a primary key that you can rely on to be unique for each product most of the time you ll create an arbitrary key for this purpose using normalization the process of separating your data into tables and creating primary keys is called normalization its main goal is to make sure each piece of information appears in the database only once duplicating data is very inefficient because it makes databases larger than they need to be and therefore slows down access but more importantly the presence of duplicates creates a strong risk that you ll update only one row of duplicated data creating inconsistencies in a database and potentially causing serious errors thus if you list the titles of books in the authors table as well as the books table and you have to correct a typographic error in a title you ll have to search through both tables and make sure you make the same change every place the title is listed it better to keep the title in one place and use the isbn in other places but in the process of splitting a database into multiple tables it important not to go too far and create more tables than is necessary which would also lead to inefficient design and slower access luckily e f codd the inventor of the relational model analyzed the concept of nor malization and split it into three separate schemas called first second and third normal form if you modify a database to satisfy each of these forms in order you will ensure that your database is optimally balanced for fast access and minimum memory and disk space usage to see how the normalization process works let start with the rather monstrous database in table which shows a single table containing all of the author names book titles and fictional customer details you could consider it a first attempt at a table intended to keep track of which customers have ordered books obviously this is inefficient design because data is duplicated all over the place duplications are high lighted but it represents a starting point table a highly inefficient design for a database table author author title isbn price us customer name customer address purchase date david sklar adam trachtenberg php cookbook emma brown rainbow road los angeles ca mar danny goodman dynamic html darren ryder emily drive richmond va dec hugh e williams david lane php and mysql earl b thurston gregory lane frankfort ky jun david sklar adam trachtenberg php cookbook darren ryder emily drive richmond va dec rasmus lerdorf kevin tatroe peter macintyre programming php david miller cedar lane waltham ma jan 2009 in the following three sections we will examine this database design and you ll see how we can improve it by removing the various duplicate entries and splitting the single table into multiple tables each containing one type of data first normal form for a database to satisfy the first normal form it must fulfill three requirements there should be no repeating columns containing the same kind of data all columns should contain a single value there should be a primary key to uniquely identify each row looking at these requirements in order you should notice straightaway that the author and author columns constitute repeating data types so we already have a target col umn for pulling into a separate table as the repeated author columns violate rule second there are three authors listed for the final book programming php i ve handled that by making kevin tatroe and peter macintyre share the author column which violates rule yet another reason to transfer the author details to a separate table however rule is satisfied because the primary key of isbn has already been created table shows the result of removing the authors columns from table already it looks a lot less cluttered although there remain duplications that are highlighted table the result of stripping the authors columns from table title isbn price customer customer address purchase date us name php cookbook emma brown rainbow road los angeles ca mar 2009 dynamic html darren ryder emily drive richmond va dec php and mysql earl b thurston gregory lane frankfort ky jun 2009 php cookbook darren ryder emily drive richmond va dec programming php david miller cedar lane waltham ma jan 2009 the new authors table shown in table is small and simple it just lists the isbn of a title along with an author if a title has more than one author additional authors get their own rows at first you may feel ill at ease with this table because you can t tell which author wrote which book but don t worry mysql can quickly tell you all you have to do is tell it which book you want information for and mysql will use its isbn to search the authors table in a matter of milliseconds table the new authors table david sklar adam trachtenberg danny goodman hugh e williams david lane rasmus lerdorf kevin tatroe peter macintyre as i mentioned earlier the isbn will be the primary key for the books table when we get around to creating that table i mention that here in order to emphasize that the isbn is not however the primary key for the authors table in the real world the authors table would deserve a primary key too so that each author would have a key to uniquely identify him or her so in the authors table the isbn is just a column for which for the purposes of speeding up searches we ll probably make a key but not the primary key in fact it cannot be the primary key in this table because it not unique the same isbn appears multiple times whenever two or more authors have collaborated on a book because we ll use it to link authors to books in another table this column is called a foreign key keys also called indexes have several purposes in mysql the fun damental reason for defining a key is to make searches faster you ve seen examples in chapter in which keys are used in where clauses for searching but a key can also be useful to uniquely identify an item thus a unique key is often used as a primary key in one table and as a foreign key to link rows in that table to rows in another table second normal form the first normal form deals with duplicate data or redundancy across multiple col umns the second normal form is all about redundancy across multiple rows in order to achieve second normal form your tables must already be in first normal form once this has been done we achieve second normal form by identifying columns whose data repeats in different places and then removing them to their own tables so let look again at table notice how darren ryder bought two books and therefore his details are duplicated this tells us that the customer columns need to be pulled into their own tables table shows the result of removing the customer columns from table table the new titles table isbn title price php cookbook 99 dynamic html 99 php and mysql 95 programming php 99 as you can see all that left in table are the isbn title and price columns for four unique books so this now constitutes an efficient and self contained table that satisfies the requirements of both the first and second normal forms along the way we ve managed to reduce the information to data closely related to book titles this table could also include years of publication page counts numbers of reprints and so on as these details are also closely related the only rule is that we can t put in any column that could have multiple values for a single book because then we d have to list the same book in multiple rows and would thus violate second normal form restoring an author col umn for instance would violate this normalization however looking at the extracted customer columns now in table we can see that there still more normalization work to do because darren ryder details are still du plicated and it could also be argued that first normal form rule all columns should contain a single value has not been properly complied with because the addresses really need to be broken into separate columns for address city state and zip code table the customer details from table isbn customer name customer address purchase date emma brown rainbow road los angeles ca mar 2009 darren ryder emily drive richmond va dec earl b thurston gregory lane frankfort ky jun 2009 darren ryder emily drive richmond va dec 2008 david miller cedar lane waltham ma jan 2009 what we have to do is split this table further to ensure that each customer details are entered only once because the isbn is not and cannot be used as a primary key to identify customers or authors a new key must be created table is the result of normalizing the customers table into both first and second normal forms each customer now has a unique customer number called custno which is the table primary key and will most likely have been created via ment all the parts of customer addresses have also been separated into distinct columns to make them easily searchable and updateable table the new customers table custno name address city state zip emma brown rainbow road los angeles ca darren ryder emily drive richmond va earl b thurston gregory lane frankfort ky david miller cedar lane waltham ma at the same time in order to normalize table we had to remove the information on customer purchases because otherwise there would be multiple instances of cus tomer details for each book purchased instead the purchase data is now placed in a new table called purchases see table table the new purchases table custno isbn date mar 2009 dec 2008 dec 2008 0596005436 jun 2009 0596006815 jan 2009 here the custno column from table is reused as a key to tie both the customers and the purchases tables together because the isbn column is also repeated here this table can be linked with either of the authors or the titles tables too the custno column can be a useful key in the purchases table but it not a primary key a single customer can buy multiple books and even multiple copies of one book so the custno column is not a primary key in fact the purchases table has no primary key that all right because we don t expect to need to keep track of unique purchases if one customer buys two copies of the same book on the same day we ll just allow two rows with the same information for easy searching we can define both custno and isbn as keys just not as primary keys there are now four tables one more than the three we had initially assumed would be needed we arrived at this decision through the normalization processes by methodically following the first and second normal form rules which made it plain that a fourth table called purchases would also be required the tables we now have are authors table titles table customers table and purchases table and we can link each table to any other using either the custno or the isbn keys for example to see which books darren ryder has purchased you can look him up in table the customers table where you will see his custno is armed with this number you can now go to table the purchases table looking at the isbn column here you will see that he purchased titles and on december 2008 this looks like a lot of trouble for a human but it not so hard for mysql to determine what these titles were you can then refer to table the titles table and see that the books he bought were dynamic html and php cookbook should you wish to know the authors of these books you could also use the isbns you just looked up on table the authors table and you would see that isbn dynamic html was written by danny goodman and that isbn php cookbook was written by david sklar and adam trachtenberg third normal form once you have a database that complies with both the first and second normal forms it is in pretty good shape and you might not have to modify it any further however if you wish to be very strict with your database you can ensure that it adheres to the third normal form which requires that data that is not directly dependent on the primary key but is dependent on another value in the table should also be moved into separate tables according to the dependence for example in table the customers table it could be argued that the state city and zip code keys are not directly related to each customer because many other people will have the same details in their addresses too however they are directly related to each other in that the street address relies on the city and the city relies on the state therefore to satisfy third normal form for table you would need to split it into tables through table third normal form customers table custno name address zip emma brown rainbow road darren ryder emily drive earl b thurston gregory lane david miller cedar lane table third normal form zip codes table zip cityid table third normal form cities table cityid name stateid los angeles 5678 richmond 4321 frankfort 8765 waltham table third normal form states table stateid name abbreviation california ca virginia va kentucky ky massachusetts ma so how would you use this set of four tables instead of the single table well you would look up the zip code in table then find the matching cityid in table given this information you could then look up the city name in table and then also find the stateid which you could use in table to look up the state name although using the third normal form in this way may seem like overkill it can have advantages for example take a look at table where it has been possible to include both a state name and its two letter abbreviation it could also contain population details and other demographics if you desired table could also contain even more localized demographics that could be useful to you and or your customers by splitting up these pieces of data you can make it easier to maintain your database in the future should it be necessary to add columns deciding whether to use the third normal form can be tricky your evaluation should rest on what data you may need to add at a later date if you are absolutely certain that the name and address of a customer is all that you will ever require you probably will want to leave out this final normalization stage on the other hand suppose you are writing a database for a large organization such as the u s postal service what would you do if a city were to be renamed with a table such as table you would need to perform a global search and replace on every instance of that city but if you have your database set up according to the third normal form you would have to change only a single entry in table for the change to be reflected throughout the entire database therefore i suggest that you ask yourself two questions to help you decide whether to perform a third normal form normalization on any table is it likely that many new columns will need to be added to this table could any of this table fields require a global update at any point if either of the answers is yes you should probably consider performing this final stage of normalization when not to use normalization now that you know all about normalization i m going to tell you why you should throw these rules out of the window on high traffic sites that right you should never fully normalize your tables on sites that will cause mysql to thrash normalization requires spreading data across multiple tables and this means making multiple calls to mysql for each query on a very popular site if you have normalized tables your database access will slow down considerably once you get above a few dozen concurrent users because they will be creating hundreds of database accesses between them in fact i would go so far as to say you should denormalize any commonly looked up data as much as you can you see if you have data duplicated across your tables you can substantially reduce the number of additional requests that need to be made because most of the data you want is available in each table this means that you can simply add an extra column to a query and that field will be available for all matching results of course you have to deal with the downsides previously mentioned such as using up large amounts of disk space and ensuring that you update every single duplicate copy of data when one of them needs modifying multiple updates can be computerized though mysql provides a feature called trig gers that make automatic changes to the database in response to changes you make triggers are however beyond the scope of this book another way to propagate re dundant data is to set up a php program to run regularly and keep all copies in sync the program reads changes from a master table and updates all the others you ll see how to access mysql from php in the next chapter however until you are very experienced with mysql i recommend that you fully normalize all your tables at least to first and second normal form as this will instill the habit and put you in good stead only when you actually start to see mysql logjams should you consider looking at denormalization relationships mysql is called a relational database management system because its tables store not only data but the relationships among the data there are three categories of relation ships one to one a one to one relationship is like a traditional marriage each item has a relationship to only one item of the other type this is surprisingly rare for instance an author can write multiple books a book can have multiple authors and even an address can be associated with multiple customers perhaps the best example in this chapter so far of a one to one relationship is the relationship between the name of a state and its two character abbreviation however for the sake of argument let assume that there can only ever be one customer at any address in such a case the customers addresses relationship in figure is a one to one relationship only one customer lives at each address and each address can have only one customer figure the customers table table split into two tables usually when two items have a one to one relationship you just include them as col umns in the same table there are two reasons for splitting them into separate tables you want to be prepared in case the relationship changes later the table has a lot of columns and you think that performance or maintenance would be improved by splitting it of course when you come to build your own databases in the real world you will have to create one to many customer address relationships one address many custom ers one to many one to many or many to one relationships occur when one row in one table is linked to many rows in another table you have already seen how table would take on a one to many relationship if multiple customers were allowed at the same address which is why it would have to be split up if that were the case so looking at table within figure you can see that it shares a one to many relationship with table because there is only one of each customer in table however table the purchases table can and does contain more than one purchase from customers therefore one customer has a relationship with many purchases you can see these two tables alongside each other in figure where the dashed lines joining rows in each table start from a single row in the lefthand table but can connect to more than one row on the righthand table this one to many relationship is also the preferred scheme to use when describing a many to one relationship in which case you would normally swap the left and right tables to view them as a one to many relation ship figure illustrating the relationship between two tables many to many in a many to many relationship many rows in one table are linked to many rows in another table to create this relationship add a third table containing the same key column from each of the other tables this third table contains nothing else as its sole purpose is to link up the other tables table is just such a table it was extracted from table the purchases table but omits the purchase date information it contains a copy of the isbn of every title sold along with the customer number of each purchaser table an intermediary table customer isbn 0596101015 0596527403 0596101015 0596005436 0596006815 with this intermediary table in place you can traverse all the information in the database through a series of relations you can take an address as a starting point and find out the authors of any books purchased by the customer living at that address for example let suppose that you want to find out about purchases in the zip code look that zip code up in table and you ll find that customer number has bought at least one item from the database at this point you can use table to find out his or her name or use the new intermediary table to see the book purchased from here you will find that two titles were purchased and can follow them back to table to find the titles and prices of these books or to table to see who the authors were if it seems to you that this is really combining multiple one to many relationships then you are absolutely correct to illustrate figure brings three tables together figure creating a many to many relationship via a third table follow any zip code in the lefthand table to associated customer ids from there you can link to the middle table which joins the left and right tables by linking customer ids and isbns now all you have to do is follow an isbn over to the righthand table to see which book it relates to you can also use the intermediary table to work your way backward from book titles to zip codes the titles table can tell you the isbn which you can use in the middle table to find id numbers of customers who bought the books and finally the customers table matches the customer id numbers to the customers zip codes databases and anonymity an interesting aspect of using relations is that you can accumulate a lot of information about some item such as a customer without actually knowing who that customer is note that in the previous example we went from customers zip codes to customers purchases and back again without finding out the name of a customer databases can be used to track people but they can also be used to help preserve people privacy while still finding useful information transactions in some applications it is vitally important that a sequence of queries runs in the correct order and that every single query successfully completes for example suppose that you are creating a sequence of queries to transfer funds from one bank account to another you would not want either of the following events to occur you add the funds to the second account but when you try to subtract them from the first account the update fails and now both accounts have the funds you subtract the funds from the first bank account but the update request to add them to the second account fails and the funds have now disappeared into thin air as you can see not only is the order of queries important in this type of transaction but it is also vital that all parts of the transaction complete successfully but how can you ensure this happens because surely after a query has occurred it cannot be undone do you have to keep track of all parts of a transaction and then undo them all one at a time if any one fails the answer is absolutely not because mysql comes with powerful transaction handling features to cover just these types of eventualities in addition transactions allow concurrent access to a database by many users or pro grams at the same time mysql handles this seamlessly by ensuring that all transactions are queued and that users or programs take their turns and don t tread on each other toes transaction storage engines to be able to use mysql transaction facility you have to be using mysql innodb storage engine this is easy to do as it simply another parameter that you use when creating a table so go ahead and create a table of bank accounts by typing the commands in example remember that to do this you will need access to the mysql command line and must also have already selected a suitable database in which to create this table example creating a transaction ready table create table accounts number int balance float primary key number engine innodb describe accounts the final line of this example displays the contents of the new table so you can ensure that it was correctly created the output from it should look like this field type null key default extra number int no pri balance float yes null rows in set sec now let create two rows within the table so that you can practice using transactions enter the commands in example example populating the accounts table insert into accounts number balance values insert into accounts number balance values 00 select from accounts the third line displays the contents of the table to confirm that the rows were correctly inserted the output should look like this number balance rows in set 00 sec with this table created and prepopulated you are now ready to start using transactions using begin transactions in mysql start with either a begin or a start transaction statement type the commands in example to send a transaction to mysql example a mysql transaction begin update accounts set balance balance where number commit select from accounts the result of this transaction is displayed by the final line and should look like this number balance rows in set 00 sec as you can see the balance of account number was increased by and is now you may also have noticed the commit command in example which is explained next using commit when you are satisfied that a series of queries in a transaction has successfully com pleted issue a commit command to commit all the changes to the database until it receives a commit mysql considers all the changes you make to be merely temporary this feature gives you the opportunity to cancel a transaction by not sending a commit but by issuing a rollback command instead using rollback using the rollback command you can tell mysql to forget all the queries made since the start of a transaction and to end the transaction see this in action by entering the funds transfer transaction in example example a funds transfer transaction begin update accounts set balance balance where number update accounts set balance balance where number select from accounts once you have entered these lines you should see the following result number balance rows in set 00 sec the first bank account now has a value that is less than before and the second has been incremented by you have transferred a value of between them but let assume that something went wrong and you wish to undo this transaction all you have to do is issue the commands in example example canceling a transaction using rollback rollback select from accounts you should now see the following output showing that the two accounts have had their previous balances restored due to the entire transaction being canceled via the roll back command number balance 61 67890 140 rows in set 00 sec using explain mysql comes with a powerful tool for investigating how the queries you issue to it are interpreted using explain you can get a snapshot of any query to find out whether you could issue it in a better or more efficient way example shows how to use it with the accounts table you created earlier example using the explain command explain select from accounts where number the results of this explain command should look like the following id table type key ref rows extra simple accounts const primary primary const row in set 00 sec the information that mysql is giving you here is as follows the selection type is simple if you were joining tables together this would show the join type table the current table being queried is accounts type the query type is const from worst to best the possible values can be all index range ref const system and null there is a possible primary key which means that accessing should be fast key the key actually used is primary this is good the key length is this is the number of bytes of the index that mysql will use ref the ref column displays which columns or constants are used with the key in this case a constant key is being used rows the number of rows that needs to be searched by this query is this is good whenever you have a query that seems to be taking longer than you think it should to execute try using explain to see where you can optimize it you will discover which keys if any are being used their lengths and so on and will be able to adjust your query or the design of your table accordingly when you have finished experimenting with the temporary ac counts table you may wish to remove it by entering the following command drop table accounts backing up and restoring whatever kind of data you are storing in your database it must have some value to you even if it only the cost of the time required for reentering it should the hard disk fail therefore it important that you keep backups to protect your investment also there will be times when you have to migrate your database over to a new server the best way to do this is usually to back it up first it is also important that you test your backups from time to time to ensure that they are valid and will work if they need to be used thankfully backing up and restoring mysql data is easy with the mysqldump command using mysqldump with mysqldump you can dump a database or collection of databases into one or more files containing all the instructions necessary to re create all your tables and repopulate them with your data it can also generate files in csv comma separated values and other delimited text formats or even in xml format its main drawback is that you must make sure that no one writes to a table while you re backing it up there are various ways to do this but the easiest is to shut down the mysql server before mysqldump and start up the server again after mysqldump finishes or you can lock the tables you are backing up before running mysqldump to lock tables for reading as we want to read the data issue the following command from the mysql command line lock tables read read then to release the lock enter unlock tables by default the output from mysqldump is simply printed out but you can capture it in a file through the redirect symbol the basic format of the mysqldump command is mysqldump u user ppassword database however before you can dump the contents of a database you must make sure that mysqldump is in your path or that you specify its location as part of your command table shows the likely locations of the program for the different installations and operating systems covered in chapter if you have a different installation it may be in a slightly different location if you are using os x with mysqldump and receive the error can t connect to local mysql server through socket tmp mysql sock when trying to connect you may be able to remedy this by issuing the following instruction ln usr local zend mysql tmp mysql sock tmp mysql sock table likely locations of mysqldump for different installations operating system program likely folder location windows bit zend server c program files zend bin windows bit zend server c program files zend bin os x zend server usr local zend mysql bin linux zend server usr local zend mysql bin so to dump the contents of the publications database that you created in chapter to the screen enter mysqldump or the full path if necessary and the command in example example dumping the publications database to screen mysqldump u user ppassword publications make sure that you replace user and password with the correct details for your instal lation of mysql if there is no password set for the user you can omit that part of the command but the u user part is mandatory unless you have root access without a password and are executing as root not recommended the result of issuing this com mand will look something like figure figure dumping the publications database to screen creating a backup file now that you have mysqldump working and have verified it outputs correctly to the screen you can send the backup data directly to a file using the redirect symbol assuming that you wish to call the backup file publications sql type the command in example remembering to replace user and password with the correct details example dumping the publications database to file mysqldump u user ppassword publications publications sql the command in example stores the backup file into the cur rent directory if you need it to be saved elsewhere you should in sert a file path before the filename you must also ensure that the directory you are backing up to has the right permissions set to al low the file to be written if you echo the backup file to screen or load it into a text editor you will see that it comprises sequences of sql commands such as the following drop table if exists classics create table classics author varchar default null title varchar default null category varchar default null year smallint default null isbn char not null default primary key isbn key author author key title title key category category key year year fulltext key author title engine myisam default charset this is smart code that can be used to restore a database from a backup even if it currently exists because it will first drop any tables that need to be re created thus avoiding potential mysql errors backing up a single table to back up only a single table from a database such as the classics table from the publications database you should first lock the table from within the mysql command line by issuing a command such as the following lock tables publications classics read this ensures that mysql remains running for read purposes but writes cannot be made then while keeping the mysql command line open use another terminal win dow to issue the following command from the operating system command line mysqldump u user ppassword publications classics classics sql you must now release the table lock by entering the following command from the mysql command line in the first terminal window which unlocks all tables that have been locked during the current session unlock tables backing up all tables if you want to back up all your mysql databases at once including the system databases such as mysql you can use a command such as the one in example which would enable you to restore an entire mysql database installation remember to use locking where required example dumping all the mysql databases to file mysqldump u user ppassword all databases sql of course there a lot more than just a few lines of sql code in backed up database files i recommend that you take a few minutes to examine a couple in order to familiarize yourself with the types of commands that appear in backup files and how they work restoring from a backup file to perform a restore from a file call the mysql executable passing it the file to restore from using the symbol so to recover an entire database that you dumped using the all databases option use a command such as that in example example restoring an entire set of databases mysql u user ppassword sql to restore a single database use the d option followed by the name of the database as in example 11 where the publications database is being restored from the backup made in example example 11 restoring the publications database mysql u user ppassword d publications publications sql to restore a single table to a database use a command such as that in example where just the classics table is being restored to the publications database example 12 restoring the classics table to the publications database mysql u user ppassword d publications classics sql dumping data in csv format as previously mentioned the mysqldump program is very flexible and supports various types of output such as the csv format example shows how you can dump the data from the classics and customers tables in the publications database to the files classics txt and customers txt in the folder c temp by default on zend server the user should be root and no password is used on os x or linux systems you should modify the destination path to an existing folder example dumping data to csv format files mysqldump u user ppassword no create info tab c temp fields terminated by publications this command is quite long and is shown here wrapped over several lines but you must type it all as a single line the result is the following mark twain samuel langhorne clemens the adventures of tom sawyer classic fiction jane austen pride and prejudice classic fiction charles darwin the origin of species non fiction charles dickens the old curiosity shop classic fiction william shakespeare romeo and juliet play mary smith jack wilson planning your backups the golden rule to backing up is to do so as often as you find practical the more valuable the data the more often you should back it up and the more copies you should make if your database gets updated at least once a day you should really back it up on a daily basis if on the other hand it is not updated very often you could probably get by with less frequent backups you should also consider making multiple backups and storing them in different locations if you have several servers it is a simple mat ter to copy your backups between them you would also be well ad vised to make physical backups of removable hard disks thumb drives cds or dvds and so on and to keep these in separate loca tions preferably somewhere like a fireproof safe once you ve digested the contents of this chapter you will be proficient in using both php and mysql the next chapter will show you how to bring these two technologies together questions what does the word relationship mean in reference to a relational database what is the term for the process of removing duplicate data and optimizing tables what are the three rules of the first normal form how can you make a table satisfy the second normal form what do you put in a column to tie together two tables that contain items having a one to many relationship how can you create a database with a many to many relationship what commands initiate and end a mysql transaction what feature does mysql provide to enable you to examine how a query will work in detail what command would you use to back up the database publications to a file called publications sql see chapter answers on page in appendix a for the answers to these questions chapter accessing mysql using php if you worked through the previous chapters you re proficient in using both mysql and php in this chapter you will learn how to integrate the two by using php built in functions to access mysql querying a mysql database with php the reason for using php as an interface to mysql is to format the results of sql queries in a form visible in a web page as long as you can log into your mysql instal lation using your username and password you can also do so from php however instead of using mysql command line to enter instructions and view output you will create query strings that are passed to mysql when mysql returns its response it will come as a data structure that php can recognize instead of the formatted output you see when you work on the command line further php commands can retrieve the data and format it for the web page to get you started in this chapter i use the standard procedural mysql function calls so that you ll be up and running quickly and able to maintain older php code however the new object oriented mysqli functions the i stands for improved are becoming the recommended way to interface with mysql from php so in the following chapter i ll show you how to use these too or instead because the old functions have become deprecated and could be re moved from php at some point the process the process of using mysql with php is connect to mysql select the database to use build a query string perform the query retrieve the results and output them to a web page repeat steps to until all desired data has been retrieved 7 disconnect from mysql we ll work through these sections in turn but first it important to set up your login details in a secure manner so people snooping around on your system have trouble getting access to your database creating a login file most websites developed with php contain multiple program files that will require access to mysql and will thus need the login and password details therefore it sen sible to create a single file to store these and then include that file wherever it needed example shows such a file which i ve called login php type the example replacing placeholder values such as username with the actual values you use for your mysql database and save it to the web development directory you set up in chapter we ll be making use of the file shortly the hostname localhost should work as long as you re using a mysql database on your local system and the database publications should work if you re typing the examples i ve used so far example the login php file php login php localhost publications username password the enclosing php and tags are especially important for the login php file in example because they mean that the lines between can be interpreted only as php code if you were to leave them out and someone were to call up the file directly from your website it would display as text and reveal your secrets but with the tags in place all that person will see is a blank page the file will correctly include in your other php files the variable will tell php which computer to use when connecting to a database this is required because you can access mysql databases on any computer connected to your php installation and that potentially includes any host anywhere on the web however the examples in this chapter will be working on the local server so in place of specifying a domain such as mysql myserver com you can just use the word localhost or the ip address the database we ll be using is the one called publications which you probably created in chapter or the one you were provided with by your server ad ministrator in which case you have to modify login php accordingly the variables and should be set to the username and password that you have been using with mysql another benefit of keeping these login details in a single place is that you can change your password as frequently as you like and there will be only one file to update when you do no matter how many php files access mysql connecting to mysql now that you have the login php file saved you can include it in any php files that will need to access the database by using the statement this is preferable to an include statement as it will generate a fatal error if the file is not found and believe me not finding the file containing the login details to your database is a fatal error also using instead of require means that the file will be read in only when it has not previously been included which prevents wasteful duplicate disk ac cesses example shows the code to use example connecting to a mysql server php login php if die unable to connect to mysql this example runs php function which requires three parameters the hostname username and password of a mysql server upon success it returns an identifier to the server otherwise false is returned notice that the second line uses an if statement with the die function which does what it sounds like and quits from php with an error message if is not true the die message explains that it was not possible to connect to the mysql database and to help identify why this happened includes a call to the function this function outputs the error text from the last called mysql function the database server pointer will be used in some of the following examples to identify the mysql server to be queried by using identifiers this way we can connect to and access multiple mysql servers from a single php program the die function is great for when you are developing php code but of course you will want more user friendly error messages on a pro duction server in this case you won t abort your php program but format a message that will be displayed when the program exits nor mally such as function msg echo we are sorry but it was not possible to complete the requested task the error message we got was p msg p please click the back button on your browser and try again if you are still having problems please a href mailto admin server com email our administrator a thank you selecting a database having successfully connected to mysql you are now ready to select the database that you will be using example shows how to do this example selecting a database php or die unable to select database the command to select the database is pass it the name of the data base you want and the server to which you connected as with the previous example a die statement has been included to provide an error message and explanation should the selection fail the only difference being that there is no need to retain the return value from the function as it simply returns either true or false therefore the php or statement was used which means if the previous command failed do the following note that for the or to work there must be no semicolon at the end of the first line of code building and executing a query sending a query to mysql from php is as simple as issuing it using the function example shows you how to use it example querying a database php query select from classics result query if result die database access failed first the variable query is set to the query to be made in this case it is asking to see all rows in the table classics note that unlike with mysql command line no semicolon is required at the tail of the query because the function is used to issue a complete query it cannot be used for queries sent in multiple parts one at a time therefore mysql knows the query is complete and doesn t look for a semicolon this function returns a result that we place in the variable result having used mysql at the command line you might think that the contents of result will be the same as the result returned from a command line query with horizontal and vertical lines and so on however this is not the case with the result returned to php instead upon success result will contain a resource that can be used to extract the results of the query you ll see how to extract the data in the next section upon failure result contains false so the example finishes by checking result if it false it means that there was an error and the die command is executed fetching a result once you have a resource returned from a function you can use it to retrieve the data you want the simplest way to do this is to fetch the cells you want one at a time using the function example combines and extends the previous examples into a program that you can type and run yourself to retrieve the returned results i suggest that you save it in the same folder as login php and give it the name query php example fetching results one cell at a time php query php login php if die unable to connect to mysql or die unable to select database query select from classics result query if result die database access failed rows result for j j rows j echo author result j author br echo title result j title br echo category result j category br echo year result j year br echo isbn result j isbn br br the final lines of code are the new ones so let look at them they start by setting the variable rows to the value returned by a call to this function reports the number of rows returned by a query armed with the row count we enter a for loop that extracts each cell of data from each row using the function the parameters supplied to this function are the resource result which was returned by the row number j and the name of the column from which to extract the data the results from each call to are then incorporated within echo state ments to display one field per line with an additional line feed between rows figure shows the result of running this program as you may recall we populated the classics table with five rows in chapter and indeed five rows of data are returned by query php but as it stands this code is actually ex tremely inefficient and slow because a total of calls are made to the function in order to retrieve all the data a single cell at a time luckily there is a much better way of retrieving the data which is getting a single row at a time using the function in chapter i talked about first second and third normal form so you may have now noticed that the classics table doesn t satisfy these because both author and book details are included within the same table that because we created this table before encountering normalization however for the purposes of illustrating access to mysql from php reusing this table avoids the hassle of typing in a new set of test data so we ll stick with it for the time being figure the output from the query php program in example fetching a row it was important to show how you can fetch a single cell of data from mysql but now let look at a much more efficient method replace the for loop of query php in example with the new loop in example and you will find that you get exactly the same result that was displayed in figure example replacement for loop for fetching results one row at a time php for j j rows j row result echo author row br echo title row br echo category row br echo year row br echo isbn row br br in this modified code only one fifth of the calls are made to a mysql calling function a full less because each row is fetched in its entirety via the function this returns a single row of data in an array which is then assigned to the variable row all that necessary then is to reference each element of the array row in turn starting at an offset of therefore row contains the author data row the title and so on because each column is placed in the array in the order in which it appears in the mysql table also by using instead of you use sub stantially less php code and achieve much faster execution time due to simply refer encing each item of data by offset rather than by name closing a connection when you have finished using a database you should close the connection you do so by issuing the command in example 7 example 7 closing a mysql server connection php we have to pass the identifier returned by back in example which we stored in the variable all database connections are automatically closed when php exits so it doesn t matter that the connection wasn t closed in example but in longer programs where you may continually open and close database connections you are strongly advised to close each one as soon as you re finished accessing it a practical example it time to write our first example of inserting data in and deleting it from a mysql table using php i recommend that you type example and save it to your web development directory using the filename sqltest php you can see an example of the program output in figure example creates a standard html form chapter 12 explains forms in detail but in this chapter i take form handling for granted and just deal with database interaction example inserting and deleting using sqltest php php sqltest php login php db_username if die unable to connect to mysql or die unable to select database if isset delete isset isbn isbn isbn query delete from classics where isbn isbn if query echo delete failed query br br br if isset author isset title isset category isset year isset isbn author author title title category category year year isbn isbn query insert into classics values author title category year isbn if query echo insert failed query br br br echo form action sqltest php method post pre author input type text name author title input type text name title category input type text name category year input type text name year isbn input type text name isbn input type submit value add record pre form query select from classics result query if result die database access failed rows result for j j rows j row result echo pre author row title row category row year row isbn row pre form action sqltest php method post input type hidden name delete value yes input type hidden name isbn value row input type submit value delete record form function var return var at over lines of code this program may appear daunting but don t worry you ve already covered many of them in example and what the code does is actually quite simple it first checks for any inputs that may have been made and then either inserts new data into the table classics of the publications database or deletes a row from it according to the input supplied regardless of whether there was input the program then outputs all rows in the table to the browser so let see how it works the first section of new code starts by using the isset function to check whether values for all the fields have been posted to the program upon confirmation each of the lines within the if statement calls the function which appears at the end of the program this function has one small but critical job fetching the input from the browser figure the output from example 8 sqltest php the array i mentioned in an earlier chapter that a browser sends user input through either a get request or a post request the post request is usually preferred and we use it here the web server bundles up all of the user input even if the form was filled out with a hundred fields and puts in into an array named is an associative array which you encountered in chapter depending on whether a form has been set to use the post or the get method either the or the associative array will be populated with the form data they can both be read in exactly the same way each field has an element in the array named after that field so if a form contained a field named isbn the array contains an element keyed by the word isbn the php program can read that field by referring to either isbn or isbn single and double quotes have the same effect in this case if the syntax still seems complex to you rest assured that you can just use the convention i ve shown in example 8 copy the user input to other variables and forget about after that this is normal in php programs they retrieve all the fields from at the beginning of the program and then ignore it there is no reason to write to an element in the array its only purpose is to communicate information from the browser to the pro gram and you re better off copying data to your own variables be fore altering it so back to the function it passes each item it retrieves through the function to strip out any characters that a hacker may have inserted in order to break into or alter your database deleting a record prior to checking whether new data has been posted the program checks whether the variable delete has a value if so the user has clicked on the delete record button to erase a record in this case the value of isbn will also have been posted as you ll recall the isbn uniquely identifies each record the html form appends the isbn to the delete from query string created in the variable query which is then passed to the function to issue it to mysql returns either true or false and false causes an error message to be displayed explaining what went wrong if delete is not set and there is therefore no record to be deleted author and other posted values are checked if they have all been given values then query is set to an insert into command followed by the five values to be inserted the variable is then passed to which upon completion returns either true or false if false is returned an error message is displayed displaying the form next we get to the part of code that displays the little form at the top of figure you should recall the echo structure from previous chapters which outputs ev erything between the tags instead of the echo command the program could also drop out of php using issue the html and then reenter php processing with php whichever style used is a matter of programmer preference but i always recommend staying within php code for these reasons it makes it very clear when debugging and also for other users that everything within a php file is php code therefore there is no need to go hunting for dropouts to html when you wish to include a php variable directly within html you can just type it if you had dropped back to html you would have had to temporarily reenter php processing output the variable and then drop back out again the html form section simply sets the form action to sqltest php this means that when the form is submitted the contents of the form fields will be sent to the file sqltest php which is the program itself the form is also set up to send the fields as a post rather than a get request this is because get requests are appended to the url being submitted to and can look messy in your browser they also allow users to easily modify submissions and try to hack your server therefore whenever possible you should use post submissions which also have the benefit of hiding the posted data from view having output the form fields the html displays a submit button with the name add record and closes the form note the use of the pre and pre tags here which have been used to force a monospaced font and allow all the inputs to line up neatly the carriage returns at the end of each line are also output when inside pre tags querying the database next the code returns to the familiar territory of example where in the following four lines of code a query is sent to mysql asking to see all the records in the classics table after that rows is set to a value representing the number of rows in the table and a for loop is entered to display the contents of each row i have altered the next bit of code to simplify things instead of using the br tags for line feeds in example i have chosen to use a pre tag to line up the display of each record in a pleasing manner after the display of each record there is a second form that also posts to sqltest php the program itself but this time contains two hidden fields delete and isbn the delete field is set to yes and isbn to the value held in row which contains the isbn for the record then a submit button with the name delete record is displayed and the form is closed a curly brace then completes the for loop which will continue until all records have been displayed finally you see the definition for the function which we ve already looked at and that it our first php program to manipulate a mysql database so let check out what it can do once you have typed the program and corrected any typing errors try entering the following data into the various input fields to add a new record for the book moby dick to the database herman melville moby dick fiction running the program when you have submitted this data using the add record button scroll down to the bottom of the web page to see the new addition it should look like figure figure the result of adding moby dick to the database now let look at how deleting a record works by creating a dummy record so try entering just the number in each of the five fields and click on the add record button if you now scroll down you ll see a new record consisting just of obviously this record isn t useful in this table so now click on the delete record button and scroll down again to confirm that the record has been deleted assuming that everything worked you are now able to add and de lete records at will try doing this a few times but leave the main records in place including the new one for moby dick as we ll be using them later you could also try adding the record with all again a couple of times and note the error message that you receive the second time indicating that there is already an isbn with the num ber practical mysql you are now ready to look at some practical techniques that you can use in php to access the mysql database including tasks such as creating and dropping tables inserting updating and deleting data and protecting your database and website from malicious users note that the following examples assume that you ve created the login php pro gram discussed earlier in this chapter creating a table let assume you are working for a wildlife park and need to create a database to hold details about all the types of cats it houses you are told that there are nine families of cats lion tiger jaguar leopard cougar cheetah lynx caracal and domestic so you ll need a column for that then each cat has been given a name so that another column and you also want to keep track of their ages which is another of course you will probably need more columns later perhaps to hold dietary requirements inocu lations and other details but for now that enough to get going a unique identifier is also needed for each animal so you also decide to create a column for that called id example shows the code you might use to create a mysql table to hold this data with the main query assignment in bold text example creating a table called cats php login php db_username if die unable to connect to mysql or die unable to select database query create table cats id smallint not null family varchar not null name varchar not null age tinyint not null primary key id result query if result die database access failed as you can see the mysql query looks pretty similar to how you would type it directly in the command line except that there is no trailing semicolon as none is needed when you are accessing mysql from php describing a table when you aren t logged into the mysql command line here a handy piece of code that you can use to verify that a table has been correctly created from inside a browser it simply issues the query describe cats and then outputs an html table with four headings column type null and key underneath which all columns within the table are shown to use it with other tables simply replace the name cats in the query with that of the new table see example example describing the table cats php login php db_username if die unable to connect to mysql or die unable to select database query describe cats result query if result die database access failed rows result echo table tr th column th th type th th null th th key th tr for j j rows j row result echo tr for k k k echo td row k td echo tr echo table the output from the program should look like this column type null key id smallint no pri family varchar no name varchar no age tinyint no dropping a table dropping a table is very easy to do and is therefore very dangerous so be careful example 11 shows the code that you need however i don t recommend that you try it until you have been through the other examples as it will drop the table cats and you ll have to re create it using example 9 example 11 dropping the table cats php login php db_username if die unable to connect to mysql or die unable to select database query drop table cats result query if result die database access failed adding data let add some data to the table using the code in example 12 example 12 adding data to table cats php login php db_username if die unable to connect to mysql or die unable to select database query insert into cats values null lion leo result query if result die database access failed you may wish to add a couple more items of data by modifying query as follows and calling up the program in your browser again query insert into cats values null cougar growler query insert into cats values null cheetah charly by the way notice the null value passed as the first parameter this is because the id column is of type and mysql will decide what value to assign ac cording to the next available number in sequence so we simply pass a null value which will be ignored of course the most efficient way to populate mysql with data is to create an array and insert the data with a single query retrieving data now that some data has been entered into the cats table example 13 shows how you can check that it was correctly inserted example 13 retrieving rows from the cats table php login php db_username if die unable to connect to mysql or die unable to select database query select from cats result query if result die database access failed rows result echo table tr th id th th family th th name th th age th tr for j j rows j row result echo tr for k k k echo td row k td echo tr echo table this code simply issues the mysql query select from cats and then displays all the rows returned its output is as follows id family name age lion leo cougar growler cheetah charly here you can see that the id column has correctly auto incremented updating data changing data that you have already inserted is also quite simple did you notice the spelling of charly for the cheetah name let correct that to charlie as in example example renaming charly the cheetah to charlie php login php db_username if die unable to connect to mysql or die unable to select database query update cats set name charlie where name charly result query if result die database access failed if you run example 13 again you ll see that it now outputs the following id family name age lion leo cougar growler 2 cheetah charlie deleting data growler the cougar has been transferred to another zoo so it time to remove him from the database see example 15 example 15 removing growler the cougar from the cats table php login php db_username if die unable to connect to mysql or die unable to select database query delete from cats where name growler result query if result die database access failed this uses a standard delete from query and when you run example 13 you can see that the row has been removed in the following output id family name age lion leo cheetah charlie 3 using when using you cannot know what value has been given to a column before a row is inserted instead if you need to know it you must ask mysql afterward using the function this need is common for instance when you process a purchase you might insert a new customer into a customers table and then refer to the newly created custid when inserting a purchase into the purchase table example 12 can be rewritten as example 16 to display this value after each insert example 16 adding data to table cats and reporting the insertion id php login php db_username if die unable to connect to mysql or die unable to select database query insert into cats values null lynx stumpy result query echo the insert id was if result die database access failed the contents of the table should now look like the following note how the previous id value of 2 is not reused as this could cause complications in some instances id family name age lion leo 3 cheetah charlie 3 4 lynx stumpy using insert ids it very common to insert data in multiple tables a book followed by its author or a customer followed by his purchase and so on when doing this with an auto increment column you will need to retain the insert id returned for storing in the related table for example let assume that these cats can be adopted by the public as a means of raising funds and that when a new cat is stored in the cats table we also want to create a key to tie it to the animal adoptive owner the code to do this is similar to that in example 16 except that the returned insert id is stored in the variable insertid and is then used as part of the subsequent query query insert into cats values null lynx stumpy result query insertid query insert into owners values insertid ann smith result query now the cat is connected to its owner through the cat unique id which was created automatically by using locks a completely safe procedure for linking tables through the insert id is to use locks or transactions as described in chapter 9 it can slow down response time a bit when there are many people submitting data to the same table but it can also be worth it the sequence is lock the first table e g cats 2 insert data into the first table 3 retrieve the unique id from the first table through 4 unlock the first table insert data into the second table you can safely release the lock before inserting data into the second table because the insert id has been retrieved and is stored in a program variable you could also use a transaction instead of locking but that slows down the mysql server even more performing additional queries ok that enough feline fun to explore some slightly more complex queries we need to revert to using the customers and classics tables that you created in chapter 8 there will be two customers in the customers table the classics table holds the details of a few books they also share a common column of isbns called isbn that we can use to perform additional queries for example to display all of the customers along with the titles and authors of the books they have bought you can use the code in example 17 example 17 performing a secondary query php login php db_username if die unable to connect to mysql or die unable to select database query select from customers result query if result die database access failed rows result for j j rows j row result echo row purchased isbn row br subquery select from classics where isbn row subresult subquery if subresult die database access failed subrow subresult echo subrow by subrow br this program uses an initial query to the customers table to look up all the customers and then given the isbn of the book each customer purchased makes a new query to the classics table to find out the title and author for each the output from this code should be as follows mary smith purchased isbn pride and prejudice by jane austen jack wilson purchased isbn the origin of species by charles darwin of course although it wouldn t illustrate performing additional quer ies in this particular case you could also return the same informa tion using a natural join query see chapter 8 like this select name isbn title author from customers natural join classics preventing sql injection it may be hard to understand just how dangerous it is to pass user input unchecked to mysql for example suppose you have a simple piece of code to verify a user and it looks like this user user pass pass query select from users where user user and pass pass at first glance you might think this code is perfectly fine if the user enters values of fredsmith and mypass for user and pass respectively then the query string as passed to mysql will be as follows select from users where user fredsmith and pass mypass this is all well and good but what if someone enters the following for user and doesn t even enter anything for pass admin let look at the string that would be sent to mysql select from users where user admin and pass do you see the problem there in mysql the symbol represents the start of a com ment therefore the user will be logged in as admin assuming there is a user admin without having to enter a password in the following the part of the query that will be executed is shown in bold the rest will be ignored select from users where user admin and pass but you should count yourself very lucky if that all a malicious user does to you at least you might still be able to go into your application and undo any changes the user makes as admin but what about the case in which your application code removes a user from the database the code might look something like this user user pass pass query delete from users where user user and pass pass again this looks quite normal at first glance but what if someone entered the following for user anything or 1 1 this would be interpreted by mysql as delete from users where user anything or 1 1 and pass ouch that sql query will always be true and therefore you ve lost your whole users database so what can you do about this kind of attack well the first thing is not to rely on php built in magic quotes which automatically escape any characters such as single and double quotes by prefacing them with a back slash why because this feature can be turned off many programmers do so in order to put their own security code in place so there is no guarantee that this hasn t happened on the server you are working on in fact the feature was deprecated as of php 5 3 and has been removed in php 6 0 instead you should always use the function for all calls to mysql example 18 is a function you can use that will remove any magic quotes added to a user inputted string and then properly sanitize it for you example 18 how to properly sanitize user input for mysql php function string if string stripslashes string return string the function returns true if magic quotes are active in that case any slashes that have been added to a string have to be removed or the function could end up double escaping some characters creating corrupted strings example 19 illustrates how you would incorporate within your own code example 19 how to safely access mysql with user input php user user pass pass query select from users where user user and pass pass function string if string stripslashes string return string remember that you can use only when a mysql database is actively open otherwise an error will occur using placeholders another way this one virtually bulletproof to prevent sql injections is to use a fea ture called placeholders the idea is to predefine a query using characters where the data will appear then instead of calling a mysql query directly you call the predefined one passing the data to it this has the effect of ensuring that every item of data entered is inserted directly into the database and cannot be interpreted as sql queries in other words sql injections become impossible the sequence of queries to execute when using mysql command line would be like that in example 20 example 20 using placeholders prepare statement from insert into classics values set author emily bront title wuthering heights category classic fiction year isbn execute statement using author title category year isbn deallocate prepare statement the first command prepares a statement called statement for inserting data into the classics table as you can see in place of values or variables for the data to insert the statement contains a series of characters these are the placeholders the next five lines assign values to mysql variables according to the data to be inserted then the predefined statement is executed passing these variables as parameters fi nally the statement is removed in order to return the resources it was using in php the code for this procedure looks like example 21 assuming that you have created login php with the correct details to access the database example 21 using placeholders with php php require login php mysql_connect db_username if die unable to connect to mysql db_database or die unable to select database once you have prepared a statement until you deallocate it you can use it as often as you wish such statements are commonly used within a loop to quickly insert data into a database by assigning values to the mysql variables and then executing the statement this approach is more efficient than creating the entire statement from scratch on each pass through the loop preventing html injection there another type of injection you need to concern yourself about not for the safety of your own websites but for your users privacy and protection that s cross site script ing also referred to as xss this occurs when you allow html or more often javascript code to be input by a user and then displayed back by your website one place this is common is in a comment form what happens most often is that a malicious user will try to write code that steals cookies from your site s users allowing him or her to discover username and password pairs or other information even worse the malicious user might launch an attack to download a trojan onto a user s computer but preventing this is as simple as calling the htmlentities function which strips out all html markup codes and replaces them with a form that displays the characters but does not allow a browser to act on them for example consider the following html script src http x com hack js script script hack script this code loads in a javascript program and then executes malicious functions but if it is first passed through htmlentities it will be turned into the following totally harmless string lt script src http x com hack js gt lt script gt lt script gt hack lt script gt therefore if you are ever going to display anything that your users enter either imme diately or after storing it in a database you need to first sanitize it with htmlentities to do this i recommend that you create a new function like the first one in example 22 which can sanitize for both sql and xss injections chapter laetrile laetrile is a colorless liquid pressed from the soft bitter insides of apricot pits in sweden you can buy the stuff in the grocery store for about the price of almond extract and you use it in baking much as you would any other extract in mexico you can buy it for fifty dollars a drop to cure your fatal cancer of course it doesn t cure anything all the evidence demonstrates that it is a cruel fraud but since no one else has anything at all to offer them terminal pa tients accept the claims of the laetrile peddlers no matter how outra geous people who are desperate enough don t look very hard at the evidence similarly lots of managers are desperate enough and their desperation makes them easy victims of a kind of technical laetrile that purports to improve productivity there is seldom any evidence at all to support the claims of what they buy they too dispense with evidence because their need is so great lose fat while sleeping one day in a moment of high silliness i started clip ping ads for products that claimed to boost productivity by one hundred percent or more within a very short time i had quite a pile the amazing thing was the diversity of the means advertised to yield big produc tivity gains there were seminars packaged pro grams methodologies books scheduling boards hard ware monitors computing languages and newsletters laetrile going uptown on the subway that night i spotted one final ad on the back of the new york post it read lose fat while sleeping it seemed to fit right in with the others we re all under a lot of pressure to improve productivity the problem is no longer susceptible to easy solutions because all the easy solutions were thought of and applied long ago yet some organizations are doing a lot better than others we re convinced that those who do better are not using any particularly advanced technology their better performance can be explained entirely by their more effective ways of handling people modifying the work place and corporate culture and implementing some of the measures that we ll discuss in parts ii through iv the relative inefficacy of technology may be a bit discouraging at least in the short run because the kinds of modification to corporate culture we advocate are hard to apply and slow to take effect what would be far prefer able is the coupon you cut out of the back pages of a magazine to send in with a few thousand bucks so that some marvelous productivity gimmick will come back to you in the mail of course it may not do much for you but then easy non solutions are often more attractive than hard solutions the seven sirens the false hopes engendered by easy technological non solutions are like those sirens that tempted poor odysseus each one reaches out to you with her own beguiling message an attractive fallacy that leads nowhere as long as you believe them you re going to be reluctant to do the hard work necessary to build a healthy corporate culture the particular sirens that plague you are a function of what industry you work in we ve identified seven from the field that we know best software development and we present them below along with our own responses sevenfalsehopes ofsoftwaremanagement there is some new trick you ve missed that could send pro ductivity soaring peopleware response you are simply not dumb enough to have missed something so fundamental you are continually investigat ing new approaches and trying out the ones that make the most sense none of the measures you ve taken or are like ly to take can actually make productivity soar what they do though is to keep everybody healthy people like to keep their minds engaged to learn and to improve the line that there is some magical innovation out there that you ve missed is a pure fear tactic employed by those with a vested interest in selling it other managers are getting gains of one hundred percent or two hundred percent or more response forget it the typical magical tool that touted to you is focused on the coding and testing part of the life cycle but even if coding and testing went away entirely you couldn t expect a gain of one hundred percent there is still all the analysis negotiation specification training acceptance testing conversion and cutover to be done technology is moving so swiftly that you re being passed by response yes technology is moving swiftly but the high tech illusion again most of what you re doing is not truly high tech work while the machines have changed enor mously the business of software development has been rather static we still spend most of our time working on requirements and specification the low tech part of our work productivity within the software industry has improved by three to five percent a year only marginally better than the steel or automobile industry changing languages will give you huge gains response languages are important because they affect the way you think about a problem but again they can have impact only on the implementation part of the project laetrile because of their exaggerated claims some of our newer lan guages qualify as laetrile sure it may be better to do a new application in powerbuilder for example rather than cobol but even before powerbuilder there were better ways than cobol niche tools that make inquiry and update pretty easy unless you ve been asleep at the switch for the past few decades change of a language won t do much for you it might give you a five percent gain nothing to sneeze at but not more because of the backlog you need to double productivity immediately response the much talked about software backlog is a myth we all know that projects cost a lot more at the end than what we expected them to cost at the beginning so the cost of a system that didn t get built this year because we didn t have the capacity for it is optimistically assumed to be half of what it would actually cost to build or even less the typical project that stuck in the mythical backlog is there because it has barely enough benefit to justify building it even with the most optimistic cost assumptions if we knew its real cost we d see that project for what it is an economic loser it shouldn t be in the backlog it should be in the reject pile you automate everything else isn t it about time you automated away your software development staff response this is another variation of the high tech illu sion the belief that software developers do easily automat able work their principal work is human communication to organize the users expressions of needs into formal pro cedure that work will be necessary no matter how we change the life cycle and it not likely to be automated peopleware your people will work better if you put them under a lot of pressure response they won t they ll just enjoy it less so far all this is rather negative if leaning on people is counterproductive and installing the latest technological doodad won t help much either then what is the manager supposed to do this is management in my early years as a developer i was privileged to work on a project managed by sharon weinberg now president of the codd and date consulting group she was a walking example of much of what i now think of as enlightened management one snowy day i dragged myself out of a sickbed to pull together our shaky sys tem for a user demo sharon came in and found me propped up at the console she disappeared and came back a few minutes later with a container of soup after she d poured it into me and buoyed up my spirits i asked her how she found time for such things with all the management work she had to do she gave me her patented grin and said tom this is management tdm sharon knew what all good instinctive managers know the manager function is not to make people work but to make it pos sible for people to work part ii the office environment in order to make it possible for people to work you have to come to grips with those factors that sometimes make it the causes of lost hours and days are numerous but not so different from one another they are mostly failures in one form or another of the environment that the organization has provided to help you work the phone rings off the hook the printer service man stops by to chat the copier breaks down the chap from the blood drive calls to revise donation times personnel continues to scream for the updated skills survey forms time sheets are due at p mu lots more phone calls come in and the day is gone some days you never spend a productive minute on anything having to do with getting actual work done it wouldn t be so bad if all these diversions affected the man ager alone while the rest of the staff worked on peacefully but as you know it doesn t happen that way everybody work day is plagued with frustration and interruption entire days are lost and nobody can put a finger on just where they went if you wonder why almost everything is behind schedule consider this there are a million ways to lose a work day but not even a single way to get one back in part ii we ll look into some of the causes of lost time and propose measures that you can take to create a healthy work con ducive environment chapter the furniture police suppose that in addition to your present duties you were made responsible for space and services for your people you would have to decide on the kind of workplace for each person and the amount of space and expense to be allocated how would you go about it you d probably want to study the ways in which people use their space the amount of table space required and the number of hours in a day spent working alone working with one other person and so forth you d also investigate the impact of noise on people effectiveness after all your folks are intel lect workers they need to have their brains in gear to do their work and noise does affect their ability to concentrate for each of the observed kinds of disturbance you d look for any easy mechanical way to protect your workers given a reasonably free hand you would investigate the advantages of closed space one and two and three person offices versus open space this would allow you to make a sensible trade off of cost against privacy and quiet finally you would take into account people social needs and provide some areas where a conversa tion could take place without disturbing others it should come as no surprise to you that the people who do control space and services for your company particularly if it a large company don t spend much time thinking about any of the concerns listed above they don t collect any raw data they don t strive to understand complex issues like productivity part peopleware of the reason for this is that they are not themselves doing the kind of work likely to suffer from a poor environment they often constitute a kind of furniture police whose approach to the problem is nearly the opposite of what your own would be the police mentality the head of the furniture police is that fellow who wanders through the new office space the day before your staff is supposed to move in with thoughts like these running through his head look at how beautifully uniform everything is you have no way to tell whether you re on the fifth floor or the sixth but once those people move in it will all be ruined they ll hang up pictures and individualize their little modules and they ll be messy they ll probably want to drink coffee over my lovely carpet and even eat then lunch right here shudder oh dear oh dear oh dear this is the person who promulgates rules about leaving each desk clean at night and prohibiting anything to be hung on the parti tions except perhaps a company calendar the furniture police at one company we know even listed a number for spilled coffee on the emergency numbers decal affixed to every phone we were never there when anyone called the number but you could proba bly expect white coated maintenance men to come careening through the halls in an electric cart with flashing lights and a siren going ooogah ooogah while on break at a seminar a fellow told me that his company doesn t allow anything to be left on the desk at night except for a five by seven photo of the worker family anything else and in the morning you ll find stuck to your desktop a nasty note on corporate let terhead yet from the furniture police one guy was so offended by these notes that he could barely restrain his anger knowing how he felt his fellow workers played a joke on him they bought a picture frame from the local five and dime store choosing one with a photograph of an all american family as a sample then they replaced the photo of his own family with the other under the photo was what looked like a note the furniture police from the furniture police stating that since his family didn t pass muster by the corporate standards he was being issued an official company family photo to leaveonhisdesk trl the uniform plastic basement to get a better feeling for the police mentality look at the floor plan of figure now becoming common in organizations all over america figure typical office floor plan this scheme deals forthiightly with the complicated question of who should have windowed space no one the trouble with windows is that there aren t enough of them to give one to every worker if some people have windows and others do not you ll be able to tell that you re in george workspace for instance by simple obser vation we can t have that now can we but look at the side effect the most frequently traveled paths from elevator to cubicle or from cubicle to cubicle do not pass in peopleware front of any window where such floor plans are used the win dows are not utilized at all the window corridors are always empty we first encountered the window corridor plan on the twentieth floor of a new skyscraper there were magnificent views in every direction views that virtually nobody ever saw the people in that building may as well have worked in a basement basement space is really preferable from the point of view of the furniture police because it lends itself more readily to uniform layouts but people work better in natural light they feel better in windowed space and that feeling translates directly into higher qual ity of work people don t want to work in a perfectly uniform space either they want to shape their space to their own convenience and taste these inconvenient facts are typical of a general class of in conveniences that come from dealing with human workers visiting a few dozen different organizations each year as we do quickly convinces you that ignoring such inconvenient facts is intrinsic to many office plans almost without exception the workspaces given to intellect workers are noisy interruptive unpri vate and sterile some are prettier than others but not much more functional no one can get any work done there the very person who could work like a beaver in a quiet little cubbyhole with two large folding tables and a door that shuts is given instead an ez whammo modular cubicle with seventy three plastic appurte nances nobody shows much interest in whether it helps or hurts effectiveness all this may seem a bit harsh on those solid citizens who plan america office space if you think so consider one last manifes tation of the mind set of these planners it is something so mon strous that you have to wonder why it tolerated at all the compa ny paging system hard as this may be to believe some companies actually use a public address system to interrupt perhaps thousands of workers people who are trying to think in order to locate one bong static attention attention paging paul portula ca will paul portulaca please call the paging center if you position yourself well you can sometimes see thirty or forty salaried workers raise their heads at the initial bong and listen po litely through the whole message then look down again wondering what they were doing before they were interrupted police mentality planners design workplaces the way they would design prisons optimized for containment at minimal cost we have unthinkingly yielded to them on the subject of workplace design yet for most organizations with productivity problems there the furniture is no more fruitful area for improvement than the workplace as long as workers are crowded into noisy sterile disruptive space it not worth improving anything but the chapter you never get anything done around here between and part of the folklore among development workers in all sectors of our economy is overtime is a fact of life this implies that the work can never get done in the amount of time worth allocating for it that seems to us a rather dubious proposition overtime is cer tainly a fact of life in the software industry for example but that industry could hardly have come through a period of such phenom enal prosperity if the software built on the whole weren t worth a lot more than was paid for it how to explain then the fact that soft ware people as well as workers in other thought intensive positions are putting in so many extra hours a disturbing possibility is that overtime is not so much a means to increase the quantity of work time as to improve its aver age quality you hear evidence that this is true in such frequently repeated statements as these i get my best work done in the early morning before anybody else arrives in one late evening i can do two or three days worth of work the office is a zoo all day but by about p m things have quieted down and you can really accomplish something you never get anything done around here to be productive people may come in early or stay late or even try to escape entirely by staying home for a day to get a critical piece of work done one of our seminar participants reported that her new boss wouldn t allow her to work at home so on the day before an important report was due she took a sick day to get it done staying late or arriving early or staying home to work in peace is a damning indictment of the office environment the amazing thing is not that it so often impossible to work in the workplace the amazing thing is that everyone knows it and nobody ever does anything about it a policy of default a california company that i consult for is very much concerned about being responsive to its people last year the company managementconducteda survey in which all programmers more than a thousand were asked to list the best and the worst aspects of their jobs the manager who ran the survey was very exci ted about the changes the company had undertaken he told me that the number two problem was poor com munication with upper management having learned that from the survey the company set up quality cir cles gripe sessions and other communication pro grams i listened politely as he described them in detail when he was done i asked what the number one problem was the environment he replied people were upset about the noise i asked what steps the company had taken to remedy that problem oh we couldn t do anything about that he said that outside our control jdm all the more discouraging is that the manager wasn t even particularly embarrassed about failing to take steps to improve the environment it was as though the programmers had complained that there was too much gravity and management had decided after due reflection that they couldn t really do much about it it was a problem whose solution was beyond human capacity this is a policy of total default changing the environment is not beyond human capacity granted there is a power group in almost every company a furni peopleware ture police group that has domain over the physical environment but it not impossible to make them see reason or to wrest control away from them for the rest of this chapter we ll present some of the reasons why you re going to have to do exactly that in subse quent chapters we ll give some hints about how to go about it coding war games observed productivity factors beginning in we have conducted some sort of a public pro ductivity survey each year so far more than three hundred organi zations worldwide have participated in these studies from on we have run our annual survey as a sort of public competition in which teams of software implementors from different organizations compete to complete a series of benchmark coding and testing tasks in minimal time and with minimal defects we call these competi tions coding war games here how they work the basic competing unit is a pair of implementors from the same organization the pair members do not work together but in fact members work against each other as well as against all the other pairs both pair members perform exactly the same work designing coding and testing a medium sized program to our fixed specification as they go through the exercise participants record the time spent on a time log after all participant testing is completed the products are sub jected to our standard acceptance test participants work in their own work areas during normal work hours using the same languages tools terminals and com puters that they use for any other project all results are kept confidential from to more than developers from com panies have participated in the games the benefit to the individual is learning how he or she compares with the rest of the competitors you never get anything done around the benefit to the company is learning how well it does against other companies in the sample and the benefit to us is learning a lot about what factors affect productivitys factors discussed in the of this chapter individual differences one of the first results of the coding wars was proof of a huge dif ference between competing individuals of course this had been observed before figure for example is a composite of the findings from three different sources on the extent of variation among individuals figure productivity variation among individuals three rales of thumb seem to apply whenever you measure varia tions in performance over a sample of individuals count on the best people outperforming the worst by about count on the best performer being about times better than the median performer count on the half that are better than median performers out doing the other half by more than peopleware the top quartile those who did the exercise most rapidly and effectively work in space that is substantially different from that of the bottom quartile the top performers space is quieter more pri vate better protected from interruption and there is more of it what did we prove the data presented above does not exactly prove that a better work place will help people to perform better it may only indicate that people who perform better tend to gravitate toward organizations that provide a better workplace does that really matter to you in the long run what difference does it make whether quiet space and privacy help your current people to do better work or help you to attract and keep better people if we proved anything at all it that a policy of default on workplace characteristics is a mistake if you participate in or man age a team of people who need to use their brains during the work day then the workplace environment is your business it isn t enough to observe you never get anything done around here between and and then turn your attention to something else it dumb that people can t get work done during normal work hours it time to do something about it chapter saving money on space if your organization is anything like those studied in our last three annual surveys the environmental trend is toward less privacy less dedicated space and more noise of course the obvious reason for this is cost a penny saved on the workspace is a penny earned on the bottom line or so the logic goes those who make such a judgment are guilty of performing a cost benefit study without benefit of studying the benefit they know the cost but haven t any idea what the other side of the equation may be sure the savings of a cost reduced workplace are attractive but compared to what the obvious answer is that the savings have to be compared to the risk of lost effectiveness given the current assault on workplace costs it surprising how little the potential savings are compared to the potential risk the entire cost of workspace for one developer is a small percent age of the salary paid to the developer how small depends on such factors as real estate values salary levels and lease versus buy tactics in general it varies in the range from to percent for a programmer analyst working in company owned space you should expect to pay directly to the worker for every dollar you spend on space and amenities if you add the cost for employee benefits the total investment in the worker could easily be times the cost of his or her workplace the ratio implies that workplace cost reduction is risky attempts to save a small portion of the one dollar may cause you to sacrifice a large portion of the twenty the prudent manager could peopleware not consider moving people into cheaper noisier and more crowd ed quarters without first assessing whether worker effectiveness would be impaired so you might expect that the planners who have undertaken a decade long program to change our office space into the voguish open plan format must first have done some very careful productivity analysis not to do so would have demon strated an irresponsible unconcern for the environment a plague upon the land irresponsible unconcern for the environment is unfortunately the norm for our times we show it in the despoliation of our natural resources so why not in workplace design in a prophetic science fiction story john brunner describes pollution of the air soil and water continuing through the end of the twentieth century no mat ter how bad the pollution gets almost no one complains like a vast herd of imperturbable sheep the inhabitants of brunner world try to ignore the problem until finally all possibility for survival is lost then and only then do they take notice brunner called his book the sheep look up american office workers have barely looked up while their work quarters have been degraded from sensible to silly not so long ago they worked in two and three person offices with walls doors and windows you remember walls doors and windows don t you in such space one could work in quiet or conduct meetings with colleagues without disrupting neighbors then without warning open plan seating was upon us like a plague upon the land the advocates of the new format produced not one shred of evidence that effectiveness would not be impaired they really couldn t meaningful measurement of productivity is a complex and elusive thing it has to be performed differently in each different work sector it takes expertise careful study and lots of data collection the people who brought us open plan seating simply weren t up to the task but they talked a good game they sidestepped the issue of whether productivity might go down by asserting very loudly that the new office arrangement would cause productivity to go up and up a lot by as much as three hundred percent they published articles many of them crafted from the purest sculpted smoke they gave their pronouncements impressive titles like this one from data management magazine open plan dp environment saving money on space boosts employee productivity after that promising title the author got right to the heart of the matter the fundamental areas of consideration in designing an open plan office within an information processing en vironment are the system electrical distribution capabilities computer support capabilities and manu facturer and dealer service period that it that all of the fundamental areas of considera tion no mention of the fact that a person is going to be trying to work in that space also missing from that article and from others like it is any notion of what employee productivity is all about there was no evidence in the data management article to support the title the only method we have ever seen used to confirm claims that the open plan improves productivity isproofbyrepeatedassertion we interrupt this diatribe to bring you a few facts before drawing the plans for its new santa teresa facility ibm violated all industry standards by carefully studying the work habits of those who would occupy the space the study was designed by the architect gerald mccue with the assistance of ibm area man agers researchers observed the work processes in action in current workspaces and in mock ups of proposed workspaces they watched programmers engineers quality control workers and managers go about their normal activities from their studies they concluded that a minimum accommodation for the mix of people slated to occupy the new space would be the following square feet of dedicated space per worker square feet of work surface per person noise protection in the form of enclosed offices or six foot high partitions they ended up with about half of all pro fessional personnel in enclosed one and two person offices peopleware the rationale for building the new laboratory to respect these minimums was simple people in the roles studied needed the space and quiet in order to perform optimally cost reduction to provide workspace below the minimum would result in a loss of effective ness that would more than offset the cost savings other studies have looked into the same questions and come up with more or less the same answers the mccue study was different only in one respect ibm actually followed the recommendations and built a workplace where people can work we predict this company will gofar how does the rest of the world match up to ibm minimum standard workplace figure shows a distribution of dedicated space per person computed across participants in our and surveys dedicated space square feet person figure range of dedicated floor space only percent of participants had square feet or more of workspace only percent of participants worked in enclosed offices or with greater than foot high partitions there were more participants in the to square foot group than in the square foot group with less than square feet you re trying to work in a total floor space less than the table space provided at santa teresa saving money on space across the whole coding war games sample percent complained that their workplace was not acceptably quiet per cent complained that it wasn t sufficiently private percent report ed that they had a workplace at home that was better than the work place provided by the company workplace quality and product quality companies that provide a small and noisy workplace are comforted by the belief that these factors don t really matter they explain away all the complaints about noise for instance as workers cam paigning for the added status of bigger more private space after all what difference could a little noise make it just something to help keep people awake in order to determine whether attitude toward noise level had any correlation to work we divided our sample into two parts those who found the workplace acceptably quiet and those who didn t then we looked at the number of workers within each group who had completed the entire exercise without a single defect workers who reported before the exercise that their workplace was acceptably quiet were one third more likely to deliver zero defect work as the noise level gets worse this trend seems to get stronger for example one company that was represented by participants had an unacceptable noise rating percent higher than average at that company those who did zero defect work came disproportion ately from the subset who found the noise level acceptable zero defect workers percent reported noise level okay one or more defect workers percent reported noise level okay again as with the other environmental correlations we asked that participants assess the noise level in their environments before per forming the exercise note that we made no objective measurements of noise levels we simply asked people whether they found the noise level accept able or not as a result we cannot distinguish between those who worked in a genuinely quiet workplace and those who were well adapted to not bothered by a noisy workplace but when a worker complains about noise he telling you he doesn t fit into either of peopleware those fortunate subsets he telling you that he is likely to be defect prone you ignore that message at your peril a discovery of nobel prize significance some days people are just more highly perceptive than other days for us a landmark day for perceptiveness was february when we began to notice a remarkable relationship between people density and dedicated floor space per person as the one goes up the other seems to go down careful researchers that we are we immediately began to document the trend in a study of companies throughout the free world we confirmed a virtually perfectinverse relationship betweenthetwo figure the demarco llster effect imagine our excitement as the data points were collected we expe rienced some of the thrill that ohm must have felt when discovering his law this was truly the stuff of which nobel prizes are made remember that you saw it here first worker density say workers per thousand square feet is inversely proportional to dedicated space per person if you re having trouble seeing why this matters you re not thinking about noise noise is directly proportional to density so saving money on space halving the allotment of space per person can be expected to double the noise even if you managed to prove conclusively that a pro grammer could work in square feet of space without being hope lessly space bound you still wouldn t be able to conclude that square feet is adequate space the noise in a square foot matrix is more than three times the noise in a square foot matrix that could mean the difference between a plague of product defects and none at all hiding out when the office environment is frustrating enough people look for a place to hide out they book the conference rooms or head for the library or wander off for coffee and just don t come back no they are not meeting for secret romance or plotting political coups they are hiding out to work the good news here is that your people really do need to feel the accomplishment of work completed they will go to great extremes to make that happen when the crunch is on people will try to find workable space no matter where in my college years at brown university the trick for getting through the mad season when all the papers came due was to find some place quiet to work at brown we had a system of carrels in the library stacks the only acceptable interruption there was a fire alarm and it had to be for a real tire we got to be experts at finding out of the way carrels where no one would ever think to look for us the fifth floor carrels of the bio library were my favorite but a friend even went so far as to work in the crypt below the american library yes the crypt complete with the remains of the woman who had endowed the build ing it was cool it was marble and as my friend reported it was quiet very quiet trl if you peek into a conference room you may find three people working in silence if you wander to the cafeteria mid afternoon you re likely to find folks seated one to a table with their work spread out before them some of your workers can t be found at all people are hiding out to get some work done if this rings true to your organization it an indictment saving money on space may be costing you a fortune intermezzo an intermezzo is a fanciful digression inserted between the pages of an otherwise serious work oh well fairly serious work productivity measurement and unidentified flying objects why can t we just measure productivity in good and bad workplaces and finally nail down the relationship between the environment and worker effectiveness that approach would cer tainly be suitable for an assembly line but when the work being measured is of a more intellectual nature it not so obvious mea surement of intellect worker productivity suffers from a reputation of being a soft science in some people minds it little better than the study of unidentified flying objects an experiment to test the effect of the workplace on produc tivity is easy enough to design measure the amount of work completed in the new work place measure the cost of doing that work compare the size and cost in the new workplace to the size and cost in the old design was easy the implementation is harder for instance how do you assess the amount of work involved in a market study or in a new circuit design or in the development of a new loan policy there may be some emerging standards as there are in the software industry for instance but these are sure to require extensive local data collection and the building of in house expertise most organi intermezzo zations don t even attempt to measure the amount of intellect work performed they don t measure costs very effectively either there may be statistics on the total quantity of hours applied to a given problem within an organization but no indication of the quality of these hours more about this in chapter and even if organizations could measure size and cost in a new workplace they would have no past figures to compare them to managers are likely to furrow their brows over this problem sigh and conclude that variation in productivity is beyond comprehension but it really not as bad as that gilb law two years ago at a conference in london i spent an afternoon with tom gilb the author of software metrics and dozens of published papers on measure ment of the development process i found that an easy way to get him heated up was to suggest that something you need to know is unmeasurable the man was offended by the very idea he favored me that day with a description of what he considered a fundamental truth about measurability the idea seemed at once so wise and so encouraging that i copied it verbatim into my journal under the heading of gilb law anything you need to quantify can be measured in some way that is superior to not measuring it at all gilb law doesn t promise you that measurement will be free or even cheap and it may not be perfect just better than nothing tdm of course it possible to measure productivity if you con voke a group of people doing the same or similar work and give them a day to work out a sensible self measurement scheme they will come up with something that confirms gilb law the num bers they then generate will give them some way to tune their own performance and when combined with quality circles or some other peer review mechanism a way to learn from each other methods the averages computed over the group will give management a reli peopleware able indicator of the effect of such parameters as improvement in the office environment in the field that we know best software construction there are any number of workable productivity measurement schemes such as those listed in the notes there is even a service that will come in and assess your productivity and show you where you stand com pared to the rest of the industry an organization that can t make some assessment of its own programming productivity rate just hasn t tried hard enough but you can t afford not to know suppose there were a foolproof productivity measurement tool and it was being applied to your people work this very moment sup pose the measurers came in to tell you that your productivity was in the top five percent of organizations doing your kind of work you d be pleased you d wander around the halls with a secret smile thinking warm thoughts about your people i suspected they were pretty good but this is terrific news ooops the measurers have just come back to tell you that they must have been holding the graph upside down when they gave you the first report you re actually in the bottom five percent now your day is ruined you find yourself thinking i might have known it all along who could expect to get any work done with turkeys like these on the staff in the one case you re ecstatic in the other despondent but in neither case are you particularly sur prised you re not likely to be surprised no matter what the news is because you haven t the foggiest idea what your productivity is given that there are ten to one differences from one organ ization to another you simply can t afford to remain ignorant of where you stand your competition may be ten times more effective than you are in doing the same work if you don t know it you can t begin to do something about it only the market will under stand it will take steps of its own to rectify the situation steps that do not bode well for you measuring with your eyes closed work measurement can be a useful tool for method improvement motivation and enhanced job satisfaction but it is almost never used for these purposes measurement schemes tend to become threatening and burdensome intermezzo in order to make the concept deliver on its potential manage ment has to be perceptive and secure enough to cut itself out of the loop that means the data on individuals is not passed up to man agement and everybody in the organization knows it data collected on the individual performance has to be used only to benefit that individual the measurement scheme is an exercise in self assessment and only the sanitized averages are made available to the boss this concept is a hard one to swallow for many managers they reason that they could use the data to do some aspects of their work more effectively precision promotion for example or even precision firing their company has paid to have the data collected so why shouldn t it be made available to them but collection of this very sensitive data on the individual can only be effected with the active and willing cooperation of the individual if ever its con fidentiality is compromised if ever the data is used against even one individual the entire data collection scheme will come to an abrupt halt the individuals are inclined to do exactly the same things with the data that the manager would do they will try to improve the things they do less well or try to specialize in the areas where they already excel in the extreme case an individual may even fire himself in order to stop depending on skills that have been found to be deficient the manager doesn t really need the individual data in order to benefit from it chapter braintimeversus body time as part of the santa teresa pre construction study described in chapter mccue and his associates looked into the amounts of time that developers spend in different work modes for a typical day they concluded that workers divide their time as follows table how developers spend their time percent work mode of time working alone working with one other person working with two or more people the significance of this table from a noise standpoint should be evident thirty percent of the time people are noise sensitive and the rest of the time they are noise generators since the work place is a mixture of people working alone and people working together there is a clash of modes those working alone are partic ularly inconvenienced by this clash though they represent a mi nority at any given time it a mistake to ignore them for it is dur brain time versus body time ing their solitary work periods that people actually do the work the rest of the time is dedicated to subsidiary activities rest and chatter flow during single minded work time people are ideally in a state that psychologists call flow flow is a condition of deep nearly meditative involvement in this state there is a gentle sense of eu phoria and one is largely unaware of the passage of time i began to work i looked up and three hours had passed there is no consciousness of effort the work just seems to well flow you ve been in this state often so we don t have to describe it to you not all work roles require that you attain a state of flow in order to be productive but for anyone involved in engineering design development writing or like tasks flow is a must these are high momentum tasks it only when you re in flow that the work goes well unfortunately you can t turn on flow like a switch it takes a slow descent into the subject requiring fifteen minutes or more of concentration before the state is locked in during this immersion period you are particularly sensitive to noise and interruption a disruptive environment can make it difficult or impossible to attain flow once locked in the state can be broken by an interruption that is focused on you your phone for instance or by insistent noise attention paging paul portulaca will paul portulaca please call extension each time you re interrupted you require an addi tional immersion period to get back into flow during this immer sion you re not really doing work an endless state of no flow if the average incoming phone call takes five minutes and your reimmersion period is fifteen minutes the total cost of that call in flow time work time lost is twenty minutes a dozen phone calls use up half a day a dozen other interruptions and the rest of the work day is gone this is what guarantees you never get any thing done around here between and 64 peopleware just as important as the loss of effective time is the accompa nying frustration the worker who tries and tries to get into flow and is interrupted each time is not a happy person he gets tanta lizingly close to involvement only to be bounced back into aware ness of his surroundings instead of the deep mindfulness that he craves he is continually channeled into the promiscuous changing of direction that the modern office tries to force upon him put yourself in the position of the participant who filled out her coding war games time sheet with these entries work period type of what interruption caused the from to work end of this work period coding phone call 23 coding boss stopped in to chat 29 coding question from colleague 39 coding phone call 44 coding phone call figure segment of a cwg time sheet a few days like that and anybody is ready to look for a new job if you re a manager you may be relatively unsympathetic to the frustrations of being in no flow after all you do most of your own work in interrupt mode that management but the people who work for you need to get into flow anything that keeps them from it will reduce their effectiveness and the satisfaction they take in their work it will also increase the cost of getting the work done time accounting based on flow chances are your company present time accounting system is based on a conventional model it assumes that work accomplished is proportional to the number of paid hours put in when workers fill out their time sheets in this scheme they make no distinction between hours spent doing meaningful work and hours of pure frustration so they re reporting body time rather than brain time to make matters worse the task accounting data is also used for payroll purposes this compels employees to make sure that the total number of hours logged always balances out to some predeter brain time versus body time mined total for the week regardless of how much overtime or undertime they put in the resultant compilation of official fictions may be acceptable to the payroll department it is equivalent to the worker responding present to a roll call but for any productivity assessment or analysis of where the money went this record is too badly tainted to be useful the phenomena of flow and immersion give us a more realis tic way to model how time is applied to a development task what matters is not the amount of time you re present but the amount of time that you re working at full potential an hour in flow really accomplishes something but ten six minute work periods sand wiched between eleven interruptions won t accomplish anything the mechanics of a flow accounting system are not very com plex instead of logging hours people log uninterrupted hours in order to get honest data you have to remove the onus from logging too few uninterrupted hours people have to be assured that it not their fault if they can only manage one or two uninterrupted hours a week rather it the organization fault for not providing a flow conducive environment of course none of this data can go to the payroll department you ll still have to retain some body present time reporting for payroll purposes a task accounting scheme that records flow hours instead of body present hours can give you two huge benefits first it focus es your people attention on the importance of flow time if they learn that each work day is expected to afford them at least two or three hours free from interruption they will take steps to protect those hours the resultant interrupt consciousness helps to protect them from casual interruption by peers second it creates a record of how meaningful time is applied to the work if a product is projected to require three thousand flow hours to complete then you ve got a valid reason to believe you re two thirds done when two thousand flow hours have been logged against it that kind of analysis would be foolish and dangerous with body present hours the e factor if you buy the idea that a good environment ought to afford workers the possibility of working in flow the collection of uninterrupted hour data can give you some meaningful metric evidence ofjust how good or bad your environment is whenever the number of peopleware uninterrupted hours is a reasonably high proportion of total hours up to approximately forty percent then the environment is allowing people to get into flow when they need to much lower numbers imply frustration and reduced effectiveness we call this metric the environmental factor or e factor e factor uninterrupted hours body present hours a somewhat surprising result of collecting e factor data is that factors vary within an organization from site to site for exam ple we recorded e factors as high as and as low as in one large government agency the agency head assured us that the physical environment had to remain as it was no matter how bad because characteristics of the workplace were determined by government policy and by civil service level in spite of this we found some sites where workers were housed in a tight noisy open office plan and others where workers doing the same job and at the same level worked in pleasant four person offices not so surpris ing was the finding that e factors were markedly higher in the four person offices e factors can be threatening to the status quo perhaps you d better not even start collecting the data if you report for a sensible space and for a cost reduced space for example people are likely to conclude that the cost reduction didn t make much sense workers in the space will have to put in times as much body present time to do a given piece of work as those in the space that means having work done in the cost reduced space could result in a performance penalty that is far greater than the space savings clearly such a heretical line of reasoning must be suppressed otherwise we jeopardize all those wonderful savings to be gained by tightening up your workers spaces burn this book before anyone else sees it a garden of bandannas when you first start measuring the e factor don t be surprised if it hovers around zero people may even laugh at you for trying to record uninterrupted hours there is no such thing as an uninter brain time versus body time rupted hour in this madhouse don t despair remember that you re not just collecting data you re helping to change people attitudes by regularly noting uninterrupted hours you are giving official sanction to the notion that people ought to have at least some interrupt free time that makes it permissible to hide out to ignore the phone or to close the door if sigh there is a door at one of our client sites there was a nearly organic phe nomenon of red bandannas on dowels suddenly sprouting from the desks after a few weeks of e factor data collection no one in power had ever suggested that device as an official do not disturb signal it just happened by consensus but everyone soon learned its significance and respected it of course there have always been certain cranky souls who have stuck up do not disturb signs peer pressure makes it hard for most of us to show that interruptions aren t welcome even for a part of the day a little emphasis on the e factor helps to change the corporate culture and make it acceptable to be uninterruptable thinking on the job in my years at bell labs we worked in two person offices they were spacious quiet and the phones could be diverted i shared my office with wendl thomis who went on to build a small empire as an electronic toy maker in those days he was working on the ess fault dictionary the dictionary scheme relied upon the notion of n space proximity a concept that was hairy enough to challenge even wendl powers of concentration one afternoon i was bent over a pro gram listing while wendl was staring into space his feet propped up on the desk our boss came in and asked wendll what are you doing wendl said i m thinking and the boss said can t you do that at home tdm the difference between that bell labs environment and a typical modern day office plan is that in those quiet offices one at least had the option of thinking on the job in most of the office space we encounter today there is enough noise and interruption to make any serious thinking virtually impossible more is the shame your peopleware people bring their brains with them every morning they could put them to work for you at no additional cost if only there were a small measure of peace and quiet in the workplace chapter the telephone when you begin to collect data about the quality of work time your attention is automatically focused on one of the principal causes of interruption the incoming telephone call it nothing to field fifteen calls in a day it may be nothing but because of the asso ciated reimmersion time it can use up most of that day when the day is over and you re wondering where the time went you can seldom even remember who called you or why even if some of the calls were important they may not have been worth interrupting your flow but who got the nerves to wait out a ringing phone the very thought of it makes you tense between the shoulders visit to an alternate reality now just relax and imagine a less complicated world in which the phone has not yet been invented in such a world you write a note to propose lunch or a meeting and you get a note in response everyone plans ahead a little bit more it common to take half an hour in the morning to read and answer your mail there are no loud bells in your life wednesday mornings in this alternate reality are dedicated to meetings of your company pension trust investment committee imagine for the moment you are one of the employee representatives charged with watching where the money is placed on this particu lar wednesday an inventor is scheduled to make a presentation to the committee the inventor has plans to change the world if only you ll invest in his new contraption his name is a g bell peopleware ladies and gentlemen this is the bellophone the man unwraps a large black box with a crank on the side and an enormous bell attached to the top this is the future we re going to put one of these on every desk in america homes too it will get to the point where people can hardly imagine a world without them as he warms up to his subject he begins gesticulating enthusiastically and hopping around the room to make his points bellophones everywhere you look all of them hooked up together with wires under the street or overhead and now this is the really exciting part you can get your bellophone specifically connected to somebody else bellophone even though it may be all the way across the city or maybe in some other city and when you ve connected it just by entering the code you can make the bell ring on the other fellow machine not just some rinky dink bell either but a real heart stopper he sets up a second device and connects it to the first on the other side of the room by manipulating a dial on the face of the first he causes the other machine to come alive it gives off a loud brrrrinnnnnnnggggggg after half a second it rings again and then again and again deafeningly now what a fellow got to do to stop this ringing he got to race over to his bellophone and pick up the receiver he picks up the receiver on the ringing device and hands it to one of the committee members then he bounds back to the other side of the room and starts shouting into the mouthpiece of the originating device hello hello can you hear me see that i ve got his complete attention now i can sell him something or get him to lend me money or try to change his religion or whatever i want the committee is stunned you raise your hand and venture a question since nobody could possibly have missed the first ring why bother to repeat it ah that the beauty of the bellophone says a g it never gives you the chance to wonder whether you want to answer it or not no matter what you re involved in at the time it rings no matter how engrossed you are you drop everything to answer it otherwise you know it will just keep on ringing we re going to sell billions of these things and never ever allow any to be sold that ring only once the committee goes into a huddle but it doesn t take very long to come up with a judgment you all decide without a dissent ing voice to throw this turkey out the door the device is so disruptive that if you were ever dumb enough to allow it to be the telephone installed nobody would ever get any work done around the office a few years effect of the bellophone and we d all be reduced to buying goods from taiwan and korea and our country might even have a negative balance of trade tales from the crypt of course there no turning the calendar back the telephone is here to stay you can t get rid of it nor would you probably want to you certainly can t remove phones from people desks without causing them to revolt but there are certain steps that can be taken to minimize the negative impact of interruptive calls the most im portant of these is to realize how much we have allowed the tele phone to dominate our time allocation do you often interrupt a discussion with co workers or friends to answer a phone of course you do you don t even consider not answering the phone yet what you re doing is a violation of the common rules of fairness taking people out of order just because theyinsistloudly bbbrrrrhinnnnggggg onyourattention not only do you do this to others you let them do it to you and you re so inured to this abuse that you hardly take note of it only in the most outrageous cases is it clear that something is definitely wrong with such behavior nearly twenty years ago i was standing in line at the parts department of the new york dealer for morgan motorcars limited i had a non functioning morgan the only kind and was hoping to get some new carburetor needles people who drive british sports cars are undoubtedly masochists but the treatment in that parts line was just too much the clerk took one phone call after another while everyone in line waited when i got to the head of the line he took four calls in quick succession before i could get in a single word i began thinking why should people calling from the comfort of their homes get priority over those of us standing here in this stupid line why should those mere window shoppers be taken ahead of customers with money in their hands ready to buy in a state of pure red rage i suggested that he let the phone ring for a while and take people ahead of bells to my peopleware surprise he was more annoyed by my behavior than i was by his he informed me very huffily that phones get priority over people and that was all there was to it my not liking it was as pointless as not liking the atlantic ocean the facts of life weren t going to change just to suit me tdm it is natural that the telephone should have reshaped somewhat the way we do business but it ought not to have blinded us to the effects of the interruptions at the least managers ought to be alert to the effect that interruption can have on their own people who are trying to get something done but often it the manager who is the worst offender one of the programmers in the coding war games wrote on his environmental survey when my boss is out he has his calls switched to me what could that manager have been thinking what was going on in the mind of the systems department head who wrote mis in a memo it has come to my attention that many of you when you are busy are letting your phones ring for three rings and thus get switched over to one of the secretaries with all these interruptions the secretaries can never get any productive work done the official policy here is that when you re at your desk you will answer your phone before the third ring a modified telephone ethic enough is enough the path toward sanity in working conditions is a new attitude toward interruptions and toward the telephone peo ple who are charged with getting work done must have some peace and quiet to do it in that means periods of total freedom from interruptions when they want to work in flow they have to have some efficient acceptable way of ignoring incoming calls acceptable means the corporate culture realizes that people may sometimes choose to be unavailable for interruption by phone efficient means that they don t have to wait out the bell in order to get back to work there are workable schemes to help people free themselves from phones and other interruptions when they find it necessary the telephone some of these cost money and thus will be possible only in organizations whose long term view extends beyond next tuesday when electronic mail was first proposed most of us thought that the great value of it would be the saving in paper that turns out to be trivial however compared to the saving in reimmersion time the big difference between a phone call and an electronic mail message is that the phone call interrupts and the e mail does not the receiver deals with it at his or her own convenience the amount of traffic going through these systems proves that priority at the receiver convenience is acceptable for the great majori ty of business communications after a period of acclimatization workers begin to use electronic mail in preference to intra company calls it doesn t make all the calls go away only most you probably have the technology already most of us now have perfectly acceptable voice mail and e mail the trick isn t in the technology it is in the changing of habits gentle reader please note this recurring theme we have to learn to ask does this news or this question deserve an interrupt can i continue to get work done while i wait for an answer does this message need immediate recognition if not how long can it wait without causing a problem once you ask these questions your best mode of communi cation is usually pretty obvious except for one problem how reliable is the mode for example how long will your e mail sit there will it actually get read or will it go directly to the inbound only filing cabinet in return for some uninterrupted brain time people have to be interruptible for at least part of each day and everybody has to agree that e mail will be checked with reasonable frequency say three times per day or so don t try this yourself have been working with a manager in los angeles for the last few years he manages a system test group which at times runs two or three shifts e mail is the way his team communicates since at any given moment about one third of the team members may be asleep he told me about his morning mail drill as soon as he awake he fires up his laptop to download all his e mail while he showers while he has his juice peopleware and cereal he reads his new mail and decides which pieces need immediate response on his way to work a typical la resident he has about a fifty mile commute he answers his critical pieces when he gets to the office he docks his laptop and sends his responses the trick is he drives in by himself every morning trl more important than any gimmick you introduce is a change in attitude people must learn that it okay sometimes not to answer their phones and they must learn that their time not just the quantity but its quality is important chapter bring back the door there are some prevalent symbols of success and failure in creating a sensible workplace the most obvious symbol of success is the door when there are sufficient doors workers can control noise and mterruptability to suit their changing needs the most obvious symbol of failure is the paging system organizations that regularly interrupt everyone to locate one person are showing them selves to be totally insensitive to the imperatives of a work con ducive environment manipulate these symbols and you not only call attention to your concern for a workable environment you also reap the imme diate associated advantage people can get on with the work but it sounds like a tall order to get rid of the paging system and bring back the door is it beyond our capacity to effect these changes the show isn t over till the fat lady sings the degradation of working conditions that has affected most of us over the past ten years has depended on the consent of the victim that doesn t mean that one such victim could have halted the trend by saying no i won t work in noisy cramped exposed space but it does mean that we as a group haven t hollered loud enough and often enough about the counterproductive side effects of saving money on space while most of us believed that the trend toward noisier tighter space was hurting productivity we kept silent because we lacked peopleware the definitive statistical evidence that proved our case the furniture police of course didn t provide any proof at all to support their contention that people would be just as productive working cheek by jowl as they had been in a more sensible environment they just asserted that it was true we need to learn from them learn to fight fire with fire so the first step toward a sane environment is a program of repeated assertion if you believe that the environment is working against you you ve got to start saying so you ll need to create a forum for other people to chime in too perhaps with a survey of people assessment of their working conditions in one such survey at a client company workers cited seven negative aspects of their work things they thought of as productivity limiters of these the first four were noise related as people begin to realize that they aren t alone in their feel ings environmental awareness increases and with this increased awareness two good things begin to happen first the environ ment improves a bit as people try to be more thoughtful about noise and interruption and second the consent of the victim is with drawn it now becomes harder for upper management to take any other step to improve productivity without first paying some atten tion to the environment don t expect the establishment to roll over and play dead just because you begin your campaign there are at least three counter arguments bound to surface almost immediately people don t care about glitzy office space they re too intelligent for that and the ones who do care are just playing status games maybe noise is a problem but there are cheaper ways to deal with it than mucking around with the physical layout we could just pipe in white noise or muzak and cover up the disturbance enclosed offices don t make for a vital environment we want people to interact productively and that what they want too so walls and doors would be a step in the wrong direction bring back the door we deal with these objections in the next three subsections the issue of glitz it true that people don t care much about glitz in one study after another workers failed to give much weight to decor in choosing for instance among variously colored panels and fixtures the feeling seemed to be that depressing surroundings would be coun terproductive but as long as the office wasn t depressing then you could happily ignore it and get down to work if all we re shooting for is an ignorable workplace then money spent on high fashion decor is a waste the fact that workers don t care a lot about appearances is often misinterpreted to mean that they don t care a lot about any of the attributes of the workplace if you ask them specifically about noise privacy and table space though you ll hear some strongly felt opinions that these characteristics matter a lot this finding is consistent with the idea of an ignorable workplace as ideal one can t ignore a workplace that is forever interrupting paging and gener ally harassing the worker we find it particularly distressing to hear workers concerns about their environment dismissed as status seeking because it more often the case that higher management is guilty of status seek ing in designing the workers space the person who is working hard to deliver a high quality product on time is not concerned with office appearances but the boss sometimes is so we see the para doxical phenomenon that totally unworkable space is gussied up expensively and pointlessly with plush carpets black and chrome furniture corn plants that get more space than workers and elabo rate panels the next time someone proudly shows you around a newly designed office think hard about whether it the func tionality of the space that is being touted or its appearance all too often it the appearance appearance is stressed far too much in workplace design what is more relevant is whether the workplace lets you work or inhibits you work conducive office space is not a status symbol it a necessity either you pay for it by shelling out what it costs or you pay for it in lost productivity peopleware creative space in response to workers gripes about noise you can either treat the symptom or treat the cause treating the cause means choosing iso lation in the form of noise barriers walls and doors and these cost money treating the symptom is much cheaper when you install muzak or some other form of pink noise the disruptive noise is drowned out at small expense you can save even more money by ignoring the problem altogether so that people have to resort to tape recorders and earphones to protect themselves from the noise if you take either of these approaches you should expect to incur an invisible penalty in one aspect of workers performance they will be less creative during the researchers at cornell university conducted a series of tests on the effects of working with music they polled a group of computer science students and divided the students into two groups those who liked to have music in the background while they worked studied and those who did not then they put half of each group together in a silent room and the other half of each group in a different room equipped with earphones and a musical selection participants in both rooms were given a fortran pro gramming problem to work out from specification to no one surprise participants in the two rooms performed about the same in speed and accuracy of programming as any kid who does his arithmetic homework with the music on knows the part of the brain required for arithmetic and related logic is unbothered by music there another brain center that listens to the music the cornell experiment however contained a hidden wild card the specification required that an output data stream be formed through a series of manipulations on numbers in the input data stream for example participants had to shift each number two digits to the left and then divide by one hundred and so on perhaps completing a dozen operations in total although the specification never said it the net effect of all the operations was that each output number was necessarily equal to its input number some people realized this and others did not of those who figured it out the overwhelming majority came from the quiet room many of the everyday tasks performed by professional work ers are done in the serial processing center of the left brain music will not interfere particularly with this work since it the brain holistic right side that digests music but not all of the work is cen bring back the door tered in the left brain there is that occasional breakthrough that makes you say ahah and steers you toward an ingenious bypass that may save months or years of work the creative leap involves right brain function if the right brain is busy listening to strings on muzak the opportunity for a creative leap is lost the creativity penalty exacted by the environment is insidious since creativity is a sometime thing anyway we often don t notice when there is less of it people don t have a quota for creative thoughts the effect of reduced creativity is cumulative over a long period the organization is less effective people grind out the work without a spark of excitement and the best people leave vital space the case against enclosed offices sooner or later gets around to the sterility of working alone but enclosed offices need not be one person offices the two or three or four person office makes a lot more sense particularly if office groupings can be made to align with work groups the worker who needs to spend fifty percent of his time with one other person will spend most of that time with a particular person these two are natural candidates to share an office even in open plan offices co workers should be encouraged to modify the grid to put their areas together into small suites when this is allowed people become positively ingenious in laying out the area to serve all their needs work space meeting space and social space since they tend to be in interaction mode together or simul taneously in flow mode they have less noise clash with each other than they would with randomly selected neighbors the space has a vital quality because interaction is easy and natural a degree of control over their space is viewed as an additional benefit breaking the corporate mold what could be less threatening than a proposal to allow people to reorganize open plan seating into shared suites instead of individual cubicles one of the great benefits of the kind of office system that is no offices that your company may have purchased is its flexibility at least that what the ez whammo panel system brochure says so it should be easy enough to move things around peopleware letting people form suites may seem nonthreatening but we predict that someone in the upper reaches of the organization will hate the idea the problem is that the hallowed principle of uniformity is violated by making everything uniform the owner of a territory exercises and demonstrates control like the gardener who plants seeds exactly under a taut string so that the carrots will grow in a perfect row this manager is threatened by the kind of disorder that nature in this case people human nature prefers the inconvenient fact of life is that the best workplace is not going to be infinitely replicable vital work conducive space for one person is not exactly the same as that for someone else if you let them your people will make their space into whatever they need it to be and the result is that it won t be uniform each person space and each team space will have a definite character of its own if it didn t they d go back and alter it until it did management at its best should make sure there is enough space enough quiet and enough ways to ensure privacy so that people can create their own sensible workspace uniformity has no place in this view you have to grin and bear it when people put up odd pictures or leave their desks a mess or move the furniture around or merge their offices when they ve got it just the way they want it they ll be able to put it out of their minds entirely and get on with the work chapter taking umbrella steps for this final chapter on the office environment we look into characteristics of an ideal workplace trying to shed some light on concerns such as these what kind of space would support your workers best to make them comfortable happy and productive what form of workspace would make these workers feel best about themselves and about their work if you work in a typical noisy and dreadfully uniform corporate space such questions may seem almost cruel but think ing about ideal space is worthwhile someday you may be in a position to make it happen even today you may be called upon to provide some input to the process of workspace improvement it sensible to indulge yourself a bit on the subject of space just to know where you ought to be headed where you ought to be headed in our opinion is toward a workspace that has certain time proven characteristics there is one timeless way of building it is thousands of years old and the same today as it has always been peopleware the great traditional buildings of the past the villages and tents and temples in which man feels at home have always been made by people who were very close to the center of this way it is not possible to make great buildings or great towns beautiful places places where you feel yourself places where you feel alive except by following this way and as you will see this way will lead anyone who looks for it to buildings which are themselves as ancient in their form as the trees and hihs and as our faces are the timeless way ofbuilding christopher alexander architect and philosopher is best known for his observations on the design process he frames his concepts in an architectural idiom but some of his ideas have had influence far beyond the field of architecture alexander book notes on the synthesis of form for example is considered a kind of holy book by designers of all kinds together with his colleagues at the center for environmental structure alexander set out to codify the elements of good architectural design the resul tant work is a three volume set entitled the timeless way ofbuild ing the effect of this work is still being debated alexander believes that most modern architecture is bankrupt and so most modern architects are understandably a bit defensive about the man and his ideas but when you hold these books in your own hands and examine their premises against your own experience it hard not to take alexander side his philosophy of interior space is a compelling one it helps you to understand what it is that has made you love certain spaces and never feel comfortable in others alexander concept of organic order imagine that your organization is about to build a complex of new space what is the first step in this process almost certainly it is development of a master plan in most cases this is a first and fatal deviation from the timeless way of building vital exciting and harmonious spaces are never developed this way the master plan envisions hugeness and grandeur steel and concrete spans modular approaches and replication to make an enormous whole of identical components the result is sterile uniformity and space that doesn t work for anyone except the one ego to whom it stands as a tribute taking umbrella steps most monolithic corporate space can only be understood in terms of its symbolic value to the executives who caused it to be built this is their mark on the firmament the lasting accomplish ment they leave behind they gloat look on my works ye mighty and despair despair of course is exactly all you can do your cubicle infinitely repeated to the horizon leaves you feeling like a numbered cog whether it is transamerica orwellian tower in san francisco or at t madison avenue mausoleum the result is depressingly the same a sense of suffocation to the indi vidual the master plan is an attempt to impose totalitarian order a single and therefore uniform vision governs the whole in no two places is the same function achieved differently a side effect of the totalitarian view is that the conceptualization of the facility is frozen in time in place of the master plan alexander proposes a meta plan it is a philosophy by which a facility can grow in an evolutionary fashion to achieve the needs of its occupants the meta plan has three parts a philosophy of piecemeal growth a set of patterns or shared design principles governing growth local control of design by those who will occupy the space under the meta plan facilities evolve through a series of small steps into campuses and communities of related buildings by respecting the shared principles they retain a harmony of vision but not a sameness like mature villages they begin to take on an evolved charm this is what alexander calls organic order as described below and as shown in figure this natural or organic order emerges when there is perfect balance between the needs of the individual parts of the environment and the needs of the whole in an organic environment every place is unique and the different places also cooperate with no parts left over to create a global whole a whole which can be identified by everyone who is a part of it peopleware the university of cambridge is a perfect example of organic order one of the most beautiful features of this university is the way the colleges st johns trinity trinity hall clare kings peterhouse queens lie between the main street of the town and the river each college is a system of residential courts each college has its entrance on the street and opens onto the river each college has its own small bridge that crosses the river and leads to the meadows beyond each college has its own boathouse and its own walks along the river but while each college repeats the same system each one has its own unique character the individual courts entrances bridges boathouses and walks are all different the oregonexperiment figure swiss town an example of organic order without a master plan patterns each of the patterns of the timeless way of building is an abstrac tion about successful space and interior order the central volume of the set a pattern language presents of these patterns and weaves them into a coherent view of architecture some of the pat taking umbrella steps terns have to do with light and roominess others with decor or with the relationship between interior and exterior space or with space for adults for children for elders or with traffic movement around and through enclosed space each pattern is presented as a simple architectural aphorism together with a picture that illustrates it and a lesson in between there is a discussion of the whys and wherefores of the pattern as an example consider the following illustration and extract from pattern workspace enclosure figure workspace enclosurt people cannot work effectively if their workspace is too enclosed or too exposed a good workspace strikes the balance you feel more comfortable in a workspace if there is a wall behind you there should be no blank wall closer than eight feet in front of you as you work you want to occasionally look up and rest your eyes by focusing them on something farther away than the desk if there is a blank wall closer than eight feet your eyes will not change focus and they get no relief in this case you feel too enclosed you should not be able to hear noises peopleware very different from the kind you make from your workplace your workplace should be sufficiently enclosed to cut out noises which are a different kind from the ones you make there is some evidence that one can concentrate on a task better if people around him are doing the same thing not something else workspaces should allow you to face in different directions apattern language to complement the basic patterns teams need to prepare a set of new patterns tailored to the specific nature of their project for the purposes of the next four subsections we have nominated ourselves to be one such team our charter is to design sensible workspace for people who make their living by thinking the four patterns we propose take aim at four of the worst failings of present day institutional space in forming these patterns we borrow heav ily from those of our clients that have succeeded in creating successful workplaces the first pattern tailored workspace from a kit today modular cubicle is a masterpiece of compromise it gives you no meaningful privacy and yet still manages to make you feel isolated you are poorly protected from noise and disruption indeed in some cases sources of noise and disruption are actively piped into your space you re isolated because that small lonely space excludes everyone but you it kind of a toilet stall without a toilet the space makes it difficult to work alone and almost im possible to participate in the social unit that might form around your work individual modules give poor quality space to the person working alone and no space at all to the team the alternative to this is to fashion space explicitly around the working groups each team needs identifiable public and semiprivate space each individual needs protected private space groups of people who have been assigned or have elected to work together need to have a meaningful role in the design of their own space ideally they are aided in this by a central space plan ning organization whose job is to find a chunk of space for the group i see there are three of you so you ll be needing three hundred square feet or more yes here a nice possibility now taking umbrella steps let think about layout and furniture the team members and their space counselor next begin to work out the possible ways their space could be arranged figure possible space arrangements because of the requirement that workers be allowed to partici pate in the design of their own space whatever system of desks and fixtures the company uses has to perform in a truly modular fashion instead of fitting only into a simple grid the furnishings must be useful in myriad different configurations the second pattern windows modern office politics makes a great class distinction in the matter of allocating windows most participants emerge as losers in the win dow sweepstakes people who wouldn t think of living in a home without windows end up spending most of their daylight time in windowless workspace alexander has very little patience with windowless space rooms without a view are like prisons for the people who have to stay in them we are trained to accept windowless office space as inevitable the company would love for every one of us to have a window we peopleware hear but that just isn t realistic sure it is there is a perfect proof that sufficient windows can be built into a space without excessive cost the existence proof is the hotel any hotel you can t even imagine being shown a hotel room with no window you wouldn t stand for it and this is for a space you re only going to sleep in so hotels are constructed with lots of windows the problem of windowless space is a direct result of a square aspect ratio if buildings are constructed in a fairly narrow shape there need be no shortage of windows a sensible limit for building width is thirty feet such as the building shown in figure figure 4 women dormitory at swarthmore college limit buildings to thirty feet in width can this be a serious proposal what about costs what about the economies of scale that come with building enormous indoor spaces some years ago the danish legislature passed a law that every worker must have his or her own window this law has forced builders to construct long narrow buildings planned along the lines of hotels and apartment buildings in studies conducted after the law had been in effect for a while there was no very noticeable change in cost of space per square meter that doesn t mean the narrow configuration had no cost significance only that the increase if any was too small to show up in the data even if there is a higher cost per worker to house people in the more agreeable space the added expense is likely to make good sense because of the savings it provides in other areas the real problem is that the cost is in a highly visible category space and services while the offsetting advantage is in poorly measured and therefore invisible categories increased productivity and reduced turnover taking umbrella steps the third pattern indoor and outdoor space the narrow configuration also makes it possible to achieve greater integration between indoor and outdoor space if you ve ever had the opportunity to work in space that had an outdoor component it hard to imagine ever again limiting yourself to working entirely indoors upon formation of the atlantic systems guild in we set out to find manhattan space to serve as a guild hall and office for new york based members the space we found and still occupy is the top floor of a greenwich village ship chandlery it consists of two thousand square feet indoors and a thousand square foot out door terrace the terrace has become our spring summer and fall conference room and eating area for at least half the year the out door space is in use virtually full time whatever work can be done outdoors is done outdoors before you dismiss our solution as an impossible luxury think about this fact we pay less than a third as much per square foot as the manhattan average our space costs a lot less because it not part of a monolith you can t accommodate thousands of people centrally in such space you d have to hunt out hundreds of special situations in order to get a large staff into anything like the situation we ve come up with and when you did that they would not all have identical facilities on a given sunny day some of your people would be working on terraces while others were in gardens or arbors or courtyards how impractical the fourth pattern public space an age old pattern of interior space is one that has a smooth intimacy gradient as you move toward the interior at the extrem ity is space where outsiders messengers and tradesmen and sales men may penetrate then you move into space that is reserved for insiders the work group or the family and finally to space that is only for the individual this pattern applies to your home as you move from foyer to living room to kitchen to bedroom to bathroom and it should be true as well of a healthy workplace at the entrance to the workplace should be some area that belongs to the whole group it constitutes a kind of hearth for the group further along the intimacy gradient should be space for the tightly knit work groups to interact and to socialize finally there is the protected quiet thinking space for one person to work alone peopleware group interaction space needs tables and seating for the whole group writing surfaces and areas to post whatever group members want to post ideally there should also be a space for members to prepare simple meals and eat together without communal eating no human group can hold together give each working group a place where people can eat together make the common meal a regu lar in particular start a common lunch in every workplace so that a genuine meal around a com mon table not out of boxes machines or bags becomes an important comfortable and daily event in our own work group at the center we found this worked most beautifully when we took it in turns to cook the lunch the lunch became an event a gathering some thing that each of us put our love and energy into apattern language the pattern of the patterns the patterns that crop up again and again in successful space are there because they are in fundamental accord with characteristics of the human creature they allow him to function as a human they emphasize his essence he is at once an individual and a member of a group they deny neither his individuality nor his inclination to bond into teams they let him be what he is a common element that runs through all the patterns both ours and alexander is reliance upon non replicableformulas no two people have to have exactly the same workspace no two cof fee areas have to be identical nor any two libraries or sitting areas the texture and shape and organization of space are fascinating issues to the people who occupy that space the space needs to be isomorphic to the work that goes on there and people at all levels need to leave their mark on the workplace return to reality now what does all this have to do with you if you work for a large institution you re not likely to convince the powers that be to admit the error of their ways and allow everyone to build a timeless taking umbrella steps way sort of workplace and perhaps you don t want to work for a small company in which charming and idiosyncratic workplaces occur rather naturally there is nonetheless a possible way to put your people into vital productive space the possibility arises because master planned space is almost always full and it a continual hassle to find a place to house any new effort if you run one of those as yet unhoused efforts turn your sights outward petition to move your group out of the corporate monolith you may be turned down but then again since there no space for you in house you may not put your people to work to find and arrange their own space never mind that it may not have the same white plastic wastebaskets or corduroy covered partitions as the headquarters space if you can pick up a lease on a run down fraternity house or garden apartment that would make cheap idiosyncratic fascinating quarters for your people well then so what that they will be housed differently from everyone else in the company if it okay with them who cares you don t have to solve the space problem for the whole institution if you can solve it just for your own people you re way ahead and if your group is more productive and has lower turnover that just proves you re a better manager it almost always makes sense to move a project or work group out of corporate space work conducted in ad hoc space has got more energy and a higher success rate people suffer less from noise and interruption and frustration the quirky nature of their space helps them form a group identity if you are part of the lofty reaches of upper management then decide which projects matter most move the key ones out it a sad comment that an important piece of work is likely to fare better off site it sad but true make it work for you part iii the right people the final outcome of any effort is more a function of who does the work than of how the work is done yet modem manage ment science pays almost no attention to hiring and keeping the right people any management course you re likely to take barely gives lip service to these aspects management science is much more concerned with the boss role as principal strategist and tactician of the work you are taught to think of management as playing out one of those battle simulation board games there are no personalities or individual talents to be reckoned with in such a game you succeed or fail based on your decisions of when and where to deploy your faceless resources in the next four chapters we will attempt to undo the damage of the manager as strategist view and replace it with an approach that encourages you to court success with this formula get the right people make them happy so they don t want to leave turn them loose of course you have to coordinate the efforts of even the best team so that all the individual contributions add up to an integrated whole but that the relatively mechanical part of management for most efforts success or failure is in the cards from the moment the team is formed and the initial directions set out with talented people the manager can almost coast from that point on chapter the hornblower factor c s forester series of novels on the napoleonic wars follows the exploits of horatio hornblower an officer in england royal navy on one level these are pure adventure stories set in a well researched historical framework on another level the horn blower books can be read as an elaborate management analogy the job of running a square rigged frigate or ship of the line is not so different from that of managing a company division or project the tasks of staffing training work allocation scheduling and tactical support will be familiar to anyone involved in management today hornblower is the ultimate manager his career advanced from midshipman to admiral through the same blend of cleverness daring political maneuvering and good luck that has promoted any of the corporate high fliers featured in the pages of business week there is a real world management lesson to be learned from his every decision born versus made a recurrent theme through all the novels is hornblower gloomy presentiment that achievers are born not made many of the subor dinates that fall to him through luck of the draw are undependable or stupid he knows that they all will let him down at some key mo ment they always do he also knows that the few good men who come his way are his only real resource sizing them up peopleware quickly and knowing when to depend on them are hornblower great talents in our egalitarian times it almost unthinkable to write some one off as intrinsically incompetent there is supposed to be inher ent worth in every human being managers are supposed to use their leadership skills to bring out untapped qualities in each subor dinate this shaping of raw human material is considered the essence of management that view may be more comforting than hornblower glum assessment and it certainly is more flattering to managers but it doesn t seem very realistic to us parents do have a shaping effect on their children over the years and individuals can obviously bring about huge changes in themselves but managers are unlikely to change their people in any meaningful way people usually don t stay put long enough and the manager just doesn t have enough leverage to make a difference in their nature so the people who work for you through whatever period will be more or less the same at the end as they were at the beginning if they re not right for the job from the start they never will be all of this means that getting the right people in the first place is all important fortunately you don t have to depend entirely on the luck of the draw you may get to play a significant part in the hiring of new people or the selection of new team members from within the company if so your skill at these tasks will determine to a large extent your eventual success the uniform plastic person even novice managers setting out to hire staff for the first time know something about the principles of good hiring they know for instance that you can t hire based on appearance the best looking candidate is not one whit more likely to deliver a good product than a candidate who is homely everybody knows that but oddly enough most hiring mis takes result from too much attention to appearances and not enough to capabilities this is not just due to ignorance or shallowness on the part of the person doing the hiring evolution has planted in each of us a certain uneasiness toward people who differ by very much from the norm it is clear how this tendency serves evolu tion purposes you can observe this evolutionary defense in yourself in your reactions to a horror film for instance the almost the hornblower factor human creature is much more upsetting than the mile wide eyeless blob that slowly digests detroit as each individual matures he or she learns to override the built in bias toward the norm in selecting friends and developing close relationships though you may have learned that lesson long ago in your personal life you have to learn it all over again as you develop your hiring skills you probably don t feel that you have an uncontrollable ten dency to hire attractive or normal looking people so why are we talking about this at all because it not just your individual ten dency toward the norm that affects your hiring it also your orga nization subliminal imposition of a norm of its own each person you hire becomes part of your little empire and also part of your boss empire and that of the next boss up the line the standard you apply is not just your own you re hiring on behalf of the whole corporate ladder above you the perceived norm of these upper managers is working on you each time you consider making a new offer that almost unsensed pressure is pushing toward the company average encouraging you to hire people that look like sound like and think like everybody else in a healthy corporate culture this effect can be small enough to ignore but when the culture is unhealthy it difficult or impossible to hire the one per son who matters most the one who doesn t think like all the rest the need for uniformity is a sign of insecurity on the part of management strong managers don t care when team members cut then hair or whether they wear ties their pride is tied only to their staff accomplishments standard dress uniformity is so important to insecure authoritarian regimes parochial schools and armies for example that they even impose dress codes different lengths of skirt or colors ofjacket are threat ening and so they are forbidden nothing is allowed to mar the long rows of nearly identical troops accomplishment matters only to the extent it can be achieved by people who don t look different companies too sometimes impose standards of dress these are not so extreme as to oblige strictly uniform attire but they remove considerable discretion from the individual when this first happens the effect is devastating people can talk and think of nothing else all useful work stops dead the most valuable people begin to realize that they aren t appreciated for their real worth that peopleware their contributions to the work are not as important as their haircuts and neckties eventually they leave and the rest of the company plods on trying to prove that having the right people wasn t so important after all throughout these pages we ve suggested remedies for some of the things that can go wrong in organizations but if what gone wrong in yours is promulgation of a formal standard of appearance forget it it too late for a remedy the organization is suffering from the last stages of brain death the corpse won t topple over immediately since there are so many hands trying to prop it up but propping up corpses is unsatisfying work get yourself a new job code word professional when i alluded to management insecurity as the cause of arbitrary standardization participants at one in house seminar could barely restrain themselves they all had stories to tell one of the silliest had to do with the company reaction to use of the coffee area microwave to pop some popcorn during afternoon break of course popping corn leaves an unmistakable smell someone from the ranfled altitudes of upper management smelted the smell and reacted he declared in a memo popcorn is not professional and so would henceforth be forbidden trl an anti popcorn standard or even a dress standard might be understandable if you worked in the customer relations department or in sales but in any other area it makes no sense at all there is seldom if ever a client wandering through such space these standards have nothing to do with the organization image as perceived by outsiders it the image perceived by insiders that matters the insiders in question typically second and third level managers with shaky self confidence are uncomfortable with any kind of behavior that is different from average they need to im pose safely homogenized mores on those beneath them to demon strate that they are in charge the term unprofessional is often used to characterize surpris ing and threatening behavior anything that upsets the weak man ager is almost by definition unprofessional so popcorn is unpro the hornblower factor fessional long hair is unprofessional if it grows out of a male head but perfectly okay if it grows out of a female head posters of any kind are unprofessional comfortable shoes are unprofessional dancing around your desk when something good happens is unpro fessional giggling and laughing is unprofessional it all right to smile but not too often conversely professional means unsurprising you will be considered professional to the extent you look act and think like everyone else a perfect drone of course this perverted sense of professionalism is patho logical in a healthier organizational culture people are thought professional to the extent they are knowledgeable and competent corporate entropy entropy is levemess or sameness the more it increases the less potential there is to generate energy or do work in the corporation or other organization entropy can be thought of as uniformity of attitude appearance and thought process just as thermodynamic entropy is always increasing in the universe so too corporate entropy is on the rise second thermodynamic law of management entropy is always increasing in the organization that why most elderly institutions are tighter and a lot less fun than sprightly young companies there is not much you can do about this as a global phe nomenon but you ve got to fight it within your own domain the most successful manager is the one who shakes up the local entropy to bring in the right people and let them be themselves even though they may deviate from the corporate norm your organization may have rigor mortis but your little piece of it can hop and skip chapter hiring a juggler circus manager candidate manager candidate manager candidate manager candidate manager candidate manager candidate manager how long have you been juggling oh about six years can you handle three balls four balls and five balls yes yes and yes do you work with flaming objects sure knives axes open cigar boxes floppy hats i can juggle anything do you have a line of funny patter that goes with your juggling it hilarious well that sounds fine i guess you re hired umm don t you want to see me juggle gee i never thought of that it would be ludicrous to think of hiring a juggler without first seeing him perform that just common sense yet when you set out to hire an engineer or a designer or a programmer or a group manager the rules of common sense are often suspended you don t ask to see a design or a program or anything in fact the interview is just talk hiring a juggler you re hiring a person to produce a product presumably sim ilar to those he or she has made before you need to examine a sample of those products to see the quality of work the candidate does that may seem obvious but it almost always overlooked by development managers there is a surface reserve at work when you meet for a job interview there seems to be an unwritten rale that says it okay to ask the candidate about past work but not to ask to see it yet when you ask candidates are almost always pleased to bring along a sample the portfolio in the spring of while teaching together in western canada we got a call from a computer science professor at the local technical college he proposed to stop by our hotel after class one evening and buy us beers in exchange for ideas that the kind of offer we seldom turn down what we learned from him that evening was almost certainly worth more than whatever he learned from us the teacher was candid about what he needed to be judged a success in his work he needed his students to get good job offers and lots of them a harvard diploma is worth something in and of itself but our diploma isn t worth squat if this year graduates don t get hired fast there are no students next year and i m out of a job so he had developed a formula to make his graduates opti mally attractive to the job market of course he taught them modern techniques for system construction including structured analysis and design data driven design information hiding structured cod ing walkthroughs and metrics he also had them work on real applications for nearby companies and agencies but the centerpiece of his formula was the portfolio that all students put together to show samples of their work he described how his students had been coached to show off their portfolios as part of each interview i ve brought along some samples of the kind of work i do here for instance is a subroutine in pascal from one project and a set of cobol paragraphs from another as you can see in this portion we use the loop with exit extension advocated by knuth but aside from that it pure structured code pretty much the sort of thing that your company standard calls for and here is the design that this code was written from the hierarchies and peopleware coupling analysis use myers notation i designed all of this particular subsystem and this one little section where we used some orr methods because the data structure really imposed itself on the process structure and these are the leveled data flow diagrams that make up the guts of our specification and the associated data dictionary in the years since we ve often heard more about that obscure technical college and those portfolios we ve met recruiters from as far away as triangle park north carolina and tampa florida who regularly converge upon that distant canadian campus for a shot at its graduates of course this was a clever scheme of the professor to give added allure to his graduates but what struck us most that evening was the report that interviewers were always surprised by the port folios that meant they weren t regularly requiring all candidates to arrive with portfolios yet why not what could be more sensible than asking each candidate to bring along some samples of work to the interview aptitude tests if it so important that the new hire be good at the various skills used in the job why not design an aptitude test to measure those skills our industry has had a long irregular flirtation with the idea of aptitude testing in the sixties the idea was positively in vogue by now you and your organization have probably given up on the concept in case you haven t we offer one good reason that you ought to the tests measure the wrong thing aptitude tests are almost always oriented toward the tasks the person will perform immediately after being hired they test whether he or she is likely to be good at statistical analysis or pro gramming or whatever it is that required in the position you can buy aptitude tests in virtually any technical area and they all tend to have fairly respectable track records at predicting how well the new hire will perform but so what a successful new hire might do those tasks for a few years and then move on to be team leader or a product manager or a project head that person might end up doing the tasks that the test measured for two years and then do other things for twenty hiring a juggler the aptitude tests we ve seen are mostly left brain oriented that because the typical things new hires do are performed largely in the left brain the things they do later on in their career how ever are to a much greater degree right brain activities man agement in particular requires holistic thinking heuristic judgment and intuition based upon experience so the aptitude test may give you people who perform better in the short term but are less likely to succeed later on maybe you should use an aptitude test but hire only those who fail it from your reading of this book you d hardly expect its authors to endorse the idea of hiring through the use of aptitude tests but it doesn t follow that aptitude tests are no good or that you ought not to be using them you should use them just not for hiring the typical aptitude test you buy or build can be a wonderful self assessment vehicle for your people frequent interesting opportunities for private self assessment are a must for workers in a healthy organization more about this in chapter holding an audition the business we re in is more sociological than technological more dependent on workers abilities to communicate with each other than their abilities to communicate with machines so the hiring process needs to focus on at least some sociological and human commu nication traits the best way we ve discovered to do this is through the use of auditions for job candidates the idea is simple enough you ask a candidate to prepare a ten or fifteen minute presentation on some aspect of past work it could be about a new technology and the experience with first trying it out or about a management lesson learned the hard way or about a particularly interesting project the candidate chooses die subject the date is set and you assemble a small audience made up of those who will be the new hire co workers of course the candidate will be nervous perhaps even reluc tant to undertake such an experience you ll have to explain that all candidates are nervous about the audition and give your reasons for holding one to see the various candidates communication skills and to give the future co workersa part in the hiring process at the end of the audition and after the candidate has left you hold a debriefing of those present each one gets to comment on the person suitability for the job and whether he or she seems likely to fit well into the team although it ultimately your responsibility to peopleware decide whether to hire or not the feedback from future co workers can be invaluable even more important any new person hired is more likely to be accepted smoothly into the group since the other group members have had a voice in choosing the candidate my first experience with auditions was in hiring peo ple to be consultants and instructors my motivation in torturing these prospective hires was simple enough i wanted to get a sense of whether they were natural explainers of matters simple or complex or people who could be taught to explain such matters or those who could never explain anything to anyone i also wanted some second opinions on the matter so i had those of my people who were in the office at the time of the audition sit in on the presentation over five years we conducted nearly two hundred auditions it soon became clear that the audition process served to accelerate the socialization process between a new hire and the existing staff members a successful audition was a kind of certification as a peer the reverse seemed to hold true as well failed auditions were a morale booster for the staff they were continuing proof that being hired for the group was more than just the dumb luck of when resumes happened to hit my desk trl one caveat about auditions make sure the candidate speaks about something immediately germane to the work your organization does it easy to be snookered by a talk on a topic from extreme left field like caring for the autistic child or effects of acid rain you re liable to catch a glimpse of a very compelling pas sion on the speaker part a passion that you ll never see again on the job chapter happy to be here this chapter begins with a pop quiz ql what annual employee turnover has your organization experienced over the last few years how much does it cost on average to replace a person who leaves score yourself as follows if you had any answer at all to the two questions you pass otherwise you fail most people fail in all fairness perhaps it not your job to know about such things okay we ll re score your quiz you pass if anyone in your organization has a real answer to the two questions most people still fail we avoid measuring turnover for the same reason that heavy smokers avoid having long serious talks with their doctors about longevity it a lot of bother that can only result in bad news turnover the obvious costs typical turnover figures we encounter are in the range of eighty percent to thirty three percent per year implying an average employee longevity of from fifteen to thirty six months assume for the moment that the turnover for your company is in the middle of this range the average person leaves after a little more than two years it costs one and a half to two months salary to hire a new employee either as a fee to an agency or as the cost of an in house peopleware personnel service that does the same function the employee once hired may go right to work on a project in which case his or her hours are all billed to the project there is no indication of startup cost this is however a pure bookkeeping fiction we all know that a new employee is quite useless on day one or even worse than useless since someone else time is required to begin bringing the new person up to speed by the end of a few months the new person is doing some useful work within five months he or she is at full working capacity a reasonable assessment of startup cost is therefore approximately three lost work months per new hire obviously the startup cost is worse or much worse to the extent that the work to be performed is highly esoteric the total cost of replacing each person is the equivalent of four and a half to five months of employee cost or about twenty percent of the cost of keeping that employee for the full two years on the job turnover varies enormously from one organization to another we hear of companies with a ten percent turnover and others in the same business with a hundred percent or higher turnover at any chance gathering of managers from rival companies you can expect that the person seated next to you has got a turnover rate that is dif ferent from yours by more than a factor of two of course neither of you knows which way this difference works and you never will because at least one of the two of you probably works for a compa ny that doesn t measure turnover the hidden costs of turnover employee turnover costs about twenty percent of all manpower expense but that only the visible cost of turnover there is an ugly invisible cost that can be far worse in companies with high turnover people tend toward a destructively short term viewpoint because they know they just aren t going to be there very long so if you find yourself cam paigning for better workspace for your staff for example don t be surprised to bump into someone up the hierarchy who counters with an argument like this hold on there buster you re talking about big bucks if we gave our engineers that much space and noise pro tection and even privacy we might end up spending fifty dollars per person per month multiply that times all the happy to be here engineers and you re into the tens of thousands of dol lars we can t spend that kind of money i m as much in favor of productivity as the next guy but have you seen what a terrible third quarter we re having of course the irrevocably logical answer to this is that investing now in a sensible environment will help to avoid terrible third quar ters in the future but save your breath you have encountered a short term perspective that no amount of irrevocable logic is going to sway this person is on his or her way out of the company the short term cost is very real but the long term benefit has no mean ing whatsoever in an organization with high turnover nobody is willing to take the long view if the organization is a bank it will lend money to the ugandan development corporation because the twenty two percent interest looks terrific on this quarter books of course the udc will default in a couple of years but who even going to be here then if the organization is a development shop it will opti mize for the short term exploit people cheat on the workplace and do nothing to conserve its very lifeblood the peopleware that is its only real asset if we ran our agricultural economy on the same basis we d eat our seed corn immediately and all starve next year if people only stick around a year or two the only way to conserve the best people is to promote them quickly that means near beginners being promoted into first level management posi tions they may have only five years of experience and perhaps less than two years with the company there is something very disconcerting about these numbers a person with a work life of say forty years will spend five years working and thirty five managing that implies an exceedingly tall narrow hierarchy fifteen percent of the staff is doing work with eighty five percent managing as little as ten percent of the cost could be spent on the workers with ninety percent going to reward the managers even marx didn t foresee such top heaviness of cap italistic structures not only is the structure wastefully top heavy it tends to have very lightweight people at the bottom this is somewhat true throughout the industry but strikingly true in high turnover com panies it not unusual to see serious mature companies turning out products that are developed by workers with an average age in their twenties and average experience of less than two years peopleware many of us have come to believe that companies that promote early are where the action is that natural because as young workers we re eager to get ahead but from the corporate perspec tive late promotion is a sign of health in companies with low turnover promotion into the first level management position comes only after as much as ten years with the company this has long been true of some of the strongest organizations within ibm for example the people at the lowest level have on the average at least five years experience the hierarchy is low and flat why people leave for the individuals considering a change in job the reasons can be as many and varied as the personalities involved for the organ ization with pathologically high turnover anything over fifty per cent a few reasons account for most departures ajust passing through mentality co workers engender no feelings of long term involvement in the job a feeling of disposability management can only think of its workers as interchangeable parts since turnover is so high nobody is indispensable a sense that loyalty would be ludicrous who could be loyal to an organization that views its people as parts the insidious effect here is that turnover engenders turnover people leave quickly so there no use spending money on training since the company has invested nothing in the individual the indi vidual thinks nothing of moving on new people are not hired for their extraordinary qualities since replacing extraordinary qualities is too difficult the feeling that the company sees nothing extraor dinary in the worker makes the worker feel unappreciated as an in dividual other people are leaving all the time so there something wrong with you if you re still here next year a special pathology the company move there is no bigger ego trip for insecure managers than moving the company to some distant place that is wheeling and dealing at its best injecting so much misery into workers lives makes the man happy to be here agers feel positively god like the normal business of running the company lets them control their people work lives but the move lets them have a measure of control over even their personal lives of course they are marvelously somber when explaining the rationale behind the move they talk about the escalating price of space or the tax structure of the old location and the benefits of the new one whatever the reasons given for the move you can be sure the real reasons are very different the real reason for the move is a political deal or a chance to build a new edifice finally a piece of physical evidence of their importance or reduction of the boss commute by moving the company to the suburb where the boss happens to live sometimes it simply the naked exercise of power the more egocentric the manager the more intense the fond ness for the company move listen to robert townsend in up the organization on the subject if you ve inherited or built an office that needs a real house cleaning the only sure cure is move the whole thing out of town leaving the dead wood behind one of my friends has done it four times with different com panies the results are always the same the good ones are confident of their futures and go with you the people with dubious futures and their wives don t have to face the fact that they ve been fired the company left town they say they get job offers quickly usually from your competitors who think they re conducting a raid the new people at des tiny city are better than the ones you left behind and they re infused with enthusiasm because they ve been exposed only to your best people up the organization what this is to use a technical term is the purest crap one thing townsend seems to have missed entirely is the presence of women in the workforce the typical person being moved today is part of a two career family the other half of that equation is probably not being moved so the corporate move comes down hard on the couple relationship at a very delicate point it brings intol erable stress to bear on the accommodation they re both striving to peopleware achieve to allow two full fledged careers that hitting below the belt modern couples won t put up with it and they won t for give it the company move might have been possible in the and today it is folly even in the sixties organizational moves didn t make much sense a case in point is the decision by at t bell labora tories to move the six hundred person project from new jersey to illinois in there were many reasons given for the move but it now seems likely that there were some political shenanigans involved in the then senators kennedy and johnson had arranged for huge new investments in massachusetts and texas and senator dirksen of illinois had something coming what a coup for dirksen if six hundred high salary low pollution jobs could be moved into his state a little pressure on at t perhaps to trade off the move against concessions in some antitrust matter or regulatory relief the rationale within the labs was that the cost wouldn t be too excessive a few thousand dol lars per person in relocation expenses and maybe a bit of turnover years after the ess cutover i arranged to interview ray ketchledge who had run the project i was writing some essays on management of large efforts and ess certainly qualified i asked him what he saw as his main successes and failures as boss forget the suc cesses he said the failure was that move you can t believe what it cost us in turnover he went on to give some figures the immediately calculable cost of the move was the number of people who quit before relocation day expressed as a percentage of those moved this initial turnover was greater than the french losses in the trenches of world war i tdm you can do less damage to your organization by lining up the staff in front of a machine gun than you do by moving and that accounts only for the initial loss in the case of bell labs there was another large exodus starting about a year after the move these were the people who had honestly tried to go along with the company they moved and when they didn t like the new location they moved again happy to be here the mentality of permanence over the years we have been privileged to work and consult for a few companies with extraordinarily low turnover you won t be surprised to learn that low turnover is not the only good thing about these companies indeed they seem to excel at many or most of the people conscious qualities discussed in these pages they are the best the best organizations are not of a kind they are more notable for their dissimilarities than for their likenesses but one thing that they all share is a preoccupation with being the best it is a constant topic in the corridors in working meetings and in bull sessions the converse of this effect is equally true in organizations that are not the best the topic is rarely or never discussed the best organizations are consciously striving to be best this is a common goal that provides common direction joint satis faction and a strong binding effect there is a mentality of perma nence about such places the sense that you d be dumb to look for a job elsewhere people would look at you as though you were daft this is the kind of community feeling that characterized jhe american small towns of the past it is something too often miss ing from the cities and municipalities where we live so it is all the more important in the workplace some ambitious companies set out explicitly to engender a sense of community at reader digest and certain hewlett packard locations for example the company has set up community gardens for employees at lunch hour the fields are full of amateur hoers and weeders and people talking tomatoes over their fences there are contests to grow the sweetest pea or the longest zucchini and active bartering sessions where you can trade away some of your garlic for corn you can prove that community gardens don t make any sense at all in the short term whatever costs there are will come right out of this quarter bottom line at most companies that would be enough to quash the concept immediately but in the best organiza tions the short term is not the only thing that matters what matters more is being best and that a long term concept people tend to stay at such companies because there is a widespread sense that you are expected to stay the company invests hugely in your personal growth there may be a master program or an extensive training period for new hires as much as a peopleware year in some places it hard to miss the message that you are expected to stay when the company has just invested that much in your formation a common feature of companies with the lowest turnover is widespread retraining you re forever bumping into managers and officers who started out as secretaries payroll clerks or mail boys they came into the company green often right out of school when they needed new skills to make a change the company provided those skills no job is a dead end again one can prove that retraining is not the cheapest way to fill a new slot it always cheaper in the short run to fire the person who needs retraining and hire someone else who already has the required skills most organizations do just that the best organ izations do not they realize that retraining helps to build the men tality of permanence that results in low turnover and a strong sense of community they realize that it more than justifies its cost at southern california edison the person in charge of all data processing began as a meter reader at eg g there is a program of retraining secretaries to become systems analysts at the bureau of labor statistics philosophy ph d are hired to become software developers and the retraining starts with their first day on the job at hitachi software the chief scientist has as his principal function the training of new hires at pacific bell a main source of new systems people is the retrained lineman or operator these com panies are different from the norm they feel different there is an energy and sense of belonging that is practically palpable it makes you feel sorry for the companies that don t have it chapter theself healingsystem an employee storms into the personnel department and resigns the next morning he and ms boss turn up to explain rather sheepishly that the whole thing was a silly mistake would it be possible to undo his resignation the workers handling the case look at the partly completed transaction in some perplexity whoever had designed the procedures for dealing with termination had made no allowance for undoing but it easy enough to see what will make it all come out right let see we can just drop this whole file into the wastebasket and pretend it never existed then we void out the final payroll check and ran over to harryfs desk and grab the insurance cancellation forms before he sees them a system has just healed itself something had teen left out of its original design something that turned out to be necessary the people who make the system go have fixed it on the fly it happens all the time deterministic and non deterministic when you automate a previously all human system it becomes entirely deterministic the new system is capable of making only those responses planned explicitly by its builders so the self heal ing quality is lost any response that will be required must be put there in the first place if ever the system needs to be healed that can only be done outside the context of its operation maintainers come in to take the system apart and reconstruct it with one or more new planned responses added peopleware in one view getting rid of the rather messy and uncontrollable self healing capacity is a positive benefit of automation the system is planned right in the first place and then there is no need for tin kering during operation but it no secret that this can be expen sive automators spend much of their time thinking through situa tions that are so unlikely or occur so rarely that the human elements of the old system never even bothered to consider them unless and until they actually happened if the business policy governing the new system has a sufficient degree of natural ad hoc racy it a mistake to automate it determinism will be no asset then the sys tem will be in constant need of maintenance the reason that non deterministic systems can often heal themselves painlessly and elegantly sometimes at no cost at all is that the humans who make up the system have an easy familiarity with the underlying goals when a new situation crops up they know immediately what action makes sense someday it may be possible to teach computers the goals of the system instead of the actions expected to achieve the goals but right now that beyond the horizon the point here is that making a system deterministic will result in the loss of its ability to heal itself the organization that you work in or manage is in some sense a system it is an amalgam of interacting people and processes that exist to achieve some end it all the vogue these days to talk of making such systems more deterministic that brings us to the sub ject of methodology the covert meaning of methodology the maddening thing about most of our organizations is that they are only as good as the people who staff them wouldn t it be nice if we could get around that natural limit and have good organizations even though they were staffed by mediocre or incompetent people nothing could be easier all we need is trumpet fanfare please a methodology a methodology is a general systems theory of how a whole class of thought intensive work ought to be conducted it comes in the form of a fat book that specifies in detail exactly what steps to take at any time regardless of who doing the work regardless of where or when the people who write the methodology are smart the people who carry it out can be dumb they never have to turn the self healing system their brains to the on position all they do is start on page one and follow the yellow brick road like happy little munchkins all the way from the start of the job to its successful completion the methodology makes all the decisions the people make none the organization becomes entirely deterministic like any other system a team of human workers will lose its self healing properties to the extent it becomes deterministic the result can be workers proceeding in directions that make no sense to them at all a sure sign that they can t be doing any good some years ago we conducted a post mortem of a failed project by asking each of the project workers to speak for an hour or so into a tape recorder they did this in the privacy of their own homes and we assured them that only we two the consultants would ever hear the tapes one of the speakers gave us this observation by march we had been doing this applying one of the techniques dictated from on high for nearly two months i couldn t see how it was helping us in any way but george kept assuring us that it was he said we should trust in the methodology and it would all work out in the end f of course it didn t the project workers are the ones most familiar with the territory of the project if a given direction doesn t make sense to them it doesri tmake sense at all there is a big difference between methodology and methodol ogy small m methodology is a basic approach one takes to getting a job done it doesn t reside in a fat book but rather inside the heads of the people carrying out the work such a methodology consists of two parts a tailored plan specific to the work at hand and a body of skills necessary to effect the plan one could hardly be opposed to methodology the work couldn t even begin without it but a methodology is very different big m methodology is an attempt to centralize thinking all meaningful decisions are made by the methodology builders not by the staff assigned to do the work those who espouse a methodol ogy have a long list of its supposed benefits including standardiza tion documentary uniformity managerial control and state of the art techniques these make up the overt case for the methodology the covert case is simpler and cruder the idea that project people aren t smart enough to do the thinking peopleware methodologymadness of course if your people aren t smart enough to think their way through their work the work will fail no methodology will help worse still methodologies can do grievous damage to efforts in which the people are fully competent they do this by trying to force the work into a fixed mold that guarantees a morass of paperwork a paucity of methods an absence of responsibility and a general loss of motivation the following paragraphs comment on each of these effects paperwork the methodologies themselves are huge and get ting huger they have to grow to add the features required by each new kind of situation it not at all unusual for a methodology to use up a linear foot or more of shelf space worse they encourage people to build documents rather than do work the documentary obsession of such methodologies seems to have resulted from para noid defensive thinking along these lines the last project generat ed a ton of paper and it was still a disaster so this project will have to generate two tons the technological sectors of our economy have now been through a decade long flirtation with the idea that more and more and more paperwork will solve its problems per haps it time to introduce this contrary and heretical notion voluminous documentation is part of the problem not part of the solution methods the centerpiece of most methodologies is the con cept of standardized methods if there were a thousand different but equally good ways to go about the work it might make some sense to choose one and standardize upon it but in our state of techno logical infancy there are very few competing methods for most of the work we do when there are genuine alternatives people have to know about and master them all to standardize on one is to exclude the others it boils down to the view that knowledge is so valuable we must use it sparingly the self healing responsibility if something goes wrong on a methodology effort the fault is with the methodology not the people the methodology after all made all of the decisions working in such an environment is virtually responsibility free people want to accept responsibility but they won t unless given acceptable degrees of freedom to control their own success motivation the message in the decision to impose a meth odology is apparent to all nothing could be more demotivating than the knowledge that management thinks its workers incompe tent the issue of malicious compliance those who build methodologies are tortured by the thought that thinking people will simply ignore them in many organizations that is just what happens even more upsetting is the opposite possibility that people won t ignore the methodology but will instead do exactly what it says to do even when they know doing so will lead to wasted time unworkable products and meaningless documentation this is what our cohort ken orr calls malicious compliance when the methodology calls for an eighteen pait operator manual developers may write one even for a product so deeply imbedded in an engine or a satellite that no operator interven tion is possible when the methodology says you have to fill out a database residency form for each data element developers may do so even though the system has no database in australia where striking uses up nearly as much labor time as working there is a charming form of strike called work to rule rather than walk off the job workers open up a fat book of proce dures and announce until you give us what we re asking for we re going to work exactly to the rule when the air traffic con trollers do this for instance they can only land one plane every seven minutes if doctors were to do it an appendectomy would take a week introduction of a methodology opens up the possi bility of work to rule actions in still more parts of the economy people might actually do exactly what the methodology says and the work would grind nearly to a halt allow changes to control them keys to successful change control include establishing a change board limiting major changes to predefined points in the project and placing major work products under change control you can most easily hit a target if you know precisely when the target is going to move and when it is going to stop you save your energy while the target is moving and take aim when it stops because of changing markets and evolving technology software project feature sets amount to moving targets some movement is inevitable other movement can be controlled projects that do not actively control changes to their feature sets that try to hit moving targets expose themselves to significant and in many cases insurmountable risk controlling changes as an integral part of project planning is critical to project success as gene forte says change control is in marked contrast to change surrender change control allows all necessary changes to be made while ensuring that change impacts are understood projectwide change control procedure at the most basic level change control addresses changes to requirements and source code more advanced projects integrate change control throughout the project activities project plans estimates requirements architecture detailed design source code test plans and documentation formally speaking change control is the practice of evaluating controlling and approving important changes made during the project and of ensuring that all project stakeholders are aware of the changes that affect them the basic change control procedure involves the following steps the initial development work for a work product such as the project requirements is performed without change control coming into play during this period changes can be made freely to the work product the work product is subjected to a technical review which deter mines whether initial development work on it can be declared complete when initial development is complete the work product is submitted to a change board the change board typically consists of representatives from each of the project major concerned parties for example project management marketing develop ment quality assurance documentation and user support this board also goes by the name of war council change czar and similar names depending on whether the project team fancies itself an executive committee military junta or russian nobility on small projects the change board might consist of only one two or three members on the largest projects involving multiple companies it can swell to or more its most important objective is to serve as a central clearinghouse for changes to ensure that all important viewpoints are considered when a work product is submitted to the change board it is baselined at this point any further changes to the work product are subject to a more systematic change process than was used during step the work product is placed under revision control revision control refers to a software revision control program also known as version control or source code control that is capable of archiving multiple versions of anything stored electronically although most commonly used for source code most revision control systems will archive anything that can be stored in elec tronic form documents project plans spreadsheets design diagrams source code test cases and so on further changes to the work product are treated systematically a changes are proposed via change proposals a change proposal describes the work product in question the pro posed change and the impact of the change both cost and benefit from the point of view of the party proposing the change creating a change proposal is a good idea on even the smallest projects because it provides a record of the project decisions that is far more reliable than people memories b the change board identifies parties that might be affected by the change and distributes the change proposal for their review c the concerned parties each assess the costs and benefits of the proposed change from their individual viewpoints d the change board members combine their assessments and prioritize the change proposal either they accept it reject it or defer it to a later time e the change board notifies all concerned parties about how the change proposal was resolved i ve described this process somewhat formally which might make it seem bureaucratic but if you read the description of each step carefully you will see that the steps are really just formalized common sense assess change impacts before making changes allow affected parties to review the pro posed changes and notify affected parties of changes once they have been approved what is the alternative to this don t assess impacts before mak ing changes don t allow affected parties to review changes before they are made don t notify affected parties of approved changes it sounds ridicu lous but that is exactly what happens on projects that don t use systematic change control change control benefits change control produces several significant benefits its primary benefit is that it does what it intended to do it protects the project from unneces sary changes by ensuring that change proposals are considered systemati cally the addition of unnecessary features is traditionally one of the most serious software development risks because of the related increases in soft ware complexity destabilizing effect on design and code and increased cost and schedule associated with expanding the product concept change control improves the quality of decisions made about the software by ensuring that all concerned parties are involved in decisions similarly it improves the visibility of necessary changes by ensuring that concerned parties are notified when changes are being considered and when they are resolved that in turn improves the project team ability to track progress change control combats mushy milestones one of the more subtle risks on a software project is that theproject team will reach a milestone and declare that milestone to have been met even though the team work doesn t really satisfy the milestone criteria the project team might not have created a really complete architecture document but when the milestone date for architecture arrives it ll simply declare whatever it has completed up to that point to be the architecture especially if its under pressure to meet an ambitious delivery date under the change control process the architecture or any other work product must be reviewed signed off and placed under change control beforeitcanbeconsideredcomplete it salothardertoslipamushyarchitecture past the architecture milestone when all the concerned parties have to sign offonthearchitecturedocumentandwhentheyknowthatanyfurtherchanges to it will have to go through the systematic change control process this elimination of mushy milestones also improves status visibility if you know that milestones are hard thefact that the project has completed a milestone is a meaningful status indicator basically change control increases accountability concerned parties have to sign off on work products before they are baselined people who propose changes to baselined work products have to justify why they want changes and their reasons become part of the permanent record of the project people who resist the changes have to explain why they don t want them and those reasons also become part ofthe project permanent record one of the common denominators of projects that are in trouble is a lack of accountability on successful projects project members actively seek accountability both for their own work andfor other work that affects them benefits of automated revision control a corollary benefit of change control is versioning automated revision control software enables project members to easily retrieve any version of any major document that has been produced on the project they can retrieve initial project plans revised project plans and current project plans they can re create any version of the product that has ever been released to a customer with change control they will have a detailed historical record of the project and will be able to retrace the development of estimates prototypes designs and source code over the course of the project an automated revision control system makes all project documents publicly available at all times any person who needs to examine the project plan requirements designs coding standards user interface prototype or other work product can retrieve that work product from the revision control system there is no danger of losing these work products or of being unable to get a copy when needed getting a current copy of a document does not depend on being able to find the person who owns that document although this might seem to be a minor benefit few software personnel who have worked on projects that make project materials so readily available ever want to work on projects that don t c most revision control software can also produce summary information that useful in assessing project status it can produce statistics about the number of lines of code added to changed in or deleted from the project each week common change control issues organizations implementing change control for the first time typically have questions about how to handle a few common issues how to consider changes the change control board will typically consider the following factors when it decides how to resolve a proposed change what is the expected benefit of the change how would the change affect the project cost how would the change affect the project schedule how would the change affect the software quality how would the change affect the project resource allocation would it addwork to people already on the project critical path can the change be deferred to a later stage of the project or a later version of the software is the project at a point when making the change would risk destabilizing the software the change board meets as needed typically biweekly in the early stages of a project between stages in the middle of the project and more often as the project approaches its release date the change board can approve changes at any time during the project but should generally try to limit the number of times that project personnel will be asked to evaluate change impacts early in the project during requirements development and architecture changes can be considered as they arise later in the project the development team should not be subjected to a continuous barrage of change requests it should be subjected only to a few barrages of change requests part of the change board job is to act as a buffer between the development team and the people requesting changes so that the develop ment team isn t continuously distracted from its work by evaluating change impacts in the later phases of a project the change board should collect proposed changes for consideration in batches in a staged delivery approach the times between stages are ideal for considering batches of changes to a large extent how often changes are considered depends on the personalities of the development team customer manager and other project stakeholders if stakeholders place a premium on orderly efficient operation they will typically want to consider changes less frequently if they place a greater premium on quick resolution of outstanding issues they ll want to consider them more frequently how to handle small changes the change board should use common sense in establishing streamlined procedures for categories of changes that do not have far reaching impacts such as minor defect corrections those changes can be considered and accepted en masse by the change control board or the change control plan can automatically approve them forexample the change control plan might automatically approve fixes for errors that crash the system or errors that affect the accuracy of a software calculations ii survival preparations how to handle political issues for people who have not worked with a formal change control process the process will seem cumbersome at first after project members become famil iar with it however it requires little time compared to the benefit it provides for small changes the change impact can often be assessed during the change board meeting itself and the time required to complete steps may be only a few minutes or less one initial effect of implementing change control is that fewer changes will be accepted than were accepted previously some people will feel that getting any changes accepted at all is impossible although the change board might initially seem to be miring the project in bureaucracy it is always free to approve as many changes as it wants before change control change im pacts are usually not fully considered once they are fully considered fewer changes will be judged valuable enough to approve this sometimes uncom fortable period marks an important transition from letting changes control a project to making the project control changes some of the loudest objections to change control will come from the people who have been most successful at ramrodding changes through without giving the project team enough time to consider the full impacts of those changes with change control in place those people will not be successful as often as they were previously one of change control benefits is its ability to rein in hastily considered changes people who are used to getting their way can still get their way with systematic change control but they ll have to do it through a process that emphasizes visible decision making and accountability development personnel need to realize that this will be a difficult tran sition for some people to make and should prepare for it which work products to place under change control the change control plan should include a list of the work products that will be placed under change control at a minimum this list should include the work products listed in table work product change control plan change proposals vision statement top risks list software development plan including project cost and schedule estimates user interface prototype user interface style guide user manual requirements specification quality assurance plan software architecture software integration procedure staged delivery plan individual stage plans including miniature milestone schedules coding standard software test cases source code media incorporated into the product including graphics sound video and so on software build instructions make files detailed design document for each stage software construction plan for each stage install program deployment document cutover handbook release checklist release sign off form software project log software project history document each of the wxnk products is initially placed under change control at the time it is baselined see table top level milestones and deliver ables on page this list of work products represents a healthy minimum set of deliver ables to place under change control when you see a set of work products like this list you might think that a lot of overhead and extra work creating these products is overhead and extra work it probably adds a few percentage points of overhead to the project but there is no way to provide the status visibility risk reduction and project control in short the greatly increased chance of project success without creating and controlling work products more or less like these because of the significant benefit they provide in most business situations the decision to accept this particular kind of overhead is not just a good trade off it an excellent one for any one project creating all these work products the first time will be a lot of work on the second or third project however the development team can create many of them simply by modifying previous versions of similar work products committing to change control for change control to work the project and the organization of which the project is a part must commit to change control this commitment needs to take place on several levels software change control activities need to be planned the change control plan procedure and list of work products described in this chapter should be expressed within a written change control plan the software development plan discussed in the next chapter should reference the change control plan as part of the official software development process project members must be given time to carry out their change control responsibilities at a minimum each project member will spend some time assessing the impact ofa fewproposed changes afew project members will also spend time attending change board meetings the organization must accept the decisions of the change board at all levels change control will be next to meaningless if the project manager or marketing department can summarily override change board decisions or if software developers add changes to the software without honoring the change control procedure samples of some of these documents can be found on the survival guide web site survival check the project has a change board the change board decisions can be reversed by manage ment marketing or the customer the project has a written approved change control plan project teammembers aren t given enough time to carry out the plan work products aren t actually put under change control change proposals are evaluated by all the project concerned parties before they are resolved the change board notifies the project concerned parties of how each change proposal is resolved the change board has the project team evaluate changes in batches so that the team is not distracted by a constant barrage of change requests preliminary planning successful projects begin planning early prelimi nary planning activities include defining a project vision identifying an executive sponsor setting targets forproject scope managingrisks and map ping out strategies for using personnel effectively thesepreliminaryplansarecaptured inasoftware development plan you might think that not much can be done to plan a project before its requirements are known on the contrary preparing preliminary plans before plunging into requirements development is both useful and impor tant the following topics should be addressed in the preliminary software development plan which is described at the end of the chapter project vision executive sponsorship project scope targets publicizing plans and progress risk management personnel strategies timeaccounting a discussion of the topics themselves constitutes most of this chapter project vision before the project gets rolling a team needs to buy in to a common vision without such a shared vision high performance teamwork cannot take place a study of teams found that in every case in which the team func tioned effectively the team had a clear understanding of its objective sharing a vision is useful on several levels agreement about the project vision helps to streamline decision making on smaller issues it also helps to keep the team focused and avoid time wasting side trips a common vision builds trust among the team members because they know that they are all working toward the same objective the team can make decisions and then execute them without squabbling and without revisiting issues that have already been decided an effective team builds a level of cooperation that allows them to outperform a mere collection of individuals with similar skills an effective vision can have a motivating effect and to do that it needs to be elevating the team needs to be presented with a challenge a mission teams don t form around mundane goals we d like to create the third best internet web site designer on the market and deliver it percent later than the industry average that is not a compelling vision the response to challenge is emotional and it is influenced as much by the way the work is assigned or described as by the work itself here a restatement of the ho hum goal we re going to create an internet web site designer that will take us from market share to percent market share in its first months we re cash poor so we re going to make it our goal to define a feature set and ship date that will allow us to use a small sharp highly efficient team in order to establish a foothold in the market before we run out of cash a team might very well form around that restated vision elevating though it must be the vision statement must also be achiev able sales and marketing personnel can be motivated by impossible goals but software developers tend to see them as illogical and often find them demotivaring the same applies to sets of goals in which each goal individu ally is achievable but the set of goals in combination is unachievable the goals of short schedule low cost or rich functionality might be achievable individually but they usually cannot all be achieved simultaneously such sets of goals are seen by most developers as manipulative and they won t work to them on some projects you can t tell at the beginning of the project whether a set of goals is achievable a common and damaging dynamic occurs when the development team begins to realize its goals are unachievable before management does if management continues to insist that the goals are achievable after the team has figured out that they are not team motivation and morale can sink fast if you re a software project manager be responsive when developers begin telling you that the project goals are collectively unrealistic defining what to leave out the vision statement should make it easy to determine what should go into the software and what should not a vision statement such as create the world best word processor might be motivating but it will not provide much guidance to the development team other than telling them to throw every conceivable feature into the software the development of microsoft word for windows was hobbled by this kind of all inclusive vision state ment that product ultimately took five years to develop which was four years longer than originally planned alternatively a vision statement such as create the world easiest to use word processor provides just as elevat ing a mission for the team to form around but provides much better guid ance about what features to exclude creating wording that excludes at least as much as it includes is the hard part of writing a vision statement but that wording is essential to the statement usefulness a good exclusive vision statement helps the project team achieve the goal of software minimalism which is essential for keeping project risk to a manageable level committing to the vision the vision should be formalized in a written vision statement when team members commit there must be something for them to commit to this vi sion statement then becomes the first item placed under change control a vision statement can t provide top level guidance if it changes haphazardly over the course of a project it can and should be allowed to change in a controlled way as everyone understanding of the developing software matures over time your collection of vision statements from different projects will become an important resource fornew projects especially ifyou have classified these statements as effective and ineffective executive sponsorship executive sponsorship is the support given to the person or group who has final authority over projectwide decisions many surveys have identified effective executive sponsorship as critical to project success for this reason a project plan should identify the executive sponsor this person or group should be responsible for committing to a feature set approving the user interface design and deciding whether the software isreadyto release to its users or customers if the decision making authority is a group each person in the group should represent a different interest management marketing development quality assurance and so on sometimes this group is the change control board described in chapter hitting a moving target early in my career i worked on a project for which i had five equal bosses this set of bosses redirected my work so frequently and in so many ways that it took me two years to complete a one year project i felt like a piece of silly putty being pulled in five different directions and our software ended up looking like a silly putty picture that had been stretched five dif ferent ways be sure that decisions come from one place whether that one place is a single person or a single change control board a single clear decision making authority is essential to an effectively functioning project project scope targets before much work has been done on the project you should have an idea about the intended budget schedule staffing and feature set ofthe software this idea is not an estimate but a tentative target as the project progresses the project team will gain insight about how much work will be needed to create the envisioned software and as this awareness develops any of several possibilities can arise you find the initial project budget and schedule targets to be in close alignment with the envisioned feature set you find the initial project budget and schedule targets to be in sufficient to support the desired feature set and you adjust the budget and schedule targets upward to support the desired fea ture set you find the initial project budget and schedule targets to be in sufficient to support the desired feature set and you shrink the feature set to conform to the desired budget and schedule as i explained in conjunction with figure on page software development is a process of continual refinement in which more is learned about the technical extent of the software in each stage well run projects invariably involve some give and take between feature set budget and schedule the best organizations plan to reestimate regularly throughout a project and periodically adjust their project plans based on the reestimation one of the most successful software organizations in the world nasa software engineering laboratory sel creates an initial project estimate after requirements have been defined and then refines that estimate five times over the course of a project at the estimation points that are shown in table at each estimation point the development team makes a base estimate of the work remaining on the project the range of uncertainty in the estimate is then expressed by taking the base estimate and multiplying it by table upper limit and lower limit numbers in addition the sel manager handbook advises that the base estimates typically grow by percent over the course of a project this dynamic was illustrated in fig ure on page table estimation point upper limit lower limit end of requirements definition and specification end of requirements analysis xl end of preliminary design xl end of detailed design xl end of implementation xl end of system testing xl considering the amount of uncertainty contained in the project esti mates even after requirements development has been completed the best way to think of preliminary schedule and budget targets created at software concept time is as aids thathelp the development team identify features that cannot be delivered within the desired targets eliminating these features from consideration early in the project helps to keep the project small which addresses the major project risks of excessive complexity and gold plating more detailed estimates are created later in the project and are ex plained in greater detail in chapter final preparations publicizing plans and progress one common characteristic of unsuccessful software projects is that plan ning is conducted in secret usually none of the planners intends to keep planning a secret but none of them tries very hard to involve the rest of the project personnel either the effect is virtually inevitable plans created with out the involvement of the developers testers and documentation staff who will actually perform the work do not address all necessary considerations and the plans cannot be carried out because the plans cannot be followed the project team ignores them and essentially runs free form that is it runs out of control project plans should be reviewed and approved by the people who will carry them out am i serious that the plans created by a manager have to be approved by the people that manager manages absolutely the project plans will in fact be approved or disapproved by the project team once the project is under way in the form ofthemanydecisions they make about how to follow the plans and whether to follow them if the project team doesn t approve of the project plans the plans will not in fact be followed the effective software project manager acts as an orchestra conductor as the person who coordinates the work of the team the manager does not have perfect knowledge of the work required to carry out each aspect of the project so it is necessary to incorporate input from each of the project participants a software project cannot thrive when there is an adversarial relation ship between project management and the project team the manager shouldn t try to trick or goad the development team into doing its work the team wants to do its work what the team needs from project management is effective coordination of activities so that its efforts aren t wasted the smart project manager makes sure the plans are approved by the project team before the bulk of the work gets under way on a healthy project all of the planning materials are made available for public review productivity assumptions schedules schedule padding risks task assignments and any other planning component publicizing progress indicators once the plan has been reviewed approved and put under change control the core indicators of project health or dysfunction should be made visible to the entire project the goal is to make basic project status readily available to all project stakeholders this list of indicators includes at least the follow ing information list of tasks completed defect statistics top risks list percent of schedule used percent of resources used project management status reports to upper management one way to achieve status visibility is to create a project intranet home pagethatcontainspointerstogeneralprojectinformationincludingprojectplan ning and tracking details technical work products and project deliverables figure shows an example of such a web page making these materials available on a web site extends one step further chapter idea of using the revision control system to make all project materials available on an intranet web site you can get current information about the project without know ing how to use the project revision control system figure example of a software project intranet web page containing up to date status planning information and key project work products there are no secrets on a successful software project both good and bad news must be able to move up and down the project hierarchy without restriction risk management one dictionary definition of risk is possibility of loss or injury software development is an activity that involves the possibility of risk or loss in the form of budget and schedule overruns no matter how it conducted but the way that software projects are typically conducted offers little chance of success at all and that level of risk is completely unnecessary as figure on the next page illustrates i have found that the average project devotes virtually none of its efforts to reducingriskand consequently accepts unnec essarily high risk exposure proportion of budget devoted to risk management figure variation in risk level according to risk management overhead the aver age project devotes virtually no attention to risk management and accepts extremely high levels of risk as a consequence successful projects devote a small amount of overhead to risk management and substantially reduce their risk exposure devoting a small amount of attention to risk management produces dramatic benefits in this book approach about percent of the project effort goes into activities that could be loosely categorized as risk manage ment this small expenditure provides a dramatically improved chance of meeting schedule and budget targets this book approach should give most projects a to percent chance of being completed on time and withinbudget successful organizations actively lookfor ways to trade small amounts of increased overheadfor large amounts of risk reduction some projects need to reduce risk even further and to accomplish that they are forced into the area of diminishing returns shown in the lightly shaded area on the right in figure still other projects such as the histori cally paperwork bound u s department of defense projects are to the far right of the graph devoting large percentages of their budgets in an attempt to assure on time completion because software development is inherently risky most medium to large projects cannot attain a percent chance of meeting schedule and budget targets without building in lavish safety margins at some level of overhead shown approximately in the graph the overhead itself becomes a risk to the project and reduces the chance of meeting the project sched ule and budget targets committing to risk management success at risk management depends on making the commitment to perform risk management developing an ability to perform it carrying out activities to perform it and verifying that the management plan for each risk has been effective if any of these important elements is missing risk management will be ineffective a commitment to risk management consists ofthree elements first the project plan must describe a risk management approach such as the one described here in writing second the budget for the project must include funds earmarked for risk resolution if no funds can be earmarked for risk resolution the project cannot be considered to have made a commitment to risk management third when risks are assessed their impacts must be incorporated into project plans some projects carry out risk assessments but don t do anything with the risk information they obtain that is roughly comparable to sending your calls to voice mail and never checking your messages developing the ability to perform risk management is partly straight forward partly subtle the straightforward part is that the project team carries out the activities described in this section the subtle part is that some organizations inadvertently discourage the flow of risk oriented information to upper management and other people who need it the plan described in this section helps to ameliorate that problem this book approach employs some practices that are intended prima rily to manage risk other practices described in this book are not specifically risk management practices but have been built into the approach because of their risk management benefits practices recommended specifically for risk management include the following planning for risk management in the software development plan which is what this section has been describing identification of a risk officer use ofa top risks list creation of a risk management plan for each risk creation of an anonymous risk reporting channel these practices are described in the next few sections risk officer the project should identify a risk officer who is a member of the project preferably someone other than the project manager responsible for spot ting emerging risks the risk officer must be part chicken little the sky is falling and part eeyore it ll never work the risk officer function is to play devil advocate during planning meetings and when reviewing planning materials continually looking for potential problems the risk officer tries to pick apart the risk management plans for each risk he or she should have management respect otherwise the risk officer simply be comes the project designated pessimist often a senior developer or tester makes a good risk officer because the project manager job is to steer the project to success the project managercanhavedifficulty adopting the devil advocate focus thatthejob of risk officer requires the risk officer role in the project is parallel to the independent tester role in testing code it is too hard for developers to try to make and then break the code at the same time the two jobs require dif ferent orientations and the jobs of project manager and risk officer do too identification of a risk officer doesn t eliminate theneed for other project personnel to try to reduce project risk the information flow patterns and reward criteria within the organization should encourage risk identification by all project personnel top risks list the key risk management tool is the top risks list which is a list of the top risks the project faces at any given time the simple act of maintaining a list of current risks helps to keep risk management at the forefront of the project manager mind the project team should create a preliminary risks list before beginning requirements work and then keep the list current through the end of the project it isn t important that the list contain exactly risks it can contain risks or what is important is that it is maintained regularly the project manager risk officer and the project manager boss should review the top risks list every two weeks or so this review should be on their biweekly schedule so that the activity isn t abandoned the act of updating the risks list prioritizing the risks and updating the risk resolution progress col umn forces them to think about risks regularly and to be alert to changes in the risks importance in support of the survival skill of projectwide visibility the list should be made available to all project personnel people who are not in the manage ment chain should be encouraged to sound an alarm if they identify signifi cant problems at their level this list can be simple such as the one shown in table on pages ii survival preparations table sample top risks list creeping user interface prototype used to requirements gather high quality requirements user manual requirements specification has been placed under explicit change control staged delivery approach will be employed to provide some ability to change features if needed requirements or developer gold plating vision statement specifies what is not included in software design emphasis placed on minimalism reviews have checklist item for extra design or implementation released software user interface prototype devel has low quality oped to assure users will accept software disciplined development process is used technical reviews are used on all requirements designs and code test planning assures all func tionality will be covered by system testing system tests are performed by independent testers unachievable project avoids making schedule schedule commitment prior to completing requirements specification upstream reviews are used to detect and correct problems when it is least expensive to do so schedule is reestimated several times over the course of the project active project tracking assures that any schedule slips will be detected early continued preliminary planning this last week week weeks on list risk risk resolutionprogress staged delivery allows for deliv ery of partial functionality even if whole project takes longer than expected unstable tools delay schedule only one or two new tools are used on this project remainder have been used on previous projects high turnover project vision encourages devel oper buy in active detailed project planning creates clear expectations periodic reestimation supports revised plans to account for changes in scope without massive overtime productivity environment sup ports high developer produc tivity high motivation and high retention friction between developers and customers unproductive office space user interface prototype aligns developers and customers on same detailed vision staged deliveries provide cus tomers with evidence of steady progress will move development to off site environment with private offices after completing user interface prototype still need budget approval for conducting project off site ii survival preparations tool support for risk tracking an alternative to creating the top risks list is to enter the risks into the defect tracking system separately from the project defects defect track ing systems will typically allow risks to be marked open or closed they can be assigned to specific project team members for resolution they can be prioritized you can print out lists of them arranged by priority by the amount of time they ve been open and by person responsible for resolving them you can track the steps taken to resolve them who took the step and so on a defect tracking system can take some of the tedium out of maintain ing the risks list detailed risk management plans each risk on the top risks list should be supported by a detailed risk management plan the risk management plans do not need to be elaborate they can take up only a page or two each the risk management plan should answer the questions shown in table why why is a risk management plan needed for this specific risk describe the risk probability of occurrence consequences and severity how how will the risk be resolved in general describe the general approach to re solving the risk list or describe the options that were considered what what specific steps will be taken to resolve the risk list the specific steps and deliverables that will be generated in addressing the risk include a description of the conditions under which the risk will be upgraded for example if the risk cannot be resolved by a specific date who who will be responsible for completing each step list the specific person re sponsible for completing each step when when will each step be completed list the completion date for each step how much how much budget is allocated to the resolution of the risk list the cost of each step that will be taken to resolve the risk preliminary planning anonymous risk reporting channel reporting good news projectwide and up the management chain seldom poses problems but reporting bad news often does the project team should establish an anonymous communication channel that project members can use to report status and risk information up the management chain this channel can be as simple as a suggestion box in the project break room if developers are turning their code over to testing later than scheduled a concerned tester can report that if testers are releasing builds to documen tation that have not been well tested a concerned tech writer can report that if project management is exaggerating the project progress in reports to upper management a concerned developer can report that if you re in top management and some technical person predicts to percent of the way through the project that the project isn t going to work you want to know that prediction after to percent of the project has elapsed not after percent has elapsed and that technical person says i tried to tell you but couldn t get past my first level manager the project home page in figure on page suggests a way that an anonymous reporting channel could be incorporated into a project home page when this approach is used a list of anonymously reported risks is available to all project personnel at all times personnel strategies to support the peopleware survival skill described in chapter i recom mend the use of people aware management accountability the idea be hind this long phrase is that managers should be held accountable for whether the organization human resources emerge from a project strength ened or diminished do five developers quit the company at the end of the project that a tangible loss to the company and should be held against the manager just as losing would be does the whole development team emerge from the project with improved skills and incredible morale that a tangible benefit to the organization and should be credited to the manager account ii survival preparations personnel development the project plan should contain explicit provisions for emerging from the project with enhanced human resources here are some characteristics of plans based on people aware management accountability managers are evaluated based on how well they retain project personnel all members of the project have access to professional growth opportunities during the project developers believe in the vision of the project and emerge from the project feeling better about the company not worse staff buildup the most efficient way to staff a medium sized project is to staff the project with senior personnel during the requirements stage then add staff during the architecture and design stages during requirements analysis and archi tecture the best work is done by small teams of about two to five people a project shouldn t begin to staff up until it passes its planning checkpoint review percent of the way through the project larger groups tend to dilute the conceptual integrity of the software and increase the project burn rate without adding productivity the later stages of detailed design and con struction can support much larger teams and remain productive but new people must be added by the early stages of construction or the late person nel addition can actually decrease the overall project productivity for small projects those with about seven or fewer developers using a flat staffing model in which all team members are present on the project the entire time is effective new developers available vs good when a project is staffed avoid the trap of hiring developers just because theyare available hold outfor those who are qualified on a month project waiting a month or two to hire a qualified programmer is usually a better strategy than hiring a less capable programmer who available earlier one of the most widely replicated results of software engineering re search is that at least a to difference in productivity exists between the most effective developers and the least effective ones this does not mean that the most effective developers are times as productive as some nomi nal productivity level it means that they are times as productive as the nominal level while the least productive programmers actually drag the project backward avoid hiring those least productive developers even if they are readily available it better to waitfor a productive programmer to become available than it is to waitfor thefirst available programmer to become productive team dynamics one of the more interesting research results of the past years was pub lished in by a researcher named b lakhanpal lakhanpal study ex amined teams to determine which had more impact on productivity individual developer capabilities or team cohesion the study found that team cohesion had a greater impact on productivity than did the team mem bers individual capabilities individual capabilities scored a close second the implication of this study is startling teams are usually assembled based solely on the technical competencies of individual team members but this study implies that when a team is assembled at least as much attention should be paid to how well the team members will work together once an especially cohesive team has formed the careful manager will think twice about automatically disbanding it at the end of a project and will look for ways to keep it together once the project is under way don t tolerate troublesome developers in another study of projects larson and lafasto found that the single biggest complaint that team members had about their team leaders was the failure to address problem personnel the team members all knew who the problem team members were but gave their leaders low marks for dealing with them paradoxically team leaders tended to score themselves high in this category if you suspect a team contains a difficult person work as quickly as possible with the project manager upper management human resources department or whoever else needs to be involved to move the person off the project or out of the company the negative productivity effect of reduced team cohesion which affects every team member will outweigh whatever positive contribution the difficult team member is thought to be making moreover in the cases i ve seen after the difficult team member leaves the project the quality of that person work is inevitably discovered to be on the same level as that person personal interactions poor there is no advantage to delaying the removal of a difficult team member key staff buildup questions here are the key questions about project staffing that the preliminary project planning should address does the project manager have software experience with one or more projects of a similar size does the project senior technical staff have knowledge of the kind of software being built and experience on similar success ful projects most teams are comprised of average personnel do expectations about the team productivity match the team members abilities will the individuals work well together team organization different organizations use a variety of approaches to team organization and some organizations are definitely more effective than others the most ef fective software organizations treat their senior software developers and managers as equals they treat their software managers the same way that professional athletic teams treat their managers they are critically important to the success of the team but no more than the star players are project team organization a project team no matter what size needs to differentiate among the vari ous roles played by team members on small projects several roles may be performed by one person project manager the project manager is responsible for orches trating the detailed technical work of the project including devel opment quality assurance and userdocumentation theproject manager is responsible for developing the project software development plan and is usually the development team link to upper management product manager the product manager is responsible for integrat ing project work at the business level on commercial software products this work includes marketing product packaging end user documentation end user support and the software itself on in house projects this work includes working with groups that will use the system to define the software setting up training and user support and planning for cut over to the new system architect the architect is responsible forthe conceptual integrity of the software at the design and implementation level user interface designer the user interface designer is responsible for the conceptual integrity of the software at the level visible to the user on in house projects this role can be played by some one from end user support user documentation development or product management on commercial products it should be performed by a user interface specialist end userliaison the end user liaison is responsible for interact ing with end users throughout the project walking them through the prototype demonstrating new releases eliciting user feedback and so on this role can be performed by a developer product manager or someone from end user support developers developers are responsible for the detailed design and implementation of the software developers are responsible for making the software work qa testers the quality assurance personnel plan and manage test activities create detailed test plans and perform tests they are responsible for finding all the ways to make the software break if the project is large enough these people may have their own qa lead or manager tool smith the tool smith is responsible for developing build scripts maintaining the source code control system developing specialized utilities needed by the project and so on build coordinator the build coordinator is responsible for main taining and running the daily build discussed in chapter and for notifying developers when their source code breaksthe build risk officer the risk officer is assigned to watch for emerging risks as described earlier in this chapter end userdocumentationspecialists thesespecialistsarerespon sible for generating help files printed documentation and other instructional materials that end users will use you might think this list identifies a lot of different roles but most projects perform all these roles either formally or informally i have found that it is useful to explicitly identify each role for planning purposes even if the day to day responsibilities of the role are carried out informally on large projects that assign one or more whole people to each role the work involved in each role is self evident indeed larger projects will have additional more specialized roles than those listed here on smaller projects in which each person performs several roles it can be easy to underestimate the full scope of a person responsibilities during project planning for example over looking the fact that a key developer is also responsible for most of the in teractions with end users explicitly defining roles helps to ensure that each person full responsibilities are considered in the project schedule and other project plans most of the roles can be intermingled in whatever way makes sense for the specific project but the roles of development and quality assurance should not be intermingled the quality assurance roles require a devil advocate mind set that is difficult or impossible for developers to adopt tiger teams a well run project will identify short term high priority tasks throughout its lifetime the project team might need to determine whether a code library update eliminates problems the team has had with previous versions of the library it might need an in depth assessment of a new release of a competitor product or it might need to extend the project user interface prototype in response to user feedback generated by a midproject release in such cases the project manager can appoint a tiger team to tackle the problem a tiger team is a small team usually one or two people who perform a single task quickly usually in two weeks or less often in just a few days once team members have accomplished their assignment they disband and return to their regular assignments preliminary planning should include time for as yet to be determined tiger team assignments be aware that each developer responds to tiger team assignments dif ferently some developers view a tiger team assignment as a recognition of their contributions and will be offended if they are never assigned to one some staff members appreciate the chance to work on something different for a while and to take a break from the project others do not like the dis traction of being diverted from their primary work rotating assignments among staff members is an effective approach to creating tiger teams if the project leaders always assign their most effective developers they can delay the main work of the project while driving those developers crazy with distractions sometimes a project manager can find individuals who thrive on short term assignments in such cases the project manager can juggle those individuals assignments to provide them with more chances to participate on tiger teams time accounting the early part of a project is also a good time to begin detailed time account ing that is keeping track of how project personnel spend their time time accounting is a critical component of project visibility and control on the cur rent project and lays the foundation for more accurate estimates and plan ning on future projects tune accounting data enables you to compare estimated time to actual time with the goal of improving future estimates you can use the breakdown of time spent on activities from one project to help plan future projects you can review the amount of time spent on rework and get an idea about whether enough time is being spent on correcting defects close to their sources if an organization doesn t have its own time accounting categories your project can use the time accounting categories shown in table on pages time accounting should begin as early in the project as possible other wise valuable historical information that can be used to plan future projects will be lost the time accounting categories need to be about as detailed as those shown in table any less detailed and the time accounting data won t contain enough detail to provide guidance in planning future projects projects that use time accounting typically employ any one of several net worked time accounting programs that allow team members to enter their own time data from their desks pointers to various time accounting pro grams can be found on the survival guide web site ii survival preparations time accounting category activity management administration process development requirements development user interface prototyping architecture plan track progress status report progress status manage project team activities manage customer end user relations manage change downtime labsetup create development process review development process rework development process educate customer or team members about development process create user manual requirements specification review user manual requirements specification rework user manual requirements specification report defects detected during requirements development create user interface prototype review user interface prototype rework user interface prototype report defects detected during prototyping create architecture review architecture rework architecture report defects detected during architecture this time accounting table is available in electronic form from the survival guide web site continued preliminary planning time accounting category activity detailed design implementation component acquisition integration system testing software release metrics create detailed design review detailed design rework detailed design report defects detected during detailed design create implementation review implementation rework implementation report defects detected during implementation investigate acquire components manage component acquisition test review acquired components maintain acquired components report defects in acquired components automate build maintain build test build distribute build plan system testing create manual for system testing create automated system test run manual system test run automated system test report defects detected during system test prepare and support alpha beta or staged release prepare and support final release collect measurement data analyze measurement data ii survival preparations the software development plan the plans created at this time are less detailed than the plans that will be in place by the end of the project but these plans should still be committed to writing ineffective software projects are often characterized by a revisiting of the same issues again and again because these issues form the planning foundation for the project it is important that they be recorded a document called the software development plan should describe the planning consider ations outlined in this chapter including decision making authority project scope publicizing of plans and progress risk management personnel strat egies and time accounting once drafted the plan should be reviewed and signed off by the project manager quality assurance group and the devel opment team it should then be placed under change control just as all other important work products are once plans have been created for one project they can more easily be modified and adapted for use on future projects the plan for the very first project takes more effort than future plans will a detailed annotated outline of a software development plan is available from the sur vival guide web site preliminary planning survival check the project has a clear vision the vision doesn t provide guidance in deciding what to leave out of the software the project team has identified an executive sponsor or commit tee with final authority over projectwide decisions the project plans and progress compared to the plans are readily available to all project team members and upper management the project has a risk officer the risk officer is the project manager the project has a top risks list the top risks list is not kept up to date the project team develops risk management plans for each risk on the top risks list the project leaders hire well qualified people waiting for quali fied people if necessary rather than just hiring whoever is avail able first time accounting is begun before requirements development begins all of the above considerations are formalized in a software de velopment plan the software development plan isn t placed under change control the software development plan isn t actuallyfollowed by the development team during requirements development the software concept becomes tangible through the creation of several versions of the user interface prototype and the user manual requirements specification this approach facilitates gathering the best possible set of requirements lays a foundation for excellent architecture work streamlines the project by eliminating the time consuming detailed require ments document and keeps user documentation off the critical path ii survival preparations software requirements development is the part of the project during which the needs of the customer are gathered and translated into a speci fication of what the system must do requirements development consists of three related activities gathering candidate requirements which is accomplished by interviewing the potential users about the system they want re viewing competitive products building interactive prototypes and so on specifying requirements which is accomplished by committing the gathered requirements to tangible media such as a written requirements document project storyboards an interactive user interface prototype or some other medium analyzing requirements which is accomplished by looking for commonalities and differences among requirements and breaking them down into their essential characteristics this is a prelimi nary design activity and is not discussed further in this chapter i depart somewhat from common terminology in referring to these activities as requirements development these activities are oftenreferred to as specification analysis or gathering i use the word development to emphasize that requirements work is usually not as simple as writing down what key users want the software to do requirements are not out there in the users minds waiting to be gathered in the same way that iron ore is in the ground waiting to be mined users minds are fertile sources of re quirements but the project team must plant the seeds and cultivate them before the requirements can be harvested the most difficult part of requirements gathering is not the act of recording what the users want it is the exploratory developmental activity of help ing users figure out what they want requirements gathering and specification is an open ended activity you can consider the project team to be finished with requirements only when it has achieved a clear stable understanding of what the users want requirements development the software to do abbreviating requirements activities is a costly mistake for each requirement that is incorrectly specified you will pay to times as much to correct the mistake downstream during coding as you would pay to correct the mistake at requirements time overview of the requirements development process here are the general steps i recommend to develop requirements identify a set of key end users who collectively have the credibil ity to define the software that the team is building interview the end users to create a set of preliminary requirements build a simple interactive user interface prototype show the simple user interface prototype to the key end users and solicit their feedback continue revising the simple proto type keeping it simple showing it to the end users and revising it again until the end users are excited about the software concept develop a style guide that codifies the prototype look and feel review it and put it under change control fully extend the prototype until it demonstrates every functional area of the software make the prototype broad enough to cover the whole system but keep it as shallow as possible it should demonstrate the functional area not actually implement it treat the fully extended prototype as the baseline specification put it under change control then require the developed software to match the prototype exactly except for changes approved through the change control process write the detailed end user documentation based on the proto type this detailed end user documentation will become the de tailed software specification and it should also be put under change control create a separate non user interface requirements document for algorithms interactions with other hardware and software and so on put that document under change control ii survival preparations this set of steps is weighted heavily toward development of interactive software if the software you are developing is an embedded system that doesn t have a user interface such as an automobile cruise control much of the discussion in this chapter won t apply to your project the following sections describe the nuances of the steps in the require ments process identify a set of key end users the first step in gathering requirements is to identify a set of users who will provide guidance in defining software requirements these users must be selected in such a way that if they say that a feature is important you can believe that it truly is important likewise if they say that a feature can be left out you should be confident that you truly can leave it out be sure to include both power users and average users if the project is in house software the project leaders can recruit a handful of actual users and make the users involvement with the project part of their job descriptions if the project is shrink wrap software the project leaders can still recruit a handful of actual users in order to use their time efficiently they ll need to make sure that interactions with the users are more planned and structured soliciting user input is a critical success factor what if you can t find any end users and the project grinds to a halt that might be a blessing in disguise the project is better off burning only calendar time while you try to find end users than burning both calendar time and money while you build software that users will ultimately reject interview the end users an initial set of user interviews should be conducted to elicit a preliminary set of requirements the preliminary set of requirements is used as the ba sis tor creating the initial simple user interface prototype it we ve learned one thing in the last years about software develop ers abilities to design software that users like it that software developers by themselves aren t very good at designing software that users like but software developers can be very good at helping users discover what they like because users by themselves aren t very good at designing software they like either requirements development build a simple user interface prototype keep the prototype as simple as possible the point of this activity is to present many alternatives to the user before you commit much effort to any particular approach the project team should develop just enough to give the user the look and feel of the software they re going for and nothing more if you want to prototype a report format for example don t even bother to have the user interface prototype print the report the developers should only mock up the report in a word processor and tell the users here what will come out when you hit the print button if the project is being developed in an environment that doesn t have good prototyping tools consider prototyping on a windows based machine and mimicking the look and feel of the target platform if the real develop ment environment is an ibm mainframe for example use microsoft visual basic to mock up screens with green characters and black backgrounds prototyping activity should be performed by a small team of about one to three senior developers these developers should be skilled at demon strating a software look and feel with the least amount of work possible if developers are taking more than a couple of hours to develop the proto type for any single part of the software they are doing too much work they are developing the prototype with too much functional depth keep it as shallow as possible developing the prototype in this way helps users visualize the soft ware they are specifying which minimizes the problem of users not know ing what they want until they see it and then changing their minds later in the project this kind of prototyping reduces the risk of creeping require ments which is traditionally one of the most serious risks a software project faces be sure the users understand that the prototype is just a prototype one risk of creating a user interface prototype is accidentally raising unrealistic expectations about future progress on the project ii survival preparations use paper storyboards if possible one low tech approach to user interface prototypes is paper storyboards in this approach developers or end users start the prototyping process by drawing pictures of the screens dialogs toolbars and other elements they would like the software to have developers and end users meet in groups and draw sample screens on flip charts they work out the details of the software by simply redrawing the prototype on flip charts until they reach agreement about the software appearance and functionality this approach offers several advantages users can do some of the work themselves they don t have to know how to use prototyping software storyboard prototypes can be generated and modified rapidly they are inexpensive paper storyboards also eliminate some of the most common risks as sociated with prototyping on the developer side they eliminate the risk of needlessly overextending the prototype and of spending too much time fid dling with the prototyping tools on the user side storyboards eliminate the problem of users thinking that a realistic looking prototype is almost the real software the one disadvantage of paper storyboards is that some developers and end users simply cannot visualize the software from a paper mock up this is a significant disadvantage since the strongest reason to create a proto type is to help the users visualize the software that will be built if you find that paper storyboards aren t helping the project stakeholders visualize the software don t hesitate to revert to electronic prototypes revise the prototype until the end users are excited about the software the first version of a prototype will rarely satisfy the users expectations developers should show it to the users with the expectation that the users will have comments that will cause the developers to revise the prototype they should explain to the users that the prototype is a work in progress and that they want the users criticism if the users are confused or think the software will be hard to use that isn t the users fault that a problem that should be corrected during prototyping before work on the actual software requirements development begins prototype development will take more time if the development team is working in a new area one in which it has less than two years of experi ence rather than in a familiar area refining the simple prototype until users are excited about it supports creation of software that will ultimately be highly satisfactory to end users rather than software that merely satisfies requirements it might seem as though developers are spending an inordinate amount of time working on something that will be thrown away but this upstream activity is a good investment in preventing costly downstream problems while revising the prototype remind users that it is just a proto type in one organization i worked with when developers reminded the users that they were seeing just a prototype the users started chanting almost in unison and it just smoke and mirrors and it will still take a long time to develop we know we know we ve heard that before the users were good natured about the reminder the developers had done a good job of educating them develop a style guide once the users have agreed to the general look and feel of the prototype the developers should create a user interface style guide that sets standards for the application look and feel this style guide should be only a few pages long including screen shots but be complete enough to guide remaining work on the prototype it should include the size and placement of standard buttons such as ok cancel and help allowable fonts the style of error messages mnemonics for common operations adherence to common design standards and other common elements of the user interface after it is de veloped the user interface style guide should be reviewed and put under change control addressing design style issues early in the project promotes a consis tent user interface it also prevents developers from continually tweakingthe user interface and prevents users from requesting arbitrary late changes to the software when it is finally delivered for an example of a prototype style guide see the survival guide web site ii survival preparations fully extend the prototype driving prototype development across the full breadth of the system enables developers to visualize the software to a degree that promotes better archi tecture and design work the project vision statement enables developers to align their work in a general way the detailed user interface prototype provides the detailed vision that enables them to align their work in thou sands of smaller ways be sure that the prototype truly addresses all of the software function ality here is a partial list of the functionality that should be represented all dialog boxes including standard dialogs such as file open file save printing and so on all graphical input screens all graphical and textual outputs interactions with the operating system such as importing and exporting data to the clipboard interactions with all third party products such as importing and exporting data to other programs providing components to be embedded in other programs and so on at the time the prototype is baselined the development team will prob ably be unclear about whether implementing some of the specific function alitv demonstrated in the prototype is technically feasible this question about feasibility is a natural aspect of prototyping since it is easy to prototype func tionality that couldn t be implemented in a million years for example you could easily prototype a system that draws a photographically realistic pic ture based on a textual description typed in by the user but actually imple menting that is well beyond the current or foreseeable state of the art a list of functionality that is at risk should be documented explained to the end users signed off and put under change control this list can be included as a section in therequirementsspecification it isa rareprojectthat willnothaveanyfunctionalityin this category soyou should expect to have a list of to be determined areas on your project requirements development remember that it s a throwaway prototype although the prototype is driven across the full breadth of the software the emphasis should continue to be on demonstrating functionality with the least possible amount of work remember that the prototype is a dead end it is a useful dead end but a dead end nonetheless a good prototype is slapped together quickly with programming duct tape and baling wire it is like a hollywood stage set in which the front of a piece of wood is painted to look like a house but the back of the piece of wood has no house connected to it you shouldn t use a user interface prototype as the basis for real software any more than you should use a hollywood stage set as the basis for a real house when a prototype is developed in this way do not under any circum stances use it as the code base for releasable software it has intentionally been developed to be disposable trying to use its code in the releasable software is tantamount to trying to build a real house on the foundation of the hollywood stage set house consider developing the prototype in a programming environment that makes it clear that the prototype code will not find its way into the fi nal software if the project team is planning to implement the software in c or java for example develop the prototype in microsoft visual basic so that it would be impractical to use it as the basis for the real software treat the fully extended prototype as the baseline specification by this time the prototype can begin to serve as the reference point to which the whole development effort will be aligned for it to serve that function it must be stable and the team must commit to developing software that ex actly matches the prototype the project decision makers must sign offon it and put it under change control the organization will base its estimates plans staffing architecture designs and development work on it it is both appropriate and necessary that the decision makers commit to the prototype and that changes to it be systematically controlled systematically controlling changes does not mean freezing the re quirements instead the project is making a commitment to handle further changes to the software in strategic ways that support the project objec tives the most common alternative approach is to handle changes indis criminately with no strategic plan in ways that ultimately impair the project ability to achieve its objectives once the prototype has been baselined it provides several valuable benefits in addition to the requirements stability benefits already described a full user interface prototype enables end user documentation and test plans to be developed in parallel with architecture design and implemen tation work otherwise these activities often end up on the critical path because they cannot be started until implementation is well under way write the detailed end user documentation based on the prototype sometime prior to placing the prototype under change control work can begin on a detailed user documentation called the user manual requirements specification this is the documentation that will eventually be delivered to the software end users typically this documentation is developed at the end of the project but in this book approach it is developed near the beginning some people might object that creating full up end user documenta tion and keeping it up to date takes more time than creating a purely tech nical requirements spec that true as far as the requirements spec alone is concerned but creating the user manual requirements specification is often cheaper than developing end user documentation and a full technical spec developing end user documentation first eliminates the necessity of producing a stand alone technical specification the document that is pro duced is more understandable to end users than is the typical technical spec which improves the quality of end user feedback about the software the early development of the end user documentation fleshes out the details of the software demonstrated by the user interface prototype filling in gaps and flushing out inconsistencies this approach involves end user advocates in the project early on technical writers tend to be good at representing the end user point of view often with more sophistication than the end users themselves are capable of this kind of spec is more likely to be kept up to date many require ments specifications become write only documents they are written at requirements time to satisfy the project plan edict to create a requirements specification but they aren t kept up to date with requirements changes and often aren t even used after they are written they are too monolithic and too boring in contrast when the development team knows that the spec will ultimately be released to end users it has a greater incentive to keep the spec up to date this approach also solves the perennial problem of whether the re quirements spec should cover the what or the how if the technical writers think the end users need to know something what they need to know should be in the user manual requirements specification anything that not covered in this specification is left to the developers discretion many products contain complex documentation sets that include a user manual tutorial reference and online help the only item that needs to be created during the prototype development stage is a document that de scribes the end to end functionality of the software if this document is en visioned as part of the online help developing the online help first will provide the most useful reference for the development team createaseparate non user interface requirements document the approach described in this chapter works best for user intensive small to medium sized projects most projects will have some functionality that cannot be specified in enough detail in a user interface prototype or user manual to provide guidance for design and implementation specifications of detailed algorithms interactions with other hardware and software per formance requirements memory usage and other less visible requirements can be recorded in a separate requirements document or as an appendix to the usermanual requirements specification afterits initial development that document or appendix will need to be reviewed baselined and placed under change control survival check the project team identifies a set of key end users who can be trusted to shape the software the developers create several versions of a basic prototype until they find one that the users are excited about prototyping stops while users are still only lukewarm about the prototype the developers extend the user interface prototype to elicit de tailed requirements from users the prototype is not kept broad and shallow and time is wasted providing deep functionality that will be rewritten for the final software the project team develops a user manual requirements speci fication to use as the detailed requirements specification the fully developed prototype is baselined and placed under change control the project team attempts to use the prototype code in the real software a separate non user interface requirements document is created reviewed and placed under change control software quality assurance used to be thought of exclusively as testing but on effective projects it now encompasses testing technical reviewing and project planning all oriented toward early economical detection and correction of defects people mean many different things when they talk about quality quality can refer to the absence of system crashes it can refer to the cor respondence between the software and the users expectations or to the intangible fit and finish of a program it can refer to the conformance to speci fied requirements or to the specification of correct requirements in the first place a good general definition of quality is the degree to which the soft ware satisfies both stated and implied requirements this chapter explains how to assure that kind of quality why quality matters even if your software doesn t have to be ultra reliable keeping defects un der control matters because it affects development speed development cost and other project characteristics chapter survival concepts described the upstream downstream effect defects cost to times as much to correct if they are detected and corrected downstream instead of upstream that should be enough reason to focus on quality but other bottom line impacts exist as well people sometimes think they can take quality shortcuts on their cur rent project and then correct them on the next project on the smallest projects those with durations of one to two months or less a project team can sometimes cheat the hangman on longer projects quality shortcuts turn out to be self defeating it isn t possible to defer all the bad effects arising from quality shortcuts to the next project many of those effects will affect the current project software quality has a bottom line impact on the costs of doing busi ness low quality software increases the burden on end user support lead ing edge companies such as microsoft have recognized this by charging end user support costs back to the business unit that produced the software responsible for the support call developing low quality software and then building upon that shaky foundation also increases maintenance costs you might think that your program will be around for only to years but the average program ac tually survives long enough for generations of maintenance programmers to support it since to percent of the lifetime cost of a program tends to be incurred after the program initial release it makes good financial sense to have version lay the groundwork for success rather than failure finally only you can decide what effect releasing low quality software will have on your business my strong impression is that customers do not remember that high quality software was delivered late or that low quality software was delivered on time as much as they remember whether they like using the software the problem with quick and dirty as some people have said is that dirty remains long after quick has been forgotten the quality assurance plan one of the themes of this book is that organizations intending to survive a software project must commit to surviving the project software project sur vival requires a commitment to quality assurance which involves at least these elements software quality assurance activities must be planned the plan for the software quality assurance activities must be committed to writing software quality assurance activities must begin during software requirements activities or earlier aseparate group for performing software quality assurance must exist depending on the size of the project that group might consist of one person it might even consist of two developers from two different one person projects swapping quality assur ance work on each other projects members of that group must be trained in how to perform soft warequality assurance activities it isnot sufficient to point to the least senior programmer and say you there you are now the quality assurance department adequate funding must be provided for performing software quality assurance activities ii survival preparations elements of the quality assurance plan an effective quality assurance plan will call for several quality assurance activities defect tracking unit testing source code tracing technical re views integration testing and system testing defect tracking runs throughout these quality assurance activities the project team keeps records of each defect found its source when it was detected when it was resolved how it was resolved fixed or not and so on defect tracking is a critical element of project tracking and control good defect information helps determine how far the project is from release what its quality is and where the greatest potential for improved efficiency in the development process lies unit testing is informal testing of source code by the developer who wrote it the word unit can refer to a subroutine a module a class or an even larger programming entity unit testing is usually performed infor mally but should be required for each unit before that unit is integrated with the master sources or turned over for independent review or testing source code tracing is stepping through source code line by line in an interactive debugger this work is performed by the developer who wrote the code many developers have found this to be an invaluable means of detecting coding defects and my own experience bears this out anecdotal evidence suggests that projects that require developers to step through their code in a debugger before integrating it experience far fewer integration problems than projects that don t use source code tracing technical reviews are reviews by developers of work their peers have completed these reviews are used to assure quality of the user interface prototype requirements specification architecture designs and all other technical work products new source code and source code changes should also be reviewed technical reviews are generally conducted by the devel opment team the quality assurance staff role in the reviews is to ensure that they are being carried out and to track defects detected during the re views integration testing isexercisingnewly developed code incombination with other code that has already been integrated this kind of testing is car ried out informally by the developer who developed the new code system testing is the execution of software for the purpose of finding defects system testing is performed by an independent test organization or quality assurance group taken in combination this set of quality assurance activities might seem to result in a lot of overhead but in actuality exactly the opposite is true the point of a multi layered quality assurance approach is to detect as many defects as possible as early as possible to keep the costs of correc tions down defect tracking defect tracking is the activity of recording and tracking defects from the time they are detected until the time they are resolved defects are tracked both at the individual level that is defect by defect and at the statistical level in terms of number of open defects percentage of defects resolved average time required to correct a defect and so on beginning defect tracking early in the project increases the awareness of the importance of eliminating defects early and over the course of a project provides the most accurate information about the number of defects detected in the software defect tracking should begin early in the project preferably by the time requirements have been baselined after a work product is placed under change control when a programmer discovers a design defect at implemen tation time that defect should be tracked because the design has been baselined if the same coder discovers a coding error in new code that has not yet been baselined that defect shouldn t be tracked but if the coder discovers the same defect after the code has been reviewed and baselined that defect should be tracked defect reports should be placed under change control making all de fects public provides valuable data about software quality and allows the project team to estimate the number of defects remaining in the software counts of defects removed should be used to track progress during testing activities this defect tracking might sound like a lot of drudge work but it doesn t have to be many specialized tools are available to make this work almost painless table on the following page lists the information that should be tracked for each defect for a current list of such tools see the survival guide web site defect id a number or other unique identifier defect description steps taken to produce the defect platform information cpu type memory disk space video card and so on defect current status open or closed person who detected the defect date the defect was detected severity based on a numeric scale such as or a verbal scale such as cosmetic serious critical and so on phase in which the defect was created requirements architecture design construction defective test case and so on phase in which the defect was detected requirements architecture design construction and so on date the defect was corrected person who corrected the defect effort in staff hours required to correct the defect work product or products corrected requirements statement design diagram code module user manual requirements specification test case and so on resolution pending engineering fix pending engineering review pending quality assurance verification corrected determined not to be a defect unable to reproduce and so on other notes collecting this defect information enables the project team to create graphs and reports that are useful for tracking and assessing project status as well as for developing a base of information that is useful on future projects chapter software release describes in more detail about how to use this information technical reviews because of their ability to detect and correct defects in upstream work prod ucts technical reviews are at least as important in controlling cost and sched ule as testing is the generic term technical review refers to any of several review techniques including walkthroughs inspections and code reading in addition to their quality assurance benefit reviews serve as a time during which junior and senior developers can cross pollinate senior de velopers can fix problems in junior developers code and junior developers can learn more about their craft by reading senior developers code reviews also provide a chance for junior developers to present new methodologies and challenge old assumptions general review pattern reviews follow a general pattern notification and distribution the author of the work product no tifies the reviewers that the work product for example the project plans requirements user interface prototype design code or test case is ready to be reviewed in formal settings the author will give material to a moderator who will decide who should review the work product and who should attend the re view meeting material is distributed for review preparation reviewers review the work product preferably aided by a checklist of errors that have been most common in the past the review meeting should be held only after reviewers have completed their reviews of the work product review meeting the author moderator if there is one and re viewers meet to examine the work product review report after the meeting the author or moderator should log the statistics for the review meeting the amount of material reviewed number and kind of defects detected amount of time spent in the review meeting and whether the work product passed or failed the review follow up the author or some other person makes any required changes the changes are reviewed and the work product is de clared to have formally passed the review ii survival preparations keys to success in using reviews for best results pay attention to the following key points begin reviews early in the project technical materials created during re quirements architecture and design should all be reviewed quality assur ance materials such as the quality assurance plan and test cases should be reviewed by both quality assurance and technical staffs reviews should continue through implementation all detailed designs and source code should be reviewed management work products including the project schedule and software development plan should also be reviewed keep technical reviews focused on defect detection during a technical review the focus should be on detecting problems spending meeting time creating and evaluating solutions typically wastes the time of at least some of the people in the review meeting and is best handled as a separate activity keep technical reviews technical if reviews are not focused they can degenerate into technical chest thumping contests the presence of any sort of authority figure at a technical review changes the focus to impressing the authority figure and for that reason neither management nor customers should be allowed to attend reviews for the benefit of management or customers might well be appropriate but they are not technical reviews in the sense described here and should be conducted separately keep track of what material has been reviewed tracking review progress of designs and code becomes another useful measure of the project status by tracking the number of modules reviewed per week you can know how many modules have yet to be reviewed if the project reviews many more modules than average in a week in order to meet a deadline the reviews might have been superficial testing may expose more defects than average in those modules record the defects detected during reviews theworkproductthat comes out of a review is a review report the review report should list the defects detected and provide a schedule for corrections and for verifying the cor rections quality assurance verify that work identified during the review is performed one common weakness of technical reviews is that the project doesn t follow up on the defects identified during the review for this reason the defects detected during a technical review should be entered into the defect tracking system just as the defects detected during system testing are that allows the defects to be tracked until they are closed just as other defects are make review results public to the project team although the review it self should contain only technical personnel review results should be made public within the project team not beyond it so that they can be used by other members of the project this would allow for example devel opers to juggle their schedules and delay writing code that interfaced with an error filled module a module that had so many errors it would have to be rewritten allow time in the schedule for reviews and for correcting problems iden tified during reviews reviews cannot be effective if the project leaders expect them to be completed in addition to the developers normal work on a successful project reviews are an integral part of developers normal work and should be scheduled just as the rest of their work is if reviews are effective they will detect problems in the plans designs test cases or source code under review the schedule should include time to correct the problems identified during the review system testing reviews are a critical means of assuring software quality upstream system testing is the critical means of assuring it downstream here are some of the keys to successful software system testing conduct system testing by using independent testers tobe effective soft ware needs to be tested by personnel other than the developers who created it developers can find a certain number of defects in their own code but finding the maximum number of defects requires a shift in mind set from making the software work to making it break and precious few developers are capable of wearing both those hats on the same project begin test planning as soon as requirements are known effective system testing depends on effective planning test cases need to be designed re viewed and implemented just as source code does if you don t want test ing to become the critical path activity that holds up release of your software be sure test planning begins as early as possible as soon as requirements are known begin system testing during stage in a staged delivery approach exe cutable software will become available midway through the first stage and system testing should begin then ensure full requirements coverage with a requirements traceability matrix software system testing should be planned so that it covers percent of the software functionality this is typically assured through a process called requirements traceability in which a large matrix is created with test cases in the rows and requirements in the columns a in a row and column indicates that that row test helps to verify that column re quirement a row without a indicates that the row test case doesn t verify any requirement is not needed and can be eliminated a column without a indicates that no test case has been created to verify that column requirement every column and every row should have at least one creation of the requirements traceability matrix is monotonous work but it is the best way to ensure that the full range of the software is being tested provide adequate resources for system testing the resources required to adequately test computer software vary depending on the kind of software being developed for high quality commercial software a ratio of one tester per developer is a good rule of thumb this is the ratio used at microsoft and other top software companies this number of testers is required because most of the test suite needs to be automated so that quality assurance can test the software end to end as often as the software changes sometimes as often as every day some mission critical business systems will also require this much testing for life critical software a greater number of testers is required the flight control software for the space shuttle used testers per developer at the other end of the scale in house business systems that do not have to be ultra reliable require a smaller testing commitment perhaps as few as one tester per three or four developers the test cases do not have to be fully automated and the software does not have to be quite as reliable so the testing requirements are less demanding break the testing addiction cycle testing by itself doesn t improve soft ware quality any more than stepping onto a scale by itself can make a per son lose weight testing is a means of discovering the quality level of a software system not a means of assuring software quality when testing is combined with defect correction the test and fix combination constitutes a means of assuring software quality but not a very effective one a more effective approach is to combine upstream quality assurance activities such as user interface prototyping and technical re views with downstream testing this approach is both more economical and more effective the lack of upstream quality assurance activities locks many organi zations into a damaging test addiction cycle their software has poor quality because too little work has been performed upstream which leaves huge numbers of defects to be detected downstream because of the huge numbers of defects many testers are needed on the low quality project which makes it hard to allocate the quality assurance resources needed for effective upstream work on the next project as more resources are devoted to downstream testing on one project and less to upstream prevention on the next project the addiction worsens the next project will experience even more downstream problems and need even more downstream testing the only way to break the testing addiction cycle is to bite the bullet and allocate the upstream resources needed for quality assurance on a project once the beneficial results of that approach are seen on one project it will become easier to obtain upstream resources for the next project ii survival preparations beta testing under this book recommended approach general quality assurance is addressed through the internal practices of technical reviews and system testing by an independent testing organization but companies release soft ware to external beta testers for a variety of reasons which are summarized in table some of the reasons listed are technical most are nontechnical the important thing to realize is that all of the technical reasons to use beta testing are better served through means other than beta testing with the possible exception of compatibility testing expert consulting some organizations show the software to expert users to see what the experts will think about it and to change it to be more appeal ing to the experts magazine reviews some organizations show software to magazine reviewers prior to releasing it to the general public in order to garner favor with them customerrelationship building some organizations release beta software to their key customers to let those customers know that they receive preferen tial treatment testimonials andspin control organizations sometimes release software to the general public in order to get complimentary statements about the soft ware which can then be used in marketing materials other organizations gather customer comments to identify popular and unpopular aspects of the software and then emphasize the most popular aspects in their market ing materials polishing the user interface design based on customer usage patterns some orga nizations put their nearly complete software into customers hands so that they can observe how customers use it with the goal of fine tuning the soft ware to overcome commonly experienced difficulties compatibility testing some organizations release beta software to customers so that they can determine how well it runs with a wider variety of hard ware and software environments than they have available in house general quality assurance some organizations put their software into as many users hands as possible under the assumption that the more users who use the software the more defects thev will find prior to release sources adapted from testing computer software ed kaner falk nguyen and software project dynamics mccarthy expert consulting during beta testing is a case of too little too late if you want expert consulting get it at requirements time in response to the user interface prototype polishing the user interface design is another case of too little too late and should also be handled at requirements time rather than at beta test time general quality assurance was once the primary reason for beta test ing but software companies have found that external beta testing is no bar gain when companies first sent out beta software most of the people who received the software didn t report any defects and they often didn t return any comments at all so organizations started to restrict to whom they would give beta software but then they found themselves overwhelmed by re quests from beta users to change the software and they still weren t receiv ing defect reports after experiences like this organizations realized that beta testing produces lots and lots of low quality feedback and doesn t serve a useful quality assurance purpose though it might serve useful marketing purposes if you want feedback from real end users pay representative users to come to the project facilities and use the software under supervision in stead of conducting a widespread beta test program videotape the users entire session with the software so that the development team can reproduce any problems the users encounter supervised user testing will produce more focused feedback than beta testing for this reason the leaders in the commercial software industry have largely moved away from using beta testing for quality assurance purposes an effective beta test program requires a great deal of coordination and typically diverts resources that could provide more quality assurance benefit iffocused inside the organization if your software will ultimately be distributed to users numbering in the thousands however you ll probably want to create some external re leases before you create the final release these external releases are not for general qualityassurancebutforspecificcompatibilitytesting noteventhe richestorganizations canaffordtopurchaseand exhaustivelytestthe incred ible variety of hardware and software combinations found on modern desktops once the software has been thoroughly system tested the only practical way to perform compatibility testing is to release it to a set of for giving external users work products covered by the quality assurance plan the quality assurance plan should indicate the work products that will be reviewed or tested table describes the quality assurance practices that should be applied to each of the work products in this book change control plan change proposals vision statement top risks list software development plan including estimates user interface prototype user interface style guide user manual requirements specification quality assurance plan softwarearchitecture software integration procedure staged delivery plan individual stage plans including miniature milestone schedules coding standard software test cases executable software source code new source code changes media including graphics sound video and so on software build instructions make files detailed design documents software construction flan for each stage install program deployment document cutover handbook release checklist release approval software project log software project history document as you can see every work product placed under change control is re viewed and some are also tested some work products such as the user manual are tested in the sense that someone actually punches every key stroke described in the manual and verifies that it works as documented specific responsibilities canvary quite a bitfromoneproject to another especially depending on the skills and interests of the specific documenters managers customers marketers and end users involved with the project ii survival preparations supporting activities in addition to their explicit quality assurance activities the quality assurance group will participate in the preparation and review of the project software development plan standards and procedures the qa group will review the software development activities to verify that reviews unit tests and source code tracing are performed it will periodically report the results of its activities to developers and management and will periodically review its activities with senior management software release criteria in many of the organizations i review for the consulting side of my business the sole determination of when to release the software lies with the software development organization this is too much like putting the fox in charge of the chicken coop developers and development managers are eager to meet their schedule targets and to believe that their software has excellent quality a set of checks and balances is needed to counteract that natural tendency and the quality assurance group provides that for this reason the quality assurance plan must specify in measurable detail what the release criteria for the software are the release criteria can be something like zero reproducible software crashes or mean time to defect of hours or percent of all reported defects corrected or no uncor rected sev or sev errors the release criteria must be measurable so that the quality assurance group can report whether the software is ready to release without having to make a politically unpopular judgment call quality assurance survival check project has a written approved quality assurance plan project isn t following the written plan quality assurance is initiated in parallel with requirements work defect tracking software is placed online at requirements devel opment time and defects are tracked from the beginning of the project developers review all designs and code before the designs and code are considered done no designs or code have failed their reviews suggesting that reviews are superficial developers don t source trace and unit test their own source code prior to submitting it for review which gives rise to an unwieldy number of defects that have to be tracked from reviews onward the quality assurance plan calls for an independent quality as surance group no funding is available for an independent quality assur ance group the quality assurance plan contains measurable criteria that are used to determine whether the software is ready to be released software architecture provides the technical structure for a project good architecture makes the rest of the project easy bad architecture makes the rest of the project almost impossible a good software architecture document describes overall pro gram organization ways in which the architecture supports likely changes components that can be reused from other sys tems or purchased commercially and design approaches to standard functional areas it also minimizes potential down stream costs by enumerating how the architecture addresses each system requirement ii survival preparations n building construction the architecture phase is a time during which a general plan for a building is mapped out through the use of scale mod els and blueprints before the building is actually constructed scale models and blueprints provide a means of exploring alternatives more cheaply than using steel concrete and other building materials in software development the architecture phase is a time during which the software is mapped out through the use of design diagrams and proto types as in building construction the architecture phase provides a means of exploring alternatives at relatively low cost typically the architecture which is also known as system architecture design high level design and top level design is described in a document referred to as the soft warearchitecturedocument during the architecture phase the architecture team partitions the system into major subsystems specifies all the interactions among sub systems and documents the software top level technical plans the team also addresses the major design issues that run throughout the system such as the approaches to error handling memory management and string stor age architecture paves the way for detailed design work by defining the structure that the detailed designs will use on small projects architecture and design might be rolled into one activity but on most projects architecture should be treated as a separate activity fred brooks authorofthemythicalman month reportsthat hav ing a system architect is the most important single step toward conceptual integrity after teaching a software engineering laboratory more than times came to insist that student teams as small as four people choose a manager and a separate architect defining distinct roles in such small teams may be a little extreme but i have observed it to work well and to contribute to design success the discussion in this chapter assumes that the software architecture is developed by a small team of architects regardless of exactly how the roles are conceived the issues described in this chapter should be consid ered caretully and addressed before proceeding with detailed design and construction easing into architecture architectural work should begin when requirements development isabout resolving ning architectural work usually isn t possible without delaying the project by the time percent of the requirements are known the project is on firm enough footing to support creation of the software architecture eighty per cent is a rule of thumb and the project leaders will need to use their own judgment on any specific project to evaluate whether requirements have been sufficiently developed for architecture to begin before the architecture team begins architectural workin earnest how ever the project team upper management and the customer should hold the planning checkpoint review described in chapter plunging into full fledged architectural development is pointless if the project isn t going to be funded characteristics of a good architecture when the architecture team does plunge into architectural development they ll face a set of core design issues which the project architecture should address system overview a system architecture needs to describe the system in broad terms with out such an overview developers will have a hard time building a coherent picture from a thousand details or even a dozen individual modules or classes the architecture should also contain a high level discussion of the major design alternatives that were considered the reasons that the selected approaches were chosen and the reasons the other alternatives were not selected conceptual integrity the objectives for the architecture should be stated clearly a design for a system with a primary goal ofmodifiability willbe different from a design for a system with a goal of uncompromised performance even if these sys tems have the same function a good architecture should fit the problem whatever it is after days or weeks of wrestling with the architectural design the architect should create an architecture that addresses the problem so well that when other people see the architecture they say that seems obvious how else could you do it harlan mills referred to this quality as deep simplicity an architecture that is more complicated is worse notbetter beware of kitchen sink architectures architectures that try to address every conceivable aspect of every conceivable problem it should be clear from the architecture developed that the architecture team has taken advan tage of opportunities to simplify its approach as one measure of simplic ity the best architecture documents are short diagram intensive and average less than pages the central thesis of the most popular software engineering book ever the mythical man month is that the essential problem of large systems is maintaining their conceptual integrity when you look at the architecture you should be pleased at how natural and easy the solution is it shouldn t look like the problem and the architecture are stuck together with duct tape subsystems and organization the architecture should define the major subsystems in a program sub systems are major clusters of functionality such as output formatting data storage analysis user input and so on most systems should contain five to nine subsystems with many more the system will be too hard to under stand at the architectural level figure shows the appropriate level of detail for subsystem design in an application program figure example of subsystem architecture diagram most architectures have five to nine top level subsystems good architectures have relatively few interactions among their subsystems in addition to a diagram like the one in figure the architecture should describe the responsibilities of each subsystem and provide a prelimi nary list of the modules or classes that will be contained within each sub system the final list will be developed during detailed design and construction the architecture should describe what communications are allowed among the different subsystems in figure only a few inter subsystem communications are allowed figure illustrates what happens when there are no rules guiding how subsystems communicate with one another subsystems interact in every possible combination which undermines the goal of minimizing complexity a good architecture holds communications among subsystems to a minimum figure example of architecture without rules limiting communication among subsystems without communication limitations architectures become uncontrollably complex restrictions on communication among subsystems typically cannot be enforced automaticallyby standard software development tools conform ance to architectural guidelines should therefore be a line item on the check lists used for detailed design and code reviews be aware however that developers who haven t worked on projects with well developed architec tures sometimes resist the restrictions the architecture places on their coding once you realize that the goal of architecture is to reduce complexity it becomes clear that the architect must focus as much on what to leave out of the software as on what to put in notation for large projects the project should adopt a standard notation such as the booch rumbaugh unified modeling language uml notation for smaller projects just be sure that everyone understands what the diagrams mean and that the diagrams are readily available in a public location later in the project at detailed design time the development team may use the same notation it used for the architecture or one that appropriate for more detailed design work if it uses uml for the architecture it might want to continue using it the development team might also begin using pseudocode to express the designs of individual routines regardless ofwhat it chooses the development team should agree upon the specific notation at architecture time so that it will be used projectwide change scenarios and change strategy one key feature of a successful architecture is the identification of program areas that are most likely to change some of the most damaging influences on a project are late unexpected changes to the software changes that oc cur after the design is complete and while implementation is under way projects that deal poorlywith such changes can run into trouble evenifthey are under control up to the point at which the changes are introduced so the architecture should list the most likely changes to the program and outline hoxv each change will be addressed one common source of changes is shaky requirements so the architecture should include a list of the requirements that are the least stable another common source of problems is changes in supporting tech nology for example the developers find that the company that provided the class library they have been using has gone out of business be sure the project isn t bound to any single source technology compiler vendor hard ware platform and so on where that not possible be sure the architec ture contains firewalls so that the developers can recover if that single source technology becomes unreliable in considering change scenarios strive to elaborate about percent of possible scenarios and then stop the architecture team cannot possibly anticipate every possible change and by the time it has elaborated per cent of the scenarios it will have reached a point of diminishing returns reuse analysis and buy vs build decisions one key component of the architecture is the identification of which com ponents will be purchased commercially which will be reused from internal sources and which will be created from scratch the project approach to reuse will affect the rest of the software design so it should be defined at architecture time if the architecture team decides to base an application on a commercial application framework for example the rest of the software will need to be designed around that frame work another reason that reuse should be considered at architecture time is that the approach to reuse has a dramatic cost and schedule implications one of the most powerful ways to shorten a software schedule and reduce costs is to buy components or reuse existing software instead of building new software the architecture team should not limit its reuse considerations to source code it should also consider how to reuse data detailed designs test cases plans documentation and even architectural components all of which can sometimes be used again approaches to standard functional areas in addition to system organizational issues the architecture should focus on the handful of design decisions that have the most sweeping implications for detailed implementation here are some of the most common functional areas the architecture should address external software interfaces is the software expected to commu nicate with other software what is the function call protocol which data structures are passed across the program interfaces and what do they look like user interface how is the user interface isolated from the rest of the system so that changes to the interface won t affect other soft ware components database organization what is the organization and content of the database data storage how is nondatabase data stored what file formats are used what are the major data structures key algorithms what are the software key algorithms have they been well defined or is part of the software project mission to define new algorithms memorymanagement whatis the strategyforallocatingmemory to different program elements stringstorage how are text strings such as error messages stored and retrieved concurrency threads is the software multi threaded if so how are concurrency re entrancy and related issues handled security is the software required to operate in a secure environ ment if so how is the software design affected localization is the software expected to work in countries that use a language other than the language for which the software was originally targeted how are different text strings different character sets and possibly different page orientations right to left instead of left to right handled networking how does the software support multiuser opera tions on a network portability how does the software run in more than one environ ment for example in both unix and microsoft windows nt programming language does the architecture allow the software to be implemented in any programming language or does it re quire the software to be implemented in a specific language errorhandling does the architecture contain a coherent error handling strategy architecture requirements traceability the architecture team should create a traceability matrix like the one de scribed for test case coverage in chapter quality assurance creation of this requirements traceability matrix is typically a tedious job it can also be frustrating because just when the architecture team thinks it has completed the architecture it finds a handful of requirements that aren t satisfied by any of the subsystems frustrating or not when the team finds an unsatisfied requirement congratulations are in order the time the team spends finding and correcting that architecture error is to times less than the time it would have spent detecting and correcting that same error during construction or system testing ifyou re going to get bad news on a software project it best to get it early because of the upstream downstream effect on project costs during requirements development the project team should have cre ated a list of functional areas that were at risk because of uncertainty about whether those functions could be implemented by the time the architecture is complete most of the items on that list should be resolved the architec ture team will have outlined ways to implement them or it will have decided that the items can t be implemented and it will eliminate them from further consideration support for the staged delivery plan one final characteristic of the architecture is of critical importance for projects that use this book approach the architecture must support the staged delivery plan the software architecture document should explain how the architecture accommodates the development and delivery of the planned functionality in the stages contained in the staged delivery plan specifically it should not depend on subsystems that have to be implemented all at once and that cannot be broken into stages it should identify the depen dencies among different parts of the system and map out a plan for develop ing the different parts in an order that supports the staged delivery plan architecture is typically a time when you run into some resistance to staged delivery staged delivery will require the architecture team to make some nonoptimal architectural decisions such as creating temporary scaf folding code that might not be needed if some other approach were used or requiring extra code to minimize dependencies among subsystems the important question to keep in mind is nonoptimal for what an architec ture that doesn t deliver critical functionality until the end of the project is not optimal neither is an architecture that depends on an implementation strategy that contains so much risk that the project is likely to be canceled the architecture cannot be designed solely with static end system goals in mind it must also be designed with dynamic development process goals in mind how to tell when architecture is complete knowing when the architecture team is finished with the architecture is a genuine challenge on the one hand you shouldn t declare the architecture to be done until the architecture team is willing to bet the project on it until the architecture team is willing to consider their architecture frozen and proceed with the project you don t actually freeze the architecture but the architecture team should be that confident in it on the other hand the architecture will never be perfect and at some point the development team just has to plunge into the rest of the project the team won t have perfect knowledge of all the project design issues until the project is well into coding so you shouldn t expect the architecture to be perfect follow the advice given to the team that designed the algol programming language the best is the enemy of the good if vou try for the best you often end up with nothing strive for minimalism simplicity and coverage of all requirements and don t worry too much about finding the single best solution be sure that activities and deliverables are well defined and closely monitored the architecture team can easily spin its wheels during the archi tecture phase causing the phase to be nonproductive the software architecture document once the architecture is complete it should be described in the software architecture document submitted to the change control board circulated for review revised as needed and baselined the software architecture docu ment then becomes the standard that controls future design and develop ment work be sure to enforce the architecture through the technical review process there is no point in creating an architecture unless it is implemented throughout the rest of the project as further work is done the architecture will inevitably need to be revised when that happens the architecture should be updated through the usual change control process following that process ensures that the archi tecture isn t changed capriciously and it helps highlight the true costs of architecture defects compared to downstream detailed design and construc tion defects final preparations the final preparations period builds on and extends the preliminary planning that was performed before the requirements development and architecture phases at final preparations time the project team is ready to create its first estimates develop a plan to deliver its most important functionality first and refine its other plans ii survival preparations preparing for survival on a well run software project is an ongoing activity after the project team has baselined the requirements and begun architecture it can create more detailed plans than were possible in the early stages of the project i refer to this point in the project as final preparations time during this time some of the earlier plans will need to be revised to reflect changes in the software concept that occurred during requirements development other plans will need to be filled in using information that became available after requirements were baselined this chapter describes the preparation work that should be done after the project requirements have been baselined and architecture workis under way this work includes the following tasks each of which is discussed in this chapter creating project estimates writing a staged delivery plan performing ongoing planning activities in addition to the tasks i just listed the project team will need to engage in another kind of planning before it can begin detailed implementation the specific planning for the next staged delivery cycle planning which is conducted at the beginning of each stage that is discussed in the next chapter project estimates as soon as requirements have been baselined the project team can create meaningful estimates for effort cost and schedule keep these rules of thumb about software estimation in mind it is possible to estimate software projects accurately accurate estimates take time accurate estimates require a quantitative approach preferably one supported by a software estimation tool the most accurate estimates are based on data from projects completed by the organization doing the current project estimates require refinement as the project progresses final preparations estimation procedure guidelines effective organizationsfollowasystematicestimationprocedure tive the estimation procedure should include the following characteristics the estimation procedure should be written a written procedure can pre vent overly ambitious project managers upper managers marketers and customers from browbeating the development team into adopting an unachievable effort or schedule estimate part of the value of an estimation procedure is that project estimates must be created according to the procedure once all project stakeholders agree to the estimation procedure you can engage in rational negotiations about the inputs to the estimate feature set and resources rather than irrational arguments about just the outputs budget and schedule estimates should be created by an expert estimator or by the most expert development quality assurance and documentation staff available good estimation requires expertise in software project estimation if you can find an expert estimator make use of that person services if no expert is available the estimate should be created by the person with the most expe rience doing similar work regardless of whether an expert is available be sure the project estimates include input from the people most familiar with the kind of work being estimated estimates should include time for all normal activities table on the next page lists some ofthe obvious and not so obvious activities that should be included in a project estimate you can download a sample estimation procedure from the survival guide web site ii survival preparations architecture detailed design general planning planning for each staged release coding testing creating user documentation creating installation program creating program to convert data from old system to new one interacting with customers or end users demonstrating the software or prototype of the software to upper management customers and end users reviewing plans estimates architecture detailed designs stage plans code test cases and so on fixing problems detected during reviews and testing maintaining the revision control system maintaining the scripts required to run the daily build assessing impacts of proposed changes answering questions from quality assurance answering questions from documentation supporting old projects involvement in tiger teams receiving technical training training personnel who will support the software holidays vacations weekends sick days some projects adopt short term attitudes and consciously plan to exclude many of the activities listed in table that approach can work for a small project but on any project that lasts longer than a few weeks which is any project long enough to need the plan described in this book these activities creep back into the project in one form or another and because these activities aren t accommodated in the project plan the ever widening gap between the plan what the team is supposed to be doing and reality what is really happening on the project creates a dangerous risk to the project the project team stops taking the project goals seriously at that point the project team loses its ability to create meaningful plans track progress and control the project the project plan should not assume the team will work overtime if the project plan assumes that the team will work overtime the project won t have any reserves to draw from if it gets behind beginning a project that has overtime built in from the start is like beginning a hike in the mountains in the winter without carrying extra food and warm clothes everything will work out okay if you re lucky but what sensible person wants to depend on being that lucky to minimize the risk of schedule overruns add more resources at the beginning of the project instead of creating plans that depend on overtime estimates should be created using estimation software a commercial software estimation tool can serve as an objective authority in estimating a software project the best tools provide lists of tasks project roles and detailed schedules tailored to specific kinds and sizes of projects using estimation software can prevent a common destructive dynamic that leads to confrontational negotiations about cost and schedule suppose one of the project stakeholders we ll say marketing rejects an initial estimate because it is too high trying to be cooperative marketing then proposes a few minor cuts and in turn expects dramatic reductions in the project cost and schedule in cases like this estimation software can serve as an impartial third party changes in assumptions about the project can be entered into the software and the software can arbitrate the effect on the project cost and schedule you can download my company estimation software construx estimate at no charge from the survival guide web site the site also contains pointers to other commercial estimation software estimates should be based on data from completed projects the best source of estimation data you have is time and effort data drawn from com pleted projects within the same organization good software estimation programs allow estimators to calibrate their estimates using data from their own organization projects estimators sometimes make the mistake of using planning data from previous projects don t use planning data from other projects unless actual data is unavailable watch for estimates that developers don t accept on projects in trouble often find that developers thought the project estimates were unrealistic from the start developers did not have a chance to review the estimates before the estimates were presented to upper management or to the cus tomer lack of buy in from developers is at best a warning sign that the project goals are unachievable at worst it indicates an adversarial relationship between developers and management which means the project probably has serious motivation and morale problems in addition to the problems it has from overcoming planning errors caused by inaccurate estimates theprojectteam shouldplanto reestimate atseveralspecifictimes during the project as mentioned in chapter survival concepts estimating a project precisely in its early stages isn t even theoretically possible as chapter preliminary planning described the most effective software organizations plan to revise their estimates several times over the course of a project the use of a planning checkpoint review with a two phase fund ing approach builds reestimation into the project plans at the top manage ment level although in this book the section on estimation follows the chapter on architectural design estimates should be made at the end of these specific phases preliminary requirements development after the user interface prototype is developed detailed requirements development after the user manual requirements specificationis complete architectural design estimates should be placed under change control after the project team has completed the estimates at each phase the estimates should be reviewed signed off and baselined just as other critical work products are one of the advantages of putting estimates under change control is that estimates must be reviewed and approved by all concerned parties a rogue customer manager or marketer cannot unilaterally foist a well meaning but unfounded estimate on the project similarly a development team can t slip a new schedule into the project in the middle of the night schedule changes will be visible to all the people affected by the changes milestone targets the project team should use the newly created estimates to set completion date targets for the project major milestones bearing in mind that these milestone targets will be revised and become more accurate as the project progresses the software development plan should include dates for the following major milestones architecturecomplete stage complete stage complete stage complete software release assuming only three stages in addition to major milestone targets as the project progresses the software development plan should be updated to include detailed mile stone targets for whatever phase comes next at this point in the project thatis final preparations the nextphase is stage details about stage are described in the next chapter nontechnical considerations in estimation software project estimation is a double barreled problem the firstpart of the problem is that estimation itself is technically difficult the guidelines described in the preceding section outline what is needed to overcome those difficulties the second part of the problem is that estimation is made even more difficult by pressures from marketers managers customers and other project stakeholders who want the estimates to turn out more to their liking it is not uncommon for an organization to use the most advanced estimation software available base its estimates on the past performance of the people who will build the new software and then have an uninformed executive cut the estimates in half because the schedule is too long saying keep the project the same but give me a shorter schedule makes about as much sense as trying to make a basketball smaller just by squeezing it harder at best you ll temporarily squeeze it into a different shape at worst you ll deflate it making it unusable for its intended purpose a software project canbe squeezed into a different shape too typically the front end gets squeezed first squeezing the front end of a project pushes work into the back end of a project when less time is spent in the front end more defects are created and fewer are removed the project team not only has to correct all the mistakes it made in the front end but it has to correct those mistakes at greater cost than if the front end work had been done correctly in the first place unlike squeezing a basketball squeezing a soft ware project estimate is likely to make the overall project larger instead of deflating it if you want to ensure the success of the software project educate the other project stakeholders about the price of arbitrarily changing cost and schedule estimates without making corresponding changes in the work that needs to be done if you re an upper manager and you want the development team to cut the schedule in half be aware that the team won t always assume the things you take for granted when you tell them to cut the schedule in half be sure to tell them that it okay to deliver only half the software too staged delivery plan under the plan recommended in this book the software is ultimately deliv ered in stages with the most important functionality delivered first figure provides a conceptual overview of staged delivery this is the same as figure on page because the most important functionality is delivered first users critical needs are met sooner staged delivery does not actually reduce the amount of time required to deliver software but it does reduce the amount of time that seems to be required to deliver software each staged delivery presents tangible evidence of the project progress which can be reassur ing if your experience with software consists of projects whose schedules seem to extend into infinity and beyond figure staged delivery staged delivery enables the software to be delivered in stages after requirements and the architectural design are developed it allows the most important functionality to be delivered in the earliest stages staged delivery cannot happen by accident it requires a solid architec ture careful management and detailed technical planning that work is a good investment in the project because it virtually eliminates the common project risks of late delivery integration failure feature creep and friction between customers managers and the project team breaking the project into stages tn a series of staged deliveries the first delivery should be the germ of the software that the project team will ultimately deliver in subsequent releases the project team adds more capabilities in a carefully planned way the final software is delivered in the last stage as figure suggests within each stage the project team performs a complete detailed design construction and test cycle and at the end of each stage it delivers working software it is important to drive the software to a releasable quality level often in order to prevent the project from accumulating a host of loose ends that can t be tied up when you actually do want to release the software planning for the first release is unique in that it tends to shake out the bugs in the architecture and requires a certain amount of initial infrastructure building to take place be wary of project plans that call for building all the infrastructure during stage such plans reduce the risk management benefit of the staged delivery approach if the architecture has been designed well only the minimum infrastructure work required to deliver stage functionality should be performed as a general goal try to deliver the software capabilities in the order of most important to least important defining the deliveries in this way forces people to prioritize their work and helps to eliminate gold plating by deferring delivery of nonessential functionality until later in the project if you do a good job of prioritizing the releases the initial deliveries will reduce schedule pressure during the later releases because the users will already have the most critical functionality stage themes defining the contents of each stage can give rise to feature by feature negotiations that take up a lot of time a good way to decide which features go into which stages is to define a theme for each stage themes are related to the project vision they are the detailed visions that the team commits to at each stage a project team that is developing a word processor might define stage themes such as text editing basic formatting advanced formatting utilities and integration using themes like these raises feature negotiations to a higher level themes make it easier to decide which stage to put a particu lar capability into most features will fall naturally into a specific theme even if a feature falls into a gray area for example automatic list number ing could be considered as advanced formatting or as a utility you can more easily classify it because you only have to decide which of the two themes is most appropriate you don t have to consider every stage both tables on the facing page show an outline of a staged delivery plan based on stage themes when you use themes the project team probably won t be able to deliverfeaturesinexactpriority order instead plantoprioritizethe themes in order of importance and then deliver the themes in priority order stage editing stage basic formatting stage advanced formatting stage utilities stage integration text editor is available and includes editing saving and printing character and basic paragraph formatting is available advanced formatting is available and includes wysiwyg page layout and on screen formatting tools utilities are available and include spell checking thesaurus grammar checking hyphenation and mail merge full integration with other software is complete theme releases work just as well for in house software projects as they do for commercial software products the next table shows an outline of a staged delivery plan for an in house project stage database stage billing stage networking stage extended reporting stage automation billing database is in place and billing information can be stored and retrieved from the database standard bills and international bills can be printed addressed and so on payments can be received networked data input is complete management summary reports and analysis reports are complete unattended monthly processing is available the use of themes shouldn t be taken as an invitation to abbreviate release planning the word processor and billing system examples are just summaries of the themes for each release the development team will still need to map out exactly which features it plans to deliver during each release if it doesn t you won t know exactly what to expect in each staged delivery and you ll lose some of the project tracking benefit of this approach stageddeliveryplan look alikes some projects follow a percentage approach and plan to deliver software re leases at percent complete percent complete and percent complete without a list detailing the contents of each stage percentages are not sufficient to guide a staged release plan if a percent target is augmented with the same level of detail as a theme release using percentages can work acceptably well although they don t provide the same kind of guidance for deciding what to put into each release that themes do similarly breaking the project into alpha beta and final releases is not actually a staged delivery approach rather it is a plan to deliver feature complete software at alpha release time with such low quality that three phases are needed to make the software run correctly this is a costly alter native to a true staged delivery approach the project team will spend the alpha beta and final release phases correcting defects downstream that could have been corrected more economically upstream releasing the releases the project team doesn t necessarily have to deliver each release to a customer to follow the staged delivery practice in the case of the word processor described earlier in this chapter the project team might not release a version to its customer until stage or or even but it can still use staged delivery as an aid to track progress coordinate drops to quality assurance and reduce the risk of integration problems the project team can get a lot of mileage out of staged releases even if they deliver them only to quality assurance marketing and other stakeholders who want to see that the team is making progress the frequency with which a project team releases software to custom ers will depend on how many customers it has and its relationship to them if it has only a handful of internal customers the release procedure can be informal and the team can release software as often as every week or so if the team has hundreds or thousands of customers the release procedure will need to be much more formal and the team probably shouldn t plan releases more frequently than every two or three months more frequent external releases will burden the project with external release overhead disabling featuresthataren tfullyoperational creating release notes walking through the release procedure coaching users through installation and fielding support calls without significantly lowering the project technical or managementrisks each staged release should hit the quality target set for the final soft ware one benefit of staged releases is that they keep the software quality from sliding for long periods without detection pushing a release out the door without bringing its quality up to the desired level hurts the chances of ever raising the final software quality to the desired level revising the staged delivery plan the staged delivery plan should be submitted to change control reviewed and then baselined just as other critical project work products are but this document should not be treated as frozen the work done during the rest of the project will expose issues that weren t considered during the initial stage planning and some adjustments to the plan will be needed at that point the staged delivery plan should be updated to reflect the revised plan through use of the change control process ongoing planning activities final preparations time is about focused planning so it is important to step back from the day to day activities of the project and take a fresh look at the plans that were developed earlier risk management explicit risk management activities that were initiated at the beginning of the project should continue throughout the project as project tasks become better defined so do the risks the top risks list should have been updated several times by now during the final preparations period look for new risks that might not have been apparent before look for risks associated with cost available computing resources available personnel and question able technical aspects of the project many of the practices recommended in this book have been selected for their ability to control the most common most critical project risks use of the change control plan and the software integration procedure regular refinement of project estimates and the staged delivery approach all reduce project risks although they are not primarily risk management activities protectvision is the project vision still appropriate tor the project in some instances so much is learned about the project during requirements development that the vision doesn t really match the project anymore project team members might all agree that the original vision is no longer valid but they won t all synchronize their work on the same new vision unless a specific new vision is given official standing check the vision statement and if necessary revise it so that it can provide direction throughout stage planning architecture and detailed design and implementation decision making authority check whether the decision making authority identified during preliminary planning is clear about the project plans and goals including the change control plan and preliminary estimates if the decision making authority is not aware of or in agreement with project plans be sure any issues are settled before detailed design and implementation begin personnel the period of final preparations is also an appropriate time to step back from personnel issues and to check that the project team is healthy here are a few guidelines for assessing the well being of the project personnel project morale should be high if it isn t find out why not and fix it the project should not have any problem team members if any team members are causing problems remove them now before they further damage the team morale and while there s still time to bring in new team members without hurting existing team members productivity the way the team is organized should be working if it isn t working change it if some team members have demonstrated weaknesses provide supplementary training checkwhetheritwillbe possible torecruitthe remaining project members that the software development plan calls for if not add that as a risk to the top risks list final preparations check whether the project is registering a strong positive in its people aware management accountability is the project running in such a way that the project s human resources will emerge from the project worth more to the organization rather than less if the project is burning people up before implementation starts the project is headed for trouble updating the software development plan the software development plan created during preliminary planning should be revised and updated to reflect the planning that has been done during this final preparations period the revised plan should be reviewed and approved by the project manager development team quality assurance people and documentation team and then it should be placed under change control according to a recent study android has seen a constantly growing market share in the mobile phone market which is now at with android phones being ubiquitous they become a worthwhile target for attacks on users privacy sensitive data felt et al classified different kinds of android malware and found that one of the main threats posed by malicious android applications are privacy violations which leak sensitive information such as location information contact data pictures sms messages etc to the attacker but even applications that are not malicious and were carefully programmed may suffer from such leaks for instance when they contain advertisement libraries many app developers include such libraries to obtain some remuneration for their efforts but few of them fully understand their privacy implications nor are they able to fully control which data these libraries process common libraries distill private information that identifies a person for targeted advertisement such as unique identifiers e g imei mac address etc country or location information taint analyses address this problem by analyzing applications and presenting potentially malicious data flows to human analysts or to automated malware detection tools which can then decide whether a leak actually constitutes a policy violation these ap proaches track sensitive tainted information through the applica tion by starting at a pre defined source e g an api method return d roid successfully finds leaks in a ing location information and then following the data flow until it subset of apps from google play and about malware apps reaches a given sink e g a method writing the information to a from the virusshare project socket giving precise information about which data may be leaked categories and subject descriptors f semantics of program ming languages program analysis d security and protec tion information flow controls where the analyses can inspect the app both dynamically and stati cally dynamic program analyses though require many test runs to reach appropriate code coverage moreover current malware can recognize dynamic monitors as the analyzed app executes causing the app to pose as a benign program in these situations while static code analyses do not share these problems they run the risk of being imprecise as they need to abstract from program inputs and to approximate runtime objects the precise modeling of the runtime execution is particularly challenging for android apps as those apps are no stand alone applications but are actually permission to make digital or hard copies of all or part of this work for personal or plugins into the android framework apps consist of different classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page copyrights for components of this work owned by others than acm must be honored abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a components with a distinct lifecycle during an app execution the framework calls different callbacks within the app notifying it of system events which can start pause resume shutdown the app etc to be able to effectively predict the app control fee request permissions from permissions acm org flow static analyses must not only model this lifecycle but must pldi june edinburgh united kingdom copyright c acm http dx doi org 2594299 also integrate further callbacks for system event handling e g for phone sensors like gps ui interaction and others as we show in d i p a e v a luated this work recognizing callbacks is anything but trivial and requires dedicated algorithms another challenge is posed by sources of sensitive information such as password fields in the user interface the respective api calls returning their contents cannot be detected based on the program code alone instead their detection requires a model of auxiliary information stored in the manifest and layout xml files last but not least like any application written in java android apps also contain aliasing and virtual dispatch constructs typical static analyses for java handle these problems through some degree of context and object sensitivity the framework nature of android makes this problem harder than usual as we found it to expose extraordinarily deep aliasing relationships past data flow analysis approaches for android handle the above challenges in an unsatisfactory manner us ing coarse grained over as well as under approximations under approximations usually caused by the lack of a faithful lifecycle model can cause these analyses to miss important data flows in practice even worse though the tools over approximations can cause many false warnings easily overwhelming security analysts to the point at which they stop using the analysis tools entirely in this work we therefore present flowdroid a novel static taint analysis system specifically tailored to the android platform and based on novel on demand algorithms that yield high precision while maintaining acceptable performance flowdroid analyzes the apps bytecode and configuration files to find potential privacy leaks either caused by carelessness or created with malicious intention opposed to earlier analyses flowdroid is the first static taint analysis system that is fully context flow field and object sensitive while precisely modeling the complete android lifecycle including the correct handling of callbacks and user defined ui widgets within the apps this design maximizes precision and recall i e aims at minimizing the number of missed leaks and false warnings to obtain deep context and object sensitivity while maintaining acceptable performance flowdroid uses a novel on demand alias analysis the analysis algorithm is inspired by andromeda but improves over andromeda in terms of precision we have open sourced flowdroid in summer the tool has already been picked up by several research groups and we are in contact with a leading producer of anti virus tools who plans to use flowdroid productively in the analysis backend for us and others to be able to measure scientific progress in this important field of research it is required that researchers are able to conduct comparative studies of android taint analysis tools unfortunately up until now there exist no benchmarks that would allow for systematic studies as another contribution of this work we thus make available droidbench a novel open source micro benchmark suite for comparing the effectiveness of taint analyses for android we have made droidbench available online in spring and know of several research groups who have used it already to measure and improve the effectiveness of their android analysis tools a first group of external researchers has already agreed to contribute further micro benchmarks to the suite flowdroid can be used to secure in house developed android apps as well as assist in the triage of android malware both use cases demand not a perfect but yet a reasonably low rate of false positives and false negatives a set of experiments with securibench micro droidbench and some well known apps containing data leaks shows that flowdroid finds a very high fraction of data leaks while keeping the rate of false positives low on droidbench flowdroid achieves recall and precision greatly outperforming the commercial tools appscan source and fortify sca further experiments with real apps confirm flowdroid utility in practice to summarize this work presents the following original contri butions flowdroid the first fully context field object and flow sensitive taint analysis which considers the android application lifecycle and ui widgets and which features a novel particularly precise variant of an on demand alias analysis a full open source implementation of flowdroid droidbench a novel open and comprehensive micro bench mark suite for android flow analyses a set of experiments confirming superior precision and recall of flowdroid compared to the commercial tools appscan source and fortify sca and a set of experiments applying flowdroid to over apps from google play and about malware apps from the virusshare project we make available online our full implementation as an open source project along with all benchmarks and scripts to reproduce our experimental results http sseblog ec spride de tools flowdroid space limitations preclude us from including some details necessary to fully reproduce our approach we thus publish an accompanying technical report which formalizes flowdroid transfer functions and gives additional details on the implementation the paper continues as follows section gives a motivating example and explains the necessary background on android security section explains how flowdroid models the android lifecycle while section gives important details about the actual taint analysis in section the paper discusses implementation details and limitations while section evaluates flowdroid section discusses related work and section concludes background and example we start by giving a motivating example and then explain the attacker model this work assumes the example in listing abstracted from a real world malware app implements an activity which in android represents a screen in the user interface the app reads a password from a text field line whenever the framework restarts the app when the user clicks on a button of the activity the password is sent via sms line this constitutes a tainted data flow from the password field the source to the sms api the sink in this example sendmessage is associated with a button in the app ui which is triggered when the user clicks the button in android listeners are defined either directly in the code or in the layout xml file as is assumed here thus analyzing the source code alone is insufficient the analysis must also process the metadata files to correctly associate all callback methods in this code a leak only occurs if onrestart is called initializing the user variable before sendmessage executes to avoid false negatives a taint analysis must model the app lifecycle correctly recognizing that a user may indeed hit the button after an app has restarted to avoid false positives an analysis of this example must be field sensitive the user object contains two fields for the user name and password but only the latter of which should be considered a private value object sensitivity while not required for this example is essential to distinguish objects originating at different allocation sites but reaching the same code locations in our experiments we found some cases requiring deep object sensitivity to be able to automatically dismiss false positives this is due to the relatively deep call and assignment chains of the android framework operations such as string concatenation line require a model that defines whether and how data flows through those operations treating such operations as normal method calls and analyzing library methods like application code can be imprecise because public class leakageapp extends activity private user user null protected void onrestart edittext usernametext edittext findviewbyid r id username edittext passwordtext edittext findviewbyid r id pwdstring string uname usernametext tostring string pwd passwordtext tostring if uname isempty pwd isempty this user new user uname pwd callback method in xml file public void sendmessage view view if user null return password pwd user getpwd string pwdstring pwd getpassword string obfpwd must track primitives for char c pwdstring tochararray obfpwd c string concat string message user user getname pwd obfpwd smsmanager sms smsmanager getdefault sms sendtextmessage 0905 null message null null listing example android application it ignores the operations semantics and as we found is often forbiddingly expensive in practice attacker model flowdroid can be used to detect data flows in general no matter whether they are caused by carelessness or malicious intent for malicious cases we assume the following attacker model the attacker may supply an app with arbitrary malicious dalvik bytecode typically the attacker goal would be to leak private data through a dangerously broad set of permissions granted by the user flowdroid makes sound assumptions on the installation environment and app inputs meaning that the attacker is free to tamper with those as well flowdroid does assume however that the attacker has no way of circumventing the security measures of the android platform or exploiting side channels further we assume that the attacker does not use implicit flows to disguise data leaks given the current kind of available malware this is a very reasonable assumption precise modelling of lifecycle in the following we explain f low d roid precise modeling of the lifecycle including entry points and asynchronously executing components and callbacks multiple entry points unlike java programs android applications do not have a main method apps instead comprise many entry points i e methods that are implicitly called by the android frame work the android operating system defines a complete lifecycle for all components in an application there are four different kinds of components an app developer can define activities are single focused user actions services perform background tasks content providers define a database like storage and broadcast receivers listen for global events all these components are implemented by deriving a custom class from a predefined operating system class registering it in the androidmanifest xml file and overwriting the lifecycle methods the android framework calls these methods to start or stop the component or to pause or resume it depending on environment needs for instance it can stop an application be cause of memory depletion and later restart it when the user returns to it in result when constructing a call graph android analy ses cannot simply start by inspecting a predefined main method instead all possible transitions in the android lifecycle must be modeled precisely to cope with this problem flowdroid con structs a custom dummy main method emulating the lifecycle in the following we explain how this method is constructed asynchronously executing components an application can con tain multiple components e g three activities and one service al though the activities run sequentially one cannot pre determine their order one activity could for instance be the main one initially visi ble to the user and then launch either one of the others depending on user input services run as parallel background tasks flowdroid models this execution by assuming that all components activities services etc inside an application can run in an arbitrary sequential order including repetition some static analyses are path sensi tive i e consider each possible program path separately in such cases considering all possible orderings would be very expensive flowdroid bases its analysis on ifds an analysis frame work which is not path sensitive and instead joins analysis results immediately at any control flow merge point flowdroid can thus generate and efficiently analyze a dummy main method in which every order of individual component lifecycles and callbacks is possible it does not need to traverse all possible paths callbacks the android operating system allows applications to register callbacks for various types of information e g location updates or ui interactions flowdroid models these callbacks in its dummy main method for instance to recognize cases where an application stores the location data that the framework passes to the callback as a parameter and later sends this data to the internet when the activity is stopped the order in which callbacks are invoked cannot generally be predicted which is why flowdroid assumes that all callbacks can be invoked in any possible order however callbacks can only happen while the parent component e g activity is running for precision flowdroid thus associates components activities services etc with the callbacks they register an activity may for instance register callbacks that get invoked when a button is pressed the respective callback handler would then have to be analyzed between the onresume and onpause events of this activity only there are two different ways to register callback handlers on the android platform firstly callbacks can be defined declaratively in the xml files of an activity alternatively they can also be registered imperatively using well known calls to specific system methods flowdroid supports both ways additionally for malware there is the risk that an attacker registers undocumented callbacks by overwriting methods of the android infrastructure some of which could even be called by native code flowdroid recognizes such overwritten methods handling them similar to normal callback handlers such as button clicks for finding callbacks registered in the application code flow droid first computes one call graph per component starting at the lifecycle methods oncreate onstop etc implemented in the respective component class this call graph is then used to scan for calls to android system methods that use one of the well known callback interfaces as a formal parameter type afterwards the call graph is incrementally extended to include these newly discovered callbacks and the scan is run again since callback handlers are free to register new callbacks on their own potentially requiring flow droid to re extend the call graph and re analyze until a fixed point it reached while this method is more expensive than just scanning for classes implementing the callback interfaces it delivers a more leakageapp la new leakageapp la oncreate la onstart la onresume p p la sendmessage la onpause p la onstop p p la onrestart la ondestroy figure cfg for dummy main method precise mapping between components and callbacks this does not only reduce false positives but we also found it to considerably de crease the runtime of the following taint analysis once the dummy main method has been constructed flowdroid computes a final call graph using this method as the app entry point for callbacks defined in the layout xml files the respective xml file is mapped to one or more application components using the respective layout controls a button click handler for instance is only valid for the activity that hosts the respective button flow droid analyzes each activity to see which identifiers from the xml file it registers this information is then used to create the mapping example note that to gain maximal precision flowdroid generates a new dummy main method for each app analyzed each main method will only involve the part of the lifecycle that according to the app xml configuration files can actually occur at runtime disabled activities are automatically filtered and callback methods are only invoked in the contexts of the components to which they actually belong a button click handler for instance is only analyzed in the context of its respective activity in figure we show the control flow graph of the dummy main method for our previous example the graph models a generic activity lifecycle augmented with the sendmessage callback in this figure p represents an opaque predicate of which we know that flowdroid won t be able to evaluate it statically in result the analysis will automatically consider on equal terms both branches for conditions involving p precise flow sensitive analysis one major difficulty in the analysis is how to implement high object sensitivity to resolve aliasing effectively figure abstracted from a real world case shows how flowdroid combines a forward taint analysis and an on demand backward alias analysis to deduce that b f is tainted at the sink in step the tainted variable w is propagated forward tainting the heap object x f step continues the taint tracking for w and x f the important step is whenever a heap object gets tainted the backward analysis searches upwards for aliases of the respective object x f in this case at the alias b f is found and then propagated forward as a normal taint a g f b f figure taint analysis under realistic aliasing flowdroid models the taint analysis problem within the ifds framework for inter procedural distributive subset prob lems section explains the transfer functions that the analysis uses most functions are relatively standard there is one important situation however in which flowdroid analysis differs from standard taint analysis algorithms namely at statements at which tainted values are assigned to the heap i e to fields or arrays this situation will cause the backward alias analysis to be called details of which we will explain in section due to space restrictions we keep the description of flow functions on an informal level to al low others to reproduce our approach the accompanying technical report contains a complete formalization taint analysis both the forward and backward analysis propagate access paths an access path is of the form x f g where x is a local variable or parameter and f and g are fields access paths can have different lengths up to a user customizable maximal length by default an access path of length is a simple local variable or parameter e g x in flowdroid an access path implicitly describes the set of all objects reachable through this path e g x f includes taints x f g x f h x f g h and so on the transfer function for assignments taints the left hand side if any of the operands on the right hand side is tainted assignments to array elements are treated conservatively by tainting the entire array assigning a new expression to a variable x erases all taints modeled by access paths rooted at x method calls translate access paths to the callee context by replacing actual with formal parameters the inverse translation happens at method returns including the return value if present as usual with ifds based analyses flowdroid also includes a call to return flow function bypassing each method call on the side of the caller this function propagates taints not relevant for the call generates new taints at sources reports taints at sinks and propagates taints for native calls section gives further information on the latter on demand alias analysis whenever a tainted value is assigned to a heap location such as a field or an array flowdroid searches backwards for aliases of the target variable to then taint them as well in listing for now consider the first call to taintit line which taints the formal parameter in in line this will cause the access path x f to get tainted due to the assignment x f in in this situation generally at all assignments to the heap flowdroid will initiate a backward search for aliases of x f finding out f in line at this point a new forward taint propagation is started for out f from this statement which will eventually discover the leak in line the backward analysis will also continue to search backwards though discovering the alias p f in main with which it then spawns a forward analysis leading to a second taint flow report at line maintaining context sensitivity algorithms and show the main loops of both the forward and the backward analysis solver in void main a new a b a g foo a sink b f void foo z x z g x f w source x f w w x f z g f entry in x f x f out f x out x f in in x f out f taint flow path edge handover naive approach figure analysis handover with context injection in taintit algorithm main loop of forward solver while worklist fw injected context do pop p d n d off worklist fw switch n case n is call statement if summary exists for call then apply summary else map actual parameters to formal parameters end if case n is exit statement install summary p d n d map formal parameters to actual parameters map return value back to caller context case n is assignment lhs rhs d replace rhs by lhs in d insert p d n d into worklist bw extend path edges via the propagate method of the classical ifds algorithm end while algorithm main loop of backward solver while worklist bw do pop p d n d off worklist bw switch n case n is call statement if summary exists for call then apply summary else map actual parameters to formal parameters end if extend path edges via the propagate method of the classi cal ifds algorithm case n is method first statement install summary p d n d insert p d n d into worklist fw do not extend path edges via the propagate method of the classical ifds algorithm killing current taint d case n is assignment lhs rhs d replace lhs by rhs in d insert p d n d into worklist fw extend path edges via the propagate method of the classi cal ifds algorithm end while void main data p new new taintit source p sink p f taintit public sink f void taintit string in data out x out x f in sink out f listing example for context injection pseudo code the algorithmic representation assumes the reader to be familiar with the algorithmic description of the original ifds algorithm both solvers operate on their own worklist containing so called path edges that summarize the data flows computed so far up to the current statement node n an edge p d n d effectively states that the analysis concluded that d holds at n if d holds at the start point p of n procedure p in our particular implementation the abstract domain values d i are effectively access paths describing references to tainted values the handover between both analyses is quite non trivial if coordinated in a naive fashion one will easily obtain two independent analyses that each on their own may be context sensitive but would in combination produce analysis information for unrealizable paths along conflicting contexts for instance note that in the example from listing the aliases of x f become tainted only if in was tainted previously in particular the analysis should not report a leak at line whose corresponding taintit call only propagated the string public figure shows both how a naive implementation could cause such a false positive and how flowdroid handles the problem by injecting contexts from one analysis to another the figure assumes some familiarity with the typical notation for flow functions within the ifds framework here the black nodes represent data flow facts before after the respective statement and the black and red edges represent data flows the fact is the tautological fact that is always true which is why one node always connects to the next the left hand side of the figure shows how the forward taint analysis determines x f to be tainted when processing the assignment to x f the forward analysis spawns an instance of the backward alias analysis shown on the right hand side the naive way to spawn this analysis would be to initialize it with an edge from to x f dotted line this implementation although straightforward leads to imprecision as its semantics state that aliases of x f are tainted no matter what in listing this could cause the analysis to incorrectly report a taint violation even for f the correct way is thus to inject into the backwards analysis the context of the forward analysis flowdroid consults the path edge to x f which the ifds algorithm stores as a side effect of its summary computation it then injects that entire edge into the backward solver see algorithm line context injection happens both ways at line in the example when the backward analysis spawns a forward analysis for out f it injects into the forward analysis the original context in see algorithm line semantically for the example this implies that all taints that both analyses discover for taintit are conditional w r t in being tainted initially a second problem is to avoid false positives due to unrealizable paths flowdroid needs to prevent the backwards analysis to return into contexts not analyzed by the forward analysis and vice versa to implement this constraint the backward analysis in flowdroid actually never returns into the caller at all instead data p new p sink f p f source sink f listing example for activation statements whenever finding an alias it triggers the forward analysis on that alias such as for out f in line it is then the task of the forward analysis to map back any relevant taints into the caller context in the example the forward analysis knows the calling context it originated from which is why it can easily make sure to map back the taints into the right context only in the example the forward pass would map out f to p f in line only not to f in line in essence the backwards analysis can descend into callees but never returns back into callers all returns are handled by the forward analysis when the backwards analysis descends into a call it will eventually spawn a forward analysis when reaching the method header see algorithm line the forward analysis can then make sure to only return into the right caller because its context is injected by the backward analysis technically its incoming set is injected whenever the forward analysis maps back to the caller a taint associated with a heap object it spawns a new alias search inside the caller maintaining flow sensitivity andromeda is another taint analysis tool that inspired flowdroid on demand alias analysis andromeda analysis however can lead to flow insensitive results in the example in listing the analysis would report two leaks at lines and even though the first call to sink definitely happens before f becomes tainted in fact the very same problem would hold also for flowdroid analysis as we described it above the backward analysis would discover the tainted alias f at line and trigger a forward pass with that value causing a taint to be reported henceforth anywhere where f is leaked flowdroid addresses this problem by keeping track of what we call activation statements whenever spawning an instance of the backwards alias analysis the respective access path is augmented with the current statement the alias activation statement also the tainted alias is marked as inactive semantically only active taints cause leaks when reaching a sink inactive taints are aliases to memory locations which have not yet been tainted whenever the backward analysis spawns the forward analysis again and when then the forward analysis propagates the aliased taint over its activation statement the taint becomes activated and thus gains its ability to actually cause leaks to be reported in the example the activation statement is at line which thus causes the analysis to only report a leak at the succeeding line avoiding the false alarm at line in general activation statements are representatives of call trees assume for a moment that the heap assignment in listing was contained inside a method call such as it was the case in the assignment at line of listing which occurs within method calls to taintit in that example when the forward analysis processes the return edge back into line of main the analysis globally associates the call to taintit line with the activation statement since whenever this call has completed the activation statement also has been processed and thus the taint will be active in other words activation statements are used for looking up the call trees in which they occur to translate them back into transitive callers to the best of our knowledge flowdroid is the first approach to implement an on demand analysis that fully maintains context and flow sensitivity in the future we plan to investigate to what extent the principles explained here can be reused outside the scope of taint analysis ideally yielding a rather generic extension of ifds source sink and entry point detection parse manifest file generate main method parse dex files build call graph parse layout xmls perform taint analysis figure overview of flowdroid further algorithmic details our implementation of ifds uses the extensions explained by naeem and lhotk with this extension the ifds implementation computes the program super graph on the fly which means that in our case we compute taint information only for those variables access paths that are indeed tainted our implementation uses two instances of the ifds solver each of which with slight adjustments as explained in algorithms and each instance contains a separate table of summary functions that as in the original ifds algorithm is used to avoid re computation for the same callees under the same contexts d i implementation flowdroid extends the soot framework which provides important prerequisites for a precise analysis in particular the three address code intermediate representation jimple and the accurate call graph analysis framework spark a plugin called dex pler allows flowdroid to convert android dalvik bytecode into jimple on top of soot and dexpler flowdroid further uses heros a scalable highly multi threaded implementation of the ifds framework we next explain flowdroid architecture while the subsequent sections explain important implementation details and flowdroid current limitations architecture figure shows flowdroid architecture an droid applications are packaged in apk files android packages which are essentially zip compressed archives after unzipping an archive flowdroid searches the application for lifecycle and callback methods as well as calls to sources and sinks this is done by parsing various android specific files including the layout xml files the dex files containing the executable code and the mani fest file defining the activities services broadcast receivers and content providers in the application next flowdroid generates the dummy main method from the list of lifecycle and callback methods this main method is then used to generate a call graph and an inter procedural control flow graph icfg starting at the detected sources the taint analysis then tracks taints by traversing the icfg as explained in section flowdroid is configured with sources and sinks inferred by our susi project by far the most comprehensive one available the concrete lists of sources and sinks are available from the flowdroid website at the end flowdroid reports all discovered flows from sources to sinks the reports include full path information to obtain this information the implementation links data flow abstraction objects to their predeces sors and to their generating statements this allows flowdroid reporting component to fully reconstruct a graph of all relevant as signment statements that might have caused a taint violation at the given sink defining shortcuts including the full jre or android platform runtime in the analysis not only requires a lot of analysis time and memory but due to approximations performed during the library analysis can also lead to undesired imprecision flowdroid therefore comprises an interface for external library models the tool supports a simple textual file format for defining certain shortcut rules predefined rules handle collection classes string buffers and similar commonly used data structures e g specifying that adding a tainted element to a set taints the entire set technically the shortcuts are implemented using the call to return edge if a library call has no associated rule then it is fully analyzed native calls both java and the android platform support the invocation of native methods written in c or other unmanaged languages for a java based analysis such methods are black boxes which cannot be analyzed flowdroid comes with explicit taint propagation rules for the most common native methods such as system arraycopy in this example the rule defines the third argument the output array to become tainted if the first argument the input array is tainted before the call for native methods without an explicit rule flowdroid assumes a sensible default call arguments and the return value to become tainted if at least one parameter was tainted before this is neither entirely sound nor maximally precise but is likely the best practical approximation in a black box setting inter component communication flowdroid over approxi mates explicit inter component communication by regarding method which send intents as sinks and callbacks which receive intents as sources android also supports implicit intent based communication e g by setting the result value of a called activity which is then automatically passed back to the calling activity by the operating system supporting such behavior together with a more accurate inter component connection model is left to future work in particu lar we are working on integrating flowdroid with epicc a novel static analysis that uses soot and heros to perform a string analysis to resolve inter app communication more precisely limitations although flowdroid is generally aiming for a sound analysis it does share some inherent limitations with most other static analysis tools for instance flowdroid resolves reflective calls only if their arguments are string constants which is not always the case on the java platform reflection analysis tools such as tamiflex can be used to make static analysis tools aware of reflective calls issued at runtime such tool require load time instrumentation through java lang instrument though which the android platform does not currently support unsoundness can also arise in case the android lifecycle contains callbacks we are not aware of or through native methods that our rules model incorrectly at the moment flowdroid is also oblivious to multi threading it assumes threads to execute in an arbitrary but sequential order which is generally unsound as well fully incorporating sound support for multi threading is a big challenge in its own right which we thus leave to future work experimental evaluation our evaluation addresses the following research questions how does flowdroid compare to commercial taint analysis tools for android in terms of precision and recall can flowdroid find all privacy leaks in insecurebank an app specifically designed by others to challenge vulnerability detection tools for android and what is its performance can flowdroid find leaks in real world applications and how fast is it how well does flowdroid perform when being applied to taint analysis problems related to java not android both in terms of precision and recall the next sections address each research question in detail section explains why unfortunately we were unable to directly compare flowdroid to other academic android analysis tools commercial taint analysis tools while there are benchmark suites for analyzing web applications or specifically for detecting different kinds of java vulnerabilities there is no android specific analysis benchmark suite at the moment this is problematic because the generic java test suites do not cover aspects like the android lifecycle callbacks or interactions with ui elements like password fields thus they cannot be used for assessing the practical effectiveness of android analysis tools droidbench specifically for this work we therefore developed an android specific test suite called droidbench in this evaluation we consider version which contains hand crafted android apps the suite can be used to assess both static and dynamic taint analyses but in particular it contains test cases for interesting static analysis problems field sensitivity object sensitivity tradeoffs in access path lengths etc as well as for android specific challenges like correctly modeling an application lifecycle adequately han dling asynchronous callbacks and interacting with the ui our tech nical report gives additional information about the individual apps we have made available online droidbench in spring and know of several research groups who have used it already to measure and improve the effectiveness of their android analysis tools a first group of external researchers has already agreed to contribute further micro benchmarks to the suite table presents the analysis results for flowdroid and two commercial analysis tools explained in the following when applied to droidbench as the results show flowdroid achieves recall and precision as explained before for performance reasons f low d roid handles array indices imprecisely the same limitation applies to causing false positives in the first category handling indices precisely and efficiently is an interesting research question in its own causes a false positive because flowdroid does not currently support strong updates in result it cannot kill taints for certain button combinations allowing strong updates would require a probably quite expensive must alias analysis incorporating such an analysis into flowdroid is out of the scope of this work is not detected because the test case contains no actual sink instead the tainted value is stored in an intent which is then handed back to the activity by the framework such cases are hard to handle without special treatment fails because soot currently assumes all static initializers to execute at the beginning of the program which in this case is not correct determining exactly where such initializers can execute at runtime is an interesting research question we plan to add better support in the future comparison with ibm appscan source we compared flow droid with ibm appscan source version on all tests from droidbench appscan source distinguishes three different categories of findings vulnerabilities exceptions of type and ex ceptions of type like reports by flowdroid vulnerabilities include a complete path from source to sink for a type exception there is a flow from source to sink as well but the semantics of some methods along the propagation path is unknown e g possible sanitization since flowdroid does not support sanitization at the moment we consider both vulnerabilities and type exceptions as findings for type exceptions on the other hand there is no trace these reports are generated when certain code constructs e g writing a variable value into the log file are detected as these find ings are highly imprecise and completely disregard data flow we do not count them as findings as table shows appscan source finds only about of all leaks major problems occur with the we exclude the analysis of implicit flows caused through control flow dependencies as none of the tools including flowdroid was designed to analyze such flows correct warning false warning missed leak multiple circles in one row multiple leaks expected all empty row no leaks expected none reported app name appscan fortify flowdroid arrays and lists callbacks button2 methodoverride1 field and object sensitivity fieldsensitivity4 inter app communication lifecycle activitylifecycle3 general java staticinitialization1 unreachablecode miscellaneous android specific inactiveactivity lognoleak sum precision and recall higher is better lower is better lower is better precision p recall r f measure p r table droidbench test results handling of callbacks and the android component it appears like the advertised support for android is mostly restricted to appscan being configured with some appropriate sources and sinks appscan shows a relatively decent precision of comparison with fortify sca fortify sca by hp is another commercial tool widely used by security analysts similar to ibm appscan source fortify also provides different kinds of findings such as data flows from sensitive sources to public sinks requests for security sensitive permissions calls to security sensitive methods etc in our evaluation we only considered findings about data flows all tests were carried out using version as can be seen in table fortify sca shows problems similar to those of ibm appscan like the handling of the android component lifecycle and callbacks figure shows that fortify detects out of data leaks for the lifecycle tests but closer inspection shows that this only happens by chance in these tests the data source involves a static field which fortify apparently treats in a special way that coincidentally causes a leak to be reported when removing the static modifier which does not change the semantics of the test case fortify does not detect the leak any longer fortify precision measures as conclusion from our experiments we conclude that to not over burden the user with false positives appscan source and fortify sca aim for relatively high precision while sacrificing recall thus risking to miss actual privacy leaks in comparison flowdroid shows a significantly higher recall and even a slightly improved precision performance on insecurebank insecurebank is a vulnerable android app created by paladion inc specifically for the purpose of evaluating analysis tools such as flowdroid it contains various vulnerabilities and data leaks similar to those found in real world applications analyzing the application takes about seconds on a laptop computer with an intel core centrino cpu and gb of physical memory running on windows with oracle java runtime version bit in its default settings flowdroid finds all seven data leaks which we all verified by hand there are no false positives nor false negatives performance on real world applications for assessing flowdroid on real applications we applied it to the most popular android applications from google play fortunately despite flowdroid high recall the analysis did not reveal any leaks hinting at truly malicious behavior nevertheless the majority of apps was reported to probably accidentally leak sensitive information like the imei a unique id or location data into logs and preference files samsung push service for instance logs the phone imei logs are problematic as the os does not impose the same access re strictions on logs as it does on files for devices running android or lower all logs are readable by any app that has the read logs permission the problem is deemed so important that the mecha nism was changed in android since this version logs are only privately visible unless the app is run with debugging enabled addi tionally samsung push service also broadcasts an android intent containing the imei all other applications can simply subscribe to this intent and get the broadcast imei thereby circumventing the android permission system for this data item the game hugo runner stores longitude and latitude into a pref erences file as we verified by hand though those preferences were correctly written in private mode precluding any access by other apps this indicates again how important a precise environment model is to reduce the number of false positives future tools should thus model the respective apis more precisely for most examined apps flowdroid terminated in under a minute the instance that took the longest to complete was samsung push service which took about minutes to analyze we also ran flowdroid on about known malware sam ples from the virusshare project the average runtime was seconds since the malware samples seem to be comparatively small the minimum runtime was seconds the maximum was seconds which only happened for a single comparatively large application most of the apps contained two data leaks leaks per appli cation on average usually with identification information like the for legal reasons we are unable to provide these applications online to be able to reproduce our results though researchers may email the first author to obtain a copy of those applications test case group tp fp aliasing arrays basic collections datastructure factory inter pred n a n a reflection n a n a sanitizer n a n a session strongupdates sum table securibench micro test results imei being sent to a remote server to register the phone or sent as part of an sms message sometimes to a premium rate number some malware applications were even found to receive data through broadcast receivers and to then send out this data in sms messages this can allow other applications to send sms messages indirectly without requiring the respective permission on their own securibench micro flowdroid was specifically designed for android and in this space gains much precision through its complete and precise han dling of android lifecycle nevertheless there is nothing that would preclude software developers from applying flowdroid to java applications as well to assess how well flowdroid is set up for this use case we evaluated flowdroid against stanford securibench micro version a common set of mi cro benchmarks originally intended for web based applications for each of the benchmarks in the suite we manually defined the nec essary lists of sources sinks and entry points since flowdroid supports a simple textual file format for defining these parameters and since all benchmarks cases have the same structure this was not much effort we omitted from our experiments test cases involving sanitization reflection predicates and multi threading as we ex plained earlier such features are out of scope for our analysis tool just as they are for all other existing android analysis tools table shows our test results grouped by test categories the tp column shows the true positives i e the number of actual leaks that flowdroid found for the example of basic for instance f low d roid found out of the fp column shows the number of false positives i e the finding that flowdroid reported that did not correspond to actual leaks but were rather artifacts of an overly approximate analysis in most cases this number is reasonably low or even zero except for the arrays category in which flowdroid reports false positives again those are caused by the failure to model array indices precisely comparison with other tools we also tried to compare flowdroid to a number of other tools from the scientific literature namely trustdroid leak miner and the tool by batyuk et al unfortunately none of those tools are available online nor did the respective authors reply to our inquiries we tried to run droidbench on scandroid but faced technical difficulties the tool did not report any findings at all in our setup though being in contact with the authors we were unable to fix these issues and both sides eventually gave up the authors of androidleaks promised to run their tool on droidbench but never delivered we also contacted the authors of chex but they were unable to provide the tool or any benchmark results due to intellectual property claimed by nec starostin declined to participate in the experiment as his tool ignores aliasing making any comparison meaningless the authors of scandal could not provide their tool due to intellectual property claimed by samsung but used our benchmark suite and feedback to improve their analysis according to the authors on droidbench the analysis now reports results similar to flowdroid in result we were unable to successfully evaluate even a single scientific taint analysis tool for android on our own this is quite unfortunate as it restricts us to comparing to those tools only based on the available publications we hope that the availability of flowdroid and droidbench will greatly improve this situation in the future after all publishing irreproducible results hinders progress in the field and is considered unacceptable in most other sciences related work there are several approaches to static analysis of android applica tions differing in precision runtime scope and focus one of the most sophisticated ones is chex a tool to detect component hijacking vulnerabilities in android applications by tracking taints between externally accessible interfaces and sensitive sources or sinks although not built for the task chex can in principle be used for taint analysis chex does not analyze calls into android framework itself but instead requires a hopefully complete model of the framework in flowdroid such a model is optional and except for native calls is used only to increase precision and performance users can thus omit the model entirely and still be sure not to lose taints chex entry point model requires an enumeration of all possible split orderings which is not necessary in flowdroid furthermore chex is limited to at most object sensitivity while flowdroid demand driven alias analysis allows for contexts of arbitrary lengths using a default of we found object sensitivity to be too imprecise in practice leakminer appears similar to our approach from a technical point of view like flowdroid it is based on soot uses spark for call graph generation it implements the android lifecycle and the paper states that an app can be analyzed in minutes on average however the analysis is not context sensitive which precludes the precise analysis of most test cases in droidbench androidleaks also states the ability to handle the android lifecycle including callback methods it is based on wala context sensitive system dependence graph with a context insensitive overlay for heap tracking but is not as precise as flowdroid because it taints the whole object if tainted data is stored in one of its fields i e is neither field nor object sensitive this precludes the precise analysis of many practical scenarios scandroid is another tool for reasoning about data flows in android applications its main focus is the inter component e g between two activities in the same app and inter app data flow this poses the challenge of connecting intent senders to their respective receivers in other applications scandroid prunes all call edges to android os methods and conservatively assumes the base object the parameters and the return value to inherit taints from arguments this is much less precise than flowdroid treatment flowdroid applies this default rule only for native calls not modeled explicitly flowdroid currently models intent sending as sink and intent reception as source yielding a sound treatment of inter app communication in the future we plan to integrate flowdroid with epicc a novel static analysis that uses string analysis to precisely resolve inter app communication other approaches like copperdroid dynamically observe interactions between the android components and the underlying linux system to reconstruct higher level behavior special stim ulation techniques are used for exercising the application to find malicious activities attackers however can easily modify an app to detect whether it is running inside a virtual machine and then leak no data during that time alternatively data leaks might only occur after a certain runtime threshold aurasium and droidscope largely suffer from the same shortcomings with respect to static leak detection taintdroid is one of the most sophisticated android taint tracking systems to date as a dynamic approach however it yields some quite different tradeoffs compared to flowdroid for in stance taintdroid has no problem tracking taints through reflective method calls as taintdroid is implemented as an extension to the execution environment for which it does not matter whether meth ods are invoked through reflection or not on the other hand if used for triaging malware before installation time then taintdroid can successfully detect malware only if paired with a dynamic testing approach that yields decent code coverage static ahead of time analyses like flowdroid do not share this shortcoming because they cover all execution paths secondly a dynamic approach such as taintdroid can be fooled by a malicious apps that recognize that it is being analyzed in which case the app could simply refrain from performing any malicious activities while this is not problem atic if the dynamic analysis is installed on the end user mobile phone in that case the malware would effectively be tamed it is problematic if the dynamic analysis is only used for ahead of time triaging of malware that could then later on be installed on a sys tem not protected by the dynamic analysis in which case the app could resume its malicious activities static approaches such as flowdroid do not share this particular shortcoming as they never actually execute the app test case generation is among the most labour intensive tasks in software testing it also has a strong impact on the effectiveness and efficiency of software testing for these reasons it has been one of the most active research topics in software testing for several decades resulting in many different approaches and tools this paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases reviewed in self standing sections the techniques presented include a structural testing using symbolic execution b model based testing c combinatorial testing d random testing and its variant of adaptive random testing and e search based testing each section is contributed by world renowned active researchers on the technique and briefly covers the basic ideas underlying the method the current state of the art a discussion of the open research problems and a perspective of the future development of the approach as a whole the paper aims at giving an intro ductory up to date and relatively short overview of research in automatic test case generation while ensuring a comprehensive and authoritative treatment elsevier inc all rights reserved e mail addresses saswat cs stanford edu s anand e k burke stir ac uk e k burke tychen swin edu au t y chen john clark cs york ac uk j clark myra cse unl edu m b cohen wgrieskamp gmail com w grieskamp mark harman ucl ac uk m harman harrold cc gatech edu m j harrold p mcminn sheffield ac uk p mcminn bertolino http www isti cnr it people a bertolino is a research director of the italian national research council cnr in pisa she investigates approaches for model based and security testing for service oriented and component based test methodologies as well as for monitoring of non functional properties of composite services currently she is involved in the european projects choreos nessos and and serves as the area editor for software testing for the elsevier journal of systems and software and as an associate editor of springer empirical software engineering and ieee transactions on software engineering in the next couple of years she will be busy organizing the edition of the international conference of software engineering in florence italy she has co authored over papers in international journals and conferences jenny li is a research scientist at avaya labs formerly part of bell labs she is an experienced industrial researcher with more than papers published in technical journals and conferences and holder of over patents with five pending applications she led a software immunization project at avaya for software reliability and security improvement her specialties are in automatic failure detection with particular emphasis on reliability security performance and testing prior to joining avaya she worked at bellcore formerly telcordia and now applied communication science for years she received her ph d from university of waterloo in hong zhu is a professor of computer science at oxford brookes university where he chairs the applied formal methods research group he is a senior member of ieee computer society a member of british computer society acm and china computer federation his research interests are in the area of cloud computing and software engineering including software testing software development methodology agent technology automated software development tools etc he is a co founder and the chair of the steering committee of the ieee acm international workshops on automation of software test ast he has published more than research papers and books see front matter elsevier inc all rights reserved http dx doi org j jss the journal of systems and software contents lists available at sciverse sciencedirect the journal of systems and software j our na l ho me p age www elsevier com locate jss s anand et al the journal of systems and software introduction software testing is indispensable for all software development it is an integral part of the software engineering discipline how ever testing is labour intensive and expensive it often accounts for more than of total development costs thus there are clear benefits in reducing the cost and improving the effectiveness of software testing by automating the process in fact there has been a rapid growth of practices in using automated software testing tools currently a large number of software test automation tools have been developed and have become available on the market among a range of testing activities test case generation is one of the most intellectually demanding tasks and it is also of the most critical challenges since it can have a strong impact on the effectiveness and efficiency of the whole testing process zhu et al bertolino pezz and young it is no surprise that a great amount of research effort in the past few decades has been spent on automatic test case generation as a result a significant number of different techniques for test case genera tion have been advanced and rigorously investigated in addition software systems have become more and more complicated for example with components developed by different vendors using different techniques in different programming languages and even running on different platforms although automation techniques for test case generation have started to be gradually adopted by the it industry in software testing practice there still exists a big gap between real software application systems and the practical usability of the test case generation techniques proposed by the research community we believe that for researchers in software test automation it is highly desirable to critically review the exist ing techniques to recognize the open problems and to put forward a perspective on the future of test case generation with this aim in mind this paper offers a critical review of a number of prominent test case generation techniques and it does so by taking a novel approach that we call an orchestrated survey this consists of a collaborative work collecting self standing sec tions each focusing on a key surveyed topic in our case a test generation technique each section is independently authored by a world renowned active researcher or researchers on the topic the surveyed topics have been selected and orchestrated by the editors generally speaking test cases as an important software arti fact must be generated from some information that is some other types of software artifacts the types of artifacts that have been used as the reference input to the generation of test cases include the program structure and or source code the software specifica tions and or design models information about the input output data space and information dynamically obtained from program execution thus the techniques we consider in this paper include symbolic execution and program structural coverage testing model based test case generation combinatorial testing adaptive random testing as a variant of random testing search based testing of course automatic test case generation techniques may exploit more than one type of software artifact as the input thus combining the above techniques to achieve better effectiveness and efficiency it is worth noting that there are many other automatic or semi automatic test case generation techniques not covered in this paper for example mutation testing fuzzing and data muta see for example the proceedings of ieee acm workshops on automation of software test ast ast url for ast http tech brookes ac uk tion testing specification based testing and metamorphic testing in order to keep this paper to a reasonable size we limited our selection to five of the most prominent approaches future endeav ors could be devoted to complement this set with further reviews orchestrated surveys can in fact be easily augmented with more sections hence after a brief report in the next section about the process that we followed in conducting the survey the paper is organized as follows in section saswat anand and mary jean harrold review typical program based test case generation techniques using sym bolic execution in section wolfgang grieskamp focuses on model based test case generation which is closely related to the currently active research area of model driven software develop ment methodology sections and focus on data centric test case generation techniques i e combinatorial testing reviewed by myra b cohen and random testing and its variant of adaptive random testing are reviewed by tsong yueh chen respectively finally in section mark harman phil mcminn john clark and edmund burke review search based approaches to test case generation about the process of orchestrated survey the idea behind this orchestrated survey as we call it origi nated from the desire to produce a comprehensive survey paper on automatic test generation within the short timeframe of this special section devoted to the automation of software test ast there are already several outstanding textbooks and survey papers on soft ware testing however the field of software testing is today so vast and specialized that no single author could harness the expertise required for all different approaches and could be informed of the latest advances in every technique so typically surveys necessar ily focus on some specific kind of approach we wanted a review that could somehow stand out from the existing literature by offer ing a broad and up to date coverage of techniques but without renouncing the depth and authoritaty that are required for each addressed technique thus we came up with this idea of selecting a set of techniques and invited renowned experts of each technique to contribute to an independent section for each of the included sections the review consists of a brief description of the basic ideas underlying the technique a survey of the current state of the art in the research and practical use of the technique a discussion of the remaining problems for fur ther research and a perspective of the future development of the approach whilst these reviews are assembled together to form a coherent paper each section remains an independently readable and referable article for those authors who accepted our invitation the submit ted sections have not been automatically accepted each section underwent a separate peer review process by at least two often three reviewers following the normal standards of this journal reviewing process some of them were subject to extensive revision and a second review round before being accepted the five finally accepted sections were then edited in their format and collated by us into this survey paper which we proudly offer as an authorita tive source both to get a quick introduction to research in automatic test case generation and as a starting point for researchers willing to pursue some further direction test data generation by symbolic execution by saswat anand and mary jean acknowledgements this research was supported in part by nsf ccf ccf and ccf and ibm software quality innovation award to georgia tech s anand et al the journal of systems and software fig a code that swaps two integers b the corresponding symbolic execution tree and c test data and path constraints corresponding to different program paths symbolic execution is a program analysis technique that analyzes a program code to automatically generate test data for the program a large body of work exists that demonstrates the technique use fulness in a wide range of software engineering problems including test data generation however the technique suffers from at least three fundamental problems that limit its effectiveness on real world software this section provides a brief introduction to sym bolic execution a description of the three fundamental problems and a summary of existing well known techniques that address those problems introduction to symbolic execution in contrast to black box test data generation approaches which generate test data for a program without considering the pro gram itself white box approaches analyze a program source or binary code to generate test data one such white box approach which has received much attention from researchers in recent years uses a program analysis technique called symbolic execu tion symbolic execution king uses symbolic values instead of concrete values as program inputs and represents the values of program variables as symbolic expressions of those inputs at any point during symbolic execution the state of a symbolically executed program includes the symbolic values of program vari ables at that point a path constraint on the symbolic values to reach that point and a program counter the path constraint pc is a boolean formula over the symbolic inputs which is an accumula tion of the constraints that the inputs must satisfy for an execution to follow that path at each branch point during symbolic execu tion the pc is updated with constraints on the inputs such that if the pc becomes unsatisfiable the corresponding program path is infeasible and symbolic execution does not continue further along that path and if the pc is satisfiable any solution of the pc is a program input that executes the corresponding path the program counter identifies the next statement to be executed to illustrate consider the code in fig a that swaps the values of integer variables x and y when the initial value of x is greater than the initial value of y we reference statements in the figure by their line numbers fig b shows the symbolic execution tree for the code fragment a symbolic execution tree is a compact this example which we use to illustrate symbolic execution is taken from reference khurshid et al representation of the execution paths followed during the sym bolic execution of a program in the tree nodes represent program states and edges represent transitions between states the num bers shown at the upper right corners of nodes represent values of program counters before execution of statement the pc is initial ized to true because statement is executed for any program input and x and y are given symbolic values x and y respectively the pc is updated appropriately after execution of if statements and the table in fig c shows the pc and their solutions if they exist that correspond to three program paths through the code fragment for example the pc of path is x y y x thus a program input that causes the program to take that path is obtained by solving the pc one such program input is x y for another example the pc of path is an unsatisfiable constraint x y y x which means that there is no program input for which the program will take that infeasible path although the symbolic execution technique was first proposed in the mid seventies the technique has received much attention recently from researchers for two reasons first the application of symbolic execution on large real world programs requires solv ing complex and large constraints during the last decade many powerful constraint solvers e g de moura and bjorner yices dutertre and de moura stp ganesh and dill have been developed use of those constraint solvers has enabled the application of symbolic execution to a larger and a wider range of programs second symbolic execution is computationally more expensive than other program analyses the limited computational capability of older generation computers made it impossible to symbolically execute large programs however today commodity computers are arguably more powerful than the supercomputers e g cray of the eighties thus today the barrier to applying sym bolic execution to large real world programs is significantly lower than it was a decade ago however the effectiveness of symbolic execution on real world programs is still limited because the tech nique suffers from three fundamental problems path explosion path divergence and complex constraints as described in section those three problems need to be addressed before the tech nique can be useful in real world software development and testing although symbolic execution has been used to generate test data for many different goals the most well known use of this approach is to generate test data to improve code coverage and expose soft ware bugs e g cadar et al godefroid et al khurshid et al other uses of this approach include privacy preserving error reporting e g castro et al automatic generation of security exploits e g brumley et al load testing e g zhang et al fault localization e g qi et al regression test ing e g santelices et al robustness testing e g majumdar and saha data anonymization for testing of database based applications e g grechanik et al and testing of graphical user interfaces e g ganov et al for example castro et al use symbolic execution to gen erate test data that can reproduce at the developer site a software failure that occurs at a user site without compromising the pri vacy of the user zhang et al generate test data that leads to a significant increase in program response time and resource usage qi et al generate test data that are similar to a given program input that causes the software to fail but that do not cause failure such newly generated test data are then used to localize the cause of the failure santelices et al generate test data that exposes difference in program behaviors between two versions of an evolving software majumdar and saha use symbolic execution to generate test data whose slight perturbation causes a significant difference in a program output a number of tools for symbolic execution are publicly available for java available tools include symbolic pathfinder pasareanu and rungta jcute sen and agha jfuzz jayaraman s anand et al the journal of systems and software et al and lct khknen et al cute sen et al klee cadar et al chipounov et al and target c language finally pex tillmann and de halleux is a symbolic execution tool for net languages some tools that are not currently publicly available but have been shown to be effective on real world programs include sage godefroid et al and cinger anand and harrold fundamental open problems a symbolic execution system can be effectively applied to large real world programs if it has at least the two features efficiency and automation first in most applications of symbolic exe cution based test data generation e g to improve code coverage or expose bugs ideally the goal is to discover all feasible program paths for example to expose bugs effectively it is necessary to dis cover a large subset of all feasible paths and show that either each of those paths is bug free or many of them expose bugs thus the sys tem must be able discover as many distinct feasible program paths as possible in the available time limit second the required manual effort for applying symbolic execution to any program should be acceptable to the user to build a symbolic execution system that is both efficient and automatic three fundamental problems of the technique must be addressed other arise in specific applications of sym bolic execution but they are not fundamental to the technique although a rich body of prior on symbolic execution exists these three problems have been only partially addressed path explosion it is difficult to symbolically execute a significantly large sub set of all program paths because most real world software have an extremely large number of paths and symbolic execution of each program path can incur high computational overhead thus in reasonable time only a small subset of all paths can be symbol ically executed the goal of discovering a large number of feasible program paths is further jeopardized because the typical ratio of the number of infeasible paths to the number of feasible paths is high ngo and tan this problem needs to be addressed for efficiency of a symbolic execution system path divergence real world programs frequently use multiple programming lan guages or parts of them may be available only in binary form computing precise constraints for those programs either requires an overwhelming amount of effort in implementing and engi neering a large and complex infrastructure or models for the problematic parts provided by the user the inability to compute precise path constraints leads to path divergence the path that the program takes for the generated test data diverges from the path for which test data is generated because of the path divergence problem a symbolic execution system either may fail to discover a significant number of feasible program paths or if the user is required to provide models will be less automated burnim and sen crest auomatic test generation tool for c url http code google com p crest for example when symbolic execution is applied to open programs i e parts of the program are missing a problem arises in maintaining symbolic values cor responding to reference type variables that store abstract data types a partial bibliography of papers published in the last decade on symbolic exe cution and its applications can be found at http sites google com site symexbib cadar et al and pasareanu and visser provide an overview of prior research on symbolic execution complex constraints it may not always be possible to solve path constraints because solving the general class of constraints is undecidable thus it is possible that the computed path constraints become too complex e g constraints involving nonlinear operations such as multiplica tion and division and mathematical functions such as sin and log and thus cannot be solved using available constraint solvers the inability to solve path constraints reduces the number of distinct feasible paths a symbolic execution system can discover existing solutions although the three problems described in the previous section have not been fully solved many techniques have been proposed to partially address them in the following we briefly describe some of those techniques techniques for path explosion problem many techniques have been proposed to alleviate the path explosion problem and they can be classified into five broad classes techniques in the first class avoid exploring paths through certain parts of a program by using a specification of how those parts affect symbolic execution some techniques anand et al godefroid automatically compute the specification referred to as the summary in terms of pre and post conditions by symbolic execution through all paths of a function then instead of repeatedly analyzing a function for each of its call sites the sum mary is used other techniques bjorner et al khurshid and suen veanes et al use specifications that are manually created in some of those techniques bjorner et al veanes et al specifications of commonly used abstract data types such as strings and regular expressions are encoded internally in the constraint solver one of the main limitations of techniques in this class is that their generated path constraints can be too large and complex to solve techniques boonstoppel et al majumdar and xu ma et al santelices and harrold in the second class are goal driven they avoid exploring a large number of paths that are not relevant to the goal of generating test data to cover a spe cific program entity e g program statement ma et al explore only those paths that lead to the goal other techniques boonstoppel et al majumdar and xu ma et al santelices and harrold use a program data or con trol dependencies to choose and symbolically execute only a subset of relevant paths however these techniques do not address the path explosion problem fully because their effectiveness depends on the structure of data or control dependencies which can differ significantly between programs techniques in the third class are specialized with respect to a program construct or characteristics of a class of programs some techniques godefroid and luchaup saxena et al in the first category focus on analyzing loops in a program in a smarter way because loops cause dramatic growth in the number of paths other techniques godefroid et al majumdar and xu in this class aim to efficiently generate test data for programs that take highly structured inputs e g parser by leveraging the pro gram input grammar techniques in this class are limited because they are specialized techniques in the fourth class use specific heuristics to choose a subset of all paths to symbolically execute but while still satisfy ing the purpose e g obtain high code coverage of using symbolic execution anand et al proposed to store and match abstract program states during symbolic execution to explore a subset of paths that are distinguishable under a given state abstraction func tion the technique presented by tomb et al which uses symbolic execution to expose bugs does not symbolically execute s anand et al the journal of systems and software paths that span through many methods finally other techniques chipounov et al godefroid et al majumdar and sen pasareanu et al in this class use specific path explo ration strategies that enable generation of test data that cover deep internal parts of a program which are difficult to cover otherwise techniques in this class can fail to discover program behavior that a systematic but inefficient technique can discover because of their use of heuristics the fifth class consists of a technique presented by anand et al unlike all aforementioned techniques that aim to reduce the number of program paths to symbolically execute this technique aims to reduce the overhead incurred in symbolic execution of each path the technique uses static analysis to identify only those parts of a program that affect computation of path constraints based on that information the technique instruments the program such that the overhead of symbolic execution is not incurred for the other parts of the program techniques for path divergence problem it is not possible to devise a technique that can entirely elimi nate the manual effort required to implement a symbolic execution system that can compute precise path constraints for large real world programs some manual effort is indispensable however two existing techniques aim to reduce the manual effort godefroid and taly proposed a technique that automatically syn thesizes functions corresponding to each instruction that is to be symbolically executed such that those functions update the program symbolic state as per the semantics of corresponding instructions anand and harrold proposed a technique that can reduce the manual effort that is required to model parts of a program that cannot be symbolically executed e g native methods in java instead of asking the user to provide models for all prob lematic parts of a program their technique automatically identifies only those parts that in fact introduce imprecision during symbolic execution and then asks the user to specify models for only those parts techniques for complex constraints problem techniques that address the problem of solving complex con straints can be classified into two classes the first class consists of two techniques the first technique referred to as dynamic sym bolic execution godefroid et al or concolic execution sen et al using this technique the program is executed normally along the path for which the path constraint is to be computed with some program inputs that cause the program to take that path that path is also symbolically executed and if the path con straint becomes too complex to solve it is simplified by replacing symbolic values with concrete values from normal execution in the second technique pasareanu et al also proposed to use concrete values to simplify complex constraints however unlike dynamic symbolic execution they do not use concrete val ues obtained from normal execution but instead they identify a solvable part of the complex constraint that can be solved and use concrete solutions of the solvable part to simplify the complex constraint techniques borges et al souza and borges lakhotia et al in the second class model the problem of find ing solutions of a constraint over n variables as a search problem in a n dimensional space the goal of the search to find a point in the n dimensional space such that the coordinates of the point represent one solution of the constraint these techniques use meta heuristic search methods glover and kochenberger to solve such search problems the advantage of using meta heuristic methods is that those methods can naturally handle constraints over floating point variables and constraints involving arbitrary mathematical functions such as sin and log the limitations of such techniques arise because of the incompleteness of the meta heuristic search methods that may fail to find solutions to a constraint even if it is satisfiable conclusion on symbolic execution symbolic execution differs from other techniques for auto matic test generation in its use of program analysis and constraint solvers however it can be used in combination with those other techniques e g search based testing an extensive body of prior research has demonstrated the benefits of symbolic execu tion in automatic test data generation more research is needed to improve the technique usefulness on real world programs the fundamental problems that the technique suffers from are a long standing open problem thus future research in addition to devising more effective general solutions for these problems should also leverage domain specific e g testing of smart phone software or problem specific e g test data generation for fault localization knowledge to alleviate these problems test data generation in model based testing by wolfgang grieskamp model based testing mbt is a light weight formal method which uses models of software systems for the derivation of test suites in contrast to traditional formal methods which aim at verifying pro grams against formal models mbt aims at gathering insights in the correctness of a program using often incomplete test approaches the technology gained relevance in practice since around the begin ning of at the point of this writing in significant industry applications exist and commercial grade tools are avail able as well as many articles are submitted to conferences and workshops the area is diverse and difficult to navigate this section attempts to give a survey of the foundations tools and applications of mbt its goal is to provide the reader with inspiration to read further focus is put on behavioral sometimes also called functional black box testing which tests a program w r t its observable input output behavior while there are many other approaches which can be called mbt stochastic struc tural architectural white box etc including them is out of scope for this survey historical context is tried to be preserved even if newer work exists we try to cite the older one first introduction to model based testing one can identify three main schools in mbt axiomatic approaches finite state machine fsm approaches and labeled transition system lts approaches before digging deeper into those some general independent notions are introduced in behav ioral mbt the system under test sut is given as a black box which accepts inputs and produces outputs the sut has an internal state which changes as it processes inputs and produces output the model describes possible input output sequences on a chosen level of abstraction and is linked to the implementation by a confor mance relation a test selection algorithm derives test cases from the model by choosing a finite subset from the potentially infinite set of sequences specified by the model using a testing criterion based on a test hypothesis justifying the adequateness of the selection test selection may happen by generating test suites in a suitable lan guage ahead of test execution time called offline test selection or maybe intervened with test execution called online test selection axiomatic approaches axiomatic foundations of mbt are based on some form of logic calculus gaudel summarizes some of the earliest work going back s anand et al the journal of systems and software to the ties in her seminal paper from gaudel gaudel paper also gives a framework for mbt based on alge braic specification see e g ehrig and mahr resulting from a decade of work in the area dating back as early as boug et al in summary given a conditional equation like p x f g x a h x where f g and h are functions of the sut a is a con stant p a specified predicate and x a variable the objective is to find assignments to x such that the given equality is sufficiently tested gaudel et al developed various notions of test hypotheses notably regularity and uniformity under the regularity hypothesis all possible values for x up to certain complexity n are consid ered to provide sufficient test coverage under the uniformity hypothesis a single value per class of input is considered suffi cient in boug et al the authors use logic programming techniques see e g sterling and shapiro to find those val ues in the essence p is broken down into its disjunctive normal form dnf where each member represents an atom indicat ing a value for x the algebraic approach can only test a single function or sequence and in its pure form it is not of practical rel evance today however it was groundbreaking at its time and the dnf method of test selection is mixed into various more recent approaches dick and faivre a test selection method for vdm plat and larsen models based on pre and post conditions the basic idea of using the dnf for deriving inputs is extended for gen erating sequences as follows a state machine is constructed where each state represents one of the conjunctions of the dnf of pre and post conditions of the functions of the model a transition is drawn between two states s test selection from fsms has been extensively researched one of the subjects of this work is discovering assumptions on the model or sut which would make the testing exhaustive even though equivalence between two given fsms is decidable the sut con sidered itself as an fsm is an unknown black box only exposed by its i o behavior one can easily see that whatever fsm the tester supposes the sut to be in the next step it can behave dif ferent however completeness can be achieved if the number of states of the sut fsm has a known maximum as chow see also vasilevskii lee and yannakakis a lot of work in the and is about optimizing the number of tests regards length overlap and other goals resulting for example in the transition tour method naito and tsunoyama or the unique input output method aho et al many practical fsm based mbt tools do not aim at complete ness they use structural coverage criteria like transition coverage state coverage path coverage etc as a test selection strategy offutt and abdurazik friedman et al a good overview of the different coverage criteria for fsms is found among others in legeard and utting seminal text book practical model based testing utting and legeard various refinements have been proposed for the fsm approach and problems have been studied based on it huo and petrenko investigated the implications of the sut using queues for buffering inputs and outputs huo and petrenko this work is impor tant for practical applications as the assumption of input and output enabledness which assumes that the sut machine can accept every input at any time resp the model machine test case every output and s labeled with a function call is not realistic in real world test setups hierons and ural and f x if there exists an x such that s pre f x and post f x s hierons have investigated distributed testing hierons on this state machine test selection techniques can be applied as showed that when testing from an fsm it is undecidable whether described in section a theorem prover is required to prove the there is a strategy for each local tester that is guaranteed to force above implications which is of course undecidable for non trivial the sut into a particular model state domains of x but heuristics can be used as well to achieve practical fsms are not expressive enough to model real software sys results tems therefore most practical approaches use extended finite state the approach of dick and faivre was applied and extended in machines efsm those augment the control state of an fsm with helke et al also legeard et al is related to it as data state variables and data parameters for inputs and outputs well as kuliamin et al today the work of brucker and efsms are described usually by state transition rules consisting of wolff is closest to it these authors use the higher order a guard a predicate over state variables and parameters and an logic theorem prover system isabelle paulson in which they update on the state variables which is performed when the rule encoded formalisms for modeling stateless and state based sys is taken which happens if the guard evaluates to true in a given tems and implemented numerous test derivation strategies based state in practice many people just say state machine when in on according test hypotheses in this approach the test hypotheses fact they mean an efsm a typical instance of an efsm is a state are an explicit part of the proof obligations leading to a framework chart in a proper efsm the domains from which data is drawn are in which interactive theorem proving and testing are seamlessly finite and therefore the efsm can be unfolded into an fsm making combined the foundations of fsms available for efsms however this has its models which use pre and post conditions can be also instru practical limitations as the size of the expanded fsm can be easily mented for mbt using random input generation filtered via astronomic a different approach than unfolding is using symbolic the pre condition instead of using some form of theorem prov computation on the data domains applying constraint resolution ing constraint resolution tools like quickcheck claessen and or theorem proving techniques hughes and others are used successfully in practice applying this approach lts approaches labeled transition systems lts are a common formalism for fsm approaches describing the operational semantics of process algebra they have the finite state machine fsm approach to mbt was ini also been used for the foundations of mbt an early annotated tially driven by problems arising in functional testing of hardware bibliography is found in brinksma and tretmans circuits the theory has later been adapted to the context of com tretmans and consolidated the theory in tretmans munication protocols where fsms have been used for a long time ioco stands for input output conformance and defines a to reason about behavior a survey of the area has been given by relation which describes conformance of a sut w r t a model tret lee and yannakakis mans starts from traditional lts systems which consist of a set of in the fsm approach the model is formalized by a mealy machine states a set of labels a transition relation and an initial state he where inputs and outputs are paired on each transition test selec partitions the labels into inputs outputs and a symbol for quies tion derives sequences from that machine using some coverage cence a special output the approach assumes the sut to be an criteria most fsm approaches only deal with deterministic fsms input enabled lts which accepts every input in every state if the which is considered a restriction if one has to deal with reactive or sut is not naturally input enabled it is usually made so by wrapping under specified systems it in a test adapter quiescence on the sut represents the situation s anand et al the journal of systems and software that the system is waiting for input not producing any output by its own which is in practice often implemented observing timeouts as well known an lts spawns traces sequence of labels because of non determinism in the lts the same trace may lead to different states in the lts the ioco relation essentially states that a sut conforms to the model if for every suspension trace of the model the union of the outputs of all reached states is a superset of the union of the outputs in the according trace of the sut hereby a suspension trace is a trace which may in addition to regular labels contain the quiescence label thus the model has foreseen all the outputs the sut can produce ioco is capable of describing internal non determinism in model and sut which distinguishes it from most other approaches there are numerous extensions of ioco among those real time extensions nielsen and skou larsen et al and extensions for symbolic lts frantzen et al jeannet et al using parameterized labels and guards the effect of distributed systems has been investigated for ioco in hierons et al an alternative approach to the ioco conformance relation is alternating simulation alur et al in the framework of inter face automata ia de alfaro and henzinger while originally not developed for mbt ia have been applied to this problem by microsoft research for the spec explorer mbt tool since campbell et al long version in veanes et al here to be conforming in a given state the sut must accept every input from the model and the model must accept every out put from the sut this makes testing conformance a two player game where inputs are the moves of the tests generated from the model with the objective to discover faults and outputs are the moves of the sut with the objective to hide faults in con trast to ioco ia does not require input completeness of the sut and treats model and sut symmetrically however it can only deal with external non determinism which can be resolved in the step it occurs though extensions waiving this restriction are found in aarts and vaandrager the ia approach to conformance has been refined for symbolic transition systems by grieskamp et al which describes the underlying foundations of microsoft spec explorer tool veanes and bjrner have provided a compar ison between ioco and ia in veanes and bjorner which essentially shows equivalence when symbolic ia are used both ioco and ia do not prescribe test selection strategies but only a conformance relation test selection has been implemented on top of those frameworks for ioco test selection based on cov erage criteria has been investigated in groz et al based on metrics in feijs et al and based on test purposes in jard and jron for ia test selection based on state partition ing has been described in grieskamp et al based on graph traversal and coverage in nachmanson et al and based on model slicing by model composition in grieskamp et al grieskamp and kicillof veanes et al test selection in combination with combinatorial parameter selection is described in grieskamp et al most of these techniques can be equally applied to online testing i e during the actual test execution or to offline testing ahead of test execution algorithms for offline test generation are generally more sophisticated as they cannot rely on feedback from the actual sut execution and typically use state space exploration engines some of them off the shelf model checkers e g ernits et al others specialized engines e g jard and jron grieskamp et al in contrast to fsm approaches even for a finite transition system test selection may not be able to achieve state coverage if non determinism is present that is because some of the transitions are controlled by the sut which may behave demonic regard ing its choices always doing what the strategy does not expect a winning strategy for a finite lts exists if whatever choice the sut does every state can be reached however in practice models with non determinism which guarantee a winning strategy are rare if the sut can be expected to be fair regarding its non determinism this does not cause a problem as the same test only needs to be repeated often enough modeling notations a variety of notations are in use for describing models for mbt notations can be generally partitioned into scenario oriented state oriented and process oriented whether they are textual or graph ical as in uml is a cross cut concern to this a recent standard produced by etsi to which various tool providers contributed collected a number of tool independent general requirements on notations for mbt following this taxonomy etsi scenario oriented notations scenario oriented notations also called interaction oriented notations directly describe input output sequences between the sut and its environment as they are visible from the viewpoint of an outside observer gods view they are most commonly based on some variation of message sequence chart dan and hierons activity chart flow chart hartmann et al wieczorek and stefanescu or use case diagram kaplan et al though textual variations have also been proposed grieskamp et al katara and kervinen test selection from scenario based notations is generally sim pler than from the other notational styles because by nature the scenario is already close to a test case however scenarios may still need processing for test selection as input parameters need to be concretized and choices and loops need expansion most existing tools use special approaches and not any of the axiomatic fsm or lts based ones however in grieskamp et al it is shown how scenarios can be indeed broken down to an lts based frame work in which they behave similar as other input notations and are amenable for model composition state oriented notations state oriented notations describe the sut by its reaction on an input or output in a given state as a result the models state is evolved and in case of mealy machine approaches an output maybe produced state oriented notations can be given in diagram matic form typically statecharts offutt and abdurazik bouquet et al huima or in textual form guarded update rules in a programming language or pre post conditions on inputs outputs and model state dick and faivre kuliamin et al grieskamp et al they can be mapped to axiomatic fsm or lts based approaches and can describe deter ministic or non deterministic suts process oriented notations process oriented notations describe the sut in a procedural style where inputs and outputs are received and sent as mes sages on communication channels process algebraic languages like lotos are in use tretmans and brinksma as well as programming languages which embed communication chan nel primitives huima process oriented notations naturally map to the lts approach tools there are many mbt tools around some of them result of research experiments some of them used internally by an enter prise and not available to the public and others which are commercially available hartman gave an early overview in hartman and legeard and utting included one in their book from utting and legeard since then the landscape has s anand et al the journal of systems and software changed and new tools are on the market whereas others are not longer actively developed it would be impossible to capture the entire market given more than a short lived temporary snapshot here three commercial grade tools are sketched which are each on the market for nearly ten years and are actively developed the reader is encouraged to do own research on tools to evaluate which fit for a given application the selection given here is not fully rep resentative for an alternative to commercial tools one might also check out binder recent overview binder of open source or open binary tools conformiq designer the conformiq huima formerly called qtronic has been around since developed originally for the purpose of protocol testing the tool can be used to model a variety of systems it is based on uml statecharts as a modeling notation with java as the action language models can also be written entirely in java the tool supports composition of models from multiple components and timeouts for dealing with real time it does not support non determinism conformiq designer has its own internal foundational approach which is probably closest to lts a symbolic exploration algo rithm is at the heart of the test selection procedure the tool can be fed with a desired coverage goal in terms of requirements diagram structure or others and will continue exploration until this goal is reached requirements are annotated in the model and represent execution points or transitions which have been reached conformiq designer can generate test suites in various formats including common programming languages ttcn and manual test instructions the generated test cases can be previewed as message sequence charts by the tool conformiq designer has so far been used in industrial projects in telecommunication enterprise it automotive industrial automa tion banking defense and medical application domains smartesting certifyit the smartesting certifyit legeard and utting for merly called smartesting test designer is around since coming out of legeard utting and others work around test ing from b specifications abrial the current instance of the tool is based on uml statecharts but also supports bpmn scenario oriented models and pre post condition style models using uml constraint language ocl smartesting certifyit uses a combination of constraint solving proof and symbolic execution technologies for test generation test selection can be based on numerous criteria including require ments coverage and structural coverage like transition coverage the tools also supports test selection based on scenarios in bpmn similar as spec explorer does the tool generates test suites for offline testing in numerous industry standard formats and sup ports traceability back to the model non determinism is not supported certifyit is dedicated to it applications secure electronic trans actions and packaged applications such as sap or oracle e business suite spec explorer microsoft spec explorer is around since the current major version called spec explorer is the third incarnation of this tool family developed in and described in grieskamp http www conformiq com http www smartesting com http www specexplorer net grieskamp et al it should not be confused with the older version which is described in veanes et al spec explorer was developed at microsoft research which makes it in contrast to the other commercial tools highly documented via research papers and moved in into a production environment mainly for its application in microsoft protocol documentation program the tool is integrated into visual studio and shipped as a free extension for vs the tool is intentionally language agnostic but based on the net framework however the main notations used for modeling are a combination of guarded update rules written in c and scenarios written in a language called cord grieskamp and kicillof the tool supports annotation of requirements and via cord ways for composing models composing a state based model written in c with a scenario expressing a test purpose defines a slice of the potentially infinite state model and is one of the ways how engineers can influence test selection the underlying approach of spec explorer are interface automata ia thus it is an lts approach supporting external non determinism spec explorer uses a symbolic exploration engine grieskamp et al which postpones expansion of param eters until the end of rule execution allowing to select parameters dependent on path conditions the tool supports online and offline testing with offline testing generating c unit tests offline test selection is split into two phases first the model is mapped into a finite ia then traversal techniques are run on that ia to achieve a form of transition coverage spec explorer has been applied amongst various internal microsoft projects in arguably the largest industry application for mbt up to now a person year project to test the microsoft protocol documentation against the protocol implementations grieskamp et al in course of this project the efficiency of mbt could be systematically compared to traditional test automa tion measuring an improvement of around in terms of the effort of testing a requirement end to end i e from the initial test planning to test execution details are found in grieskamp et al conclusion on model based testing at the etsi mbt user conference in berlin in october over participants from different companies came together discussing application experience and tool support many of the academic conferences where general test automation work is pub lished like icst ictss formerly testcom fates ase issta issre ast etc regularly see a significant share of papers around mbt two dagstuhl seminars have been conducted around the sub ject since brinksma et al grieskamp et al the report from the last event lists some of the open problems in the area these all document a lively research community and very promising application area test data generation in combinatorial testing by myra b combinatorial testing has become a common technique in the software tester toolbox in combinatorial testing the focus is on http www model based testing de acknowledgements the authors would like to thank the anonymous reviewers for their helpful comments this work is supported in part by the national science foundation through award ccf and by the air force office of scientific research through awards 0129 and any opinions findings conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the position or policy of nsf or the afosr s anand et al the journal of systems and software fig modeling configurations for testing selecting a sample of input parameters or configuration settings that cover a prescribed subset of combinations of the elements to be tested the most common manifestation of this sampling is com binatorial interaction testing cit where all t way combinations of parameter values or configuration settings are contained in the sample in the past few years the literature on this area of testing has grown considerably including new techniques to gen erate cit samples and applications to novel domains in this section we present an overview of combinatorial testing starting at its roots and provide a summary of the two main directions in which research on cit has focused sample generation and its application to different domains of software systems introduction to combinatorial testing throughout the various stages of testing we rely on heuristics to approximate input coverage and outcomes combinatorial testing has risen from this tenet as a technique to sample in a systematic way some subset of the input or configuration space in combi natorial testing the parameters and their inputs or configuration options and their settings are modeled as sets of factors and val ues for each factor f i orthogonal latin squares mandl around the same time ostrand and balcer developed the category partition method and the test case specification language tsl which gives us a way to model the factors and values so that they can be combined in tsl test inputs or configurations are modeled as categories and partitions and for each a set of choices are described which are equivalence classes for testing there are mechanisms to reduce the combinatorial space choices can be tagged as single or error or predicates can be defined that describe dependencies such as requires between elements the full cartesian product that satisfy these constraints is then generated in brownlie et al presented an extension of mandl work called the orthogonal array testing system oats in which they used orthogonal arrays to define the combinations of an at t email system all pairs of factor values are tested exactly once the work of cohen et al leveraged a key insight that all factor values must be tested at least once lifting the exactly once restriction of orthogonal arrays this led to the use of covering arrays and the core underpinnings of combinatorial interaction testing or cit as is used today out of this work came the automatic efficient test case generator aetg a greedy algorithm that generates cov we define a set of values x x x j that partition the factor space from this model test cases or specific program configurations instances are generated selecting a sub set based on some coverage criterion of the cartesian product of the values for all factors a program with five factors each with three values has or program configurations in total cit has traditionally been used as a specification based system testing technique to augment other types of testing it is meant to detect ering array samples and includes both a modeling language and test process cohen et al over the past several years we have seen a large increase in the number of algorithms for generating cit samples and new applica tions that use cit we don t attempt to provide a complete survey of cit for a recent survey see nie and leung but instead provide a short overview and highlight some key research direc tions one particular type of fault those that are due to the interactions of the combinations of inputs or configuration options for instance example of cit if the aim is to detect faults due to combinations of pairs of con in fig a we show the view preferences tab from a version figuration options using combinatorial testing can satisfy this test of microsoft powerpoint the user has seven configuration options goal using only eleven configurations that they can customize some of the configuration options such the roots of combinatorial testing come from the field of as ruler units have multiple settings to choose from we provide statistics called design of experiments fisher cochran and an enlargement of this selection menu which offers the user the cox in the fisher described a means to lay out crop choice of inches centimeters points or picas in fig b other experiments that combine independent variables in a systematic options such as end with black slide are binary the user can select or way in order to isolate their impact on the observed outcomes deselect this setting in total we have seven configuration options in mandl used these ideas to sample the combinations factors which we have shown as columns in the table fig of parameter values in compiler software through the use of the first factor vertical ruler has two possible values the second fig way cit sample for fig has four values etc in total there are or unique configurations of view preferences in practice this is only a part of the configuration space if we combine this with the preferences from the save tab we have almost configurations and if we model the entire preference space this becomes intractable the literature has reported the optimization configuration space of gcc the gnu compiler collection free software foundation an open source widely used compiler framework is in the order of cohen et al if instead we sample the configuration space so that we cover all pairs of combinations of each factor value we can achieve this using only twelve configurations we show one such sample in fig this sample constitutes a covering array defined next covering arrays a covering array ca n t k v is an n k array on v symbols such that every n t sub array contains all t tuples from the v symbols at least once in a covering array t is called the strength and n is the sample size the parameter t tells us how strongly to test the combinations of settings if t we call this pairwise cit in fig all combinations of the factor value ruler units centimeters have been combined with all values from default view but we can t guarantee any specific combinations of three or more factor values are cov ered we notice that the factors have different numbers of values we define a more general structure next a mixed level covering array mca n t w samples while the second direction refines cit to work in novel application domains we highlight work in each direction next cit generation there is a long history of research on the mathematics of cov ering arrays which we do not attempt to cover but instead refer the reader to two excellent surveys one by colbourn and another by hartman and raskin mathematical techniques may be probabilistic i e they do not construct but prove existence of arrays or provide direct constructions constructions are deter ministic and produce arrays of known sizes but are not as general as heuristic methods known only on a limited subset of possible parameter settings for t k and v c colbourn maintains a website with references of known array sizes and associated techniques colbourn heuristic techniques to generate cit samples have dominated the literature on cit the first algorithms were greedy of which there are two primary types one class follows an aetg like mech anism cohen et al examples are the test case generator tcg tung and aldiwan deterministic density algorithm dda colbourn et al and pict czerwonka one test case at a time is added the test case that increases the number of combinations that are covered the most and within each row the algorithms greedily chose the next best factor value for inclusion the heuristics used to select the next best factor n k array on i k contains v symbols where v only elements from a set k i w of i size and w w each i value differentiate these algorithms some newer variants of this algorithm use meta heuristic search techniques such as genetic algorithms tabu search or ant colony optimization to optimize selection of the factors with in a row i e that part is no longer greedy but still retain the one row at a time greedy step bryce and colbourn a similar type of greedy algorithm the con strained array test system cats sherwood was proposed around the same time as aetg it too selects test cases one at a time but full test cases are enumerated and then re ordered so that the earliest test cases provide the greatest value towards covering uncovered factor values a second class of greedy algo rithms are of the form used in the in parameter order algorithm tai and lei lei et al in ipo the algorithm begins with some number of factors k k and expands the size of the covering array horizontally by increasing k and vertically by adding new test cases to the sample to complete coverage if needed meta heuristic search techniques have been used to generate cit samples working on the entire sample at once some size of n is chosen as a start guided by a fitness function and a stochastic process to transition through the search space different solutions are tried and evaluated until a covering array either is found for w k is an column i and the rows of each n t subarray cover all t tuples of values from the t columns at least once we typically use a shorthand notation to equal con secutive entries in w i entries each equal to i k for example three consecutive can be written as fig is a mixed level covering array with k and t an mca when we use the term covering array for cit we usually are referring to a mixed level covering array the minimization of the covering array sample size i e n has been a focus of much work in this domain while it is has been shown that the upper bound on the size of a covering array grows logarithmically in k cohen et al it is non trivial to construct cit samples some instances of cit construction have been shown to be np hard such as finding the minimal size given forbidden constraints colbourn et al bryce and colbourn research directions the research on cit has branched in two main directions the first direction develops methods and algorithms to generate cit s anand et al the journal of systems and software s anand et al the journal of systems and software that n or a time out has occurred n is then adjusted in subsequent iterations until the smallest array is produced simulated annealing has been the most widely discussed meta heuristic algorithm for constructing covering arrays cohen et al garvin et al other approaches include genetic algorithms stardom tabu search nurmela and constraint solvers hnich et al another primary direction for cit research has been generat ing samples that consider dependencies or constraints between factor values called constrained cit ccit for example in fig suppose that the vertical ruler is not visible when using picas because this functionality is unsupported we do not want to include this combination in our samples it will render the con figuration infeasible in ccit satisfiability solvers have been used to aid in the evaluation of legal combinations of factor values in the work of cohen et al standard meta heuristic search algorithms and greedy algorithms for cit have been tightly interwoven with sat solvers to achieve this goal calvagna and gargantini and grieskamp et al use solvers as the primary method to generate the samples and more recently binary decision diagrams bdd have been employed as a way to generate ccit samples segall et al cit application domains the original uses of cit was for test case generation where the factors and their values are system inputs or parameters and each row of the covering array is a test case brownlie et al cohen et al dalal et al dunietz et al cit has also been applied to test protocol conformance grieskamp et al burroughs et al more recent work samples configurations to be tested under which many test cases will be run qu et al yilmaz et al kuhn et al kuhn and okun fouch et al dumlu et al one type of configurable system a software product line has been an area of active research on cit cohen et al perrouin et al software product lines are sys tems of program families that have a well managed asset base and feature model from which one can derive a cit model cohen et al clements and northrop mcgregor first suggested that products in an spl could be sampled for testing using cit cohen et al described a mapping from a feature model to a relational mode perrouin et al have more tightly integrated construction with the feature model there has been recent work that explores the use of cit when testing sequences kuhn et al yuan et al in traditional cit there is no notion of order any two columns of the covering array can be swapped as long as a mapping is maintained for the con crete test cases to be applied in sequence based cit each factor becomes a location within a sequence and the values within each factor are repeated at each location this has been used to test graphical user interfaces guis yuan et al and devices kuhn et al cit has also been used to characterize the configuration option combinations that are likely to be the cause failures through the use of classification trees yilmaz et al fouch et al more recently colbourn and mcclary present special types of covering arrays called locating and detecting arrays for the pur pose of directly isolating the causes another new direction for cit is to tune the test process and coverage through the use of variable strength covering arrays cohen et al prioritization bryce and colbourn qu et al and incremental cover ing arrays fouch et al where the size of t can vary the order of testing is prescribed or we generate increasingly stronger t by re using existing tests from lower strength t when moving to higher t conclusion on combinatorial testing in this section we have presented an overview of combinato rial testing defined the primary mathematical object on which this research is based and presented some research directions that are being pursued this is a promising area of research for auto mated software test generation with opportunities to enhance new domains of its application fruitful future research directions for generating cit samples includes automated model extraction adapting to model evolution and developing techniques that re use or share information between different test runs in addition to applying cit to novel application domains an area of potential for improvement in this direction is the combination of program anal ysis techniques with cit to refine the sample space and to target specific interactions at the code as opposed to only the specifica tion level test data generation by adaptive random testing by tsong yueh chen empirical studies have shown that failure causing inputs tend to form contiguous failure regions consequently non failure causing inputs should also form contiguous non failure regions therefore if previously executed test cases have not revealed a failure new test cases should be far away from the already executed non failure causing test cases hence test cases should be evenly spread across the input domain it is this concept of even spreading of test cases across the input domain which forms the basic intuition for adap tive random testing a family of test case selection methods designed to enhance the failure detection effectiveness of random testing by enforcing an even spread of randomly generated test cases across the input domain this section provides a brief report on the state of the art of adaptive random testing introduction to adaptive random testing random testing rt is one of the most fundamental and most popular testing methods it is simple in concept easy to implement and can be used on its own or as a component of many other test ing methods it may be the only practically feasible technique if the specifications are incomplete and the source code is unavail able furthermore it is one of the few testing techniques whose fault detection capability can be theoretically analysed adaptive random testing art chen et al has been proposed as an enhancement to rt several empirical studies have shown that failure causing inputs tend to form contiguous failure regions hence non failure causing inputs should also form contiguous non failure regions white and cohen therefore if previous test cases have not revealed a failure new test cases should be far away from the already executed non failure causing test cases hence test cases should be evenly spread across the input domain it is this concept of even spreading of test cases across the input domain which forms the basic intuition of art anti random test ing malaiya also aims at even spreading of test cases across the input domain however a fundamental difference is that art is a nondeterministic method and anti random testing is in essence a deterministic method with the exception of the first test case which is randomly chosen another difference is that anti random test ing requires testers to specify the number of test cases in advance whereas there is no such a constraint for art to facilitate discussion it is first necessary to define some ter minology by failure rate we mean the ratio of the number or size of failure causing inputs to the number or size of the set of all possible inputs hereafter referred to as the input domain by failure patterns we mean the distributions and geometry of the s anand et al the journal of systems and software failure causing inputs by efficiency we refer to the computation time required with lower computation time indicating higher effi ciency strictly speaking efficiency should also include memory but memory will not be considered in this section due to space limitations and the lack of implementation details such as the data structures used by effectiveness we refer to the fault detection capability which can be measured by the effectiveness metrics including p measure e measure f measure etc the f measure is defined as the expected number of test cases required to detect the first failure the p measure is defined as the probability of detecting at least one failure and the e measure is defined as the expected number of failures detected a set of test cases is assumed when using p measure or e measure rt is a popular testing method and art has been originally pro posed as an enhanced alternate to rt this section will focus on the state of the art of art from the perspective of using rt as a baseline we will only compare art to rt and not compare art to other testing methods we are interested in the problem that when rt has been chosen as a viable testing method for a system is it worthwhile to use art instead as a reminder to avoid any confusion and misunderstanding cost effectiveness in this section refers to the fault detection capa bility achieved for the resources spent some researchers such as arcuri and briand used the term effective where we use cost effective therefore when comparing across papers such a difference in the meanings should be noted we decide to deal with effectiveness and efficiency separately in this section because such an approach will give us a better picture about which aspect or direction shall be improved various art algorithms various approaches have been identified to implement the con cept of even spreading of test cases across the input domain as a consequence a number of different art algorithms have been developed chan et al chen et al ciupa et al lin et al liu et al mayer shahbazi et al tappenden and miller the major approaches include selection of the best candidate as the next test case from a set of candidates this approach first generates a set of random inputs as candidates from which the best candidate as defined against set criteria is selected as the next test case exclusion in each round of test case generation this approach first defines an exclusion region around each already executed test case random inputs are generated one by one until one input is outside all exclusion regions of the already executed test cases and then this input is selected as the next test case partitioning this approach uses the information about the loca tion of already executed test cases to divide the input domain into partitions and then to identify a partition as a designated region from which the next test case will be generated test profiles instead of using a uniform test profile as normally adopted by rt this approach uses a specially designed test pro file which is able to achieve an even spreading of test cases over the input domain dynamic adjustment of the test profile during testing is required in this approach metric driven distribution metrics such as discrepancy and dispersion are normally used to measure the degree of even distribution for a set of points instead of being used as a measurement metric this approach uses the distribution met rics as selection criteria to select new test cases such that a more even distribution of the resultant test cases could be obtained the above is not an exhaustive list but rather gives some of the most popular approaches furthermore it should be noted that for each approach different methods can be used to achieve an even spreading of test cases therefore many art algorithms have been developed for example the most popular algorithm taking the first approach is the fixed sized candidate set art hereafter referred to as fscs art chen et al in which a fixed size candidate set of random inputs is first generated whenever a new test case is needed for each candidate set a selection criterion is applied to select the best candidate as the next test case adopted selection criteria include maxi min maxi maxi maxi sum etc for maxi min the distance or dissimilarity in the case of non numeric inputs between each candidate and its nearest already executed test case is first calculated the candidate with the largest such distance is then selected as the next test case for maxi sum the distances between each candidate and all the already executed test cases are first summed the candidate with the highest such sum is then selected as the next test case intuitively speaking inputs near the boundaries of the input domain will have a higher probability of being selected as test cases when maxi min is used instead of maxi sum in other words different art algorithms have different effectiveness performance efficiency performance and characteris tics which in turn give rise to different favourable and unfavourable conditions for their applications as fscs art is the first published art algorithm and has been the most cited art algorithm since the inception of art some previous studies have treated fscs art and art as equivalent or exchangeable we would like to emphasize that fscs art is only one of the many members of the family of art algorithms and fscs art is not equivalent to art which refers to the family of testing methods in which test cases are random and evenly spread across the input domain obviously the strengths and weaknesses of a particu lar art algorithm for a specific type of software are not necessarily valid nor expected to be similar for other art algorithms for numeric input domains the distance or dissimilarity metric used to measure far apart is easily and naturally defined how ever the choice of a distance metric for non numeric input domains may not be straightforward we have proposed a generic distance metric based on the concept of categories and choices kuo merkel ciupa et al have proposed a specific dis tance metric for object oriented software tappenden and miller in press have proposed a specific distance metric for cookies collection testing of web applications it is understood that there are currently investigations into the application of art in input domains involving strings trees finite state machines etc effectiveness in the studies of art the adopted effectiveness metrics include f measure p measure and the time to detect the first failure obviously different effectiveness metrics have different strengths and weaknesses and there is no single best effectiveness metric also it is common that a testing method is better than another testing method with respect to one effectiveness metric but worse if measured against another metric therefore a metric may be appropriate in one scenario but inappropriate in another the selec tion of an appropriate metric is in itself a challenging problem the f measure has been the most frequently used metric to compare the effectiveness of art and rt chen et al compared rt and art using open source numerical analysis programs written in c with seeded faults for three out of these programs there was no significant difference between the f measures of art and rt for one program the f measure of art was about of the f measure of rt and for the remaining eight programs the f measure of art was between and of the f measure of rt ciupa et al compared rt and art using 1990 s anand et al the journal of systems and software real life faulty versions of object oriented programs selected from the eifflebase library their results showed that the average f measure of art was about of the f measure of rt lin et al compared rt and their art using six open source java soft ware artifacts with seeded faults the average f measures for art and rt were and respectively in the study conducted by zhou et al they used four numerical programs written in c from gnu scientific library with seeded faults they used two art algorithms and hence there were eight comparison scenar ios between art and rt their results showed that in one of the eight comparison scenarios the f measure of art was about of the f measure for rt and for the remaining seven out of the eight comparison scenarios the f measure of art was between and of the f measure for rt tappenden and miller used simulations to compare art and rt they observed that all of the testing methods ear fscs rrt and the sobol sequence significantly outperformed rt with respect to the block failure pattern with respect to the strip pattern art methods significantly outperformed rt for all failure rates and point pattern simulation yielded results similar to the strip pattern art methods performed slightly better and not worse than rt with significant effect sizes ranging from r to r arcuri and briand have observed that for one mutant of a program the f measures for rt and art were and respectively see table in arcuri and briand so there has been a general consensus that art is better than rt with respect to the f measure the superiority of art over rt with respect to the f measure is intuitively expected as the concept of even spreading of test cases originates from the objective of hitting the contiguous failure regions using fewer test cases furthermore the f measure improvement is quite significant and is in no way diminished by any potential challenge to previous experiments validity a recent analytical study chen and merkel proves that even if we know the shapes sizes and orientations of the failure regions but not their locations it is impossible to have a strategy that guarantees the detection of a failure with its f measure being less than half of the f measure for rt in other words of rt f measure is an upper bound of the effectiveness improvement that we can possibly achieve when we know the sizes shapes and ori entations of the failure regions in reality we are not able to know the sizes shapes and orientations of the failure regions prior to testing since art never uses nor assumes such information art shall not have a lower f measure than the optimal strategy which is designed according to such information when interpreted with the simulation and experimental results of the f measures of art this theoretical result implies a rather surprising but most welcome conclusion that art is close to the optimal strategy and that the upper bound is indeed a tight bound as shown in the proofs and examples in chen and merkel technically speaking the opti mal strategy is to construct a grid of test cases according to the sizes shapes and orientations of the failure regions an even spreading of test cases is a lightweight approach to implement an approxima tion to such a grid and hence art can be viewed as a lightweight approach to implementing the optimal strategy in other words it seems unlikely that there are other testing techniques which can use significantly fewer test cases than art to detect the first failure unless there is access to the information about the locations of the failure regions which is usually not possible an immediate conclu sion is that future research shall be focused on either how to use the information of the location of failure causing input to develop new testing strategies that can outperform art for example see zhou et al or how to improve the efficiency of art by reducing the cost for test case generation for example see chan et al chen et al as pointed out by arcuri and briand previous empirical studies or simulations only involved failure rates larger than and hence were perhaps not comprehensive enough therefore it is worthwhile to conduct further experiments to verify whether or not art does still have a lower f measure than rt for extremely low failure rates however both the proofs of the theoretical anal ysis chen and merkel and the results of a simulation study chen et al about the impact of the geometry of failure regions show that the fewer the distinct failure regions are the better performance of f measure art has this implies that art will have a better f measure performance than rt at later stages relatively fewer distinct failure regions and lower failure rates than at earlier stages relatively more distinct failure regions and higher failure rates of software development as far as we know all existing art algorithms tend to achieve increasingly even spread ing with more test cases in other words lower failure rates are actually favourable scenarios for art with respect to f measures we are not aware of any work showing that at lower failure rates rt has a lower f measure than art obviously it is worthwhile to see more experimental data on this aspect in summary there is no challenge to the fact that art has a significantly lower f measure than that of rt chen et al have used simulations to compare the p measure between art and rt and have found that art out performs rt recently shahbazi et al have proposed an innovative approach to use the concept of centroidal voronoi tes sellations to evenly spread random test cases over the input domain and developed rt rbcvt and rt rbcvt fast methods which also belong to the art approach as they evenly spread random test cases over the input domain a very important result is that their rt rbcvt fast method is of the same order of computational com plexity as rt in the application of their methods the size of a set of test cases is defined first in both their simulations which used var ious types of failure patterns and their empirical analysis which used mutants the rt rbcvt method consistently demonstrated higher p measures than rt as reported that rbcvt is significantly superior to all approaches for the block pattern in the simulation framework at all failure rates as well as the studied mutants at all test set sizes although the magnitude of improvement in testing effectiveness results is higher for the block pattern compared to the point pattern the results demonstrate statistically significant improvement in the point pattern in arcuri and briand empirical analysis using mutants arcuri and briand it was reported that although the results in fig suggest that art can be better than random testing the odds ratios are lower than in most cases the results in fig show that art is still very unlikely to detect faults in most of the cases the p measure is lower than i e art would have less than chance of finding failure by definition the p measure for rt or art is a function of the size of the test set furthermore the value of the p measure for rt or art will be increased if the size of the test set is increased as an example for illustration consider a program with failure rate of on average rt needs to use test cases to detect a failure in other words the p measure for rt using test cases will be very close to thus if the size of the test set is chosen to be then the p measure for both rt and art will be even closer to on the other hand if the size of the test set is chosen to be then obviously the p measures for rt and art will be close to furthermore for the scenarios of using and test cases the differences between their p measures for rt and art if any are likely to be very small therefore when comparing the p measures of rt and art a full range of the sizes of the test set should be used in order to get a comprehensive and meaning ful comparison however this problem may not occur when rt or art is compared to other testing strategies which require a par ticular number of test cases for a specific program suppose that a program has k paths for path coverage testing a set of k test cases is required in this case when p measure is used as the effectiveness s anand et al the journal of systems and software 2001 metric to compare rt and path coverage testing it is not only mean ingful but also fair that a random test set of k elements for rt should be compared with a path coverage test set of k elements irrespec tively of the failure rate for the program under test this specific value of k is not arbitrarily chosen and there is a justification how ever when p measure is used to compare rt and art an immediate question is what should be the appropriate size of the test set used simply because the size of the test set has significant impact on the returned values of the p measure hence the f measure is more appropriate than the p measure in the comparison of rt and art compared to the f measure the p measure has been less often used in evaluating art nevertheless all studies have consistently shown that art outperforms rt with respect to the p measure this universal observation is consistent with an analytical result that the p measure of the proportional sampling strategy pss is not lower than that of rt chen et al 2001 pss is a test case selection strategy for subdomain testing which allocates a number of test cases to a subdomain in proportion to the subdomain size pss is in fact an art algorithm using a partitioning approach thus it is intuitively appealing to expect the p measure of other art algorithms to be not lower than that of rt furthermore it is important to note that pss has been proved to be a necessary and sufficient condition for partition testing to outperform rt with respect to the p measure with regard to the e measure pss and rt have been theoretically proved to have the same e measure chen et al 2001 in addition to the f measure and p measure the amount of time to taken to detect the first failure or fault has been used as a per formance metric by ciupa et al in their investigation using real life faulty programs and lin et al in their investigation using open source java programs with seeded faults strictly speak ing the measurement of the time to detect the first failure is better interpreted as a cost effectiveness metric rather than an effective ness metric ciupa et al found that art required an average of times the amount of time required by rt to detect the first failure but lin et al found that art required an average of times the amount the apparently different observations are understandable because this metric depends on the characteristics of the programs which are different in these two studies ciupa et al have proposed using a clustering technique to reduce the distance computation overheads with the basic idea being to only compute the distances to the cluster centres rather than to each of the already executed test cases their preliminary study shows an average improvement of the time to first fault over artoo of at no cost in terms of faults found since the time to detect first failure for art is times that for rt in their study a improvement is in fact a very encouraging result that justifies more research being conducted in this area in summary both simulations as well as empirical analyses using real life faulty programs and mutants have consistently shown that art outperforms rt with respect to the p measure and the f measure but art may still use less time to detect the first failure than rt despite the fact that art requires more computa tion time for test generation because of the additional task of evenly spreading the test cases across the input domain efficiency compared to rt art algorithms are expected to use more com putation time and memory because of the additional task of evenly spreading the test cases chen et al as explained above we will only consider computation time in this section obviously dif ferent art algorithms have different orders of complexity for the generation of test case ranging from the highest order of n log n to n where n denotes the number of already executed test cases intuitively speaking algorithms with higher orders of complexity for test case generation are expected to have better even spreading of test cases and hence are expected to have a better fault detec tion capability such an expectation normally occurs but not always since different methods are used to achieve an even spread of test cases we have different art algorithms each of which has its own strengths and weaknesses as well as favourable and unfavourable conditions for its application for example a conventional imple mentation of fscs art has complexity therefore it would be inappropriate to apply fscs art to programs with very small fail ure rates unless the program execution time and test setup time are considerably larger than the time required by fscs art to generate a test case there exist general techniques that are applicable to most of the art algorithms to reduce their cost of test case generation as discussed above ciupa et al have proposed to use the technique of clustering to reduce the distance computation over heads and have obtained positive results another technique is called mirroring chen et al its basic idea is to divide the input domain into k partitions of which one partition is referred to as the source partition and the other partitions are referred to as the mirror partitions art is applied only on the source partition to gen erate test cases within it with simple mappings the test cases gen erated in the source partition are mapped into the mirror partitions to generate new test cases within themselves for fscs art its test case generation overheads can be effectively reduced by a factor of k another technique is called forgetting or aging chan et al instead of using all already executed test cases to determine the next test case we use only a portion or a constant number of already executed test cases to determine the next test case if the option of a constant number of already executed test cases is used the order of complexity for generating the next test case will be independent of n generally speaking when a reduction method for distance computations is applied the reduction may bring in new kinds of overheads and may be at the expense of the fault detection capability however such a deterioration of fault detection capa bility does not always occur one instance is observed by ciupa et al in their investigation on the technique of clustering that an average improvement of the time to first fault over artoo of at no cost in terms of faults found apart from ciupa et al study which involved real life faulty programs other investigations into the impact of reduction techniques have used simulations and mutants therefore it is important to have further experiments using real life faulty programs to investigate the impact of these general reduction techniques on the efficiency of art frameworks for cost effective application of art after discussing the effectiveness and efficiency of art we are now ready to discuss how to apply art in practice as a reminder this section only compares art and rt therefore our objective is to determine how to use art as a cost effective alternate to rt when rt has been chosen as a viable testing method to test a system there are two possible application scenarios one with a fixed number of test cases equivalently a limited resource and the other without such a constraint for the first scenario our recommen dation is to use tappenden and miller rt rbcvt fast method because it has the same order of computational complexity as rt but it has a higher p measure than rt now let us consider the other scenario since different programs may have different execution times different test setup times and different test case generation times obviously an art algorithm may be cost effective for one program but not cost effective for another as compared with rt therefore for a given program it is a challenging problem to select a cost effective art algorithm let alone to select the most cost effective art algorithm let us explain the difficulty by first visiting the problem of selecting a cost effective sorting algorithm for a given file similar to art there are 1992 s anand et al the journal of systems and software 2001 many sorting algorithms which have different orders of computa tion complexity and favourable and unfavourable conditions for their applications however when we are going to do sorting nor mally we have some information about the file to be sorted such information will help us to choose an appropriate sorting algorithm for example if the file is known to be nearly ordered we would use bubble sort instead of quick sort if the file is known to be random or nearly random then quick sort rather than bubble sort should be used similarly if we know the execution time test setup time and failure rate of the software under test we would be able to use the information about test case generation complexity for an art algorithm to determine whether it is more cost effective than rt to test the software but in reality though we may know some information about the execution time and test setup time of the software under test we do not know its failure rate in other words we do not have sufficient information to determine whether an art algorithm is more cost effective than rt for this given software then does it mean that art is practically useless as we are not able to determine whether an art algorithm is more cost effective than rt for the given program the answer is no some potential frameworks for cost effective applications of art are presented as follows a simple framework is to successively apply rt rbcvt fast with test sets of sizes n applications of art and tools compared to rt art has been applied to fewer real life pro grams however we expect a growth in the application of art to real life programs because more and more efficient art algo rithms have been emerging chen et al have applied art to testing open source numerical analysis programs written in c using mutants ciupa et al have compared rt and art using real life faulty versions of object oriented programs selected from eiffelbase library their experimental results showed that art used significantly fewer test cases to detect the first failure than rt times but art used more time to detect the first failure than rt times also observed is that art revealed faults that rt did not reveal in the same allocated time iqbal et al have com pared rt art and search based testing using genetic algorithms and the evolutionary algorithm their study included a real life real time embedded system which was a seismic system they have observed that art was the best performer but there is a probability of wrongly claiming that art is better than rt if that is actually not the case iqbal et al hemmati et al have used art to test a safety monitoring component of a safety critical control system written in c and a core component of a video conference system written in c tappenden and miller in n n n k an estimation of the fail press have used evolutionary art in their cookie collection test ure rate should first be made which is then used to determine the ing of six open source web applications faults were detected in value of n an over estimation of the failure rate is recommended five out of the six web applications their results showed that evo as an example for illustration if some available information such lutionary art was an effective testing method lin et al have as past testing history and program size suggests that the failure used six open source java software artifacts with manually seeded rate for the software under test is not less than then we may faults in evaluating their art method five of the six subjects were assume the estimated failure rate to be with a failure rate of from apache common library and the other was siena rt needs to use on average test cases in order to detect with regard to the automated art tools autotest ciupa et al failure thus we may set n if rt rbcvt fast cannot find supports art for object oriented programs a very good failure with a random set of test cases then one of the many design feature of autotest is to use artoo as a plug in strategy for possible ways is to set n n and n k such that n n input generation with such a feature other art algorithms could n k k and successively apply rt rbcvt fast using test sets be easily supported by autotest in the study by shahbazi et al of sizes n n and n k programs were developed to support fscs art restricted another framework can be built upon the technique of adaptive random testing art by exclusion evolutionary art rbcvt and testing by adaptive testing it basically means that in the process of rbcvt fast iqbal et al have developed an automated test software testing a testing method may be replaced by another test framework which can support art to test real time embedded sys ing method in response to some on line collected feedbacks cai tems and the framework has been found effective lin et al suppose we are required to test a program p let e denote its have developed the tool artgen that supports the testing of java average execution time and g n denote its generation time for the programs using a divergence oriented approach to art the major nth test case since the f measure of art shall not be less than half ity of art algorithms consist of two processes namely a process of the f measure of rt as proved analytically chen and merkel for random generation of inputs and a process to ensure an even obviously it is only worthwhile to continue art if g n is spreading of test cases across the input domain in fact the major less than e suppose that we have art a art b and art c whose ity of the processes of ensuring an even spreading of test cases are test case generation complexities are of the orders of n log n and quite simple therefore it is not difficult to build one own art tool n respectively as a note normally the higher the order of complex on top of a random test case generator in other words it should be ity is the higher the fault detection effectiveness an art algorithm quite straightforward to plugin art even spreading component has here we assume art a performs better than art b which into an existing rt tool in turn performs better than art c with respect to f measure or p measure we shall start testing with art a first until we reach future challenges and work g n e then we use art b until we reach a new n such that g n e then we use art c until we reach a new n such that majority of the previous art investigations involved simulations that g n e by then we may apply rt rbcv fast successively and failure rates greater than therefore it is important to with different test sets as explained in the immediately preceding have more investigations which will involve lower failure rates paragraph or use the general reduction techniques to keep the cost using real life faulty programs or mutants empirical analysis of generating a new test case steady such as the technique of for is required to validate the conjecture that lower failure rate is getting using a constant number of already executed test cases in a favourable condition for art with respect to f measure as distance computation discussed above the above sketches of the frameworks are very high level but as explained above further research should be focused on reduc they are conceptually feasible obviously a lot of technical details ing the cost of test case generation for art algorithms in order to need to be defined in the actual application also new types of overheads may be introduced therefore the cost effectiveness of these proposed frameworks needs to be validated by experimental the software is available at url http www steam ualberta ca main papers analysis involving real life programs rbcvt s anand et al the journal of systems and software 2001 enhance their cost effectiveness so far the investigated reduc tion techniques include clustering mirroring and forgetting with the exception of ciupa et al preliminary investigation on the technique of clustering which involved real life faulty programs other investigations on the general reduction tech niques only involved simulations and mutants though ciupa et al results are very positive for the technique of clustering the impact of the other reduction techniques should be further analysed using more real life programs the proposed framework for how to apply art using the tech nique of adaptive testing has been briefly outlined above the sketches of the framework are very high level but the frame work is conceptually feasible a lot of technical details need to be defined in actual application such as how to deal with the already executed test cases after switching from one art algo rithm to another art algorithm obviously its feasibility needs to be validated by experimental analysis involving real life pro grams failure patterns provide valuable information to help us to develop new and effective test case selection strategies we coined this area as failure based testing the domain test strategy proposed by white and cohen is not only a fault based testing strategy as stated by them but also a failure based testing technique it is indeed the first failure based testing technique its target is the domain fault which gives rise to a specific failure pattern in the input domain the concept of geometry is applied to the resultant failure pattern to design test cases that guar antee to detect the relevant fault art is a failure based testing method using the most primitive information of the contiguity of failure causing inputs since failure patterns also have other information there is still great potential benefit to be gained from the use of this other information to develop new testing strategies the search based testing community has devel oped many searching techniques some of which may become or be adapted to become new search techniques for failure regions conclusion on adaptive random testing all existing investigations have consistently shown that art outperforms rt with respect to the f measure and p measure these investigations include simulations and experimental anal ysis using both mutants and real life faulty programs the positive results of these simulation and experimental investigations are consistent with the interpretations and results of the theoretical analysis though the scope of existing investigations may not be considered sufficiently comprehensive arcuri and briand the superiority of art over rt with respect to the f measure and p measure is unlikely to be challenged nevertheless more com prehensive experiments on the f measure and p measure of rt and art will still be worthwhile compared to rt art has the additional task of evenly spread ing the test cases therefore art will unavoidably consume more computation time and memory than rt hence it is understand able that an art algorithm is not necessarily more cost effective than rt for a given program despite the fact that it is superior to rt with respect to the f measure and the p measure on the other hand with respect to the metric of time required to find the first failure or fault rt is not always superior to art even though art incurs more computation time than rt this is also understandable because the characteristics of the programs under test will affect the value of this metric obviously the characteristics of the pro gram under test must be considered when determining whether an art algorithm will be more cost effective than rt since its inception the art research has been focused on the development of new algorithms which would have a lower f measure as explained above a recent analytical result shows that art is in fact a lightweight approach to implementing the optimal strategy which is essentially equivalent to constructing a grid of test cases according to the sizes shapes and orientations of the failure regions an immediate conclusion is that art has great potential to be a cost effective alternate to rt attention should then be shifted from the effectiveness to the efficiency of art that is to the reduction in time and space complexity in order to make art a cost effective alternate to rt conceptually speaking reduction in the computation and memory overheads are possible but may be at the expense of the degree of even spreading which in turn may affect the effectiveness however the recently published method of rt rbcvt fast shows that art can indeed serve as a cost effective alternate to rt because it has the same order of computational complexity as rt test data generation in search based software testing by mark harman phil mcminn john clark and edmund k burke search based software testing sbst is a branch of search based software engineering sbse in which search algorithms are used to automate the process of finding test data that maximises the achievement of test goals while minimising testing costs there has been much interest in sbst leading to several recent surveys this paper presents some emerging challenges and open problems for the development of this exciting research agenda these include hybrids of sbst and dse dynamic symbolic execution optimi zing to best handle demands of the oracle co evolving tests and software simultaneously hyper heuristics where sbst may be integrated into other aspects of sbse e g requirements prioriti sation and optimization of failures for ease of debugging introduction to search based testing as this paper shows the problem of automatically generating test inputs is hard for example even the most basic activities such as seeking to cover a branch in the code involve reachability ques tions that are known to be undecidable in general weyuker the testing community has therefore focused on techniques that seek to identify test sets that cover near optimal sets of branches in reasonable time many of these techniques are covered in other sections of this paper this section is concerned with the area of search based soft ware testing sbst sbst is a branch of search based software engineering sbse harman and jones 2001 in which optimisa tion algorithms are used to automate the search for test data that maximises the achievement of test goals while minimising test ing costs there has been much interest in sbst leading to several recent surveys this section presents some emerging challenges and open problems for the development of this exciting research agenda sbst is the process of generating test cases or often the inputs of test cases using search based algorithms guided by a fit ness function that captures the current test objective sbst has been applied to a wide variety of testing goals including struc tural harman and mcminn mcminn et al michael et al 2001 tonella functional wegener and bhler non functional wegener and grochtmann and state based properties derderian et al search based approaches have been developed to address a wide and diverse range of domains including testing approaches acknowledgements the authors would like to thank yue jia for fig s anand et al the journal of systems and software 2001 based on agents nguyen et al aspects harman et al interactions cohen et al integration colanzi et al briand et al mutation harman et al zhan and clark regression walcott et al yoo et al stress grosso et al and web applications alshahwan and harman in all approaches to sbst the primary concern is to define a fitness function or set of fitness functions that capture the test objectives the fitness function is used to guide an algorithm which searches the space of test inputs to find those that meet the test objectives because any test objective can in principle be re cast as a fitness function the approach is highly generic and there fore widely applicable as the foregoing list of testing applications demonstrates there are many different search based algorithms to choose from though much of the literature has tended to focus on evolutionary algorithms harman there are several surveys of these aspects of sbst afzal et al ali et al harman et al mcminn in these surveys the reader can find more detailed treatments of the work on sbst for non functional properties afzal et al empirical evidence regarding sbst ali et al as well as overviews of techniques harman et al mcminn mcminn et al therefore in this section we do not seek to provide yet another overview of sbst rather we focus on some of the exciting and challenging avenues that lie ahead for future work in this rapidly growing research and practitioner community in describing these future directions we seek to consider work which is already underway as well as more blue skies directions for open challenges that could yield major breakthroughs for example we consider work underway on co evolution and management of oracle cost as well as work on hybridising sbst with other test data generation techniques such as dynamic symbolic execution dse a topic covered in more detail elsewhere in this paper the oracle problem is important because the automation of testing requires automation of the checking of outputs as well as the generation of inputs co evolution is interesting and important because it fits so well the way in which the testing process operates as we shall see in all these emerging areas we can expect more work in the immediate future we also consider open challenges such as the problem of migrating from generation of test cases to generation of testing strategies using search and optimising the insight that can be gained from sbst hybrids of sbst and dse the dynamic symbolic execution dse approach godefroid et al to handling dynamic data structures proved very effec tive leading lakhotia et al to incorporate dse approach into sbst conversely sbst handles floating point computation well while dse is limited by the power of the constraint solvers available which typically cannot solve floating point constraints efficiently naturally therefore one might expect that the advan tage of combining the two techniques will be that the strengths of one can overcome the shortcomings of the other this led several authors to develop approaches to augment dse with search based approaches to solving floating point com putations lakhotia et al used a local search to augment the pex dse based testing tool from microsoft while souza and borges augmented standard constraint solving with a par ticle swarm optimiser to improve the performance of symbolic pathfinder the first authors to propose a combination of sbst and dse to produce a hybrid were inkumsah and xie who introduced the evacon framework which composes the two approaches reporting the first results for a combined dse sbst approach the austin search based software testing tool also provides hybrid capabilities for which results have been reported to compare sbst and dse for out of the box test data generation lakhotia et al baars et al developed a new approach to sbst in which symbolic execution is integrated into the search by augmenting the fitness function used to guide sbst in a way this work does for sbst what dse does for constraint based testing however the differences between sbst and dse mean that the modifications to symbolic execution required to make it scalable are also different that is whereas dse performs a complete symbolic execution using concrete values baars et al use a purely symbolic execution with no concrete values but apply it only to local regions of code to improve the fitness function harman et al also combined dse and sbst to produce the first approach to test data generation for strong and higher order mutation testing they use dse to achieve weak mutation adequacy following a variant of the approach of liu et al and papadakis and malevris this approach generates con straints the satisfaction of which yields weak mutation adequacy to extend this to strong mutation adequacy harman et al search the space of additional conjuncts for constraints to augment those that extend weak to strong the fitness function seeks maximal control flow disruption in order to increase the likelihood of strong adequacy as this recent work demonstrates there is much activity at the interface between sbst and dse that is producing a form of crossover and mutation of the two approaches because of their complementary nature we can expect to see more work on the combination of these two promising test data generation tech niques the proliferation of publicly available tools that support both approaches and hybrids thereof creates a rich infrastructure from which future research can draw handling the oracle testing involves examining the behaviour of a system in order to discover potential faults determining the desired correct behaviour for a given input is called the oracle problem man ual testing is expensive and time consuming particularly because of the manual effort devoted to solving the oracle problem this is the human oracle cost we need to develop sbst algorithms and methods that automatically generate test inputs that reduce human oracle cost thereby significantly reducing the overall cost of testing we also need search based techniques that can help to generate test oracles as well as test cases fraser and zeller of course the cost of generating test inputs by hand is high this has driven the growth of the search based testing research area indeed over papers have been published in the area accord ing to a recent survey harman et al however despite this considerable publication output there is very little work on either reducing the oracle cost harman et al mcminn et al or using sbst to generate oracles fraser and zeller most previous work concentrates on the problem of searching for good test inputs but it does not address the equally important problem of reducing the cost of checking the output produced in response to the inputs generated the current state of the art in sbst thus addresses only the benefit half of the testing problem that of generating inputs that meet the testing criterion it fails to address the other half of the problem the cost of checking the output produced this is simply not realistic for many testing applications it assumes that all that matters to the tester is the achievement of the highest possible coverage at any cost however a tester might for example prefer an approach that achieves coverage with test cases over an alternative that achieves coverage with s anand et al the journal of systems and software 2001 test cases or the tester may prefer a test suite that lowers the comprehension cost of individual test cases by minimising test case verbosity fraser and zeller and maximising readability mcminn et al fortunately the sbst paradigm naturally generalises to multi objective optimisation formulations thereby allowing us to develop techniques that balance the multiple objectives of cost and benefit if we can measure the oracle cost then we can make it a min imisation objective in our sbst test data generation approach this will mean that all approaches to test data generation will be natu rally multi objective harman et al because they will need to balance cost and benefit this is a natural step forward for sbst since testing is all about balancing cost and benefit we know that exhaustive testing is impossible so we wish to achieve maximum benefit ultimately fault finding measured through surrogates such as coverage for minimum cost ultimately monetary measured through surrogates such as effort and time opportunities for co evolution with co evolutionary computation two or more populations evolve simultaneously using possibly different fitness functions in competitive co evolution the idea is to capture a predator prey model of evolution in which both evolving populations are stimu lated to evolve to better solutions in co operative co evolution the idea is to symbiotically co evolve several populations each rely ing on the other to work in concert as part of a larger system that contains them adamopoulos et al were the first to suggest the appli cation of co evolution for sbst arguing that this could be used to evolve sets of mutants and sets of test cases where the test cases act as predators and the mutants as their prey for testing the compet itive model has hitherto proved best suited since test cases make natural predators various forms of testing and bug fixing have been attacked using competitive co evolution arcuri et al also used co evolution to evolve programs and their test data from specifications arcuri arcuri and yao using co evolution arcuri and arcuri and yao also developed a co evolutionary model for bug fixing in which one population essentially seeks out patches that are able to pass test cases while test cases can be produced from an oracle in an attempt to find the shortcomings of a current population of proposed patches in this way the patch is the prey while the test cases once again act as predators we can expect to see work in which various software artefacts and their test cases are co evolved the test cases will be evolved to find counter examples that demonstrate that the artefacts being evolved are not yet optimal the artefacts can then be re evolved to produce new versions for which the counter examples no longer apply through iteration of this co evolutionary cycle we seek to obtain not only high quality artefacts but also test cases that can be used to demonstrate their effectiveness hyper heuristic software engineering one key question for the sbse sbst research agenda is can we radically increase automation by integrating sbst with other forms of sbse consider the two connected trees depicted in fig the lower tree is a sub tree of software engineering defined by the acm classification system existing work on sbse is currently applied at the leaves of this tree more work can and will surely be done across the community within each of the leaves to better address instances of each software engineering problem however such approaches on their own can only offer improvement of a fig hyper heuristic sbse using hyper heuristics we may be able to develop tactics an strategies that will unite different software engineering activities with sbst narrow set of very similar engineering activities that is we will have improved test input generation and separately improved requirements prioritisation what we will still need is to find ways to unite the two so that we can have optimised tests generated from requirements with requirements prioritised in a manner that takes account of testability to solve this larger software engineering challenge sbst needs to make a transition from solving instances to automatically finding tactics that solve instances this will increase the abstraction level at which we apply sbse as indicated in the upper tree in fig drawing together sets of related software engineering activities for instance we shall be able to combine different kinds of test data generation searching for improved tactics that deploy each to maximise their effectiveness and minimise cost automatically tailoring the methodology to suit the particular test problem in hand in this way we would be making a leap from tactics that solve classes of problems to strategies that cross the existing software engineering boundaries as illustrated by the top node of the upper tree in fig ultimately the goal would be to unify previously poorly connected areas of software engineering activity within a single computational search process this would be a kind of hyper heuristic software engineering as an example of the possibilities for hyper heuristic software engineering suppose we succeed in combining requirements opti misation zhang et al with sbst we shall now be able to optimise the selection of requirements based not only on tradi tional aspects of sbse for requirements customer satisfaction cost etc but also on the implications for regression testing coverage achievable test execution time we would reach the pinnacle the root of the upper tree in fig with hyper heuristic requirements and testing however we could go further still in our quest for a unified hyper heuristic software engineering suppose we now also manage to draw sbse for project planning antoniol et al chicano and alba into our hyper heuristic software engineering framework we will then have a combined approach to optimised requirements project planning and testing instead of merely discussing requirements choices devoid of their technical and managerial consequences we can use our combined automated approach to check the implications of requirement choices on the project plan used to staff the project and implement the requirements we can also explore implica tions for regression testing and seek multi objective solutions that 1996 s anand et al the journal of systems and software 2001 balance the competing objectives of requirement choices effective implementation plans and efficient and effective regression testing armed with a hyper heuristic software engineering tool deci sion makers could then enter negotiations armed able to respond in real time to changing requirement choices with analysis and results on the implications for the cost and duration of the project and its testing this would not be merely better requirements engi neering or better testing it would be a fundamentally different approach to software development in which optimisation would be at the heart a lingua franca within which decisions could be made about requirements design and testing with detailed investiga tion of their consequences gradually as hyper heuristic software engineering strategies draw in more of the process the information available would be further enriched bringing together aspects of marketing negotiation customer relations project management design and testing optimising and understanding failures some failures are caused by exceptionally long and complex sequences of actions and events this makes it theoretically hard and sometimes practically impossible to find the fault or faults that cause the failure therefore a natural yet challenging and important question to ask is can we simplify the failure to make it easier to debug in some ways this problem is related to the oracle cost problem described in section that is if we can reduce the cost of under standing the output of a test then we reduce the oracle cost to the human on the other hand there is also a human cost in under standing the input to a test if a long sequence and or a complex sequence of actions is required to replicate a failure at the develo pers site the engineers may find it too complicated to understand the causes of the failure and therefore too difficult to find the faults that cause failure suppose we can capture the failing behaviour with an asser tion or some such similar mechanism now we can have a fitness function that measures how close a test case comes to causing a failure to manifest itself this would be one potential fitness func tion if we can additionally measure the complexity of a test case then we can seek to minimize this while maximising similarity to the failure of interest another multi objective formulation of test data generation a natural starting point for failures would be wrong values in variables since this would be easy to capture within existing sbst frameworks fitness computation would be no different to that for branch coverage one could use a simple testability transformation harman mcminn et al to insert a branch that cap tures the failing behaviour and seek to cover the branch a starting point for test complexity would be simply the length of the input sequence required to reveal the fault the challenge will be in find ing supporting fitness functions and ways to smooth an otherwise rather spiky landscape in order to provide guidance to shorter test inputs that manifest the desired failure conclusion on search based testing search based software testing sbst is a branch of search based software engineering sbse which re formulates test objectives as fitness functions to guide automated search procedures this provides a way to automate test generation for many different forms of testing the approach is supremely general and aston ishingly widely applicable because any test objective that can be measured is a candidate for this transformation into a fitness func tion there surely remain many exciting important and productive test objectives that have yet to be attacked using this sbse reform ulation thereby providing many fruitful avenues for future work conclusion and acknowledgement this paper presents a survey of some of the most prominent techniques of automated test data generation including sym bolic execution model based combinatorial adaptive random and search based testing the survey has followed the novel approach of orchestrated surveys we believe that by coordinating renowned specialists of carefully selected topics we are able to balance breadth with depth to produce one overall article of reasonable size editing this paper has been a new experience for the editors the editors would like to thank the authors of the sections for their participation and excellent work carried out in the project the editors would also like to express their appreciation to the review ers of the sections their constructive and critical comments have been invaluable to the success of the project the editors are most grateful to prof hans van vliet the editor in chief of the journal of systems and software for his support to this project and for the valuable advice and direction he has given to the editors of the paper as well as for his patience during the long process of the project introduction c provides just enough abstraction above assembly language for programmers to get their work done without having to worry about the details of the machines on which the programs run despite this abstraction c is also known for the ease in which it allows programmers to write buggy programs with no runtime checks and little static checking in c the programmer is to be trusted entirely despite the abstraction the language is still low level enough that programmers can take advantage of assumptions about the underlying architecture trust in the programmer and the ability to write non portable code are actually two of the design principles under which the c standard was written these ideas often work in concert to yield intricate platform dependent bugs the potential subtlety of c bugs makes it an excellent candidate for formalization as subtle bugs can often be caught only by more rigorous means in this paper we present a formal semantics of c that can be used for finding bugs rather than being an on paper semantics it is executable machine readable and has been tested against the gcc torture tests see section the semantics describes the features of the standard but we often cite the text from the proposed standard we use the text because it will eventually supersede the standard and because it offers clearer wording and more explicit descriptions of certain kinds of behavior our semantics can be considered a freestanding implementation of the standard defines a freestanding implementation as supported in part by nsa contract c and by romanian smis csnr contract no permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee popl january philadelphia pa usa copyright acm 01 a version of c that includes every language feature except for and types and that includes only a subset of the standard library our semantics is the first arguably complete dynamic semantics of c see section above all else our semantics has been motivated by the desire to develop formal yet practical tools our semantics was developed in such a way that the single definition could be used immediately for interpreting debugging or analysis described in section at the same time this practicality does not mean that our definition is not formal being written in a subset of rewriting logic rl it comes with a complete proof system and initial model semantics briefly a rewrite system is a set of rules over terms constructed from a signature the rewrite rules match and apply everywhere making rl a simple uniform and general formal computational paradigm this is explained in greater detail in section our c semantics defines c syntactic operators the defini tions of these operators are given by semantic rules spread over source lines of code sloc however it takes only of those rules sloc to cover the behavior of statements and another for expressions sloc there are rules for dealing with declarations and types rules for memory and technical rules defining helper operators finally there are rules for the core of our standard library the semantics itself is described in more detail in section and is available in its entirety at http c semantics googlecode com contributions the specific contributions of this paper include a detailed comparison of other c formalizations the most comprehensive formal semantics of c to date which is executable and has been thoroughly tested demonstrations as to its utility in discovering program flaws constructive evidence that rewriting based semantics scale features our semantics captures every feature required by the standard we include a partial list here to give an idea of the completeness and explain any shortcomings in section all aspects related to the below features are included and are given a direct semantics not by a translation to other features expressions referencing and dereferencing casts array index ing a i structure members and arithmetic bitwise and logical operators sizeof increment and decrement assign ments sequencing ternary conditional statements for do while while if if else switch goto break continue return types and declarations enums structs unions bitfields initializers static storage typedefs variable length arrays values regular scalar values signed unsigned arithmetic and pointer types structs unions compound literals standard library malloc free set longjmp basic i o conversions implicit argument and parameter promotions and arithmetic conversion and explicit casts comparison with existing formal c semantics there have already been a number of formal semantics written for c one might rightfully ask why yet another we claim that the definitions so far have either made enough simplifying assumptions that for many purposes they are not c or have lacked any way to use them other than on paper while paper semantics are useful for teaching and understanding the language we believe that without a mechanized definition it is difficult to gain confidence in a definition appropriateness for any other purpose below we highlight the most prominent definitions and explain their successes and shortcomings in comparison with our work gurevich and huggins one of the earliest formal descrip tions of ansi c is given by gurevich and huggins using abstract state machines asms then known as evolving algebras their semantics describes c using four increasingly precise layers each formal and analyzable their semantics covers all the high level constructs of the language and uses external oracles to capture the underspecification inherent in the definition of c their seman tics was written without access to a standard and so is based on kernighan and ritchie however many behavioral details of the lowest level features of c are now partially standardized including details of arithmetic type representation and evaluation strategies the latter has been investigated in the context of asms but none are present in the original definition based on our own ex perience the details involving the lowest level features of c are incredibly complex see section but we see no reason why the asm technique could not be used to specify them their semantics was never converted into an executable tool nor has it been used in applications however their purpose and context was different from ours as pointed out elsewhere p their semantics was constructed without the benefit of any mechanization according to gurevich their purpose was to discover the structure of c at a time when c was far beyond the reach of denotational semantics algebraic specifications etc cook cohen and redmond soon after the previous definition cook et al describe a denotational semantics of using a custom made temporal logic for the express purpose of proving properties about c programs like us they give semantics for particular implementation defined behaviors in order to have a more concrete definition these choices are then partitioned off so that one could in theory choose different implementation defined values and behaviors they have given at least a basic semantics to most c constructs we say at least without malicious intent although their work was promising they moved on to other projects before developing a testable version of their semantics and without doing any concrete evaluation additionally no proofs were done using this semantics cook and subramanian the related work of cook and subramanian is a semantics for a restricted subset of c based loosely on the semantics above this semantics is embedded in the theorem prover nqthm a precursor to they were successful in verifying at least two functions one that takes two pointers and swaps the values at each and one that computes the factorial they were also able to prove properties about the c definition itself for example they prove that the execution of p a n puts the address of the nth element of the array a into p p their semantics is at its roots an interpreter it uses a similar technique to that described by blazy and leroy to coax an interpreter from recursive functions but there is no description in their work of any reference programs they were capable of executing as above it appears the work was terminated before it was able to blossom personal communication norrish the next major semantics was provided by nor rish who gives both static and dynamic formal semantics inside the hol theorem proving system for the purpose of verifying c pro grams later extended to c his semantics is in the structural operational semantics sos style using small step for expressions and big step for statements one of the focuses of his work is to present a precise description of the allowable evaluation orders of expressions his semantics still stands as a precise representation of evaluation in c in section we demonstrate how our definition captures the same behaviors working inside hol provides an elegant solution to the under specification of the standard norrish can state facts given by the standard as axioms theorems to maintain executability we chose instead to parameterize our definition for those implementation defined choices in that respect our definitions conceptually comple ment each other his is better for formal proofs about c while ours is better for searching for behaviors in programs see section proofs of program correctness as well as semantics level proofs have already been demonstrated in the framework used by our semantics but we have not yet applied these techniques to c norrish uses his definition to prove some properties about c itself as well as to verify some strong properties of simple line programs but was unable to apply his work to larger programs his semantics is not executable so it has not been tested against actual programs however the proofs done within the hol system help lend confidence to the definition papaspyrou 2001 a denotational semantics for is described by papaspyrou using a monadic approach to domain construction the definition includes static typing and dynamic semantics which enables him not only to represent the behavior of executing programs but also check for errors like redefinition of an identifier in the same scope papaspyrou norrish and cook et al each give a typing semantics in addition to the dynamic semantics while we and blazy and leroy below give only dynamic semantics papaspyrou represents his semantics in haskell yielding a tool capable of searching for program behaviors this was the only semantics for which we were able to obtain a working interpreter and we were able to run it on a few examples having modeled expression non determinism and being denotational his semantics evaluates a program into a set of possible return values however we found his interpreter to be of limited capability in practice for example using his definition we were unable to compute the factorial of six or the fourth fibonacci number blazy and leroy a big step operational semantics for a subset of c called clight is given by blazy and leroy while they do not claim to have given semantics for the entirety of c their semantics does cover most of the major features of the language and has been used in a number of proofs including the verification of the optimizing compiler compcert to help validate their semantics they have done manual re views of the definition as well as proved properties of the semantics such as determinism of evaluation they additionally have verified semantics preserving transformations from their language into sim pler languages which are easier to develop confidence in their semantics is not directly executable but they describe a mechanism by which they could create an equivalent recursive function that would act as an interpreter this work has not yet been completed clight does not handle non determinism or sub expressions with side effects however since publication they have added a new front end small step definition called compcert c that does handle these features and is also being used to handle goto personal communication personal communication 534 definition feature gh ccr cr no pa bl er bitfields enums floats string literal struct as value arithmetic bitwise casts functions exp side effects break continue goto switch longjmp malloc variadic funcs feature gh ccr cr no pa bl er fully described partially described not described gh represents gurevich and huggins ccr is cook et al cr is cook and subramanian no is norrish pa is papaspyrou bl is blazy and leroy and er is our work figure dynamic semantics features we condense our study of related works in figure for interested parties this chart may be contentious however we believe that it is useful both for developers of formal semantics of c and for users of them to give a broad though admittedly incomplete overview of the state of the art of the formal semantics of c also it may serve as an indication of the complexity involved in the c language although not all features are equally difficult we did our best to give the authors the benefit of the doubt with features they explicitly mentioned but the other features were based on our reading of their semantics we have also discussed our views with the authors where possible to try and establish a consensus obviously the categories are broad but our intention is to give an overview of some of the more difficult features of c we purposefully left off any feature that all definitions had fully defined finally there are a number of other emergent features such as multi dimensional arrays that are difficult to discern correctness through simple inspection of the formal semantics i e without testing or verifying it it is also difficult to determine if feature pairs work together for example does a definition allow bitfields inside of unions we decided to leave most of these features out of the chart because they are simply too hard to determine if the semantics were complete enough for them to work properly background in this section we give a little background on the c standard including some important definitions we additionally explain the rewriting formalism we use to give our semantics of c c standard information the c standard uses the idea of undefined and partially defined behaviors in order to avoid placing difficult requirements on imple mentations it categorizes the particular behaviors of any c imple mentation that are not fully defined into four categories unspecified implementation defined undefined and locale specific behavior for the purposes of this paper we focus on three of these unspecified behavior use of an unspecified value or other behav ior with two or more possibilities and no further require ments on which is chosen in any instance implementation defined unspecified behavior where each imple mentation documents how the choice is made undefined behavior behavior upon use of a non portable or erro neous program construct or data with no requirements an example of unspecified behavior is the order in which the argu ments to a function are evaluated an example of implementation defined behavior is the size of an int an example of undefined behavior is referring to an object outside of its lifetime to put these definitions in perspective for a c program to be maximally portable it shall not produce output dependent on any unspecified undefined or implementation defined behavior this is called strictly conforming however programmers use c for many inherently non portable tasks such as writing device drivers the standard offers another level of conformance called conforming where the program may rely on implementation defined or even unspecified but never undefined behavior based on this our definition is parametric in implementation defined be haviors and uses symbolic computation to describe unspecified behaviors as much as possible this behavior is kept separate from the semantics underlying the high level defined for all implemen tations aspects of the language more details about our parame terization are described in section and about our use of sym bolic values in section why details matter it is tempting to gloss over the details of c arithmetic and other low level features when giving it a formal semantics however c is designed to be translatable to machine languages where arithmetic is handled by any number of machine instructions the effects of this overloading are easily felt at the size boundaries of the types it is a common source of confusion among programmers and so a common source of bugs here we give a few examples that reveal even apparently simple c programs can involve complex semantics for the purposes of these examples assume that ints are bytes capable of representing the values to and long ints are bytes to also unless specified in c a type is assumed to be signed in the following program what value does c receive int a b long int c a b one is tempted to say but that misses an important c specific detail the two operands of the multiplication are ints so the multiplication is done at the int level it therefore overflows which according to the c standard makes the expression undefined what if we make the types of a and b unsigned to unsigned int a b long int c a b here the arithmetic is again performed at the level of the operands but overflow on unsigned types is completely defined in c the result is computed by simply reducing the value modulo one more than the max value mod gives us one last variation signed chars are one byte in c to what does c receive signed char a b int c a b except chars and bitfields whose signedness is implementation defined bytes are only required to be at least bits long since the chars are signed then based on the first example above the result would seem undefined 127 however this is not the case in c types smaller than ints are promoted to ints before doing arithmetic there are essentially implicit casts on the two operands int c int a int b thus the result is actually while the above examples might seem like a game the conclu sion we draw is that it is critical when defining the semantics of c to handle all of the details the semantics at the higher level of functions and statements is actually much easier than at the level of expressions and arithmetic these issues are subtle enough that they are very difficult to catch just by manually inspecting the code and so need to be represented in the semantics if one wants to find bugs in real programs even though errors related to the above details continue to be found in real compilers previous semantics for c either did not give semantics at this level of detail or were not suit able for identifying programs that misused these features this is one of our primary reasons for wanting an executable semantics we give some of the rules associated to binary arithmetic in section rewriting logic and k to give our semantics we use a rewriting based semantic framework called k inspired by rl in particular our semantics is written using the k maude tool which takes k rewrite rules and translates them into maude maude is a rewriting logic engine that provides facilities for the execution and analysis of rewriting logic theories rl organizes term rewriting modulo equations namely associa tivity commutativity and identity as a logic with a complete proof system and initial model semantics the central idea behind using rl as a formalism for the semantics of languages is that the evo lution of a program can be clearly described using rewrite rules a rewriting theory consists essentially of a signature describing terms and a set of rewrite rules that describe steps of computation given some term allowed by signature e g a program together with input deduction consists of the application of the rules to that term this yields a transition system for any program a single path of rewrites describes the behavior of an interpreter while searching all paths would yield all possible answers in a nondeterministic program for the purposes of this paper the k formalism can be regarded as a front end to rl designed specifically for defining languages in k parts of the state are represented as labeled nested multisets as seen in figure these collections contain pieces of the program state like a computation stack or continuation e g k environments e g env types stacks e g callstack etc as this is all best understood through an example let us consider a typical rule for a simple imperative language see section for the equivalent rule in c for finding the address of a variable x l k x l env we see here two cells k and env the k cell represents a list or stack of computations waiting to be performed the left most i e top element of the stack is the next item to be computed the env cell is simply a map of variables to their locations the rule above says that if the next thing to be evaluated which here we call a redex is the application of the referencing operator to a variable x then one should match x in the environment to find its location l in memory with this information one should transform the redex into that location in memory l this example exhibits a number of features of k first rules only need to mention those cells again see figure relevant to the rule the rest of the cell infrastructure can be inferred making the rules robust under most extensions to the language second to omit a part of a cell we write for example in the above k cell we are only interested in the current redex x but not the rest of the context finally we draw a line underneath parts of the state that we wish to change in the above case we only want to evaluate part of the computation but neither the context nor the environment change this unconventional notation is actually quite useful the above rule would be written out as a traditional rewrite rule like this x  k  x l  env l  k  x l  env items in the k cell are separated with which can now be seen the thing  to and notice   is take that the place of the above the most important nearly the entire rule is duplicated on the right hand side rhs duplication in a definition requires that changes be made in concert in multiple places if this duplication is not kept in sync it leads to subtle semantic errors in a complex language like c the configuration structure is much more complicated and would require actually including additional cells like control and local figure these intervening cells are automatically inferred in k which keeps the rules more modular going back to k we use to represent the unit element of any algebraic lists or sets including the list we also use to stand for a term that we do not care to name finally in order to get the redexes to the top of the k cell i e in order to identify which positions in the syntax tree can be reduced next the grammar of c is annotated with additional strictness annotations for example for addition we say that exp exp exp strict meaning that either argument of the addition operator can be taken out for evaluation nondeterministically in contrast the if construct looks like this stmt if exp stmt strict indicating that only the first argument can be taken out for evaluation the two annotations above cause the following six rules to be automatically generated e e k e k if e s k e e if s v e v e e e e e k v e k v if s if v s k here e e v represents an e evaluated and e represent unevaluated expressions and v expression i e a value while these are the rules generated by k maude in the theory of k they can apply anywhere not just at the top of the k cell there are additional annotations for specifying more particular evaluation strategies and can be found in documentation on k we also give names to certain contexts that are evaluated differently for example the left hand side lhs of an assignment is evaluated differently than the rhs the use of this is described in section the semantics of c in k in this section we describe the different components of our defini tion and give a number of example rules from the semantics syntax we use the frontc parser with additions made and included in cil an off the shelf c parser and transformation tool frontc itself parses only ansi c but cil extended it with syntax for we use only the parser here and none of the transformations of cil we give semantics directly to the abstract syntax tree generated by the parser the frontc parser with extensions is used by a number of other tools including compcert and frama c configuration program state the configuration of a running program is represented by nested multisets of labeled cells and figure shows the most important cells used in our semantics while this figure only shows cells k k t figure subset of the c configuration we use over in the full semantics the outer t cell contains the cells used during program evaluation at the top a k cell contains the current computation itself and a local cell holds a number of cells related to control flow and below there are a number of cells dealing with global information in the local cell there is a callstack used for calling and returning from functions and a control cell which gets pushed onto the call stack inside the control cell there is a local variable environment env a local type environment types local aggregate definitions structs a loop stack a record of the locations that have been written to since the last sequence point section and the name of the current function the cells inside the control cell were separated in this manner because these are the cells that get pushed onto the call stack when making a function call outside the local cell are a number of global mappings such as the global variable environment genv the global type environment gtypes global aggregate definitions gstructs the heap mem the dynamic allocation map malloced and a map from function name label pairs to continuations for use by goto and switch memory layout our memory is essentially a map from locations to blocks of bytes it is based on the memory model of both blazy and leroy and ros map env map types map structs list loopstack bag locswrittento k currfunction map genv control list callstack map gtypes map gstructs map mem map malloced map gotomap local semantics we now give the flavor of our semantics by examining a few of the rules for the rules below recall that in k what is above the line is considered the lhs of the rule while what is below the line is considered the rhs parts of a rule without a line at all are considered to be on both sides of the rule lookup and assignment we first consider one of the most basic expressions the identi fier according to the standard an identifier is a primary expres sion designating an object in which case it is an lvalue or a function in which case it is a function designator although in informal language an lvalue is an expression that appears on the lhs of an assignment this is not the case according to the c standard an lvalue can be more accurately thought of as any expression that designates a place in memory a footnote in the standard suggests it might better be called a locator value we denote lvalues with brackets an lvalue that points to location l which is of type t is denoted by l t with this in mind here then is our lookup rule x l t k x l env x t type u et al in the sense that the actual locations themselves are symbolic numbers however it is more like the former in that the actual blocks of bytes are really maps from offsets to bytes below we see a snippet of a memory cell holding four bytes obj mem this rule is actually very similar to the example address of rule we gave in section it says that when the next thing to evaluate is the program variable x both its location l and its type t should be looked up in the env and type cells and the variable should be replaced by an lvalue containing those two pieces of information we distinguish between objects and functions based on type this says that at symbolic location there is an object whose in almost all contexts this lvalue will actually get converted size is bytes those bytes are and all objects are to the value at that location broken into individual bytes including aggregate types like arrays or structs as well as base types like integers our pointers are actually base offset pairs which we write as sym b o where b corresponds to the base address of an object itself while the o represents the offset of a particular byte in the object we wrap the base using sym because it is symbolic despite representing a location it is not appropriate to e g directly except when it is the operand of the sizeof operator the unary operator the operator the operator or the left operand of the operator or an assignment operator an lvalue that does not have array type is converted to the value stored in the designated object and is no longer an lvalue compare b b section it is better to think of the above we call these contexts reval for right evaluation here is the as representing object as opposed to location rule for simplifying lvalues in the right value context when looked up the bytes are interpreted depending on the type of the construct used to give the address the simplest example possi ble is dereferencing a pointer sym of type unsigned char reval l t read l t which would simply yield the value of type unsigned char looking up data using different pointer types requires taking into account a number of implementation defined details such as the use of signed magnitude one or two complement representation or the order of bytes endianness these choices are made para metric in the semantics and can be configured depending on which implementation a user is interested in working with section when new objects ints arrays structs etc get allocated each is created as a new block and is mapped from a new symbolic number the block is allowed to contain as many bytes as in the object and accesses relative to that object must be contained in the block we represent information smaller than the byte i e bitfields by using offsets within the bytes themselves while it might seem that it would be more consistent to treat memory as mappings from bit locations to individual bits bitfields themselves are not addressable in c so we decided on this hybrid approach where isarraytype t isfunctiontype t the rule for read then does the actual read from memory its evaluation involves a series of rules whose job is to determine the size of the type pull the right bytes from memory and to piece them together in the right order to reconstruct the value there are over highly technical rules defining read just for integer types alone this process results in a normal value instead of an lvalue which we represent simply as v t reference and dereference we can now take a look at the rule for the operator l t l pointertype t k this rule says that when the next computation to be performed is taking the address of an lvalue it should simply be converted into a true value holding the same address but whose type is a pointer type to the original type we can expect to find an lvalue as the argument because the reval context does not include the arguments of the address operator the rule for dereference is similarly simple l pointertype t checkderefloc l l t k where t void this will first make sure that the location l is allowed to be dereferenced e g it is valid memory and will then evaluate to an lvalue of the same location as with lookup no memory is read by default notice that checkderefloc is blocking the top of the k cell as long as it stays there no rules that match other constructs on the top of k can apply if checkderefloc succeeds it will simply evaluate to the unit of the construct and disappear this is called dissolving our rule for checkderefloc is checkderefloc sym b o k b obj len mem where o len here we match the constituent parts of a location b and o or base and offset as explained in section we then match the base part of the pointer in the memory cell giving us an object and check that the offset is within the bounds of the object if this is the case we dissolve the checkderefloc task structure members the standard says a postfix expression followed by the operator and an identifier designates a member of a structure or union object the value is that of the named member and is an lvalue if the first expression is an lvalue here is the rule for when the first expression is an lvalue l structtype s f l offset t k s f offset t structs this rule finds the offset offset and type t of the field f in struct s and simply adds the offset to the base address l of the struct to evaluate the expression the result is another lvalue of the type of the field in contrast the rule for when the first expression is not an lvalue cannot simply work with pointers v structtype s f extractfield v sd s f k s f sd structs one situation in which this arises is when a function returns a struct and the programmer uses the function call to access a particular field as in the expression fun field the call to fun will result in a struct value represented in the rule above by v structtype s the helper function extractfield will look at the bytes of the struct represented by v and read a value of the appropriate type sd contains the offset and type of the field there are many rules shared by the extractfield and read helpers since both have to piece together bytes in implementation defined orders to make new values the semantics for the arrow operator p f is identical to that of the dot operator above after dereferencing the first subexpression e f e f there are similar rules as above for union where all offsets of a union fields are multiplication and related conversions as mentioned in section the rules for arithmetic in c are non trivial to show this in more detail here we give many of the rules related to integer multiplication here is the core multiplication rule i t i t arithinterpret t i where hasbeenpromoted t this rule matches when multiplying values with identical promoted types more on promotion shortly it then uses a helper operator arithinterpret to convert the resulting product into a proper value arithinterpret t i i t int i where min t i max t i arithinterpret t i arithinterpret t i int where isunsignedinttype t i max t arithinterpret t i arithinterpret t i int max t int where isunsignedinttype t i min t the first rule creates a value as long as the product is the range of the type the next two rules collapse out of range unsigned products into range by not giving rules to out of range signed types we catch signed overflow here with the above rules defined the question becomes how to promote and convert the types of the operands so that the core multiplication rule can take effect first all arithmetic in c takes place at or above the size of ints this means smaller types need to be coerced into int or unsigned int t promote t max t int k where hasbeenpromoted t the above rule and its commutative partner cause unpromoted multiplication operands to be promoted of the actual promotion the standard says if an int can represent all values of the original type the value is converted to an int otherwise it is converted to an unsigned int promote t int where min int min t max int max t promote t unsigned int where min int min t max int max t finally in order to perform the multiplication the types of the operands have to be identical if the types are not identical an implicit conversion takes place to convert the different types to a common type there are eight rules for this given in the standard to give an idea of their flavor we give a few of the rules for integer conversions here first the rule to enable conversion i t k cast  i i t cast  i where t t t t  arithconv t t the standard says if both operands have signed integer types or both have unsigned integer types the operand with the type of lesser integer conversion rank is converted to the type of the operand with greater rank arithconv t t maxtype t t where hassamesignedness t t rank is a partial ordering on integer types based on their ranges and signedness e g rank short int rank int additionally the ranks of unsigned integer types equal the ranks of the corresponding signed integer types continuing with the conversion rules otherwise if the operand that has unsigned integer type has rank greater or equal to the rank of the type of the other operand then the operand with signed integer type is converted to the type of the operand with unsigned integer type arithconv t t t isunsigned t issigned t rank t rank t and similarly for the commutative case the above equations use a number of helper operators in the side conditions the definitions for min and max are given in section the other operators are defined as expected malloc and free here we show our semantics of malloc and free these are functions from the standard c library that perform dynamic memory allocation and deallocation the declarations of these functions are void malloc size void free void ptr where is an unsigned integer type that is implementation defined when a programmer calls malloc an implementation k where can return a new pointer pointing to a new block of memory the size specified by the programmer or it can return null e g if there is no memory available here is the rule for a successful call to malloc malloc n alloc l n l pointertype void k l n malloced where l is fresh if the user requests n bytes the semantics will schedule that many bytes to be allocated at a new location and record that this memory was dynamically allocated in the malloced cell here is the related rule for a failed called to malloc malloc nullpointer pointertype void k this rule is usually only useful when searching the state space a call to free is meant to deallocate space allocated by malloc its rule is also straightforward free l k l n malloced l obj n mem when the user wants to free a pointer l it is removed from both the malloced and mem cells by matching these cells the rule ensures that the pointer has not already been freed and once applied ensures no other rules that use that address can match into the memory setjmp and longjmp finally we show our semantics of setjmp and longjmp these are functions from the standard c library that perform complex control flow they are reminiscent of call cc and are often used as a kind of exception handling mechanism in c the declarations of these functions are int setjmp env void longjmp env int val where is an array type suitable for holding the information needed to restore a calling environment a call to setjmp saves its calling environment for later use by the longjmp function additionally the call to setjmp evaluates to zero here is our rule for setjmp setjmp l write l c  k  k c local int because is an array type it will evaluate to an address l in the rule above we match the remaining computation  similar to a continuation as well as the local execution environment c this includes cells like the call stack and the map from variables to locations which we also call the environment the rule then causes this information to be written at the location of the a call to longjmp restores the environment saved by the most recent invocation of setjmp with the corresponding argument when the user calls longjmp this address is read to find that previous context longjmp longjmp aux l t read l t k and it is then restored longjmp aux c  k k c local this function returns the val that the user passes unless this is a in which case it returns it should be clear that these rules operate on the configuration itself treating it as a first class term of the formalism the fact that k allows one to grab the continuation  as a term is what makes the semantics of these constructs so easy to define this is in sharp opposition to semantic formalisms like sos where the context is a derivation tree and not directly accessible as an object inside a definition i int if i then else i fi int  parametric behavior we chose to make our definition parametric in the implementation defined behaviors and are not the first to do so thus one can configure the definition based on the architecture or compiler one is interested in using and then proceed to use the formalism to explore behaviors this parameterization allows the definition to be fleshed out and made executable for a simple example of how the definition is parametric our k maude module c settings starts with numbytes signed char numbytes short int numbytes int numbytes long int numbytes long long int numbytes float numbytes double numbytes long double these settings are then used to define a number of operators numbits t numbytes t bitsperbyte where isbitfieldtype t min int int int int max int int int int here we use a side condition to check when a type is not a bitfield finally the above rules are used to define how an integer i of type t is cast to an unsigned integer type t cast t where i isintegertype t t i int max t isunsignedinttype t int t i max t here we use helper predicates in our side conditions to make sure this rule only applies when casting from integer types to unsigned integer types there are similar equations used to define other cases expression evaluation strategy and undefined behavior the c standard allows compilers freedom in optimizing code which includes allowing them to choose their own expression evaluation order this includes allowing them to delay side effects e g allowing the write to memory required by x or x to be made separately from its evaluation or use interleave evaluation e g a b c can be evaluated in the order b a c at the same time the programmer must be able to write programs whose behaviors are reproducible and only allow non determinism in a controlled way therefore the standard makes undefined certain situations where reordering creates a race condition the latest treatment of this restriction is given by the standard if a side effect on a scalar object is unsequenced relative to ei ther a different side effect on the same scalar object or a value computation using the value of the same scalar object the behavior is undefined if there are multiple allowable order ings the behavior is undefined if such an unsequenced side effect occurs in any of the orderings this means that if there are two writes or a write and a read to the same object that are unsequenced i e either is allowed to happen before the other then the expression is undefined examples of expressions made undefined by this clause include x x and x x and x x and p x for int x and int p x this relation is related to the concept of sequence points also defined by the standard sequence points cause the expressions they fall between to be sequenced the most common example of a sequence point is the semicolon i e the end of an expression statement all previous evaluations and side effects must be complete before crossing sequence points a hasty read of the standard may wrongly indicate that detecting this kind of undefined behavior is an easy problem that can be checked statically in fact it is undecidable statically moreover one needs to use the entire semantics in order to check it dynamically consider the following example int x y p y int f void if guard p x return int main void return x p f the undefinedness of this program is based on what happens in the call to f if f is called before the other subexpressions in main are evaluated and if the guard expression which could be arbitrarily complex is true then the remaining expression effectively becomes x x which is undefined the possible complexity of the guard is a witness to the static undecidability of this problem the evaluation of the guard may make arbitrary use of the entire c language so the entire semantics is needed in order to determine whether this program is undefined based on this note that when two expressions are unsequenced it means that evaluation can happen in any order thus it is natural to map unsequenced behavior into nondeterministic behavior this way we can use state space exploration as a single mechanism to find unsequenced behavior to identify this kind of undefined behavior naively can be incredibly computationally expensive some optimizations are necessary to make this feasible we offer two such optimizations below first with a little case analysis of the definition of the sequencing relation it is clear that there can be no sequenced write before a read of the same object with no intervening sequence point this means that if in searching the semantic state space we find an execution in which the write of a scalar object happens before a write or read of the same object with no intervening sequence point then we can conclude that this write write or write read pair is unsequenced whenever a write is made its location is recorded in the locswrittento cell which is emptied whenever a sequence point is crossed this cell is first checked whenever a read or write is made to ensure that there is no conflict this strategy has the added benefit that some undefined behaviors of this kind can be detected even during interpretation where only a single path through the state space is explored it is similar to the strategy used by norrish second it turns out that a large subset of allowed orderings do not need to be considered in order to detect undefined behavior or possible nondeterministic behaviors because we are looking for writes before other events we can take the liberty of applying side effects immediately instead of delaying them what would it mean for there to exist an expression whose definedness relied on whether or not a side effect a write occurs later instead of earlier there must be three parts to the expression a subexpression e generating a side effect x and for generality sake further subexpressions e and e the particular evaluation where we do side effects immediately would look like exe e because this is always a possible execution and we assume it does not show a problem we can conclude neither e nor e reads or writes to x if there is a problem only when we delay the side effect it can be seen in a path like e e x e for this to be different than applying the changes to x immediately it means there must be some use of x in the evaluation of e but this contradicts the previous assumption this shrinks the state space dramatically while at the same time not missing any undefined behavior our semantics does capture the appropriate state space as seen in section kcc using a simple frontend that mimics the behavior of gcc c programs are parsed and translated into a maude term then reduced using the rules of our formal semantics for defined programs this process produces indistinguishable behavior from the same c program run as native code we call this interpreter obtained automatically from our formal semantics kcc as we will show in section kcc is significantly more than an interpreter in addition to simple interpretation it is also capable of debugging catching undefined behaviors state space search and model checking once kcc is installed on a system compilation of c programs generates a single executable file an a out containing the semantics of c together with a parsed representation of the program and a call to maude the output is captured by a script and presented so that for working programs the output and behavior is identical to that of a real c compiler to emphasize the seamlessness here is a simple transcript kcc helloworld c a out hello world while it may seem like a gimmick it helped our testing and debugging tremendously for example we could run the definition using the same test harness gcc uses for its testing see section it also means people with no formal background can get use out of our semantics simply by using it as they would a compiler testing the semantics no matter what the intended use is for a formal semantics its actual use is limited if one cannot generate confidence in its correctness to this aim we ensured that our formal semantics remained executable and computationally practical gcc torture tests as discussed in the previous section our semantics is encapsulated inside a drop in replacement for gcc which we call kcc this enables us to test the semantics as one would test a compiler we were then able to run our semantics against the gcc c torture test and compare its behavior to that of gcc as well as the intel c compiler icc and clang c compiler for llvm we ran all compilers with optimizations turned off we use the torture test for gcc specifically those tests inside the testsuite gcc c torture execute directory we chose these tests because they focus particularly on portable machine independent executable tests the readme gcc for the tests says the torture tests are meant to be generic tests that can run on any target we found that generally this is the case although there are also tests that include gcc specific features which had to be excluded from our evaluation there were originally tests of which we excluded tests because they used gcc specific extensions or builtins they used the data type or certain library functions which are not required of a freestanding implementation of c or they were machine dependent this left us with tests further manual inspection revealed an additional tests that were non conforming according to the standard mostly signed overflow or reading from uninitialized memory bringing us to a grand total of viable tests in order to avoid overfitting our semantics to the tests we ran domly extracted about of the conforming tests and developed our semantics using only this small subset and other programs dis cussed in section after we were comfortable with the quality of our semantics when running this subset we ran the remaining tests out of previously untested programs we successfully ran after this initial test we began to use all of the tests to help develop our semantics we now pass of the compliant tests torture tests run of compiler count percent gcc icc clang kcc the tests represent about sloc or sloc file correctness analysis our executable formal semantics performed nearly as well as the best compiler we tested and better than the others we incorporated the passing tests into our regression suite that gets run every time we commit a change this way upon adding features or fixing mistakes our accuracy can only increase three of the six failed tests rely on floating point accuracy problems two more rely on evaluating expressions inside of function declarators as in int fun int i int array i return i which we are not handling properly the last is a problem with the lifetime of variable length arrays coverage analysis in order to have some measure of the effective ness of our testing we recorded the application of every semantic rule for all of the torture tests out of core rules non library non helper operator the gcc torture tests exercised in addition to getting a coverage measure this process suggests an interesting application for example in the gcc tests looked at above a rule that deals with casting large values to unsigned int was never applied by looking at such rules we can create new tests to trigger them these tests would improve both confidence in the semantics as well as the test suite itself exploratory testing we have also tested our semantics on programs gathered from around the web including programs of our own design and from open source compilers not counting the gcc tests we include over sloc in our regression tests that are run when making changes to the semantics these tests include a number of programs from the lcc and compcert compilers we also execute the c reference manual tests also known as cq c which go through kernighan and ritchie and test each feature described in about sloc when these tests are added to the gcc tests described above it brings our rule coverage to 867 rules we can successfully execute duff device an unstructured switch statement where the cases are inside of a loop inside of the switch statement itself as well as quines programs whose output are precisely their source code and a number of programs from the obfuscated c code contest all of these test programs as well as our semantics are available from our project webpage http c semantics googlecode com applications formal semantics is useful here we describe applications of our formal semantics which are in addition to the interpreter already mentioned these tools are automatically derived from the semantics changes made to the semantics immediately affect the tools we are permitted this luxury because we take advantage of general purpose tools available to rl theories of which our semantics is one contrast this to the nearly universal strategy of writing analysis tools independently of semantics instead of developing a different model for each tool a plethora of tools can be created around a single semantic definition these tools are essentially wrappers or views of the semantics debugging by introducing a special function that acts as a break point we can turn the maude debugger into a simple debugger for c programs this provides the ability to step through interesting parts of execution to find out what rules of semantics are invoked in giving meaning to a program in the semantics we handle this function by giving a labeled rule that causes it to evaluate to a void value it is essentially equivalent have been unable to determine the author or origin of this test suite please contact us with any information to void int i if this function is called during execution it starts a debugger that allows the user to inspect the current state of the program one can step through more rules individually from there or simply note the information and proceed if the call is inside a loop the user will see a snapshot each time it reaches the expression for example int main void for int i i i i printf done n we can run or debug the program above as follows kcc debug c a out run the program normally done debug a out or run it in the debugger debug where int k i l env debug resume int k i l env the user can use this to see what the value of the argument is each time through the loop as well as the entire state of the program when the breakpoint was reached the state presented to the user includes all of the cells of the language figure this elided state is represented by the ellipses above in addition to the where and resume commands there is also a step command to step through the application of a single semantic rule runtime verification there are two main avenues through which we can catch and identify runtime problems with a program catching undefined behavior and symbolic execution undefined behavior the first mechanism is based around the idea that when something lacks semantics i e when its behavior is undefined according to the standard then the evaluation of the program will simply stop when it reaches that point in the program we use this mechanism to catch errors like signed overflow or array out of bounds in this small program the programmer forgot to leave space for a string terminator the call to strcpy will read off the end of the array int main void char dest src hello strcpy dest src gcc will happily execute this and depending on the state of memory even do what one would expect it is still undefined and our semantics will detect trying to read past the end of the array because this program has no meaning our semantics gets stuck when exploring its behavior it is through this simple mechanism that we can identify undefined programs and report them to the user by default when a program gets stuck we report the state of the configuration a concrete instance of that shown in figure and what exactly the semantics was trying to do at the time of the problem we have also begun to add explicit error messages for common problems here is the from our tool for this code kcc c a out error encountered while executing this program description reading outside the bounds of an object function strcpy line and elsewhere in this section we take the liberty to slightly simplify the output to make it fit in less vertical space symbolic execution through the use of symbolic execution we can further enhance the above idea by expanding the behaviors that we consider unde fined while maintaining the good behaviors symbolic execution is straightforward to achieve using a rewriting based semantics whether a term is concrete or abstract makes no difference to the theory rules designed to work with concrete terms do not need to be changed in order to work with symbolic terms as we explained in section we treat pointers not as concrete integers but as symbolic values these values then have certain behavior defined on them such as comparison difference etc this technique is based on the idea of strong memory safety which had previously been explored with a simple c like language in this context it takes advantage of the fact that addresses of local variables and memory returned from allocation functions like malloc are unspecified however there are a number of restrictions on many addresses such as the elements of an array being completely contiguous and the fields in a struct being ordered though not necessarily contiguous for example take the following program int main void int a b if a b if we gave objects concrete numerical addresses then they would always be comparable however this piece of code is actually unde fined according to the standard symbolic locations that are actually base offset pairs allow us to detect this program as problematic we only give semantics to relational pointer com parisons where the two addresses share a common base thus eval uation gets stuck on the program above kcc c a out error encountered while executing this program description cannot apply to different base objects function main line of course sometimes locations are comparable if we take the following code instead int main void struct int a int b if a b the addresses of a and b are guaranteed to be in order and in fact our semantics finds the comparison to be true because the pointers share a common base another example can be seen when copying a struct one byte at a time as in a c implementation of memcpy every byte needs to be copied even uninitialized fields or padding and no error should occur because of this our semantics must give it meaning using concrete values here would mean missing some incorrect programs so we use symbolic values that allow reading and copying to take place as long as the program never uses those uninitialized values in undefined ways state space search we can also use our semantics to do both matching based state search and explicit state model checking with linear temporal logic ltl the basic examples below show how our semantics captures the appropriate expression evaluation semantics precisely exploring evaluation order to show our semantics captures the evaluation orders of c expres sions allowed by the specification we examine some examples from related works the results given below are not just theoretical results from our semantics but are actual results obtained from executing the tools provided by our semantic framework to start with a simple example from papaspyrou and maco we take a look at x x in an environment where x is this expression is undefined because the read of x the lone x is unsequenced with respect to the write of x the assignment using our semantics to do a search of the behaviors of this expression finds this unsequenced read write pair and reports an error norrish offers the deceptively simple addition expression x x which in many languages would be valid how ever in c it is again a technically undefined expression due to the unsequenced assignments to x our semantics reports an error for this expression as well another example in the literature is given by papaspyrou which shows how c can exhibit nondeterministic behavior while staying conforming the driving expression is the addition of two function calls in c function evaluation is not allowed to interleave so the behavior of this program is determined solely on which call happens last int r int f int x return r x int main void f f return r if f is called with the argument last then the result will be and similarly for searching with our semantics gives the behaviors r and r which are indeed the two possible results as a last example we look at a more complex expression of our own devising f a b c d except for f each function call prints out its name and returns the function f however prints out its name and then returns a function pointer to a function that prints e the function represented by this function pointer will be passed results of a we elide the actual function bodies because the behavior is more easily understood by this tree e f a b c d this tree or hasse diagram describes the sequencing relation for this expression that is it must be the case that d happens before c that b and c happen before a and that f and a happen before e running this example through our search tool gives precisely the behaviors allowed by the standard kcc nondet c search a out solutions found bdcafe bdcfae bdfcae bfdcae dbcafe dbcfae dbfcae dcbafe dcbfae dcfbae dfbcae dfcbae fbdcae fdbcae fdcbae model checking in addition to the simple state search we showed above one can also use our semantics for ltl model checking for example consider the following program typedef enum green yellow red state state lightns green state lightew red int changens switch lightns case green lightns yellow return case yellow lightns red return case red if lightew red lightns green return int main void while changens changeew this program is meant to represent two orthogonal traffic lights lightns and lightew at the same intersection it provides an implementation of an algorithm to change the state of the lights from green to yellow to red and back we elide the nearly identical changeew function the program takes advantage of the unspecified order of evaluation of addition in the expression changens changeew to nondeterministically choose the order in which the lights are changed there are a number of properties one might like to prove about this program including safety and liveness properties one safety property is that it should always be the case that at least one of the lights is red or d lightns red lightew red we have added a special allowing the programmer to write and name ltl formulae if we call the above formula safety then we can invoke the model checker as follows kcc lights c modelcheck safety a out result bool true similarly it is important that the lights always make progress i e that it is always the case the lights will eventually become green if we try to check d lightns green we find that it does not hold of the above program kcc lights c modelcheck progress a out result modelcheckresult counterexample the reason this property is not verified is that the algorithm is wrong because the calls to changens and changeew can occur in any order it is possible for either of the lights to get stuck on red the program starts with ns gre ew red consider the following execution changens changeew ns yel ew red changeew changens ns red ew red changens changeew ns gre ew red by alternating evaluation orders the program can change the n s light without ever changing the e w light this evaluation order is highly implausable in most c compilers but the se mantics allows it if we fix an evaluation order by changing changens changeew to changens changeew then the property holds kcc lights c modelcheck progress a out result bool true limitations and future work here we delineate the limitations of our definition and explain their causes and effects there are two main ways in which semantics can be incomplete under definedness and over definedness typically when one thinks of incompleteness one thinks of failure to give meaning to correct programs however because we want to be able to identify incorrect or unportable programs the semantics must be balanced appropri ately between defining too much or too little it is equally important not to give semantics to programs that should be undefined in the first case we are not missing any features we have given semantics to every feature required of a freestanding implementation of c with this said our semantics is not perfect for example we still are not passing of our test cases see section also our semantics of floating point numbers is particularly weak during execution or analysis we simply rely on an ieee implementation of floating point arithmetic provided to us by our definitional framework k this is fine for interpretation and explicit state model checking but not for deductive reasoning in the second case although our semantics can catch many bad behaviors other tools cannot e g we have not found any other tool that catches the undefined programs in sections or 8a conforming way to add implementation defined behavior to c there is still room for improvement for one our semantics aligns all types to one byte boundaries this means we cannot catch undefined behavior related to alignment restrictions note that others have worked on formalizing alignment requirements but it has never been incorporated into a full semantics for c we also do not handle type qualifiers like const or volatile we simply ignore them this is safe to do when interpreting correct programs but it means we are not detecting problems related to those features in incorrect programs it also means that we are missing possible behaviors when searching programs that use volatile we have not yet used our c definition for doing language or program level proofs even though the k framework supports both program level and semantics level proofs to do so we need to extend our semantics with support for formal annotations e g assume assert invariant and connect it to a theorem prover this is already being done for a subset of the c language and we intend to apply those techniques to actual c in the future we still do not cover all of the standard library headers so far we have added library functions by need in order to run example programs which is why we have semantics for library functions like malloc longjmp parts of printf variadic functions and over others we intend on covering more libraries in the future but for now one could supplement what we provide by using implementations of libraries written in c in our current semantics only some of the implementation defined behaviors are available the most common ones by mak ing the semantics parametric we hope others can add or change implementation defined rules to suit their needs finally we should mention the speed of our system while it is not nearly as fast as c compiled natively it is usable of the gcc torture test programs described listed in section our semantics ran over of these programs in under seconds each an additional completed in minutes in hours and further in under days in comparison it takes gcc about for each test the reader should keep in mind that this is an interpreter obtained for free from a formal semantics in addition the search and model checking tools suffer the same state explosion problems inherent in all explicit state model checking conclusion it is a shame that despite the best efforts of over years of re search in formal programming languages most language design ers still consider the difficulties of defining formal semantics to outweigh the benefits formal semantics and practicality are not typically considered together when c was being standardized the standards committee explored using formal semantics but in the end decided to use simple prose because anything more ambitious was considered to be likely to delay the standard and to make it less accessible to its audience this is a common sentiment in the programming language community indeed startlingly few real languages have ever been completely formalized and even fewer were designed with formal specification in mind based on our experience with our semantics the development of a formal semantics for c could have taken place alongside the development of the standard within roughly person months we had a working version of our semantics that covered more of the standard than any previous semantics the version presented in this paper is the result of person months of work to put this in perspective one member of the standards committee estimated that it took roughly person years to produce the standard we are not claiming that we have done the same job in a fraction of the time obviously writing a semantics based on the standard is quite different than writing the standard itself we are simply saying that the effort it takes to develop a rewriting based semantics is quite small compared to the effort it took to develop the standard now the obvious question is why should programming lan guage researchers and the popl community in particular care about these developments the answer is clear some of our most important infrastructure our networks will soon be running an entirely new kind of program using our experience principles tools and algorithms our community has a unique opportunity to define the languages these programs will be written in and the in frastructure used to implement them we can have major impact and help make future networks easier to program more secure more reliable and more efficient as a step toward carrying out this agenda we propose a high level language called netcore the network core programming language for expressing packet forwarding policies netcore has an intuitive syntax based on familiar set theoretic operations that allows programmers to construct and reason about rich policies in a natural way netcore primitives for classifying packets in clude exact match bit patterns and arbitrary wildcard patterns it also supports using arbitrary functions to analyze packets and his torical traffic patterns this feature makes it possible to describe complicated dynamic policies such as authentication and load bal ancing in a natural way using ordinary functional programs unfortunately compiling these rich policies is challenging on the one hand the controller machine has the computational power to evaluate arbitrary policies but the switches do not they can only implement simple kinds of bit matching rules on the other hand directing a packet to the controller for processing incurs orders of magnitude more latency than processing it on a switch hence despite the limited computational power of the switches it is critical to find ways for them to perform most packet processing the netcore compiler and run time system surmounts this chal lenge by analyzing programs and automatically dividing them into two pieces one that runs on the switches and another that runs on the controller moreover this division of labour does not occur once at compile time it occurs dynamically and repeatedly intuitively when a packet cannot be handled by a switch it is redirected to the controller the controller partially evaluates the packet with re spect to the current network policy and dynamically generates new switch level rules that handle said packet as well as others like it the new rules are subsequently sent to the switches so similar pack ets arriving in the future are handled in the network fast path over time more and more rules are added to the switches and less and less traffic is diverted to the controller we call this iterative strategy reactive specialization our strategy is inspired by the idiom commonly used for sdn applications today in which an event driven program manually installs a rule to handle future traffic every time a packet is diverted to the controller however many programs written man ually in this style use inefficient exact match rules rather than wild card rules because reasoning about the semantics of overlapping wildcards quickly becomes very complicated too complicated to do by hand hence our strategy improves on past work by pro viding high level abstractions that obviate the need for program mers to deal with the low level details of individual switches synthesizing efficient forwarding rules that exploit the capabili ties of modern switches including wildcard rules implemented by ternary content addressable memories tcams automating the process of dynamically unfolding packet processing rules on to switches instead of requiring that programmers craft tricky low level event based programs manually to summarize the central contribution of this paper is a frame work for implementing a canonical high level network program ming language correctly and efficiently more specifically authserver a figure example topology we define the syntax and semantics for netcore section and model the interaction between the netcore run time system and the network in a process calculus style section this is the first formal analysis of how a controller platform interacts with switches we develop novel algorithms for compiling network programs and managing controller switch interactions at run time includ ing classifier generation and reactive specialization section we prove key correctness theorems section establishing simulation relations between our low level distributed imple mentation strategy and our high level netcore semantics we also prove an important quiescence theorem showing that our implementation successfully relocates computation from the controller onto switches we describe a prototype implementation and an evaluation on some simple benchmarks demonstrating the practical utility of our framework section netcore arose out of our previous work on frenetic an other high level network programming language frenetic has three main pieces an sql like query language for reading network state a language for specifying packet forwarding policies and a functional reactive glue language that processes the results of queries and generates streams of forwarding policies for the network netcore replaces frenetic language for expressing for warding policies with a significantly more powerful language that supports processing packets using arbitrary functions in addition netcore also contains a minimalist query language as its predi cates can analyze traffic history the main contribution of this paper relative to earlier work on frenetic is the design of new algorithms for compiling these rich policies and for managing the controller switch interactions that arise as compiled policies are executed in a network these algorithms handle netcore new policy language and the core elements of frenetic old policy language even better in particular the netcore compiler generates efficient switch classifiers by using wildcard rules that process more packets on switches instead of simple exact match rules and generating rules proactively i e in advance of when they are needed again to process more packets on switches instead of strictly reactively i e on demand finally netcore has a formal semantics and correctness proofs for its core algorithms whereas frenetic had none netcore overview this section presents additional background on sdns and netcore using examples to illustrate the main ideas for concreteness we focus on the openflow sdn architecture but we elide and take liberty with certain inessential details our compiler does not assume the specifics of the current openflow platform openflow overview openflow is based on a two tiered archi tecture in which a controller manages a collection of subordinate switches figure depicts a simple topology with a controller c controller c network switch s network managing a single switch s packets may either be processed on switches or on the controller but processing a packet on the con troller increases its latency by several orders of magnitude hence to ensure good performance the controller typically installs a clas sifier consisting of a set of packet forwarding rules on each switch each forwarding rule has a pattern that identifies a set of pack ets an action that specifies how packets matching the pattern should be processed counters that keep track of the number and size of all packets processed using the rule and an integer priority when a packet arrives at a switch it is processed in three steps first the switch selects a rule whose pattern matches the packet if it has no matching rules then it drops the packet and if it has multiple matching rules then it picks the one with the highest pri ority second the switch updates the counters associated with the rule finally the switch applies the action listed in the rule to the packet in this paper we are concerned with two kinds of actions a forwarding action l l k which forwards the packet to a set of usually one sometimes zero rarely more than one adjacent another switch network network locations or l i host where each and l i a may be the name of controller action  which forwards the packet to the controller for processing netcore a simple static forwarding policy netcore is a declar ative language for specifying high level packet forwarding poli cies the netcore compiler and run time system handle the details of translating these policies to switch level rules and issuing com mands to install the generated rules on switches the simplest netcore policies are specified using a predicate e that matches some set of packets and a set s of locations to which those packets should be forwarded we write these policies e s the simplest predicates match bits in a particular packet header field for example the predicate srcaddr specifies that the first octet of the packet source address must be us ing the standard notation for expressing ip prefix patterns more complex predicates are built by taking the union intersection negation or difference of simpler predicates analo gous set theoretic operations may be used to compose more com plex policies from simpler policies as an example consider the following policy srcaddr srcaddr dstport switch it states that packets from sources in subnet should be forwarded to switch except for packets coming from or going to a destination on port the first challenge in compiling a high level language such as netcore to a low level sdn framework such as openflow arises from the relative lack of expressiveness in the switch packet matching primitives for instance because switches cannot express the difference of two patterns in a single rule this policy needs to be implemented using three rules installed in a particular prioritized order one that drops packets from another that drops all packets going to port and a final rule that forwards all remaining packets from to switch the following switch level classifier implements this policy we write these classifiers with the highest priority rule first switch level patterns are on the left actions are on the right and a colon separates the two srcaddr dstport srcaddr switch next consider a similar high level policy to the first real openflow switches locations are actually integers corresponding to physical ports on the switch in this paper we model them symbolically srcaddr srcaddr dstport switch we can generate a classifier for this policy in the same way srcaddr dstport srcaddr switch now suppose that we want to generate a classifier that implements the union of the two policies we cannot combine the classifiers in a simple way e g by concatenating or interleaving them because the rules interact with each other for example if we were to simply concatenate the two lists of rules the rule that drops packets to port would incorrectly shadow the forwarding rule for traffic from instead we need to perform a much more complicated translation that produces the following classifier srcaddr dstport srcaddr dstport switch srcaddr switch srcaddr dstport switch srcaddr switch switch srcaddr srcaddr dstport srcaddr switch dealing with these complexities often leads sdn programmers to use exact match rules i e rules that fully specify every bit in every single header field exact match rules for instance do not use wildcard patterns that match many values for a single header field such as nor do they leave certain header fields completely unconstrained our first implementation of frenetic used exact match rules exclusively because such rules were far easier for its run time system to reason about particularly when it came to composing multiple user policies this paper presents new general purpose algorithms for syn thesizing low level switch classifiers that use wildcard rules to the extent possible these new algorithms result in far more efficient system than the one we built in earlier work in frenetic origi nal exact match architecture many more packets wound up being sent to the controller suffering orders of magnitude increase in la tency and many more rules had to be sent to switches the results of our experiments presented in section highlight the magnitude of these differences netcore richer predicates and dynamic policies the policies presented in the previous section were relatively simple they did nothing besides match bits in header fields and forward packets ac cordingly such static policies can be expressed in frenetic sim ple policy language though they are not be implemented nearly as efficiently as in the netcore system however many applications demand dynamic policies whose forwarding behavior depends on complex functions of traffic history and other information and these richer policies cannot be implemented by simply analyzing bits in header fields as an example suppose we want to build a security application that implements in network authentication for the topology shown in figure the network n contains a collection of internal hosts n represents the upstream connection to the internet a is the server that handles authentication for hosts in n and all three el ements are connected to each other by the switch s informally we want the network to perform routing and access control accord ing to the following policy forward packets from unauthenticated hosts in n to a from authenticated hosts in n to their intended destination in n and from a and n back to n although not from n to a this policy can be described succinctly in netcore as follows inport network inspect ps auth network inport network inspect ps auth server a inport server a inport network network where ps inport server a auth  p any isaddr p  isaddr p p p srcaddr p dstaddr this policy uses an inspector predicate to classify traffic from n as authenticated or unauthenticated an inspector predicate inspect e f has two arguments a filter predicate e over the net work traffic history and an almost arbitrary boolean valued func tion f the filter predicate generates a controller state  which is a collection of traffic statistics represented abstractly as a multiset of switch packet pairs the switch being the place where the packet was processed the boolean valued function f receives the con troller state as one its arguments and may analyze it as part of its decision making process in the example above the filter predicate ps selects all traffic coming from the authentication server in this idealized example we will treat an entity sending a packet p as authenticated if the authentication server has ever sent it a packet at any point in the past the function auth takes three arguments the controller state  the switch that should be handling the packet and the packet p to which the policy applies here the auth function tests whether the srcaddr field of the packet p being processed is equal to the dstaddr of any other packet p in the filtered traffic history and because there is only one switch in this example auth ignores its argument in other words it tests whether the authentication server has sent a packet to that sender in the past if it has the inspector predicate is satisfied if not it is not satisfied the auth function performs these tests using the auxiliary functions any a built in function that tests whether a boolean function is true of any element of a multiset and isaddr a user defined function that tests whether one packet dstaddr is equal to another packet srcaddr this inspector is combined with the other set theoretic operators to implement the overall packet forwarding policy policies that use inspectors are easy to write because forwarding decisions can be expressed using arbitrary functional programs these programs can query past traffic history or look up facts they need such as authentication status in a database on the other hand these programs never have to manage the low level details of generating or installing switch level rules the run time system does that tedious error prone work for the programmer of course this expressiveness presents an extreme challenge for the compiler and run time system while it would be easy to evaluate the results of such policies by sending all packets to the controller doing so would be totally impractical we must find a way to implement the policy while processing the majority of traffic on switches our implementation strategy for such policies proceeds as fol lows first we compile the parts of the policy that do not involve inspectors as effectively as we can the system generates normal forwarding rules when it can and rules that send packets to the con troller otherwise next whenever a packet that cannot be handled by a switch arrives at the controller the run time system evaluates the packet against the current policy which includes inspectors producing a set of forwarding actions there are two possibilities the policy with respect to this packet and similar ones is invariant in other words every subsequent time the system evaluates the policy against this packet it will return the same set of forwarding actions the policy with respect to this packet and similar ones is volatile in other words the set of forwarding actions to be applied to this policy may change in the future in the first case the system can install rules on the switch that as sociate packets similar to the one just processed with the set of forwarding actions just computed because the set of computed ac tions will never change installing such rules on switches preserves the semantics of the policy in the second case the system can not install rules on the switch the next packet might be forwarded differently so the system will have to reevaluate it on the con troller in our example once a host has been authenticated it stays authenticated once the auth function evaluates to true it will con tinue to do so and is therefore invariant since inferring invariance automatically from an arbitrary program is a difficult problem we currently ask netcore programmers to supply invariance informa tion to the compiler in the form of an auxiliary hand written func tion in this simple case writing the invariance function is trivial it is true whenever auth is true  p auth  p to effectively generate rules even in the presence of inspector predicates the run time system must be able to determine when inspector returns the same results on one packet as it does on another i e it must be able to calculate the similar packets re ferred to above observe that an inspector always returns the same results on two different packets if those packets agree on all header fields that the inspector function examines conversely if the in spector does not examine a particular header field the value of that field does not affect its result hence when generating a policy after evaluating it against a single packet the run time can substi tute wildcards for all header fields that the policy does not inspect though it is likely possible to infer the set of headers any inspector function examines at least conservatively our current implemen tation assumes that programmers supply this information explicitly overall these techniques run time evaluation of policies against particular packets on the controller invariance and specification of header information collaborate to turn the diffi cult problem of evaluating policies containing arbitrary functions back in to the simpler problem of compiling static forwarding poli cies efficiently we call these techniques reactive specialization a core calculus for network programming this section defines the syntax and semantics of netcore a core calculus for high level network programming the calculus has two major components predicates which describe sets of packets and policies which specify where to forward those packets figure presents the syntax of these constructs as well as various network values such as headers and packets notation throughout this paper whenever we define syntax as in the grammar for packets we will use the grammar non terminal p as a metavariable ranging over the objects being defined the capitalized version of the non terminal p as a metavariable rang ing over sets or multisets of such objects and vector notation p for sequences of objects we describe finite sets using the notation x x k and combine sets using operations and union intersection complement and difference respectively typically we give defi nitions for intersection and complement and x  x  x figure netcore semantics values from headers processed h to by bitstrings programs b which we write we represent p h for as the a finite bitstring map associated with the header h in p we assume all fields have fixed finite length and therefore the set of complete packets is finite the controller state  accumulates information about packets that arrive at each switch we represent controller state as a multiset of switch packet pairs predicates informally predicates select sets of packets that are of interest in some forwarding policy formally a predicate e de notes a set of snapshots x comprising a controller state  a switch and a packet p located at the state component is essential for modeling predicates that depend upon historical traffic patterns such as the past load on particular links or packets sent and re ceived from various locations figure defines the semantics of predicates we say that a snapshot x matches a predicate e when it belongs to the denotation of e we sometimes say that a packet p matches e leaving the state and the switch implicit because they are irrelevant or uninteresting we also say that a bitstring b matches a wildcard w whenever the corresponding bits match for example and both match the wildcard basic predicates have the form h w a packet p matches h w if p h i e the h header of p matches w for example the predicate dstport matches all packets with dstport header field equal in binary another basic predicate switch matches all packets in any state sent to more complex predicates are built up from simpler ones using the intersection and complement operators additional building blocks such as true false e e or e e can be implemented as derived forms the most interesting component of the language is the inspector predicate inspect e f the first component of an inspector is a filter predicate e that selects switch packet pairs matching e from the current state creating a refined state  in other words e acts as a query over the network traffic history the second component f is an almost arbitrary boolean valued function over  and the switch packet pair and p in question the authentication example defined in the previous section used an inspector another example is inspect filterweb cond where filterweb dstport cond  p cardinality  p srcaddr here cardinality is a function that counts the number of ele ments in a multiset this inspector extracts all web traffic dstport is from the current state the inspector is satisfied if the total number web packets sent is less than or the packet p comes from a particular sender srcaddr is to make compilation tractable two additional pieces of infor mation are associated with inspector functions f the first piece of information comes from the sets of headers mentioned in its in dexed type state h switch packet h bool such a type restricts f to only examine headers h of packets in the state and headers h in the packet for instance the function cond above may be assigned a type wherein h is the empty set as summing packet counts requires looking at no headers and h is srcaddr as cond only examines the srcaddr field of its packet argument the second piece of information associated with f comes from its invariance oracle a function f is invariant on  p written invariant  p f if for all  we have f   p f  p intuitively a function is invariant on a state when its result does not change no matter what additional information is added to it again as an example thecond function above is invariant on all snapshots involving packets from as well as all snapshots where cardinality  once the total volume of web traffic has crossed the threshold the function always returns true in our implementation the programmer writes invariance oracles by hand as simple haskell functions together the header sets in the inspector types and the invari ance oracle allow the compiler to generate effective switch level rules even though the inspector function itself cannot be analyzed however the language of predicates does have one significant lim itation it depends upon permanent invariance of predicates there are predicates that are invariant for a long time and hence could have rules installed on switches for that time but are not perma nently invariant we believe our framework can be extended to han dle such semi permanent invariance properties having the compiler uninstall rules at the end of a time period or in response to a net work event but defer an investigation of this topic to future work figure reference machines policies policies  specify how packets should be forwarded through the network basic policies written e s say that pack ets matching e should be forwarded to the switches in s as with predicates we build complex policies by combining simple poli cies using intersection and negation figure defines the semantics of policies as a function from snapshots x to sets of switches s al though the policy language is syntactically simple it is remarkably expressive in particular inspectors are a powerful tool that can be used to express a wide range of behaviors including load balancing fine grained access control and many standard routing policies machines to understand how the network behaves over time we define two abstract machines both machines forward pack ets according to  but they differ in how often the switches syn chronize traffic statistics with the controller the synchronous ma chine defines an idealized implementation that at all times has perfect information about the traffic sent over the network of course it would be impractical to implement this machine in a real network because in general it would require sending ev ery packet to the controller if any packet were forwarded by a switch there would be a delay between when the packet was for warded and when the controller state was augmented with infor mation about that packet the asynchronous machine defines a looser more practical implementation like the first machine it is policy compliant it forwards packets according to the policy but it updates its state asynchronously instead of in lockstep with each packet processed hence it makes no guarantees about what it knows about the network traffic while the synchronous ma chine can be thought of as the best possible policy compliant ma chine the asynchronous machine can be thought of as the worst policy compliant machine any reasonable implementation will sit between the two in other words implementations should be policy compliant but users should not expect perfect synchrony the cost of implementing it would be prohibitive in practice synchroniza tion with switches typically happens at periodic timed intervals modulo variances in the latency of communication but for sim plicity we do not model time explicitly figure defines both reference machines they use the function forward s p which generates a multiset of transmissions forward s p t p s the state of the synchronous machine m sync includes the netcore policy  the state  and a multiset t of pending transmissions at each step the machine removes a transmission from t processes it using the policy updates the machine state and adds the new transmissions generated by the policy to the multiset of pending transmissions the state of the asynchronous machine m async includes the program  state and two multisets of transmissions t which represents transmissions waiting to be processed by the policy and t which represents transmissions that have been processed by the policy but have not yet been added to the state the first inference rule for the second machine takes a transmission from t processes it using the policy and places it in t the set of transmissions waiting to be incorporated into  the second rule takes a transmission from t and adds it to  the run time system in this section we discuss how to implement netcore semantics on a software defined network by giving an operational semantics to the netcore run time system and the underlying network de vices this operational semantics explains the basic interactions between the controller and the switches switch classifiers before we can present the run time system we need a concrete representation of the rules that switches use to process packets a classifier r is a sequence of rules r each containing a switch level pattern z and an action  while our high level semantics uses sets classifiers are represented as sequences to model rule priority within a classifier rules on the left have higher priority than rules on the right the pattern z component of a rule recognizes a set of packets and hence is similar to but less general than a predicate in net core we write p z when packet m matches pattern z we hold patterns abstract to model the variety of different matching capabil ities in today switches for example openflow switches support prefix pattern matching on source and destination ip addresses i e patterns like but only exact or full unconstrained matching on most other headers some switches support various other ex tended patterns such as ranges of the form n n an action  is either a set of switches s which forwards packets to each switch in the set or  which forwards packets to the controller most switches support other actions such as modifying header fields but for simplicity we only model forwarding given a packet p and a classifier r we match the packet against the classifier by finding the first rule whose pattern matches the packet we write r p z  for the matching judgment more formally we define classifier matching as follows note that it selects the highest priority leftmost matching rule p z p z i p z i z  z i  i z i  i z n  n p z i  i molecular machine we formalize the operational semantics of the run time system as a molecular machine in the style of the chemical abstract machine the machine components called molecules are given on the left side of figure for simplicity we assume that packets arriving at a switch may be processed in any order and do not model failures that cause packets to be dropped the molecule c   represents the controller machine run ning the netcore policy  in state  the molecule s r z represents switch with packet classifier r and local switch state z the switch state records the patterns of rules that have been used to match packets but not yet queried and processed by the controller real switches use integer counters as state for simplicity we rep resent these counters in unary using a multiset of patterns a trans mission molecule t p represents a packet p en route to switch pattern z switch action  s  rule r z  e switchhelp r p z  classifier r r r n s r z t p s r z h p switch state z z z n molecule m c   s r z t p e controller   p s forward s p t specialize  p  r c   s r z h p p c   p s r r z t h p machine m m m n observation o p e c ollect r p z  c   s r z z c   p s r z e step m o m m m o m m figure the run time system finally a help molecule h p represents a request issued by pattern only specifies part of a packet perhaps its ip address or switch to the controller for assistance in processing packet p vlan tag but not the packet itself hence the transfer operation the operational semantics of the molecular machine is defined must fabricate those parts of the packets that are not specified in by the inference rules on the right side of figure to lighten the the switch pattern and the system as a whole must be correct no notation in this figure we drop the multiset braces when writing matter how the under specified parts of a packet are fabricated a collection of molecules in other words we write m m this places an important constraint on the compiler if the rules instead of m m each operational rule may optionally be and their patterns are not specific enough then although one packet labelled with an observation o which records when transmissions may have matched a rule on a switch a completely different packet are processed we use observations in section where we establish may be fabricated and passed back to the controller consequently equivalences between the molecular machine and the reference the controller state will not model past network traffic sufficiently machines defined in the last section accurately and forwarding policies that depend upon past network the rules e switchprocess and e switchhelp model the traffic will not be implemented correctly work done by switches to process packets the former rule is a second subtle issue with the e collect rule is that the pat invoked when a packet matches a rule with a non controller action terns of higher priority rules partially overlap and take precedence a set of switches to forward to in this case the switch forwards over patterns from lower priority rules hence examining the pat the packet accordingly and records the rule pattern in its state the tern of a low priority rule in isolation does not provide sufficient latter rule is invoked when a packet matches a rule with a controller information to synthesize a packet that might have matched that action in this case the switch generates a help molecule rule one must take all of the rules of the classifier and their prior the rule e c ontroller models the work done by the con ity order in to account when synthesizing a packet that may have troller to process help molecules the controller interprets the matched a pattern the e collect rule does this through the use packet using its netcore policy generating new transmissions t of the full classifier matching judgement to be sent into the network and adds the packet to its state in ad finally note that the implementation does not actually fabricate dition the controller uses the netcore compiler to generate new all of these packets in practice the switch passes integer coun rules to process future similar packets on switches instead of on ters associated with patterns back to the controller still this non the controller the compiler is accessed through the call to the deterministic rule effectively captures a key correctness criterion specialize function which generates the new rules for the switch for the system the controller program cannot distinguish between in question we hold the definition of this function abstract for now any of the packets that might be synthesized by the e collect it is defined precisely in the next section rule and must be correct no matter which one is fabricated of the rule e collect models the work done by the controller course this is also where the compiler use of header information to transfer information about the packets that matched a particular comes in to play the fabricated packets are only different in fields switch level rule from the switch to the controller state more pre that inspector functions and other predicates do not analyze cisely it chooses a pattern z from a switch state and then uses the lookup judgement r p z  to synthesize a packet p that might have matched the corresponding rule in the switch classifier the the netcore algorithms pair of the packet and the switch are then stored in the controller state as a past transmission that might have occurred the netcore system performs two distinct tasks the interesting part of this transfer is that the controller stores classifier generation given a netcore policy construct a set full packets whereas switches only store sets of patterns and a of classifiers one for each switch in the network netcore classifier generation reactive specialization given a packet not handled by the current classifier installed on a switch generate additional rules that allow the switch to handle future packets with similar header fields without consulting the controller this section presents the key algorithms that implement these tasks parameters the netcore system is parameterized on several structures a lattice of switch patterns and two oracles that map primitive predicates onto switch level and wildcard patterns respectively abstracting some of the low level details of compilation makes it possible to execute netcore policies on many diverse kinds of hardware and even use switches with different capabilities in the same network formally we assume that switch patterns form a bounded lattice a pattern z sits lower than or equal to another pattern z writ ten z z when z matches a subset of the packets matched by z the element matches every packet and the element matches none abusing notation slightly we write p z to indicate that packet p matches pattern z to ensure that intersections are com piled correctly we require that meets be exact in the sense that p z z if and only if p z and p z the first oracle called the compilation oracle o maps primi tives h w into the pattern lattice in many cases the pattern gener ated by o h w will match the set of packets described by h w exactly but sometimes this is not possible for example open flow switches only support prefix wildcards for ip addresses so the best approximation of the non prefix pattern srcaddr is srcaddr we give the oracle some flexibility in selecting pat terns and only require it to satisfy two conditions it must return an overapproximation of the primitive and it must be mono tonic in the sense it translates semantically larger primitives to larger patterns formally the requirements on compilation oracles are as follows  p h w implies p o h w and h w h w implies o h w o h w the second oracle called the refinement oracle u takes a prim itive h w and a packet p as arguments and produces a pattern h w unlike the compilation oracle which overapproximates predicates the refinement oracle underapproximates predicates al lowing the compilation infrastructure to generate effective switch level rules for a subset of the pattern of interest while there is gen erally one best overapproximation there often exist many useful underapproximations in such cases we disambiguate by selecting the best underapproximation that matches p for example if we were to refine srcaddr which can t be compiled exactly on openflow with a packet with source address we would gen erate the underapproximation srcaddr yet if we refined the same predicate with a packet with source address we would instead generate srcaddr classifier generation ideally given a policy the netcore compiler would generate a classifier with the same semantics i e one that denotes the same function on packets but certain netcore features such as inspec tors and wildcard patterns when not supported by the underlying hardware cannot be implemented on switches so in general the generated classifier will only approximate the policy and certain packets will have to be processed on the controller the classifier generator works in two phases in the first phase it translates high level policies to an intermediate form containing switch level patterns and actions as well as precise semantic in formation about the policy being compiled in the second phase it builds a classifier by attaching actions to patterns using the seman tic information produced in the first phase to determine whether it is safe to attach forwarding actions to a pattern or whether the special controller action  must be used instead the grammars at the top of figure define the syntax for the intermediate forms used in classifier generation the intermediate form for predicates u z b h contains four values an ideal pattern u a switch pattern z a three valued boolean b and a set of headers h the ideal pattern u represents the pattern we would generate if the pattern lattice supported arbitrary wildcards ideal patterns are represented as a conjunction of header and wild card pairs we write for the unconstrained ideal pattern i e an empty conjunction the switch pattern z represents the actual pattern generated by the compiler which is an overapproximation of the ideal pattern in general the three valued boolean b indi cates whether packets matching the predicate should definitely be accepted true rejected false or whether there is insufficient information and a definitive answer must be made by the controller maybe to combine three valued booleans we extend the stan dard boolean operators as follows maybe false false maybe true maybe maybe maybe maybe maybe maybe the set of headers h keeps track of the header fields within pack ets in the controller state that inspector functions may examine this header information is used to ensure the compiler generates sufficiently fine grained switch rules so that when information is transferred from the switch to the controller using the e collect rule discussed in the previous section the information is precise enough to guarantee the correctness of the inspectors the intermediate form for policies u z s s h is similar to the form for predicates but instead of a three valued boolean it records lower and upper bounds s and s on the sets of switches to which a packet might be forwarded intuitively a proper forwarding rule can only be generated when we know exactly which switches to forward packets to i e when s and s are equal in other cases the compiler will generate a rule that sends packets to the controller predicate translation the heart of classifier generation is the function i e presented in figure which takes a predicate e and switch as arguments and produces a sequence of interme diate predicates that approximate e on one of the invariants of the algorithm is that it always generates a complete sequence i e intermediate predicates whose patterns collectively match ev ery packet in addition the algorithm attempts to produce a se quence whose patterns separate packets into two sets one with packets that match the predicate being compiled and another with those that do not however it does not always succeed in doing so for two fundamental reasons the algorithm cannot analyze the decisions made by inspectors as far as the analysis is con cerned inspectors are black boxes and certain primitive predi cates cannot be expressed precisely using switch patterns the in termediate predicates contain sufficient information for the com piler to reason about the precision of the rules it generates we write i returns a sequence e i u i z i b i h i to indicate that compiling e of intermediate predicates whose ith element is u i z i b i h i and i u i z i b i h i to denote the sequence of intermediate predicates out of compo nents u i z i b i h i indexed by i the first equation at the top of figure states that the compiler translates primitive predicates h w into two intermediate pred icates h w o h w true which contains the switch pattern produced by the compilation oracle and false which by using the pattern ensures that the sequence is com plete like classifiers these sequences should be interpreted as a prioritized series of rules hence the second quadruple only rejects things the first does not match the case for switch predicates switch has two possible out comes if the switch whose classifier is being compiled is the same as then the compiler generates an intermediate form that associates every packet with true otherwise the compiler gener ates an intermediate form that associates every packet with false intuitively the classifier generated for inspect e f must satisfy three conditions first it should approximate the semantics of f because the behavior of f is unknown at compile time the approx imation cannot be exact hence the intermediate predicates gener ated for the inspector should contain maybe indicating that match ing packets should be sent to the controller for processing second it should be structured so that it can identify packets matched by the traffic filter predicate e i e the packets that must be present in the controller state to evaluate f third it should also be suffi ciently fine grained to provide information about the set of headers h mentioned in the type of f which represent the headers of pack ets in the state that f examines hence the compiler recursively generates a sequence of intermediate predicates from e and then it erates through it replacing the three valued boolean b i with maybe and adding h to the set of headers h i in each u i b i z i h i to generate intermediate forms for an intersection e e the compiler combines each pair of intermediate predicates gener ated for e and e the resulting classifier captures intersection in the following sense if a packet form the form and with matches patternz z j performing this construction in the i z j matches z i in the first intermediate second intermediate form it in the result and likewise naively would result matches foru in a combinato i andu j rial blowup however it is often possible to exploit algebraic prop erties of patterns to reduce the size of the sequence in practice see the examples below and also section finally the case for negated predicates e iterates through the sequence generated by the compiler for e and negates the three valued boolean in each intermediate predicate predicate translation examples to illustrate some of the details of predicate compilation consider the translation of the inspector free predicate e e where e is h and e is h and h and h are distinct headers assume that switch patterns support wildcards such as on h the left and right sides of the intersection generate the following intermediate predicates i e h h true false i e h h false true note that the negation in e flips the parts of the intermediate forms designated as true i e it inverts the parts of the sequence that match and do not match the predicate next consider compilation of the intersection and note that we simplify the results slightly using identities such as z z and u u and b true b h h h h true h h false h h false false this classifier can then be simplified further as the last three rules overlap and are associated with the same three valued boolean h h h h true false now suppose instead that the switch only has limited support for wildcards and cannot representh in this case the compilation note that the compiler does not add the set h mentioned in the type of f to h i this set is used during specialization to determine similar packets oracle provides an overapproximation of the pattern say hence the intermediate predicate for e above would be as follows i e h true false for another example consider compiling a predicate that in cludes an inspector such as inspect h f h in this case h compiles similarly to the simple clauses above h h true false if the set of headers f examines on the state is h the inspector inspect h f compiles to the following h h maybe h maybe h note that the definitive booleans above have been replaced with maybe indicating that the controller will need to determine whether packets match the predicate however when we inter sect the results of compiling h with the results of compiling the inspector we obtain the following h h h h maybe h h h maybe h h h false h false h importantly even though the inspector is uncertain i e it has maybe in each intermediate predicate the result is not entirely un certain because b false is false even when b is maybe intersect ing inspectors with definitive predicates can resolve uncertainty likewise as b true is true compiling the union of a definitive clause with an inspector also eliminates uncertainty and although the calculus does not represent unions explicitly its encoding oper ates as expected a fact we exploit in reactive specialization policy translation the function i  which translates a pol icy into intermediate form is similar to the translation for pred icates figure gives the formal definition of the translation to translate a basic policy e s the compiler first generates a se quence from e and then attaches a pair of actions representing lower and upper bounds for each rule there are three cases if the three valued boolean b i is true it uses s as both the upper and lower bounds if b i is false it uses as the bounds if b i is maybe it uses as the lower bound and s as the upper bound which rep resents the range of possible actions the translations of intersected and negated policies are analogous to the cases for predicates classifier construction the second phase of classifier genera tion analyzes the intermediate form of the policy and produces a bona fide switch classifier the c  function that implements this phase is defined in figure it first uses i  to gener ate a sequence of intermediate policies and then analyzes each u i s s z i h i to generate a rule there are two possible outcomes for each intermediate policy in the sequence first if the boundss ands are tight z i is sufficiently fine grained to collect information about all headers in h i and we get the same switch bounds s s regardless of whether we match pack ets using the ideal primitives or the switch level patterns then it is safe for the compiler to throw away the high level semantic in formation bounds and ideal primitives and emit an effective rule z i s otherwise it generates a rule with the controller action  the formal conditions needed for this analysis are captured by consistent  i and headers z the predicate consistent  i is satisfied if looking up an arbitrary packet matching the ith switch pattern yields the same switch bounds as looking it up using the ideal pattern where we extend classifier lookup to sequences of in termediate policies in the obvious way the function headers z calculates the set of headers constrained fully by z formal properties for a classifier to be sound it must satisfy two properties it must forward packets according to the policy and its rules must encode enough information about the packets that match them to implement inspectors the correctness of classifier generation is captured in the following definition and lemma definition classifier soundness a classifier r is sound on switch with respect to  if the following two criteria hold routing soundness for all snapshots  p if r p s then   p s and collection and r p soundness z s then for for all all snapshots packets p  p if r p z s   p and p p   p p lemma classifier generation soundness classifier c  is sound on switch with respect to  intuitively routing soundness ensures that the actions computed by looking up rules in the classifier r are consistent with  formally the condition states that if looking up a packet p in r on switch produces a set of switches s then evaluating  on snapshots containing and p also produces s note that this condition does not impose any requirements if looking up p in r yields  as the packet will be sent to the controller which will evaluate  on p directly collection soundness ensures that the rules in r are sufficiently fine grained so that when the controller collects traffic statistics from switches the rule patterns contain enough information to implement the policy inspectors this is seen in the e collect rule in the molecular machine figure which fabricates packets that match the rule being collected collection soundness ensures that fabricated packets are correct formally it requires that  behave the same on all snapshots in which the state  has been extended with arbitrary packets p and p matching a given rule z s lemma states that the classifiers generated by the netcore compiler are sound reactive specialization the algorithm described in the preceding section generates classi fiers that can be installed on switches but as we saw it has some substantial limitations dynamic policies that use inspectors cannot be analyzed and even for purely static policies if the switch has poor support for wildcards the classifier needed to implement the policy may be large much larger than would be practical to gener ate to deal with these situations we define reactive specialization a powerful generalization of the simple reactive strategy imple mented manually by openflow programmers we define reactive specialization using two operations program refinement which ex pands the policy relative to a new snapshot witnessed at the con troller and pruning which extracts new effective rules from the classifier generated from the expanded policy program refinement when the controller receives a new packet that a switch could not handle it interprets the policy with respect to the packet switch and its current state the idea in program refinement is to augment the program with additional information gleaned from this packet that can be used to build a specialized classifier that handles similar packets on the switch in the future figure defines the refinement function the key invariant of this program transformation is that the semantics of the old and new policies are identical however syntactically the new program will typically have a different structure as the transformation uses the packet to unfold primitives and inspectors this makes compilation more precise and the recompiled program more effective the rules for refining a predicate appear at the top of figure the first rule uses the refinement oracle u to refine basic predi cates unlike the compilation oracle which may overapproximate r x e e r  p h w h w u h w p r x switch switch r  p inspect e f inspect inspect inspect e e e f f f e e if invariant x f f x if invariant x f f x if invariant x f where f state h switch packet h bool and x  p and e r x e r x e similar p h and e similar p h r x e e r x e r x e r x e r x e r x   r x e s r x e s r x   r x  r x  r x  r x  specialize x  r specialize  p  prune r p where r c r  p  figure netcore refinement the predicate the refinement oracle underapproximates it so that the rest of the compilation infrastructure will be able to generate an effective switch level rule that matches the given packet be cause the new predicate is the union of the old predicate and an underapproximation the overall semantics is unchanged in some cases especially if the switch supported patterns are weak the best underapproximation the refinement oracle can generate is an exact match predicate in many other cases however if the switch sup ports prefix matching or wildcards the refinement oracle will pro duce a predicate that matches many more packets the second rule refines switch predicates switch because the switch predicate already reveals the maximum amount of informa tion it cannot be refined further the rule for inspectors is the most interesting it uses a simi larity predicate that describes the set of packets sent to the same switch that agree on a set of headers h similar p h switch h h h p h we first refine the traffic filter predicate e to add additional struc ture for traffic collection to ensure that the refined classifier has sufficiently fine grained rules to collect the packets in the controller state examined by f we form the union of the refined traffic filter and the similarity predicate similar p h restricted to the re fined traffic filter next we add additional information about the inspector decision on the packet p to the policy recall that if f is invariant with respect to a snapshot x which includes the controller state switch and packet then it will return the same decision on all similar packets in the future in the first case if the inspector is in variant and evaluates to true on the current snapshot  p then we refine it by taking the union of the inspector and the similarity predicate similar p h the second case is similar except that the inspector does not evaluate to true and hence we refine the in spector by subtracting the similarity predicate finally in the third case the inspector is not invariant so no sound refinement exists the decision returned by the inspector may change in the future if the controller state changes hence packets must continue being diverted to the controller until the inspector becomes invariant the rules for refining intersection e e and negation e predicates and policies are all straightforward pruning in general after a policy has been refined and recom piled some of the new rules will be useless they will not pro cess additional packets on the switch we prune away these useless rules using a function prune r p that removes rules from r that send packets to the controller adding such rules does not im prove the efficiency of the switch have nothing to do with the packet p meaning they are irrelevant to specialization with respect to p or overlap with a rule we removed earlier to preserve the semantics of the rules putting it all together we define reactive specialization the function specialize at the bottom of figure by composing refinement recompilation and pruning to generate a specialized classifier from a snapshot x and policy  formal properties we first establish that specialization and therefore reactive rule generation is sound lemma specialization soundness if r is sound on switch with respect to  and r specialize  p  then r r is sound on with respect to  to establish the other properties we need a way of characterizing the packets that go to the controller we define the controller set of a classifier r as follows  r p r p  the second property we establish is that refinement is monotonic that is if we append reactive rules to a switch classifier the resulting classifier does not send more packets to the controller that the original one formally lemma specialization monotonicity for all policies  and classifiers r and r such that r specialize  p  we have  r r  r the final property we establish is that under certain assump tions appending reactive rules to a classifier results in strictly fewer packets going to the controller to make such a guarantee we need two conditions first the policy  must be realizable intuitively it must only use features that can be implemented on switches e g on an openflow switch the policy must not match on payloads definition realizable a policy is realizable if for every sub term h w of  and p h w we have  p u h w p if and only if p o u h w p realizability states that compiling an underapproximation of a high level predicate with respect to a packet matching the predicate yields a switch level rule that exactly corresponds to the predicate second all inspectors in the policy must be determinate we for malize this by extending the notion of invariance to full invariance definition fully invariant a policy  is fully invariant on  if for every subterm of  of the form inspect e f and we have invariant  p f for all switches and packets p for policies satisfying these conditions we can guarantee that the packet used to refine and recompile the policy will never be sent to the controller again lemma specialization progress if  is realizable and fully invariant on  and r specialize  p  then for any classifier r we have p  r r system wide correctness properties this section uses the tools developed in the previous section to de liver our two central theoretical results a proof of functional correctness for netcore and a proof of quiescence another fundamental theorem which establishes that when inspectors are invariant the network eventually reaches a state in which all pro cessing occurs efficiently on its switches functional correctness recall in section we defined two ide alized reference machines the synchronous reference machine which at all times knows and has recorded information about every packet processed in the network and the asynchronous ref erence machine which nondeterministically learns about packets processed in the network to demonstrate the correctness of the netcore compiler we show that it inhabits the space between the asynchronous and synchronous reference machines more for mally we prove that the asynchronous reference machine simulates the netcore molecular machine and the molecular machine simu lates the synchronous reference machine given a set of switches s and a policy  we initialize the molecular machine as follows init s  c  s c  s the next theorem establishes the relationship between the reference and molecular machines theorem functional correctness given a set of switchess an initial set of transmissions t such that t p t implies s and a molecular machine m init s  t we have the asynchronous machine  t weakly simulates m m weakly simulates the synchronous machine  t proof sketch we describe the first simulation only the second is similar the simulation relation between the asynchronous machine and the molecular machine satisfies the following each switch classifier on the molecular machine is sound with respect to  there exists an observation preserving bijection between pending transmissions in the asynchronous machine and transmissions and help molecules in the molecular machine and there exists an observation preserving bijection between the processed transmis sions in the asynchronous machine and the switch states in the molecular machine the initial state satisfies these criteria by clas sifier generation soundness now consider taking a transition if it forwards a packet the first bijection is preserved by routing cor rectness if it collects a pattern the second bijection is preserved by collection soundness finally if it generates reactive rules they are sound by specialization soundness quiescence the quiescence theorem demonstrates that the net core compiler effectively moves work off of the controller and onto switches even when the program is expressed in terms of patterns the switch cannot implement precisely and inspector functions the compiler cannot analyze formally quiescence states that if all of the inspectors in the program are invariant then the netcore com piler will eventually install rules on switches that handle all future traffic i e eventually the system can reach a configuration where no additional packets need to be sent to the controller before we can state the quiescence theorem precisely we need a few definitions and supporting lemmas first we say that m is derived from policy  if init s  t m where is the reflexive transitive closure of the single step judgement ignoring observations second we lift the notion of full invariance to machines m m is fully invariant if the controller policy is fully invariant with respect to the controller state we also lift the notion of controller set on classifiers to machines m  m p s r z m and p  r the first lemma controller set monotonicity states that the set of packets that require processing on the controller never increases lemma controller set monotonicity if m is derived from  and m o m then  m  m proof follows from specialization monotonicity now we are ready to prove the key lemma needed for quies cence the controller set progress lemma states that if the con troller program is realizable and has become fully invariant then every time the controller processes a help molecule the controller set becomes strictly smaller in other words every help molecule contains enough information and the compiler is powerful enough to exploit it for the e controller rule to generate useful new classifier rules lemma controller set progress for every realizable policy  and fully invariant m derived from  if m o m is an instance of e controller then  m  m proof follows from specialization progress quiescence follows from these lemmas as the total number of possible packets is finite the precise statement of quiescence says that the run time system may as opposed to does quiesce because the machine may non deterministically choose to continue forwarding packets using the switches instead of processing the remaining help molecules formally a machine configuration m may quiesce if there exists a configuration m such that m m and the rule e controller is not used in any derivation in the operational semantics starting from m with this definition in hand we can state quiescence theorem quiescence for every realizable policy  and fully invariant m derived from  we have that m may quiesce implementation and evaluation we have implemented a prototype netcore compiler in haskell us ing the ideas presented in this paper the core algorithms are for mulated in terms of abstract type classes e g lattices for switch patterns and oracles this makes it easy to instantiate the com piler for different switch architectures one simply has to define instances for a few type classes and provide the appropriate oracles we have built two back ends both targeting openflow switches the first generates coarse grained wildcard rules the other back end used for comparison generates the kind of exact match rules used in our earlier work on frenetic and most hand written nox applications optimizations the implementation uses a number of heuristic optimizations to avoid the combinatorial blowup that would result from compiling classifiers naively for example it applies algebraic rewritings on the fly to remove useless patterns and rules and re duce the size of the intermediate patterns and classifiers it needs to manipulate the compilation algorithms identify and remove pat terns completely shadowed by other patterns and patterns whose spqe e i m h c t i w s spe ipe e e i m h c t i w s i m h c t i w s full compiler flow compiler spe spqe ipe full compiler switch misses classifier size flow compiler switch misses classifier size 7k figure experimental results effect is covered by a larger pattern with lower priority but the same actions although these heuristics are simple they go a long way toward ensuring reasonable performance in our experience evaluation to evaluate our implementation we built an instru mented version of the run time system that collects statistics about the sizes of the classifiers generated by the compiler and the amount of traffic handled on switches as opposed to the controller be cause space for classifiers is a limited resource on switches and because the cost of diverting a packet to the controller slows down its processing by orders of magnitude these metrics quantify some of the most critical performance aspects of the system we compared the performance of the full which makes use of all openflow rules including wildcards and flow which only generates exact match rules also known as microflow rules compilers on the following programs static policy experiment spe implements the simple static policy described at the beginning of section this benchmark measures the in efficiency of compilation strategies based on generating exact match rules static policy with query experiment spqe forwards packets using the same policy as in spe but also collects traf fic statistics for each host due to this collection this program cannot be directly compiled to a switch classifier at least not without expanding all billion possible hosts thus this benchmark measures the efficiency of reactive specialization inspector policy experiment ipe forwards packets and col lects traffic statistics using the authentication application pre sented in section this benchmark measures the performance of a more realistic application implemented using inspectors to drive these experiments we generated packets using fs a tool that synthesizes realistic packet traces from several statistical parameters we ran each experiment on packets in total for the spe and spqe benchmarks we generated traffic with ac tive hosts sending packets to an external network for seconds each for the ipe benchmark we generated traffic with hosts a class c network sending traffic to the authentication server and an external network for seconds each the results of the exper iments are shown in figure the graphs on the top row show the number of packets that missed and had to be sent to the controller against the total number of packets processed likewise the graphs on the bottom row show the size of the compiled classifier in terms of number of rules versus total packets the table at the right gives the final results after all packets were processed in terms of the proportion of packets processed on switches the full openflow compiler outperforms the microflow based com packets packets packets e z i s r e fi i a l c e e z i s r z i s r 50k 25k 50k 75k packets e fi i a l c e fi i a l c 5k packets packets piler on nearly all of the benchmarks on the spe benchmark the full compiler generates a classifier that completely handles the policy so no packets are sent to the controller the line for the full compiler overlaps with the x axis the microflow compiler of course diverts a packet to the controller for each distinct mi croflow generating 7k rules in total on the spqe benchmark the full compiler generates wildcard rules using reactive special ization that handle all future traffic from each unique host after seeing a packet from it these rules handle many more packets than the exact match rule produced by the microflow compiler on this benchmark it is worth noting that the classifiers produced by the full compiler are larger than the ones produced by the microflow compiler especially initially this is due to the fact that the full compiler generates multiple rules in response to a single controller packet attempting to cover a broad space of future similar pack ets whereas the microflow compiler predictably generates a single microflow for each controller packet one can see that the work done by the full compiler pays off in terms of the number of pack ets that must be diverted to the controller moreover over time the size of the microflow compiler generated classifier approaches that of the full compiler lastly the ipe benchmark demonstrates that the full compiler generates more effective classifiers than the mi croflow compiler even in the presence of inspector functions that it cannot analyze directly note that a large number of packets must be diverted to the controller in any correct implementation until they authenticate the inspector is not invariant for any host how ever the full compiler quickly converges to a classifier that pro cesses all traffic directly on the switch related work building on ideas first proposed in ethane and nox was the first concrete system to popularize what is cur rently known as software defined networking it provides an event driven interface to openflow and requires that programmers write reactive programs using callbacks and explicit switch level packet processing rules there are numerous examples of network applications built on top of nox using microflows but relatively few that use wildcard rules though wang load bal ancer is a nice example of the latter networking researchers are now actively developing next generation controller platforms some of them such as beacon designed for java and nettle designed for haskell pro vide elegant openflow interfaces for new programming languages others such as onix and maestro improve scalability and fault tolerance through parallelization and distribution none of these systems automatically generate reactive protocols or provide formal semantics or correctness guarantees like netcore does both netcore and ndlog use high level languages to pro gram networking infrastructure but the similarities end there nd log programs are written in an explicitly distributed style whereas high level netcore programs are written as if the program has an omniscient centralized view of the entire network the netcore implementation automatically partitions work onto a distributed set of switches and synthesizes a reactive communication protocol that simulates the semantics of the high level language part of the job of the netcore compiler is to generate efficient packet classifiers most previous research in this area see tay lor for a survey focuses on static compilation the netcore compiler generates classifiers in the face of non static policies with unknown inspector functions and synthesizes a distributed switch controller implementation bro snortran shangri la and fpl compile rich packet filtering and monitoring pro grams designed to secure networks and detect intrusions down to special packet processing hardware and fpgas the main differ ence between netcore and all of these systems is that they are lim ited to a single device they do not address the issue of how to pro gram complex dynamic policies for a collection of interconnected switches and they do not synthesize the distributed communication patterns between the switches and controller active networking as in the switchware project shares many high level goals with software defined networking but the implementation strategy is entirely different the former uses smart switches to interpret programs encapsulated in packets while the latter uses dumb switches controlled by a remote host acknowledgments we wish to thank ken birman mike freedman sharad malik jen rexford cole schlesinger and alec story and the anonymous popl reviewers for discussions about this research and comments on drafts of this paper our work is supported in part by onr grants 0770 and 0652 and nsf grants cns and cns any opinions findings and recom mendations are those of the authors and do not necessarily reflect the views of the onr or nsf abstract image processing pipelines combine the challenges of stencil com putations and stream programs they are composed of large graphs of different stencil stages as well as complex reductions and stages with global or data dependent access patterns because of their complex structure the performance difference between a naive im plementation of a pipeline and an optimized one is often an order of magnitude efficient implementations require optimization of both parallelism and locality but due to the nature of stencils there is a fundamental tension between parallelism locality and introducing redundant recomputation of shared values we present a systematic model of the tradeoff space fundamen tal to stencil pipelines a schedule representation which describes concrete points in this space for each stage in an image processing pipeline and an optimizing compiler for the halide image process ing language that synthesizes high performance implementations from a halide algorithm and a schedule combining this compiler with stochastic search over the space of schedules enables terse composable programs to achieve state of the art performance on a wide range of real image processing pipelines and across different hardware architectures including multicores with simd and hetero geneous cpu gpu execution from simple halide programs writ ten in a few hours we demonstrate performance up to faster than hand tuned c intrinsics and cuda implementations optimized by experts over weeks or months for image processing applications beyond the reach of past automatic compilers categories and subject descriptors d programming lan guages processors compilers optimization code generation keywords domain specific language compiler image processing locality parallelism redundant computation optimization gpu vectorization introduction image processing pipelines are everywhere and are essential to capturing analyzing mining and rendering the rivers of visual information gathered by our countless cameras and imaging based sensors applications from raw processing to object detection and recognition to microsoft kinect to instagram and photoshop to permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee medical imaging and neural scanning all demand extremely high performance to cope with the rapidly rising resolution and frame rate of image sensors and the increasing complexity of algorithms at the same time the shrinking cameras and mobile devices on which they run require extremely high efficiency to last more than a few minutes on battery power while power hungry radios and video codecs can implement slow changing standards in custom hardware image processing pipelines are rapidly evolving and diverse requiring high performance software implementations image processing pipelines combine the challenges of stencil computation and stream programs they are composed of large graphs of many different operations most of which are stencil computations these pipelines are simultaneously wide and deep each stage exhibits data parallelism across the many pixels which it must process and whole pipelines consist of long sequences of different operations which individually have low arithmetic intensity the ratio of computation performed to data read from prior stages and written to later stages for example an implementation of one recent algorithm local laplacian filters is a graph of different stages fig including many different stencils and a large data dependent resampling as a result of this structure the performance difference between a naive implementation of a given pipeline and a highly optimized one is frequently an order of magnitude or more with current tools often the only way to approach peak performance is by hand writing parallel vectorized tiled and globally fused code in low level c cuda intrinsics and assembly simple pipelines become hun dreds of lines of intricately interleaved code complex pipelines like adobe camera raw engine become hundreds of thousands tun ing them requires immense effort by expert programmers and the end result is not portable to different architectures nor composable with other algorithms without sacrificing much of this painstakingly earned performance libraries of optimized subroutines do not solve the problem either since many critical optimizations involve fusion for producer consumer locality across stages we address this challenge by raising the level of abstraction and decoupling the algorithm definition from its execution strategy to improve portability and composability while automating the search for optimized mappings of the resulting pipelines to parallel ma chines and complex memory hierarchies effective abstraction and automatic optimization enable radically simpler programs to achieve higher performance than hand tuned expert implementations while running across a wide range of architectures image processing pipelines stencils have been well studied in scientific applications in the form of iterated stencil computations where one or a few small stencils pldi june seattle wa usa are applied to the same grid over many iterations in copyright c acm contrast we are interested in other applications in image processing level size w h w h w h figure imaging pipelines employ large numbers of interconnected heterogeneous stages here we show the structure of the local laplacian filter which is used for a variety of tasks in photographic post production each box represents intermediate data and each arrow represents one or more functions that define that data the pipeline includes horizontal and vertical stencils resampling data dependent gathers and simple pointwise functions and computer graphics where stencils are common but often in a very different form stencil pipelines stencil pipelines are graphs of different stencil computations iteration of the same stencil occurs but it is the exception not the rule most stages apply their stencil only once before passing data to the next stage which performs different data parallel computation over a different stencil graph structured programs have been studied in the context of streaming languages static communication analy sis allows stream compilers to simultaneously optimize for data parallelism and producer consumer locality by interleaving compu tation and communication between kernels however most stream compilation research has focussed on streams where sliding win dow communication allows stencil patterns image processing pipelines can be thought of as programs on and streams and stencils the model of computation required by image processing is also more general than stencils alone while most stages are point or stencil operations over the results of prior stages some stages gather from arbitrary data dependent addresses while others scatter to arbitrary addresses to compute operations like histograms pipelines of simple map operations can be optimized by tradi tional loop fusion merging multiple successive operations on each point into a single compound operation improves arithmetic intensity by maximizing producer consumer locality keeping intermediate data values in fast local memory caches or registers as it flows through the pipeline but traditional loop fusion does not apply to stencil operations where neighboring points in a consumer stage depend on overlapping regions of a producer stage instead sten cils require a complex tradeoff between producer consumer locality synchronization and redundant computation because this tradeoff is made by interleaving the order of allocation execution and com munication of each stage we call it the pipeline schedule these tradeoffs exist in scheduling individual iterated stencil computations in scientific applications and the complexity of the choice space is reflected by the many different tiling and scheduling strategies introduced in past work in image processing pipelines this tradeoff must be made for each producer consumer relationship between stages in the graph often dozens or hundreds and the ideal schedule depends on the global interaction among every stage often requiring the composition of many different strategies contributions halide is an open source domain specific language for the complex image processing pipelines found in modern computational pho tography and vision applications in this paper we present the optimizing compiler for this language we introduce a systematic model of the tradeoffs between locality parallelism and redundant recomputation in stencil pipelines a scheduling representation that spans this space of choices a dsl compiler based on this representation that combines halide programs and schedule descriptions to synthesize points anywhere in this space using a design where the choices for how to execute a program are separated not just from the definition of what to compute but are pulled all the way outside the black box of the compiler a loop synthesizer for data parallel pipelines based on simple interval analysis which is simpler and less expressive than polyhedral model but more general in the class of expressions it can analyze a code generator that produces high quality vector code for image processing pipelines using machinery much simpler than the polyhedral model and an autotuner that can infer high performance schedules up to faster than hand optimized programs written by experts for complex image processing pipelines using stochastic search our scheduling representation composably models a range of tradeoffs between locality parallelism and avoiding redundant work it can naturally express most prior stencil optimizations as well as hierarchical combinations of them unlike prior stencil code generation systems it does not describe just a single stencil scheduling strategy but separately treats every producer consumer edge in a graph of stencil and other image processing computations our split representation which separates schedules from the underlying algorithm combined with the inside out design of our compiler allows our compiler to automatically search for the best schedule the space of possible schedules is enormous with hundreds of inter dependent dimensions it is too high dimensional for the polyhedral optimization or exhaustive parameter search employed by existing stencil compilers and autotuners however we show that it is possible to discover high quality schedules using stochastic search given a schedule our compiler automatically synthesizes high quality parallel vector code for and arm cpus with sse avx and neon and graphs of cuda kernels interwoven with host management code for hybrid gpu execution it automatically infers all internal allocations and a complete loop nest using simple but general interval analysis directly mapping data parallel dimensions to simd execution including careful treatment of strided access patterns enables high quality vector code generation without requiring any general purpose loop auto vectorization copy copy sub add up the algorithm uses pyramid levels dda copy copy up up upsample dda y down up down up add addition t t o t t x i x y o x y i x y i x y sub add sub subtraction down down o x y i down downsample x y i x y t t o x y lut dda i t x t y lut look up table o x y k lut i x y k dda data dependent access k  o x y floor i i x y  x y  i x y k  k  i x y k the end result is a system which enables terse composable programs to achieve state of the art performance on a wide range of real image processing pipelines and across different hardware architectures including multicores with simd and heterogeneous cpu gpu execution from simple halide programs written in a few hours we demonstrate performance up to faster than hand tuned c intrinsics and cuda implementations written by experts over weeks or months for image processing applications beyond the reach of past automatic compilers the halide dsl we use the halide dsl to describe image processing pipelines in a simple functional style a simple c implementation of local laplacian filters fig is described by dozens of loop nests and hundreds of lines of code this is not practical to globally optimize with traditional loop optimization systems the halide version distills this into lines describing just the essential dataflow and computation in the stage pipeline and all choices for how the program should be synthesized are described separately sec in halide values that would be mutable arrays in an impera tive language are instead functions from coordinates to values it represents images as pure functions defined over an infinite inte ger domain where the value of a function at a point represents the color of the corresponding pixel pipelines are specified as chains of functions functions may either be simple expressions in their arguments or reductions over a bounded domain the expressions that define functions are side effect free and are much like those in any simple functional language including arithmetic and logical operations loads from external images if then else expressions references to named values which may be function arguments or expressions defined by a functional let construct calls to other functions including external c abi functions for example a separable unnormalized box filter is expressed as a chain of two functions in x y uniformimage in uint var x y func blurx x y in x y in x y in x y func out x y blurx x y blurx x y blurx x y this representation is simpler than most functional languages it does not include higher order functions dynamic recursion or additional data structures like lists functions simply map from integer coordinates to a scalar result constrained versions of more advanced features such as higher order functions are added as syntactic sugar but do not change the underlying representation this representation is sufficient to describe a wide range of image processing algorithms and these constraints enable flexible analysis and transformation of algorithms during compilation critically this representation is naturally data parallel within the domain of each function also since functions are defined over an infinite domain boundary conditions can be handled safely and efficiently by computing arbitrary guard bands of extra values as needed guard bands are a common pattern in image processing code both for performance concerns like alignment and for safety wherever specific boundary conditions matter to the meaning of an algorithm the function may define its own reduction functions in order to express operations like his tograms and general convolutions halide also needs a way to ex press iterative or recursive computations like summation histogram and scan reductions are defined in two parts an initial value function which specifies a value at each point in the output domain a recursive reduction function which redefines the value at points given by an output coordinate expression in terms of prior values of the function unlike a pure function the meaning of a reduction depends on the order in which the reduction function is applied the programmer specifies the order by defining a reduction domain bounded by minimum and maximum expressions for each dimension the value at each point in the output domain is defined by the final value of the reduction function at that point after recursing in lexicographic order across the reduction domain this pattern can describe a range of algorithms outside the scope of traditional stencil computation but essential to image processig pipelines in a way that bounds side effects for example histogram equalization combines multiple reductions and a data dependent gather a scattering reduction computes a histogram a recursive scan integrates it into a cdf and a point wise operation remaps the input using the cdf uniformimage in uint rdom r in width in height ri 255 var x y i func histogram i histogram in r x r y func cdf i cdf ri cdf ri histogram ri func out x y cdf in x y the iteration bounds for the reduction and scan are expressed by the programmer using explicit reduction domains rdoms scheduling image processing pipelines halide representation of image processing algorithms avoids imposing constraints on the order of execution and placement of data values need to be computed before they can be used to respect the fundamental dependencies in the algorithm but many choices remain unspecified when and where should the value at each coordinate in each function be computed where should they be stored how long are values cached and communicated across multiple consumers and when are they independently recomputed by each these choices can not change the meaning or results of the algo rithm but they are essential to the performance of the resulting implementation we call a specific set of choices for when and where values are computed the pipeline schedule in the presence of stencil access patterns these choices are bound by a fundamental tension between producer consumer locality parallelism and redundant recomputation of shared values to understand this tradeoff space it is useful to look at an example motivation scheduling a two stage pipeline consider the simple two stage blur algorithm which computes a box filter as two passes the first stage blurx computes a horizontal blur of the input by averaging over a window blurx x y in x y in x y in x y the second stage out computes the final isotropic blur by averaging a window of the output from the first stage out x y blurx x y blurx x y blurx x y a natural way to think about scheduling the pipeline is from the perspective of the output stage how should it compute its input there are three obvious choices for this pipeline high coarse interleaving low locality low locality parallelism breadth first compute granularity more parallelism less synchronization overlapping tiles sliding windows valid within tiles schedules fine interleaving high locality loop fusion sliding window fine storage granularity storage coarse storage granularity redundant computation granularity no redundant computation figure a natural way to visualize the space of scheduling choices is by granularity of storage x axis and granularity of computation y axis breadth first execution does coarse grain computation into coarse grain storage total fusion performs fine grain computation into fine grain storage small temporary buffers sliding window strategies allocate enough space for the entire intermediate stage but compute it in in fine grain chunks as late as possible these extremes each have their pitfalls breadth first execution has poor locality total fusion often does redundant work and using sliding windows to avoid redundant recomputation constrains parallelism by introducing dependencies across loop iterations the best strategies tend to be mixed and lie somewhere in the middle of the space first it could compute and store every required point in blurx before evaluating any points in out applied to a megapixel image this is equivalent to the loop nest alloc blurx for each y in 2048 for each x in blurx y x in y x in y x in y x alloc out for each y in for each x in out y x blurx y x blurx y x blurx y x this is the most common strategy in hand written pipelines and what results from composing library routines together each stage executes breadth first across its input before passing its entire output to the next stage there is abundant parallelism since all the required points in each stage can be computed and stored independently but there is little producer consumer locality since all the values of blurx must be computed and stored before the first one is used by out at the other extreme the out stage could compute each point in blurx immediately before the point which uses it this opens up a further choice should points in blurx which are used by multiple points in out be stored and reused or recomputed independently by each consumer interleaving the two stages without storing the intermediate results across uses is equivalent to the loop nest alloc out for each y in for each x in alloc blurx for each i in blurx i in y i x in y i x in y i x out y x blurx blurx blurx each pixel can be computed independently providing the same abundant data parallelism from the breadth first strategy the dis tance from producer to consumer is small maximizing locality but because shared values in blurx are not reused across iterations this strategy performs redundant work this can be seen as the result of strategy span iterations max reuse dist ops work ampl breadth first 2048 full fusion sliding window tiled sliding in tiles 2048 figure different points in the choice space in figure each make different trade offs between locality redundant recomputation and parallelism here we quantify these effects for our two stage blur pipeline the span measures the degree of parallelism available by counting how many threads or simd lanes could be kept busy doing useful work the max reuse distance measures locality by counting the maximum number of operations that can occur between computing a value and reading it back work amplification measures redundant work by comparing the number of arithmetic operations done to the breadth first case each of the first three strategies represent an extreme point of the choice space and is weak in one regard the fastest schedules are mixed strategies such as the tiled ones in the last two rows applying classical loop fusion through a stencil dependence pattern the body of the first loop is moved into the second loop but its work is amplified by the size of the stencil the two stages can also be interleaved while storing the values of blurx across uses alloc out 3072 alloc blurx 3072 for each y in 2047 for each x in 3072 blurx y x in y x in y x in y x if y continue out y x blurx y x blurx y x blurx y x this interleaves the computation over a sliding window with out trailing blurx by the stencil radius one scanline it wastes no work computing each point in blurx exactly once and the maximum distance between a value being produced in blurx and consumed in out is proportional to the stencil height three scanlines not the entire image but to achieve this it has introduced a dependence between the loop iterations a given iteration of out depends on the last three outer loop iterations of blurx this only works if these loops are evaluated sequentially interleaving the stages while producing each value only once requires tightly synchronizing the order of computation sacrificing parallelism each of these strategies has a major pitfall lost locality redun dant work or limited parallelism fig in practice the right choice for a given pipeline is almost always somewhere in between these extremes for our two stage example a better balance can be struck by interleaving the computation of blurx and out at the level of tiles alloc out 3072 for each ty in 2048 for each tx in 3072 alloc blurx for y in 33 for x in blurx y x in ty y tx x in ty y tx x in ty y tx x for y in for x in out ty y tx x blurx y x blurx y x blurx y x this trades off a small amount of redundant computation on tile boundaries for much greater producer consumer locality while still out in blurx out in blurx out in blurx out in blurx breadth first each function is total fusion values are computed sliding window values are tiles overlapping regions are entirely evaluated before the next on the fly each time that they are computed when needed then processed in parallel functions one needed stored until not useful anymore are evaluated one after another in blurx out serial y serial x serial x serial y serial y vectorized x 33 33 24 sliding windows within tiles tiles are evaluated in parallel using sliding windows 15 11 20 24 25 33 27 31 domain order figure even simple pipelines exhibit a rich space of scheduling choices each expressing its own trade off between parallelism locality and redundant recompute the choice made for each stage is two fold the simpler choice is the domain order which can express thread parallelism vectorization and traversal order row major vs column major dimensions can be split into inner and outer components which recursively expands the choice space and can express tiling strategies the more complex question each stage must answer is when its inputs should be computed choices include computing all dependencies ahead of time breadth first computing values as late as possible and then discarding them total fusion and computing values as late as possible but reusing previous computations sliding window the two categories of choice interact for example the fastest schedules often split the domain into tiles processed in parallel and then compute either breadth first or with sliding windows within each tile leaving parallelism unconstrained both within and across tiles in the iterated stencil computation literature the redundant regions are often called ghost zones and this strategy is sometimes called overlapped tiling 17 31 on a modern this strategy is faster than the breadth first strategy using the same amount of multithreaded and vector parallelism this is because the lack of producer consumer locality leaves the breadth first version limited by bandwidth this difference grows as the pipeline gets longer increasing the ratio of intermediate data to inputs and outputs and it will only grow further as the computational resources scale exponentially faster than external memory bandwidth under moore law the very fastest strategy we found on this architecture interleaves the computation of the two stages using a sliding window over scanlines while splitting the image into strips of independent scanlines which are treated separately alloc out 3072 for each ty in 2048 alloc blurx 3072 for y in for x in 3072 blurx y x in ty y tx x in ty y tx x in ty y tx x if y continue for x in 3072 out ty y x blurx y x blurx y x blurx y x relative to the original sliding window strategy this sacrifices two scanlines of redundant work on the overlapping tops and bottoms of independently processed strips of blurx to instead reclaim fine grained parallelism within each scanline and coarse grained parallelism across scanline strips the end result is faster still than the tiled strategy on one benchmark machine but slower on another the best choice between these and many other strategies varies across different target architectures there are also global consequences to the decision made for each stage in a larger pipeline so the ideal choice depends on the composition of stages not just each individual stages in isolation real image parallel y vectorized x split split x into y o x i serial into y o 2y x o y o y i x i i processing pipelines often have tens to hundreds of stages making the choice space enormous a model for the scheduling choice space we introduce a model for the space of important choices in schedul ing stencil pipelines based on each stage choosing at what granular ity to compute each of its inputs at what granularity to store each for reuse and within those grains in what order its domain should be traversed fig the domain order our model first defines the order in which the required region of each function domain should be traversed which we call the domain order using a traditional set of loop transformation concepts each dimension can be traversed sequentially or in parallel constant size dimensions can be unrolled or vectorized dimensions can be reordered e g from column to row major finally dimensions can be split by a factor creating two new dimensions an outer dimension over the old range divided by the factor and an inner dimension which iterates within the factor after splitting references to the original index become outer factor inner splitting recursively opens up further choices and enables many common patterns like tiling when combined with other transforma tions vectorization and unrolling are modeled by first splitting a dimension by the vector width or unrolling factor and then schedul ing the new inner dimension as vectorized or unrolled because halide model of functions is data parallel by construction dimen sions can be interleaved in any order and any dimension may be scheduled serial parallel or vectorized for reduction functions the dimensions of the reduction domain may only be reordered or parallelized if the reduction update is associative free variable dimensions may be scheduled in any order just as with pure functions our model considers only axis aligned bounding regions not general polytopes a practical simplification for image processing and many other applications but this also allows the regions to be defined and analyzed using simple interval analysis since our model of scheduling relies on later compiler inference to determine the bounds of evaluation and storage for each function and loop it is essential that bounds analysis be capable of analyzing every expression and construct in the halide language interval analysis is simpler than modern tools like polyhedral analysis but it can effectively analyze through a wider range of expressions which is essential for this design the call schedule in addition to the order of evaluation within the domain of each function the schedule also specifies the granularity of interleaving the computation of a function with the storage and computation of each function on which it depends we call these choices the call schedule we specify a unique call schedule for each function in the entire call graph of a halide pipeline each function call schedule is defined as the points in the loop nest of its callers where it is stored and computed fig top for example the three extremes from the previous section can be viewed along these axes the breadth first schedule both stores and computes blurx at the coarsest granularity which we call the root level outside any other loops the fused schedule both stores and computes blurx at the finest granularity inside the innermost x loop of out at this granularity values are produced and consumed in the same iteration but must be reallocated and recomputed on each iteration independently the sliding window schedule stores at the root granularity while computing at the finest granularity with this interleaving values of blurx are computed in the same iteration as their first use but persist across iterations to exploit this by reusing shared values in subsequent iterations the loops between the storage and computation levels must be strictly ordered so that a single unique first iteration exists for each point which can compute it for later consumers together the call schedule and domain order define an algebra for scheduling stencil pipelines on rectangular grids composing these choices can define an infinite range of schedules including the vast majority of common patterns exploited by practitioners in hand optimized image processing pipelines the loop transformations defined by the domain order interact with the inter stage interleaving granularity chosen by the call schedule because the call schedule is defined by specifying the loop level at which to store or compute a function call site may be stored or computed at any loop from the innermost dimensions of the directly calling function to the surrounding dimensions at which it is itself scheduled to be computed and so on through its chain of consumers splitting dimensions allows the call schedule to be specified at finer granularity than the intrinsic dimensionality of the calling functions for example interleaving by blocks of scanlines instead of individual scanlines or tiles of pixels instead of individual pixels since every value computed needs a logical location into which its result can be stored the storage granularity must be equal to or coarser than the computation granularity schedule examples revisiting the optimized examples from sec 3 the tiled schedule can be modeled as follows the domain order of out is split y ty y split x tx x order ty tx y x this is similar to the tiled domain or der shown rightmost in fig all dimensions may be parallel x is vectorized for performance the call schedule of blurx is set to both store and compute for each iteration of tx in out the domain of blurx under tx is scheduled order y x and x is vectorized for performance the parallel tiled sliding window schedule is modeled by making the call schedule of blurx store at ty in out but compute at finer granularity at out y dimension then the domain order of out is split y ty y order ty y x ty may be parallel x is vectorized for performance y must be sequential to enable reuse the domain of blurx under y only has dimension x which is vectorized for performance compiling scheduled pipelines our compiler combines the functions describing a halide pipeline with a fully specified schedule for each function to synthesize the machine code for a single procedure which implements the entire pipeline the generated pipeline is exposed as a c abi callable function which takes buffer pointers for input and output data as well as scalar parameters the implementation is multithreaded and vectorized according to the schedule internally manages the alloca tion of all intermediate storage and optionally includes synthesized gpu kernels which it also manages automatically the compiler makes no heuristic decisions about which loop transformations to apply or what will generate fast code for all such questions we defer to the schedule at the same time the generated code is safe by construction the bounds of all loops and allocations are inferred bounds inference generates loop bounds that ultimately depend only on the size of the output image bounded loops are our only means of control flow so we can guarantee termination all allocations are large enough to cover the regions used by the program given the functions defining a halide pipeline and a fully specified schedule as input fig left our compiler proceeds through the major steps below lowering and loop synthesis the first step of our compiler is a lowering process that synthesizes a single complete set of loop nests and allocations given a halide pipeline and a fully specified schedule fig middle lowering begins from the function defining the output in this case out given the function domain order from the schedule it generates a loop nest covering the required region of the output whose body evaluates the function at a single point in that domain fig middle top the order of loops is given by the schedule and includes additional loops for split dimensions loops are defined by their minimum value and their extent and all loops implicitly stride by this process rounds up the total traversed domain of dimensions which have been split to the nearest multiple of the split factor since all loops have a single base and extent expression at this stage loop bounds are left as simple symbolic expressions of the required region of the output function which is resolved later the bounds cannot have inter dependent dimensions between the loops for a single function so they represent a dense iteration over an axis aligned bounding box each loop is labeled as being serial parallel unrolled or vectorized according to the schedule lowering then proceeds recursively up the pipeline from callers to callees here from out to blurx callees apart from those scheduled inline are scheduled to be computed at the granularity of some dimension of some caller function this corresponds to an existing loop in the code generated so far this site is located and code evaluating the callee is injected at the beginning of that loop body this code takes the form of a loop nest constructed using the domain order of the callee the allocation for the callee is similarly injected at some containing loop level specified by the schedule in fig middle blurx is allocated at the level of tiles out x o while it is computed as required for each scanline within the tile compiler algorithm sliding window optimization code autotuner schedule storage folding input algorithm sec lowering out blurx x y in x y in x y par for out y o in x y out x y blurx x y blurx x y blurx x y lowering bound inference flattening vectorization unrolling generation running time in out y extent sec for for out x out y o in i bounds inference out x extent in let blurx y min out y o min out y i min vec for out x blurx x out x o x i y i i in y o y i blurx x blurx x i i sec flattening input schedule i y y i i blurx split vectorize store compute x at by out x at x i out y x o x i i sec lowering blurx out out y stride out y blurx blurx y stride out y out x o out x o min out x o out y i blurx y min out x i o min out y i i blurx x min sec vectorization vec for blurx x i out split x by x o alloc blurx blurx y extent blurx x extent for blurx y in blurx y min blurx y max for blurx x vec for blurx x o in blurx x min blurx x max i blurx blurx y stride out y blurx blurx y stride out y i i blurx y min out x blurx y min out x i i blurx x min blurx x min in split reorder parallelize y by y o y x o y y o i x y i i blurx blurx y stride blurx y blurx x o vectorize x i o in x i blurx x in x o o x i y blurx blurx y stride blurx y blurx x figure our compiler is driven by an autotuner which stochastically searches the space of valid schedules to find a high performance implementation of the given halide program the core of the compiler lowers a functional representation of an imaging pipeline to imperative code using a schedule it does this by first constructing a loop nest producing the final stage of the pipeline in this case out and then recursively injecting the storage and computation of earlier stages of the pipeline at the loop levels specified by the schedule the locations and sizes of regions computed are symbolic at this point they are resolved by the subsequent bound inference pass which injects interval arithmetic computations in a preamble at each loop level that set the region produced of each stage to be at least as large as the region consumed by subsequent stages next sliding window optimization and storage folding remove redundant computation and excess storage where the storage granularity is above the compute granularity a simple flattening transform converts multidimensional coordinates in the infinite domain of each function into simple one dimensional indices relative to the base of the corresponding buffer vectorization and unrolling passes replace loops of constant with k scheduled as vectorized or unrolled with the corresponding k wide vector code or k copies of the loop body finally backend code generation emits machine code for the scheduled pipeline via llvm out y i the allocation and computation for blurx are inserted at dimension are less expressive than the polyhedral model they can the corresponding points in the loop nest only describe iteration over axis aligned boxes rather than arbitrary reductions are lowered to a pair of loop nests the first initializes polytopes however it is trivial to synthesize efficient loops for the domain and the second applies the reduction rule both alloca any set of intervals in contrast to the problem of scanning general tion and loop extents are tracked as symbols of the required region polyhedra for many domains including image processing this is an of the function used by its callers once lowering has recursed to acceptable simplification most functions are applied over rectilinear the end of the pipeline all functions have been synthesized into a regions single set of loops most critically interval analysis can analyze a more general class of expressions it is straightforward to compute intervals through bounds inference at this stage for allocation sizes and loop bounds the pipeline relies on symbolic bounds variables for each dimension of each function the next stage of lowering generates and injects appropriate defini tions for these variables like function lowering bounds inference proceeds recursively back from the output for each function it symbolically evaluates the bounds of each dimension based on the bounds required of its caller and the symbolic indices at which the caller invokes it at each step the required bounds of each dimen sion are computed by interval analysis of the expressions in the caller which index that dimension given the previously computed nearly any computation from basic arithmetic to conditional ex pressions to transcendentals and even loads from memory as a result this analysis can be used pervasively to infer the complete bounds of every loop and allocation in any pipeline represented in halide it also generalizes through constructs like symbolic tile sizes which are beyond the scope of polyhedral analysis for cases where interval analysis is over conservative e g when computing the bounds of a floating point number loaded from memory which the programmer knows will be between and halide includes a simple clamp operator which simultaneously declares and enforces a bound on an expression bounds of all downstream functions after bounds inference has recursed to the top of the pipeline it 3 sliding window optimization and storage folding walks back down to the output injecting definitions for the bounds after bounds inference the compiler traverses the loop nests seeking variables used as stand ins during lowering they are defined by opportunities for sliding window optimizations if a realization of expressions which compute concrete bounds as a preamble at each a function is stored at higher loop level than its computation with loop level e g in fig right the minimum bound of blurx y is an intervening serial loop then iterations of that loop can reuse computed from interval analysis of the index expressions at which it values generated by previous iterations using the same interval is accessed combined with the bounds of the calling function out analysis machinery as in bounds inference we shrink the interval in practice hoisting dynamic bounds evaluation expressions to the to be computed at each iteration by excluding the region computed outermost loop level possible makes the runtime overhead of more by all previous iterations it is this transformation that lets us trade complex bounds expressions negligible off parallelism because the intervening loop must be serial for interval analysis is an unusual choice in a modern loop synthesis reuse because we avoid recomputing values already computed by and code generation system the resulting min max bounds for each previous iterations x i in x x o i y in in y stride blurx y min blurx y blurx x in x o x x i i y y o ramp o ramp for example in fig blurx is stored for reuse within each tile of out but computed as needed for each scanline within the tile because scanlines out y i are traversed sequentially intermediate values of blurx are computed immediately before the first scanline of out which needs them but may be reused my later scanlines within the tile for each iteration of out y i the range of blurx y is computed to exclude the interval covered by all prior iterations computed within the tile storage folding is a second similar optimization employed at this stage of lowering if a region is allocated outside of a serial loop but only used within it and the subregion used by each loop iteration marches monotonically across the region allocated we can fold the storage by rewriting indices used when accessing the region by reducing them modulo the maximum extent of the region used in any given iteration for example in fig each iteration of out y i only needs access to the last 3 scanlines of blurx so the storage of blurx can be reduced to just 3 scanlines and the value blurx x y 3 will reuse the same memory address as blurx x y blurx x y 3 and so on this reduces peak memory use and working set size flattening next the compiler flattens multi dimensional loads stores and allo cations into their single dimensional equivalent this happens in the conventional way a stride and a minimum offset are computed for each dimension and the buffer index corresponding to a multidimen sional site is the dot product of the site coordinates and the strides minus the minimum cf fig right by convention we always set the stride of the innermost dimension to to ensure we can perform dense vector loads and stores in that dimension for images this lays them out in memory in scanline order while our model of scheduling allows extreme flexibility in the order of execution we do not support more unusual layouts memory such as tiled or sparse storage we have found that modern caching memory hierarchies largely obviate the need for tiled storage layouts in practice vectorization and unrolling after flattening vectorization and unrolling passes replace loops of constant size scheduled as vectorized or unrolled with transformed versions of their loop bodies unrolling replaces a loop of size n with n sequential statements performing each loop iteration in turn that is it completely unrolls the loop unrolling by lesser amounts is expressed by first splitting a dimension into two and then unrolling the inner dimension vectorization completely replaces a loop of size n with a single statement for example in fig lower right the vector loop over blurx x i is replaced by a single wide vector expression any occurrences of the loop index blurx x i are replaced with a special value ramp n representing the vector n a type coercion pass is then run over this to promote any scalars combined with this special value to n wide broadcasts of the scalar expression all of our ir nodes are meaningful for vector types loads become gathers stores become scatters arithmetic becomes vector arithmetic ternary expressions become vector selects and so on later during code generation loads and stores of a linear expression of k ramp n o will become dense vector loads and stores if the coefficient k or strided loads and stores with stride k otherwise in contrast to many languages halide has no divergent control flow so this transformation is always well defined and straight forward to apply in our representation we never split a vector into a bundle of scalars it is always a single expression containing ramps and broadcast nodes we have found that this yields extremely efficient code without any sort of generalized loop auto vectorization back end code generation finally we perform low level optimizations and emit machine code for the resulting pipeline our primary backends use llvm for low level code generation we first run a standard constant folding and dead code elimination pass on our ir which also performs symbolic simplification of common patterns produced by bounds inference at this point the representation is ready to be lowered to llvm ir there is mostly a one to one mapping between our representation and llvm but two specific patterns warrant mention first parallel for loops are lowered to llvm code that first builds a closure containing state referred to in the body of a for loop the loop body is lowered to a separate function that accepts the closure as an argument and performs one iteration of the loop we finally generate code that enqueues the iterations of the loop onto a task queue which a thread pool consumes at runtime second many vector patterns are difficult to express or generate poor code if passed directly to llvm we use peephole optimization to reroute these to architecture specific intrinsics for example we perform our own analysis pass to determine alignment of vector loads and stores and we catch common patterns such as interleaving stores strided loads vector averages clamped arithmetic fixed point arithmetic widening or narrowing arithmetic etc by mapping specific expression ir patterns to specific simd opcodes on each architecture we provide a means for the programmer to make use of all relevant simd operations on arm using neon and using sse and avx gpu code generation the data parallel grids defining a halide pipeline are a natural fit for gpu programming models our com piler uses the same scheduling primitives along with a few simple conventions to model gpu execution choices gpu kernel launches are modeled as dimensions loops scheduled to be parallel and annotated with the gpu block and thread dimensions to which they correspond the limitations of gpu execution place a few constraints on how these dimensions can be scheduled in particular a sequence of block and thread loops must be contiguous with no other intervening loops between the block and thread levels since a kernel launch corresponds to a single multidimensional tiled parallel loop nest sets of kernel loops may not be nested within each other on current gpus which do not directly implement nested data parallelism additionally the extent of the thread loops must fit within the corresponding limits of the target device other than that all the standard looping constructs may still be scheduled outside or within the block and grid dimensions this corresponds to loops which internally launch gpu kernels and loops within each thread of a gpu kernel respectively given a schedule annotated with gpu block and thread dimen sions our compiler proceeds exactly as before synthesizing a single set of loop nests for the entire pipeline no stage before the backend is aware of gpu execution block and thread dimensions are treated like any other loops the gpu backend extends the backend including its full feature set outside the loops over block and thread dimensions the compiler generates the same optimized sse code as it would in the pure cpu target at the start of each gpu block loop nest we carve off the sub nest much like a parallel for loop in the cpu backend only it is spawned on the gpu we first build a clo sure over all state which flows into the gpu loops we then generate a gpu kernel from the body of those loops and finally we generate the host api calls to launch that kernel at the corresponding point in the host code passing the closure as an argument we also generate dynamic code before and after launches to track which buffers need to be copied to or from the device every allocated buffer which is used on the gpu has a corresponding device memory allocation and their contents are lazily copied only when needed the end result is not individual gpu kernels but large graphs of hybrid cpu gpu execution described by the same scheduling model which drives the cpu backends a small change in the schedule can transform a graph of dozens of gpu kernels and vectorized cpu loop nests tied together by complex memory management and synchronization into an entirely different graph of kernels and loops which produce the same result expressively modeling an enormous space of possible fusion and other choices in mapping a given pipeline to a heterogeneous machine autotuning pipeline schedules we apply stochastic search to automatically find good schedules for halide pipelines the automatic optimizer takes a fixed algorithm and attempts to optimize the running time by searching for the most efficient schedule the schedule search space is enormous far too large to search exhaustively for example in the local laplacian filters pipeline we estimate a lower bound of schedules this is derived by labeling functions with three tilings per function and all possible store and compute granularities the actual dimensionality of the space is likely much higher the optimal schedule dependends on machine architecture image dimensions and code generation in complex ways and exhibits global dependencies between choices due to loop fusion and caching behavior in this section we describe our autotuner because the search problem has many local minima we use a genetic algorithm to seek out a plausible approximate solution inspired by the search procedure in petabricks we first describe the schedule search space we show how domain specific knowledge can be used to select reasonable starting points then we explain the general operations of the genetic algorithm finally we show how further knowledge can be incorporated as search priors for more effective mutation rules schedule search space our full model of scheduling is per call but to simplify autotuning we schedule each function identically across all call sites the domain transformations include splitting and reordering dimensions and marking them parallel vectorized or unrolled or mapping a pair of dimensions to a gpu grid launch variable and block size arguments are randomized and chosen from small powers of two because schedules have complex global dependencies not all schedules are valid for example a schedule could be computed or stored inside a dimension that does not exist in the caller loop order genetic operations such as mutate and crossover may invalidate correct parent schedules in general therefore we reject any partially completed schedules that are invalid and continue sampling until we obtain valid schedules we also verify the program output against a correct reference schedule over several input images this is just a sanity check all valid schedules should generate correct code finally to prevent explosion of generated code due to complex pipelines we limit the number of domain scheduling operations for each function search starting point one valid starting schedule is to label all functions as computed and stored breadth first at the outermost root granularity the tuner converges from this starting point albeit slowly we can often do better by seeding the initial population with reasonable schedules for each function we find its rectangular footprint relative to the caller via bounds inference and inline functions with footprint one remaining functions are stochastically scheduled as either fully parallelized and tiled or simply parallelized over y we define fully parallelized and tiled as tiled over x and y vectorized within the tile s inner x coordinate and parallelized over the y outer tile dimension these choices are selected by a weighted coin that has fixed weight from zero to one depending on the individual this allows us to often discover good starting points for functions that vectorize well or fall back to naive parallelism when that is not the case the dimensions x and y are chosen from adjacent dimensions at random except when there are optional bounds annotations provided by the halide programmer such as the number of color channels dimensions with small bound are not tiled genetic algorithm search we use a fixed population size individuals per generation for all examples in this paper and construct each new generation with population frequencies of elitism crossover mutated individuals and random individuals elitism copies the top individuals from the previous generation crossover selects parents by tournament selection followed by two point crossover with crossover points selected at random between functions random individuals are generated either by the reasonable schedules described previously or with equal probability by scheduling each function independently with random schedule choices this is directly derived from the petabricks autotuner schedule mutation rules we incorporate further prior knowledge about schedules into our mutation rules mutation selects a function at random and modifies its schedule with one of eight operations selected at random six are fairly generic randomize constants replace a function s schedule with one randomly generated copy from a randomly selected function s schedule and for the function s list of domain transformations either add remove or replace with a randomly selected transformation our remaining two mutations incorporate specific knowledge about imaging these are chosen with higher probability first a pointwise mutation of compute or storage granularity is generally ineffective at fusing loops thus we have a loop fusion rule which schedules the chosen function as fully parallelized and tiled fol lowed by scheduling callees as vectorized by x and computed and stored under the tile s inner x dimension the callee scheduling is repeated recursively until a coin flip fails finally we incorporate prior knowledge by applying a template we replace a function s schedule with one of three common schedule patterns sampled from a text file these are 1 compute and store under x and vectorize by x fully parallelized and tiled and 3 parallelized over y and vectorized over x if generating a cuda schedule we inject a fourth pattern which simply tiles on the gpu the x and y dimensions are determined as in the starting point results to evaluate our representation and compiler we applied them to a range of image processing applications and compared the best autotuned result found by our compiler to the best previously published expert implementation we could find we selected this set of examples to cover a diversity of algorithms and communication patterns it includes pipelines ranging from two to stages and including many different stencils data dependent access patterns and histogram reductions we describe each application below fig summarizes their properties and fig summarizes their performance all evaluation was performed on a quad core xeon cpu an nvidia tesla gpu blur is the simple example used in sec 3 1 which convolves the image with two 3 1 box kernels in two steps a horizontal 3 1 kernel then a vertical 1 3 kernel this is a simple example of two consecutive stencils our reference comparison is a hand optimized manually fused and multithreaded loop nest defined entirely in sse intrinsics this version is 12 faster than a simple pair of loops in c compiled by gcc the version found by our autotuner is faster still while being generated from a two line halide algorithm rather than lines of intrinsics functions stencils graph structure blur 2 simple bilateral grid 3 moderate camera pipeline complex local laplacian filters 85 very complex multi scale interpolation complex figure 6 properties of the example applications in some cases the number of functions exceeds the number of program lines in fig because halide functions are meta programmed using higher order functions camera pipeline transforms the raw data recorded by the camera sensor into a usable image its demosaicking alone is a complex combination of interleaved and inter dependent stencils the reference comparison is a single carefully tiled and fused loop nest from the frankencamera expressed in lines of c 1 all producer consumer communication is staged through scratch buffers tiles are distributed over parallel threads using openmp and the tight loops are autovectorized by gcc the halide algorithm is lines describing functions and 22 different stencils literally translated from the pseudocode in the comments explaining the original source it compiles to an implementation 3 faster than the hand tuned original the autotuned schedule fuses long chains of stages through complex interleaved stencils on overlapping tiles fully fuses other stages vectorizes every stage and distributes blocks of scanlines across threads multi scale interpolation uses an image pyramid to interpolate pixel data for seamless compositing this requires dealing with data at many different resolutions the resulting pyramids are chains of stages which locally resample over small stencils but through which dependence propagates globally across the entire image the reference implementation is a carefully structured set of loop nests which were hand tuned by an adobe engineer to generate a fully vectorized implementation in gcc the halide algorithm is substantially simpler but compiles to an implementation 1 faster than the original parallel vector code the same halide algorithm also automatically compiles to a graph of cuda kernels and code which is 5 faster bilateral grid is an efficient implementation of the bilateral filter which smoothes an image while preserving its main edges 5 it first scatters the image data into a grid effectively building a windowed histogram in each column of the grid then blurs the grid along each of is axes with three 5 point stencils finally the output image is constructed by trilinear interpolation within the grid at locations determined by the input image the cpu reference code is a tuned but clean implementation from the original authors in lines of c it is partially autovec torized by gcc but is nontrivial to multithread a naive openmp parallelization of major stages results in a slowdown on our bench mark cpu so the reference is single threaded the halide algo rithm is lines and compiles to an implementation 4 2 faster than the original the speedup comes from a combination of par allelism tile level fusion of some stages and careful reordering of dimensions to control parallel grain size in the grid we also compared our implementation to a hand tuned gpu implementation from the original authors written in lines of cuda code the same halide algorithm less than 1 10th the code found a different schedule which was 2 3 faster than the hand written cuda the halide compiler generates similar cuda code to the reference but the autotuner found an unintuitive point in the schedule space which sacrificed some parallelism in the grid construction step to reduce synchronization overhead in the scattering reduction it also uses a tiled fusion strategy which halide expert speedup lines lines factor tuned tuned halide expert shorter ms ms blur 11 13 1 2 2 35 bilateral grid 158 4 4 122 4 camera pipe 49 3 4 2 interpolate 1 21 local laplacian 1 5 cuda halide expert speedup lines lines factor tuned tuned halide expert shorter ms ms bilateral grid 1 2 3 34 11 interpolate 1 54 5 21 local laplacian 21 189 5 figure comparison of autotuned halide program running times to hand optimized programs created by domain experts in c in trinsics and cuda halide programs are both faster and require fewer code lines no gpu reference available compared to cpu reference passes intermediate results through gpu scratchpad memory to improve locality through the blur steps at the expense of redundant computation these tradeoffs were counter intuitive to the original author and also much harder to express in cuda but are easily described by our schedule representation and found by our autotuner local laplacian filters uses a multi scale approach to tone map images and enhance local contrast in an edge respecting fashion 3 22 it is used in the clarity tone mapping and other filters in adobe photoshop and lightroom the algorithm builds and manipulates several image pyramids with complex dependencies between them the filter output is produced by a data dependent resampling from several pyramids with the parameters we used the pipeline contains different stages operating at many scales and with different computational patterns the reference implementation is lines of c developed at adobe and carefully parallelized with openmp and offloading most intensive kernels to tuned assembly routines from intel per formance primitives 20 it has very similar performance to a version deployed in their products which took several months to develop including 2 3 weeks dedicated to optimization it is faster than an algorithmically identical reference version written by the authors in pure c without ipp or openmp the halide version was written in one day in lines of code it compiles to an implementation which is 1 faster than the highly optimized ex pert implementation roughly 20 faster than the clean c without ipp and openmp the resulting schedule is enormously complex mixing different fusion tiling vectorization and multithreading strategies throughout the expansive stage graph in c it would correspond to hundreds of loops over more than 000 lines of code the same program compiles with a different automatically generated schedule to a hybrid cpu gpu program with unique gpu kernels each representing a differently tiled and partially fused subset of the overall graph and with the lookup table and several smaller levels of the pyramids scheduled as vector code on the cpu the generated program has more distinct cuda kernels than the halide algorithm describing it has lines of code it runs 5 faster than the hand tuned adobe implementation and is more than 4 faster than the best parallel vector implementation on the cpu this source target cross autotuned slowdown size size tested on target vs target mp mp time ms ms autotuned blur 3 13 11 1 2 bilateral grid 0 3 2 35 0 interpolate 0 3 2 31 0 blur 0 3 1 1 0 bilateral grid 2 0 3 6 6 1 4 interpolate 2 0 3 7 5 2 1 9 figure cross testing of autotuned schedules across resolutions each program is autotuned on a source image size the resulting schedule is tested on a target image size giving a cross tested time this is compared to the result of running the autotuner directly on the target resolution we report the ratio of the cross tested time to the autotuned on target time as the slowdown note that schedules generalize better from low resolutions to high resolutions in theory the slow down should always be at least one but due to the stochastic nature of the search some schedules were slower when autotuned on the target is by far the fastest implementation of local laplacian filters we know of 6 1 autotuning performance these examples took between 2 hours and 2 days to tune from to of generations in all cases the tuner converged to within 15 of the final performance after less than one day tuning on a single machine improvements to the compiling and tuning infrastructure for example distributing tests across a cluster could reduce these times significantly we generally found the tuned schedules to be insensitive to moderate changes in resolution or architecture but extreme changes can cause the best schedule to change dramatically table 8 shows experiments in cross testing schedules tuned at different resolutions we observe that schedules generalize better from low resolutions to high resolutions we also mapped the best gpu schedule for local laplacian filter to the cpu and found that this is 7 slower than the best cpu schedule 6 2 discussion across a range of image processing applications and target architec tures our scheduling representation is able to model our compiler is able to generate and our autotuner is able to discover implemen tation strategies which deliver state of the art performance this performance comes from careful navigation of the extremely high dimensional space of tradeoffs between locality parallelism and re dundant recomputation in image processing pipelines making these tradeoffs by hand is challenging enough as shown by the much greater complexity of hand written implementations but finding the ideal points is daunting when each change a programmer might want to test can require completely rewriting a complex loop nest hundreds of lines long the performance advantage of the halide implementations is a direct result of simply testing many more points in the space than a human programmer ever could manually describe at the level of explicit loops nonetheless based on our experience brute force autotuning still poses significant challenges of robustness and usability in a real system the tuning process is an entire extra stage added to a programmer s normal development process straightforward implementations are sensitive to noise in testing environment and many other factors in halide while simple pipelines like blur can tune effectively with only trivial mutation rules we found heuristic mutation rules are essential to converge in a reasonable amount of time when tuning more complex imaging pipelines however these rules may be specific to the algorithm structure for example different template mutation rules may be necessary for voxel data or unconventional color layouts we also found that it is necessary to maintain diversity to avoid becoming trapped in local minima which we did by using a large population individuals per generation even so the tuner can occasionally become trapped requiring restarting with a new random initialization and taking the best of several runs in general we feel that high dimensional stochastic autotuning has not yet developed the robust methodology or infrastructure for real world use found elsewhere in the compiler community 7 prior work image processing pipelines include similar structure to several problems well studied in compilers split compilers sequoia s mappings and spiral s loop syn thesis algebra echo our separation of the model of scheduling from the description of the algorithm and its lifting outside our com piler 9 25 stream programs compiler optimization of stream programs was studied extensively in the streamit and brook projects 4 11 in this framework sliding window communication implements stencils on streams 12 24 stream compilers generally did not consider the introduction of redundant work as a major optimization choice and nearly all work in stream compilation has focussed on these streams image processing pipelines in contrast are effectively stream programs over streams stencil optimization iterated stencil computations are important to many scientific applications and have been studied for decades frigo and strumpen proposed a cache oblivious traversal for effi cient stencil computation 10 this view of locality optimization by interleaving stencil applications in space and time inspired our model of scheduling the pochoir compiler automatically trans forms stencil codes from serial form into a parallel cache oblivious form using similar algorithms overlapping tiling is a strategy which divides a stencil compu tation into tiles and trades off redundant computation along tile boundaries to improve locality and parallelism 16 modeled in our schedule representation as interleaving both storage and computa tion inside the tile loops other tiling strategies represent different points in the tradeoff space modeled by our representation past compilers have automatically synthesized parallel loop nests with overlapped tiling on cpus and gpus using the polyhedral model 13 16 these compilers focussed on synthesizing high quality code given a single user defined set of overlapped tiling parameters autotuning has also been applied to iterated stencil com putations but past tuning work has focussed on exhaustive search of small parameter spaces for one or a few strategies 15 image processing languages critically many optimizations for iterated stencil computations are based on the assumption that the time dimension of iteration is large relative to the spatial dimension of the grid in image processing pipelines most individual stencils are applied only once while images are millions of pixels in size image processing pipelines also include more types of computa tion than stencils alone and scheduling them requires choices not only of different parameters but of entirely different strategies for each of many heterogeneous stages which is infeasible with either exhaustive search or polyhedral optimization most prior image pro cessing languages and systems have focused on efficient expression of individual kernels as well as simple fusion in the absence of stencils 6 8 27 recently cornwall et al demonstrated fast ________________ gpu code generation for image processing code using polyhedral optimization 7 earlier work on the halide language included a weaker model of schedules and required programmers to explicitly specify schedules by hand this is the first automatic optimizer and therefore the first fully automatic compiler for halide programs we show how they can be automatically inferred starting from just the algorithm defining the pipeline stages and using relatively little domain specific knowledge beyond the ability to enumerate points in the space of schedules our state of the art performance shows the effectiveness of our scheduling model for representing the underlying choice space the scheduling model presented here is also richer than in past work in particular it separates computation frequency from storage frequency in the call schedule enabling sliding window schedules and similar strategies which trade off parallelism for redundant work while maintaining locality in all this gives a dramatic new result automatic optimization of stencil computations including full consideration of the parallelism locality and redundancy tradeoffs past automatic stencil optimiza tions have targeted individual points in the space but have not auto matically chosen among different strategies spanning these multi dimensional tradeoffs and none have automatically optimized large heterogeneous pipelines only individual or small multi stencils natural languages like english are rich complex and powerful the highly creative and graceful use of languages like english and tamil by masters like shakespeare and avvaiyar can certainly delight and inspire but in practice given cognitive constraints and the exigencies of daily life most human utterances are far simpler and much more repetitive and predictable in fact these utterances can be very usefully modeled using modern statistical methods this fact has led to the phenomenal success of statistical approaches to speech recognition natural language translation question answering and text mining and comprehension we begin with the conjecture that most software is also natural in the sense that it is created by humans at work with all the attendant constraints and limitations and thus like natural language it is also likely to be repetitive and predictable we then proceed to ask whether a code can be usefully modeled by statistical language models and b such models can be leveraged to support software engineers using the widely adopted n gram model we provide empirical evidence supportive of a positive answer to both these questions we show that code is also very repetitive and in fact even more so than natural languages as an example use of the model we have developed a simple code completion engine for java that despite its simplicity already improves eclipse built in completion capability we conclude the paper by laying out a vision for future research in this area keywords language models n gram natural language pro cessing code completion and code suggestion i i ntroduction the word natural in the title of this paper refers to the fact that code despite being written in an artificial language like c or java is a natural product of human effort this use of the word natural derives from the field of natural language processing where the goal is to automatically process texts in natural languages such as english and tamil for tasks such as translation to other natural languages summarization understanding and speech recognition the field of natural language processing nlp see sparck jones for a brief history went through several decades of rather slow and painstaking progress beginning with early struggles with dictionary and grammar based abram hindle is now with university of alberta edmonton mark gabel was at uc davis when this work was done efforts in the in the and the field was re animated with ideas from logic and formal semantics which still proved too cumbersome to perform practical tasks at scale both these approaches essentially dealt with nlp from first principles addressing language in all its rich theoretical glory rather than examining corpora of actual utterances i e what people actually write or say in the a fundamental shift to corpus based statistically rigorous methods occurred the availability of large on line corpora of natural language text including aligned text with translations in multiple languages along with the computational muscle cpu speed primary and secondary storage to estimate robust statistical models over very large data sets has led to stunning progress and widely available practical applications such as statistical translation used by translate google com we argue that an essential fact underlying this modern exciting phase of nlp is natural language may be complex and admit a great wealth of expression but what people write and say is largely regular and predictable our central hypothesis is that the same argument applies to software programming languages in theory are complex flexible and powerful but the programs that real people actually write are mostly simple and rather repetitive and thus they have usefully predictable statistical proper ties that can be captured in statistical language models and leveraged for software engineering tasks we believe that this is a general useful and practical notion that together with the very large publicly available corpora of open source code will enable a new rigorous statistical approach to a wide range of applications in program analysis error checking software mining program summarization and code searching this paper is the first step in what we hope included the canadian hansard parliamentary proceedings and similar outputs from the european parliament a renowned pioneer of the statistical approach fred jelenik is reputed to have exclaimed every time a linguist leaves our group the performance of our speech recognition goes up see http en wikiquote org wiki c ieee icse zurich switzerland the probability distribution of code sequences with the ability to calculate such a distribution and if this distribution has low entropy we will often be able to guess with high confidence what follows the prefix of a code sequence what should the form of a such a distribution be and how should we estimate its parameters in nlp these distributions are called language models a language models a language model essentially assigns a probability to an utterance for us utterances are programs more formally consider a set of allowable program t and the over generous set of possible program sequences t we assume the set of possible implemented systems to be s t a language model is a probability distribution p over systems s s p s p in practice given a corpus c of programs c s and a suitably chosen parametric distribution p we attempt to calculate a maximum likelihood estimate of the parameters of this distribution this gives us an estimated language model the choice of a language model is usually driven by practicalities how easy is it to estimate and use for these reasons the most ubiquitous is the n gram model which we now describe consider the sequence of tokens in a document in our case a estimate system how a likely a a a token i a is n to n gram follow models other will be a long and fruitful journey we make the following contributions we provide support for our central hypothesis by instan tiating a simple widely used statistical language model using modern estimation techniques over large software corpora we demonstrate using standard cross entropy and perplexity measures that the model indeed captures the high level statistical regularity that exists in software at the n gram level probabilistic chains of tokens we illustrate the use of such a language model by devel oping a simple code suggestion tool that substantially improves upon the existing suggestion facility in the widely used eclipse ide and we lay out our vision for an ambitious research agenda that exploits large corpus statistical models of natural software to aid in a range of different software engi neering tasks ii m otivation and b ackground there are many ways one could exploit the statistics of natural programs we begin with a simple motivating example we present more ambitious possibilities later consider a speech recognizer receiving a noisy signal corresponding to in brussels today the european cen tral radio phizz announced that interest rates remain unchanged a good speech recognizer might guess that the noisy word was bank rather than fish from context likewise consider an integrated development environment statistically tokens thus ide into which a programmer has typed in the partial statement for i i in this context it would be quite we can estimate the probability of a document based on the product of a series of conditional probabilities reasonable for the ide to suggest the completion i p p a p a a p a a a p a n a a n to the programmer why do these guesses seem so reasonable to us in the first case the reason lies in the highly predictable nature of newscasts news reports like many other forms of culturally contextualized and stylized natural language expression tend n gram models assume a markov property i e token occurrences are influenced only by the n tokens that precede the token under consideration thus for gram models we assume to be well structured and repetitive with a reasonable prior knowledge viz a good statistical model of this style it is possible to rank order likely utterances thus if we hear the words european central the next word is more likely to be bank rather than fish this fact is well known and exploited by speech recognizers natural language translation p a i a a i p a i a i a i a i these models are estimated on a corpus using maximum likelihood based frequency counting of token sequences thus if is a wildcard we can estimate the probability that a follows the tokens a a a with devices and even some ocr optical character recognition tools the second example relies on a lesser known fact p a count a a a a natural programs are quite repetitive this fact was first count a observed and reported in a very large scale study of code by gabel and su which found that code fragments of surprisingly large size tend to reoccur thus if we see the fragment for i i we know what follows in most cases in general if we know the most likely sequences in a code body we can often help programmers complete code what this essentially amounts to is using a code corpus to estimate in practice estimation of n gram models is quite a bit more complicated the main difficulties arise from data sparsity i e the richness of the model in comparison to the available data for example with token vocabulary a trigram model must estimate coefficients some trigrams may never occur in one corpus but may in fact we use token to mean its lexeme a a a a a lower occur elsewhere this leads to technical difficulties when we probabilities to rare ones if one could manage to encounter a previously unseen n gram we are in principle deploy a hypothetical truly superb model within an ide infinitely surprised because an infinitely improbable event to help programmers complete code fragments it might be x which did not occur in the training corpus and was able to guess with high probability most of the program so therefore estimated to have p x actually occurs this that most of the programming work can be done by just leads to infinite entropy values as will become evident hitting a tab key in practice of course we would probably below smoothing is a technique to handle such cases while be satisfied with a lot less still producing usable results with sufficient statistical rigour but how good are the models that we can actually build fortunately there exist a variety of techniques for smoothing for natural software is software is really as natural i e the estimates of a very large number of coefficients some unsurprising as natural language of which are larger than they should be and others smaller sometimes it is better to back off from a trigram model to a iii methodology and findings bigram model the technical details are beyond the scope of to shed light on this question we performed a series of this paper but can be found in any advanced nlp textbook experiments with both natural language and code corpora first in practice we found that modified kneser ney smoothing comparing the naturalness using cross entropy of code e g koehn gives good results for software corpora with english texts and then comparing various code corpora compared to plain kneser ney lidstone add one smoothing to each other to further gain insight into the similarities and and maximum likelihood for instance maximum likelihood differences between code corpora is often infinitely surprised and lidstone smoothing tends our natural language studies were based on two very to overemphasize surprises however we note that these widely used corpora the brown corpus and the gutenberg are very early efforts in this area and new modeling and corpus for code we used two corpora a collection of java estimation techniques tailored for software might improve projects and a collection of applications from ubuntu broken on the results presented below up into application domain all are listed in table i how do we know when we have a good language model after removing comments the projects were lexically b what makes a good model given a repetitive and highly predictable corpus of docu ments or programs a good model captures the regularities in the corpus thus a good model estimated carefully from a representative corpus will predict with high confidence the contents of a new document drawn from the same population such a model can guess the contents of the new document with very high probability in other words the model will not find a new document particularly surprising or perplexing in nlp this idea is captured by a measure called perplexity or its log transformed version cross entropy given a document a a n of length n and a language model m we assume that the probability of the document estimated by the model is p m we can write down the cross entropy measure as analyzed according to language syntax to produce token sequences that were used to estimate n gram language models most of our corpora are in c and java extending to other languages is trivial the java projects were our central focus we used them both for cross entropy studies and some experiments with an eclipse plug in for a language model based code suggestion task table i describes the java projects that we used the version indicates the date of the last commit to the master branch in the git repository when we cloned the project lines is calculated using unix wc on each file within each repository and tokens are extracted from each of these files tokens counts the total tokens extracted unique tokens counts the distinct tokens the ubuntu domain categories were quite large in some cases ranging up to million lines million tokens one million unique the number h m n log p m a a n of unique tokens is interesting and relevant as they give a very rough indication on the potential surprisingness of and by the formulation presented in section ii a the project corpus if these unique tokens were uniformly h m n n i distributed throughout the project highly unlikely we could expect a cross entropy of log log p m a i a a i or approximately bits a similar calculation for the java projects ranges this is a measure of how surprised a model is by the from to bits given document a good model has low entropy for target documents it gives higher probabilities closer to and thus lower absolute log values to more frequent words and a cross entropy of code cross entropy is a measure of how surprising a test document is to a distribution model estimated from a corpus en wikipedia org wiki see also page equation retrieved these corpora from http www nltk org table i j ava projects c code from u buntu c ategories and english cross entropy e nglish c orpus used in our study e nglish is the java projects cross entropy order of n grams figure comparison of english cross entropy versus the code cross entropy of java projects java language project corpora using averages over fold cross validation each corpus to itself as described above the results are in figure the single line at the top is the average over the folds of the english corpus beginning at about bits for unigram models and trailing down to under bits for gram models when you build a model on one project we call the computation of cross entropy on test data from that same project self cross entropy the average self cross entropy for the java projects are shown below in boxplots one for each order from unigram models to gram models several observations can be made each project was concatenated and viewed as a single document first software unigram entropy is much lower than might be expected from a uniform distribution over unique tokens because token frequencies are obviously very skewed second cross entropy declines rapidly with n gram order saturating around tri or grams the similarity in the decline in english and the java projects is striking this decline suggests that the language model captures as much repetitive local context in java programs as it does in english corpora we take this to be highly encouraging the ability to model the regularity of the local context in natural languages has proven to be extremely valuable in statistical natural language processing we hope and provide some evidence to support the claim that this regularity can be exploited for software tools finally software is far more regular than english with entropies sinking down to under bits in some cases n concatenation of brown and gutenberg ubuntu maverick was released on the number of projects in each category is in parentheses tokens java project version lines total unique ant 27008 batik 30298 cassandra 13002 eclipse 6807301 8056 lucene 32676 263831 114527 xalan j 39383 xerces 19542 tokens ubuntu domain version lines total unique admin 1140555 doc 15373 graphics 188792 interp 201538 mail 137324 net 541896 sound 436377 tex 375845 text 155177 web 216474 tokens english corpus version lines total unique brown 56057 gutenberg 51156 thus if one tests a corpus against itself one has to set aside some portion of the corpus for testing and estimate train the model on the rest of the corpus in all our experiments we measured cross entropy by averaging over a fold cross validation we split the corpus in lines at random locations trained on the and tested on and measured the average cross entropy we used a open vocabulary model tokens unseen in the training text were smoothed to a small probability allocated to unknown tokens a further bit of notation when we say we measured the cross entropy of x to y y is the training corpus used to estimate the parameters of the distribution model m y used to calculate h m y o i t a d i l a v o r c d l o f y p o r t n e o r c r o y t i x e l p r e p g o l x first we wanted to see if there was evidence to support the claim that software was natural in the same way that english is natural viz whether regularities in software could be captured by language models do n gram language models capture regularities in software to answer this question we estimated n gram models for several values of n over both the english corpus and the is no risk of over fitting thus to evaluate self cross entropy with fold cross validation of the lines act as a test document and the corpus is the other of the lines this suggests that useful language models can be built even for small code corpora y p o r t n e o r c language models capture significant levels of local regularity that are not an artifact of the programming language syntax but rather arise from naturalness or repetitiveness specific to each project furthermore we self cross entropy project cross entropy have captured this regularity in projects with only about lines of code corpus projects figure cross entropy versus self cross entropy of the java projects studied this is noteworthy it appears each project has its own type of local non java specific regularity that the mode is capturing furthermore the local regularity of each project corpus based statistical language models capture a high level of local regularity in software even more so than in english is special unto itself and different from that of the other projects clearly each project has its own vocabulary and specific local patterns of iteration field access method calls etc language models are therefore capturing non java specific project regularity beyond the differences in unigram this raises a worrying question is the increased regularity we are capturing in software merely a difference between the english and java languages themselves java is certainly a much simpler language than english with a far more structured syntax might not the lower entropy be simply an artifact of java artificial simple syntax if the statistical regularity of the local context captured by the language model were simply arising from the simplicity of java then we should find this uniformly across all the projects in particular if we train a model on one java project and test on another we should successfully capture the local regularity vocabularies in section iv we discuss the application of the multi token local regularity captured by the models to a completion task as we demonstrate in that section the models are able to successfully suggest non linguistic tokens tokens that are not java keywords about of the time this also provides evidence that the low entropy produced by the models are not just because of java language simplicity but projects do not exist in isolation the entire idea of product line engineering rests on the fact that products in similar domains are quite similar to each other this raises the interesting question in the language thus we sublimate this anxiety provoking question into the following do n gram models capture similarities within and differences between project domains is the local regularity that the statistical language model captures merely language specific or is it also project specific ant batik cassandra eclipse lucene xalan j we approached this question by studying categories of applications within ubuntu listed in table i for each category we calculated the self cross entropy within the this is a simple experiment for each of the projects category red box and the other cross entropy the cross we train a trigram model and evaluate its cross entropy entropy against all the other categories boxplot shown in against each of the others then compare the result with figure here again as in figure we see that there appears the average fold self cross entropy we chose trigrams to be a lot of local regularity repeated within application because they do not use much memory and use minimal domains and much less so across application domains some context to produce low cross entropy this plot is shown in domains e g the web appear to have a very high level of figure the x axis lists all the different java projects and regularity and lower self entropy this is an interesting for each the boxplot shows the range of cross entropies with phenomenon requiring further study while larger projects the other nine projects the red line at the bottom shows the i e more data is better these results suggest that even new average self cross entropy of the project against itself in this projects can leverage corpora in the same or similar domains figure each document was the concatenation of a project as can be seen the self entropy is always lower even for small b concluding discussion projects like and maven the self cross entropy is low a high degree of local repetitiveness or regularity is because it is also obtained by fold cross validation there present in code corpora and captured by n gram models y p o r t n e o corpus categories r c self cross entropy cross entropy of alternative ubuntu categories admin doc graphics interpreters mail net sound tex text web figure categories of ubuntu applications cross entropy categories total packages the data suggest that these local regularities are specific to both projects and to application domains the data also indicate that these regularities are not simply due to the more regular when compared to natural languages syntax of java but arise from other types of project and domain specific local regularities that exist in the code next we show that these project specific regularities are actually useful we exploit project specific models to extend the eclipse suggestion engine we also show that the n gram models quite often about of the time provide suggestions that are project specific rather than merely suggesting context relevant java keywords in natural language these local regularities have proven to be of profound value for tasks such as translation it is our belief that these simple local regularities can be used for code summarization and code searching we also believe that deeper semantic properties will also in general manifest themselves in these same local regularities these are discussed further in future work section vi iv suggesting the next token the strikingly low entropy between and bits produced by the smoothed n gram model indicates that even at the local token sequence level a high degree of naturalness obtains with just tries we may very well guess the right next token a eclipse suggestion plug in we built an eclipse plug in to test this idea most modern ides have a built in suggestion engine that suggests a next token whenever it can typically suggestions are based on type information available in context we conjectured that algorithm mse esugg nsugg maxrank minlen require esugg and nsugg are ordered sets of eclipse and n gram suggestions elong p esugg maxrank strlen p minlen if elong then return esugg maxrank end if return esugg maxrank nsugg maxrank corpus based n gram models suggestion engine for brevity ngse could enhance eclipse built in suggestion engine for brevity ecse by offering tokens that tend to naturally follow from preceding ones in the relevant corpus the ngse uses a trigram model built from a project corpus after each token ngse uses the previous two tokens the test document already entered into the text buffer and attempts to guess the next token currently based on a static corpus of source code the language model estimates the probability of a specific choice of next token this probability can rank order the likely next tokens our implementation produces rank ordered suggestions in less than seconds on average both ngse and ecse produce many suggestions too many to present so we use a heuristic to merge the lists from the two groups given an admissible number n of suggestions to be presented to the user choose n candidates from both ngse and ecse offers in general ngse was good at recommending shorter tokens while ecse was better at longer tokens we discuss the reasons for this phenomenon later in this section this suggested the simple merge algorithm mse defined in algorithm in our experiments is the break even length after which eclipse outperforms our n gram model so we set minlen whenever eclipse offers long suggestions within the top n we greedily pick all the top n offers from eclipse otherwise we pick half from eclipse and half from n grams the relative performance of mse and ecse in practice might depend on a great many factors and would require a well controlled human study to be done at scale a suggestion engine can present more or fewer choices it may offer all suggestions or only offer suggestions that are long suggestions could be selected with a touch screen with a mouse or with a down arrow key since our goal here is to gauge the power of corpus based language models as opposed to building the most user friendly merged suggestion engine which remains future work we conducted an automated experiment rather than a human subject study we controlled for factors in our experiments the string length of suggestions l and the number of choices n presented to the user we repeated the experiment varying n for n and l for l we omitted suggestions less than characters as not useful also when merging two suggestion lists we chose to pick at least one from each and thus n we felt that more than choices would be overwhelming although our findings do not change very much at all even with and choices we choose projects for study ant maven xalan and xerces each project was a mature apache foundation java project used adopted by many java open source projects in each project we set aside a test set of randomly chosen files set aside in all and built a trigram language model on the remaining files for each project trigrams are chosen because they use sufficient context tokens they use less memory than grams or grams to represent and trigrams represent an inflection point where increasing the order of n for n grams results in a decreasing reduction of cross entropy see figure we then used the mse and ecse algorithms to predict every token in the set aside files and evaluated how many more times the mse made a successful suggestion when compared to the basic ecse we do not report precision or recall since there is usually only one correct suggestion files were chosen to reduce run time while maintaining necessary statistical significance and power in each case we evaluated the advantage of mse over ecse measured as the percent and absolute gain in number of correct suggestions at each combination of factors n and l b how does the language model help figure shows the results note the two y scales the left side black circle points is percent additional correct percent gain raw gain count c gain using top suggestions figure suggestion gains from merging n gram suggestions into those of eclipse p suggestion length suggestion length percent gain raw gain count a gain using top suggestions e ip l c e r e v o n i a g t r t n u o c in a g n e c r w a e p percent gain raw gain count b gain using top suggestions suggestion length e ip l c e r e v o n i a g t t n u o c n i a g n e c r e w a r p e ip l c e r e v o n i a g t n r t n u o c n i a g e c r e w a modern suggestions and the right side red square points are the ides provide both code completion and code raw counts since the raw count of successful suggestions suggestion often with a unified interface two notable java from the eclipse engine ecse also declines with length based examples are eclipse and intellij idea both draw both measures are useful as can be seen mse provides possible completions from existing code but they operate measurable advantage over ecse in all settings of both quite differently from our completion prototype factors though the advantage generally declines with l the eclipse and idea respond to completion requests a gains up through character tokens are quite substantial in keyboard shortcut such as ctrl space by conservatively the range of additional suggestions from the language deducing what tokens might apply in the current syntactic model that are correct between and characters the gains context eclipse and idea implement dozens of syntactic range from and semantic completion rules that are primarily guided by the additional suggestions from ngse run the the java language specification for example both eclipse gamut including methods classes and fields predictable and idea first parse the surrounding code and infer the from frequent trigrams in the corpus e g println current context they then create a short list of expected iterator transform ioexception append tostring token types if this list contains say a reference type the assertequals package names e g apache tools util tools use the rules of the type system to add a list of applicable java as well as language keywords e g import public type names to the list of completions similarly if a variable return this an examination of the tokens reveals why is expected the tools list visible names from the symbol the n grams approach adds most value with shorter tokens table as a final step both tools rank the completions with the language model we build is based on all the files a collection of apparently hand coded heuristics in the system and the most frequent n grams are those we complement this approach rather than using language that occur frequently in all the files in the corpus we semantics and immediate context to guess what might apply find that more frequently used tokens have shorter names our n gram model captures what most often does apply naturally these give rise to stronger signals that are picked our approach is much more flexible since it is language up by the n gram language model note that a significant independent and tolerates inchoate code it also has the portion viz of the successful suggestions are not potential to be much more precise the space of commonly java keywords guessed from language context they are used completions is naturally far smaller than the space of project specific tokens statistical language models thus language allowed completions note that our approach capture non language specific local regularity in each complements the current ide approach language based project guesses can be enhanced or ordered using corpus statistics in the table below we present another view of the benefit it is noteworthy that perhaps some of the strongest evidence of mse the total number of keystrokes saved by using for the naturalness of software is how much our n gram the base ecse first row the mse second row and the based suggestion engine improves eclipse language based percent gain from using mse engine section iv there are approaches arguably more advanced than those top top top currently available in ides the bmn completion algorithm ecse mse increase of bruch et al is focused on finding the most likely method calls that could complete an expression by using frequency of method calls and prior usage in similar circum stances we propose a broad vision for using language finally here we used one specific language model to enhance one specific software tool a suggestion engine with more sophisticated language models specifically ones that combines syntax scoping and type information we expect to achieve even lower entropy and thus better performance in this and other software tools models of code corpora in software tools our specific illustrative completion application has a broader completion goal completing all types of tokens not just method calls later bruch et al lay out a vision for next generation ides that take advantage of collective wisdom embodied in code bodies and recorded human action we enthusiastically concur with this vision our specific approach is that natural software has statistical regularities that allow techniques v r elated w ork from statistical nlp to be profitably applied in the endeavor a code completion and suggestion to make this vision a reality robbes and lanza compare a set of different method by completion we mean the task of completing a partially call and class name completion strategies which start with typed in token by suggestion we mean suggesting a complete a multi letter prefix they introduce an approach based token the discussion above concerned suggestion engines on history and show that it improves performance our d approach is complementary to theirs it can provide full token software mining completion of any token and is based a language model that exploits regularities in program corpora han et al uses hidden markov models hmm to infer likely tokens from short form tokens they make use of a dynamic programming trellis approach for backtracking and suggestion their hmm is in fact a language model but the paper does not describe how effective a model it is or how well it would perform for completion tasks without user provided abbreviations work in this very active area aims to mine useful information from software repositories many papers can be found in msr conference series at icse and representative works include mining api usages patterns of errors topic extraction guiding changes and several others the approaches used vary we argue that the naturalness of software provides a conceptual perspective for this work and also offers some novel jacob and tairas used n gram language models for a implementation approaches the conceptual perspective rests different application to find matching code clones relevant on the idea useful information is often manifest in software to partial programming task language models were built over clone groups not entire corpora as we propose and in uniform and uncomplicated ways the implementation approach indicates that the uniform and uncomplicated used to retrieve and present candidate clones relevant to a manifestation of useful facts can be determined from a partially completed coding task large representative software corpus in which the required hou and pletcher propose and evaluate several information is already known and annotated this corpus strategies for improving eclipse standard code completions can be used to estimate the statistical relationship between they focus their effort on one specific class of eclipse the required information and readily observable facts this completions method calls and they find that ranking calls relationship can be used to reliably find similar information in by frequency of past use is effective both this work and new programs similar to the corpus we explain this further our own completion prototype drive completions with usage in future work sections vi c and vi d data but our work is more general we use a much more general language model to predict and complete arbitrary vi f uture code not solely method calls and we propose many other potential applications as well b the naturalness of names in code this line of work aims to automatically evaluate if the names reflect the meanings of the artifacts and if not how could they be improved work by hst and stvold also concerns method naming they combine static analysis with an entropy based measure over the distribution of simple semantic properties of methods in a corpus to determine which method names are most discriminatory then use it to detect names whose usage are inconsistent with the corpus this work does not use language models to capture repetition in code c summarization and concern location this line of work aims to generate natural language descriptions summaries of code this work uses semantic properties of code derived by static analysis rather than using statistical models of the natural regularities of code it is complementary to ours properties derived by static analysis as long as they can be done efficiently and at scale could enrich statistical models of large software corpora another line of work seeks to locate parts of code relevant to a specified concern e g place auction bid which could be local or cross cutting based on fragments of code names facts mined from code or co occurrence of related words in code d irections we present now possible applications of corpus based statistical methods to aid software engineering tasks a improved language models in this paper we exploited a common language model n grams that effectively captures local regularity there are several avenues for extension existing very large bodies of code can be readily parsed typed scoped and even subject to simple semantic analysis all this data can be modeled using enhanced models to capture regularities that exist at syntactic type scope and semantic levels there is a difficulty here the richer a model the more data is needed to provide good estimates for the model parameters thus the risk of data sparsity grows as we enrich our models ideas analogous to the smoothing techniques used in n gram models will have to be adapted and applied to build richer models of software corpora still if these models do capture regularities they may then be employed for software engineering tasks some of which we discuss below b language models for accessibility some programmers have difficulty using keyboards be cause of rsi or visual impairment there has been quite a bit of work on aiding such programmers using speech recognition e g however these approaches suffer from fairly high recognition error rates and are not widely used none of the published approaches make use of a statistical language model trained on specific code corpora we hypothesize that the use of a language model compared can significantly reduce the error rates they certainly play to the cost or even infeasibility of determining a crucial role in conventional speech recognition engines these properties by sound or complete static analysis because a large proportion of development work occurs in for example the use of unprotected string functions like a maintenance or re engineering context language models strcat as opposed to strncat is evidence for a potential derived from existing code should improve the performance buffer flow but not conclusive proof as another example of these speech recognition systems even when only a small suppose related methods wherein the relatedness has amount of relevant code exists language model adaptation been detected using a recommender system open techniques could be applied using corpora of similar access close are called together in the same method with code the methods occurring in that textual order in the code and c summarizing and or retrieving code access occurring within a loop this is evidence albeit not conclusive that the methods are to be used with the protocol consider the task of summarizing code fragments or code open access close these are heuristics analogous to changes in english consider also the approximate reverse the probabilistic constraints used in merlin see livshits task finding retrieving a relevant fragment of code e g et al figure but where do they come from in method call given an english description we draw an merlin they are hard coded heuristics based on researchers analogy between these two problems and statistical natural intuitions we argue that they should be derived from corpus language translation snlt code and english are two level distribution models that make use of prior knowledge languages and essentially both the above are translation tasks about protocols already known to be used within those snlt relies on access to an aligned corpus which is a large corpora set of sentences simultaneously presented in two or more this admittedly is a leap of faith however if it holds languages e g proceedings of parliaments in canada and up and we have found anecdotal evidence that it does europe consider the problem of translating a tamil sentence and some prior research implicitly makes a version of this t to an english sentence e the translation process is primed assumption one can leverage this notion to build simple using an aligned english tamil corpus one estimates using scalable and effective approximations in a wide variety of the aligned corpus of e t pairs the conditional distribution settings we contend that the annotation of code corpora using a bayesian formulation of english output sentences e for instance api usage rules can be automated using data given tamil sentences t the translation process calculates mining techniques manually annotated code corpora may the most likely sentence e given a specific t be well worth the investment by analogy with the penn we propose to tackle the summarization retrieval task using tree bank and can be constructed using a volunteer statistical estimates derived from several corpora first we community or perhaps via market mechanisms like the use an aligned english code corpora built from multiple mechanical turk sources one source arises from the version history of a program each commit in a typical project offers a matched vii conclusion pair of a log message english and some changes code although linguists sometimes revel in the theoretical another source of aligned examples are in line comments complexities of natural languages most natural utterances that are clearly matchable with nearby code second we in practice are quite regular and predictable and can in fact can use any available english language text associated with be modeled by rigorous statistical methods this fact has a given project including code comments code design revolutionized computational linguistics we offer evidence documents bug reports discussions on mailing lists to supporting an analogous claim for software though software build a relevant english corpus finally models of the code in theory can be very complex in practice it appears and the associated english can be used select most likely that even a fairly simple statistical model can capture a translations surprising amount of regularity in natural software this d software tools simple model is strong enough for us to quickly and easily implement a fairly powerful suggestion engine that already we hypothesize that the naturalness of software implies improves a state of the art ide we also lay out a vision for a naturalness of deeper properties of software such as those future work specifically we believe that natural language normally computed by powerful but expensive software tools translation approaches can be used for code summarization as programmers tend towards repetitive use of code idioms and code search in a symmetric way we also hypothesize that we hypothesize that deeper more semantic properties of pro the naturalness of software implies a sort of naturalness grams also manifest themselves in programs in superficially of deeper properties of software such as those normally similar ways more specifically we hypothesize that semantic computed by powerful traditional software analysis tools properties usually manifest themselves in superficial ways these are challenging tasks but with potentially high pay off that are computationally cheap to detect particularly when and we hope others will join us in this work the articulation of the agile manifesto in a little over a decade ago has brought unprecedented changes to the software engineering field indeed the transformation that the manifesto has brought in its wake is quite remarkable it is hard to think of a decade in the twentieth century that has witnessed the intro duction of so many software methods tools techniques and best practices while this unparalleled growth has been readily accepted by many practitioners much work has still to be undertaken to bring coherence to the current discourse on agility as with any nascent discipline the early years of agile devel opment were marked by exuberance of a few and by scepticism among many a host of methods adhering to varying degrees to the tenets of the manifesto appeared on the landscape these include extreme programming xp scrum lean software development feature driven development fdd and crystal methodologies to name but a few broadly speaking all these methods endeavoured to address the core principles of the manifesto first there was a dis tinct move towards collaborative development with people being accorded privileges over processes that formerly constrained them second a dominant lean mentality was advocated with a view to minimizing unnecessary work particularly with regard to the cre ation of wasteful documentation while this was misconstrued by many to mean no documentation the discerning realized that this meant documenting only what was absolutely necessary and noth ing more third customers stakeholders were no longer just at the fringes of software development but actively shaped and guided the evolution of the end software product or service fourth there was an acceptance of the fact that uncertainty was a part and parcel of software development and that the inherent tendency to control variations through statistical and other means was futile see http agilemanifesto org open access under cc by nc nd license open access under cc by nc nd license after much discussion about the idiosyncrasies of the many methods that were proposed the conversation shifted to the rel ative merits of plan driven and agile methods the need to have a balanced approach the circumstances under which each would be more appropriate and so forth for example see boehm and turner in recent times the attention has been focused on issues related to managing the actual project agile planning control and estimation streamlining flow of stories e g kanban using lean six sigma and so forth most of these ideas have spawned a num ber of practices that are claimed to be efficacious but empirical validation of such assertions is lacking the early research on agile focused quite understandably on issues related to the adoption of agile methods e g boehm nerur et al and on the efficacy of pairs vis  vis indi viduals in software development nawrocki and wojciechowski williams et al other studies have investigated var ious aspects of team dynamics e g trust self organization and communication moe et al consequences of test driven development erdogmus et al janzen and saiedian adoption and post adoption issues cao et al mangalaraj et al challenges of implementing agile in distributed set tings ramesh et al and the like despite the copious research on agile software development and its ramifications one cannot help but sense a lack of a unified framework that brings coherence to the seemingly disparate streams of research being pursued clearly more work has to be done to articulate quintessential principles of agile software development that are at once unequivocal and useful for practice the goal of this special issue is to draw attention to this imperative and to present articles that could further our understanding of the myriad implications of agile software development the rest of the article is structured as follows in the next section we present an overview of research on agile software development specifically we examine publications and citations related to agile development to delineate the structure of the field subsequently we summarize prior research on agile followed by a brief account elsevier inc doi j jss the journal of systems and software contents lists available at sciverse sciencedirect the journal of systems and software jo u rn al hom epage www elsevier com locate jss a decade of agile methodologies towards explaining agile software development the journal of systems and software of the contributions made by the papers in this special issue finally the conclusions and directions for future research are discussed an overview of research on agile software development agile principles and agility according to the agile principles enunciated in the agile motivated and empowered software developers rely ing on technical excellence and simple designs create business value by delivering working software to users at regular short intervals these principles have spawned a number of practices that are believed to deliver greater value to customers at the core of these practices is the idea of self organizing teams whose members are not only collocated but also work at a pace that sustains their creativity and productivity the principles encour age practices that accommodate change in requirements at any stage of the development process furthermore customers or their surrogates are actively involved in the development process facil itating feedback and reflection that can lead to more satisfying outcomes the principles are not a formal definition of agility but are rather guidelines for delivering high quality software in an agile manner while individual principles and practices of agile development were not entirely new to the software community the way in which they were put together into a cogent theoret ical and practical framework was certainly novel williams and cockburn ever since the manifesto was articulated practi tioners and researchers have been trying to explicate agility and its different facets at its core agility entails ability to rapidly and flexibly create and respond to change in the business and techni cal domains henderson sellers and serour highsmith and cockburn other aspects of agility explored include light ness or leanness i e having minimal formal processes cockburn and related concepts such as nimbleness quickness dexter ity suppleness or alertness erickson et al in essence these ideas suggest a light methodology that promotes manoeuvrability and speed of response cockburn more formal definitions of agility have started to appear in the recent past drawn mainly from manufacturing and management domains where agile appears to have its roots for henderson sellers and serour agility involves both the ability to adapt to different changes and to refine and fine tune development processes as needed lee and xia define software develop ment agility as the software team capability to efficiently and effectively respond to and incorporate user requirement changes during the project life cycle conboy provides by far the most comprehensive definition of software development agility by systematically examining its various facets and definitions from related disciplines he makes a distinction between agility flexibil ity and leanness in fact agility is conceptualized to include and go beyond both flexibility and leanness while flexibility relates to the ability of a systems development method to create change or proactively reactively or inherently embrace change in a timely manner through its internal components and its relationships with its environment leanness captures the contribution to perceived customer value through economy quality and simplicity thus conboy p defines software development agility as the continued readiness to rapidly or inherently create change proac tively or reactively embrace change and learn from change while contributing to perceived customer value economy quality and simplicity through its collective components and relationships with its environment while leanness emphasizes cost reduction through eliminating waste and inefficiencies agility treats leanness i e cost reduction through waste elimination as a qualifier to focus more heavily on creating effective responses and valuable outcomes agarwal et al thus leanness may be perceived as efficiency oriented while agility entails embracing lean processes with an emphasis on realizing effective outcomes lyytinen and rose suggest that agility is achieved through learning processes involving both explo ration and exploitation the next section highlights the extent of research on agile development undertaken during the past decade across countries and across journals conferences research on agile software development a literature search in the isi web of identified research papers on agile software development that were pub lished between and inclusive as shown in fig the number of journal articles as well as conference papers has been steadily increasing until a plausible explanation for the decline in the number of conference publications in is that the agile conference was not indexed in isi web of science database the increase in journal articles indicates that the research field is maturing a systematic review of empirical research published before revealed a lack of theoretical and methodological rigor dyb and dingsyr the total num ber of publications shows that agile development has received much interest from the academic community however most of the research is inspired by practices emerging in industry our literature search also permitted us to examine the extent of agile research undertaken in different countries fig shows the number of publications by country darker colours indicating a larger volume of agile papers note that although the majority of the articles originate in the us canada and western europe agile software development has been a research theme on all continents in a total of countries see appendix for details we also tried to identify the popular conferences and journals in which publications on agile research appear it can be seen from fig that the international conference on agile software develop ment xp based in europe has been the main forum for agile research followed by in the us this is not surprising as the two conferences focus exclusively on issues related to agile software development other popular avenues for agile research are profes and eurospi both of which focus on process improve ment and the international conference on software engineering icse euromicro and the conference on software engineering edu cation and training csee t have also been able to attract some papers on agile finally the ifip community and the international conference of global software engineering icgse as well as the international symposium on empirical software engineering and mea surement esem have devoted some attention to this topic thus several research communities have focused on agile development a vast majority of the papers out of are from the top ten conferences the remaining articles are spread over about other forums this shows that agile development has received widespread attention across various scientific communities an overview of journal publications see fig reveals that ieee software has the largest number of papers followed by the jour nal of systems and software information and software technology and empirical software engineering the leading publisher of agile we used the following search topic agile development or agile software development or agile methodologies or agile methods or agile project man agement or lean development or lean software development or scrum or extreme programming or pair programming or test driven development refined by subject areas computer science or engineering or telecommu nications or operations research management science and document type proceedings paper or review or article or book chapter these numbers also include the previous agile development conference and the xp agile universe but not all years have had the proceedings indexed fig publications on agile software development from to total number top conference papers middle and journal articles bottom a decade of agile methodologies towards explaining agile software development the journal of systems and software fig publications on agile software development by country darker colour indicates more publications see appendix for details fig number of papers by conference a decade of agile methodologies towards explaining agile software development the journal of systems and software fig number of papers in scientific journals articles among non software engineering journals is the european journal of information systems thanks to a special issue on the topic yet another popular outlet communications of the acm has pub lished five articles on agile software development thus we see that the topic has gained traction not just in software engineering but in other areas as well seminal contributors and their relationships there is perhaps no better way to understand a field than to identify its seminal sources of information and the relation ships among them indeed experts in bibliometric studies such as mccain white and griffith and white have elaborated on the use of co citation analysis to delineate the con ceptual underpinnings of disciplines specifically researchers have used either authors e g nerur et al or documents e g ramos rodrguez and ruz navarro as the units of anal ysis whatever be the case the unit of analysis author or the article written by the author is regarded as a concept or con cepts that it promulgates co citation analysis whether it is based on co citations of authors or documents rests on the premise that joint citations between two units occur when they share some conceptual similarity this study used author co citation analy sis aca to unravel dominant conceptual themes in the agile literature the procedure followed is very consistent with those outlined by mccain and nerur et al the first step was to iden tify the authors most frequently cited in the agile literature isi web of science was used for this purpose our yielded articles the cited references of each of these articles were then used to get a list of the most frequently cited authors fifty one same search as in endnote but limited to articles or reviews authors were included in our final analysis a matrix of raw co citation frequencies was then computed based on a cited references search involving each of the authors specifically the co citation frequency between a pair of authors was obtained by determining the number of matching records in their respec tive cited references the input to the cluster analysis program was a correlation matrix derived from the matrix of raw co citation counts consistent with other aca studies we used the ward method the results are shown in fig fig shows the key conceptual themes that have appeared in the agile literature distances in the figure provide a sense of the level of conceptual similarity between two authors with shorter distances implying greater thematic closeness in their writings for example the short linkage distance between fowler and gamma both of whom have written about patterns implies that their works are highly related as mentioned earlier the formative years of the field saw the proliferation of research related to the differ ences between process oriented approaches such as cmm cmmi and agile methods such as xp further there were extensive debates on topics related to reconciling differences between agile and erst while practices striking a balance between traditional and agile using a risk driven approach to choosing practices and so forth the presence of pair programming and test driven development is no surprise since a lot of research has been devoted to these prac tices sharp and robinson distinguish themselves by their work on xp particularly in the context of organizational culture distributed cognition and the role of physical artefacts in agile development see sharp and robinson sharp et al the influ ence of patterns both analysis and design patterns based on the works of fowler and gamma is also evident in the analysis robert martin book on agile software development shows the role of pat terns in agile development hence the link among martin fowler and gamma the next section identifies the various theoretical per spectives used in prior agile research a decade of agile methodologies towards explaining agile software development the journal of systems and software baskerville fitzgerald paulk yin case stud y method olo gy table theoretical perspectives used in agile research kitchenham erdogmus mcdowell humphrey nawrocki robinson arisholm madeyski demarco canfora williams lindvall jeffries george brooks fenton layman muller wohlin boehm reifer nosek sharp glass basili dyba lui tradi onal so ftwa re eng ineering cmm proj ect mana gement software es ma on pair dev elopm ent distribute d co gni on theoretical perspective number of articles abrahamsson poppendieck schwaber stephens cohn salo agile method beck fowler gamma pa erns fig key research themes in agile software development cluster analysis of sem inal authors using the ward method theoretical exploration of agile development agile development evolved from the personal experiences and collective wisdom of the consultants and thought leaders of the software community while most individual agile practices have intuitive appeal as they are based on generally accepted management principles they certainly lacked theoretical under pinnings or empirical support for their stated benefits when initially conceived thus there was a pressing need for theoretical perspectives that throw light on how these disparate practices and their interactions could produce valued outcomes theoretically comprehending the distinction between agile methods and their traditional counterparts was another concern begging for research attention some early studies sought to address these concerns for instance nerur and balijepally illustrate that the shifts in the approach to software development reflected in the agile methods has parallels to similar shifts in design thinking evident in several disparate fields e g architecture and strategic management domains they suggested the theory of holographic organization and its various principles as a theoretical lens to explore agile development to get a sense of the various theoretical perspectives used in studies on agile development during the last decade we did a quick analysis of the topic search of the articles published between and that we identified earlier for author co citation analysis using isi web of science we searched through the top ics of these articles using keywords outlined in table the various theoretical perspectives and the number of articles identi fied by isi web of science using these terms in the topic results are showcased in table an obvious limitation of the approach used for identifying these articles relates to the keywords used which are based on our understanding of the popular theoreti cal perspectives found in agile studies and the search procedure article knowledge management dingsyr and hanssen holz and maurer sena and shan doran fang et al bellini et al crawford et al salazar torres et al chan and thong personality sfetsos et al choi et al layman et al sfetsos et al acuna et al hannay et al organizational learning holz and maurer cockburn highsmith palmer double loop learning mcavoy and butler triple loop learning mcavoy and butler constantine kruchten larman ambler martin turk user cent ere d desi gn agil e me thodolo gies complex adaptive systems meso and jain socha and walter social facilitation arisholm et al balijepally et al meyer back adaptive structuration theory distan ces cao et al chaos theory levardy and browning complexity theory falessi et al coordination theory pikkarainen et al distributed cognition sharp and robinson evolutionary theory of knowledge northover et al fuzzy set theory mafakheri et al game theory hazzan and dubinsky graph theory zimmer socio technical johannessen and ellingsen teamwork model moe et al theory of diagnosis trinidad et al adopted here i e searching in the topic field of isi web of sci ence database however we believe this provides a broad sense of the various theoretical perspectives used in agile studies and their relative popularity as is evident from table knowledge management personal ity and organizational learning and related perspectives have been more popular with agile researchers as software development is a knowledge creation activity knowledge management should be an attractive perspective when exploring knowledge genera tion in software teams in general and agile teams in particular similarly personality theories e g big five personality theory should be useful in exploring the interpersonal dynamics of col located agile teams and programming pairs as agile principles of change readiness and adaptability are expected to foster a learn ing environment in agile teams organizational learning and related perspectives would be a logical choice for researchers when explor ing learning outcomes of agile development as is also apparent from table other theoretical perspectives have been used to a much lesser extent most importantly a majority of agile studies do not seem to be concerned about any theoretical underpinnings for their research exploration which reinforces the general popular perception that agile research tends to be a theoretical towards a theory of agile software development in this section we first describe the state of the art in agile development and then proceed to place this special issue in con text a decade of agile methodologies towards explaining agile software development the journal of systems and software an overview of prior research introductions to and overviews of agile development are pro vided by abrahamsson et al cohen et al erickson et al and dyb and dingsyr these four reports describe the state of the art and state of the practice in terms of characteristics of the various agile methods and lessons learned from applying such methods in industry in addition the book agile software development current research and future directions dingsyr et al contains eleven overviews of the main streams within agile research structured in chapters explaining foundations and background of agile development agile methods in practice and principal challenges and new frontiers from until five special issues and one special section on agile software development have been published including articles the most common agile methods described were xp and scrum an examination of these special issues revealed that most articles were devoted to furthering our understanding of agile con cepts other dominant topics included adoption and or adaptation of agile reconciliation of the tension between agile and plan driven development i e flexibility and control and evaluation of adop tion issues in environments that are not inherently conducive to agile we will now give a short summary of these special issues and the special section in williams and cockburn edited a special issue in computer titled agile software development it about feed back and change the primary emphasis of the special issue was on determining how to blend agile methodologies with plan driven approaches to software development the six articles included cov ered the history of iterative and incremental development the debate on mixing agile and plan driven development and how and when to mix these two approaches furthermore the special issue reported experience on the use of xp and scrum as well as on intro ducing agile processes into an organization working in an iso or cmmi environment the second special issue appeared in the journal of database management in siau in addition to a review of the state of research on xp and agile methods the issue covered top ics related to adoption of agile methods process improvement xp and the underlying assumptions of agile in abrahamsson et al selected seven articles for a special issue in the european journal of information systems to further our understanding of various phenomena in agile system development the title of the special issue editorial was lots done more to do the current state of agile systems development research the papers not only addressed the fundamental question of what constitutes agility and agile methods but also demon strated approaches to broadening the scope of the applicability of agile concepts gerfalk fitzgerald and slaughter also edited a special issue in gerfalk et al seven papers were published in the information systems research under the banner flexible and dis tributed is development state of the art and research challenges the papers attempted to explore and or define the central con cept of agility the enablers and inhibitors of agility the question of how to balance flexibility and control the circumstances under which agile methods are most effective and the challenges of agile in distributed projects yet another special issue was published in software practice and experience which was edited by greer and hamon the papers addressed a range of research areas including the appli cation of agile methods to safety critical software development the relationship between agile development and user experience design and the measurement of flow in lean software development finally in dyb edited a special section in the jour nal of information and software technology based on best papers from the conference the four articles that were pub lished described the relationship between organizational culture and development of agile methods customer involvement in agile projects self management and the evolution of the practice of agile information systems development within a company over a year period this special issue for this special issue we asked for contributions that crit ically reflect on the current status of research and practice in agile development in particular we were looking for contributions questioning and exploring the theoretical underpinnings of agile and lean development and the agile manifesto we received a total of submissions of which five were selected for the special issue three of these articles focus on specific aspects of agile practices coordination decision making and post adaptive use while the last two articles provide insight on broad topics of agile develop ment a grounded theory of software development and a review of experience reports on lean and agile software development we describe these contributions in more detail below in their article coordination in co located agile software devel opment projects strode huff and hope link agile development to theory of coordination using a model with three components synchronization structure and boundary spanning decision making an important aspect of software development is the focus of drury conboy and power article obstacles to decision making in agile software development teams using a mixed method approach they investigate decisions involved in iteration planning execution review and retrospective and iden tify six obstacles to decision making they connect the findings to a theory of descriptive decision making and describe the effects of these obstacles senapathi and srinivasan focus on the use of agile development methods in the post adoption stage in their article understanding post adoptive agile usage an exploratory cross case analysis by adapting theories from systems development and diffusion of innovations they develop a model that seeks to explain post adoptive usage of agile practices in the article reconciling perspectives how people man age the process of software development adolph and kruchten develop a grounded theory of social factors in software develop ment they conceptualize software development as a negotiation process that involves reconciling perspectives i e seeking conver gence by sorting out different points of view or perspectives about a software process thus it offers a unique perspective on how agile software development is undertaken in organizations a growing interest is evident at agile conferences on iden tifying ways to combine principles of lean development with software development in the article leagile software develop ment an experience report analysis of the application of lean approaches in agile software development wang conboy and cawley distil lessons from experience reports in six types of lean applications from practices for continuous process improvement to flow based development with the kankan approach conclusion it should be apparent from this introductory article that the research community has lavished attention on the issues related to agile software development ever since the agile manifesto was pronounced in this is evident from the number of scientific publications the widespread interest in the topic in various sci entific forums and the number of countries that have been engaged in agile research the number of special issues devoted a decade of agile methodologies towards explaining agile software development the journal of systems and software to agile development is also an indication of the keen interest dis played in software engineering and other related fields notably information systems a systematic review of empirical studies published until dyb and dingsyr called for an increase in both the num ber and quality of studies the review also found that most studies focused on extreme programming and very few on the scrum development process which was gaining significant traction in industry further the review showed the urgent need for more studies involving mature agile development teams as most stud ies until then had focused on projects that were just starting to use agile methods has recent progress brought us any closer to a unified frame work that brings coherence to the seemingly disparate streams of research being pursued our overview of research shows that the number of studies has increased significantly since and the increased number of journal articles not just the increased num ber of conference proceedings is a sign of increase in quality as well going by the attention that they have received some subfields of agile development appear to be more mature than others for example there are meta studies summarizing experiments on pair programming with focus on effectiveness dyb et al and on use in university education salleh et al our overview of theories in use in explaining agile software development shows that a range of theories drawn from many fields have been applied this special issue is a further contribution with five articles with strong focus on theory after the initial spurt of studies on extreme programming the academic community seems to have turned its attention to scrum flow based as well as lean software develop ment has been popular among industry practitioners but has not yet been extensively researched as the article by wang et al in this issue shows many have called for directions to research on agile develop ment at dingsyr et al suggested a roadmap for research on agile software development focusing on provid ing more empirical research primarily on experienced agile teams and organizations connecting better to existing streams of research in more established fields giving more attention to management oriented approaches and finally give more emphasis to core ideas in agile software development in order to increase our understand ing gerfalk et al in their introductory article to the spe cial issue on agile distributed systems development compiled a top ten list of future research areas in the list developed from the inputs of the authors of the special issue articles the following research areas figured at the top suitability of agile develop ment to new context such as open source software and software as service factors affecting the organizational adaptation of agile methods including tailoring to specific projects different forms of distributed development and factors facilitating the flexibility efficiency and effectiveness of such work ways to extend agile practices beyond software teams into the organizational realm and identifying boundaries to agile development by applying agility to projects traditionally considered to be non agile at the conference freudenberg and sharp com piled a list of top ten burning questions based on feedback from practitioners among other issues they identified agile and large projects barriers to self organization distributed agile and the role of architecture as high priority topics at a workshop on new and emerging ideas at we posed the question what should be researched less and what should be researched further to a group mainly consisting of academics among other things they opined that pair programming in educational settings and the reuse of code did not require any further attention themes that were deemed to be important included agile across projects and across organizations the core of agile distributed agile and the role of architecture and knowledge management in agile develop ment we concur that these are exciting research areas that can fur ther our understanding of the effectiveness of agile methods and practices particularly in different project organizational contexts however our limited analysis of the theoretical perspectives used in prior agile development research suggests that not enough attention is being paid to establishing theoretical underpinnings when investigating agile development and its various practices as jacobson and spence point out sound theoretical roots help us glean the essential concepts or the truths of software devel opment that are methodology independent such theory driven research enables us to separate true innovations among agile prac tices from the reinventions and remixes of old approaches thereby helping us adopt such innovations at a faster rate in the future therefore we urge agile researchers to embrace a more theory based approach in the future when inquiring into these promising research areas of agile development clearly the pioneers as well as subsequent researchers of agile development have established a foundation on which the edifice of software development theory and practice can be built as we stand on the shoulders of these giants and endeavour to extend the frontiers of software engineering it is important to remember that the field can mature and progress as a scientific discipline only if efforts are made to provide a robust theoretical scaffold for the conduct of research on agile development we hope that the articles in this special issue are a step in this direction program evolution and repair are major components of software maintenance which consumes a daunting fraction of the total cost of software production automated techniques to reduce their costs are therefore especially beneficial developers for large software projects must con firm triage and localize defects before fixing them and validating the fixes although there are a number of tools available to help with triage e g localization e g validation e g and even confirmation e g generating repairs remains a predominantly manual and thus expensive process at the same time cloud computing in which virtualized processing power is purchased cheaply and on demand is becoming commonplace research in automated program repair has focused on reducing defect repair costs by producing candidate patches for validation and deployment recent repair projects include clearview which dynamically enforces invariants to patch overflow and illegal control flow transfer vulnera bilities autofix e which can repair programs anno tated with design by contract pre and post conditions and afix which can repair single variable atomicity viola tions in previous work we introduced genprog a general method that uses genetic programming gp to repair a wide range of defect types in legacy soft ware e g infinite loops buffer overruns segfaults integer overflows incorrect output format string attacks without requiring a priori knowledge specialization or specifica tions genprog searches for a repair that retains required functionality by constructing variant programs through com putational analogs of biological processes the goal of this paper is to evaluate dual research ques tions what fraction of bugs can genprog repair and how much does it cost to repair a bug with genprog we combine three important insights to answer these questions our key algorithmic insight is to represent candidate repairs as patches rather than as abstract syntax trees these changes were critical to genprog scalability to millions of lines of code an essential component of our evaluation we introduce new search operators that dovetail with this representation to reduce the number of ill formed variants and improve performance our key performance insight is to use off the shelf cloud computing as a framework for exploiting search space parallelism as well as a source of grounded cost measurements our key experimental insight is to search version control histories exhaustively focusing on open source c programs to identify revisions that corre spond to human bug fixes as defined by the program most current test suite we combine these insights and present a novel scalable approach to automated program repair based on gp and then evaluate it on real world defects taken from open source projects totaling mloc and including test cases the main contributions of this paper are genprog a scalable approach to automated program repair based on gp new gp representation mutation and crossover operators allow genprog to scale to large programs and take advantage of cloud computing 4673 c ieee icse zurich switzerland parallelism we evaluate directly against our previous approach on its own benchmarks and find that the improved algorithm finds repairs more often a systematic evaluation of genprog on defects from mloc of open source projects equipped with test cases we generate a benchmark set by exhaustively searching inclusive version ranges to help address generalizability concerns all reproducible bugs in a time window are considered and all defects considered were important enough for developers to test for and fix manually this evaluation includes two or ders of magnitude more source code than autofix e or our previous work two orders of magnitude more test cases than clearview and two orders of magnitude more defects than afix in addition to being strictly larger than each of those four previous projects on each of the those metrics separately results showing that genprog repairs of those defects because our experiments were conducted using cloud computing and virtualization any organization could pay the same rates we did and reproduce our results for or per successful run on this dataset a successful repair takes minutes of wall clock time on average while an unsuccessful run takes hours including cloud instance start up times ii m otivation this section motivates automated program repair and identifies monetary cost success rate and turnaround time as important evaluation metrics the rate at which software bugs are reported has kept pace with the rapid rate of modern software development in one mozilla developer noted everyday almost bugs appear far too much for only the mozilla programmers to handle p bugzilla mozilla org gives similar bug report numbers for since there are not enough developer resources to fix all defects programs ship with both known and unknown bugs in light of this problem many companies have begun offering bug bounties to outside developers paying for candidate repairs well known companies such as and offer significant rewards for security fixes with bounties raising to thousands of dollars in bidding wars although security bugs command the highest prices more wide ranging bounties are available consider tarsnap com an online backup provider over a four month period tarsnap paid for fixes for issues ranging from cos metic errors e g typos in source code comments to gen eral software engineering mistakes e g data corruption to www mozilla org security bug bounty html bug blog chromium org encouraging more chromium security html bug www computerworld com article google calls raises mozilla bug bounty for chrome flaws www tarsnap com bugbounty html input full fitness predicate fullfitness patch b input sampled fitness samplefit patch r input mutation operator mutate patch patch input crossover operator crossover patch input parameter popsize output patch that passes fullfitness let pop map mutate over popsize copies of repeat let parents tournselect pop popsize samplefit let offspr map crossover over parents pairwise pop map mutate over parents offspr until candidate pop fullfitness candidate return candidate figure high level pseudocode for the main loop of our technique security vulnerabilities of the approximately candidate patches submitted to claim various bounties about addressed spelling mistakes or style concerns while about addressed more serious issues classified as harmless or minor one issue was classified as major developers at tarsnap confirmed corrections by manually evaluating all submitted patches if we treat the non trivial repairs as true positives and the trivial reports as overhead tarsnap paid an average of for each non trivial repair and received one about every hours despite the facts that the bounty pays a small amount even for reports that do not result in a usable patch and that about of all non trivial submissions fixed harmless bugs the final analysis was worth the money every penny bug bounties suggest that the need for repairs is so pressing that companies are willing to pay for outsourced candidate patches even though repairs must be manually reviewed most are rejected and most accepted repairs are for low priority bugs these examples also suggest that relevant success metrics for a repair scheme include the fraction of queries that produce code patches monetary cost and wall clock time cost we now present an automated approach to program repair with a use case similar to that of the outsourced bug bounty hunters the method is powerful enough to fix over half of the defects it tackles and we evaluate it using these and other metrics iii a utomated r epair m ethod in this section we describe genprog an automated pro gram repair method that searches for repairs to off the shelf programs we highlight the important algorithmic and representational changes since our preliminary work that enable scalability to millions of lines of code improve performance and facilitate implementation on a cloud com puting service a genetic programming genprog uses genetic programming gp an it erated stochastic search technique to search for program www daemonology net blog 1265 dollars of tarsnap bugs html repairs the search space of possible repairs is infinitely large and genprog employs five strategies to render the search tractable coarse grained statement level patches to reduce search space size fault localization to focus edit locations existing code to provide the seed of new repairs fitness approximation to reduce required test suite evaluations and parallelism to obtain results faster high level pseudocode for genprog main gp loop is shown in figure it closely resembles previous work fitness is measured as a weighted average of the positive i e initially passing encoding required functionality and negative i e initially failing encoding a defect test cases the goal is to produce a candidate patch that causes the original program to pass all test cases in this paper each individual or variant is represented as a repair patch stored as a sequence of ast edit operations parameterized by node numbers e g replace see section iii b given a program and a test suite i e positive and negative test cases we localize the fault section iii d and compute context sensitive information to guide the search for repairs section iii e based on program structure and test case coverage the functions samplefit and fullfitness evaluate variant fitness section iii c by applying candidate patches to the original program to produce a modified program that is evaluated on test cases the operators mutate and crossover are defined in section iii f and section iii g both generate new patches to be tested the search begins by constructing and evaluating a pop ulation of random patches line of figure initializes the population by independently mutating copies of the empty patch lines correspond to one iteration or generation of the algorithm on line tournament selection selects from the incoming population with replacement parent individuals based on fitness by analogy with genetic crossover events parents are taken pairwise at random to exchange pieces of their representation two parents produce two offspring section iii g each parent and each offspring is mutated once section iii f and the result forms the incoming population for the next iteration the gp loop terminates if a variant passes all test cases or when resources are exhausted i e too much time or too many generations elapse we refer to one execution of the algorithm described in figure as a trial multiple trials are run in parallel each initialized with a distinct random seed the rest of this section describes additional algorithmic details with emphasis on the important improvements on our preliminary work including a new patch based representation large scale use of a sampling fitness function at the individual variant level fix localization to augment fault localization and novel mutation and crossover operators to dovetail with the patch representation b patch representation an important genprog enhancement involves the choice of representation each variant is a patch represented as sequence of edit operations cf in the original algo rithm each individual was represented by its entire abstract syntax tree ast combined with a weighted execution path which does not scale to large programs in the cloud computing setting for example for at least of the defects considered in this paper a population of asts did not fit in the gb of main memory allocated to each cloud node in our dataset half of all human produced patches were lines or less thus two unrelated variants might differ by only lines with all other ast nodes in common representing individuals as patches avoids storing redundant copies of untouched lines this formulation influences the mutation and crossover operators discussed below c fitness evaluation to evaluate the fitness of a large space of candidate patches efficiently we exploit the fact that gp performs well with noisy fitness functions the function samplefit applies a candidate patch to the original program and eval uates the result on a random sample of the positive tests as well as all of the negative test cases samplefit chooses a different test suite sample each time it is called fullfitness evaluates to true if the candidate patch when applied to the original program passes all of the test cases for efficiency only variants that maximize samplefit are fully tested on the entire test suite the final fitness of a variant is the weighted sum of the number of tests that are passed where negative tests are weighted twice as heavily as the positive tests d fault localization genprog focuses repair efforts on statements that are visited by the negative test cases biased heavily towards those that are not also visited by positive test cases for a given program defect set of tests t test evaluation function pass t b and set of statements visited when evaluating a test visited t p stmt we define the fault localization function faultloc stmt r to be faultloc t t visited t t t visited t pass t otherwise that is a statement never visited by any test case has zero weight a statement visited only on a bug inducing test case has high weight and statements covered by both bug inducing and normal tests have moderate weights this strategy follows previous work sec on the defects considered here the total weight of possible fault locations averages other fault localization schemes could potentially be plugged directly into genprog e fix localization we introduce the term fix localization or fix space to refer to the source of insertion replacement code and ex plore ways to improve fix localization beyond blind random choice as a start we restrict inserted code to that which includes variables that are in scope at the destination so the result compiles and that are visited by at least one test case because we hypothesize that certain common behavior may be correct for a given program and defect we define the function fixloc stmt p stmt as follows fixloc d t t visited t varsused inscope d the previous approach chose an ast node randomly from the entire program as a result an average of of generated variants did not compile usually due to type checking or scoping issues for larger programs with long compilation times this is a significant overhead for the defects considered here less than of the variants failed to compile using the fix localization function just defined f mutation operator earlier work used three types of mutation delete insert and swap however we found swap to be up to an order of magnitude less successful than the other two tab we thus remove swap in favor of a new operator replace equivalent to a delete followed by an insert to the same location in a single mutation a destination statement d is chosen from the fault localization space randomly by weight with equiprobability genprog either deletes d i e replaces it with the empty block inserts another source statement before d chosen randomly from fixloc d or replaces d with another statement chosen randomly from fixloc d as in previous work inserted code is taken ex clusively from elsewhere in the same program this decision reduces the search space size by leveraging the intuition that programs contain the seeds of their own repairs g crossover operator the crossover operator combines partial solutions help ing the search avoid local optima our new patch subset crossover operator is a variation of the well known uniform crossover operator tailored for the program repair domain it takes as input two parents p and q represented as ordered lists of edits section iii b the first resp second offspring is created by appending p to q resp q to p and then removing each element with independent probability one half this operator has the advantage of allowing parents that both include edits to similar ranges of the program e g parent p inserts b after a and parent q inserts c after a to pass any of those edits along to their offspring previous uses of a one point crossover operator on the fault localization space did not allow for such recombination e g each offspring could only receive one edit to statement a table i subject c programs test suites and historical defects tests were taken from the most recent version available in may defects are defined as test case failures fixed by developers in previous versions program loc tests defects description fbc legacy coding gmp precision math gzip data compression libtiff image processing lighttpd web server php web programming python general coding wireshark packet analyzer total iv e xperimental s etup this section describes how we selected a set of subject programs and defects for our systematic evaluation and it describes the parameter settings used for the experiments a subject programs and defects our goal was to select an unbiased set of programs and defects that can run in our experimental framework and is indicative of real world usage we required that subject programs contain sufficient c source code a version control system a test suite of reasonable size and a set of suitable subject defects we only used programs that could run without modification under cloud computing virtualization which limited us to programs amenable to such environ ments we required that subject defects be reproducible and important we searched systematically through the pro gram source history looking for revisions that caused the program to pass test cases that it failed in a previous revision such a scenario corresponds to a human written repair for the bug corresponding to the failing test case this approach succeeds even in projects without explicit bug test links and it ensures that benchmark bugs are important enough to merit a human fix and to affect the program test suite table i summarizes the programs used in our experiments we selected these benchmarks by first defining predicates for acceptability and then examining various program reposi tories to identify first acceptable candidate programs that passed the predicates and second all reproducible bugs within those programs identified by searching backwards from the checkout date late may the next subsection formalizes the procedure in more detail b selecting programs for evaluation a candidate subject program is a software project contain ing at least lines of c code viable test cases and versions in a revision control system we consider all viable versions of a program defined as a version that checks out and builds unmodified on bit fedora linux a lowest common denominator os available on the cloud computing framework a program builds if it produces its primary executable regardless of the exit status of make we define test cases to be the smallest atomic testing units for which individual pass or fail information is available for example if a program has major areas which each contain minor tests and each minor test can pass or fail we say that it has test cases we define a viable test case as a test that is reproducible non interactive and deterministic in the cloud environment over at least trials testsuite i denotes the set of viable test cases passed by viable version i of a program we use all available viable tests even those added after the version under consideration we exclude programs with test suites that take longer than one hour to complete in the cloud environment we say that a testable bug exists between viable versions i and j of a subject program when testsuite i testsuite j and there is no i i or j j with the testsuite j testsuite i testsuite j testsuite i and the only source files changed by developers to reach version j were c h y or l the second condition requires a minimal i j the set of positive tests i e encoding required behavior is defined as testsuite i testsuite j the negative tests i e demon strating the bug are testsuite j testsuite i note that the positive and negative tests are disjoint given a viable candidate subject program its most recent test suite and a range of viable revisions we construct a set of testable bugs by considering each viable version i and finding the minimal viable version j if any such that there is a testable bug between i and j we considered all viable revisions appearing before our start date in late may as a potential source of testable bugs however we capped each subject program at defects to prevent any one program from dominating the results given these criteria we canvassed the following sources the top c foundry programs on sourceforge net the top c programs on google code the largest non kernel fedora source packages programs in other repair papers or known to the authors to have large test suites many otherwise popular projects failed to meet our crite ria many open source programs have nonexistent or weak test suites opaque testing paradigms non automated gui testing or are difficult to modularize build and reproduce on our architecture e g eclipse firefox ghostscript handbrake openjpeg openoffice for several programs we were unable to identify any viable defects according to our definition e g gnucash openssl some projects e g bash cvs openssh have inaccessible or unusably small version control histories other projects were ruled out by our test suite time bound e g gcc glibc subversion some projects have many revisions but few viable ver sions that compile and run against recent test cases e g valgrind earlier versions of certain programs e g gmp and require incompatible versions of automake libtool the set of benchmark programs and defects appears in table i the authors acknowledge that it is not complete and that other additions are possible while it is certainly best effort to our knowledge it also represents the most systematic evaluation of automated program repair to date c experimental parameters we ran genprog trials in parallel for each bug we chose popsize and a maximum of generations for consistency with previous work sec each indi vidual was mutated exactly once each generation crossover is performed once on each set of parents and of the population is retained with mutation on each generation known as elitism each trial was terminated after generations hours or when another search found a repair whichever came first samplefit returns of the test suite for all benchmarks we used amazon cloud computing infrastructure for the experiments each trial was given a high cpu medium medium instance with two cores and gb of memory simplifying a few details the virtualization can be purchased as spot instances at per hour but with a one hour start time lag or as on demand instances at per hour these august september prices summarize cpu storage and i o charges v e xperimental r esults this section reports and analyzes the results of running genprog on our benchmark suite of defects we address the following questions how many defects can genprog repair and at what cost section v a what determines the success rate section v b what is the impact of alternative repair strategies section v c how do automated and human written repairs com pare section v d a how many defects can genprog repair table ii reports results for defects in mloc from subject programs genprog successfully repaired of the defects including at least one defect for each subject program the non repairs met time or generation limits before a repair was discovered we report costs in terms of monetary cost and wall clock time from the start of the request to the final result recalling that the process terminates as soon as one parallel search finds a repair results are reported for cloud computing spot instances and thus include a one hour start lag but lower cpu hour costs for example consider the repaired fbc defect where one of the ten parallel searches found a repair after aws amazon com instance types aws amazon com pricing table ii repair results of the defects were repaired successfully and are reported under the cost per repair columns the remaining are reported under the non repair columns hours columns report the wall clock time between the submission of the repair request and the response including cloud computing spot instance delays us columns reports the total cost of cloud computing cpu time and i o the total cost of generating the results in this table was defects cost per non repair cost per repair program repaired hours us hours us fbc gmp gzip libtiff 04 04 lighttpd php 84 python wireshark total 60h wall clock hours this corresponds to hours of cloud computing cpu time per instance the total cost for the entire bug repair effort for that to repair that defect is thus hours hour 08 see section iv c the successful repairs return a result in hours each on average the unsuccessful repairs required hours each on average unsuccessful repairs that reach the generation limit as in the first five benchmarks take less than hours the total cost for all attempted repairs is or per successful run these costs could be traded off in various ways for example an organization that valued speed over monetary cost could use on demand cloud instances reducing the average time per repair by minutes to minutes but increasing the average cost per successful run from to table ii does not include time to minimize a repair an optional deterministic post processing step this step is a small fraction of the overall cost we view the successful repair of of defects from programs totaling million lines of code as a very strong result for the power of automated program repair similarly we view an average per repair monetary cost of as a strong efficiency result b what determines the success rate this section explores factors that may correlate with genprog success in repairing a given defect we first quantitatively analyze the algorithmic changes we made to genprog program representation and genetic operators we next investigate the relationship between genprog success and defect complexity using several external metrics includ ing developer reported defect severity and the number of files touched by developers in a repair we also consider internal metrics such as localization size representation and genetic operators we compare our new representation and operators to the previous ap proach using the benchmarks from first to allow table iii new algorithm the final column reports the ratio of successful repairs found by our enhanced algorithm to those found by the originally published algorithm on that work benchmarks higher is better program fault loc ratio of repairs found gcd infinite loop uniqw utx segfault look utx segfault look svr infinite loop units svr segfault deroff utx segfault nullhttpd buffer exploit indent infinite loop flex segfault atris buffer exploit average for a direct comparison and second because the previous approach does not scale to our new benchmarks we held population size number of generations mutation rate and fault localization strategy constant changing only the inter nal representation and genetic operators we ran random repair trials per benchmark success rate is the number of trials that find a repair as in fig table iii shows results the new representation outper formed the old on all benchmarks except atris where success drops slightly and look where both approaches succeed on all trials averaged over these benchmarks the new representation allows genprog to find repairs more frequently than the original method this result is consistent with our hypothesis that the new representation would enable a more efficient search for solutions correlating repair with external metrics one con cern is that genprog might succeed only on unimportant or trivial bugs we investigated this hypothesis by ana lyzing the relationship between repair success and external metrics such as human time to repair human repair size and defect severity with one exception we were unable to identify significant correlations with these external metrics we manually inspected version control logs bug databases and associated history to link defects with bug reports although all of our benchmarks are associated with source control and bug tracking databases not all defect associated revisions could be linked with a readily available bug report we identified publicly accessible bug or security vulnerability reports in out of of our cases all bug reports linked to a defect in our benchmark set were eventually marked confirmed by developers we measure developer time as the difference between when the bug report was marked assigned and when it was closed which we know is a rough approximation we extracted developer reported defect severities on a scale we assigned php security bug reports marked private a severity of ultimately we identified severity information for of the defects results on this subset are comparable to those on the full dataset genprog repaired of the defects associated with bug reports and of the associated with severity ratings we investigated both linear and non linear relationships between repair success and search time and the external metrics correlation values are pearson unless otherwise noted we found a significant correlation in only one case the number of files touched by a human generated patch is slightly negatively correlated with genprog success r p the more files the humans changed to address the defect the less likely genprog was to find a repair although we note that the correlation is not very strong we were unable to identify a significant relationship between either human time to repair or human patch size in diff lines and genprog repair success we found no significant correlation between bug report severity and genprog ability to repair exploring fur ther we found no significant difference between the mean severity of repaired and unrepaired defects student t test and wilcoxon rank sum test at  these results suggest that the defects that genprog can and those that it cannot repair are unlikely to differ in human provided sever ity we note that no defect associated with a severity report has lower than normal priority in our scheme recall that by definition our dataset restricts attention to bugs important enough for developers to fix see section iv a correlating repair with internal metrics we define the space of possible program repairs by both the fault section iii d and fix section iii e space previous work reported that the time to repair scaled roughly linearly with the size of the weighted path or fault localization size fig fix space size has not been previously studied we find a statistically significant though not very strong relationship between the log of the fault weight and repair success r p as well as the log of the number of fitness evaluations to repair r p as fault space size increases the probability of repair success decreases and the number of variants evaluated to a repair increases this result corroborates our previous findings we additionally find a significant negative correlation between the log of the fix space size and the log of the number of fitness evaluations required to find a repair r p one possible explanation for these results is that while bad fault localization can preclude a repair e g the variable x must be zeroed just before this function call imprecise fix localization may make it difficult but still possible e g there are many ways to set x to without using x a larger fix space may include more candidate repair options reducing the time to find any one even if it does not appear to correlate with actual success c what is the impact of alternative repair strategies in this subsection we evaluate two alternative repair strate gies searching for multiple repairs and using annotations table iv a lternate defect repair results unique patches counts the number of distinct post minimization patches produced if each of the parallel searches is allowed to run to completion the final column reports that more defects can be repaired via our technique if human localization annotations are provided defects unique patches repaired program repaired patches per repair w annotat fbc gmp gzip libtiff lighttpd php 44 python wireshark total search for multiple repairs diverse solutions to the same problem may provide multiple options to developers or enable consideration of multiple attack surfaces in a se curity context to investigate genprog utility in generating multiple repairs we allowed each of the ten independent tri als per bug to run to completion instead of terminating early when any trial found a repair to identify unique patches we convert each repair into a tree structured expression level edit script using the diffx algorithm and minimize the edit script using delta debugging effectively removing unnecessary edits we consider a repair unique if the result of using this patch is textually unique table iv shows how many different patches were discov ered in this use case genprog produced unique patches for repairs or an average of distinct patches per repaired bug the unique patches are typically similar often involving different formulations of guards for inserted blocks or different computations of required values because all tri als including successful ones must now run to completion the total cost increases from to for all runs include human annotations genprog is fully auto mated however we might instead use programmer annota tions to guide a repair search similar in spirit to program ming by sketching in sketching a programmer specifies high level implementation strategies a sketch of general structure as well as details such as likely relevant variables invariants or function calls but leaves low level details to a program synthesizer the synthesizer uses these inputs to generate the complete code in these experiments we relax our assumption of full automation and assume that humans provide an unordered superset of statements that may be used to construct a patch i e fix localization information and pinpoint critical areas where patch actions mights be applied i e fault localization such annotations are easier to provide than a concrete patch but are not automatic we are interested in annotations to explore the upper limits of our fully automated method and to explore what a hybrid human machine approach might achieve we use the actual human repairs for our defect set as the source of our annotations we say that a defect can be repaired with annotations if it can be repaired automatically or it can be repaired with fault and fix information restricted to those lines and changes made by the human developers the final column of table iv shows results with annota tions the statement level repair method can address out of bugs annotations also reduce time to first repair by on this dataset data not shown this is consistent with the relationship between search space size and repair success section v and suggests that benefits might be gained from improved localization these results also illuminate our decision to use only statement level changes human developers used at least one extra statement level change e g introducing a new global variable in of the subject defects however the unannotated statement level approach can repair of those defects for example we observed that humans often introduce new variables to hold intermediate computations or to refactor buggy code while repairing it genprog achieves the same effect by reusing existing variable definitions to hold intermediate results the statement level technique is less likely to repair such defects addressing only of them vs overall repair rate statistically whether a human repair restricts attention to statement only changes moderately correlates with whether our technique can repair that same bug r p restricting attention to statements reduces the search space by one to two orders of magnitude these results suggest that is a good trade off however they also suggest that more powerful or finer grained operators might allow genprog to address many other real world defects d comparing automated and human written repairs in this subsection we compare the repairs produced by humans with those produced by genprog for two indicative defects we have not inspected all unique repairs man ually a user study of patch quality is left as future work python date handling in one bug six python tests failed based on whether the date maps to or the human patch removed a global dictionary lines of processing using that dictionary and a flag preserving that dictionary during checking the automated repair removes the lines of special processing but leaves un touched the empty dictionary and unused flag this retains required functionality but increases run time memory usage by one empty dictionary the patch is thus as functionally correct as the human patch but degrades some non functional aspects maintainability and memory footprint neither of which are tested this normal priority was open for days and involved developer messages and two different candidate patches submitted for review by human developers bugs python org php global object accessor crash php uses refer ence counting to determine when dynamic objects should be freed php also allows user programs to overload internal accessor functions to specify behavior when undefined class fields are accessed version had a bug related to a combination of those features at a high level the read property function which handles accessors always calls a deep reference count decrement on one of its arguments potentially freeing both that reference and the memory it points to this is the correct behavior unless that argument points to this when this references a global variable a situation that arises if the user program overrides the internal accessor to return this in such circumstances the global variable has its reference count decremented to zero and its memory is mistakenly freed while it is still reachable causing the interpreter to incorrectly return an error later the human written patch replaces the single line that always calls the deep decrement with a simple if then else in the normal case i e the argument is not a class object call the deep decrement on it as before otherwise call a separate shallow decrement function on it the shallow decrement function may free that particular pointer but not the object to which it points the genprog patch adapts code from a nearby unset property function the deep decrement is unchanged but additional code is inserted to check for the abnormal case in the abnormal case the reference count is deeply incremented through machinations involving a new variable and then the same shallow decrement is called thus at a very high level the human patch changes to if normal else while the gp generated patch changes it to if abnormal the logical effect is the same but the command ordering is not both patches increase the file size by four lines the human patch is perhaps more natural it avoids the deep decrement rather than performing it and then undoing it e summary genprog repaired of defects from subject programs spanning mloc and tests in a commercial cloud computing setting genprog repaired these bugs in hours for each on average this includes a hour start time paying more for on demand instances reduces trial time but increases cost all defects in this study were at least moder ately severe and were important enough for developers to fix we were unable to identify a significant relationship between human reported severity or human time to repair etc and repair success however genprog was less successful at repairing defects in which humans touched a large number of files or for which the fault could not be precisely localized qualitative comparison suggests that genprog repairs are often functionally equivalent to the human patches but deemphasize untested non functional requirements such as memory usage readability or maintainability our extensive use of parallelism is novel compared to previous work cf sec and yields an average return time of minutes per successful repair however if we continue to search beyond the first repair genprog finds unique patches per successful repair which provides developers more freedom and information when augmented with sketching inspired annotations genprog repairs of the defects the remaining presumably require algorithmic or localization improvements although we report reproducible monetary costs for fixing a defect once a test case is available it is difficult to directly compare our costs to those for human generated repairs our programs do not report per issue effort tracking as an indirect time comparison wei et al survey effort tracked issues in jboss an open source java project of comparable scale to our subject programs their mean time taken per issue was hours with a median of hours as an indirect cost comparison the tarsnap com bug bounty averaged for each non trivial repair section ii similarly an ibm report gives an average defect cost of during coding rising to at build time during testing qa and post release p in personal communication robert o callahan of novell the lead engineer for the mozilla gecko layout engine noted that our costs would be incredibly cheap if it carried over to our project but cautioned that the fix must be the right one to avoid damaging the long term health of the code we note three potential complexities in cost comparisons first we require test cases that identify the defects this is standard practice in some organizations e g at ibm testing qa might prepare test cases for particular bugs that separate maintenance developers may then use to fix them in others e g much open source development test case construction may introduce additional costs second candi date patches produced by our technique must be inspected and validated by developers while even incorrect tool generated patches have been shown to reduce the amount of time it takes developers to address an issue the exact reduction amount is unknown finally we note that human patches are imperfect in a survey of os fixes yin et al find that are incorrect and of those bad fixes led to crashes hangs corruption or security problems one of the php defects that genprog successfully repairs corresponds to a use after free vulnerability with an asso ciated security cve the human patch uses intermediate variables to hold deep copies of the function arguments such that when one is destroyed the others are unaffected genprog inserts code that copies the vulnerable argument cve mitre org cgi bin cvename cgi name cve an additional time preserving the relevant values when they are converted we note that all three of the bug bounties surveyed in section ii pay at least for a single security fix which exceeds the entire cost of our runs including the one that obtained this security repair vi limitations threats to validity an important threat to validity involves whether our results generalize to other settings i e whether our bench marks represent an indicative sample we attempt to mit igate selection bias i e cherry picking by defining vi able subject programs and defects and then including all matching defects found by an exhaustive search we ac knowledge that our benchmark set is best effort however our requirements limit the scope from which we draw conclusions for example using deterministic bugs leaves race conditions out of scope while using only c code leaves multi language bugs out of scope in addition we only evaluate on open source software and thus cannot directly speak to industrial development finally using checked in bugs that trigger checked in test cases has the advantage that all bugs considered were of at least moderate priority but our technique cannot be applied to bugs without tests an orthogonal threat relates to the sensitivity of our algorithm to gp parameters we address this issue directly in previous work representation choice and genetic operators matter more than the particular parameter values in this setting for example increasing the number of generations has a minimal effect fig vii r elated w ork automated repair this work extends our previous work in several important ways we systematically develop a large benchmark set and conduct a significant study of the technique on two orders of magnitude more code propose novel gp representations and operators to enhance effectiveness and enable scalability characterize actual real world costs propose and characterize fix space as an important consideration in search based software re pair and explore factors influencing repair success as well as theoretical and practical limitations clearview uses monitors and instrumentation to flag erroneous executions and generate and evaluate candidate binary patches to address invariant violations autofix e leverages contracts present in eiffel code and abstract state diagrams to propose semantically sound candidate bug fixes afix uses reports generated by an atomicity violation detector to automatically generate patches for single variable atomicity violations in the space of dynamic error recovery jolt assists in the dynamic detection of and recovery from infinite loops demsky et al use run time monitoring and formal specifications to detect and repair inconsistent data structures smirnov et al automatically compile programs with code to detect memory overflow and generate trace logs attack signatures and proposed patches and sidiroglou and keromytis use intrusion detection to identify vulnerable memory allocations and enumerate candidate repair patches these approaches address particular defect types via re pair strategies or templates enumerated a priori whereas genprog has developed patches for many defect types unlike afix genprog is unlikely to repair concurrency errors although it can repair deterministic bugs in multi threaded programs unlike autofix e genprog does not require specifications or annotations at most of these bugs might be detected by the two monitors with which clearview is evaluated establishing an upper bound on what it might repair from this dataset in addition our evaluation includes two orders of magnitude more subject code than autofix e two orders of magnitude more test cases than clearview and two orders of magnitude more defects than afix and is strictly larger than these projects and our own related work on each of these metrics separately debugging assistance he and gupta use weakest precon ditions to statically debug fully specified programs written in a restricted variant of c bugfix mines user input and common bug fix scenarios tracked over a project life cycle to suggest bug repairs debugadvisor helps programmers query databases of history source control etc to identify context or prior issues potentially relevant to a given bug bugfix user annotations may be useful in repair scenarios such as the one we evaluate in section v c molnar et al find integer bugs in million lines of code using dynamic test generation they also evaluate costs using cloud computing prices finding but not fixing bugs for each a natural next step would be to combine both approaches to both find and fix defects evolutionary search and gp arcuri and yao proposed to use gp to automatically co evolve defect repairs and unit test cases demonstrating on a hand coded example of bubble sort however the work relies on formal speci fications limiting generalizability and scalability orlov and sipper have experimented with evolving java bytecode using specially designed operators however our work is the first to report substantial experimental results on real defects in real programs recently debroy and wong independently validated that mutations targeted to probably faulty state ments can repair bugs without human intervention white et al use gp to improve non functional program properties particularly execution time ackling et al recently proposed to encode variants as a list of rewrite rules and a modification table evaluating on lines of code our patch representation follows this spirit search based software engineering sbse has applied evolutionary and related search methods to software con cerns such as testing project management and effort estima tion identification of safety violations and to re factoring of large software bases see harman for a survey viii c onclusion we report novel enhancements to genprog an automated program repair technique based on genetic program which significantly improve repair success and enable scalability with new representation mutation and crossover operators genprog finds more repairs than previous work these changes enable scalability to bugs in large open source programs while taking advantage of cloud computing parallelism we systematically evaluate genprog on re producible defects that developers have previously patched those defects come from programs including million lines of code and test cases this evaluation includes orders of magnitude more code test cases and defects than related or previous work our overall goal is to reduce the costs associated with defect repair in software maintenance genprog requires test cases and developer validation of candidate repairs but reduces the cost of actually generating a code patch while these results are only a first step they have implications for the future of automated program repair for example part of the high cost of developer turnover may be mitigated by using the time saved by this technique to write additional tests which remain even after developers leave to guide future repairs genprog could also be used to generate fast cheap repairs that serve as temporary bandages and provide time and direction for developers to find longer term fixes we directly measure the time and monetary cost of our technique by using public cloud computing resources our runs can be reproduced for this can be viewed as and minutes for each of bug repairs while we do not have a quantitative theory that fully explains how genprog works the systematic benchmark suite presented here will allow us to investigate such issues in the future we consider our results to be strongly competitive and hope that they will increase interest in this research area cases may be human written taken from a regression test suite steps to reproduce an error or generated automati cally we use the terms repair and patch interchange ably genprog does not require formal specifications program annotations or special coding practices genprog approach is generic and the paper reports results demon strating that genprog can successfully repair several types of defects this contrasts with related approaches which repair only a specific type of defect such as buffer overruns genprog takes as input a program with a defect and a set of test cases genprog may be applied either to the full program source or to individual modules it uses genetic programming gp to search for a program variant that retains required functionality but is not vulnerable to the defect in question gp is a stochastic search method inspired by biological evolution that discovers computer programs tailored to a particular task gp uses computational analogs of biological mutation and crossover to generate new program variations which we call variants a user defined fitness function evaluates each variant genprog uses the input test cases to evaluate the fitness and individuals with high fitness are selected for continued evolution this gp process is successful when it produces a variant that passes all tests encoding the required behavior and does not fail those encoding the bug although gp has solved an impressive range of problems e g it has not previously been used either to evolve off the shelf legacy software or to patch real world vulnerabilities despite various proposals directed at c le goues and w weimer are with the department of computer science automated error repair e g university of virginia engineer way po box charlottesville va e mail legoues weimer cs virginia edu t nguyen and s forrest are with the department of computer science university of new mexico university of new mexico a significant impediment for gp efforts to date has been the potentially infinite space that must be searched to find a correct program we introduce three key innovations to albuquerque nm e mail tnguyen forrest cs unm edu address this longstanding problem first genprog manuscript received mar revised oct accepted sept operates at the statement level of a program abstract syntax published online sept recommended for acceptance by j m atlee and p inverardi for information on obtaining reprints of this article please send e mail to tse computer org and reference ieeecs log number tsesi digital object identifier no 1109 tse quality is a pernicious problem mature soft ware projects are forced to ship with both known and unknown bugs because the number of outstanding software defects typically exceeds the resources available to address them software maintenance of which bug repair is a major component is time consuming and expensive accounting for as much as percent of the cost of a software project at a total cost of up to billion per year in the us put simply bugs are ubiquitous and finding and repairing them are difficult time consuming and manual processes techniques for automatically detecting software flaws include intrusion detection model checking and light weight static analyses and software diversity methods however detecting a defect is only half of the story once identified a bug must still be repaired as the scale of software deployments and the frequency of defect reports increase some portion of the repair problem must be addressed automatically this paper describes and evaluates genetic program repair genprog a technique that uses existing test cases to automatically generate repairs for real world bugs in off the shelf legacy applications we follow rinard et al in defining a repair as a patch consisting of one or more code changes that when applied to a program cause it to pass a set of test cases typically including both tests of required behavior as well as a test case encoding the bug the test tree ast increasing the search granularity second we hypothesize that a program that contains an error in one area likely implements the correct behavior elsewhere therefore genprog uses only statements from the program  ieee published by the ieee computer society le goues et al genprog a generic method for automatic software repair fig pseudocode of a buggy webserver implementation and a repaired version of the same program itself to repair errors and does not invent new code finally genprog localizes genetic operators to statements that are executed on the failing test case this third point is critical fault localization is in general a hard and unsolved problem the scalability of our approach relies on existing imperfect strategies and there exist classes of defects e g nondeterministic bugs which cannot always be localized for the defects considered here however we find that these choices reduce the search space sufficiently to permit the automated repair of a varied set of both programs and errors the gp process often introduces irrelevant changes or dead code along with the repair genprog uses structural differencing and delta debugging in a postproces sing step to obtain a minimal set of changes to the original program that permits it to pass all of the test cases we call this set the final repair the main contributions of this paper are genprog an algorithm that uses gp to automatically generate patches for bugs in programs as validated by test cases the algorithm includes a novel and efficient representation and set of operations for applying gp to this domain this is the first work to demonstrate the use of gp to repair software at the scale of real unannotated programs with publicly documented bugs experimental results showing that genprog can efficiently repair errors in c programs because the algorithm is stochastic we report success rates for each program averaged over trials for every program at least one trial found a successful repair with the average success rates ranging from to percent across all programs and all trials we report an average success rate of percent exerimental results demonstrating that the algo rithm can repair multiple types of errors in programs drawn from multiple domains the errors span eight different defect types infinite loop segmentation fault remote heap buffer overflow to inject code remote heap buffer overflow to over write variables nonoverflow denial of service local stack buffer overflow integer overflow and format string vulnerability the benchmark programs in clude unix utilities servers media players text processing programs and games the bench marks total over m lines of code loc although genprog operates directly on lines of program or module code some of these points were previously presented in early versions of this work or summarized for general audiences this paper extends those results to include new repairs previous work showed repairs on programs totaling lines of code and four classes of errors we present five additional programs and show that genprog can operate on both an entire program source code as well as at the module level the new benchmarks consist of new lines of source code new lines of repaired code either module or whole program and four new types of errors a significant increase that substantiates gen prog ability to scale to real world systems closed loop repair a description and proof of concept evaluation of a closed loop repair system that integrates genprog with anomaly intrusion detection repair quality a partial evaluation of the quality of the produced repairs first manually and then quanti tatively using indicative workloads fuzz testing and variant bug inducing input our preliminary findings suggest that the repairs are not fragile memorizations of the input but instead address the defect while retaining required functionality m otivating e xample in this section we use an example defect to highlight the important insights underlying the genprog approach and to motivate important design decisions consider the pseudocode shown in fig adapted from a remote exploitable heap buffer overflow vulnerability in the nullhttpd webserver function process request processes an incoming request based on data copied from the request header note that on line the call to calloc to allocate memory to hold request contents trusts ieee transactions on software engineering vol no january february the content length provided by a post request as copied from the header on line a malicious attacker can provide a negative value for content length and a malicious payload in the request body to overflow the heap and kill or remotely gain control of the running server to automatically repair this program we must first codify desired behavior for example we can write a test case that sends a post request with a negative content length and a malicious payload to the webserver and then checks the webserver to determine if it is still running unmodified nullhttpd fails this test case at a high level genprog searches for valid variants of the original program that do not display the specified buggy behavior however searching randomly through related programs may yield undesirable results consider the following variant this version of processrequest does not crash on the bug encoding test case but also fails to process any requests at all the repaired program should pass the error encoding test case while retaining core functionality such functionality can also be expressed with test cases such as a standard regression test that obtains index html and compares the retrieved copy against the expected output to satisfy these goals program modifications should ideally focus on regions of code that affect the bad behavior without affecting the good behavior we therefore employ a simple fault localization strategy to reduce the search space we instrument the program to record all lines visited when processing the test cases and favor changes to locations that are visited exclusively by the negative test case the standard regression test visits lines and and lines in dogetrequest the test case demonstrating the error visits lines and mutation and crossover opera tions are therefore focused on lines which exclusively implement post functionality despite this fault localization there are still many possible changes to explore to further constrain the search we assume that most defects can be repaired by adapting existing code from another location in the program in practice a program that makes a mistake in one location often handles a similar situation correctly in another this hypothesis is correct for nullhttpd although the post request handling in processrequest does not do a bounds check on the user specified content length the function implemented elsewhere does fault localization biases the modifications toward post request code the restriction to use only existing code for insertions further limits the search and eventually genprog tries inserting the check from into processrequest shown in fig a program with this version of processrequest passes both test cases we call it the primary repair gp can produce spurious changes in addition to those that repair the program for example the search might have randomly inserted return doget in practice we use several test cases to express program requirements we describe only one here for brevity fig high level pseudocode for genprog lines describe the gp search for a feasible variant subroutines such as mutatev p ath v  are described subsequently request socket length at line after the original return this insertion is not dangerous because it will never be executed but it does not contribute to the repair we remove such extraneous changes in a postprocessing step the resulting minimal patch is the final repair we present it in traditional diff format we formalize this procedure and describe concrete implementation details in the next section t echnical a pproach fig gives pseudocode for genprog genprog takes as input source code containing a defect and a set of test cases including a failing negative test case that exercises the defect and a set of passing positive test cases that describe requirements the gp maintains a population of program variants represented as trees each variant is a modified instance of the original defective program the modifications are generated by the mutation and cross over operations described in section the call to initial population on line uses mutation operators to construct an initial gp population based on the input program and test cases a fitness function evaluates each individual fitness or desirability genprog uses the input test cases to guide the gp search lines of fig section as well as to evaluate fitness section a gp iterates by selecting high fitness individuals to copy into the next generation line section and introdu cing variations with the mutation and crossover opera tions lines and line this cycle repeats until a goal is achieved a variant is found that passes all the test cases or a predetermined resource limit is consumed finally genprog minimizes the successful variant line section le goues et al genprog a generic method for automatic software repair program representation genprog represents each variant candidate program as a pair an abstract syntax tree that includes all of the statements in the program a weighted path consisting of a list of program statements each associated with a weight based on that statement occurrence in various test case execution traces genprog generates a program ast using the off the shelf cil toolkit asts express program structure at multiple levels of abstraction or granularity genprog operates on the constructs that cil defines as statements which includes all assignments function calls conditionals blocks and looping constructs genprog does not directly modify expressions such as or p nor does it ever directly modify low level control flow directives such as break continue or goto this genotype representa tion reflects a tradeoff between expressive power and scalability because of these constraints on permitted program modifications the gp never generates syntacti cally ill formed programs e g it will never generate unbalanced parentheses however it can generate variants that fail to compile due to a semantic error by for example moving the use of a variable out of scope the weighted path is a sequence of hstatement weighti pairs that constrains the mutation operators to a small likely relevant more highly weighted subset of the program tree statements not on the weighted path i e with weight are never modified although they may be copied into the weighted path by the mutation operator see section each new variant has the same number of pairs and the same sequence of weights in its weighted path as the original program this is necessary for the crossover operation described below to construct the weighted path we apply a transforma tion that assigns each statement a unique number and inserts code to log an event visit each time the statement is executed lines of fig duplicate statements are removed from the list that is we do not assume that a statement visited frequently e g in a loop is likely to be a good repair site however we do respect statement order determined by the first time a statement is visited so the weighted path is a sequence rather than a set any statement visited during the execution of a negative test case is a candidate for repair and its initial weight is set to all other statements are assigned a weight of and never modified the initial weights of the statements on the negative test case execution path are modified further by changing the weights of those statements that were also executed by a positive test case the goal is to bias the modifications toward portions of the source code that are likely to affect the bad behavior while avoiding those that influence good behavior set weightspath negt p ath post  on line of fig sets the weight of every path statement that is visited during at least one positive test case to a parameter w path choosing w path prevents modifica tion of any statement visited during a positive test case by removing it from the path we found that values such as w path typically work better in practice the weighted path serves to localize the fault this fault localization strategy is simple and by no means state of the art but has worked in practice for our benchmark programs we do not claim any new results in fault localization and instead view it as an advantage that we can use relatively off the shelf approaches path weighting is necessary to repair the majority of the programs we have investigated without it the search space is typically too large to search efficiently however effective fault localiza tion for both automatic and manual repair remains a difficult and unsolved problem and there exist certain types of faults which remain difficult to impossible to localize we expect that genprog will improve with advances in fault localization and leave the extension of the technique to use more sophisticated localization methods as future work selection and genetic operators selection the code on lines of fig implements the process by which genprog selects individual variants to copy over to the next generation genprog discards individuals with fitness variants that do not compile or that pass no test cases and places the remainder in v iable on line it then uses a selection strategy to select pop size members of a new generation from the previous iteration these individuals become the new mating pool we have used both stochastic universal sampling in which each individual probability of selection is directly proportional to its relative fitness f and tournament selection where small subsets of the population are selected randomly a tournament and the most fit member of the subset is selected for the next generation this process is iterated until the new population is selected both selection techniques produce similar results in our application two gp operators mutation and crossover create new variants from this mating pool mutation fig shows the high level pseudocode for the mutation operator mutation has a small chance of changing any particular statement along the weighted path line changes to statements in path p are reflected in its corresponding ast p a statement is mutated with fig the mutation operator updates to path p also update the ast p ieee transactions on software engineering vol no january february fig the crossover operator updates to path c and path d update the asts c and d probability equal to its weight with the maximum number of mutations per individual determined by the global mutation rate the parameter w mut set to and in our experiments see section line uses these probabilities to determine if a statement will be mutated in genetic algorithms mutation operations typically involve single bit flips or simple symbolic substitutions because our primitive unit is the statement our mutation operator is more complicated and consists of either a deletion the entire statement is deleted an insertion another statement is inserted after it or a swap with another statement we choose from these options with uniform random probability line in the case of an insertion or swap a second statement stmt j is chosen uniformly at random from anywhere in the program lines and not just along the weighted path a statement weight does not influence the probability that it is selected as a candidate repair this reflects our intuition about related changes a program missing a null check probably includes one somewhere but not necessarily on the negative path in a swap stmt i is replaced by stmt j while at the same time stmt j is replaced by stmt i we insert by transforming stmt i into a block statement that contains stmt i followed by stmt j in the current implementation stmt j is not modified when inserted although we note that intermediate variants may fail to compile if code is inserted which references out of scope variables deletions transform stmt i into an empty block statement a deleted statement may therefore be modified in a later mutation operation in all cases the new statement retains the old statement weight to maintain the invariant of uniform path lengths and weights between program variants and because inserted and swapped statements may not come from the weighted path and may thus have no initial weight of their own crossover fig shows the high level pseudocode for the crossover operator crossover combines the first part of one variant with the second part of another creating offspring variants that combine information from two parents the crossover rate is every surviving variant in a population undergoes crossover though a variant will only be the parent in one such operation per generation only statements along the weighted paths are crossed over we choose a cutoff point along the paths line and swap all statements after the cutoff point we have experimented with other crossover operators e g a crossover biased by path weights and a crossover with the original program and found that they give similar results to the one point crossover shown here fitness function the fitness function evaluates the acceptability of a program variant fitness provides a termination criterion for the search and guides the selection of variants for the next generation our fitness function encodes software requirements at the test case level negative test cases encode the fault to be repaired while positive test cases encode functionality that cannot be sacrificed we compile the variant ast to an executable program and then record which test cases the executable passes each successful positive test is weighted by the global para meter w post each successful negative test is weighted by the global parameter w negt the fitness function is thus simply the weighted sum fitnessp w post  jft post j p passes tgj  w negt  jft negt j p passes tgj the weights w post and w negt should be positive we give concrete values in section a variant that does not compile has fitness zero for full safety the test case evaluations can be run in a virtual machine or similar sandbox with a time out since test cases validate repair correctness test suite selection is an important consideration repair minimization the search terminates successfully when gp discovers a primary repair that passes all test cases due to randomness in the mutation and crossover algorithms the primary repair typically contains at least an order of magnitude more changes than are necessary to repair the program rendering the repairs difficult to inspect for correctness therefore genprog minimizes the primary repair to produce the final repair expressed as a list of edits in standard diff format defects associated with such patches are more likely to be addressed genprog performs minimization by considering each difference between the primary repair and the original program and discarding every difference that does not affect the repair behavior on any of the test cases standard diff patches encode concrete rather than abstract syntax since concrete syntax is inefficient to minimize we have adapted the diffx xml differencing algorithm to work on cil asts modified diffx generates a list of tree structured edit operations such as move the subtree rooted at node x to become the y th child of node z this encoding is typically shorter than the corresponding diff patch and applying part of a tree based edit never results in a syntactically ill formed program both of which make such patches easier to minimize the minimization process finds a subset of the initial repair edits from which no further elements can be dropped without causing the program to fail a test case a minimal subset a brute force search through all subsets of the initial list of edits is infeasible instead we use delta debugging to efficiently compute the one minimal subset which is le goues et al genprog a generic method for automatic software repair fig benchmark programs used in our experiments with size of the program and the repaired program segment in lines of code the unix utilities are repaired in their entirety however for example while the entire wu ftpd server was processed as a unit a smaller io module of openldap was selected for repair a y indicates an openly available exploit worst case this minimized set of changes is the final repair diffx edits can be converted automatically to standard diff patches which can either be applied automatically to the system or presented to developers for inspection in this paper patch sizes are reported in the number of lines of a unix diff patch not diffx operations r epair d escriptions in this section we substantiate the claim that automated repair of real world defects is possible by describing several buggy programs and examples of the patches that genprog generates the benchmarks for all experiments in this and subsequent sections are shown in fig the defects considered include infinite loops segmentation faults several types of memory allocation errors integer overflow and a well known format string vulnerability in most cases we consider all of the program source when making a repair in a few cases we restrict attention to the single module visited by the negative test case gcd is a small example based on euclid algorithm for computing great est common divisors zune is a fragment of code that caused all microsoft zune media players to freeze on december the unix utilities were taken from miller et al work on fuzz testing in which programs crash when given random inputs the remaining benchmarks are taken from public vulnerability reports in the following sections we describe several case studies of several exemplar repairs only one of which has been previously published the case studies are all taken from the security domain but they illustrate the repair process in the context of large programs with publicly documented bugs in each case we first describe the bug that corresponds to a public vulnerability report we then describe an indicative patch discovered by genprog nullhttpd remote heap buffer overflow the nullhttpd webserver is a lightweight multithreaded webserver that handles static content as well as cgi scripts version contains a heap based buffer overflow vulnerability that allows remote attackers to execute arbitrary code section illustrates this vulnerability for explanatory purposes nullhttpd trusts the content length value provided by the user in the http header of post requests negative values cause nullhttpd to overflow a buffer we used six positive test cases that include both get and post requests and a publicly available exploit to create the negative test case the negative test case request crashes the webserver which is not set to respawn to determine if the attack succeeded we insert a legitimate request for index html after the exploit the negative test case fails if the correct index html is not produced the actual buffer overflow occurs in the readpost data function defined in http c the value is supplied by the attacker however there is a second location in the program the function on line of cgi c where post data are processed and copied the evolved repair changes the high level header function so that it uses the post data processing in instead of calling readpost data the final minimized repair is five lines long ieee transactions on software engineering vol no january february although the repair is not the one supplied in the next release by human developers which inserts local bounds checking code in readpostdata it both eliminates the vulnerability and retains desired functionality openldap nonoverflow denial of service the openldap server implements the lightweight directory access protocol allowing clients to authenticate and make queries e g to a company internal telephone directory version is vulnerable to a denial of service attack ldap encodes protocol elements using a lightweight basic encoding rule ber nonauthenticated remote attackers can fig exploit post request for lighttpd the random text creates a request of the correct size line uses a fake fastcgi record to mark the end of the data line overwrites the execute script so that the vulnerable server responds with the contents of etc passwd crash the server by making improperly formed requests the assertion visibly fails in liblber io c so we restricted attention to that single file to demonstrate that we can repair program modules in isolation without requiring a whole program analysis to evaluate the fitness of a variant io c we copied it in to the openldap source tree and ran make to rebuild and link the liblber library then applied the test cases to the resulting binary the positive test cases consist of an unmodified second prefix of the regression suite that ships with openldap the negative test case was a copy of a positive test case with an exploit request inserted in the middle the problematic code is around line of io c the for loop contains both a sanity check and processing for large ber tags the first tag values are represented with a single byte if the high bit is set the next byte is used as well and so on the repair removes the entire loop lines leaving the run out of bytes check untouched this limits the number of ber tags that the repaired openldap can handle to a more natural repair would be to fix the sanity check while still supporting multibyte ber tags however only about tags are actually defined for openldap requests so the repair is fine for all openldap uses and passes all the tests lighttpd remote heap buffer overflow lighttpd is a webserver optimized for high performance environments it is used by youtube and wikimedia among others in version the fastcgi module which improves script performance is vulnerable to a heap buffer overflow that allows remote attackers to overwrite arbitrary cgi variables and thus control what is executed on the server machine in this case genprog repaired a dynamically linked shared object so with out touching the main executable the positive test cases included requests for static content i e get index html and a request to a line cgi perl script which among other actions prints all server and cgi environment variables the negative test case is the request shown in fig which uses a known exploit to retrieve the contents of etc passwd if the file contents are not returned the test case passes the key problem is with the function which uses memcpy to add data to a buffer without proper bounds checks is called many times in a loop by controlled by the following bounds calculation the repair modifies this calculation to wewant is thus uninitialized causing the loop to exit early on very long data allocations however the repaired server can still report all cgi and server environment variables and serve both static and dynamic content php integer overflow the php program is an interpreter for a popular web application scripting language version is vulnerable to an integer overflow attack that allows context dependent attackers to execute arbitrary code by exploiting the way the interpreter calculates and maintains bounds on string objects in single character string replacements as with the openldap repair example we restricted genprog operations to the string processing library we manually generated three positive test cases that exercise basic php functionality including iteration string splitting and concatenation and popular built in functions such as explode the negative test case included basic php string processing before and after the following exploit code a program variant passed this test if it produced the correct output without crashing single character string replacement replaces every in stance of a character a in the attack in a string a with a larger string b this functionality is implemented by which is called le goues et al genprog a generic method for automatic software repair by function at line of file string c uses a macro defined in a header file to calculate the new string length this macro expands to len count  on line wrapping around to a small negative number on the exploitative input the repair changes lines 482 to if search single character string replaces are thus disabled with the output set to an unchanged copy of the input while multicharacter string replaces performed by work as before the function replaces every instance of one substring with another and is not vulnerable to the same type of integer overflow as because it calculates the resulting length differently disabling functionality to suppress a security violation is often a legitimate response in this context many systems can be operated in a safe mode or read only mode although acceptable in this situation disabling functionality could have deleterious consequences in other settings we address this issue in section wu ftpd format string wu ftpd is an ftp server that allows for anonymous and authenticated file transfers and command execution ver sion is vulnerable to a well known format string vulnerability if site exec is enabled a user can execute a restricted subset of quoted commands on the server because the user command string is passed directly to a printf like function anonymous remote users gain shell access by using carefully selected conversion characters although the exploit is similar in structure to a buffer overrun the underlying problem is a lack of input validation genprog operated on the entire wu ftpd source we used five positive test cases obtaining a directory listing transferring a text file transferring a binary file correctly rejecting an invalid login and an innocent site exec command the negative test used an posted exploit to dynamically craft a format string for the target architecture the bug is in the function of ftpcmd y which manipulates the user supplied buffer cmd lreply x y z provides logging output by printing the executing command and providing the return code denotes success in the ftp protocol the lreply cmd on line calls printf cmd which with a carefully crafted cmd format string compro mises the system the explicit attempt to sanitize cmd by skipping past slashes and converting to lowercase does not prevent format string attacks the repair replaces lreply cmd with lreply char which disables verbose debugging output on cmd itself but does report the return code and the properly sanitized in buf while maintaining required functional ity g en p rog r epair p erformance this section reports the results of experiments that use genprog to repair errors in multiple legacy programs evaluating repair success over multiple trials and measuring performance and scalability in terms of fitness function evaluations and wall clock time experimental setup programs and defects the benchmarks consist of all programs in fig these programs total loc the repaired errors span eight defect classes infinite loop segmentation fault remote heap buffer overflow to inject code remote heap buffer overflow to overwrite variables nonoverflow denial of service local stack buffer overflow integer overflow and format string vulnerability and are repaired in lines of module or program code our experiments were conducted on a quad core ghz machine test cases for each program we used a single negative test case that elicits the given fault for the unix utilities we selected the first fuzz input that evinced a fault for the others we constructed test cases based on the vulnerability reports see section for examples we selected a small number e g of positive test cases per program in some cases we used noncrashing fuzz inputs in others we manually created simple cases focusing on testing relevant program function ality for openldap we used part of its test suite parameters we report results for one set of global genprog parameters that seemed to work well we chose pop size which is small compared to typical gp applications on each trial we ran the gp for a maximum of generations also a small number for fitness ieee transactions on software engineering vol no january february computation we set w post and w negt in related work we note that it is possible to select more precise weights as measured by the fitness distance correlation metric however we find that the values used here work well on our benchmark set these heuristically chosen values capture our intuition that the fitness function should emphasize repairing the fault and that the positive test cases should be weighted evenly we leave a more thorough exploration for future work with the above parameter settings fixed we experimen ted with two parameter settings for w path fw path and w mut fw path w mut 03g note that w path w mut 00 means that statements executed by both the negative test case and any positive test case will not be mutated and w path means such statements will be considered infrequently the parameter set w path and w mut works well in practice additional experiments show that genprog is robust to changes in many of these parameters such as population size and that varying the selection or crossover techniques has a small impact on time to repair or success we have experimented with higher probabilities finding that success worsens beyond w mut the weighted path length is the weighted sum of statements on the negative path and provides one estimate of the complexity of the search space statements that appear only on the negative path receive a weight of while those also on a positive path receive a weight of w path this metric is correlated with algorithm performance section trial we define a trial to consist of at most two serial invocations of the gp loop using the parameter sets above in order we stop a trial if an initial repair is found otherwise the gp is run for generations per parameter set we performed random trials for each program and report the percentage of trials that produce a repair average time to the initial repair in a successful trial and time to minimize a final repair a deterministic process performed once per successful trial an initial repair is one that passes all input test cases given the same random seed each trial is deterministically reproducible and leads to the same repair with unique seeds and for some programs genprog generates several different patches over many random trials for example over random trials genprog produces several different acceptable patches for ccrypt but only ever produces one such patch for openldap such disparities are likely related to the program error and patch type we do not report the number of different patches found because in theory there are an infinite number of ways to address any particular error however we note that our definition of repair as a set of changes that cause a program to pass all test cases renders all such patches acceptable ranking of different but acceptable patches remains an area of future investigation optimizations when calculating fitness we memorize fitness results based on the pretty printed abstract syntax tree so that two variants with different asts but identical source code are not evaluated twice similarly variants that are copied unchanged to the next generation are not reevaluated beyond such caching the prototype tool is not optimized in particular we do not take advantage of the fact that the gp repair task is embarrassingly parallel both the fitness of all variant programs and also the test cases for any individual variant can all be evaluated independently repair results fig summarizes repair results for c programs the initial repair heading reports timing information for the gp phase and does not include the time for repair minimization the time column reports the average wall clock time per trial that produced a primary repair execution time is analyzed in more detail in section repairs are found in seconds on average the fitness column shows the average number of fitness evaluations per successful trial which we include because fitness function evaluation is the dominant expense in most gp applications and the measure is independent of specific hardware configuration the success column gives the fraction of trials that were successful on average over percent of the trials produced a repair although most of the benchmarks either succeeded very frequently or very rarely low success rates can be mitigated by running multiple independent trials in parallel the size column lists the size of the primary repair diff in lines the final repair heading gives performance informa tion for transforming the primary repair into the final repair and a summary of the effect of the final repair as judged by manual inspection minimization is deterministic and takes less time and fewer fitness evaluations than the initial repair process the final minimized patch is quite manageable averaging lines of the patches seven insert code gcd zune look u look units ccrypt and indent seven delete code uniq deroff openldap lighttpd flex atris and php and two both insert and delete code nullhttpd and wu ftpd note that this does not speak to the sequence of mutations that lead to a given repair only the operations in the final patch a swap followed by a deletion may result in a minimized patch that contains only an insertion while a comprehensive code review is beyond the scope of this paper manual inspection suggests that the produced patches are acceptable we note that patches that delete code do not necessarily degrade functionality the deleted code may have been included erroneously or the patch may compensate for the deletion with an insertion the uniq deroff and flex patches delete erroneous code and do not degrade untested functionality the openldap patch removes unnecessary faulty code handling of multibyte ber tags when only tags are used and thus does not degrade functionality in practice the nullhttpd and wu ftpd patches delete faulty code and replace them by inserting nonfaulty code found elsewhere the wu ftpd patch disables verbose logging output in one source location but does not modify the functionality of the program itself and the nullhttpd patch does not degrade functionality the effect of the lighttpd patch is machine specific it may reduce functionality on very long messages though in our experiments it did not more detailed patch le goues et al genprog a generic method for automatic software repair fig experimental results on lines of program or module source code from programs totaling 25m lines of source code we report averages for random trials the positive tests column describes the positive tests the path columns give the weighted path length initial repair gives the average performance for one trial in terms of time the average time taken for each successful trial fitness the average number of fitness evaluations in a successful trial success how many of the random trials resulted in a repair size reports the average unix diff size between the original source and the primary repair in lines final repair reports the same information for the production of a minimal repair from the first initial repair found the minimization process always succeeds effect describes the operations performed by an indicative final patch a patch may insert code delete code or both insert and delete code descriptions are provided in section above we evaluate repair quality using indicative workloads and held out fuzz testing in section in many cases it is also possible to insert code without negatively affecting the functionality of a benchmark program the zune and gcd benchmarks both contain infinite loops zune when calculating dates involving leap years and gcd if one argument is zero in both cases the repair involves inserting additional code for gcd the repair inserts code that returns early skipping the infinite loop if the argument is zero in zune code is added to one of three branches that decrements the day in the main body of the loop allowing leap years with exactly days remaining to be processed correctly in both of these cases the insertions are carefully guarded so as to apply only to relevant inputs i e zero valued arguments or tricky leap years which explains why the inserted code does not negatively impact other functionality similar behavior is seen for look where a buggy binary search over a dictionary never terminates if the input dictionary is not presorted our repair inserts a new exit condition to the loop i e a guarded break a more complicated example is units in which user input is read into a static buffer without bounds checks a pointer to the result is passed to a lookup function and the result of lookup is possibly dereferenced our repair inserts code into lookup so that it calls an existing initializa tion function on failure i e before the return reinitia lizing the static buffer and avoiding the segfault combined with the explanations of repairs for nullhttpd section and wuftpd section which include both insertions and deletions these changes are indicative of repairs involving inserted code this experiment demonstrates that genprog can success fully repair a number of defect types in existing programs in a reasonable amount of time reports suggest that it takes human developers days on average to address even security critical repairs nine days elapsed between the posted exploit source for wu ftpd and the availability of its patch scalability and performance genprog is largely cpu bound an average repair run took seconds fig shows the proportion of time taken by each important component executing the test cases for the fitness function takes much of this time on average positive test cases take 76  0 and negative test cases 99  of the time in total fitness evaluations comprise  37 of total repair time many test cases include time outs e g negative test cases that specify an infinite loop error others involve explicit internal delays e g ad hoc instructions to wait seconds for the web server to get up and running before requests are sent the openldap test suite makes extensive use of this type of delay contributing to their runtime compilation of variants averaged  of repair time our initial implementation makes no attempt at incremental compila tion the high standard deviations arise from the widely varying test suite execution times e g from 0 seconds for zune to seconds for openldap fig plots weighted path length against search time measured as the average number of fitness evaluations until the first repair on a log log scale the straight line suggests ieee transactions on software engineering vol no january february fig percentage of total repair time spent on particular repair tasks a relationship following a power law of the form y axb where b is the best fit slope and b indicates a linear relationship fig suggests that the relationship between path length and search time is less than linear with slope 0 recall that the weighted path is based on observed test case behavior and not on the much larger number of loop free paths in the program we note that weighted path length does not fully measure the complexity of the search space notably as program size grows the number of possible statements that could be swapped or inserted along the path grows which is not accounted for in the weighted path length accordingly this relationship is only an approximation of scalability and search time may not grow sublinearly with search space using other measures however the results in fig are encouraging because they suggest that search time is governed more by weighted path rather than program size the test cases comprise fitness evaluation and define patch correctness test suite selection is thus important to both scalability and correctness for example when repair ing nullhttpd without a positive test case for post data functionality genprog generates a repair that disables post functionality entirely in this instance all of the post processing functionality is on the weighted path i e visited by the negative test case but not by any positive test cases and deleting those statements is the most expedient way to find a variant that passes all tests as a quick fix this is not unreasonable and is safer than the common alarm practice of running in read only mode however including the post functionality test case leads genprog to find a repair that does not remove functionality adding positive test cases can actually reduce the weighted path length while protecting core functionality and thus improve the success rate while possibly also increasing runtime experiments have shown that larger test suites increase fitness variability in early gp generations additional experiments confirm that test suite selection techniques can improve the performance of genprog on programs with large regression suites reducing repair times by up to percent while finding the same repairs these results suggest that genprog can repair off the shelf code in a reasonable amount of time that genprog performance scales with the size of the weighted path and that there are several viable avenues for applying the technique to larger programs with more comprehensive test suites in the future g en p rog r epair q uality although the results of the previous sections are encoura ging they do not systematically address the important issue of repair quality genprog reliance on positive test cases provides an important check against lost functionality the use of test cases exclusively to define acceptability admits the possibility of repairs that degrade the quality of the design of a system or make a system more difficult to maintain concerns that are difficult to evaluate automati cally and are beyond the scope of this paper however certain dangers posed by for example inadequate test suites such as repairs that reduce functionality or intro duce vulnerabilites can be evaluated automatically using indicative workloads held out test cases and fuzz testing additionally the claim of automated program repair relies on manual initialization and dispatch of genprog in principle automated detection techniques could signal the repair process to complete the automation loop integrating genprog with automated error detection produces a closed loop error detection and repair system that would allow us fig genprog execution time scales with weighted path size data are shown for benchmark programs including some not described here included for increased statistical significance see for details on the additional benchmarks and excluding gcd and zune the x axis shows weighted path length the y axis shows the number of fitness evaluations performed before a primary repair is found averaged over runs note the base log log scale le goues et al genprog a generic method for automatic software repair fig closed loop system outcomes per request as a function of anomaly detector and repair success cases and are new concerns to study repair quality and overhead on programs with realistic workloads this section therefore evaluates genprog in the context of a proof of concept closed loop system for webserver based programs with several experimental goals outline the prototype closed loop repair system and enumerate new experimental concerns measure the performance impact of repair time and quality on a real running system including the effects of a functionality reducing repair on system throughput analyze the quality of the generated repairs in terms of functionality using fuzz testing and variant bug inducing input and measure the costs associated with intrusion detec tion system ids false positives closed loop system overview our proposed closed loop repair system has two require ments beyond the input required by genprog anomaly detection in near real time to provide a signal to launch the repair process and the ability to record and replay system input so we can automatically construct a negative test case anomaly detection could be provided by existing behavior based techniques that run concurrently with the program of interest operating at almost any level e g by monitoring program behavior examining network traffic using saved state from regular checkpoints etc our prototype adopts an intrusion detection system that detects suspicious http requests based on request features in a preprocessing phase the ids learns a probabilistic finite state machine model of normal requests using a large training set of legitimate traffic after training the model labels subsequent http requests with a probability corresponding to suspiciousness given these components the system works as follows while the webserver is run normally and exposed to untrusted inputs from the outside world the ids checks for anomalous behavior and the system stores program state and each input while it is being processed when the ids detects an anomaly the program is suspended and genprog is invoked to repair the suspicious behavior the negative test case is constructed from the ids flagged input a variant is run in a sandbox on the input with the program state stored from just before the input was detected if the variant terminates successfully without triggering the ids the negative test case passes otherwise it fails the positive tests consist of standard system regression tests for the purpose of these experiments we use the tests described in section to guide the repair search and add new large indicative workloads to evaluate the effect of the repair search and deployment on several benchmarks if a patch is generated it can be deployed immediately if genprog cannot locate a viable repair within the time limit subsequent identical requests should be dropped and an operator alerted while genprog runs the system can either refuse requests respond to them in a safe mode or use any other technique e g fast signature generation to filter suspicious requests certain application domains e g supply chain management requests banking or e commerce support buffering of requests received during the repair procedure so they can be processed later fig summarizes the effects of the proposed system on a running program these effects depend on the anomaly detector misclassification rates false positives negatives and the efficacy of the repair method the integration of genprog with an ids creates two new areas of particular concern the first new concern case is the effect of an imperfect repair e g one that degrades functionality not guaranteed by the positive tests to a true vulnerability which can potentially lead to the loss of legitimate requests or in the worst case new vulnerabilities for security vulnerabilities in particular any repair system should include a strong final check of patch validity before deployment to evaluate the suitability of genprog on real systems it is therefore important to gain confidence first that genprog repairs underlying errors and second that it is unlikely to introduce new faults in case a repair generated in response to an ids false alarm could also degrade functionality again losing legitimate requests the remainder of this section evaluates these concerns and uses them as a framework to motivate and guide the evaluation of automated repair quality and overhead in terms of their effect on program throughput and correctness measured by held out test suites and indicative workloads experimental setup we focus the repair quality experiments on three of our benchmarks that consist of security vulnerabilities in long running servers lighttpd nullhttpd and php there exist many mature intrusion detection systems for security vulnerabilities providing a natural means of identifying bugs to be repaired similarly web servers are a compelling starting point for closed loop repair they are common attack targets they are important services that run continually and they are event driven making it easier to isolate negative test cases note that for the php experi ments we repair the php interpreter used by an un changing off the shelf apache webserver in libphp so ieee transactions on software engineering vol no january february fig closed loop repair system evaluation each row represents a different repair scenario and is separately normalized so that the prerepair daily throughput is percent the nullhttpd and lighttpd rows show results for true repairs the php row shows the results for a repair that degrades functionality the false pos rows show the effects of repairing three intrusion detection system false positives on nullhttpd the number after  indicates one standard deviation lost to repair time indicates the fraction of the daily workload lost while the server was offline generating the repair lost to repair quality indicates the fraction of the daily workload lost after the repair was deployed generic fuzz test failures counts the number of held out fuzz inputs failed before and after the repair exploit failures measures the held out fuzz exploit tests failed before and after the repair several experiments in this section use indicative work loads to measure program throughput pre during and postrepair we obtained workloads and content layouts from the university of virginia cs department webserver to evaluate repairs to the nullhttpd and lighttpd webservers we used a workload of http requests spanning 12 743 distinct client ip addresses over a hour period on november to evaluate repairs to php we obtained the room and resource reservation system used by the university of virginia cs department which features authentication graphical animated date and time selection and a mysql back end it totals 417 lines of php including uses of the subject of the php repair and is a fairly indicative three tier web application we also obtained 12 375 requests to this system spanning all of november recall that the php repair loses functionality we use this workload to evaluate the effect of such a repair in all cases a request was labeled successful if the correct bit for bit data were returned to the client before that client started a new request success requires both correct output and response time our test machine contains gb of ram and a ghz dual core cpu to avoid masking repair cost we uniformly sped up the workloads until the server machine was at percent utilization and additional speedups resulted in dropped packets to remove network latency and band width considerations we ran servers and clients on the same machine we use two metrics to evaluate repair overhead and quality the first metric is the number of successful requests a program processed before during and after a repair to evaluate repair time overhead we assume a worst case scenario in which the same machine is used both for serving requests and repairing the program and in which all incoming requests are dropped i e not buffered during the repair process the second metric evaluates a program on held out fuzz testing comparing behavior pre and postrepair can suggest whether a repair has introduced new errors and whether the repair generalizes the cost of repair time we first measure the overhead of running genprog itself by measuring the number of requests from the indicative workloads the unmodified programs successfully handle next we generated the repair noting the requests lost during the time taken to repair on the server machine fig summarizes the results the requests lost to repair time column shows the requests dropped during the repair as a fraction of the total number of successful requests served by the original program to avoid skewing relative performance by the size of the workload the numbers have been normalized to represent a single day containing a single attack note that the absolute speed of the server is not relevant here a server machine that was twice as fast overall would generate the repair in half the time but would also process requests twice as quickly fewer than percent of daily requests were lost while the system was offline for repairs buffering requests repairing on a separate machine or using techniques such as signature generation could reduce this overhead cost of a repair that degrades functionality the requests lost to repair quality column of fig quantifies the effect of the generated repairs on program throughput this row shows the difference in the number of requests that each benchmark could handle before and after the repair expressed as a percentage of total daily throughput the repairs for nullhttpd and lighttpd do not noticeably affect their performance recall that the php repair degrades functionality by disabling portions of the function the php row of fig shows that this low quality loss of functionality repair does not strongly affect system performance given the low quality repair potential for harm the low lost percentage for php is worth examining of the reservation application uses of involve replacements of multicharacter substrings such as replacing with for strings placed in html comments our repair leaves multicharacter substring behavior unchanged many of the other uses of occur on rare paths for example in le goues et al genprog a generic method for automatic software repair is used to make a form label but is only invoked if another variable is null other uses replace for example underscores with spaces in a form label field since the repair causes single character to perform no replacements if there are no underscores in the field then the result remains correct finally a few of the remaining uses were for sql sanitization such as replacing with however the application also uses so it remains safe from such attacks repair generality and fuzzing the experiments in the previous sections suggest that genprog repairs do not impair legitimate requests an important component of repair quality two additional concerns remain first repairs must not introduce new flaws or vulnerabilities even when such behavior is not tested by the input test cases to this end microsoft requires that security critical changes be subject to 000 fuzz inputs i e randomly generated structured input strings similarly we used the spike black box fuzzer from immunitysec com to generate 000 held out fuzz requests using its built in handling of the http protocol the generic column in fig shows the results of supplying these requests to each program each program failed no additional tests postrepair for example lighttpd failed the same 410 fuzz tests before and after the repair second a repair must do more than merely memorize and reject the exact attack input it must address the underlying vulnerability to evaluate whether the repairs generalize we used the fuzzer to generate held out variants of each exploit input the exploit column shows the results for example lighttpd was vulnerable to nine of the variant exploits plus the original exploit attack while the repaired version defeated all of them including the original in no case did genprog repairs introduce any errors that were detected by 000 fuzz tests and in every case genprog repairs defeated variant attacks based on the same exploit showing that the repairs were not simply fragile memorizations of the input the issue of repair generality extends beyond the security examples shown here note that because this particular experiment only dealt with the repair of security defects fuzz testing was more applicable than it would be in the general case establishing that a repair to a generic software engineering error did not introduce new failures or otherwise overfit could also be accomplished with held out test cases or cross validation cost of intrusion detection false positives finally we examine the effect of ids false positives when used as a signal to genprog we trained the ids on requests from an independent data set this process took seconds on a machine with quad core ghz and gb of ram the resulting system assigns a score to each incoming request ranging from 0 0 anomalous to 0 normal however the ids perfectly discriminated be tween benign and exploitative requests in the testing workloads no false negatives or false positives with a threshold of 0 02 therefore to perform these experiments we randomly selected three of the lowest scoring normal requests closest to being incorrectly labeled anomalous and attempted to repair nullhttpd against them using the associated requests as input and a diff against the baseline result for the negative test case we call these requests quasi false positives qfps the false pos rows of fig show the effect of time to repair and requests lost to repair when repairing these qfps qfp is a malformed http request that includes quoted data before the get the genprog repair changed the error response beha vior so that the response header confusingly includes http 0 ok while the user visible body retains the correct not implemented message but with the color coding stripped the header inclusion is ignored by most clients the second change affects the user visible error message neither causes the webserver to drop additional legitimate requests and fig shows no significant loss due to repair quality qfp is a head request such requests are rarer than get requests and only return header information such as last modification time they are used by clients to determine if a cached local copy suffices the repair changes the processing ofhead requests so that the cache control no store line is omitted from the response the no store directive instructs the browser to store a response only as long as it is necessary to display it the repair thus allows clients to cache pages longer than might be desired it is worth noting that the expires date also included in the response header remains unchanged and correctly set to the same value as the date date header also indicating that the page should not be cached so a conforming browser is unlikely to behave differently fig indicates negligible loss from repair quality qfp is a relatively standard http request genprog fails to generate a repair within one run seconds because it cannot generate a variant that is successful at get index html one of the positive test cases but fails the almost identical qfp request since no repair is deployed there is no subsequent loss to repair quality these experiments support the claim that genprog produces repairs that address the given errors and do not compromise functionality it appears that the time taken to generate these repairs is reasonable and does not unduly influence real world program performance finally the experiments suggest that the danger from anomaly detection false positives is lower than that of low quality repairs from inadequate test suites but that both limita tions are manageable d iscussion l imitations and t hreats the experiments in sections and suggest that genprog can repair several classes of errors in off the shelf c programs efficiently the experiments indicate that the overhead of genprog is low the costs associated with false ieee transactions on software engineering vol no january february positives and low quality repairs are low that the repairs generalize without introducing new vulnerabilities and that the approach may be viable when applied to real programs with real workloads even when considering the additional concerns presented by a closed loop detection and repair system however there are several limitations of the current work nondeterminism genprog relies on test cases to encode both an error to repair and important functionality some properties are difficult or impossible to encode using test cases such as nondeterministic properties genprog cannot currently repair race conditions for example we note however that many multithreaded programs such as nullhttpd can already be repaired if the threads are independent this limitation could be mitigated by running each test case multiple times incorporating scheduler constraints into the gp representation and allowing a repair to contain both code changes and scheduling directives or making multithreaded errors deterministic there are certain other classes of properties such as liveness fairness and noninterference that cannot be disproved with a finite number of execution examples it is not clear how to test or patch noninterference information flow properties using our system test suites and repair quality genprog defines repair acceptability according to whether the patched program passes the input test suite consequently the size and scope of the test suite can directly impact the quality of the produced patch even when minimized to reduce unneces sary changes because the test cases don t encode holistic design choices the repairs produced by genprog are not always the same as those produced by human developers repeated automatic patching could potentially degrade source code readability because even though our patches are small in practice they sometimes differ from those provided by human developers related research in automatic change documentation may mitigate this concern 44 repairs may reduce functionality if too few test cases are used and the utility of the closed loop architecture in particular requires that a test suite be sufficient to guard against lost functionality or new vulnerabilities however test cases are more readily available in practice than specifications or code annotations and existing test case generation techniques could be used to provide new positive or negative test cases and a more robust final check for patch validity we found in section that several security critical patches are robust in the face of fuzzed exploit inputs and do not appear to degrade functionality additionally the experiments in section suggest that even repairs that reduce functionality do not produce prohibitive effects in practice these results corroborate the precedent in previous work for this definition of repair acceptability ultimately however genprog is not designed to replace the human developer in the debugging pipeline as it is unable in its current incarnation to consider higher level design goals or in fact any program behavior beyond that observed on test cases results in section show that genprog running time is dominated by fitness evaluations too many test cases may thus impede running time however genprog has been shown to integrate well with test suite selection techniques permitting speedups of percent while finding the same repairs fault localization fault localization is critical to the success of genprog without weighting by fault localization our algorithm rarely succeeds e g gcd fails percent of the time genprog scalability is predicated on accurate fault localization using positive and negative test cases in the current implementation which makes use of a simple fault localization technique genprog scales well when the positive and negative test cases visit different portions of the program in the case of security related data only attacks where good and bad paths may overlap completely the weighted path will not constrain the search space as effectively potentially preventing timely repairs more precise bug localization techniques might mitigate this problem though fault localization in general remains a difficult and unsolved problem a related concern is genprog assumption that a repair can be adapted from elsewhere in the same source code this limitation could potentially be addressed with a small library of repair templates to augment the search space in the case of a very large code base the randomized search process could be overwhelmed by too many statements to select from in such cases new methods could be developed for fix localization we leave further repair localization techni ques as an avenue of future work intrusion detection for the closed loop system described in section we used an intrusion detection system that does not apply to all fault types and does not actually locate the fault we note that fault isolation by the ids is not necessary to integrate with our proposed architecture because genprog does its own fault isolation using existing techniques although the success of our approach is limited to faults that can be well localized by lightweight techniques e g excluding data only attacks it also means that we do not need to rely on an ids that can pinpoint fault locations instead our proposed closed loop system requires only a monitoring system that can identify an input that leads to faulty behavior a significantly easier problem and that permits the construction of a negative test case an input and an oracle we note that any limitations associated with intrusion detection apply only to the closed loop system evaluation and not to genprog in general experimental validity there exist several threats to the validity of our results many of the parameters in the implementation and experimental setup e g section were heuristically chosen based on empirical performance they may not represent the optimum set of parameter values representing a threat to construct validity i e we may not actually be measuring a well tuned genetic algorithm for this domain although we note that they appear to work well in practice additionally these parameters as well as the patterns seen in figs and might not generalize to other types of defects or other programs representing a threat to the external validity of the results the experiments focus particularly on security critical vulnerabilities in open source software which may not be indicative of all programs or errors found in industry to mitigate this threat we attempted to select a variety of benchmarks and le goues et al genprog a generic method for automatic software repair errors on which to evaluate genprog more recent publica tions on the subject of this technique have added several additional benchmarks 46 we note that such bench marks are often difficult to find in practice they require sufficient public information to reproduce an error access to the relevant source code and revision number and access to the correct operating environment to enable the reproduction of a given error investigating whether the costs reported in section are similar for other application domains e g bind or openssl and for other types of errors e g time of check to time of use or unicode parsing problems remains an area of future research r elated w ork there are several research areas broadly related to the work presented in this paper automatic bug detection localization and debugging automatic error preemption repair auto matic patch generation intrusion detection genetic pro gramming and search based software engineering sbse research advances in debugging include replay debug ging and cooperative bug isolation trace localization minimization and explanation projects also aim to elucidate faults and ease repairs these approaches typically narrow down a large counterexample backtrace the error symptom to a few lines a potential cause however a narrowed trace or small set of program lines is not a concrete repair second genprog can theoretically work on any detected fault not just those found by static analysis tools that produce counterexamples finally these algorithms are limited to the given trace and source code and can thus never localize the cause of an error to a missing statement adding or swapping code to address a missing statement is necessary for many of our repairs this research can be viewed as complementary to ours a defect found by static analysis might be repaired and explained automati cally and both the repair and the explanation could be presented to developers however a common thread in debugging research is that while information or flexibility is presented to the developer repairs for unannotated pro grams must be made manually one class of approaches to automatic error preemption and repair uses source code instrumentation and runtime monitoring to detect and prevent harmful effects from particular types of errors demsky et al automatically insert runtime monitoring code to detect if a data structure ever violates a given formal consistency specification and modify it back to a consistent state allowing buggy programs to continue to execute smirnov et al 52 automatically compile c programs with integrated code for detecting overflow attacks creating trace logs containing information about the exploit and generating a correspond ing attack signature and software patch dyboc instruments vulnerable memory allocations such that over or underflows trigger exceptions that are addressed by specific handlers other research efforts have focused more directly on patch generation in previous work we developed an automatic static algorithm for soundly repairing programs with specifications clearview uses runtime monitors to flag erroneous executions and then identify invariant violations characterizing the failure generates candidate patches that change program state or control flow accordingly and deploys and observes those candidates on several program variants to select the best patch for continued deployment selected transactional emulation executes potentially vulnerable functions in an emula tion environment preventing them from damaging a system using prespecified repair approaches a more accurate approach uses rescue points sidiroglou and keromytis proposed a system to counter worms by using an intrusion detector to identify vulnerable code or memory and preemptively enumerated repair templates to automatically generate patches these and similar techniques have several drawbacks first they require an a priori enumeration of vulnerability types and possible repair approaches either through the use of formal specifications or the use of external runtime monitors or predefined error and repair templates in practice despite recent advances in specification mining e g 56 formal specifications are rarely available none of the programs presented in this paper are specified moreover specifications are limited in the types of errors they can find and fix and cannot repair multithreaded code or violations of liveness properties e g infinite loops although some of the nonspecification based techniques are theoretically applicable to more than one type of security vulnerability typically evaluations are limited to buffer over and underflows the exception to this rule clearview is shown to also address illegal control flow transfers but is limited by the availability of external monitors for any given vulnerability type by contrast genprog designed to be generic does not require formal specifications or advanced knowledge of vulnerability types and has successfully repaired eight classes of errors to date including buffer overruns second these techniques require either source code instrumentation smirnov demsky which increases source code size by percent on apache in dyboc runtime monitoring dyboc clearview keromytis et al stemsead or virtual execution clearview selected trans actional emulation imposing substantial runtime overhead percent for dyboc up to percent for smirnov percent on apache for stemsead percent on firefox for clearview and in general at least the runtime cost of the chosen monitors genprog does not impose preemptive performance or size costs and minimizes patches as much as possible though in theory generated patches can be of arbitrary size our patches are also much more localized than a system that requires system wide instrumentation and are easily inspected by a human third these approaches do not evaluate generated repairs for quality or repaired programs for loss of functionality the clearview authors note that a manual inspection of their repaired program suggests that function ality is not dramatically impaired similarly they do not evaluate the effect of runtime monitor false positives while we cannot guarantee correctness genprog explicitly en codes testing a patch for correctness with its use of regression tests in fitness evaluation genprog produces patches with low overhead in terms of repair time and quality we have explicitly evaluated the effect of ids false ieee transactions on software engineering vol no january february positives on system performance and used standard methods to show that they are general in a method for automatically generating exploits from program patches was described generating concern that the method could be used by attackers although there are questions about the validity of this threat it is worth noting that there is no need in our system to distribute a patch a negative test case can be distributed as a self certifying alert and individual systems can generate their own repairs there is a large literature on intrusion detection for web servers including anomaly based methods e g in principle many of those techniques such as those of kruegel and vigna tombini et al and wang and stolfo could be incorporated directly into our proposed closed loop repair system non webserver programs would require other types of anomaly detection such as methods that track other layers of the network stack or that monitor system calls or library calls other approaches such as instruction set randomization or specification mining could also report anomalies for repair in each of these systems however false positives remain a concern although section provides evidence that false positives can be managed a fielded system could incorporate multiple independent signals initiating a repair only when they agree finally false positives might be reduced by intelligently retraining the anomaly detector after the patch has been applied arcuri et al proposed the idea of using gp to automate the co evolution of repairs to software errors and unit test cases demonstrating the idea on a hand coded example of the bubble sort algorithm the details of our approach are quite different from arcuri et al proposal allowing us to demonstrate practical repairs on a wide variety of legacy programs important differences include we leverage several representation choices to permit the repair of real programs with real bugs we minimize our high fitness solution after the evolutionary search has finished instead of controlling code bloat along the way we use execution paths to localize evolutionary search operators and we do not rely on a formal specifications for fitness evaluation several aspects of arcuri et al work could augment our approach such as using co evolutionary techniques to generate or select test cases however his work relies on formal specifications which limits both the programs to which it may apply and its scalability orlov and sipper have experimented with evolving java bytecode using specially designed operators to modify the code however our work is the first to report substantial experimental results on real programs with real bugs recently debroy and wong have independently validated that mutations targeted to statements likely to contain faults can affect repairs without human intervention the field of search based software engineering uses evolutionary and related methods for software testing e g to develop test suites sbse also uses evolutionary methods to improve software project manage ment and effort estimation to find safety violations and in some cases to refactor or reengineer large software bases in sbse most innovations in the gp technique involve new kinds of fitness functions and there has been less emphasis on novel representations and operators such as those we explored in this paper making better use of resources the many fault prediction models published are complex and disparate and no up to date comprehensive picture of the current state of fault prediction exists two previous reviews of the area have been performed in and our review differs from these reviews in the following ways timeframes our review is the most contemporary because it includes studies published from fenton and neil conducted a critical review of software fault prediction research up to catal and diri review covers work published between and systematic approach we follow kitchenham and charters original and rigorous procedures for conducting systematic reviews catal and diri did not report on how they sourced their studies stating that they adapted jrgensen and shepperd methodology fenton and neil did not apply the systematic approach introduced by kitchenham and charters as their study was published well before these guidelines were produced t hall and s counsell are with the department of information systems and computing brunel university uxbridge middlesex united kingdom e mail tracy hall steve counsell brunel ac uk s beecham is with lero the irish software engineering research centre systematic literature review slr aims to identify and analyze the models used to predict faults in source code in studies published between january and december our analysis investigates how model performance is affected by the context in which the model was developed the independent variables used in the model and the technique on which the model was built our results enable researchers to develop prediction models based on best knowledge and practice across many previous studies our results also help practitioners to make effective decisions on prediction models most suited to their context prediction modeling is an important area of research and the subject of many previous studies these studies typically produce fault prediction models which allow software engineers to focus development activities on fault prone code thereby improving software quality and the term fault is used interchangeably in this study with the terms defect or bug to mean a static fault in software code it does not denote a failure i e the possible result of a fault occurrence comprehensiveness we do not rely on search engines alone and unlike catal and diri we read through relevant journals and conferences paper by paper as a result we analyzed many more papers university of limerick tierney building limerick ireland e mail sarah beecham lero ie d bowes and d gray are with the science and technology research institute university of hertfordshire hatfield hertfordshire analysis we provide a more detailed analysis of each paper catal and diri focused on the context of studies including where papers were published united kingdom e mail d h bowes d gray herts ac uk year of publication types of metrics used datasets manuscript received oct revised july accepted sept used and modeling approach in addition we report published online sept recommended for acceptance by d sjberg for information on obtaining reprints of this article please send e mail to note that two referencing styles are used throughout this paper ref refers to papers in the main reference list while sref refers to papers in tse computer org and reference ieeecs log number tse the separate systematic literature review list located before the main digital object identifier no 1109 tse reference list 12 00  ieee published by the ieee computer society hall et al a systematic literature review on fault prediction performance in software engineering table the research questions addressed on the performance of models and synthesize the findings of studies we make four significant contributions by presenting a set of studies addressing fault prediction in software engineering from january to december researchers can use these studies as the basis of future investigations into fault prediction a subset of fault prediction studies which report sufficient contextual and methodological detail to enable these studies to be reliably analyzed by other researchers and evaluated by model users planning to select an appropriate model for their context a set of criteria to assess that sufficient contextual and methodological detail is reported in fault prediction studies we have used these criteria to identify the studies mentioned above they can also be used to guide other researchers to build credible new models that are understandable usable replicable and in which researchers and users can have a basic level of confidence these criteria could also be used to guide journal and conference reviewers in determining that a fault prediction paper has adequately reported a study a synthesis of the current state of the art in software fault prediction as reported in the studies satisfying our assessment criteria this synthesis is based on extracting and combining qualitative information on the main findings reported by studies quantitative data on the performance of these studies detailed quantitative analysis of the models or model variants reported in studies which report or we can calculate from what is reported precision recall and f measure perfor mance data this paper is organized as follows in the next section we present our systematic literature review methodology in section we present our criteria developed to assess whether or not a study reports sufficient contextual and methodological detail to enable us to synthesize a particular study section shows the results of applying our assessment criteria to studies section reports the results of extracting data from the studies which satisfy our assessment criteria section synthesizes our results and section discusses the methodological issues asso ciated with fault prediction studies section identifies the threats to validity of this study finally in section we summarize and present our conclusions m ethodology we take a systematic approach to reviewing the literature on the prediction of faults in code systematic literature reviews are well established in medical research and increasingly in software engineering we follow the systematic literature review approach identified by kitch enham and charters research questions the aim of this systematic literature review is to analyze the models used to predict faults in source code our analysis is based on the research questions in table inclusion criteria to be included in this review a study must be reported in a paper published in english as either a journal paper or conference proceedings the criteria for studies to be included in our slr are based on the inclusion and exclusion criteria presented in table before accepting a paper into the review we excluded repeated studies if the same study appeared in several publications we included only the most comprehensive or most recent identification of papers included papers were published between january and december our searches for papers were completed at the end of may and it is therefore unlikely that we missed any papers published in our time period as a result of publication time lags there were four elements to our searches key word searching using the search engines acm digital library ieeexplore and the isi web of science these search engines covered the vast ieee transactions on software engineering vol no november december http www informatik uni trier de ley db table paper selection and validation process majority of software engineering publications and the search string we used is given in appendix a an issue by issue manual reading of paper titles in relevant journals and conferences the journals and conferences searched are shown in appendix b these were chosen as highly relevant software engineering publications found previously to be good sources of software engineering research a manual search for publications from key authors using dblp these authors were selected as appear ing most frequently in our list of papers khoshgof taar menzies nagappan ostrand and weyuker the identification of papers using references from included studies table shows that our initial searches elicited 073 papers the title and abstract of each was evaluated and 762 were rejected as not relevant to fault prediction this process was validated using a randomly selected papers from the initial set of 073 three researchers separately interpreted and applied the inclusion and exclusion criteria to the papers pairwise interrater reliability was measured across the three sets of decisions to get a fair good agreement on the first iteration of this process based on the disagreements we clarified our inclusion and exclusion criteria a second iteration resulted in percent agreement between the three researchers we read the remaining papers in full this resulted in a further papers being rejected an additional secondary papers were identified from references and after being read in full accepted into the included set we also included two extra papers from catal and diri review which overlapped our timeframe our initial searches omitted these two of catal and diri papers as their search terms included the word quality we did not include this word in our searches as it generates a very high false positive rate this process resulted in the papers included in this review table inclusion and exclusion criteria s a ssessing the uitability of p apers for s ynthesis the previous section explained how we included papers which both answered our research questions and satisfied our inclusion criteria this section describes how we identified a subset of those papers as suitable from which to extract data and synthesize an overall picture of fault prediction in software engineering we then describe the extraction and synthesis process the assessment criteria our approach to identifying papers suitable for synthesis is motivated by kitchenham and charter notion of a quality check our assessment is focused specifically on identifying only papers reporting sufficient information to allow synthesis across studies in terms of answering our research questions to allow this a basic set of information must be reported in papers without this it is difficult to properly understand what has been done in a study and equally difficult to adequately contextualize the findings reported by a study we have developed and applied a set of criteria focused on ensuring sufficient contextual and methodological information is reported in fault prediction studies our criteria are organized into four phases described below phase establishing that the study is a prediction study in this slr it is important that we consider only models which actually do some form of prediction some studies which seem to be reporting prediction models actually turn out to be doing very little prediction many of these types of studies report correlations between metrics and faults such studies only indicate the propensity for building a prediction model furthermore a model is only doing any prediction if it is tested on unseen data i e data that were not used during the training process to be considered a prediction model it must be trained and tested on different data table shows the criteria we apply to assess whether a study is actually a prediction study hall et al a systematic literature review on fault prediction performance in software engineering table prediction criteria table shows that a study can pass this criterion as long as they have separated their training and testing data there are many ways in which this separation can be done holdout is probably the simplest approach where the original dataset is split into two groups comprising training set test set the model is developed using the training set and its performance is then assessed on the test set the weakness of this approach is that results can be biased because of the way the data have been split a safer approach is often n fold cross validation where the data are split into n groups fg g n g ten fold cross validation is very common where the data are randomly split into groups and experiments carried out for each of these experiments one of the groups is used as the testing set and all others combined are used as the training set performance is then typically reported as an average across all experiments m n fold cross validation adds another step by generating m different n fold cross validations which increases the reliability of the results and reduces problems due to the order of items in the training set stratified cross validation is an improvement to this process and keeps the distribution of faulty and nonfaulty data points approximately equal to the overall class distribution in each of the n bins although there are stronger and weaker techniques available to separate training and testing data we have not made a judgment on this and have accepted any form of separation in this phase of assessment phase ensuring sufficient contextual information is reported we check that basic contextual information is presented by studies to enable appropriate interpretation of findings a lack of contextual data limits the user ability to interpret a model performance apply the model appro priately or repeat the study for example a model may have been built using legacy systems with many releases over a long time period and has been demonstrated to perform well on these systems it may not then make sense to rely on this model for a new system where the code has only recently been developed this is because the number and type of faults in a system are thought to change as a system evolves if the maturity of the system on which the model was built is not reported this severely limits a model user ability to understand the conditions in which the model performed well and to select this model specifically for legacy systems in this situation the model could be applied to newly developed systems with disappointing predictive performance the contextual criteria we applied are shown in table and are adapted from the context checklist developed by petersen and wohlin our context checklist also overlaps with the project characteristics proposed by zimmermann et al as being relevant to understanding a project sufficiently for cross project model building it was impractical for us to implement all characteristics as none of our included studies report all context data are particularly important in this slr as it is used to answer research question and interpret our overall findings on model performance we only synthesize papers that report all the required context information as listed in table note that studies reporting several models based on different datasets can pass the criteria in this phase if sufficient contextual data are reported for one or more of these models in this case data will only be extracted from the paper based on the properly contextua lized model phase establishing that sufficient model building informa tion is reported for a study to be able to help us to answer our research questions it must report its basic model building elements without clear information about the independent and dependent variables used as well as the modeling techni que we cannot extract sufficient data to allow synthesis table describes the criteria we apply phase checking the model building data data used are fundamental to the reliability of models table presents the criteria we apply to ensure that studies report basic information on the data they used in addition to the criteria we applied in phases to we also developed more stringent criteria that we did not apply these additional criteria relate to the quality of the data used and the way in which predictive performance is measured although we initially intended to apply these this was not tenable because the area is not sufficiently mature applying these criteria would have resulted in only a handful of studies being synthesized we include these criteria in appendix c as they identify further important criteria that future researchers should consider when building models applying the assessment criteria our criteria have been applied to our included set of fault prediction studies this identified a subset of finally included studies from which we extracted data and on which our synthesis is based the initial set of included papers was divided between the five authors each paper was assessed by two authors independently 1280 ieee transactions on software engineering vol no november december table model building criteria table context criteria hall et al a systematic literature review on fault prediction performance in software engineering table data criteria with each author being paired with at least three other authors each author applied the assessment criteria to between and papers any disagreements on the assessment outcome of a paper were discussed between the two authors and where possible agreement estab lished between them agreement could not be reached by the two authors in cases these papers were then given to another member of the author team for moderation the moderator made a final decision on the assessment outcome of that paper we applied our four phase assessment to all included studies the phases are applied sequentially if a study does not satisfy all of the criteria in a phase then the evaluation is stopped and no subsequent phases are applied to the study this is to improve the efficiency of the process as there is no point in assessing subsequent criteria if the study has already failed the assessment this does have the limitation that we did not collect information on how a paper performed in relation to all assessment criteria so if a paper fails phase we have no information on how that paper would have performed in phase this assessment process was piloted four times each pilot involved three of the authors applying the assessment to included papers the assessment process was refined as a result of each pilot we developed our own mysql database system to manage this slr the system recorded full reference details and references to pdfs for all papers we identified as needing to be read in full the system maintained the status of those papers as well as providing an online process to support our assessments of papers the system collected data from all authors performing assessments it also provided a moderation process to facilitate identifying and resolving disagreements between pairs of assessors the system eased the administration of the assessment process and the analysis of assessment outcomes all data that were extracted from the papers which passed the assessment is also recorded on our system an overview of the system is available from and full details are available from the third author extracting data from papers data addressing our three research questions was extracted from each of the finally included studies which passed all assessment criteria our aim was to gather data that would allow us to analyze predictive performance within individual studies and across all studies to facilitate this three sets of data were extracted from each study context data data showing the context of each study were extracted by one of the authors this data give the context in terms of the source of data studied and the maturity size application area and programming language of the system studied 1282 ieee transactions on software engineering vol no november december qualitative data data related to our research continuous studies there are studies reporting con questions were extracted from the findings and tinuous dependent variables these studies report their conclusions of each study this was in terms of what results in terms of the number of faults predicted in a unit the papers reported rather than on our own of code it was not possible to convert the data presented in interpretation of their study this data supplemen these studies into a common comparative measure we ted our quantitative data to generate a rich picture of report the individual measures that they use most results within individual studies measures reported by continuous studies are based on two authors extracted qualitative data from all reporting an error measure e g mean standard error studies each author extracted data indepen mse or measures of difference between expected and dently and compared their findings to those of the observed results e g chi square some continuous studies other author disagreements and omissions were report their results in ranking form e g top percent of discussed within the pair and a final set of data faulty units we extract the performance of models using agreed upon whatever measure each study used quantitative data predictive performance data two authors extracted quantitative data from all were extracted for every individual model or model studies a pair approach was taken to extracting this variant reported in a study the performance data data since it was a complex and detailed task this meant we extracted varied according to whether the study that the pair of authors sat together identifying and reported their results via categorical or continuous extracting data from the same paper simultaneously dependent variables some studies reported both categorical and continuous results we extracted only one of these sets of results depending on the way in which the majority of results were presented by those studies the following is an overview of menzies et al claim that values of precision vary greatly when used with models applied to different datasets however reporting precision and recall via an f measure effectively evaluates classifier performance even in highly imbalanced domains 12 synthesizing data across studies synthesizing findings across studies is notoriously difficult and many software engineering slrs have been shown to present no synthesis in this paper we have also found how we extracted data from categorical and con synthesizing across a set of disparate studies very challen tinuous studies ging we extracted both quantitative and qualitative data categorical studies there are studies reporting catego rical dependent variables categorical studies report their results in terms of predicting whether a code unit is likely to be fault prone or not fault prone where possible we report the predictive performance of these studies using precision from studies we intended to meta analyze our quantitative data across studies by combining precision and recall performance data however the studies are highly dis parate in terms of both context and models meta analyzing this quantitative data may generate unsafe results such a recall and f measure as many studies report both precision meta analysis would suffer from many of the limitations in and recall from which an f measure can be calculated slrs published in other disciplines f measure is commonly defined as the harmonic mean of we combined our qualitative and quantitative data to precision and recall and generally gives a good overall generate a rich picture of fault prediction we did this by picture of predictive performance we used these three organizing our data into themes based around our three measures to compare results across studies and where research questions i e context independent variables and necessary we calculate and derive these measures from modeling techniques we then combined the data on each those reported appendix e explains how we did this theme to answer our research questions this synthesis is conversion and shows how we calculated f measure presented in section standardizing on the performance measures reported allows comparison of predictive performances across studies lessmann et al recommend the use of r esults of our consistent performance measures for cross study compar ison in particular they recommend use of area under the curve auc we also extract auc where studies report this appendix d summarizes the measurement of pre dictive performance we present the performance of categorical models in boxplots box plots are useful for graphically showing the differences between populations they are useful for our results as they make no assumptions about the distribution of the data presented these boxplots present the precision recall and f measure of studies according to a range of model factors these factors are related to the research questions presented at the beginning of section an example is a boxplot showing model performance relative to the modeling technique used a ssessment this section presents the results from applying our assessment criteria detailed in tables and to establish whether or not a paper reports sufficient con textual and methodological detail to be synthesized the assessment outcome for each study is shown at the end of its reference in the list of included studies table shows that only of our initially included studies passed all assessment criteria of these finally included studies three are relatively short and this means that it is possible to report necessary contextual and methodological detail concisely without a significant overhead in paper length table also shows that papers failed at phase of the assessment because they did not report prediction models as such this includes studies that only present correlation studies or these papers are s73 hall et al a systematic literature review on fault prediction performance in software engineering models that were not tested on data unseen during training this is an important finding as it suggests that a relatively high number of papers reporting fault prediction are not really doing any prediction this finding is also reported by table also shows that studies provided insufficient information about their data without this it is difficult to establish the reliability of the data on which the model is based table also shows that a very high number of studies reported insufficient information on the context of their study this makes it difficult to interpret the results reported in these studies and to select an appropriate model for a particular context several studies passing all of our criteria anonymized their contextual data for example and s110 although these studies gave full contextual details of the systems they used the results associated with each were anonymized this meant that it was impossible to relate specific fault information to specific systems while a degree of commercial confidenti ality was maintained this limited our ability to analyze the performance of these models of the studies which did not report sufficient context information were based on nasa data located in nasa mdp or promise this is because we could find no information about the maturity of the systems on which the nasa data are based maturity information is not given in either the mdp or promise repository documentation and no included paper provided any maturity information turham et al report that the nasa data are from numerous nasa contractors for an array of projects with a wide range of reuse this suggests that a range of maturities might also be represented in these datasets no clear insight is given into whether particular datasets are based on systems developed from untested newly released or legacy code based on many releases the only three studies using nasa data which passed the context phase of the assess ment were those which also used other datasets for which full context data are available the nasa based models were not extracted from these studies whether a study uses nasa data sourced from mdp or promise is shown at the end of its reference in the list of included studies table also shows that two studies failed the assessment due to the other reasons reported in table table issues with the measurement of performance table results of applying assessment criteria e r esults xtracted from p apers this section presents the results we extracted from the papers that passed all of our assessment criteria the full set of data extracted from those papers are contained in our online appendix https bugcatcher stca herts ac uk this online appendix consists of the following context of study table for each of the studies the context of the study is given in terms of the aim of the study together with details of the system used in the study the application area the system maturity and size categorical models table for each study reporting categorical results each model is described in terms of the independent variable the granularity of the dependent variable the modeling technique and the dataset used this table also reports the performances of each model using precision recall f measure and where given by studies auc some studies present many models or model variants all of which are reported in this table continuous models table for each study reporting continuous results including those reporting rank ing results the same information describing their model is presented as for categorical models however the performance of each continuous model is reported in terms of either the error measure the measure of variance or the ranked results as reported by a study 4 qualitative data table for each study a short summary of the main findings reported by authors is presented the remainder of this section contains boxplots illustrat ing the performance of the models in relation to various model factors e g modeling technique used independent variable used etc these factors are related to the research questions that we posed at the beginning of section 2 the boxplots in this section set performance against individual model factors e g modeling technique used this is a simplistic analysis as a range of interacting factors are likely to underpin the performance of a model however our results indicate areas of promising future research 1284 ieee transactions on software engineering vol no november december the boxplots represent models reporting only categorical results for which precision recall and f measure were either reported or could be calculated by us such models are reported in of the categorical studies of the remaining four three report auc we are unable to present boxplots for the studies using continuous data as the measures used are not comparable or convertible to comparable measures each boxplot includes data only where at least three models have used a particular factor e g a particular independent variable like loc this means that the numbers n at the top of the boxplots will not add up to the same number on every plot as factors used in less than three studies will not appear the total of ns will therefore vary from one boxplot to the next the boxplots contain performance data based on precision recall and f measure this is for all categorical models and model variants presented by each study models or model variants some studies present many model variants while others present only one model we also created boxplots of only the best results from each study these boxplots did not change the pattern of good performances but only pre sented limited information about poor performances for that reason we do not include these best only boxplots performances of models reported in individual studies fig is a boxplot of the performances of all the models reported by each of the categorical papers full details of which can be found in the online appendix for each individual paper f measure precision and recall is re ported fig 1 shows that studies report on many models or variants of models some with a wide range of performances fig 1 performances of the models reported in each of the categorical studies the details of these can be found in the models table in the online appendix https bugcatcher stca herts ac uk for example schro ter et al present model variants with a wide range of precision recall and f measure many of these variants are not particularly competitive the most competitive models that schro ter et al report are based on training the model on only the faultiest parts of the system this is a promising training technique and a similar technique has also been reported to be successful by zhang et al bird et al report model variants with a much smaller range of perfor mances all of which are fairly competitive fig 1 also shows the performance tradeoffs in terms of precision and recall made by some models for example bird et al report consistent precision and recall whereas moser et al and shivaji et al report performances where precision is much higher than recall fig 1 also shows that some models seem to be performing better than others the models reported by shivaji et al based on naive bayes performed extremely competitively in general naive bayes performed relatively well see fig however shivaji et al also used a good modeling process including feature selection and appropriate measures derived during model training in addition their dataset contained a relatively large proportion of faulty components making it fairly balanced this may improve performance by providing many exam ples of faults from which the modeling technique can train there are many good aspects of this study that mean it is likely to produce models which perform well on the other hand the performance of arisholm et al models are low in terms of precision but hall et al a systematic literature review on fault prediction performance in software engineering fig 2 data used in models competitive in terms of recall the two arisholm et al studies others for example the models built for embedded are different but use the same datasets this low precision telecoms systems are not particularly competitive this is reportedly because of the sampling method used to may be because such systems have a different profile of address the imbalance of the data used though the datasets faults with fewer postdelivery faults relative to other used are also small relative to those used in other studies systems developers of such systems normally prioritize kloc arisholm et al studies are interesting reducing postdelivery faults as their embedded context as they also report many good modeling practices and in makes fixing them comparatively expensive some ways are exemplary studies but they demonstrate how fig shows how models have performed relative to the the data used can impact significantly on the performance of size of systems on which they are based eclipse is the most a model it is also essential that both high and low common system used by studies consequently fig 3 performances be reported as it is only by identifying these shows only the size of versions of eclipse in relation to that our overall understanding of fault prediction will model performance fig 3 suggests that as the size of a improve the boxplots in the rest of this section explore in system increases model performance seems to improve more detail aspects of models that may underpin these this makes sense as models are likely to perform better performance variations because the performances of aris given more data holm et al models are very different from those of fig 4 shows the maturity of systems used by studies the other studies we have removed them from the rest of the relative to the performance of models the context table in boxplots we have treated them as outliers which would the online appendix shows how systems have been skew the results we report in other boxplots categorized in terms of their maturity fig 4 shows that no 2 performances in relation to context factors immature systems are used by more than two models in this set of studies i e where n 3 there seems to be little fig 2 shows the datasets used in the studies it shows that difference between the performance of models using mature models reported in the studies are based on data from or very mature systems this suggests that the maturity of eclipse eclipse is very well studied probably because the systems may not matter to predictive performance this fault data are easy to access and its utility has been well finding may be linked to the finding we report on size it proven in previous studies in addition data already may be that what was previously believed about the extracted from eclipse are available from saarland uni versity http www st cs uni saarland de softevo bug data eclipse and promise http promisedata org an exception to this is found in studies where immature systems are used with promising performances reported see the online appendix for full details fig 2 shows that there is a wide variation in model performance using eclipse fig 2 also suggests that it may be more difficult to build models for some systems than for this may mean that it is not important to report maturity when studies describe their context many more studies would have passed our assessment had that been the case however much more data on maturity is needed before firm conclusions can be drawn 1286 ieee transactions on software engineering vol no november december importance of maturity was actually about size i e maturity is a surrogate for size indeed there is a significant relationship between size and maturity in the data we report here however we do not have enough data to draw firm conclusions as the data we analyze contain no studies using immature systems more research is needed to test for possible association between maturity and size and whether data extracted from immature systems can be used as a basis for reliable fault prediction fig shows the language used in the systems studied in relation to the performance of models we present only studies reporting the use of either java or c c there are fig 4 the maturity of the systems used fig 3 the size of the datasets used for eclipse several single studies using other languages which we do not report fig suggests that model performance is not related to the language used fig shows model performance relative to the granularity of dependent variables e g whether fault prediction is at the class or file level it shows no clear relationship between granularity and performance it does not seem to be the case that higher granularity is clearly related to improved performance models reporting at other levels of granular ity seem to be performing most consistently these tend to be high levels of granularity defined specifically by individual studies e g nagappan et al fig the language used hall et al a systematic literature review on fault prediction performance in software engineering 3 performance in relation to independent variables fig shows model performance in relation to the independent variables used the categorical models table in the online appendix shows how independent variables as expressed by individual studies have been categorized in relation to the labels used in fig it shows that there is variation in performance between models using different independent variables models using a wide combination of metrics seem to be performing well for example models using a combination of static code metrics scm process metrics and source code text seem to be performing best overall e g shivaji et al similarly bird et al study which uses a wide combination of socio technical metrics code dependency data together with change data and developer data also performs well though the results from bird et al study are reported at a high level of granularity process metrics i e metrics based on changes logged in repositories have not performed as well as expected oo metrics seem to have been used in studies which perform better than studies based only on other static code metrics e g complexity based metrics models using only loc data seem to have performed competitively compared to models using other independent variables indeed of these models using only metrics based on static features of the code oo or scm loc seems as good as any other metric to use the use of source code text seems related to good performance mizuno et al studies have used only source fig the granularity of the results code text within a novel spam filtering approach to relatively good effect 4 performance in relation to modeling technique fig shows model performance in relation to the modeling techniques used models based on naive bayes seem to be performing well overall naive bayes is a well understood technique that is in common use similarly models using logistic regression also seem to be performing well models using linear regression perform not so well though this technique assumes that there is a linear relationship between the variables studies using random forests have not performed as well as might be expected many studies using nasa data use random forests and report good performances fig also shows that svm support vector machine techniques do not seem to be related to models performing well furthermore there is a wide range of low performances using svms this may be because svms are difficult to tune and the default weka settings are not optimal the performance of models using the technique is fairly average however arisholm et al models used the technique as previously explained these are not shown as their relatively poor results skew the data presented is thought to struggle with imbalanced data and and this may explain the performance of arisholm et al s models 1288 ieee transactions on software engineering vol no november december r s ynthesis of esults this section answers our research questions by synthesizing the qualitative and quantitative data we have collected the qualitative data consist of the main findings reported by fig modeling technique used fig independent variables used in models each of the individual finally included studies presented in the qualitative data table in our online appendix the quantitative data consist of the predictive performance of the individual models reported in the studies summarized in hall et al a systematic literature review on fault prediction performance in software engineering the categorical and continuous models tables in our online appendix the quantitative data also consist of the detailed predictive performance data from studies models or model variants comparing performance across models reported in section this combination of data addresses model performance across studies and within individual studies this allows us to discuss model performance in two ways first we discuss performance within individual studies to identify the main influences on model perfor mance reported within a study second we compare model performances across the models reported in studies this is an important approach to discussing fault prediction models most studies report at least one model which performs well though individual studies usually only compare performance within the set of models they present to identify their best model we are able to then compare the performance of the models which perform well within a study across other studies this allows us to report how well these models perform across studies 1 answering our research questions how does context affect fault prediction analyzing model performance across the studies in detail suggests that some context variables may influence the reliability of model prediction our results provide some evidence to suggest that predictive performance improves as systems get larger this is suggested by the many models built for the eclipse system as eclipse increases in size the performance of models seems to improve this makes some sense as models are likely to perform better with more data we could find no evidence that this improved performance was based on the maturing of systems it may be that size influences predictive performance more than system ma turity however our dataset is relatively small and although we analyzed models or model variants very few were based on immature systems our results also suggest that some applications may be less likely to produce reliable prediction models for example the many models built for embedded telecoms applications generally per formed less well relative to other applications our results also show that many models have been built using eclipse data this corpus of knowledge on eclipse provides a good opportunity for future researchers to meta analyze across a controlled context the conventional wisdom is that context determines how transferrable a model is to other systems despite this none of the finally included studies directly investigate the impact on model performance of specific context variables such as system size maturity application area or program ming language one exception is which demonstrates that transforming project data can make a model more comparable to other projects many of the finally included studies individually test how well their model performs when transferred to other contexts releases systems application areas data sources or companies few of these studies directly investigate the contextual factors influencing the transfer ability of the model findings reported from individual studies on model transferability are varied most studies report that models perform poorly when transferred in fact bell et al report that models could not be applied to other systems denaro and pezze reported good predictive performance only across homogenous applications nagappan et al report that different subsets of complexity metrics relate to faults in different projects and that no single set of metrics fits all projects nagappan et al conclude that models are only accurate when trained on the same or similar systems however other studies report more promising transfer ability weyuker et al report good performance when models are transferred between releases of systems and between other systems however shatnawi and li report that the performance of models declines when applied to later releases of a system shatnawi and li conclude that different metrics should be used in models used for later releases the context of models has not been studied extensively in the set of studies we analyzed although every model has been developed and tested within particular contexts the impact of that context on model performance is scarcely studied directly this is a significant gap in current knowledge as it means we currently do not know what context factors influence how well a model will transfer to other systems it is therefore imperative that studies at least report their context since in the future this will enable a meta analysis of the role context plays in predictive performance which independent variables should be included in fault prediction models many different independent variables have been used in the finally included studies these mainly fall into process e g previous change and fault data and product e g static code data metrics as well as metrics relating to developers in addition some studies have used the text of the source code itself as the independent variables e g mizuno et al mizuno and kikuno model performance across the studies we analyzed in detail suggests that the spam filtering technique based on source code used by mizuno et al mizuno and kikuno performs relatively well on the other hand models using only static code metrics typically complexity based perform relatively poorly model performance does not seem to be improved by combining these metrics with oo metrics models seem to perform better using only oo metrics rather than only source code metrics however models using only loc seem to perform just as well as those using only oo metrics and better than those models only using source code metrics within individual studies zhou et al report that loc data performs well ostrand et al report that there was some value in loc data and hongyu reports loc to be a useful early general indicator of fault proneness zhou et al report that loc performs better than all but one of the chidamber and kemerer metrics weighted methods per class within other individual studies loc data were reported to have poor predictive power and to be out performed by other metrics e g bell et al overall loc seem to be generally useful in fault prediction model performance across the studies that we analyzed suggests that the use of process data is not particularly related to good predictive performance however looking at 1290 ieee transactions on software engineering vol no november december the findings from individual studies several authors report that process data in the form of previous history data performs well e g d ambros et al specifically report that previous bug reports are the best predictors more sophisticated process measures have also been reported to perform well in particular nagappan et al introduce change burst metrics which demonstrate good predictive performance however these models per form only moderately when we compared them against models from other studies the few studies using developer information in models report conflicting results ostrand et al report that the addition of developer information does not improve predictive performance much bird et al report better performances when developer information is used as an element within a socio technical network of variables this study also performs well in our detailed comparison of performances bird et al report results at a high level of granularity and so might be expected to perform better the models which perform best in our analysis of studies seem to use a combined range of independent variables for example shivaji et al use process based and scm based metrics together with source code bird et al combine a range of metrics the use of feature selection on sets of independent variables seems to improve the performance of models e g optimized sets of metrics using for example feature selection make sense which modeling techniques perform best when used in fault prediction while many included studies individually report the comparative performance of the modeling techniques they have used no clear consensus on which perform best emerges when individual studies are looked at separately mizuno and kikuno report that of the techniques they studied orthogonal sparse bigrams markov models osb are best suited to fault prediction bibi et al report that regression via classification rvc works well khoshgoftaar et al report that modules whose fault proneness is predicted as uncertain can be effectively classified using the treedisc td technique khoshgoftaar and seliya also report that case based reasoning cbr does not predict well with 5 also performing poorly arisholm et al report that their comprehensive performance comparison revealed no predictive differences between the eight modeling techniques they investigated a clearer picture seems to emerge from our detailed analysis of model performance across the studies our findings suggest that performance may actually be linked to the modeling technique used overall our comparative analysis suggests that studies using support vector machine svm techniques perform less well these may be underperforming as they require parameter optimization something rarely carried out in fault prediction studies for best performance where svms have been used in other prediction domains and may be better understood they have performed well models based on 5 seem to underperform if they use imbalanced data e g arisholm et al as the technique seems to be sensitive to this our comparative analysis also suggests that the models performing comparatively well are relatively simple tech niques that are easy to use and well understood naive bayes and logistic regression in particular seem to be the techniques used in models that are performing relatively well models seem to have performed best where the right technique has been selected for the right set of data and these techniques have been tuned to the model e g shivaji et al rather than relying on default tool parameters m ethodological i ssues in f ault p rediction the methodology used to develop train test and measure the performance of fault prediction models is complex however the efficacy of the methodology used underpins the confidence which we can have in a model it is essential that models use and report a rigorous methodology without this the maturity of fault prediction in software engineering will be low we identify methodological problems in existing studies so that future researchers can improve on these throughout this slr methodological issues in the published studies came to light during our assessment of the initially included studies and the extraction of data from the finally included studies methodological weak nesses emerged in this section we discuss the most significant of these methodological weaknesses these generally relate to the quality of data used to build models and the approach taken to measure the predictive perfor mance of models 1 data quality the quality of the data used in fault prediction has significant potential to undermine the efficacy of a model data quality is complex and many aspects of data are important to ensure reliable predictions unfortunately it is often difficult to assess the quality of data used in studies especially as many studies report very little about the data they use without good quality data clearly reported it is difficult to have confidence in the predictive results of studies the results of our assessment show that data quality is an issue in many studies in fact many studies failed our synthesis assessment on the basis that they either reported insufficient information about the context of their data or about the collection of that data some studies explicitly acknowledge the importance of data quality e g jiang et al collecting good quality data is very hard this is partly reflected by the number of studies which failed our assessment by not adequately explaining how they had collected their independent or dependent data fault data collection has been previously shown to be particularly hard to collect usually because fault data are either not directly recorded or recorded poorly collecting data is made more challenging because large datasets are usually necessary for reliable fault prediction jiang et al investigate the impact that the size of the training and test dataset has on the accuracy of predictions tosun et al present a useful insight into the real challenges associated with every aspect of fault prediction but particularly on the difficulties of collecting reliable metrics and fault data once collected data is usually noisy and hall et al a systematic literature review on fault prediction performance in software engineering often needs to be cleaned e g outliers and missing values dealt with very few studies report any data cleaning even in our finally included studies the balance of data i e the number of faulty as opposed to nonfaulty units on which models are trained and tested is acknowledged by a few studies as fundamental to the reliability of models see appendix f for more information on class imbalance indeed across the studies we analyzed in detail some of those performing best are based on data with a good proportion of faulty units e g s11 our analysis also suggests that data imbalance in relation to specific modeling techniques e g 5 may be related to poor performance e g several studies specifically investigated the impact of data balance and propose techniques to deal with it for example khoshgoftaar et al and shivaji et al present techniques for ensuring reliable data distributions schro ter et al base their training set on the faultiest parts of the system similarly seiffert et al present data sampling and boosting techniques to address data imbalance data imbalance is explored further in fioravanti and nesi and zhang et al many studies seem to lack awareness of the need to account for data imbalance as a consequence the impact of imbalanced data on the real performance of models can be hidden by the performance measures selected this is especially true where the balance of data is not even reported readers are then not able to account for the degree of imbalanced data in their interpretation of predictive performance 2 measuring the predictive performance of models there are many ways in which the performance of a prediction model can be measured indeed many different categorical and continuous performance measures are used in our studies there is no one best way to measure the performance of a model this depends on the class distribution of the training data how the model has been built and how the model will be used for example the importance of measuring misclassification will vary de pending on the application performance comparison across studies is only possible if studies report a set of uniform measures furthermore any uniform set of measures should give a full picture of correct and incorrect classification to make models reporting categorical results most useful we believe that the raw confusion matrix on which their performance measures are derived should be reported this confusion matrix data would allow other researchers and potential users to calculate the majority of other measures pizzi et al provide a very usable format for presenting a confusion matrix some studies present many models and it is not practical to report the confusion matrices for all these menzies et al suggest a useful way in which data from multiple confusion matrices may be effectively reported alternatively lessmann recommends that roc curves and auc are most useful when comparing the ability of modeling techniques to cope with different datasets roc curves do have some limitations either of these approaches adopted widely would make studies more useful in the future comparing across studies reporting continuous results is currently even more difficult and is the reason we were unable to present comparative boxplots across these studies to enable cross comparison we recommend that continuous studies report average relative error are in addition to any preferred measures presented the impact of performance measurement has been picked up in many studies zhou et al report that the use of some measures in the context of a particular model can present a misleading picture of predictive performance and undermine the reliability of predictions arisholm et al discuss how model performance varies depending on how it is measured there is an increasing focus on identifying effective ways to measure the perfor mance of models cost and or effort aware measurement is now a significant strand of interest in prediction measure ment this takes into account the cost effort of falsely identifying modules and has been increasingly reported as useful the concept of cost effectiveness measurement originated with the simula group e g arisholm et al but has more recently been taken up and developed by other researchers for example nagappan et al and mende and koschke 3 fault severity few studies incorporate fault severity into their measure ment of predictive performance although some faults are more important to identify than others few models differentiate between the faults predicted in fact shatnawi and li s was the only study in the final to use fault severity in their model they report a model which is able to predict high and medium severity faults these levels of severity are based on those reported in bugzilla by eclipse developers lamkanfi et al singh et al and zhou and leung are other studies which have also investigated severity this lack of studies that consider severity is probably because although acknowledged to be important severity is considered a difficult concept to measure for example menzies et al say that severity is too vague to reliably investigate nikora and munson says that without a widely agreed definition of severity we cannot reason about it and ostrand et al state that severity levels are highly subjective and can be inaccurate and inconsistent these problems of how to measure and collect reliable severity data may limit the usefulness of fault prediction models companies develop ing noncritical systems may want to prioritize their fault finding effort only on the most severe faults 4 the reporting of fault prediction studies our results suggest that overall fault prediction studies are reported poorly out of the studies initially included in our review only passed our assessment criteria many of these criteria are focused on checking that studies report basic details about the study without a basic level of information reported it is hard to have confidence in a study our results suggest that many studies are failing to report information which is considered essential when reporting empirical studies in other domains the poor reporting of studies has consequences for both future researchers and potential users of models it is difficult for researchers to meta analyze across studies and it is difficult 1292 ieee transactions on software engineering vol no 6 november december to replicate studies it is also difficult for users to identify suitable models for implementation 5 nasa data nasa s publicly available software metrics data have proven very popular in developing fault prediction models we identify all studies which use nasa data in the reference list of the included studies the nasa data is valuable as it enables studies using different modeling techniques and independent variables to be compared to others using the same dataset it also allows studies to be replicated a meta analysis of the studies using nasa data would be valuable however although the repository holds many metrics and is publicly available it does have limitations it is not possible to explore the source code and the contextual data are not comprehensive e g no data on maturity are available it is also not always possible to identify if any changes have been made to the extraction and computation mechanisms over time in addition the data may suffer from important anomalies it is also questionable whether a model that works well on the nasa data will work on a different type of system as menzies et al point out nasa works in a unique niche market developing software which is not typical of the generality of software systems however turhan et al have demonstrated that models built on nasa data are useful for predicting faults in software embedded in white goods t hreats to v alidity searches we do not include the term quality in our search terms as this would have resulted in the examination of a far wider range of irrelevant papers this term generates a high number of false positive results we might have missed some papers that use the term quality as a synonym for defect or fault etc however we missed only two papers that catal and diri s 2 searches found using the term quality this gives us confidence that we have missed very few papers we also omitted the term failure from our search string as this generated papers predominately reporting on studies of software reliability in terms of safety critical systems such studies of reliability usually examine the dynamic behavior of the system and seldom look at the prediction of static code faults which is the focus of this review we apply our search terms to only the titles of papers we may miss studies that do not use these terms in the title since we extend our searches to include papers cited in the included papers as well as key conferences individual journals and key authors we are confident that the vast majority of key papers have been included studies included for synthesis the studies which passed our assessment criteria may still have limitations that make their results unreliable in the first place the data on which these models are built might be problematic as we did not insist that studies report data cleaning or attribute selection nor did we apply any performance measure based criteria so some studies may be reporting unsafe predictive performances this is a particular risk in regard to how studies have accounted for using imbalanced data this risk is mitigated in the categorical studies where we are able to report precision recall and f measure it is also possible that we have missed studies which should have been included in the set of from which we extracted data some studies may have satisfied our assessment criteria but either failed to report what they did or did not report it in sufficient detail for us to be confident that they should pass the criteria similarly we may have missed the reporting of a detail and a paper that should have passed a criterion did not these risks are mitigated by two authors independently assessing every study the boxplots the boxplots we present set performance against individual model factors e g modeling technique used this is a simplistic analysis as a number of interacting factors are likely to underpin the performance of a model for example the technique used in combination with the dataset and the independent variables is likely to be more important than any one factor alone furthermore metho dological issues are also likely to impact on performance for example whether feature selection has been used our boxplots only present possible indicators of factors that should be investigated within the overall context of a model more sophisticated analysis of a larger dataset is needed to investigate factors influencing model performance our boxplots do not indicate the direction of any relationship between model performance and particular model factors for example we do not investigate whether a particular modeling technique performs well because it was used in a good model or whether a model performs well because it used a particular modeling technique this is also important work for the future in addition some studies contribute data from many models to one boxplot whereas other studies contribute data from only one model this may skew the results we do not calculate the statistical significance of any differences observed in the boxplots this is because the data contained within them are not normally distributed and the individual points represent averages from different sizes of population c onclusions fault prediction is an important topic in software engineer ing fault prediction models have the potential to improve the quality of systems and reduce the costs associated with delivering those systems as a result of this many fault prediction studies in software engineering have been published our analysis of of these studies shows that the vast majority are less useful than they could be most studies report insufficient contextual and methodological information to enable full understanding of a model this makes it difficult for potential model users to select a model to match their context and few models have transferred into industrial practice it also makes it difficult for other researchers to meta analyze across models to identify the influences on predictive performance a great deal of effort has gone into models that are of limited use to either practitioners or researchers the set of criteria we present identify a set of essential contextual and methodological details that fault prediction studies should report these go some way toward addres sing the need identified by myrtveit et al for more ________________ hall et al a systematic literature review on fault prediction performance in software engineering reliable research procedures before we can have confidence in the conclusions of comparative studies our criteria should be used by future fault prediction researchers they should also be used by journal and conference reviewers this would ensure that future studies are built reliably and reported comparably with other such reliable studies of the studies we reviewed only satisfied our criteria and reported essential contextual and methodological details we analyzed these studies to determine what impacts on model performance in terms of the context of models the independent variables used by models and the modeling techniques on which they were built our results suggest that models which perform well tend to be built in a context where the systems are large we found no evidence that the maturity of systems or the language used is related to predictive performance but we did find some evidence to suggest that some application domains e g embedded systems may be more difficult to build reliable prediction models for the independent variables used by models performing well seem to be sets of metrics e g combina tions of process product and people based metrics we found evidence that where models use kloc as their independent variable they perform no worse than where only single sets of other static code metrics are used in addition models which perform well tend to use simple easy to use modeling techniques like naive bayes or logistic regression more complex modeling techniques such as support vector machines tend to be used by models which perform relatively less well the methodology used to build models seems to be influential to predictive performance the models which performed well seemed to optimize three aspects of the model first the choice of data was optimized in particular successful models tend to be trained on large datasets which contain a relatively high proportion of faulty units second the choice of independent variables was optimized a large range of metrics were used on which feature selection was applied third the modeling technique was optimized the default parameters were adjusted to ensure that the technique would perform effectively on the data provided overall we conclude that many good fault prediction studies have been reported in software engineering e g the which passed our assessment criteria some of these studies are of exceptional quality for example shivaji et al however there remain many open questions about how to build effective fault prediction models for software systems we need more studies which are based on a reliable methodology and which consistently report the context in which models are built and the methodology used to build them a larger set of such studies will enable reliable cross study metaanalysis of model performance it will also give practitioners the confidence to appropriately select and apply models to their systems without this increase in reliable models that are appropriately reported fault prediction will continue to have limited impact on the quality and cost of industrial software systems 